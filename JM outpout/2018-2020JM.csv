Number,Title,Abstract,Body
1,"A Cinderella Story: How Past Identity Salience Boosts Demand for Repurposed Products Like Cinderella, many repurposed products involve a biographical transformation, from a tattered past identity (e.g., an old airbag) to a product with a valuable but different new identity (e.g., a backpack made from an airbag). In this article, the authors argue that marketers should help customers infer such product stories by highlighting the products' tattered past identities. Three field experiments and four controlled experiments show that making a product's past identity salient boosts demand across a variety of repurposed products. This is because past identity salience induces narrative thoughts about these products' biographies, which in turn allows customers to feel special. Results also suggest that this strategy of past identity salience needs to be particularly well-crafted for products with easily discernible past identities. These findings highlight a promising new facet of storytelling (i.e., stories that customers self-infer in response to minimal marketer input); create new opportunities for promoting products with a prior life; and deliver detailed guidance for the largely unexplored, growing market for upcycled and recycled products.KEYWORDS_SPLITA worn-out airbag becomes a backpack. A leaky boat turns into a table. An old mosquito net transforms into a laptop sleeve. Product biographies that entail transformations from an old past identity to a new product identity are characteristic of production modes that involve the repurposing of old or dysfunctional products into new products. These include recycling, which is already common practice ([86]), and upcycling, which is gaining in popularity ([63]; [85]). For instance, a search on Instagram for upcycling yields over 1 million results, and the upcycled offers on the online marketplace Etsy have increased by 1,000% since 2011, with Etsy now featuring more than 300,000 different upcycled products in the United States alone. Many established companies, such as the outdoor brand Patagonia or the fashion retailers ASOS and Urban Outfitters, have also started to operate in this domain.However, when companies offer new products manufactured from old or waste products, how can they ensure that customers will demand these products? In this article, we show how the unique properties of repurposed and transformed products illustrate a novel way of storytelling in marketing. Unlike conventionally produced goods, such products have both a clear past and present product identity. Although they may vastly diverge in their form and purpose, both identities are embodied in the product. We argue that the past identity of a repurposed product amounts to the starting point of its biographical story of transformation, and that this holds storytelling potential. We refer to this strategy of alerting customers to a product's past identity as ""past identity salience"" and argue that it increases demand because it triggers narrative product thoughts and allows customers to feel special with the storied product. Notably, the past identity involves a waste product (e.g., a broken mosquito net) that may not serve the product's current primary function (e.g., a wallet made from a broken mosquito net). Even though a product's past identity is effectively useless (and potentially even disgusting), we suggest that marketers highlight it because it unlocks the product's storytelling potential.Results of three field experiments, four experiments conducted in controlled settings, and several replication studies support these claims. Our evidence pertains to different proxies of demand, stretches across multiple product categories and past identities, holds at both the store and product level, and emerges across different methods of making the past identity salient. We rule out several alternative accounts (perceived environmental friendliness, interestingness, surprise, originality, authenticity, a handmade effect, and informational value) and deduce two relevant boundary conditions. When a mere glance at the product allows customers to discern its past, simple appeals to the past identity do not further increase demand. Moreover, making the past identity salient only increases demand if the product had a prior life—and thus, a starting point of its biographical story. This applies to upcycling and the closely related practice of recycling ([55]; [86]) but not to conventionally produced products.The present research offers important contributions to the storytelling literature in marketing. First, we show that marketers can utilize products' own biographical stories rather than craft stories around product use or brand values, which dominate existing practices ([ 3]; [27]; [71]; [79]). Second, and in line with the story-prone nature of the human mind ([10]; [15]), we show that storytelling does not require explicit detail. Simple cues suffice for customers to note the presence of a story and essentially tell it to themselves ([40]). Notably, we show that this boosts demand even if the starting point of these stories is prosaic, such as a past identity as a mundane pallet or an old mosquito net. Third, and drawing on the finding that storied objects are deemed special ([34]), we identify a rarely examined key mechanism that underlies our effects on demand: storied products provide customers with felt specialness.We also contribute to the growing literature on products that entail the use of materials with a prior life and identity ([ 1]). As one of the first empirical studies of upcycling from a marketer's perspective, we highlight that success is more likely if marketers focus on the special biographies of upcycled products as their unique selling proposition ([46]). This is an important contribution, because it also extends to recycling and provides marketers of upcycled and recycled goods with actionable techniques to increase customer demand—an effort that is needed to realize the environmental and economic potential of these market practices ([48]). Conceptual Background Repurposed Products and the Salience of Past IdentitiesUpcycling is a sustainable production mode that prolongs the life of old objects by creatively reusing and reshaping them into new products ([13]; [85]). This repurposing practice allows brands to tap into new markets and generate value from what otherwise might be waste ([12]; [30]; [88]).Upcycling shares this benefit with other sustainable practices of product reuse, such as secondhand products ([ 2]; [45]; [46]), vintage products ([20]; [82]), or social recycling ([22]). However, these practices amount to the simple reuse of the same product by a different owner. In contrast, upcycling entails repurposing old products and results in a new product. It is thus similar to recycling, wherein the value is in old materials being transformed by breaking them down into raw materials before turning them into new products again ([12]; [77]; [84]). Both upcycling and recycling involve the repurposing of old products and entail a true and substantive transformation, in which the nature of the outcome product differs from the nature of its discarded source product.As a result of this transformation, repurposed products have two differing identities: a past identity, which is derived from the form and functionality of the source product, and a present identity, which captures the product's current and mostly different form and functionality. While marketers can focus on the upcycled product's benefits and emphasize elements of its present identity, they can also highlight the product's now dysfunctional past and draw customers' attention to the old or waste materials that compose the product. We call this strategy of alerting customers to the repurposed product's past identity ""past identity salience."" For example, an ad for a wallet made out of old mosquito nets could either not mention this past or explicitly state that it is made from mosquito nets, which is what some upcycling brands do. For example, the luxury bag brand Elvis & Kresse (www.elvisandkresse.com) prominently references to the past identities of its products in its communications, and the Swiss brand Freitag leverages the fact that its bags and accessories are made from truck tarps (www.freitag.ch). The Berlin-based store Upcycling Deluxe even enables its customers to search for products on the basis of what they used to be (www.upcycling-deluxe.com). Recycling brands, in contrast, frequently draw attention to the recycled nature of their products but do not disclose their specific past identities. It is still unclear whether the demand for repurposed products benefits or suffers from a strategy of ""past identity salience.""Several findings actually discourage highlighting repurposed products' past identities. Many customers are skeptical about purchasing used goods ([41]), and they can be sensitive to the physical distortion of a product, which makes them dispose of it more quickly ([77]). Indeed, upcycled products often show traces of wear and tear from their original purpose, which makes customers aware that they are not the first person to interact with the product. Drawing attention to the product's past identity may thus elicit processes of contagion that prevent customers from opting for a product contaminated by other people ([ 5]; [56])—a danger that has been observed in the context of product reuse ([45]) and could extend to repurposing.However, we suggest that it does not, and that repurposed products escape the stigma of the past because they have effectively been transformed into a new product (see [86]), who find that thoughts of transformation trigger recycling). Through this transformation, a salient past identity not only fails to harm demand but, on the contrary, fuels demand, and we propose that this is because past identity salience draws attention to the product's special story. Repurposed products can serve as a prime example of how past identity salience can trigger the persuasive story of a product's biographical transformation. Past Identity Salience as a Story CueThis argument is new to the storytelling literature in marketing. To support it, we first need to address the question ""What is a story?"" Essentially, a story is a linear temporal sequence of causally related events ([27], [28]). It is a chronological description of which events occurred and how they are connected. Another term consumer researchers have used to describe this is ""narrative"" ([ 3]; [27], [28]). Note, however, that other literature has distinguished between these terms in that ""story"" refers to a chronological chain of events and ""narrative"" to their causal factors ([68]; [73]).Chronology and causality are the central structural characteristics that enable narrative thinking ([19]; [27], [28]; [73]). Chronology refers to the temporal sequence or episodes of events ([65]) and to the fact that stories have a beginning, a middle, and an end ([28]; [74]). Causality is defined as the causal connections or relationships between story elements ([27]) that enable customers to assign meaning to a narrative. Rather than causality, some authors use the term ""plot,"" which refers to the theme of a story and imbues story events with meaning ([65]). Stories can feature any number of specific story elements, which are woven together in a plot. Several typologies have been offered to explain the plots of stories throughout history (e.g., [ 9]; [16]; [76]). Although plots differ, a prototypical narrative features a protagonist who is the main character of the story ([65]), such as Cinderella, who participates in story events ([19]). Notably, the mere presence of a main character can serve as a means to provide causality ([19]; [79]).Humans have been attuned to stories and narrative thinking since the dawn of humankind and stories wield considerable power over people ([15]). They are able to demonstrate, communicate, and persuade ([27], [26]). The marketing literature has been well aware of stories' ability to fuel demand ([79]; [83]; [87]), and storytelling is a common marketing practice ([71]; [80]). Marketers tell brand stories ([71]) and stories in which products affect consumers ([49]; [83]), and artists tell stories that feature products or brands as contextual elements or accessories, as in the case of product placement ([44]; [70]). In most of these stories, the product is not the main protagonist, and all of them are clearly recognizable as fully fledged stories that someone tells.Advancing current theorizing, we suggest adding the product's own biographical story to marketers' storytelling toolbox. Moreover, we suggest that marketers can trigger these stories without spelling them out. Simply making the product's past identity salient, as we propose, can induce customers to infer a repurposed product's biographical story and in turn increase demand for the storied product.But how can customers comprehend such stories when an ad features no more than the product and a reference to its past identity? The answer lies in the fact that humans are uniquely attuned to engage in narrative thinking and discern, self-tell, and appreciate a good story ([ 3]; [27]). Moreover, even simple past identity appeals map onto the key structural story characteristics of chronology and causality. As to chronology, a product's past identity is an episode in its life that chronologically precedes its present identity. Making the past identity salient thus ensures that multiple chronologically ordered episodes in the product's life become salient. As to causality, salience of the product's past and present identities implies their causal connection. The repurposed product is the protagonist that ensures causality through its implied identity transformation.Transformational stories are historically one of the most popular forms of story ([16]; [67]). They involve a change in identity as part of the protagonist's biography. Also known as metamorphosis ([76]), this type of plot involves a temporally bounded event (i.e., the transformation) wherein the protagonist changes from one permanent state to a new permanent state without the disappearance of the protagonist from the story ([40]). Metamorphosis plots are present throughout history ([33]), ranging from stories from antiquity (e.g., Ovid's Metamorphoses), to more recent novels (e.g., Franz Kafka's The Metamorphosis), fairy tales (e.g., Cinderella), and popular culture (e.g., The Incredible Hulk, The Matrix). Notably, it also extends to the biographical stories of repurposed goods, which transform from being a depreciated product on the verge of the waste-bin to a storied new object with multiple identities.Transformational stories are particularly powerful in terms of inducing narrative thoughts. They only need what narratologists term a ""minimal narrative"" to unfold in people's minds. A minimal narrative consists of identical entities that are present in two temporally and qualitatively distinct states ([66]). The essence of minimal narratives is time change and, often, transformation ([54]). In our research, the minimal narrative consists of the protagonist in time 1 (the salience of the product's past identity) and the transformed protagonist in time 2 (the upcycled or recycled present product identity). Minimal narratives are more than a mere ordered sequence of events because of their overall meaning ([54]), which unfolds in perceivers' minds.We propose that customers will infer a repurposed product's story when its past is salient because this salience allows for an awareness of the plot (i.e., how the different identities are connected via the transformation). To attain that meaning, customers need to engage in inferential processing. Inferencing forms the narrative linkages that empower minimal narratives (past identity, present identity). To engage in inferential processing, individuals need to draw on their personal knowledge and imagination. In a dynamic process, individuals interact with the presented story elements, infer missing information, and include past information to disambiguate stories ([32]; [42]). Research in psychology and consumer research shows that humans are genuinely prone to take agency and engage in inferencing and using their imagination to generate story causality ([27]; [35]; [79]) and to thus complete or comprehend a story that is not fully spelled out.A plot of transformation provides a particularly rich resource for inferential narrative processing. The multiple identities and the metamorphosis itself allow for multifaceted interpretations ([32]; [42]) and provide perceivers with a wide projection; inferencing; and, thus, storytelling space. We therefore expect that a salient cue for a repurposed product's past identity will suffice to induce perceivers to infer its biographical story of transformation. Specialness as a MechanismWhy would people demand a product that holds a story of having been waste more than a product that does not? We propose that storied products cause higher demand because they imbue customers with felt specialness—that is, the belief that they will feel more special as a result of acquiring and utilizing a product that holds a story.To understand how stories can evoke a sense of specialness in people, it is important to understand the ways in which stories affect people ([15]; [27]; [49]). On the one hand, stories focus them on the narrative rather than on rational arguments ([49]), potentially even transporting (i.e., absorbing) recipients into a story ([36]; [79]). However, for this to happen, people must experience the pathos of a dramatic story ([64]) and feel empathy for story characters ([79]). This tends to necessitate the act of telling a fully-fledged story to passive recipients. The minimal narrative provided by quick exposure to an ad including past identity cues is unlikely to allow for either of these experiences.On the other hand, and largely fueled by inferential processes, stories help people in their sensemaking process ([87]). This mechanism is well-suited to explain the appeal triggered by minimal narratives. People's narrative thoughts help shape their perceptions of the story's protagonist and its meanings ([27]). When stories are self-inferred, they are personal to the individual ([74]; [83]) and can evoke diverse special and individual meanings ([57]; [69]). For example, if a customer sees an ad for a wallet made from an old mosquito net, they might draw on their own associations and think of the lives the net has saved or connect it with their own personal travels. These self-inferred, special meanings help decommodify the object ([25]; [46]).Simply ""having"" a story suffices for the protagonist to become more special. Cinderella, for example, is a special princess thanks to her story of transformation (see [31] for how biographical transformations affect the appreciation of the ""transformer""). Like Cinderella, repurposed products are the protagonist in a transformational story that imbues them with a unique biographical history, and objects with such a story are likely to be perceived as special ([46]), even by young children ([62]).Importantly, the meanings of objects tend to transfer to those who acquire and utilize them ([52]) and can help individuals in their identity work, a process that often motivates the decision of whether to acquire an object ([ 4]; [ 7]; [14]). Storied—and thus, special—objects have the power to make individuals feel special about themselves. Given that objects that promise feelings of specialness are known for being high in demand ([ 8]; [38]), storied objects are likely to boost demand ([34]; [58]).In summary, we propose that making a repurposed product's past identity salient alludes to the minimal narrative of its transformational story. This invites customers to engage in narrative thinking and allows them to infer an individualized and special version of the product's story. Perceiving the product as storied, in turn, enables customers to feel special with the product, which eventually triggers demand. Figure 1 provides an overview of the propositions made.Graph: Figure 1. Conceptual model. Summary of StudiesWe test our propositions in seven studies. Studies 1a, 1b, and 2 provide evidence for the effect of past identity salience on real-world product demand. Studies 1a and 1b investigate this in actual Facebook campaigns. Study 2 examines sales data for an experimental upcycling pop-up store. Studies 3, 4, and 6 demonstrate that past identity salience increases demand because the product's biographical story affords people with specialness and generalizes our findings to different types of products and past identities as well as the various claims that make these identities salient.We also provide evidence of the limits of past identity salience. Marketer-crafted allusions to a product's past identity are not the only means through which individuals may infer a product's biographical story. Products with an easily discernible past identity, such as a bag sewn out of highly visible candy wrappers, already provide all the cues needed for the story to unfold in a customer's mind. We thus do not expect additional appeals to a product's past identity to increase demand when the past identity is already salient. Study 5 demonstrates that visual discernibility of a product's past identity acts as a moderator.Given that any biographical transformation requires a past identity, we also do not expect results to generalize to genuinely new products. Study 6 thus compares different production modes and shows that our effect is specific to repurposed products. In particular, it extends our findings to the more prevalent market practice of recycling and demonstrates that they do not generalize to conventional modes of production.Finally, we address several alternative explanations across studies. Study 3 rules out the possibility that the effects are driven by increased perceptions of environmental friendliness. Study 4 shows that results are not a manifestation of a handmade effect, and Study 6 addresses the possibility of other plausible confounds (perceived originality, authenticity, surprise; see also the Study 2 posttest in the Web Appendix). All stimuli (Web Appendix W1), additional analyses (Web Appendix W2), supplementary studies (Web Appendix W3), and replication studies (Web Appendix W4) are included in the Web Appendix. Studies 1a and 1b: Real Campaigns and Online ResponsesIn Study 1, we use two online field experiments to examine how people respond to Facebook ads that make the past identities of upcycled products salient. In Study 1a, we examine how these ads affect Facebook page likes. In Study 1b, we look at the effects on clicks. Method Participants and procedureTo ensure ecological validity, we teamed up with an upcycling store and jointly created two Facebook ad campaigns. The objective of the first campaign (Study 1a) was to increase the number of likes of the store's Facebook page. The objective of the second campaign (Study 1b) was to drive traffic to an external website featuring an online voucher promotion. Each campaign targeted people between the ages of 18 and 65 years old (Facebook estimated a potential target audience of 1.34 million individuals) living in the store's vicinity. To prevent people from being exposed to both campaigns on the same day, we activated only one at a time for seven days and six days, respectively. StimuliWe created small rectangular ads that featured different upcycled products from the store: a cake stand made from old pot lids, a vase made from a light bulb, and a pen holder made from used forks (Web Appendix W1). We manipulated past identity salience by stating what the products used to be (""I used to be a...pot lid, light bulb, fork""). The control group read: ""Now I am...a cake stand, a vase, a pen holder."" By using the term ""Now I am,"" it also hinted at a transformation but made the products' present identities salient. MeasuresIn Study 1a, we measured unique and total like rate (the number of page likes the ad generated relative to its unique and total reach). In Study 1b, we measured unique and total click rate (the number of clicks the ad generated relative to its unique and total reach). To make measures comparable and to control for variance in unique reach (total number of unique people who saw the ad) and total reach (total number of times the ad was shown), we used relative measures (i.e., likes/clicks in percentages of reach) in both studies. Results and DiscussionTable 1 presents a summary of campaign statistics and shows ad performance across conditions. Because we only had access to aggregate behavioral data, we conducted two-sample proportions z-tests to determine which ad was relatively more successful.GraphTable 1. Descriptives of Appeal Measures by Condition (Studies 1a, 1b, and 2).   Study 1aAs expected, the ad yielded a higher unique (.16% vs..06%; Z = 6.72, p <.001) as well as total like rate (.06% vs..03%; Z = 4.65, p <.001) when the products' past identities were made salient. Study 1bA similar pattern emerged for the promotion campaign. Past identity salience significantly increased unique (.29% vs..26%; Z = 1.91, p =.06) and total (.11% vs..08%; Z = 4.97, p <.001) clicks on the promotion. DiscussionDrawing on ecologically valid experimental field evidence in an online context and featuring a portfolio of different upcycled products, Studies 1a and 1b show that past identity salience can increase demand for products of an upcycling store. Owing to the context in which low levels of engagement are common (for average clickthrough rates, see [17]]), absolute effect sizes were quite small, even though like rates doubled and click rates increased by more than 37%. We designed Study 2 to provide ecologically valid field evidence in a context that allows for more pronounced absolute differences. Study 2: Real Purchases in an Upcycling Pop-Up ShopStudies 1a and 1b measured online interest as a demand proxy. Study 2 extends the inquiry to an experimentally controlled brick-and-mortar context and actual sales data. Method Participants and procedureIn collaboration with two upcycling stores, we set up our own upcycling pop-up store on the campus of a large European university. The assortment included 24 different upcycled products (e.g., bags, wallets, bowls). Items were made from a variety of source products (e.g., mosquito nets, parachutes, bicycle tubes) and priced between €5 and €85. Study participants were all potential customers who passed the shop. The shop was open over six days for five hours each day during the Christmas season in 2017. We hired two sales assistants to run the shop on alternate days. To allow for a constant setup, we provided them with a sales script. We instructed them to be friendly but passive (no sales pitches) and to discuss the product's past identity only when prompted by customers. We also instructed assistants to take notes of all that happened during the day. Stimuli and measuresWe manipulated past identity salience by alternating the marketing materials at the point of sale. These included ( 1) a leaderboard that highlighted the products' past (""We used to be...parachutes, truck tarps, pot lids, etc.""; experimental condition) or present (""We are...bags, wallets, cake stands, etc.""; control condition) identity, ( 2) a corresponding price list with individual price tags and product descriptions, ( 3) product flyers, and ( 4) a promotional poster (Web Appendix W1). We changed conditions on a daily basis.For each day, we used the receipt data to assess the number of purchases, the number of products sold, and total revenue. To account for any effects associated with general customer frequency and engagement, the sales assistants tracked the number of visitors to the shop with a manual clicker, self-assessed the busyness level at the shop site every 30 minutes (1 = ""not very busy,"" and 7 = ""very busy""), and measured conversation time per visitor. Results and Discussion Main resultsAverage busyness at shop site (Msalient = 2.55, SD = 1.17; Mcontrol = 2.77, SD = 1.34; t(58) =.46, p =.65) and conversation time per visitor (Msalient =.56 min, Mcontrol =.50 min; z = 1.09, p =.28) did not differ across conditions. When the past identities were made salient at the point of sale, however, the shop had approximately 60% more visitors (266 vs. 165; Mann–Whitney U-test: z = 1.96, p <.05); triple the amount of purchases (25 vs. 8; z = 1.99, p <.05); four times more products sold (36 vs. 8; z = 1.99, p <.05); and, as a result, more than quadruple the revenue (€572 vs. €127; z = 1.96, p <.05). Moreover, the conversion rate (proportion of visitors making a purchase) was nearly twice as high when the products' past identities were made salient (9.4% vs. 4.8%; z = 1.63, p =.08, Table 1). DiscussionMaking the past identities of the products salient to customers in an actual upcycling store increased demand beyond our expectations and with regard to every single factor that can increase revenue: interest (i.e., visitors), conversion, and sales volume. To explore the underlying dynamics of these effects, we followed up on Study 2 with a posttest (Web Appendix W3), which revealed that our effects are unlikely to have been driven by how interesting, boring, or surprising people perceived the products to be.[ 6] Instead, and in line with our assumptions, people feel more special with the products, find them more appealing, and are more likely to purchase them when products' past identities are made salient. Study 3: Felt Specialness and a Product's Biographical StoryHaving established real-world support for our proposed main effect, Study 3 aims to show that past identity salience affects demand because it increases customers' felt specialness with the product. We proposed that past identity salience would increase felt specialness because it triggers thoughts about the product's biographical story. To test for this, we also qualitatively explore whether past identity salience triggers narrative thoughts—and, if so, what elements of the product's story these thoughts refer to. Finally, we address the alternative explanation of felt environmental friendliness. Repurposing products is a proenvironmental practice. Stressing the product's past identity may thus play into sustainable purchase motives ([60]). We aim to rule out that past identity salience may drive demand because it increases felt environmental friendliness rather than specialness. Method Participants, design, and procedureTwo hundred twenty-four U.S. panelists from Amazon Mechanical Turk (MTurk; 44% female, Mage = 35 years) were instructed to evaluate a backpack upcycled from an old airbag in a one-factor (past identity: salient vs. control) between-subjects experiment. We manipulated past identity salience as in prior studies (see Web Appendix W1) and informed all participants that the backpack was upcycled. Participants in both conditions thus rationally knew that the product had a past identity, but this was made salient and concrete in only one condition. MeasuresAs proxies of demand, we first assessed the backpack's appeal (""How would you evaluate this product?"" 1 = ""unappealing/don't like it at all,"" and 7 = ""appealing/like it a lot""; α =.95) and participants' purchase intention (""Would you buy this product?"" 1 = ""No, definitely not,"" and 7 = ""Yes, definitely""). We measured felt specialness as the focal process variable (three items adapted from [53]]): ""How special/unique/recognized would you feel with this product?"" 1 = ""not at all,"" and 7 = ""very""; α =.91) and felt environmentalism as an alternative process (three similar items: ""How sustainable/environmentally conscious/environmentally friendly would you feel with this product?"" 1 = ""Not at all,"" and 7 = ""Very""; α =.93). To explore whether an appeal to the past identity would trigger narrative thoughts, we asked participants to describe what they thought was special about the product in an open-ended question. Finally, we asked whether they took the study seriously and answered conscientiously. Two participants who had indicated that they did not were excluded from further analyses. Results and Discussion Product demandIn line with previous results, past identity salience increased product demand. In particular, participants perceived the backpack as more appealing (Msalient = 4.80, SD = 1.84; Mcontrol = 4.04, SD = 1.94; t(220) = 3.01, p <.01) and were more likely to purchase it (Msalient = 4.17, SD = 2.00; Mcontrol = 3.43, SD = 2.07; t(220) = 2.70, p <.01) when its past identity as an old airbag was made salient. Process variablesParticipants also perceived the backpack as more specialness-affording (Msalient = 4.34, SD = 1.68; Mcontrol = 3.82, SD = 1.81; t(220) = 2.22, p <.05) and felt more environmentally friendly with the backpack (Msalient = 5.37, SD = 1.33; Mcontrol = 4.29, SD = 1.70; t(220) = 5.28, p <.001) when its past identity was made salient. Mediation analysesTo test for the proposed effect of past identity salience on demand through specialness, we conducted two bootstrap mediation analyses (Model 4, [39]). We entered past identity salience as the independent variable (0 = control, 1 = salient), felt specialness as the mediator, and the respective demand variables as dependent measures. Because past identity salience also affected how environmentally friendly people felt, we included it as an alternative mediator. We found an indirect effect of past identity salience on product appeal (indirect effect =.40; 95% confidence interval [CI95] = [.05,.79]) and purchase intention (indirect effect =.44; CI95 = [.06,.86]) through felt specialness. Felt environmentalism did not mediate the effect on appeal (indirect effect = −.02; CI95 = [−.23,.20]) or purchase intention (indirect effect =.002; CI95 = [−.19,.20]). It can thus be ruled out as an alternative process. Type of thoughts triggeredFinally, we analyzed the open-ended answers to explore differences in narrative thinking across conditions. All answers were coded in terms of whether or not they signaled narrative thinking and in terms of their content elements. Thoughts that hinted at chronology and causality, the central structural characteristic of narrative thinking, were coded as narrative, whereas other thoughts were coded as descriptive. As to the content elements, eight recurring themes emerged. Three narrative thought topics focused on different story elements. The respective codes are past identity, in which the prior life of the product was prominent; metamorphosis, in which the product's transformation was prominent; and other biographical elements, in which chronology and causality of the product's story were present but neither could be clearly identified as central. The five descriptive thought topics comprise type of material, production mode, environmental aspects, other product attributes, and product evaluations (for descriptions and examples, see Table 2). Two independent raters who were blind to the condition coded responses with regard to the presence of each of these codes. More than one code could be assigned to one response, and remaining disagreements were resolved through discussion (interrater reliability: all κs >.61).GraphTable 2. Study 3: Thoughts and Themes Associated with Product Specialness by Condition.  1 a Numbers in brackets refer to participant numbers.Table 2 shows the relative prevalence of codes across conditions. Despite the fact that all participants knew that the backpack had a history, narrative thoughts were more pronounced when the ad made the product's past salient. Participants in the past identity salience condition more often reported narrative thoughts (69.6%) than those in the control condition (2.8%; χ2 = 105.53, p <.001). In particular, they more often referred to the product's past identity (66.1% vs. 1.9%; χ2 = 100.30, p <.001), its metamorphosis (50.4% vs..9%; χ2 = 69.60, p <.001), and other biographical story elements (17.4% vs..9%; χ2 = 17.53, p <.001). In addition, descriptive thoughts about the production mode became more prevalent (34.8% vs. 15.9%; χ2 = 10.37, p <.01) when the past life of the product was made salient. In contrast, descriptive thoughts of general product attributes (37.4% vs. 61.7%; χ2 = 13.09, p <.001) and evaluations (21.7% vs. 35.5%; χ2 = 5.18, p <.05) were more prevalent in the control condition. Thoughts about the product's material (19.1% vs. 18.7%; χ2 =.01, p =.93) or its environmental aspects (7.8% vs. 4.7%; χ2 =.93, p =.33) were equally prevalent across conditions. DiscussionStudy 3 replicates the effect of past identity salience on product demand in a controlled setting and supports our proposed account. Making the past identity of an upcycled product salient boosted its specialness-affording potential, which, in turn, increased product appeal and purchase intention. Although past identity salience also affected how environmentally friendly people felt with the product, environmental friendliness did not mediate the effect.[ 7] In addition, Study 3 provides qualitative insights into why customers feel more special once the past identity of a repurposed product is made salient. Past identity salience appears to trigger the product's story and brings to mind additional narrative thoughts that relate in particular to the product's past identity and metamorphosis while decreasing evaluative descriptions and focus on other attributes such as design or weight. Study 4: Testing the Full ProcessStudy 4 provides additional evidence about the proposed process. It tests whether past identity salience enhances people's perceptions of the product as storied, which in turn induces felt specialness and demand (see Figure 1). To corroborate the role of felt specialness as a driver of demand, we not only measured felt specialness but also tried to moderate it. If felt specialness drives the effect, then this route should be less pronounced for those who feel very special already. In addition, Study 4 ensures robustness of the results by extending the inquiry to minimal appeals (i.e., simple ""made from"" claims that neither spell out the story directly nor depict the past identity) and to a new product category and different source product (i.e., a wooden table made from a pallet). Finally, it also addresses another viable alternative explanation: it is possible that highlighting the past identity of a product induces people to think that the product is handmade, a product characteristic that is known to increase product attractiveness ([29]). Method Participants, design, procedure, and measuresA total of 98 MTurk workers (41% female; Mage = 37 years) were randomly assigned to one of two ads that promoted a wooden table (Web Appendix W1). In the past identity salience condition, the ad read, ""I was made from an old pallet""; in the control condition it read, ""I was made for dining."" Prior to ad exposure, we assessed baseline feelings of personal specialness with the same three items used to capture product-specific felt specialness (""In general, how special/unique/recognized by others do you feel?"" 1 = ""not at all,"" and 7 = ""very""; α =.86). After ad exposure, we measured product appeal (α =.93) and purchase intention as in Study 3. To assess whether past identity salience triggers narrative thoughts about the product and its biography, we adapted four items from [49] (""The product tells a story,"" ""The product's story has a beginning, a middle and an end,"" ""The product has evolved over time,"" ""The story of the product has a chronological order""; 1 = ""Strongly disagree,"" and 7 = ""Strongly agree""; α =.95). Finally, we measured felt specialness, as in Study 3 (α =.93), and assessed the degree to which participants thought that the product was homemade (""This product looks..."" 1 = ""homemade,"" and 7 = ""made by a company""). Results and Discussion Main effectsWhen the past identity of the wooden table was made salient, participants perceived it as more appealing (Msalient = 5.15, SD = 1.56; Mcontrol = 4.29, SD = 1.69; t(96) = 1.80, p <.05) and were more likely to buy it (Msalient = 5.21, SD = 1.50; Mcontrol = 4.48, SD = 1.70; t(96) = 2.25, p <.05). Past identity salience also increased product story perceptions (Msalient = 5.13, SD = 1.30; Mcontrol = 3.89, SD = 1.95; t(96) = 3.71, p <.001) and made participants feel more special with the product (Msalient = 4.85, SD = 1.49; Mcontrol = 3.90, SD = 2.00; t(96) = 2.66, p <.01). Felt personal specialness prior to ad exposure (Msalient = 5.04, SD = 1.48; Mcontrol = 5.02, SD = 1.37; t(96) =.07, p =.95) and perceptions of the product as handmade (Msalient = 4.32, SD = 2.03; Mcontrol = 4.67, SD = 1.94; t(96) = −.87, p =.39) did not differ across conditions. Note that the latter also did not moderate the effect of past identity salience on demand (see additional analyses in Web Appendix W2). The role of felt specialnessTo corroborate the role of felt specialness, we ran moderated mediation analyses (Model 7, [39]) with past identity salience as the predictor (0 = control, 1 = salient), felt specialness as the mediator, product appeal and purchase intention as the outcome variables, and baseline personal specialness as a continuous moderator (M = 5.03, SD = 1.42). In support of our propositions, felt specialness mediated the effect for people low (−1 SD) and average (mean) in felt personal specialness, but not for people who already felt very special to begin with (+1 SD) (see Table 1 and Web Appendix W2). Sequential mediationWe next tested whether product story perceptions led to an increase in felt specialness and, as a result, in demand. Two bootstrap sequential mediation analyses (Model 6, [39]) with past identity salience as the independent predictor (0 = control, 1 = salient), product story and felt specialness as sequential mediators, and product appeal and purchase intention as outcome variables found evidence for sequential mediation via product story and felt specialness on product appeal (indirect effect =.15, CI95 = [.06,.27]) and on purchase intention (indirect effect =.15; CI95 = [.05,.28]). Notably, both individual indirect effects through product story and felt specialness on demand became nonsignificant (CIs include 0). Moreover, switching the order of the mediators resulted in a nonsignificant mediation (CIs include 0). This supports the proposed sequential mediation chain. DiscussionResults of Study 4 rule out the competing explanation that our effects are due to the handmade effect ([29]) and fully support our proposed process. Even a simple ""made from [past identity]"" claim increased demand, and this was due to the claim imbuing the product with a story, which in turn made customers feel more special with the product. Study 4 thus supports our proposition that a product's past identity holds storytelling potential. It also corroborates the central role of felt specialness as our underlying process. Baseline felt specialness moderated the effect of past identity salience on specialness and, as a result, the indirect effect of past identity salience on demand. Study 5: The Moderating Role of Past Identity DiscernibilityOur findings suggest that past identity salience is effective because it imbues products with a prior life with their specialness-affording biographical story. However, what if the product itself already tells the story (i.e., the past identity is visually discernible and salient in the product)? Our theorizing suggests this to be a relevant boundary condition for the power of ad-induced past-identity salience. In Study 5, we test for this boundary condition. Specifically, we used a vase made from an obviously discernible light bulb and a vase made from a less discernible electric insulator (for a replication across product categories, see Web Appendix W4). We expect our effect to generalize to the insulator vase but to be attenuated for the light bulb vase because its past life is already salient. Method Participants, design, and procedureWe recruited 562 volunteers from a university mailing list who were familiar with upcycling (61% female; Mage = 24 years) to participate in a 2 (past identity: salient vs. control) × 2 (past identity discernibility: subtle vs. discernible) between-subjects experiment. Depending on condition, participants saw an ad that either did or did not highlight the past identity of a vase made either from an easily discernible light bulb or from a less discernible electrical insulator (see Web Appendix W1). MeasuresWe measured product appeal (""How much do you like this vase?"" 1 = ""don't like it at all,"" and 7 = ""like it a lot""), purchase intention (""If you were looking for a vase, would you buy this particular one?"" 1 = ""No, definitely not,"" and 7 = ""Yes, definitely""), and willingness to pay (WTP; ""What is the maximum amount you would pay for this vase?"" open-ended). As a study incentive and a measure of behavioral product demand, participants could choose to win a product of their choice at the end of the study: the promoted upcycled or a conventional vase. Results and Discussion Product appealA two-way analysis of variance on product appeal produced a main effect of past identity salience (F( 1, 558) = 8.19, p <.01) and a main effect of past identity discernibility (F( 1, 558) = 34.60, p <.001) on product appeal. The prior effect is in line with previous results, and the latter is indicative of product differences. Importantly, these effects were qualified by a marginally significant interaction (F( 1, 558) = 3.41, p =.06). Planned contrast comparisons revealed that past identity salience increased appeal when the past identity of the vase was not discernible (Msalient = 4.03, SD = 1.68; Mcontrol = 3.37, SD = 1.60; t(280) = 3.36, p <.01). It did not, however, affect the appeal of the visibly discernible vase (Msalient = 4.59, SD = 1.68; Mcontrol = 4.45, SD = 1.65; t(278) =.71, p =.48). Purchase intentionA two-way analysis of variance again produced a main effect of past identity salience (F( 1, 558) = 10.99, p <.01) and past identity discernibility (F( 1, 558) = 20.56, p <.001), qualified by a marginally significant interaction effect (F( 1, 558) = 3.63, p =.06). Making the past identity salient boosted demand when it was difficult to discern (Msalient = 3.18, SD = 1.74; Mcontrol = 2.46, SD = 1.46; t(280) = 3.78, p <.001) but not when it was visibly discernible (Msalient = 3.55, SD = 1.67; Mcontrol = 3.35, SD = 1.69; t(278) =.98, p =.33). Willingness to payTo assess effects on the highly skewed WTP measure, we built three equally sized groups of amounts that participants were willing to pay for the vase (<€5, €5–€10, ≥€10; Figure 2, Panel A). Chi-square tests per product show that past identity salience led to a significant increase in the number of people who were willing to pay more than €10 (57% vs. 43%; χ2 = 5.53, p =.06) when the past identity was difficult to discern, but this did not affect WTP when the past identity was visible to begin with.Graph: Figure 2. Effects of past identity salience (Study 5). Product choiceFinally, we ran a logistic regression with past identity salience, past identity discernibility, and their interaction term as the predictors, and product choice as the outcome. We found a significant interaction effect (Wald = 3.70, B =.68, p =.05) but found neither a main effect of past identity salience (Wald = 2.24, B = −.82, p =.13) nor a main effect of past identity discernibility (Wald = 1.29, B = −.62, p =.26). Past identity salience increased the choice of the upcycled product when its past identity was hard to discern (χ2 = 4.42, p <.05) but not when it was easily discernible (χ2 =.36, p =.55; Figure 2, Panel B). DiscussionStudy 5 demonstrates that making the past identity salient is effective, particularly when customers cannot easily discern this identity and infer the product's biographical story by simply looking at it.[ 8] When the past identity was discernible, emphasizing it did not boost demand further. To see how this boundary condition can nonetheless be overcome, see Studies 4a and 4b in Web Appendix W3, which also rule out that a salient past identity could be effective simply because it offers more information. Study 6: Reinforcing the Importance of a Prior LifeStudy 5 showed that appeals to a product's prior life primarily work if this prior life is not salient already. Study 6 addresses another managerially relevant boundary condition. So far, we have tested our propositions in the context of upcycled products, which doubtlessly hold prior identities. In Study 6, we thus ask whether the effect is indeed specific to repurposed products (i.e., products with a prior life). To do so, we generalize our inquiry to recycled products, which are also made from products with a prior life, and conventional products, which are made from brand-new raw materials that lack such a prior life. We expect results to generalize to recycled products but not to conventional products. In addition, we control for various alternative accounts. It is plausible that making the past identity of a product salient might simply trigger surprise and perceptions of product originality or authenticity.[ 9] All of these are connected to positive customer responses and have been studied in the context of vintage products, which are—like upcycled and recycled products—characterized by strong past identities (e.g., [20]; [78]; [82]). Moreover, surprise and novelty have been connected to storytelling ([27]). Method Participants, design, and procedureWe used the same backpack as in Study 3 and randomly assigned 163 individuals (57% female; Mage = 32 years) to one of four product ads, which manipulated production mode and thus the backpack's past identity (Web Appendix W1). All conditions featured the same slogan ""I am a trendy backpack."" In the upcycling condition, participants also read, ""In my previous life, I used to be an airbag."" In the recycling condition, they read, ""In my previous life, I used to be old plastic,"" and in the conventional condition, they read, ""In my previous life, I used to be polyester."" Past identity was not made salient in the control condition; it only read, ""I am a trendy backpack."" MeasuresFollowing ad exposure, we first assessed demand variables. Product appeal (α =.94) and purchase intention were measured as in Study 4, and WTP was measured on a slider scale ranging from €0–€200. In addition, we assessed relative purchase intention by asking participants to rank different backpacks according to their purchase likelihood (1 = ""highest"" to 4 = ""lowest"" likelihood). These backpacks included our focal backpack from the ad and three additional backpacks participants had not been exposed to before. To corroborate the process, we also measured perceptions of product story (α =.90) and felt specialness (α =.93) as in Study 4. As a manipulation check, we added three items assessing the degree to which participants perceived that the product had a prior life (i.e., a true past identity; ""The product contains..."" 1 = ""little history/past/identity,"" and 7 = ""a lot of history/ past/identity""; α =.94). Finally, we assessed perceived originality (""This product is novel/original/different""; α =.78), adapted from [47], as well as surprise (""The product surprises me/is unexpected""; α =.91) and authenticity (""The product is authentic/genuine and real""; α =.89) adapted from [43] and [59], all on seven-point scales (1 = ""Strongly disagree,"" and 7 = ""Strongly agree""). Results and DiscussionTable 3 provides means, standard deviations, and pairwise comparisons of all assessed variables by condition. Conditions differ with regard to all assessed variables.GraphTable 3. Study 6: Means Across Conditions (Past Identity Salience of Different Production Modes).  2 Notes: Different superscripts indicate different cell means (p <.05) based on planned contrast comparisons. Manipulation checkIn confirmation of the intended manipulation, we found stronger past identity perceptions in both the upcycling and recycling conditions than in the conventional backpack and control conditions (F( 3, 159) = 33.12, p <.001). Main effectsConditions also significantly differ on product appeal (F( 3, 159) = 7.25, p <.001), purchase intention (F( 3, 159) = 7.41, p <.001), WTP (F( 3, 159) = 3.72, p <.05), relative purchase intention (F( 3, 159) = 4.32, p <.01), product story perceptions (F( 3, 159) = 12.48, p <.001), felt specialness (F( 3, 159) = 24.60, p <.001), product originality (F( 3, 159) = 4.32, p <.01), surprise (F( 3, 159) = 4.32, p <.01), and authenticity (F( 3, 159) = 4.32, p <.01). Compared with the control group, all measures of demand were higher in both the upcycling and recycling conditions but not for the conventional backpack. Likewise, product story perceptions and felt specialness were significantly higher when the product was upcycled or recycled but not when it was conventionally produced. The same significant pattern of results emerged for our alternative processes, perceived originality, surprise, and authenticity. We thus controlled for them in all subsequent analyses. Sequential mediationTo test whether past identity salience affects demand through product story and felt specialness, we ran sequential mediation analyses per demand variable. Production mode served as the multicategorical predictor (Model 6, [39]). We compared all groups in which we had made a past identity salient (upcycling, recycling, conventional) to the control group in which we had not done so. To control for potential confounds, we added our alternative process variables as covariates to the model (Web Appendix W2 provides further analyses ruling out these suggested accounts). We found evidence for a sequential mediation from past identity salience through product story and felt specialness on product appeal in both the upcycling (indirect effect =.09, SE =.05; CI95 = [02,.20]) and the recycling (indirect effect =.14, SE =.07; CI95 = [03,.30]) conditions. For these two production modes, we replicate the significant indirect effects on purchase intention (upcycling: indirect effect =.08, SE =.04, CI95 = [02,.18]; recycling: indirect effect =.13, SE =.06; CI95 = [03,.26]), WTP (upcycling: indirect effect =.74, SE =.43; CI95 = [.03, 1.72]; recycling: indirect effect = 1.21, SE =.58; CI95 = [.27, 2.55]), and relative purchase intention (upcycling: indirect effect = −.02, SE =.02; CI95 = [−.05, −001]; recycling: indirect effect =.−.03, SE =.02; CI95 = [−.08, −.003]). In the conventional product condition, however, neither indirect effect became significant (all CIs included 0). DiscussionStudy 6 extends our findings to a different context and a new product category. It also addresses an important boundary condition. Our effects are specific to products that are—like upcycling—associated with a prior life (i.e., recycled products) but are attenuated for products that carry little past identity in them (i.e., conventional products). Study 6 also rules out important alternative accounts around perceptions of product originality, surprise, and authenticity. While past identity salience boosts all of these factors, it is the story-induced felt specialness that primarily drives demand. General DiscussionIf products could speak, then each product made from repurposed materials would have a rich story to tell: a Cinderella-like story of a change in identity and purpose, from a discarded and useless past identity to a vastly changed and useful new identity as a new product. We show that appealing to the past identity (i.e., a discarded, broken object that is at odds with the product's current functional purpose) fuels demand for upcycled and recycled products. This is because past identity salience draws attention to the product's special story of metamorphosis, which allows customers to feel special themselves. These insights advance our current understanding of customer reactions to goods made from repurposed materials and open the window to a new facet of powerful storytelling in marketing. Theoretical ImplicationsIn this article, we provide novel contributions to the extant literature on storytelling and to the burgeoning literature on upcycling and recycling as two production modes that involve the use of repurposed materials. With regard to the storytelling literature, we show that one salient piece of information, the product's past identity (i.e., the beginning of its biographical story) suffices to trigger the perception of the product as having a story. We observe this regardless of how we worded this information, from the somewhat story-like ""in my previous life I used to be..."" (Study 5) to the less story-like ""I was made from..."" (Study 4), to the purely informative ""upcycled from..."" (Supplementary Study 2, Web Appendix W3).In contrast to prevalent storytelling practices, which focus on stories about product use or storied metaphors of a brand's essence ([21]; [27]), we focused on the story entailed in a product's own biography. We thus extend this literature by highlighting how products can be protagonists in their own life stories. This raises the question of what such product life stories need to look like to persuade. Our evidence suggests that a product's metamorphosis, its transformation from a past identity, plays a crucial role. Metamorphosis or transformations are powerful narratives ([18]; [40]), and it seems that the source or consequence of the transformation is secondary to there being a transformation. Whatever past identity we used (from airbags to insulators) and whatever the transformation (from bags to tables), the results generalized. Just knowing that there has been a different past identity appears to suffice. We argue that it is this knowledge that invites the inference of all elements necessary for narrative thinking ([27]), a sequence of episodes (chronology) that logically build onto each other (causality).Contrary to most current literature, we never actually spelled out the product's story. Instead, we had customers infer the story by using a ""minimal narrative"" of making two different identities of the same product salient ([66]). Our minimal intervention of past identity salience highlights that the mere presence of a story can be persuasive. This adds a novel facet to storytelling in marketing and advances prior literature that suggests that stories work particularly well, if people experience narrative transportation ([26]; [79])—that is, become absorbed in narrated stories (see [21]] and [49]], who also find that stories can persuade without transportation).Why are these minimal stories persuasive? Because customers feel more special with a product that holds a biography (i.e., a story). We consistently found this, even when we controlled for novelty, surprise, authenticity, and originality. A good story increases appreciation for the story character ([ 3]; [61]) and conveys specialness ([52]). Because people want to feel special, the promise of specialness fuels demand ([38] ; [81]). This observation fits with prior insights on the specialness-affording value of object history ([20]; [46]; [82]), but it constitutes a novel insight in the domain of storytelling. Having a story to tell should thus be added among the factors known to make customers feel special (e.g., [ 8]; [38]).The power of the mere presence of a product's biographical story also allows our insights to be positioned with regard to different production modes. On the one side, our results help distinguish upcycling (our key phenomenon) from consumption of secondhand goods and vintage items, in which a product's prior life matters greatly and contagion is an issue ([ 2]; [37]; [45]). In contrast, upcycling encompasses the presence of a radical transformation away from a product's past identity. This transformation eliminates any taint associated with an upcycled product's past, although this past still makes it special. Notably, our findings generalize to recycling, in which products with past identities are broken down prior to being transformed into new products ([55]). A process of transformation thus appears to be a managerially relevant conceptual distinction within market practices that draw on used materials. This insight also aligns well with [86], who find that highlighting a product's transformative potential increases rates of recycling waste materials. Notably, our focus on metamorphosis or transformation also distinguishes upcycling and recycling from conventional production modes that create products from raw materials that are mostly devoid of a prior life and identity (Study 6). Our results suggest that insights from recycling may generalize to upcycling and vice versa. For both practices, marketers would do well to utilize transformational product biographies as their unique selling proposition. This sets a first benchmark for marketers in upcycling and provides a new lens for thinking about recycling. To date, most recycled products stress the environmental benefits rather than the specialness affordance inherent to past identities. More broadly, our results answer calls for marketing research on products created in environmentally friendly ways ([48]) and add to findings that suggest that environmentally friendly options are not necessarily preferred or avoided for their environmental impact ([14]; [50]). We show that these products may be preferred if they afford specialness. Future Research DirectionsHighlighting a novel facet of storytelling within an upcoming mode of production provides substantial scope for future research. On the one hand, there are several open questions with regard to storytelling. One question is whether there are story elements that might be particularly powerful. In Study 3, we found that past identity salience triggered different narrative thoughts. Future research could explore how and whether such differences affect felt specialness and demand. Another open question is whether results would differ if marketers were to narrate the product's biography in full. Would the effect be enhanced because everyone picks up on the story, or would it be reduced because there is less scope for self-inferencing? A related question asks how powerful a product's biographical story would be compared with other stories told in marketing. This is a relevant but tricky question because stories are more than the sum of their elements. We nonetheless explored this in a supplementary study (Supplementary Study 2, Web Appendix W3). An ad for a laptop sleeve either implied its story through its stated past identity (upcycled from an old mosquito net) or narrated a story of its inception (the product creators were inspired by watching a spider web). The respective stories affected demand to the extent to which participants perceived the product to hold a story. Notably, the marketer-narrated story about the design inspiration imbued the product with less of a story and led to a smaller increase in appeal than the self-inferred story triggered by past identity salience. Future research is needed to identify whether the embodiment of a story in the product outperforms narrated stories that are meant to ""rub off"" onto the product.Future research is also needed to ensure that the results are unaffected by the specific sampling frames we used. We tested our predictions across a range of samples from Austria, including Facebook users (Studies 1a–b), passers-by on campus (Study 2), convenience samples (Study 5), and pure student samples (Study 6). In addition, we conducted some studies with U.S.-based MTurk workers (Studies 3 and 4). Future research is needed to ensure that our conclusions extend beyond these populations. In particular, non-Western cultures may be more reluctant to adopt used goods ([89]).Running our own pop-up store, we learned firsthand how different reactions to upcycling can be. Some people appear to be entirely averse to the notion of old source products. Past identity salience may intensify these individuals' aversions, but these individuals might never fall into the target group. This does, however, raise the question of whether brands should highlight the fact that their branded products become repurposed, and whether they should use the same or a different brand for repurposed products.The effects we observed appeared rather robust regarding who makes the product (see follow-up analyses on perceptions as handmade for Study 4, Web Appendix W2) and what the product was made of. We surmise that the metamorphosis removes the potential taint of nearly any past identity, and we find in a supplementary study (Supplementary Study 3, Web Appendix W3) that even the salience of a truly disgusting past identity (dirty mosquito net) did not hurt demand. The appeal of the past identity may nonetheless influence the size of the effect. This speaks to the power of transformations in overcoming issues of contagion and opens up several lines of future inquiry. For example, does the extent to which the past identity becomes transfigured and distorted in the process of upcycling matter ([77])? Do functional source products (e.g., a glass window) result in different responses to hedonic source products (e.g., a decorative glass vase)? Moreover, who reacts most to past identity appeals, and are there people for whom such appeals backfire? Given that we find that demand is driven by felt specialness, it is plausible that customers who are low in power ([23]) or who feel a need for status ([24]; [75]) would react more strongly.The observed power of a prior life may also advance research on other effects attributed to a product's origin or production mode. Storytelling principles might further increase the effectiveness of appeals related to prior users (e.g., [ 6]), brands, and production sites (e.g., [59]) as well as production and design modes (e.g., [29]). Whether results generalize to such diverse aspects of a product's origin is, however, an open question. Prior research has only tended to find positive effects of appeals to a product's origin when the highlighted origin aspect is desirable. This rests on the notion that the essence of what goes into a product persists in the final product. For example, [59] found that products made in a firm's original manufacturing location hold more appeal because they are believed to contain more of the brand's authentic essence. In contrast, we found that past identity appeals work even when they highlight a discarded origin. In this respect, our findings are particularly novel. Practical ImplicationsThe salience of a product's past identity robustly increased demand across a wide variety of contexts (store and product level, online and offline), categories (e.g., wallets, vases, chandeliers), ad appeals, and product origins (e.g., mosquito nets, parachutes, airbags). In a real store (Study 2), revenues more than quadrupled when we made the past identities of products the focal point of our marketing materials. Online (Study 1a), social media likes more than doubled, and in all instances where we expected to find an increase in WTP or choice share, such an increase was found. The upcycled (vs. conventional) alternative was chosen at least 12% more often when the past identity was made salient.Importantly, our results generalized to recycling. When marketing products made from either of those production modes—recycling or upcycling—marketers should appeal to the product's past, even though this past has no bearing on the product's present identity or functionality. Regardless of what source product we used, past identity salience never reduced demand. However, when upcycled products told their story at a glance, simply restating the past identity did not further boost demand. This finding holds two important implications. First, in recycling, products are broken down to their granular structure. The past identities of recycled products are thus never visually discernible, suggesting that the more prevalent practice of recycling may benefit from past identity salience even more consistently than upcycling. This is an important contribution. In line with our market observations, most companies that use recycled materials simply stress their environmental friendliness but do little to highlight the specific source material going into their products.Second, the boundary condition of past identity discernibility (Study 5) may be overcome even for upcycled products. In two additional studies (Supplementary Studies 4a and 4b in Web Appendix W3), we devised ways in which marketers can do so. We found that easily executed visual tweaks that logically reinforce the product's biographical story boosted product demand even when the product's past identity was discernible at first glance. The key appears to lie in storytelling principles. These suggest reinforcing the chronological and causal structure inherent to the product's story ([51]; i.e., feature the past identity first and more prominently than the resulting product).Our implications also extend to the general adoption of the practice of upcycling. Every year, two billion tons of waste go to landfills around the world, posing a continuous threat to the environment, the economy, and society ([77]; [84]). Encouraging upcycling means that product waste will be reduced, resulting in less landfill and incineration, more energy savings, and decreases in industrial emissions ([11]). Upcycling has experienced impressive growth rates ([72]) and allows for a value-adding possibility in companies' own waste management, but it is still a niche phenomenon. Entering this market may be a worthwhile opportunity. Whenever we asked participants to choose between an upcycled product and conventional product, a substantial proportion of people preferred the upcycled option, and our pop-up store sparked interest. The addition of upcycled products to a retailer's portfolio may be an actionable way to attract customers who want to feel special but lack the financial resources for status symbols.Like Cinderella, the life of upcycled products holds the ingredients for the plot of a bestselling story. This article shows that we can learn from this story and that there may be more to storytelling than currently practiced in marketing. Stories truly unfold in customers' minds ([27]; [79]), but to date marketers appear to think that they have to do all the telling. Our results suggest that customers infer stories, even when they only see a single piece of information. Our results also suggest that stories may imbue potential weaknesses in a product's image (such as a useless past identity) with meaning that benefits rather than hurts demand. Customers appear to feel special when they obtain a product that allows them to infer its story. Perhaps it is time to think of marketing as the creation of a projection space for stories that customers tell and help marketers sell. "
2,"A Meta-Analysis of When and How Advertising Creativity Works Although creativity is often considered a key success factor in advertising, the marketing literature lacks a systematic empirical account of when and how advertising creativity works. The authors use a meta-analysis to synthesize the literature on advertising creativity and test different theoretical explanations for its effects. The analysis covers 93 data sets taken from 67 papers that provide 878 effect sizes. The results show robust positive effects but also highlight the importance of considering both originality and appropriateness when investing in advertising creativity. Moderation analyses show that the effects of advertising creativity are stronger for high- (vs. low-) involvement products, and that the effects on ad (but not brand) reactions are marginally stronger for unfamiliar brands. An empirical test of theoretical mechanisms shows that affect transfer, processing, and signaling jointly explain these effects, and that originality mainly leads to affect transfer, whereas appropriateness leads to signaling. The authors also call for further research connecting advertising creativity with sales and studying its effects in digital contexts.KEYWORDS_SPLITCreativity is important in marketing and is often considered to be at the heart of the advertising industry. The importance of creativity is highlighted, for example, in the popularity of industry competitions, such as the Cannes Lions International Festival of Creativity, and the growing academic literature on its effects (e.g., [68]; [85]). However, the value of creativity is also subject to longstanding debate ([ 5]; [52]), and recent reports highlight that marketers are increasingly growing skeptical of advertising creativity ([63]; [66]) and decreasing their investments in it ([27]).When and how is advertising creativity most valuable? Marketers wanting to answer these questions will find little guidance in the academic literature. Although the link between advertising expenditure and advertising effects has been consistently supported ([38]; [80]), to date, there is no comprehensive account of advertising creativity and its influence on consumer response. Even [83] failed to account for creativity as a factor in their insightful and influential review of how advertising works.Several factors seem to hold back scholarship in advertising creativity: ( 1) contrasting empirical results on its effects in terms of ad and brand outcomes (e.g., [51]; [77]; [82]), ( 2) disagreements over what creativity is and how it should be assessed (e.g., [60]; [77]), ( 3) limited understanding of moderators of its effect (e.g., [86]), and ( 4) ambiguity about the kind of theories that best explain how creativity works (e.g., [85]; [86]). Given the apparent confusion about what advertising creativity is and when it might benefit a brand, it is not surprising that marketers often make the wrong decisions when investing in advertising creativity ([68]).In this article, we synthesize the fragmented literature on consumer response to advertising creativity. Based on a literature review, we develop a conceptual framework linking advertising creativity to consumer outcome responses in terms of ad, brand, and sales. Through a meta-analysis, we then integrate 878 effect sizes in the first quantitative empirical overview of the literature. Thus, we capture the impact of advertising creativity on 19 different consumer responses taken from 93 data sets in 67 papers. We thereby contribute a comprehensive and empirically grounded account of how and when advertising creativity works, providing researchers with generalized findings that can serve as benchmarks and a common foundation for future studies of this important topic.First, we provide an empirically validated account of how advertising creativity works. The results show robust positive effects of advertising creativity on consumer responses but also inform researchers about the relative importance of various consumer responses to advertising creativity. Overall, effects are stronger for ad rather than brand responses (r =.491 vs..317) and for attitudinal rather than memory outcomes (all below.140). This suggests that the main advantages of advertising creativity are not grabbing attention and making ads memorable but rather the ability to foster positive ad and brand attitudes.Second, we highlight that advertising creativity is different from originality. Effects on consumer response are greater when creativity is measured as a bipartite construct comprising of originality and appropriateness. Effects of originality only on ad and brand attitudes are comparatively small (.362 and.164), suggesting that marketers who view creativity as synonymous with originality will not reap the full benefits of investments in advertising creativity.Third, we show that the different theoretical accounts used in the literature to explain how advertising creativity works are complementary. Although previously considered separately, affect transfer, processing, and signaling provide the best account when considered jointly. The results further show that processing is a key mediator of the effects, whereas originality fosters affect transfer and appropriateness signaling. When marketers invest in bipartite creativity, affect transfer and signaling occur in parallel to processing, which can explain the stronger effects of creativity compared with originality only.Fourth, we find that when the three theoretical accounts are considered jointly, the effects of advertising creativity on brand response are not dependent on ad responses. This is in line with the signaling account of advertising creativity and suggests that to fully understand how advertising creativity works, marketers should assess consumer responses in terms of brand rather than ad outcomes.For managers, the results provide guidance on how, when, and why to invest in advertising creativity. For example, advertising creativity leads to greater ad responses in high (vs. low) involvement communication contexts (.653 vs..340) and (marginally) for unfamiliar (vs. familiar) brands (.577 vs..435). The literature review also highlights the need for more studies on the relationship between advertising creativity and sales, as well as its effects in digital media contexts, two areas that are especially important given the ongoing debate about the value of advertising creativity in contemporary marketing practice. Conceptual FrameworkFigure 1 presents the conceptual framework guiding the meta-analysis. We developed this framework on the basis of a review of the literature on consumer responses to advertising creativity. The framework focuses on consumer responses that have been empirically studied and distinguishes between immediate and outcome responses. The categorization of immediate responses is based on the three main theoretical accounts of how advertising creativity works found in the literature: affect transfer, processing, and signaling (i.e., consumer response in terms of affect, processing, and perceived signals at the time of exposure to creative advertising). The categorization of outcome variables is based on ad and brand responses and characterized in terms of attitudinal or memory responses (i.e., longer-lasting responses related to the ad and brand, such as attitudes, memory effects, and sales). The framework also highlights two moderators of these responses: definitions and assessments of advertising creativity and properties of the communication context (involvement and familiarity).Graph: Figure 1. How and when advertising creativity works: conceptual framework.aNot tested empirically due to lack of data.Whereas research on advertising creativity generally has found positive effects on immediate and outcome responses (for reviews, see [74]] and [77]]), empirical findings suggest that the effects vary between different types of consumer responses. Findings generally show that advertising creativity has benefits in terms of immediate responses, such as attention ([65]; [78]), positive affect ([30]; [86]), and signals, such as perceived sender effort ([19]; [50]), but results are inconsistent regarding when and how this might also lead to outcome responses, such as attitudes and intentions ([61]; [77]) or memory effects ([65]; [82]). Results also varied regarding whether attitude and memory outcomes are affected ([ 4]). In line with the overall literature, we hypothesize that advertising creativity has positive effects on immediate and outcome responses: H1:  Advertising creativity has positive effects on (a) immediate responses and (b) outcome responses.However, from a managerial perspective, understanding whether investments in advertising creativity mainly affect ad rather than brand response, and whether advertising creativity is better at stimulating attitude or memory outcomes, is also important. Given the inconsistencies in the literature, we qualify this hypothesis by asking what type of consumer responses are most affected: RQ1:  Is consumer response stronger for ad versus brand outcomes? RQ2:  Is consumer response stronger for memory versus attitude outcomes? Defining and Measuring Advertising CreativityAs indicated in the introduction, a key challenge in the literature is the different views on advertising creativity. Creativity is a general construct that has been widely researched in fields such as psychology and organizational behavior, as well as in marketing ([37]; [74]). Creativity can be used to describe individuals (e.g., an art director at an advertising agency), processes (e.g., design thinking methods used to brainstorm advertising campaigns), or outputs (e.g., the actual ad executions used in a marketing campaign). In this article, we adopt the output perspective.Drawing on the creativity literature ([ 2]; [33]; [73]), we define advertising creativity as advertising execution(s) that is (are) original and appropriate. This bipartite definition of creativity has been widely adapted in the marketing literature, in which the definition has been applied in advertising ([14]; [41]), new product development ([ 8]; [37]), and consumer behavior ([ 9]; [62]). As argued by [ 2], a bipartite definition of creativity is required, because outputs that are original or unique but carry no use or meaning are perceived as weird or bizarre. However, any judgments of creativity are subjective and likely to vary across context and time. For example, judgments about originality and appropriateness in a work of art differ from the same judgments in an advertising context (even if the actual object being judged is the same). Similarly, judgments of the creativity of the same object can vary over time. In the art domain, for example, there are several artists who are now considered creative but whose art was controversial or even rejected in their time (e.g., Monet, Picasso, Dalí). Such works were initially seen as ""weird"" or ""bizarre,"" mainly because the type of appropriateness expected of them at the time of creation was a literal representation of reality. Thus, these artists were redefined as creative later, when judgment of appropriateness changed.In the advertising context, the best documented dimension of creativity is originality. This dimension has also been referred to as novelty, divergence, unexpectedness, and newness ([42]; [48]; [76]; [78]). Originality has positive effects on consumer responses to advertising, as originality makes advertising more likely to be attended and processed ([65]; [77]). Originality also has a positive effect on consumer response, as people have a predisposition to appreciate divergent stimuli and deem them intrinsically interesting ([86]). Advertising practitioners typically view originality as the most defining aspect of advertising creativity ([48]; [60]), especially when it comes to advertising creativity awards ([15]; [78]). Thus, it is not surprising that many scholars focus primarily or exclusively on originality when assessing advertising creativity ([49]; [65]).When it comes to appropriateness, this dimension complements originality by connecting the advertisement with brand strategy and consumer problem-solving abilities and goals ([ 3]; [25]; [60]; [78]). In the advertising literature, appropriateness is also referred to as relevance and usefulness (and when assessed by practitioners as ""on strategy""; cf. [41]; [74]). Appropriateness as such has received much research attention (often using the term ""relevance""; e.g., [31]). However, in contrast with originality, scholars rarely consider appropriateness to be an indicator of creativity in and of itself. Instead, researchers typically view appropriateness as a prerequisite for advertising to be interesting to its intended audience regardless of its level of creativity.Theoretically, it is clear that creativity is both originality and appropriateness. Some scholars also argue that additional dimensions could be needed to fully understand advertising creativity ([ 3]; [30]). They argue in favor of including a third advertising-specific dimension of creativity—namely, the quality of the ad execution, also referred to as artistry or production ([60]; [78]). In the literature, we distinguish four approaches to empirically assess advertising creativity. First, some studies measure advertising creativity as a holistic perception of the creativity of an ad, typically by using a single item ""creative"" or multiple creativity items that do not refer specifically to different dimensions (e.g., [71]). Second, other studies rely on only one dimension of advertising creativity (typically originality; e.g., [57]). Third, acknowledging the bipartite definition of creativity, some researchers use the interaction between originality and appropriateness as a creativity measure (e.g., [78]). Fourth, acknowledging the multidimensionality of the bipartite definition, some studies rely on measures of both originality and appropriateness ([42]), sometimes combined with one or more additional dimensions related to the production quality or artistic value ([60]; [68]).We argue that researchers who focus on originality only (e.g., [57]; [65]) are likely to get different results from those who study creativity in terms of a bipartite (e.g., [60]; [78]). Although we cannot test the validity of different assessments, we propose that the best measure of advertising creativity should explain more variance in outcome response variables than alternative measures of creativity, leading to stronger effect sizes (for a similar argument, see [23]). The approach that has the highest explanatory value should also be the most managerially relevant. Given that creativity is defined as a combination of originality and appropriateness, we propose that the effect sizes should be stronger when both dimensions are considered and weaker when only originality is used. Thus, H2:  The effect of advertising creativity on (a) ad response and (b) brand response is stronger when creativity is assessed as a bipartite versus as originality only. When Advertising Creativity Works: Contextual ModeratorsAlthough we expect advertising creativity to generally have positive effects on consumer response (H1), we also expect properties of the communication context to moderate these effects. In the literature review, it was apparent that little attention has been paid to such contextual moderators in the existing literature ([86]). However, we identified two theoretically relevant moderators: involvement and familiarity. Both variables have been found to affect consumer response to advertising in general, but of interest here is how they affect consumer responses to advertising creativity. Specifically, the literature suggests that advertising creativity (i.e., a combination of originality and appropriateness) has benefits regardless of the type of processing (peripheral vs. central; e.g., [10]) depending on these moderators. InvolvementConsumers' involvement with advertising reflects their level of interest in brand evaluation in any given context and has been found to moderate the effects of advertising processing and response (e.g., [56]; [59]). Specifically, consumer responses to advertising differ depending on how much effort goes into processing it. For example, high involvement with a product category motivates consumers to pay attention to and actively process advertising. When involvement is low, attention is typically allocated to other things, and consequently, ad processing is limited and utilizes few processing resources and peripheral cues (e.g., [10]; [56]).Although advertising creativity has typically been thought of as an attention-grabbing device (e.g., [65]), implying that it would work best in low-involvement contexts (where it can foster situational involvement; [10]), creativity has been found to have additional processing advantages when it comes to high-involvement contexts ([79]; [86]). In a low-involvement context, any additional processing stimulated by a creative ad is likely to be shallow ([56]; [86]). In a high-involvement context, however, additional processing makes consumers more likely to actively assess the claims. In this context, the combination of originality and appropriateness fosters more open-minded and less defensive processing of claims made (""willingness to delay closure""; [39]; [86]). This means that consumers will be more open to new information about the brand and less likely to use defensive mechanisms when processing advertising messages that are communicated creatively. Whereas advertising creativity in low-involvement contexts stimulates more affective processing, in high-involvement contexts creativity influences affective and cognitive processing ([86]). In both contexts, advertising creativity should have a positive impact on consumer response, but in a high-involvement context, in-depth processing, coupled with the willingness to delay closure, makes the effects stronger. Thus, H3:  The effect of advertising creativity on (a) ad response and (b) brand response is stronger for high-involvement versus low-involvement products. FamiliarityFamiliarity reflects the extent of consumers' direct and indirect experience with a stimulus, such as a product or a brand ([ 1]; [11]). Consumer response to advertising has been found to vary with familiarity ([55]; [75]). Specifically, the effects of advertising are generally stronger for unfamiliar than familiar brands. This effect is due to consumers not being able to draw from previous experiences (neither their own nor the experiences of others) when evaluating unfamiliar brands, which makes advertising more important for these brands. However, advertising for unfamiliar brands wears out faster ([11]). For marketers of unfamiliar brands, this poses a challenge, as they need advertising to build familiarity but also must be careful about how they advertise to avoid negative reactions caused by (too much) repetition.Familiarity has also been found to moderate the effects of advertising creativity in terms of familiarity with the specific ad ([14]; [65]). [14] found that advertising creativity has two main benefits when it comes to repetition: ( 1) it generates more positive effects upon initial exposure, and ( 2) it resists wear-out over multiple exposures. The latter finding is in line with results showing that advertising creativity (in terms of originality) helps draw attention to familiar ads that might otherwise be overlooked due to tedium ([65]). For unfamiliar brands, these advantages are more important ([11]). Overall, this suggests that advertising creativity should be beneficial for unfamiliar and familiar brands, but given its attention-grabbing character ([65]), the immediate wear-in effect that it can generate ([14]), and the higher impact of advertising in general ([55]; [75]), the effects should be stronger for unfamiliar brands. Thus, H4:  The effect of advertising creativity on (a) ad response and (b) brand response is stronger for unfamiliar versus familiar brands. How Advertising Creativity Works: MediatorsIn the literature, scholars have used three main theories to explain the effects of advertising creativity on consumer responses. These accounts focus on different immediate responses as key mediators of the effects of advertising creativity on outcome responses. The affect transfer model focuses on the potential of creativity to evoke positive feelings that spill over into consumer responses to the ad and brand (i.e., regarding ""positive affect"" as a key mediator; [86]). The processing model focuses primarily on the ability of creativity to get consumers interested in the ad and brand (i.e., ""ad processing"" is the key mediator; [78]). The signaling model proposes that creativity works as a marketing signal, directly influencing perceptions about the sender and thus, consumer responses to the brand (i.e., ""perceived sender effort"" is the key mediator; [19]).[ 5] Affect Transfer ModelA common explanation for the effects of advertising creativity is based on affect transfer ([20]; also referred to as affect infusion; [26]). This explanation focuses on the ability of affectively loaded information to transfer into other, more or less unrelated, targets. In the advertising creativity context, the affect transfer model builds on the fact that consumers generally like creative ads ([71]; [77]). Processing creative ads is seen as intrinsically motivating and pleasurable ([71]; [86]), which means that consumers are likely to experience positive affect while exposed to such advertising. This positive affect spills over to the ad and brand, leading them to be evaluated more favorably ([30]; [86]). According to this explanation, the positive effects are driven by creative ads being more enjoyable and liked, and the positive feelings that this stimulates. Although theoretically exceptions might occur, as may be the case for executions of fear appeal advertising that combines originality and appropriateness, the reviewed literature on advertising creativity unanimously provides empirical support for positive reactions to advertising creativity. Thus, H5:  The effect of advertising creativity on (a) ad response and (b) brand response is mediated by positive affect. Processing ModelExplanations of how advertising works often draw on information processing models (e.g., [56]; [59]). Specifically, they explain consumer responses to advertising based on different levels of ad processing. This is also a common explanation for the effects of advertising creativity. Creativity is said to stand out, thus making creative ads more likely to be attended to and processed ([78]; [86]). This means that more creative advertising stimulates more ad processing, resulting in longer exposure and greater attention ([30]; [65]), which has positive effects on consumer outcome response. According to this explanation, the positive effects of creativity are driven by creativity being more interesting and therefore, processed more. Thus, H6:  The effect of advertising creativity on (a) ad response and (b) brand response is mediated by ad processing. Signaling ModelA third explanation for the effects of advertising creativity focuses on creativity as a signal of brand and company ability (e.g., [19]; [50]). This model builds on research on marketing signals ([44]; [46]), showing that certain behaviors on behalf of a firm (e.g., offering long-lasting warranties) can be used to signal unobservable quality to consumers. For example, advertising spending (i.e., monetary investments) has been found to work as a signal of brand quality ([45]; [47]). Similarly, advertising creativity has been found to be perceived by consumers as a signal that the sender has invested effort (in terms of time and money) in their brand ([18]; [19]). Through creative advertising, a brand conveys that it is committed to its advertising and its products, which is interpreted as a signal that positively affects how the brand is perceived and evaluated. According to this explanation, the positive effects of creativity are driven by creativity signaling effort on behalf of the sender, thus affecting the ad and brand positively. In contrast to the other two models, this account considers process reactions to creative advertising to be about the immediate perceptions of the brand rather than the ad. Thus, H7:  The effect of advertising creativity on (a) ad response and (b) brand response is mediated by perceived sender effort. Full ModelAlthough the three theoretical accounts typically have been used in isolation (for an exception, see [86]]), combining the three models should provide a more comprehensive account of how advertising creativity works. Furthermore, the omission of any one of the intermediary effects might lead to the overestimation of the other ([83]). Therefore, H8:  A model of advertising creativity that considers (a) positive affect, (b) ad processing, and (c) perceived sender effort jointly better explains the effects of advertising creativity on ad and brand responses than any of the three models used separately.To test this hypothesis, we propose a full model, which incorporates all three theoretical mechanisms (see Figure 2). Given that the initial models focus on different variables, we propose several additional relationships in the full model. First, ad processing is likely to spur stronger perceptions of sender effort. This is because processing facilitates a more careful understanding of the ad ([77]), which should, in turn, enhance perceptions of sender effort stimulated by advertising creativity. Second, affective responses can influence not only the ad and brand but also perceptions of sender effort. The underlying logic is, again, affect transfer ([86]). Furthermore, affect and processing should be positively related, because feelings ease processing (mood theory), and easy processing is experienced as a good feeling (processing fluency theory).Graph: Figure 2. How advertising creativity works: Full model including estimation results (standardized path coefficients).†p <.10.*p <.05.**p <.01.***p <.001.Notes: Dotted lines indicate paths that were added to the combined model to form the full model. Method Data SetFor this meta-analysis, we selected papers that provide estimates of the effects of advertising creativity on various consumer responses. According to our bipartite definition of advertising creativity, advertising creativity comprises originality and appropriateness. To be able to assess the relevance of different assessments of creativity, we included all studies that identify as ""ad* creativity"" studies independent of their definition and operationalization of advertising creativity. This means that we also included all studies that relied on advertising stimuli judged to be creative (even if they did not use the bipartite definition), as well as studies that investigated the two main dimensions of creativity, even if they did not use the term ""creativity"" ([51]).To identify relevant papers, we first referred to review articles that provide an overview of previous research on advertising creativity (e.g., [74]). We applied an ancestry tree search by searching all papers that refer to the review papers that were available in the Web of Science database. Second, we performed a keyword search of electronic databases (e.g., ABI/INFORM, Emerald, Elsevier, EBSCO, and ProQuest Dissertation Publishing) using ""advertising creativity,"" ""ad creativity,"" ""advertisement creativity,"" and ""advertising creative,"" ""ad creative,"" and ""advertisement creative"" as key words, followed by a search with key words that relate to the two main dimensions of advertising creativity (""original*,"" ""novel*,"" ""newn*,"" ""unexpected*,"" ""divergen*,"" ""innovati*,"" ""incongru*,"" ""relevan*,"" ""appropriate*,"" ""useful*,"" and ""meaningful*"" combined with ""advertis*""). The database search was complemented by a search on Google Scholar. Third, we performed a manual search of the journal outlets that turned out to be major sources for articles on advertising creativity. Fourth, once we identified a paper, we examined the references in a search for additional studies. The search period covered all papers (published and unpublished) that were available by December 2018. The retrieval approach was consistent with recommendations in the literature ([35]) and closely followed the steps taken in recent meta-analyses published in marketing ([69]; [88]).After identifying manuscripts for potential inclusion in the data set, we applied inclusion and exclusion criteria to determine which manuscripts to retain. We included all empirical studies that measured or manipulated advertising creativity (as described previously) and provided estimates on its effects on consumer responses. We excluded any manuscripts outside this scope. For instance, we excluded studies that investigated nonconsumer response to creative ads (e.g., advertisers; [84]), or studies on creative media choice, but not creative ads ([16]). We also excluded studies that failed to provide sufficient data for the meta-analysis and for which necessary data could not be retrieved from the authors.To avoid duplications in the data set, a document with original analyses and findings by the same authors (e.g., journal article, working paper, conference paper) is called a ""paper."" In some papers, the authors analyzed more than one distinct data set (e.g., a paper with several experiments), while some data sets were analyzed in more than one paper (e.g., a study that was published as a conference paper and a journal paper). The analysis is based on data sets. Each data set can provide single or multiple effect sizes that refer to the effect of advertising creativity on any consumer response variable. The search resulted in 67 usable papers covering 93 data sets (see Web Appendix Table 1). The sample includes journal articles, book chapters, working papers, unpublished theses, and conference proceedings, thus reducing the risk of a biased representation of the state of research because of the source of publication. The variation of sources is similar, and the number of papers and data sets is higher than in other major meta-analyses in marketing ([12]; [87]). CodingWe categorized the consumer response variables measured in the studies based on the conceptual framework (see Figure 1). Specifically, we classified consumer responses in terms of immediate responses (affect, processing, and signals) and lasting outcomes related to the ad and brand (none of the data sets provided data for sales). The outcome responses were further divided based on attitude and memory responses. In addition, we identified a few consumer responses that did not fit in either category (e.g., actual creativity, brand familiarity, willingness to pay). Because these consumer response variables appeared either in only one or two data sets or in only one paper, we eliminated them from further analysis.[ 6] We did this to ensure a minimum degree of generalizability, because a meta-analysis should provide a high degree of generalization and thus, requires more information than a single manuscript or a single-study manuscript followed by a replication study. This left 878 effect sizes. For an overview of the consumer response variables and categorization scheme, see Table 1.GraphTable 1. Variables Used in the Meta-Analysis.  1 a The moderator variable is measured at the effect-size level, while all other moderators are measured at the data-set level.2 Notes: Intercoder reliability is provided for all high-inference coding with AR = agreement rate and α = Krippendorff's alpha.In terms of creativity moderators, we coded the variables at the effect size level, meaning that multiple effect sizes from one data set can be assigned different codes. Specifically, we coded whether creativity was assessed as originality only, as appropriateness only, as an interaction effect between originality and appropriateness, as a multidimensional concept (including originality, appropriateness, and potentially more dimensions), or as a holistic concept (measured with a single item ""creative"" or corresponding multiple items or manipulated as such; this is the base alternative in the model). As an illustration, [86] presented results based on originality and appropriateness separately, as well as for the interaction between the two allowing us to code three types of measurements for each of the variables studied. Although our main interest is in comparing a bipartite view of advertising creativity with a view of advertising creativity as originality only, this coding process allows a more complete understanding of how different assessments of creativity affect consumer response.In terms of communication context moderators, we dummy coded the variables on the data-set level. Specifically, we coded the data sets 1 if the advertised category was a high-involvement product and if the advertised brand was familiar. In addition, we added three control variables that captured substantial differences between studies and that could be related to the context variables (medium, year, and award). Two authors independently assigned variables in the primary studies to consumer responses and coded the moderators and control variables based on the information available in each study. The agreement rate was above 98% (Krippendorff's alpha =.932), and inconsistencies were resolved by discussion. Effect Size ComputationThe effect size metric selected for the meta-analysis was the correlation coefficient; higher absolute values of the coefficient indicate a stronger influence of advertising creativity on consumer responses. For papers that reported other measures (e.g., Student's t, mean differences), we converted those measures following guidelines for meta-analysis ([53]; [64]).[ 7] We adjusted all correlations for unreliability. When a paper did not report the reliability, or when the paper used a single-item measure, we used the mean reliability for that construct across all studies, following the procedure in previous meta-analyses in marketing (e.g., [43]).We dealt with integrating dependencies between effect sizes using the following approach. When a data set provided findings for different consumer response variables, we treated the findings as independent, because we integrated and analyzed the estimates for each consumer response variable separately. Some data sets reported multiple relevant tests for the same consumer response variable. We accounted for the dependencies of the effect sizes and the nested structure of the meta-analytic data by using a mixed-effects multilevel model ([67]). We estimated the following model: rij=γ00+μ0j+eij, Graph1where i = 1, 2, 3...I effect sizes, j = 1, 2, 3...J data sets. This formula estimates the average effect size γ00, the deviation of the average effect size in a data set from γ00 (μ0j), and the deviation of each effect size in the kth data set from γ00 (eij). The latter two terms have variances that follow a normal distribution and are uncorrelated.To address publication bias, we computed fail-safe Ns ([72]), which represents the number of additional effects with null results needed to render the results for an integrated effect size not statistically significant at p =.05. The fail-safe Ns were calculated for all statistically significant integrated effect sizes (p <.05) using the effect size estimates that were adjusted for measurement error. Furthermore, we provided a homogeneity test as an aid in deciding whether the observed effect sizes were more variable than would be expected from sampling error alone. If they are, there is a strong basis for including moderators. The homogeneity test involves the Q statistic, in which the distribution is similar to a chi-square with K − 1 degrees of freedom ([32]). Moderator AnalysisIf the homogeneity test indicated heterogeneity, we proceeded with a moderator analysis. We added the moderators specified by the hypotheses and the control variables simultaneously to Equation 1 and ran multilevel meta-regression models in hierarchical linear modeling separately for the major outcome variables. The model was a mixed-effects model, because fixed effects for the moderators were considered in addition to random components. We specified the following model: rij=γ00+ γ01×involvementj+γ02×familiarityj+γ04×mediumj+γ05×yearj+γ06×awardj+γ10×originalityij+γ20×appropriatenessij+γ30×interactionij+γ40×multidimensionalityij+γ50×partial correlationij+u0j+eij, Graph2where rij is the ith effect size describing the relationship between advertising creativity and the respective consequence variable reported within the jth data set.Assuring the robustness of the model required a sufficient sample size. The major restriction is often the higher-level sample size, and the literature recommends a sample of around 50 to avoid biased estimates of the second-level standard errors ([54]). Thus, we applied the model only to the outcome variables in the data set with a sufficiently large sample of data sets: Aad and Abrand (43 and 44 data sets, respectively). Structural Model EstimationTo investigate the different processes that explain how advertising creativity works, we developed a correlation matrix including integrated effect sizes of the consumer responses to advertising creativity and added integrated effect sizes for the interrelationships between the consumer response variables. We followed recommendations in the literature about collecting meta-analytic data for the correlation matrix, deciding about sample size, analytical decisions, and reporting ([ 6]). We searched the papers in the meta-analysis for correlations for the interrelationships between consumer response variables. For a construct to be included in such analysis, multiple study effects must relate it to every other construct in the model. Therefore, no additional variables shown in Table 1 could be considered. For example, because we did not find correlations between sender effort and recall or memory measures, the latter could not be included in the model. We found at least three correlations for each relationship, which equals or exceeds the requirements of other meta-analytic correlation matrices found in the literature ([28]). We integrated and adjusted the correlations in the same way as the correlations between advertising creativity and consumer response variables. That is, we first adjusted all correlations for unreliability. We accounted for the dependencies of effect sizes and the nested structure of meta-analytic data by using a mixed-effects multilevel model as described previously ([67]).We then used this correlation matrix (see Web Appendix Table 2) as input in a structural equation modeling (SEM) analysis using the maximum likelihood method. The matrix was based on 449 correlations, and the harmonic mean of the cumulative sample size for each cell equaled 1,293. Each construct was measured with a single indicator in the structural model. We fixed the error variances for these indicators to zero because we had already considered measurement errors when we integrated the effect sizes. We used the harmonic mean of the cumulative sample size underlying each integrated effect size (i.e., effect size cells comprising each entry in the correlation matrix) as the sample size for the analysis. ResultsTable 2 reports the integration of the reliability-corrected correlations between advertising creativity and all consumer response variables.GraphTable 2. Influence of Advertising Creativity on Consumer Responses (H1).  3 †p <.10.4 *p <.05.5 **p <.01.6 ***p <.001.7 a These variables are used to test H2–H7.8 Notes: Only relationships for which effects were available in more than one paper and/or more than two independent data sets are shown. The corrected average correlation coefficients (r) are the sample size-weighted, reliability-corrected estimates of the population correlation coefficients. The fail-safe N indicates the number of nonsignificant, unpublished (or missing) effects that would need to be added to a meta-analysis to reduce an overall statistically significant (p <.05) observed result to nonsignificance.Looking at immediate responses, we found statistically significant effects on affect in terms of positive affect and perceived humor. Interestingly, although positive affect has been studied more, the effects of humor were significantly stronger as indicated by nonoverlapping confidence intervals (95% CI for positive affect [.198,.388] vs. humor [.428,.832]). We also found significant positive effects on processing in terms of attention, interest in the ad, and ad processing, but only a marginal effect on complexity and positive thoughts. The effects on attention, interest in the ad, and ad processing were comparable in size (95% CI for attention [.218,.592], interest in ad [.215,.615], and ad processing [.015,.659]). Furthermore, advertising creativity had statistically significant positive effects on perceived signals: sender effort, brand value/quality, brand trust, and brand credibility. These effects were comparable in terms of size (95% CI for perceived sender effort [.282,.510], value/quality [.171,.407], brand trust [.171,.603], and brand credibility [166,.628]).Turning to outcome responses, advertising creativity had a statistically significant effect on all ad responses: Aad, ad recall, and ad recognition. The strongest and most widely studied effect was that on Aad. In terms of brand responses, the effects followed a similar pattern: Abrand was the most widely studied variable and statistically significantly affected. We also found a statistically significant positive effect on purchase/behavioral intention and brand memory, but not on brand recall or brand recognition. Overall, the pattern of results support H1 by highlighting that advertising creativity has positive effects on consumer reactions in terms of ad and brand. Answering RQ1, we found that the effect on Aad was statistically significantly larger than that on Abrand and purchase intentions (95% CI for Aad [.407,.575] vs. Abrand [.235,.399]) and purchase intention [.225,.387]), and that the effects on ad recall ([.214,.408]) were significantly larger than the effects on brand memory ([.072,.208]). This suggests that ad responses are more affected than brand responses. Related to RQ2, the pattern of results suggests that effects of advertising creativity are stronger for attitudes than for memory. Aad was statistically significantly different from ad recognition (95% CI for Aad [.407,.575] vs. ad recognition [.107,.307]), and marginally different from ad recall ([.214,.408]). Similarly, the effect on Abrand was significantly stronger than the effect on brand memory (95% CI for Abrand [.235,.399] vs. brand memory [.072,.208]).All homogeneity tests (except for brand memory) were statistically significant at p <.05 and showed that the variation in effect sizes cannot be explained by sampling error alone. The fail-safe N indicates that the statistically significant integrated correlations do not suffer from publication bias according to [72] rule of thumb (fail-safe N should be at least 5 times the number of effects plus 10).Table 3 presents the results for the multilevel moderator regression model for the relationship between advertising creativity and Aad and Abrand. To investigate whether the positive effects of advertising creativity depend on the type of assessment used, we examined the moderating effect of creativity assessments. The analysis showed that relying only on originality led to lower effect sizes for Aad and Abrand; thus, H2 was supported. We found a similar pattern for assessments relying on appropriateness only and for interaction effects, although the negative effect was only marginally significant for the latter when it came to Abrand. The findings also showed that multidimensional measures of advertising creativity led to stronger effect sizes for Abrand, but not for Aad. Overall, this pattern of results suggests that assessing advertising creativity only in terms of ( 1) originality, ( 2) appropriateness, or ( 3) an interaction effect between the two will lead to an underestimation of the effects. From a managerial perspective, the result also suggests that a multidimensional view of advertising creativity is the most relevant, as brand responses are more important than ad responses.GraphTable 3. Influence of Moderator Variables on Effect Sizes: Multivariate Meta-Regression Analysis Results (H2–H4).  9 †p <.10.10 *p <.05.11 **p <.01.12 ***p <.001.We then turned to the moderating effect of the communication context. The results showed stronger effects on Aad and Abrand for high-involvement products; thus, H3 was supported. Furthermore, the effects on Aad were marginally stronger for unfamiliar products, but there was no statistically significant difference in terms of Abrand. Thus, H4 was only partially supported. The control variables showed that using a partial correlation coefficient led to smaller effects on Aad. None of the remaining control variables affected Aad. However, the effects on Abrand were higher for audiovisual media (TV/movies) and marginally lower for award-winning ads. We did not find any statistically significant differences in terms of year of study.[ 8]To better understand why advertising creativity has positive effects on consumer responses, we performed a SEM analysis of the different models using the meta-analytic correlation matrix (cf. Web Appendix Table 2). Table 4 presents the results of the SEMs (standardized coefficients and model fit statistics). As we suggested alternative models implying that the relationship between advertising creativity and Aad is mediated by more than one mediating variable, we added a path between advertising creativity and Aad that captured alternative processes to each model. All three individual models showed a very good model fit, and all paths were statistically significant and in line with the suggested effects; thus, H5, H6, and H7 were supported.GraphTable 4. Coefficients and Fit Indices of the Meta-Analytic SEMs (H5–H8).  13 †p <.10.14 *p <.05.15 **p <.01.16 ***p <.001.The model that combines the three individual models showed a comparatively weak fit but was significantly improved by adding the proposed relationships between processing and perceived sender effort and positive affect and perceived sender effort suggested by the full model (Δχ2/d.f. = 96.527/5, p <.001; see Figure 2). To determine whether the full model provided a better explanation than the three parsimonious models that were nested within it, we compared the fit of the full model that was restricted to any of the nested models with the fit of the full model with unrestricted paths. The model fit worsened significantly when it was restricted to the affect transfer model (Δχ2/d.f. = 1,629.935/8, p <.001), the processing model (Δχ2/d.f. = 1,733.093/8, p <.001), or the signaling model (Δχ2/d.f. = 1,528.916/8, p <.001). Thus, the full model provides an explanation that goes beyond the explanatory power of each nested model; H8 was empirically supported. Interestingly, in the full model, the mediating effect of Aad on Abrand dropped from around.5 in the individual models to a marginally significant effect of.078 (Δχ2/d.f. = 96.512/1, p <.001). This suggests that the effect of advertising creativity on brand response is only weakly mediated by ad response, which adds additional insight into RQ1 about the effects of creativity on ad versus brand response.We performed two additional analyses to further explore how well the three models explain the effects of creativity on consumer response. First, we compared how much of the variance in Aad was explained directly by advertising creativity and indirectly by either process suggested by the three individual models (we could not apply this comparison to Abrand, as there was no direct effect of creativity on Abrand in the model). We computed the proportion of mediation as the ratio of indirect to total effect; that is, the indirect path(s) was/were divided by the sum of the direct path and indirect path(s) ([36]). The proportion of mediation via positive affect was 26.8%, via ad processing was 28.3%, and via perceived sender effort was 33.9%. When we tested the mediation paths in the full model against each other by restricting two corresponding paths at a time (see Web Appendix Table 3), we found no differences between the paths from advertising creativity to any of the three mediators (positive affect, ad processing, and sender effort). However, the effect of sender effort on Aad was significantly different and stronger than the effect of either positive affect or ad processing on Aad. The findings indicate that signaling explains more variance in Aad than the two other models, thus providing the strongest explanation for the effect of creativity on Aad of the three models.Second, we compared the theoretical explanation offered by the full model between the two dimensions of creativity by using correlation matrices that considered the variable relationships with either originality or appropriateness instead of creativity (see Web Appendix Table 4). The results showed that the positive effects on ad processing are equally strong for both dimensions. However, affect transfer mainly explains the effects of originality as indicated by the fact that the path from creativity to positive affect was statistically significant for originality, but not for appropriateness. When it comes to signaling, however, appropriateness seems more important, as indicated by the significantly stronger link between creativity and sender effort. Discussion Summary of FindingsIn this article, we offer a comprehensive synthesis of the effects of advertising creativity on consumer responses. The study highlights the importance of advertising creativity by showing robust positive effects on a wide range of immediate and outcome responses. The effects are stronger for ad responses compared with brand responses and for attitudinal compared with memory outcomes. Moderation analyses show that the effects of advertising creativity are weaker when creativity is assessed as originality only, compared with a bipartite comprising originality and appropriateness. This suggests that the effects of advertising creativity go beyond those of originality alone. The results further show that advertising creativity has stronger effects in high-involvement contexts, and that effects on ad response are (marginally) stronger for unfamiliar brands. Furthermore, we find empirical support for all three theoretical accounts (affect transfer, processing, and signaling) used in the literature, but also that a full model (where the three accounts are considered jointly) best explains the effects of advertising creativity on consumer outcome response. In the full model, the effect of the three advertising creativity mediators (positive affect, ad processing, and perceived sender effort) on brand response is only marginally mediated by ad response, suggesting that although ad responses are generally more affected than brand responses, they are not needed for advertising creativity affect brand response. Additional analyses show that affect transfer mainly explains the effects caused by originality and that signaling provides the strongest account of advertising creativity in terms of ad response. Theoretical ImplicationsAlthough marketing researchers and practitioners tend to agree that advertising creativity is important, there are contrasting views on what advertising creativity is, and how and when it can lead to positive outcomes. Through this meta-analysis, we provide a synthesis of the growing, but dispersed, literature on advertising creativity, thus building a common foundation for future studies of this important topic. The results inform about important outcome variables and moderators of advertising creativity effects. The meta-analytic findings can serve as benchmarks for future advertising creativity studies, as well as for studies dealing with other ad execution elements. Future findings can be compared against the meta-analytic results in terms of explained variance as a measure of advertising effectiveness. The results also have several implications for future studies of advertising creativity.First, we offer an empirically validated understanding of how advertising creativity works. The pattern of results suggests that advertising creativity has a role to play in stimulating positive consumer responses that goes beyond being a source of attention. If the attention-grabbing nature of advertising creativity were the key benefit, its effects should be greater for memory rather than attitudinal responses, and in communication contexts where consumers are less likely to attend to and process ads, such as for low-involvement products and for unfamiliar brands ([19]; [65]), which is not in line with the empirical results. Although claims that advertising creativity enables advertising to ""cut through clutter"" and make advertising more memorable ([65]) are true, they risk directing focus away from attitudinal consumer responses, which are more affected. The fact that advertising creativity has stronger effects in high-involvement contexts suggests that processing is important for the effects to occur. It also raises the question of what to expect from advertising creativity in contexts where consumers are unlikely to pay attention to and process ads, such as digital and mobile media. The meta-analysis did not include any such studies, but the results suggest that effects should be weaker in media such as smartphones where focus is very directed at other focal tasks ([58]). At the same time, effects should be stronger for advertising content in own channels and in media where consumers voluntarily seek out advertising ([70]). However, future research is needed to explicitly study the role of advertising creativity in these contexts.Second, we contribute insights into how to define and assess advertising creativity. In line with the creativity literature ([ 2]; [73]), the results indicate that creativity is not just about originality. A bipartite definition and multidimensional assessments of creativity offer better explanations of the effects (for a similar argument, see [ 3] and [60]). This suggests that researchers should be mindful when using the term advertising creativity and restrict it to studies of original and appropriate ads. When studying original advertising only, the term creativity should be avoided. It also suggests that the reliance on advertising awards as an operationalization of advertising creativity is not valid, as such awards tend to focus on originality ([15]; [41]). The fact that empirical studies have found positive effects of original and award-winning ads, however, is reassuring, as the results suggest that, if anything, those studies underestimate the effects.Third, we contribute to the theoretical understanding of how advertising creativity works. The findings show that the different theoretical accounts of advertising creativity available in the literature are complementary, but also that they have different relationships with creativity dimensions. Our meta-analytic path analysis show that originality primarily stimulates affect transfer, whereas appropriateness is more important for signaling. We also find that signaling has the highest explanatory value. Again, this reinforces the notion that a bipartite view of advertising creativity is most relevant, as ads that combine originality with appropriateness allow these mechanisms to work simultaneously, whereas original ads do not. It also suggests that future studies of advertising creativity should include more comprehensive theoretical frameworks than what has previously been the case. Together, these insights offer the basic building blocks for a more complete processing model of advertising creativity called for by [85].Fourth, the finding that the three theoretical mediators of advertising creativity have direct effects on brand response (Abrand) that are only weakly mediated through ad response (Aad) adds further to our understanding of how advertising creativity works.[ 9] It shows that although creativity has stronger effects on ad responses than brand responses, these effects are not necessarily dependent on ad response. Again, this pattern can be understood in terms of the combination of (high) originality and (high) appropriateness in creative ads. In line with [78] finding that originality has advantages in terms of attention and that appropriateness stimulates downstream effects and brand response, advertising creativity allows the two to work in parallel, which also has more direct brand outcomes. This finding is in line with the signaling account of advertising creativity that suggests a more direct effect on the brand. For researchers, it suggests that when studying the effects of advertising creativity, brand (and sales) responses must be included through direct measures rather than relying on Aad or other ad responses as proxies of such effects.Overall, the empirical results provide convincing evidence of the positive effects of advertising creativity on consumer responses and thus highlight the need for marketing scholars to consider not only media investments (ad spend; [38]; [80]) but also creativity investments in models of how advertising work. Managerial ContributionsFor marketers, we contribute a systematic account and empirical evidence of the value of advertising creativity. Specifically, we offer important insights into how, when, and why to invest in advertising creativity. Given the ongoing debate about the value of creativity in advertising ([27]; [66]), this contribution is timely and useful. It also shows no evidence of advertising creativity becoming less (or more) effective over time.When it comes to how to invest, [68] found that many marketers make suboptimal decisions regarding investments in advertising creativity. We suggest that a tendency to focus on originality might be the root of this problem. Creativity is more than originality, and by incorporating appropriateness consumer response will be more positive. To achieve this, marketers must find ways to assess advertising creativity. This is easier said than done, given that creativity judgments are subjective and vary across context and time. We find that award-winning ads lead to marginally weaker brand response, suggesting that consumer rather than professional judgments should be used. This supports [ 3] argument that marketers should involve consumers more in advertising development. Whereas there is a growing literature focusing on consumers as cocreators of advertising ([17]; [81]), consumers could also be engaged as prejudges of advertising. A post hoc analysis of the role of ad judges provided additional support for this notion. Specifically, we coded a variable that distinguished between ads that were judged to be creative by either consumers, by experts, or selected from award shows. As some studies did not provide details on ad judges, we first ran analysis of variance models for a combination of all three outcome responses (Aad, Abrand, and intentions) to ensure sufficiently large sample sizes. We found significant effects (F( 2, 351) = 4.931, p =.008) on outcome response. The effects were stronger when consumers judged advertising creativity (.373) compared with experts (.300) or award shows (.193). When we analyzed the three responses separately, the effect held for Abrand and intentions, but not for Aad. As brand outcomes are more valuable for marketers, this reinforces the potential in allowing consumers to (pre)judge advertising creativity.When it comes to when to invest, the results suggest that advertising creativity has positive effects in general but also that the effects are stronger for attitudinal rather than memory response and marginally stronger in audiovisual media (TV/movies vs. print/outdoor). Furthermore, the effects are stronger for high-involvement contexts. For marketers, this challenges the established view of advertising as a tool for gaining attention and suggests that creativity is especially valuable in contexts where consumers are likely to process advertising. Although we studied product involvement, this logic should also hold for media context involvement, meaning that creativity is more likely to work in situations where more focused ad processing occurs. Thus, advertising creativity should be more important for media contexts in which consumers voluntarily direct their attention to, or are forced to focus directly on, advertising than in in media contexts that rely on incidental and divided attention (see also [17]; [70]).We also find that advertising creativity has marginally stronger effects for unfamiliar compared with familiar brands. However, this effect is related only to ad rather than brand response. As suggested by [11], ad response is a strong indicator of brand response for unfamiliar brands (as consumers have little other information on which to base evaluations), suggesting that this finding is still managerially important. By investing in advertising creativity, such brands can increase the value of their advertising to consumers (""advertising equity""; [70]). Taken together, this suggests that advertising creativity is especially valuable when establishing a new brand in the market.When it comes to why advertising creativity works, the mechanisms underlying its positive effects are more profound than many marketers might think. An in-depth understanding of how affect transfer, processing, and signaling jointly contribute to brand response help make investments in advertising creativity less risky ([85]). Although marketers who focus on originality can expect positive effects due to affect transfer, they will miss out on the potential effects of signaling and appropriateness. By investing in bipartite advertising creativity, marketers can increase the chances of their ads being liked, processed, and interpreted as signals of what the brand has to offer. It also means that there is little risk that positive effects will be for ad response only.From a managerial perspective, the effects of signaling are especially important to consider, as they offer the strongest explanation for the effects on ad response and because appropriateness is especially important in high-involvement and low-familiarity contexts, where advertising creativity also has the strongest effects. It suggests that advertisements can produce effects by way of the signals they send rather than the specific messages they convey. Signals are especially important in situations where there is information asymmetry between marketers and their customers ([13]; [46]). This is arguably the case for unfamiliar brands and high-involvement products as well as in other situations where the decision-making process is complex, such as in business-to-business, business-to-government, and recruitment contexts ([13]; [18]). In fact, recent research suggests that the effect of advertising signals extends beyond consumers to other stakeholders, such as employees and investors ([18]), though this is beyond the scope of the present study. Limitations and Further ResearchGiven the nature of a meta-analysis, we could study only consumer responses that previous researchers had investigated. This means, for example, that we could not consider potential negative effects of creativity on, for example, confusion, negative affect, and fear appeals. However, we found a marginally significant negative effect of complexity, suggesting that the potential downsides of creativity warrant further investigation.Similarly, the literature review revealed a lack of studies on the effects of advertising creativity on sales (for an exception, see [68]) and the effects of advertising creativity in digital contexts, such as the effects of advertising creativity on social media influencer engagement ([34]). Future studies are needed to explore how advertising creativity works in those contexts. Studies linking the effects of advertising creativity to behavioral measures, such as brand choice or sales, seem especially important. This could be done by combining quantitative (advertising spend) and qualitative (advertising creativity) assessments of advertising investments with behavioral outcomes, for example, adding advertising creativity in marketing-mix models or adding sales as a dependent variable in experimental studies. In such efforts, additional moderators, such as clutter ([65]) and repetition ([14]), should also be considered.As another limitation, the present study focused on consumer responses to advertising creativity only. There are several related issues in the literature that could contribute to our understanding of advertising creativity. For example, there is a vast literature on creative processes in agencies that foster creativity in advertising ([29]; [40]), and synthesizing this literature should bring additional insights to marketers. Relatedly, there should be room to further integrate the literature on advertising creativity with creativity research focusing on other marketing contexts ([ 8]; [21]) to allow for a more complete understanding of how creativity works in marketing more broadly. It is our hope that this article can contribute to this development. "
3,"A Study of Bidding Behavior in Voluntary-Pay Philanthropic Auctions The authors investigate compliance behavior and revenue implications in winner-pay and voluntary-pay auctions in charity and noncharity settings. In the voluntary-pay format, the seller asks all bidders to pay their own high bid. The authors explore motives and boundary conditions for compliance behavior based on internal and external triggers of social norms. The voluntary-pay format generates higher revenue than the winner-pay format for charity auctions, despite imperfect compliance, but it generates lower revenues in noncharity settings. To characterize bidding strategy, the authors study time to bid, auction choice, and jump bidding and find evidence that bidders in voluntary-pay auctions more commonly use jump bidding and late entry. The findings have important implications for marketing managers, augmenting the growing stream of empirical auction studies and work on corporate social responsibility. Specifically, combining an auction with a charitable cause may result in increased revenues, but managers should ensure that they are accounting for differential compliance rates between auction formats. Even if low-compliance bidders can be identified and screened out, doing so is not advantageous, because noncompliant bidders bid up prices.Online Supplement: http://dx.doi.org/10.1509/jm.16.0476In a voluntary-pay auction format, all bidders—both winning and losing—are asked to pay their highest bid. This format contrasts with a winner-pay format, in which only the winners are asked to pay. Little is known about voluntary-pay auctions’ impact on auction revenue and how bidders bid in voluntary-pay auctions, their willingness to pay, and bidding strategies. This study addresses this lack by examining voluntary-pay and winner-pay auctions in charity and noncharity settings. We examine whether the voluntary-pay format increases collected revenues and, if so, whether this is merely a charity-related phenomenon or extends to noncharity settings.In winner-pay auctions, which include most commonly used nonphilanthropic auction formats, one bidder wins the item and pays for it. Although payment compliance is known to be imperfect even in legally unambiguous settings, payment enforcement is not generally an issue if the seller can ensure that payment is delivered prior to the buyer obtaining the item.Payment compliance is a far greater problem in voluntary-pay auctions, where all bidders are asked to pay whether or not they win. Although a substantial theoretical literature has advocated the use of all-pay auctions, this body of theory largely assumes perfect payment compliance. Therefore, a pressing question is whether there is high payment compliance in legally unenforceable settings in which the participating bidder commits to pay regardless of winning, specifically in a voluntary-pay auction context. We examine the factors that influence this rate of compliance.To study the conditions that foster payment compliance, we test different appeals for compliance. Specifically, we explore motives and boundary conditions for compliance behavior based on internal and external triggers of positive (pride) versus negative (shame) social norms.In addition, we conduct two field studies to study voluntary-pay auctions and payment compliance in a real-world setting to determine whether this format offers any benefits. We study these issues on an active large-scale auction website with a strong reputation for charity auctions (though thiswebsite also runs noncharity auctions). It operates primarily in one major North American city and has raised $4 million for local charities in the past decade. The auctions run on the site are ascending-bid (English-language) auctions, and sellers have flexibility in terms of format.In analyzing strategic considerations, we focus on three factors: The strategic interaction between jump bidding and late bidding in a field setting. This interaction is far more important in settings with contractual obligations for losing bidders, as waiting to bid might be a better option. The opportunity cost of entering a bid in an auction. This cost is far more important when entry involves a nonbinding contractual obligation—the opportunity cost being an alternative for obtaining the same item. Contract compliance when social consequences may differ between settings: a noncharity setting, in which the social contract is with only the contracting party, and a charity setting, in which the social contract is broader.To study these factors, we estimate a model with four components: time to bid, auction choice, jump bid, and propensity to pay. Background All-Pay Auctions in Philanthropic SettingsThe auctions we study are not strictly all-pay auctions owing to imperfect collection; therefore, we call these “voluntary-pay” auctions. They are motivated similarly to all-pay auctions: all bidders, including losing bidders, are asked to pay an amount equal to their highest bid. In all-pay auctions, payment compliance is assumed to be fully enforceable (which may be inconsistent with real constraints; Budig, Butler, and Murphy 1993; Posner 1977). Voluntary-pay auctions relax that full-enforceability assumption. In contrast to both these formats, in winner-pay auctions only the bidder with the highest bid is asked to pay. We summarize the differences between the winner-pay, all-pay, and voluntary-pay auction formats in Table 1.All-pay noncharity auctions belong in the class of auctions that satisfy revenue equivalence under standard conditions, even though, in general, revenue equivalence has not been established empirically (e.g., Gneezy and Smorodinsky 2006). However, this equivalence does not hold for ascending-bid all-pay auctions, which we use, because in dynamic all-pay auctions a bid reveals something about the value of the bidder. Another difference is that in the sealed-bid auction, bidders have no opportunity to place a second bid after they find that they are not the high bidder (and that they need to pay their bid anyway) and thus need to consider this possibility before placing a bid. In the ascending-bid auction, bidders can update their bids and adjust their strategy as the auction progresses. They are more likely to continue bidding due to the commitment already made, which may result in bidding frenzy (Heyman, Orhun, and Ariely 2004).Furthermore, in charity auctions, revenue equivalence between formats is not theoretically expected. When participation rates are exogenous and pledges are binding (neither is the case in the current study), all-pay charity auctions are expected to dominate other common formats in terms of revenues (Engers and McManus 2007; Goeree et al. 2005; Schram and Onderstal 2009).Revenue in the voluntary-pay auction consists of the ending price and the compliance rate. Because compliance is not enforceable, revenue depends on losing bidders’ propensity to pay. However, bidders with a low propensity to pay are likely to bid more aggressively. Furthermore, we expect greater bidder entry into the voluntary-pay than into the all-pay auction, which also has a positive effect on ending price. Higher revenue is likely to result as long as a considerable proportion of losing bidders pay. However, this possibility may not hold for non-charity auctions, where bidders tend to be less likely to enter and may bid less aggressively. Payment ComplianceIssues with payment compliance are commonplace in consumer auctions. For example, eBay has a help page for sellers who do not receive payments.1 In philanthropic settings, the compliance problem is exacerbated. Even with the most explicit legal contracts, perfect enforcement of social responsibility is nearly impossible from a legal standpoint (e.g., Budig, Butler, and Murphy 1993; Posner 1977).2 However, in recent years theoretical and experimental articles have advocated the use of the all-pay auction, particularly for charity. These articles, which we discuss next, offer assurance that all-pay auctions are sound under the assumption that enforcement is perfect. Background on Field StudiesAll-pay field studies and endogenous entry. A comparison of revenues for sealed-bid auction formats—including all-pay charity auctions, first-price charity auctions, and second-price charity auctions—revealed that all-pay auctions were revenue-dominated by winner-pay auctions, a result the authors attributed to low participation and low bids (Carpenter, Holmes, and Matthews 2008). This highlights the importance of accounting for endogenous entry, which we do. However, that finding emerged in a sealed-bid context, in which jump bidding and timing do not play a role, and with binding payment with perfect compliance—possible in the smaller controlled setting.Another study also used a sealed-bid format to compare voluntary contribution mechanisms, lottery, and all-pay auctions in a door-to-door fundraising field experiment (Onderstal, Schram, and Soetevent 2013). Results indicated that the all-pay condition performed worse than all other formats owing to lower participation—reinforcing our focus on endogenous participation.A field experiment on Taskcn, a large Chinese crowdsourcing site, modeled the submissions of solutions as a first-price sealed-bid all-pay auction, because all submitters had to expend some effort regardless of whether they won (Liu et al. 2014). Results showed that providing higher rewards led to more and higher-quality submissions but at the cost of entry deterrence. Field Experiment Methodology Involving SimultaneityStudies 1 and 2 involve clean experimental manipulations, with each setting in isolation. Thus, summary measures can be compared from settings, and differences in these measures can be attributed to the differences between the settings.In contrast, in auction field comparisons—the kind collected from real auction sites such as eBay and the platform used in the current research—a conventional approach is to compare revenues for different overlapping or simultaneous auctions for the same items side by side. This practice is common in empirical auction research (for examples, see Table 2). Although this approach would be unacceptable in laboratory methodology, it is not only acceptable but also desirable in field methodology. In field auctions, the notion of ceteris paribus cannot be routinely assumed between auctions run at different times. Therefore, it is important to run alternative formats during approximately the same time frame. Second, one cannot typically exclude real-world bidders from bidding in other formats or designs that overlap. Third, giving bidders a choice between auctions is advantageous because it indicates preference. Study 3’s approach clearly has shortcomings, and we urge readers to be cautious in comparing formats in this study, because comparisons are based on mixed experimental manipulations.TABLE: TABLE 1 Core Principles of Different Auction Types   HypothesesThe hypotheses are summarized in the conceptual framework in Figure 1. We first identify the conditions that foster payment compliance. Without payment compliance, voluntary-pay auctions will not generate any revenues from losing bidders.Social pressure is effective in generating compliance with costly social norms. Field experiments (held during elections) designed to explore whether positive (pride) or negative (shame) social pressure is effective in reinforcing voting (the social norm) revealed that negative pressure was a stronger motivator (Panagopoulos 2010). More closely related to our current work is research on tax compliance that showed social pressure to be critical in increasing payment compliance in tax scenarios (Bobek and Hatfield 2003). However, results revealed that moral obligation—the intrinsic pressure—is key in that respect, thus creating what we call internal social pressure, which primes intrinsic motives. We distinguish between internal and external social pressure, as well as between negative and positive framing. The manipulations examine appeals to internal moral code versus external social pressure for compliance (Trevino 1986). We contrast positive versus negative social appeals for compliance, wherein positive and negative appeals can be through either external consequences (Bearden and Rose 1990; Burke and Logsdon 1996) or internal consequences (Yi 1990). We state these relationships in a single hypothesis.H1a: Social pressure increases payment compliance by bidders in voluntary-pay auctions.Results of a charity field experiment with various formats conducted in preschools revealed that participation in the all-pay format is lower than in the other formats (Carpenter,Holmes, and Matthews 2008), also observed by Schram and Onderstal (2009). Thus, as we state in H2b, owing to costly entry in voluntary-pay auctions, bidder participation is expected to be lower. In addition, the presence of social pressure, which (per H1a) increases pressure to pay, is expected to further decrease entry, as potential bidders who anticipate the increased compliance pressure will not enter the auction. Accordingly, we conjecture that social pressure reduces participation.H1b: Social pressure decreases participation in voluntary-pay auctions.Theoretically, the expectation is that although individual bidders reduce their willingness to pay in all-pay auctions, total revenues are higher in all-pay auctions than in any other form of winner-pay auction when all potential bidders participate (Engers and McManus 2007; Goeree et al. 2005). However, with endogenous participation (Liu et al. 2014), entry in all-pay auctions is generally highly sensitive to incentives. Accordingly, when bidders enter because of social pressure (per H1a), a negative incentive, we expect that (applying similar rationale as for H1b) they are willing to pay less (Carpenter, Holmes, and Matthews 2008), resulting in the following hypothesis:H1c: Social pressure reduces bidders’ willingness to pay in voluntary-pay auctions.Choice of auction format. The remaining hypotheses pertain to the theory for the charity setting with choice between formats. The choice is between a voluntary-pay and a winner-pay format for the same item. We focus on bidders’ propensity to pay (if they lose) in the voluntary-pay auction. Conceivably, some degree of adverse selection may occur whereby bidders with lower propensity to pay (upon losing) find engaging in the voluntary-pay auction to be less costly in expectation. Adverse selection in auction choice has been previously documented (Dewan and Hsu 2004), and we extend this to format choice in auctions, resulting in the following hypothesis:TABLE: TABLE 2 The Literature on Field and Natural Auction Experiments  H2a: Bidders with lower propensity to pay are more likely to bid in voluntary-pay auctions than in winner-pay auctions.Endogenous entry. Bidder entry is a particularly problematic dimension in a voluntary-pay context, because it can result in complete reversal of revenue predictions (e.g., Carpenter, Holmes, and Matthews 2008). However, in contrast to prior work using sealed-bid auctions (Carpenter, Holmes, and Matthews 2008), we consider an ascending-bid format, in which entry tends to be less costly (because entry cost is generally lower than the amount pledged). As a result, more bidders should enter an ascending-bid auction than into a sealed-bid auction, but overall we expect that the number of bidders will be lower in the voluntary-pay auction than in the winner-pay auction owing to the financial commitment.H2b: Fewer bidders enter voluntary-pay auctions than enter winner-pay auctions.Jump bidding. Do bidders use jump bidding and time to bid strategically to reduce competition (e.g., Isaac, Salmon, and Zillante 2007)? Jump bidding can be effective in deterring competitors from entry by signaling aggressiveness when bidding is costly (e.g., Avery 1998; Easley and Tenorio 2004). If bidding is costly, this aspect makes the auction an all-pay variant. Thus, jump bidding is a critical aspect of the present investigation into voluntary-pay auctions, because it can have animportant deterrence potential owing to the costly commitment by all bidders. Jump bids do not have that deterrence potential in winner-pay auctions because losing bids do not entail a financial commitment. Thus, we expect that if jump bids are an effective deterrent, then in voluntary-pay auctions large jump bids (especially early in the bidding process) will deter other competitors from entering or continuing to bid, thereby increasing the likelihood of winning an auction.H3a: Jump bidding occurs more frequently in voluntary-pay auctions than in winner-pay auctions.Late bidding. Late bidding can be effective in avoiding early commitment and bidding wars (Roth and Ockenfels 2002). Given the entry (commitment) costs in voluntary-pay auctions, bidders will have a greater incentive to avoid early commitment and reduce competition.H3b: Late bidding occurs more frequently in voluntary-pay auctions than in winner-pay auctions.Effect of propensity to pay on bid pledges.. In a voluntary-pay auction, a bid pledge is equal to the highest bid by a bidder. When people make ethical choices in hypothetical scenarios with no commitment mechanism, they pledge to make ethical choices that they do not adhere to in incentivized scenarios (FeldmanHall et al. 2012). Likewise, bidders who have a lower propensity to pay (upon losing) are expected to bid (pledge) higher in voluntary-pay auctions, because if they do not win the auction they are likely to default on the losing-bidder payment.H4: Bidders with a higher propensity to pay tend to have a lower willingness to pay in voluntary-pay auctions than bidders with a lower propensity to pay.Bid pledges in auction format. Drawing on economic theory (Isaac, Pevnitskaya, and Salmon 2010), we predict that for both charity and noncharity settings, both willingness to pay and the winning bid in the voluntary-pay auction are lower than in the winner-pay format. The intuition is that in the voluntary-pay format, bidders have to allow for the probability of losing the auction and still paying their bids, so bidders will not bid up to their valuations.H5: Bid pledges are lower in voluntary-pay auctions than in winner-pay auctions.Auction revenue. Current theory posits that in charity settings, revenues will be higher with all-pay auctions than with winner-pay auctions, given exogenous entry and full compliance from nonwinning bidders (Engers and McManus 2007; Goeree et al. 2005). The basic intuition is that in a winner-pay charity auction, a bidder who outbids a competitor forgoes a positive externality associated with the competing bidder’s contribution. In contrast, in all-pay charity auctions, competing bidders pay irrespective of whether they win, so the issue of forgoing competitors’ contributions by outbidding them does not arise. This result has a positive effect on revenue in all-pay charity auctions, despite the expected adverse effect on bidder entry. We expect that this difference in revenue will be preserved or be even greater in the voluntary-pay auction, because payment by losing bidders is not enforced. Bidders with low propensity to pay may drive up prices, because losing bidders will be less concerned about paying, as the money goes to charity.H6: Total revenue in voluntary-pay auctions is higher than in winner-pay auctions. Study 1: Compliance Behavior for Different Triggers of Social NormsOf importance in the analysis is the rate of compliance. Study 1 tests different appeals for compliance in voluntary-pay charity auctions. Specifically, we explore motives and boundary conditions for compliance behavior based on internal and external triggers of social norms.We report on an experiment involving 125 Amazon Mechanical Turk participants in five treatments. The experiment is a 1 + 2 · 2 design with the following conditions: benchmark, negative internal, negative external, positive internal, and positive external. Appendix A shows the scenarios used. The benchmark condition included only the nonitalicized text. (None of the text was in italics when shown to participants—italics are for this exposition only.) The square brackets contain the manipulated text in the 2 · 2 conditions. The manipulations enable us to examine appeals to internal moral code versus external social pressure for compliance (Trevino 1986).For each manipulation, we elicited the highest bid for each of the two formats of winner-pay and voluntary-pay.We expected bidders to pay far higher amounts in the winner-pay auction, but an internal validity check is still necessary. One relevant construct for comparison of manipulations is the difference in the highest bids between winner-pay and voluntary-pay.The primary issues are compliance and format choice, and particularly the trade-off between the two. If we compel people to pay—either legally (not an option here) or socially (which is investigated here)—then they will pay, but that comes at the cost of reduced entry and more cautious bidding. We observe this particularly for the negative internal and positive internal conditions, which are characterized by very high compliance but low choice of the voluntary-pay auctions (Table 3). The positive internal condition has the added disadvantage of a large drop (25%) in willingness to pay in the voluntary-pay format relative to the benchmark. Low entry combined with low bids mean that the higher compliance cannot possibly compensate ina terms of revenue. An attempted manipulation through positive appeals to internal norms would not have been effective.The external comparisons provide the best of both worlds. They increase compliance even more than internal appeals but also raise preference for the voluntary-pay auction. In the case of positive external appeal, the appeal resulted in higher willingness to pay. In summary: Negative and positive external social pressure both result in a significantly higher likelihood of paying than internal social pressure. Internal social pressure—negative or positive—improves the likelihood of paying over the benchmark of no social pressure. External negative or positive social pressure changes format choice in favor of voluntary-pay, indicating that decision makers derive value from public acknowledgment. Internal pressure—negative or positive—relative to the benchmark setting improves the likelihood of paying but does not significantly change format preference, perhaps because it confers no additional benefit to subjects beyond the benchmark. The positive internal pressure condition has the highest willingness-to-pay difference between formats, with willingness to pay in the voluntary-pay format being the lowest of all formats by nearly $100. It is possible that bidders perceive that format as a donation scheme. Thus, although positive internal pressure increases compliance, it does so at a drop of approximately 40% in revenues. Negative internal pressure is a close second in terms of willingness-topay differences between formats.Overall, we find that social pressure (whether internal or external, negative or positive) increases payment compliance compared with the benchmark without social pressure (t = 1.74, p = .042),3 providing strong support for H1a. We also find support for H1b that social pressure decreases participation in the voluntary-pay format, though only for (both positive and negative) external pressure (t = 1.83, p = .036) and not for internal pressure (t = .06, p = .48). Finally, we find no statistical support for H1c’s proposition that social pressure has a greater negative influence on willingness to pay in voluntary-pay auctions than in winner-pay auctions, for either internal pressure (t = .87, p = .19) or external pressure (t = .10, p = .46). We test this hypothesis through the difference in willingness to pay between the winner-pay and voluntary-pay auctions. Although we find a larger difference for positive and negative internal pressure, consistent with H1c, this difference is not statistically significant.In summary, the conjecture that one can “guilt” participants into paying is correct, but the result may be a loss of revenues. External pressure is the exception, possibly because of conditionally higher conferred benefits such as higher social status. Field StudiesOur data for Studies 2 and 3 come from a local nonprofit auction website in a midsized metropolitan area in North America with a population greater than one million. At the time of the study, the site had roughly 9,000 registered members and is the same site as discussed in Haruvy, Popkowski Leszczyc, and Ma (2014). Bidders ( 1) are notified of auction events through emails to the site’s user base, ( 2) are registered users, and ( 3) have been exposed to both charity and noncharity ascending-bid auctions. Voluntary-pay auctions are introduced at the time of the study and represent only the second time bidders had been exposed to voluntary-pay auctions on this particular platform.TABLE: TABLE 3 Results of Appeals for Compliance Behavior based on Internal and External Triggers of Social Norms   Study 2: Nonsimultaneous Winner-Pay and Voluntary-Pay AuctionsWe conducted a field study comparing voluntary-pay and winner-pay auctions that were run at different times. The study controlled for self-selection on the bases of auction format (no choice of auction format) and products. We randomized the products sold. That is, prior to the auction, bidders did not know what products were in the auction.We tested three auction formats in this order: ( 1) winner-pay regular auctions, ( 2) winner-pay charity auctions, and ( 3) voluntary-pay charity auctions. All treatments were identical with respect to items and differed only in auction format. For each format, we sold an identical batch consisting of 75 different products, for a total of 225 auctions (three identical replicates for the 75 products). Web Appendix A provides a detailed summary of the products sold.Importantly, at any time, all 75 products were sold using a single auction format, and subjects were not exposed to the competing format within a week of the treatment. Recruitment did not mention the format and the products to be sold, and subjects found out about both only when they logged in to the auction site. They were given no indication that the same products would be sold in the future using a different format. All 75 auctions within a batch or format ran for four days. Auctions started at approximately 10 P.M. and ended at 10 P.M. on the last day with a 30-second interval between individual auctions.One day before the start of the auctions, all registered members received anemail announcing the upcoming auctions. In contrast to Study 3, they received no details about the auctions or the products sold. Details about the auction format (rules for the voluntary-pay auctions and payment-rule description) and the charity component (if present) were displayed on the front page of the auction website and in all individual auctions just above the bid box.Each auction consisted of a picture and a product description. All auctions were ascending-bid auctions with a proxy bidding system, similar to eBay. Bidding started at $.01, and all items were sold to the highest bidder. An important difference from eBay auctions is that bidders may submit either proxy bids or jump bids. With a proxy bid, the computer bids on behalf of the bidder incrementally up to his or her specified maximum willingness to pay. A jump bid specifies an amount to which the current high bid will jump. A bidder may also submit an incremental bid, in which the amount of the jump is equal to the minimum required bid increment. For estimation purposes, we include only proxy bids that are decisions by bidders, not computerized bids generated by the proxy bidding machine.4 The winning bidders paid for their items when they came to collect them at a local UPS store. The store does not sell any of the products sold in this study. In addition, all losing bidders in the voluntary-pay auctions were sent an email with a PayPal money request for the total of their nonwinning high bids for all auctions in which they participated (including a detailed calculation of the total). Importantly, the payments from losing bidders were collected separately from those of winning bidders. Thus, bidders could collect and pay for items they had won but could refuse to pay for the other amounts they had pledged as nonwinning bidders.In total, 108 unique bidders participated in Study 2. Eighty-nine bidders participated in the winner-pay format, and of these only ten bidders participated in both voluntary-pay and winner-pay auctions, indicating that very little overlap in auction format occurred between bidders. In contrast, in Study 3, 116 (96) bidders participated in the charity (noncharity) winner-pay format and 58 (43) bidders crossed formats. Thus, whereas in Study 2 nearly no crossing occurred, in Study 3 roughly half of the participants crossed between formats. This is appropriate because Study 2 was intended to compare behavior between formats, whereas Study 3 was designed to examine bidding strategy while accounting for the interaction between bidding and format choice. Study 3: Simultaneous AuctionsStudy 3 compared winner-pay and voluntary-pay in a simultaneous choice setting. Investigation into the simultaneous choice setting has a dual purpose. First, the literature on the all-pay format has been adamant that comparison between formats should permit endogenous entry—that is, bidders should be permitted to opt out of an auction format. Study of endogenous entry requires running auctions for both formats simultaneously. Second, by examining choices between auctions of different formats, we can infer preference over formats based on choice.Study 3, setting A: Simultaneous charity auctions. All events consisted of auctions for books, DVDs, games, household items, computer and electronic products, and other items (Web Appendix B summarizes the products sold). In charity auctions, a written announcement appeared at the bottom of the auction description, just above the bid box, stating that the donation would be made to the local United Way campaign. In addition, the charity component and the auction format were described in an email to all registered members.The event included 221 pairs of identical product auctions for a total of 442 auctions over a period of five days. A pair always had one voluntary-pay auction matched with one winner-pay auction. All auctions lasted one day, starting between 8:00 P.M. and 9:00 P.M. and ending the next day between 8:00 P.M. and 9:00 P.M. All other aspects of the auctions were identical, including the text (explanation and example) at the bottom of voluntary-pay auction descriptions.All registered members received an email about the upcoming auctions two days before the start of the auctions. The email provided details about the duration, the types of products sold, and the different auction formats, including the charity component and the charity involved. Similar to Study 2, the payment-rule description (winner-pay or voluntary-pay) was prominently displayed on the front page of the website listing the auctions and just above the bid box in all individual auctions. All other aspects of the study, including the platform, the type of auctions (ascending-bid auctions), and processing of the winning bidders and all nonwinning bidders in the voluntary-pay auctions, were identical to those described for Study 2.Study 3, setting B: Simultaneous noncharity auctions. We conducted a similar event to that of Setting A with 466 auctions (233 pairs of identical auctions), but without the charitable component. Again, auctions were run in simultaneous pairs, in which one auction is in a winner-pay format and the other a voluntary-pay format for the same product. The study was conducted one year after the charity event (the temporal distance minimizes order effects wherein experiences carry over to the next event), and over a six-day period (excluding the weekend). All other aspects were identical to those in setting A. The products sold were similar, although not identical, to those in the charity auctions (Web Appendix B summarizes the products sold).The purpose of having two simultaneous auction formats was to allow bidders to choose between them, and from these choices to infer preference. In the noncharity condition, 44.79% (SE = 5.10%) of the bidders moved between formats. In the charity condition, the corresponding number is 54.72% (SE = 4.86%). Thus, a reasonable assumption is that most bidders consider both formats. We find that restricting the analysis to only those subjects observed to move between formats does not change the results. We provide the regression results for all subjects and results with the more restricted population in Web Appendix C. Hypothesis TestsStudy 2. Summary statistics related to auction outcome and bidder participation for Study 2 are summarized in Table 4. We report average winning bid, revenue, average number of bidders per auction, total number of unique bidders in the format, total number of switchers, revenue from winners, revenue from losers, revenue committed, revenue collected from losers, and total revenues. We report these findings for all three treatments of Study 2 with two-sample t-tests reported for comparison between pairs of formats for winning bid, revenue, bidders, and bids per bidder. To compare the revenues and other value-related variables, we normalized these variables by dividing them by the retail value. All tests are based on the normalized variables.H2a posits that bidders with lower propensity to pay are more likely to enter or bid in voluntary-pay auctions than in winner-pay auctions. This expectation is tested in Study 1 and Study 3, but not in Study 2, because bidders do not choose the format.H2b posits that voluntary-pay auctions have fewer entrants than winner-pay auctions. Table 4 shows that the number of bidders is smaller in the voluntary-pay format than in the winner-pay auction in the charity setting (t = 7.57, p < .001). In the charity setting, the winner-pay auctions had 4.41 bidders versus 2.52 bidders in voluntary-pay auctions. This finding strongly supports H2b.H3a posits that more jump bidding will occur in voluntary-pay auctions than in winner-pay auctions. Recall our assertion that jump bids have an important deterrence potential in the voluntary-pay auction, owing to costly commitment, but do not have a similar potential in winner-pay auctions because those do not involve costly commitment. We find that of 614 final bids in the voluntary-pay auctions, 50.3% (SE = 3.7%) involved a jump bid. Of the 189 bids in the winner-pay auctions, 42.5% (SE = 2.0%) involved a jump bid. This difference is significant, based on a one-sided t-test (t = 1.88, p = .03), in support of H3a.We find that 65.2% (SE = 1.9%) of final bids in the winner-pay auctions are late bids (during the last hour of the auction). In comparison, 63.5% (SE = 3.5%) of final bids in the voluntary-pay auctions are late bids. Thus, while last-hour bids are clearly the majority of final bids (and this is likely a strategic choice), we find no support for H3b in the non-simultaneous auctions. That is, we do not observe a higher incidence of late bids in the voluntary-pay auctions (t = .42, p = .68).H4 asserts that bidders with a higher propensity to pay in voluntary-pay auctions have a lower willingness to pay. As with H2a, propensity is a latent construct. It appears in our discussion of model estimation. However, we have supporting model-free evidence for H4, because maximum willingness to pay in charity auctions for noncompliant bidders is $10.35 versus $5.82 for compliant bidders (t = 2.92, p = .004).H5 asserts that bid pledges are lower in voluntary-pay auctions than in winner-pay auctions. Results from Table 4 show that the winning bid is $13.51 in charity winner-pay auctions and $6.76 in charity voluntary-pay auctions (p = .006). Thus, H5 is strongly supported.H6 pertains to revenue comparisons. In Table 4, we see the auction outcomes for the two formats. The total combined pledged bids from the winning and losing bidders is higher in voluntary-pay ($1,352.40) than revenue in winner-pay ($1,013.06) auctions—a 33.5% improvement in revenue. The pairwise t-test is significant (t(74) = 2.34, p = .022). In the voluntary-pay charity auctions, $686.49 of $845.16, or 81.23% of the amount pledged by losing bidders, was collected, although this constituted merely 50.76% of the number of pledges by the losers. This constitutes 88.27% of the total pledged amount, including winners and losers, which is remarkable.TABLE: TABLE 4 Summary Statistics for Study 2  Study 3. In Study 3, the formats were running concurrently, and therefore, we cannot infer a manipulation effect. Nevertheless, we can look at t-test comparisons over bidders and auctions with the caveat that these are not manipulation checks but are rather comparisons for completion in light of the results of Study 2. We provide a fully specified econometric model subsequently. H2a posits that bidders with lower propensity to pay are more likely to enter voluntary-pay auctions than winner-pay auctions. This expectation involves a latent construct that requires a model (discussed subsequently). Nevertheless, we find that noncompliant bidders (nonpayers) placed more bids in voluntary-pay than in winner-pay charity auctions (2.70 vs. 2.37, respectively; t = 2.68, p = .01) and noncharity auctions (1.88 vs. 1.62, respectively; t = 2.83, p < .01), in support of H2a.In support of H2b, we find that the number of bidders is smaller in the voluntary-pay format than in the winner-pay format in both the charity (t = 7.03, p < .001) and the noncharity (t = 12.95, p In opposition to H3a, we find that the rate of jump bids in voluntary-pay auctions is lower than in winner-pay auctions, in both charity (t = 4.40, p < .001) and noncharity settings (t = 1.85, p = .065). Although we found support for H3a in Study 2, we do not find it in Study 3.To investigate the effectiveness of jump bidding as a deterrent to entry, we considered all large jump bids (10% of the item’s retail value). Results suggest that large, early jump bids increase the likelihood of winning in a voluntary-pay (vs. winner-pay) auction from 22.64% to 34.09% (t = 1.46, p = .07, one-tailed test) in charity auctions and from 13.64% to 38.10% in noncharity auctions (t = 2.52, p = .007, one-tailed test). This implies that early jump bids may be effective deterrents to entry in both charity and noncharity voluntary-pay auctions. H3b is concerned with late bidding. Because entry into voluntary-pay auctions is more costly, late bidding is predicted to be more prevalent in this auction format. We observe a significantly higher degree of late bidding in voluntary-pay auctions. Specifically, in the charity setting, looking at the last bid for each bidder, we observe 32.3% late bids. This result is in contrast to 15.8% late bids in the winner-pay auctions. This result is significant at p < .01, providing support for H3b.In the noncharity setting, we observe 20.01% late bidding in the voluntary-pay and 14.41% late bidding in the winner-pay (t = 4.39, p < .001) formats. This result provides strong support for H3b. Note that we did not find support for H3b in Study 2. However, in Study 3 bidders could choose which auction to bid in and, thus, select the winner-pay format. Bidders in Study 2 did not have this option, and more bidders entered early because bid levels were still relatively low.Consistent with H4, we find that bidders with a higher propensity to pay in voluntary-pay auctions have a lower willingness to pay. In particular, for noncompliant versus compliant bidders willingness to pay in charity auctions was $8.76 versus $5.20 (t = 2.09, p = .037) and in noncharity auctions was $14.22 versus $4.90 (t = 6.34, p < .01). In support of H5, winning bids are significantly lower in voluntary-pay auctions than in winner-pay auctions. The winning bid in the charity winner-pay auction is $20.19 compared with $15.44 in the charity voluntary-pay auction (p < .001), and the winning bid is $13.43 in noncharity winner-pay auctions compared with $7.50 in noncharity voluntary-pay auctions (p < .001).H6 predicts that revenue is higher in the voluntary-pay auction than in the winner-pay auction. The total combined pledged bids from the winning ($3,413.11) and losing ($4,427.95) bidders is $7,841.06 in voluntary-pay auctions, compared with $4,462.51 in winner-pay charity auctions; voluntary-pay auctions thus had 75.71% more bids pledged (t = 3.16, p = .002). In noncharity auctions, combined pledged bids from the winning ($1,698.58) and losing ($2,002.28) bidders is $3,704.46, versus $3,131.05 for winner-pay auctions (when normalized by retail price, revenue from the voluntary-pay format is slightly and insignificantly lower = .33, p = .745).5 Thus, we find higher revenues for the voluntary-pay format in the charity setting but not in the noncharity setting.However, not all pledges from losing bidders are collected. In the voluntary-pay charity auctions, 48.58% of the amount pledged by the losers was collected, whereas in the noncharity auctions, 44.00% was collected. Overall, the revenue collected for voluntary-pay auctions is 24.69% higher than the total revenue obtained in the winner-pay auctions for the charity setting ($5,564.24 vs. $4,462.51). In contrast, in the noncharity setting, voluntary-pay revenue is 17.61% lower than winner-pay revenue ($2,579.76 vs. $3,131.05). Thus, the collected revenues provide support for H6, because revenue in voluntary-pay charity auctions is higher than that in winner-pay charity auctions. We find the opposite in the noncharity setting because revenue is higher for winner-pay than for voluntary-pay auctions.Overall, results are highly consistent for Studies 2 and 3. The only difference is between jump bidding (H3a) and late bidding (H3b), because bidders have the option to bid in an alternative format in Study 3. Model EstimationSo far, we have compared aggregate statistics between formats. We next examine whether individual bidding strategies are consistent with theory—in particular, whether they employ strategies that suggest they view bids in the voluntary-pay format as implying commitment. Our focus is on two potential strategies expected to be effective in the voluntary-pay auction: early jump bidding and late incremental bidding. We expect these strategies to be more prevalent in the voluntary-pay auction if bidders perceive their bids as involving commitment—something bidders in the winner-pay auction do not have. We found model-free evidence, reported in Table 4, to suggest that these strategies are employed differently in the voluntary-pay format, but ultimately the strategies are process-related constructs that are driven by process-related variables. Without a joint model of strategic bidding that controls for dynamically changing explanatory variables as well as the relationship between the decision variables, we cannot correctly identify the role of commitment in the voluntary-pay format. A Four-Component ModelWe employ a four-component model similar to that of Park and Bradlow (2005). The components capture the four strategic considerations of the bidder in the voluntary-pay format: auction choice, time to bid, jump bid, and propensity to pay. For the simultaneous design in Study 3, we estimate the full four-component model, while for the sequential design in Study 2 the model reduces to three components as auction choice is not present.Endogeneity. When format choice is present (Study 3), jump bidding, time to bid, and propensity to pay link in the model to the propensity of auction choice. This type of endogeneity is a sample selection problem. In the Heckman approach (Heckman 1979), the inverse Mills ratio (IMR) —a transformation of the predicted selection propensity—serves as an explanatory variable in the outcome equation.The preference for the voluntary-pay format is estimated first, followed by the other three decisions. This sequence allows the propensity to choose the voluntary-pay format to be incorporated into the other decisions through the IMR. We allow for bidder-specific random effects correlated across decisions.Component 1: Auction choice. Each bidder chooses between two simultaneous auctions for identical items. Choice 1 in the pair is the voluntary-pay auction and choice 2 is the winner-pay auction. We use a latent utility model to capture auction preferences. This model implies that the choice of auction format is a choice between two utilities (Haruvy and Popkowski Leszczyc 2010). Given the binary choice, only the difference between the utilities is identified. The “difference” always refers to the voluntary-pay minus the winner-pay auctions. The utility difference between the voluntary-pay and winner-pay choices in auction pair I for bid k by bidder j is denoted DUijk and is defined as DUijk = b0, charityCharityi, k + b0, noncharityLagPricei, k denotes the previous price. DLagPricei, k in the choice regression captures the difference in lag price on auction choice. DLagCumNoBiddersi, k captures the difference in competition between the auctions based on the cumulative number of bidders at the time of the previous bid. DLagJumpBidi, k is the amount of the previous bid increment, differenced between the two simultaneous auctions.Lag$Committedi, k is the amount a bidder has committed in the voluntary-pay auction for auction pair i. LagTimeElapsedi, k is the time elapsed in the auction in seconds since the time of the previous bid. DLagFrenzy is the difference between the extent of frenzy for the two auction types, defined as the lag cumulative total bids divided by elapsed time (Dholakia, Basuroy, and Soltysinski 2002). Finally, Categories 1–3 are three dummy variables that account for heterogeneity in the utility across different product categories, partly addressing self-selection related to product choice.The implied probability of bidder j choosing the voluntary-pay auction over the winner-pay auction in bid k of auction pair I follows a logistic distribution as follows:We define the joint likelihood for bidder j’s format choices lchoicej as the product of all PrðChoiceijkÞ for bidder j over all auction pairs I in which bidder j participated, and bids k by bidder j.Component 2: Time to bid. Let tijk and tijk-1 denote the timing of bid k and bid k - 1 by bidder j in auction pair i. We assume that the time between bids, Timeijk = tij k - tijk-1 of bidder j, who places the kth bid in auction pair i, follows a Weibull distribution, with a probability density function:LagPricei, k denotes the previous price and therefore captures the effect of price dynamic on the timing of bids. LagCumNoBiddersi, k and LagFrenzyi, k are both important variables that influence the timing of bids. LagTimeElapsedVoluntary-pay is the interaction between LagTimeElapsed and an indicator variable for the voluntary-pay format. The parameter h1j represents an individual-specific error term for bidder j, such that E(h1j) = 0 and variance s2 h1 > 0. Finally, e1ijk represents the error term. We can then define the joint likelihood for bidder j’s timing choices lTime j as the product of all fðTimeijk; lij kÞ for bidder j over all auction pairs I inwhich bidder j participated, and all bids k by bidder j.Component 3: Jump bid. We model jump bids by the amount the bidder bids over the previous highest bid minus the minimum bid increment. Jump bidding can serve as a deterrent in all-pay auctions but not in winner-pay auctions (Dekel, Jackson, andWolinsky 2007).Let I here denote auction I rather than auction pair. Let –i denote the other auction in the pair. LetVoluntary-Payi, k be an indicator function (dummy variable) that takes the value of 1 if auction I follows the voluntary-pay format, and 0 otherwise.After controlling for two separate intercepts for charity and noncharity, the next three explanatory variables in the equation denote the lag prices in the current auction, separately for charity and noncharity, and in the competing auction. LagCumNoBiddersi, k is critical in that it measures how responsive jump bids are to the presence of competitors. If jump bids are a deterrent, as we claim, we expect to see a positive and significant coefficient for this variable, indicating that bidders facing a competitive environment use jump bids to eliminate some of the competition. The variable Lag$Committedi, k is exactly the same as the variable by the same name in the “Component 1: Auction Choice” subsection. The next four terms pertain to the time elapsed in the auction and its interaction with auction format and whether it is a charity auction. Frenzyi, k is an important explanatory variable because it could be responsible for some jump-bidding activity. When auctions are slow (corresponding to low frenzy), bidders may incur significant opportunity or monitoring costs, and the jump bid size becomes a strategic decision, with bidders choosing larger jump bids (Kwasnica and Katok 2007).We add an IMR regressor to account for self-selection separately for charity and noncharity auctions. The IMR is used as a regressor in the estimation of Equation 5 per Heckman (1979). Equation 8 indicates the joint estimation and points out the IMR.The error e3ijk is normal i.i.d. censored at 0, with standard deviation s3. The individual random intercept h3j has a mean of 0 and variance s2 h3 > 0. The correlation between the individual effect for jump amount and the time to bid is denoted by rh13. Let Iijk be an indicator function of whether a positive jump was observed. The likelihood of a jump of a specific magnitude, accounting for the censoring at 0, isWedefine the joint likelihood for bidder j’s jumpchoices ljump j as the product of all PrðJumpijkÞ for bidder j over all auction pairs I in which j participated and all bids k submitted by bidder j.Component 4: Propensity to pay. We investigate a latent bidder’s propensity to pay, whereby higher propensity to pay means higher probability of observing payment on a losing bid. The propensities are unobserved, but we do observe who paid a losing bid. The propensity to pay clearly depends on what happened in the auction—both process and outcome. Wemodel propensity to pay, whichwe capture as an individual intercept in the utility from the decision to pay plus the explanatory variables specified in Equation 7; the IMR coming from the choice regression in Stage 1 of the Heckman two-step procedure highlighted previously; and a random component.Because we have already usedUto denote choice utility, we use V to denote utility from paying a losing bid. The probability of paying a losing final bid (one final bid per bidder per auction) in auction I by bidder j is PrðPayijÞ = Fð-DVijÞ, whereCharityi, k is a dummy variable indicating whether an auction is a charity auction. The IMR is a regressor in Equation 7 perHeckman (1979); however, because it is just an intermediate step, we do not explicitly state it in Equation 7. Equation 8 indicates where IMRis used in the joint estimation. Wecan then define the likelihood of bidder j’s payment choices lpay j as the product of all PrðPayijÞ for bidder j over all auction pairs I in which bidder j participated.The parameter h4j represents an individual-specific error term for bidder j, such that Eðh4jÞ = 0 across bidders. The individual bidder’s h4j remains the same for all decisions made by individual j, and h4j’s variance is denoted by s2 h4 > 0. The term e4ij represents the error term for propensity to pay and follows a normal distribution. Note the absence of subscript k, denoting the bid, on all terms except the last price. This absence occurs because the decision to pay is at the auction level and not at the bid level.Joint estimation. We estimate jump bid and bid timing jointly through a maximum likelihood involving jointly distributed errors for the two decisions with a bivariate normal distribution. The choice decision is estimated by itself in Stage 1 of the Heckman two-step procedure as a standalone. Jump bid and time are linked to it through the IMR, which appears in Equation 8.Equation 8 shows the joint likelihood of the four components. We define H1j as the vector of bidder-specific intercepts for both the propensity-to-pay and the format-choice equations. WedefineH2j as the vector of bidder-specific intercepts for both the jump-bid and the time-to-bid equations. Hj follows a multivariate normal distribution with mean 0 and variances S. Let fðH, SÞ be themultivariate normal distribution of parameter vector H, conditional on the covariance matrix S. Using the bidder-specific likelihoods defined after each equation, we write the joint likelihood function for all bidders in Equation 8 as follows:Equation 8 is maximized to estimate the coefficients, which requires two steps. In the first step, we get the IMR from the choice equation, and in the second step, we recover the second half after the IMR is estimated. Results from the Four-Component ModelOur discussion focuses on the results of the four-component model from Study 3, which are provided in Table 5. We compare these results with the results from Study 2. Results for auction choice. The auction choice component captures the choice of the voluntary-pay auction as the dependent variable. The significant negative coefficient for LagPrice means that as prices evolve over time, bidders are more likely to choose the auction with the lower price. The significant negative coefficients for DLagPrice in both the charity and noncharity settings suggest that as the difference between the current prices or high bids becomes larger, bidders are more likely to choose the auction with the lower price.TABLE: TABLE 5 Results for Study 3: Simultaneous Winner-Pay Versus Voluntary-Pay Auctions in a Charity and Noncharity Setting  The positive significant coefficient for DLagCumNoBidders in noncharity auctions suggests herd behavior, consistent with competitive arousal. This is consistent with the positive signifi-cant coefficient for DLagFrenzy, suggesting that bidders are more likely to choose an auction in response to an increase in Lag-Frenzy. The positive significant coefficient for DLagJumpBid in both noncharity and charity auctions means that bidders are more likely to react to a large competing jump bid.We find a positive significant coefficient for Lag$Committed in both charity and noncharity auctions. This means that a bidder finds the voluntary-pay format more attractive if (s)he has already made a commitment in that format (which needsto be paid whether (s)he wins or loses). The positive significant coefficient for Lag-TimeElapsed in charity and noncharity auctions means that bidders are more likely to bid as more time has elapsed in the auction.Results for time-to-bid component. Time between consecutive bids is inversely related to the lambda parameter. Thus, a positive coefficient for any explanatory variable implies more concentrated bidding. We see a positive significant effect for LagPrice for noncharity auctions, suggesting that bids are more concentrated as price rises. This finding is consistent with our finding of strategic late bidding. We observe the opposite pattern—a negative coefficient on LagPrice—for charity auctions, wherein bidders with charitable motives have an incentive to bid earlier to push up the bid to help raise money for the organization (Haruvy and Popkowski Leszczyc 2009; Popkowski Leszczyc and Rothkopf 2010). The positive coefficient on cumulative bidders indicates that bids are more concentrated in auctions with more bidders. A positive coefficient for LagFrenzy suggests that an increase in LagFrenzy leads to more concentrated bidding. The positive coefficient for LagTimeElapsed is consistent with late bidding. The positive coefficient for LagTimeElapsedVoluntary-Pay indicates that bidders tend to wait to bid in voluntary-pay auctions. This result provides support for H5 for charity, where H5 predicts more late bidding in voluntary-pay auctions than in winner-pay auctions.Results for the jump bid component. The jump bid component of the model considers the magnitude of a jump bid. The positive coefficient for LagPrice_charity shows that bidders place a greater jump bid when the price is high, while the significant positive coefficient for LagCompetingPrice indicates that jump bids are influenced by the competing auction’s price. The positive coefficient may mean that a high price in the competing auction indicates that the price in the current auction is too low—thus the jump.The positive significant coefficients on LagCumNoBidders for both charity and noncharity auctions indicates that bidders in both types of auctions are more likely to place greater jump bids when the auction is more competitive. The coefficient for Lag$Committed in the voluntary-pay auction is significant and positive in both charity and noncharity auctions, indicating more aggressive bidding by bidders who have already committed more money in an auction. Although the coefficient is significant, we observe half the magnitude of aggressive bidding in charity auctions, in which bidders may be less concerned with the amount they have committed because the money goes to charity.The negative effect for LagTimeElapsed in both charity and noncharity auctions suggests that the more time that has elapsed, the less likely bidders are to place higher jump bids in these auctions. However, the positive interaction between time elapsed and voluntary-pay auction format in both charity and noncharity auctions suggests that bidders are more likely to place a higher jump bid later on in a voluntary-pay auction, implying that bidders use jump bidding to try to win the auction. Finally, we observe differences in jump bidding across product categories, with less jump bidding for category 1 (DVDs, games, and books).Results for propensity-to-pay component. For the decision to pay, we find that losing bidders are more likely to pay in charity auctions, in more competitive auctions (frenzy), and when the final ending price is low. We further observe that bidders are more likely to pay for products in category 1 but less likely for products in category 2. Finally, IMR has a positive effect, suggesting that bidders who have a higher likelihood to bid in voluntary-pay auctions are more likely to pay. This result suggests a strong relationship between bidders who self-select into the voluntary-pay format and bidders’ propensity to pay for a losing bid. Comparison of Results from Studies 2 and 3Because of the endogenous choice of auction format in Study 3, we did not expect identical results between Studies 2 and 3. However, as we expected, we find that the ranking across conditions is consistent for winning bids, pledged revenues, collected revenues, compliance rate, number of bidders, and number of bids per bidder.The rankings of collected revenues and compliance rates are comparable across the two studies. In Study 2, total revenue is $871.06 for noncharity winner-pay auctions, $1,013.06 for charity winner-pay auctions, and $1,193.73 for charity voluntary-pay auctions. All revenues are for the same set of 75 auctions run at different times, with zero overlap between formats. While the percentage difference between collected revenues in the winner-pay and voluntary-pay formats is somewhat smaller in Study 3 (17.83%) than in Study 2(24.69%), both differences indicate that voluntary-pay auctions produce greater revenues than winner-pay. Although the percentage of collected revenues is slightly higher in the non-simultaneous setting than in the simultaneous setting—51% versus 49%—we consider these patterns remarkably robust.Statistics for the paying and nonpaying losers in the voluntary-pay auctions show that paying losers bid less aggressively than nonpaying losers ($6.94 vs. $9.33; t = 2.92, p =.004), very similar to the results reported for Study 1 ($5.24 vs. $8.76; t = 2.09, p = .037). Finally, we look at the extent of jump bidding and the timing of bids. We find that the extent of jump bidding was similar in magnitude, although the rank order was not consistent across conditions for Study 2 versus Study 3 for winner-pay charity (46.62% vs. 47.51%), winner-pay non-charity (44.04% vs. 36.95%), and voluntary-pay charity(56.91% vs. 41.87%). Furthermore, we find a greater percentage of early jump bids in voluntary-pay charity in Study 2 (5.79%) than in Study 3 (16.66%), suggesting that when a winner-pay format is also present, bidders avoid making a commitment in the voluntary-pay auction early on and tend to bid later on, when the price difference between the auctions becomes larger.The extent of snipe bidding had the same rank order across conditions for Study 2 versus Study 3 for winner-pay charity (9.49% vs. 13.87%), winner-pay noncharity (13.99% vs. 15.09%), and voluntary-pay charity (22.73% vs. 41.24%). However, the magnitude of snipe bidding was significantly higher in Study 3 than in Study 2 for the voluntary-pay auction. This result is consistent with a lower number of early jump bids in Study 3.For Study 2 we estimate a three-component model including the decisions on time to bid, jump bid, and propensity to pay. The explanatory variables are the same as for Study 3. However, any explanatory variables pertaining to the competing auction (e.g., LagCompetingPrice) are not applicable, nor are any variables pertaining to the missing treatment of voluntary-pay noncharity (e.g., Lag$Committed_noncharity; LagTimeElapsedVoluntary-Pay_noncharity). Furthermore, the main treatment intercepts (charity, noncharity) cannot be identified separately from voluntary-pay, because noncharity is present only for winner-pay. Estimation results appear in Web Appendix D.Table 5’s comparison of the results from the sequential setting of Study 2 with those from the simultaneous setting of Study 3 reveals three major differences: In the time-to-bid component, all signs and significance levels are preserved except for the reaction to past prices, which we term “strategic late bidding.” Specifically, in Study 3, we find that lag price in the noncharity setting has a significant positive effect—meaning late concentrated bidding in response to high prices. In the charity setting in Study 3, we find the opposite pattern. In Study 2, in contrast, we found no significant effect of past prices on the timing of bids in either the charity or noncharity setting, possibly owing to the absence of choice between formats reducing the benefit of strategic late bidding. In Study 2, in the jump-bid component, only the time variables are significant. This finding is different from Study 3, in which bidders are less likely to place higher jump bids late in voluntary-pay auctions, probably because bidders have no alternative outlet in which to bid. In the propensity-to-pay component, charity is not included in the sequential model (it is confounded with the intercept since the voluntary-pay was run only in a charity setting). In contrast to Study 3, bidders are no more likely to pay when the ending price is low or when auctions are more competitive.In summary, the main insight from the three-component model is that the relationships indicated in Study 3 are largely preserved. The same was the case for the hypotheses discussed previously. The purpose of the four-component model was to extract revealed preferences from the observed format choices, which Study 2 cannot do. We controlled for self-selection owing to auction format, but self-selection on product choice in Study 3 may persist. That is, people may have self-selected themselves into Study 3 on the basis of product preferences. However, the three-component exercise served as a robustness check and ensured that there would be no reversed patterns or exploding magnitudes in one study versus the other. We find that the patterns are robust, suggesting that self-selection bias owing to auction format is not a concern. Discussion Main ContributionWe studied compliance behavior and revenue implications in winner-pay and voluntary-pay auctions in charity and noncharity settings. Study 1 was an experiment that tested different social appeals (using internal and external pressure) for compliance. We find that although we can effectively apply social pressure to compel bidders to pay, the unintended—yet intuitive—consequences of that action may be reduced entry and lower bids.In addition, we conduct two field studies to study voluntary-pay auctions and payment compliance in a real-world setting. In particular, we focused on revenue implications for voluntary-pay versus winner-pay auctions in charity and noncharity settings as well as bidding strategies such as jump bidding and strategic late bidding.Overall, results showed that in the charity setting, in which social responsibility was high, the voluntary-pay auction resulted in higher collected revenues for the seller even though commitments were not binding. In the noncharity setting, in which social responsibility was lower, we found higher revenues for winner-pay auctions. The difference in the relative success of voluntary-pay auctions between charity and non-charity settings can be attributed to three factors. First, bidders are more likely to participate in charity voluntary-pay auctions than in noncharity voluntary-pay auctions. Second, bidders are willing to bid higher in charity settings than noncharity settings. Third, higher revenues can be collected from losing bidders in charity settings. Broader Conceptual and Theoretical InsightsWhile payment noncompliance in auctions is occasionally addressed in the literature (e.g., Bruce, Haruvy, and Rao 2004), for the most part it is ignored as nonconsequential. In this work, we provide a compelling demonstration that noncompliance is highly consequential. Not only does compliance vary between settings, but its dependence on the setting changes the profitability of possible formats relative to one another. Non-compliant bidders self-select into formats in which they can take advantage of the trust of the format to extract higher surplus. We found important differences in bidding behavior between losers who reneged on their commitment versus those who did not. In particular, nonpaying losers tended to bid more aggressively, placing more and higher bids. Although these bidders reneged, discouraging them may be unwise, as they play an important role in increasing prices and revenues in voluntary-pay auctions. Alternatively, we find that different appeals may be effective to increase compliance. In particular, external social pressure in the form of public acknowledgment of payments results in a significantly higher likelihood of payment than internal social pressure and improves format choice to favor voluntary-pay auctions.Second, our research has broad and important implications for bidding strategies. We focused on two strategies in voluntary-pay auctions: early jump bidding and late incremental bidding. We expected these strategies to be more effective in voluntary-pay auctions because bidders face costly commitments—something bidders in winner-pay auctions do not have. Late bidding is helpful because it allows bidders to defer commitment and thus lower their risk. Accordingly, when bidders have a choice between formats, we observed more late bidding in voluntary-pay auctions (for both charity and noncharity settings).The interaction between time to bid and jump bidding is a key strategic consideration for bidders (Haruvy, Popkowski Leszczyc, and Ma 2014; Kwasnica and Katok 2007) and is of paramount importance. Bidders with a high cost of time will place higher jump bids (Kwasnica and Katok 2007). Thus, jump bidding coincides with longer bidding intervals. Our empirical demonstration is coupled with a methodological implementation that gives researchers and practitioners a simple way to implement this insight in an estimation framework that can demonstrably apply in a field setting. Main Managerial ImplicationsThis article has important implications for marketing managers. Our findings augment the growing stream of empirical auction studies and corporate social responsibility in marketing and contribute behavioral insights.Improving fundraising capabilities. Our findings add to the increased focus on fundraising in the corporate social responsibility literature. We find that combining an auction with a donation to charity may result in increased revenues—a finding with wide-reaching implications. While all-pay auctions have long been advocated as advantageous in raising funds for social causes, in practice the feasibility of collecting on such auction formats has hindered their wide adoption. We showed that perfect compliance is not essential, and a similar format—the voluntary-pay auction—is also advantageous in charitable settings.The importance of compliance incentives for format selection. The finding that voluntary-pay auctions substantially increase collected revenue in charity settings while resulting in lower collected revenue in noncharity settings suggests that in the absence of strong contractual enforcement mechanisms, voluntary-pay auctions tend to be better suited to settings with moderate to high social responsibility. When selecting an auction format, or any two possible pricing formats, managers should ensure that they are accounting for differential compliance rates between the auction formats they are considering. If the two formats are running concurrently, managers should consider whether buyers with different levels of compliance propensity might self-select among formats in a manner that distorts incentives.Compliance by losing bidders in voluntary-pay auctions. While managers may apply social pressure to comply people to pay, they need to take care that this will not result in reduced entry and more cautious bidding. To increase compliance, they may want to use positive external pressure (e.g., by publicly posting losing bidders who paid). Finally, while it may be possible to identified and screened out low-compliance bidders, doing so is not clearly advantageous, because noncompliant bidders bid up prices. Limitations and Directions for Future ResearchIn the long run, bidders might sort themselves into charity or noncharity, winner-pay, or voluntary-pay auctions according to preferences and expected payoffs. Clearly, our results would be weaker if noncompliant bidders entered en masse into the voluntary-pay charity auction. Although this is not the case with the population we study, it is an important boundary condition. Two factors should be considered: the proportion of compliant bidders that would enter the voluntary-pay auction (charity or otherwise) and the bidding aggression of noncompliant bidders. Any drop in the proportion of compliant bidders would have to be offset by an increase in bidding aggressiveness of non-compliant bidders to keep voluntary-pay auctions superior. To characterize this frontier, a controlled study would need to systematically vary the proportion of compliant and non-compliant bidders. APPENDIX AExperimental Conditions for Different Compliance AppealsYou are facing two possible auction formats:Voluntary-pay auction: This is a special format where all bidders—including losing bidders—receive a bill for the amount of their highest bid (e.g., if you place several bids and your highest bid is $150, then whether you win or lose you will receive a bill for $150). We cannot force payment on losing bids but all payments by losing bidders will be donated to charity.While we cannot force you to pay the amount of your bid if you lose, your compliance with the auction rules is important toward raising funds for the Children’s Hospital Foundation for the purpose of helping young children with cancer. [Negative External: Losing bidders who fail to pay their highest bids as requested will be listed on the foundation’s website as unmet pledges.] [Negative Internal: Losing bidders who fail to pay their highest bids as requested cause immeasurable harm to our fundraising efforts and to the children who rely on them.] [Positive External: Losing bidders who pay their highest bids as requested will be listed on the foundation’s website as donors.] [Positive Internal: Losing bidders who pay their highest bids as requested greatly benefit our fundraising efforts and the children who rely on them.]Regular winner-pay auction: Only the winning bidder is asked to pay—and receives the item he or she won. Losing bidders pay nothing. The proceeds are donated to the same charity as the voluntary-pay auction.1 http://pages.ebay.com/help/sell/unpaid-items.html.2 Posner (1977, p. 411) explains the legal principle that “promises are not enforceable unless supported by ‘consideration.’ This means, roughly speaking, that a promise will not create an enforceable contract unless it is made in exchange for something of value—goods, money, another promise, or whatever.” Posner lays out potential legal exceptions to this rule.3 The p-values are based on one-sided t-tests for all directional comparisons, including this one and the ones that follow.4 As an example, suppose the bidding increment is $.25, and Bidder A bids $7 by proxy (= zero jump), while the required minimum bid is $3. Thus, the actual displayed bid by this bidder is $3. Bidder B observes this bid of $3, chooses proxy and bids $4. The proxy tool will immediately outbid him at $4.25 on behalf of Bidder A. If Bidder B now chooses jump and bids again at $5 (the indicated minimum bid is $4.50 and so the jump = $.50), the proxy by Bidder A will still outbid him at $5.25. The resulting outcome is identical to a situation in which Bidder B had used a proxy at $5, but the observation here is counted as a jump of $.50, and a use of the proxy would have been a jump of $0. Bidder B finally selects a jump bid at $9. The bid now shows as $9—not as $7.25, because Bidder B did not choose a proxy bid.5Voluntary-pay charity auction revenue from winners is 76.49% ($3,413.11/$4,462.51) of that from the winner-pay auction. For the noncharity auctions, revenue from winners is only 54.25% ($1,698.58/$3,131.05).DIAGRAM: FIGURE 1 Conceptual Model with Hypotheses for Bidding in Voluntary-Pay Versus Winner-Pay Auction FormatPHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)  REFERENCES   1  Ariely, Dan, and Itamar Simonson (2003), “Buying, Bidding, Playing, or Competing? Value Assessment and Decision Dynamics in Online Auctions,” Journal of Consumer Psychology, 13 (1/2), 113–23. 2  Avery, Christopher (1998), “Strategic Jump Bidding in English Auctions,” Review of Economic Studies, 65 (2), 185–210. 3  Ayres, Ian, Mahzarin Banaji, and Christine Jolls (2015), “Race Effects on eBay,” RAND Journal of Economics, 46 (4), 891–917. Bajari, Patrick, and Ali Hortacsu (2003), “The winner’s Curse, Reserve Prices, and Endogenous Entry: Empirical Insights from eBay Auctions,” RAND Journal of Economics, 34 (2), 329–55. 4  Bearden, William O., and Randall L.Rose (1990), “Attention to Social Comparison Information: An Individual Difference Factor Affecting Consumer Conformity,” Journal of Consumer Research, 16 (4), 461–71. 5  Bobek, Donna D., and Richard C.Hatfield (2003), “An Investigation of the Theory of Planned Behavior and the Role of Moral Obligation in Tax Compliance,” Behavioral Research in Accounting, 15 (1), 13–38. 6  Bradlow, Eric, and Young-Hoon Park (2007), “Bayesian Estimation of Bid Sequences in Internet Auctions Using a Generalized Record-Breaking Model,” Marketing Science, 26 (2), 218–29. 7  Bruce, Norris, Ernan Haruvy, and Ram Rao (2004), “Seller Rating, Price, and Default in Online Auctions,” Journal of Interactive Marketing, 18 (4), 37–50. 8  Budig, Mary Frances, Gordon T. Butler, and Lynne M. Murphy (1993), “Pledges to Nonprofit Organizations: Are They Enforceable and Must They Be Enforced,” University of San Francisco Law Review, 27, 47. 9  Burke, Lee, and Jeanne M.Logsdon (1996), “How Corporate Social Responsibility Pays Off,” Long Range Planning, 29 (4), 495–502. Carpenter, Jeffrey, Jessica Holmes, and Peter Hans Matthews (2008), “Charity Auctions: A Field Experiment,” Economic Journal (London), 118 (525), 92–113. Chan, Tat Y., Vrinda Kadiyali, and Young-Hoon Park (2007), “Willingness to Pay and Competition in Online Auctions,” Journal of Marketing Research, 44 (2), 324–33. Cox, Richard Guy (2005), “Optimal Reservations Prices and Superior Information in Actions with Common-Value Elements: Evidence from Field Data,” Ekonomia, 8 (2), 142–67. Dekel, Eddie, Matthew O. Jackson, and Asher Wolinsky (2007), “Jump Bidding and Budget Constraints in All-Pay Auctions and Wars of Attrition,” Discussion Papers No. 1454, Center for Mathematical Studies in Economics and Management Science, Northwestern University. Dellarocas, Chrysanthos, and Charles A. Wood (2008), “The Sound of Silence in Online Feedback: Estimating Trading Risks in the Presence of Reporting Bias,” Management Science, 54 (3), 460–76. Dewan, Sanjeev, and Vernon Hsu (2004), “Adverse Selection in Electronic Markets: Evidence from Online Stamp Auctions,” Journal of Industrial Economics, 52 (4), 497–516. Dholakia, Utpal M., Suman Basuroy, and Kerry Soltysinski (2002), “Auction or Agent (or Both)? A Study of Moderators of the Herding Bias in Digital Auctions,” International Journal of Research in Marketing, 19 (2), 115–30. Dholakia, Utpal M., and Itamar Simonson (2005), “The Effect of Explicit Reference Points on Consumer Choice and Online Bidding Behavior,” Marketing Science, 24 (2), 206–17. Easley, Robert F., and Rafael Tenorio (2004), “Jump Bidding Strategies in Internet Auctions,” Management Science, 50 (10), 1407-19. Engers, Maxim, and Brian McManus (2007), “Charity Auctions,” International Economic Review, 48 (3), 953-94. FeldmanHall, Oriel, Dean Mobbs, Davy Evans, Lucy Hiscox, Lauren Navrady, and Tim Dalgleish (2012), “What We Say and What We Do: The Relationship Between Real and Hypothetical Moral Choices,” Cognition, 123 (3), 434-41. Gneezy, Uri, and Rann Smorodinsky (2006), “All-Pay Auctions- An Experimental Study,” Journal of Economic Behavior & Organization, 61 (2), 255-75. Goeree, Jacob K., Emiel Maasland, Sander Onderstal, and John L. Turner (2005), “How (Not) to Raise Money,” Journal of Political Economy, 113 (4), 897-926. Haruvy, Ernan, and Peter T.L. Popkowski Leszczyc (2009), “Bidder Motives in Cause Related Auctions,” International Journal of Research in Marketing, 26 (4), 324-31. Haruvy, Ernan, and Peter T.L. Popkowski Leszczyc (2010), “Search and Choice in Online Auctions,” Marketing Science, 29 (6), 1152-64. Haruvy, Ernan, Peter T.L. Popkowski Leszczyc, and Yu Ma (2014), “Does Higher Transparency Lead to More Search in Online Auctions,” Production and Operations Management, 23 (2), 197-209. Heckman, James J. (1979), “Sample Selection Bias as a Specifi-cation Error,” Econometrica, 47 (1), 153-61. Heyman, James E., Yesim Orhun, and Dan Ariely (2004), “Auction Fever: The Effect of Opponents and Quasi-Endowment on Product Valuations,” Journal of Interactive Marketing, 18 (4), 7-21. Hossain, Tanjim, and John Morgan (2006), “Plus Shipping and Handling: Revenue (Non) Equivalence in Field Experiments on eBay,” Advances in Economic Analysis & Policy, 5 (2), 153-61. Isaac, R. Mark, Svetlana Pevnitskaya, and Timothy C. Salmon (2010), “Do Preferences for Charitable Giving Help Auctioneers?” Experimental Economics, 13 (1), 14-44. Isaac, R. Mark, Timothy C. Salmon, and Arthur Zillante (2007), “A Theory of Jump Bidding in Ascending Auctions,” Journal of Economic Behavior & Organization, 62 (1), 144-64. Katkar, Rama, and David H. Reiley (2007), “Public Versus Secret Reserve Prices in eBay Auctions: Results from a Pokémon Field Experiment,” B.E. Journal of Economic Analysis & Policy, 5 (2), 1-25. Kwasnica, Anthony M., and Elena Katok (2007), “The Effect of Timing on Jump Bidding in Ascending Auctions,” Production and Operations Management, 16 (4), 483-94. List, John A., and David Lucking-Reiley (2000), “Demand Reduction in Multiunit Auctions: Evidence from a Sportscard Field Experiment,” American Economic Review, 90 (4), 961-72. Liu, Tracy Xiao, Jiang Yang, Lada A. Adamic, and Yan Chen (2014), “Crowdsourcing with All-Pay Auctions: A Field Experiment on Taskcn,” Management Science, 60 (8), 2020-37. Lucking-Reiley, David (1999), “Using Field Experiments to Test Equivalence Between Auction Formats: Magic on the Internet,” American Economic Review, 89 (5), 1063-80. Onderstal, Sander, Arthur J.H.C. Schram, and Adriaan R. Soetevent (2013), “Bidding to Give in the Field,” Journal of Public Economics, 105, 72-85. Panagopoulos, Costas (2010), “Affect, Social Pressure and Prosocial Motivation: Field Experimental Evidence of the Mobilizing Effects of Pride, Shame and Publicizing Voting Behavior,” Political Behavior, 32 (3), 369-86. Park, Young-Hoon, and Eric T. Bradlow (2005), “An Integrated Model Who Bids and Whether, When, and How Much to Bid in Internet Auctions,” Journal of Marketing Research, 42 (4), 470-82. Popkowski Leszczyc, Peter T.L., and Gerald Häubl (2010), “To Bundle or Not to Bundle: Determinants of the Profitability of Multi-Item Auctions,” Journal of Marketing, 74 (4), 110-24. Popkowski Leszczyc, Peter T.L., and Michael Rothkopf (2010), “Charitable Motives and Bidding in Charity Auctions,” Management Science, 56 (3), 399-413. Posner, Richard A. (1977), “Gratuitous Promises in Economics and Law,” Journal of Legal Studies, 6 (2), 411-26. Reiley, David H. (2006), “Field Experiments on the Effects of Reserve Prices in Auctions: More Magic on the Internet,” Rand Journal of Economics, 37 (1), 195-211. Resnick, Paul, Richard Zeckhauser, John Swanson, and Kate Lockwood (2006), “The Value of Reputation on eBay: A Controlled Experiment,” Experimental Economics, 9 (2), 79-101. Roth, Alvin E. and Axel Ockenfels (2002), “Last Minute Bidding and the Rules for Ending Second-Price Auctions: Evidence from eBay and Amazon Auctions on the Internet,” American Economic Review, 92 (4), 1093-103. Schram, Arthur J.H.C., and Sander Onderstal (2009), “Bidding to Give: An Experimental Comparison of Auctions for Charity,” International Economic Review, 50 (2), 431-57. Suter, Tracy A., and David M. Hardesty (2005), “Maximizing Earnings and Price Fairness Perceptions in Online Consumer-to-Consumer Auctions,” Journal of Retailing, 81 (4), 307-17. Trevino, Linda Klebe (1986), “Ethical Decision Making in Organizations: A Person-Situation Interactionist Model,” Academy of Management Review, 11 (3), 601-17. Yi, Youjae (1990), “Cognitive and Affective Priming Effects of the Context for Print Advertisements,” Journal of Advertising, 19 (2), 40-48. Zeithammer, Robert (2006), “Forward-Looking Bidding in Online Auctions,” Journal of Marketing Research, 43 (3), 462-76.Copyright of Journal of Marketing is the property of American Marketing Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.Record: 6A Theories-in-Use Approach to Building Marketing Theory. By: Zeithaml, Valarie A.; Jaworski, Bernard J.; Kohli, Ajay K.; Tuli, Kapil R.; Ulaga, Wolfgang; Zaltman, Gerald. Journal of Marketing. Jan2020, Vol. 84 Issue 1, p32-51. 20p. 2 Diagrams, 6 Charts. DOI: 10.1177/0022242919888477. Persistent link to this record (Permalink): http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=140321873&site=ehost-liveCut and Paste: <A href=""http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=140321873&site=ehost-live"">A Theories-in-Use Approach to Building Marketing Theory.</A>"
4,"A Theories-in-Use Approach to Building Marketing Theory This article's objective is to inspire and provide guidance on the development of marketing knowledge based on the theories-in-use (TIU) approach. The authors begin with a description of the TIU approach and compare it with other inductive and deductive research approaches. The benefits of engaging in TIU-based research are discussed, including the development of novel organic marketing theories and the opportunity to cocreate relevant marketing knowledge with practitioners. Next, they review criteria for selecting research questions that are particularly well-suited for examination with TIU-based research. This is followed by detailed suggestions for TIU research: focusing on developing new constructs, theoretical propositions (involving antecedents, moderators, and consequences), and arguments for justifying theoretical propositions. A discussion of TIU tradecraft skills, validity checks, and limitations follows. The authors close with a discussion of future theory-building opportunities using the TIU approach.KEYWORDS_SPLITThe marketing discipline is at a crossroads ([28]; [47]). Marketing scholars can continue on the well-worn road of largely testing or extending theories by borrowing from allied disciplines, or we can challenge ourselves to make a significant difference in the lives of managers, public policy officials, and/or consumers. Our point of view is that this road less traveled necessitates deeply and richly exploring marketing topics from the perspectives of individuals (i.e., consumers, managers, and/or public policy officials) who are closest to the problem. This means leaving the comfortable confines of our faculty offices to explore, identify, and define new marketing concepts in their natural habitat.Importantly, as we leave our offices to engage with individuals closest to the problem, we are not simply advocating recording, summarizing, and building rich descriptive narratives. While these narratives are valuable in their own right, we are advocating something more. Namely, we advocate constructing new-to-the-world marketing theories. It is widely acknowledged that theories launch the fundamental knowledge of a discipline ([49]) and are the building blocks for the maturation of a discipline. Articles whose primary contribution is based on proposing theories are generally viewed favorably ([60]). In fact, theoretical advances are critical to the development of marketing as a discipline ([30]). Not surprisingly, editors welcome new theories that are particular to the marketing discipline (see [33]). Against this background, our objective is to discuss an approach that is ideally suited to the development of theories in marketing: the ""theories-in-use"" (TIU) approach.A TIU is a person's mental model of how things work in a particular context ([ 2]). As part of daily life, all individuals employ mental models ([66]). All stakeholders in marketing—among them managers, customers, employees, and public policy makers—have mental models that can be elicited by TIU research to surface interesting, novel theories and concepts that can advance both marketing practice and scholarship. Specifically, we argue that TIU is a natural approach for creating theories that are specific to marketing-related issues—what have been referred to as organic ([26]) or home-grown ([49]) theories. Organic marketing theories involve central constructs that are uniquely or primarily grounded in the marketing context rather than borrowed from other disciplines such as economics or psychology. In this regard, TIU has served as an approach for organic contributions to the marketing discipline by bringing to fore concepts such as service quality, market orientation, experiential consumption, customer solutions, and hybrid offerings.More specifically, a TIU approach can help address three fundamental problems in our discipline. First, when we borrow from other fields, our own stakeholders' problems do not guide our research. Rather than allowing our own stakeholders' problems to guide us, we force-fit a theory or framework on which to base our research. The result is that we are not building a discipline-based body of knowledge. This borrowing approach is certainly one reason that marketing scholarship is losing touch with the practice of marketing ([28]; [47]). Second, borrowing constrains us because we restrict ourselves to what is already known, thereby hampering our search for novel and interesting phenomena. Third, when using abstract theoretical constructs from other fields, we lessen our ability to communicate with our stakeholders in a vocabulary they understand. It is much easier to advance the practice of marketing if one speaks the same language as practitioners than it is to introduce an entirely new glossary of terms.Paradoxically, only a (relatively) small number of TIU articles have been published to date. This is surprising because TIU articles not only are published in our most respected journals but have won major awards (e.g., Shelby D. Hunt/Harold H. Maynard Award, Sheth Foundation/ Journal of Marketing Award), have established subfields of study within the discipline (e.g., service quality, market orientation), and have been a key catalyst for endowed chair appointments at some of the best business schools. As Table 1 notes, three of the top ten articles in Journal of Marketing are TIU articles. Despite this clear discipline and career impact, few researchers pursue TIU research.GraphTable 1. Citations of Top Ten Articles Published in the Journal of Marketing.  1 Notes: Articles in bold employ a TIU approach. WOS = Web of Science Index; GS = Google Scholar. Citation counts gathered on October 4, 2019.Accordingly, this article aims to inspire and support development of knowledge based on TIU among marketing stakeholders. To achieve this objective, we organize this article as follows. We begin with a definition of the TIU approach. In this section, we compare and contrast TIU with other grounded theory methods and deductive research methods for knowledge development. Following this, we discuss key benefits of engaging in TIU-based research. With this foundation in mind, we turn to the practice of TIU research in the field. We divide this practice discussion into two sections: one that overviews the ""basics"" of TIU research and one that provides insight on the advanced tradecraft of the practice. As with any method, one must be able to judge ""good and bad"" practice; thus, we then turn to an assessment of rigor in TIU. This is followed by a discussion of the limitations of the approach. We conclude with suggestions for future research. Theory Construction and TIU[66] note that individuals' TIU may be envisioned as a set of ""if-then"" relationships among actions and outcomes. For example, an advertising manager's TIU may include the proposition that if she associates her brand with an important social cause, then millennial consumers may be more likely to buy her brand. People's TIU may also include complex if-then relationships. For example, a marketer's TIU may include the idea that a firm's customer-centricity improves its profitability, but an increase in customer centricity beyond a certain level adversely affects firm profitability because it is too costly. That is, there is an inverted U-shaped relationship between customer centricity and firm profitability.At its core, the theory construction process involves developing novel if-then propositions. In contrast, the theory-testing process involves empirically assessing the validity of previously developed propositions. While the two processes and their aims are distinct, they potentially can be interrelated. For instance, a theory-testing effort may reveal unexpected findings, which may lead to the construction of new theory to account for the findings. Our focus in this article is on the theory construction process for developing new theory about a phenomenon. The TIU Approach to Theory Construction: Key Qualities[ 2] coined the term TIU to refer to individuals' mental models of the world that guide their deliberate behavior. They contrasted the concept with ""espoused theories"" that refer to the mental models individuals claim or purport to have. While overlap may exist between individuals' TIU and their espoused theories, often these two types of theory differ. For instance, individuals may be unable to articulate parts of their TIU that are tacit. More often still, defensive reasoning mindsets develop that discourage sharing revealing insights ([ 1]).The TIU approach has unique characteristics that bear highlighting. The approach involves soliciting from study participants—the theory holders—the ideas they feel are important and how they are linked to one another. The emerging set of interrelated constructs, regardless of how complete or incomplete they may be as theories, become a researcher's starting point for harvesting constructs, propositions, and arguments. Researchers, however, are not simply passive recorders of participants' thinking. They use their viewing lenses to elicit, evaluate, abstract and extend what they ""hear"" from theory holders included in the study ([63]). The TIU approach relies on one-on-one participant conversations and elicits theories from a relatively small number of participants (often 15–25).The TIU approach is also unique in that it is a partnership that allows for the cocreation of a theory. Participants are treated as active partners in the theory development process, allowing for the presence of implicit and explicit causal thinking among them about the ideas they consider important. Researchers may then draw on other sources of insight they have acquired about the topic to modify the ultimate constructs' abstraction levels and causal connections among them to develop theoretical propositions. Said differently, a TIU approach assumes that the theory holders being interviewed have theories that researchers can uncover and extend using other sources of insight. This is what makes a TIU approach a partnership. It is grounded in two different mindsets—that of the researcher and the interviewees—each focused on theory.A TIU approach becomes an even stronger partnership when researchers convene representative stakeholders including some original study participants to critique and discuss the researcher's tentative formal theory. In this way, two mindsets, the researcher's and the theory holders', are formally brought to bear on the topic. A new and better theory is likely to be created. This is less likely or even unlikely to occur with other approaches falling under the rubric of grounded theory construction. TIU Versus Other Approaches to Theory ConstructionIn general, the theory construction process is inductive in nature. Scholars collect various types of data through means such as unobtrusive observations, secondary data, and participant interviews. They reflect on these data to identify patterns and create new theory. The theory so developed is termed ""grounded theory"" to indicate that it is created from observations and data pertaining to a phenomenon on the ground ([ 7]; [12]; [15]; [54]).We provide an overview of three formal approaches for building grounded theory in Table 2: TIU, case studies and ethnography. The TIU approach relies on elicitation of theories held by individuals with proximity to the problem (e.g., [ 4]). Case studies are in-depth studies of one or a few comparative cases (e.g., [ 5]; [14]). Ethnographies are in-depth studies of a phenomenon aimed at describing its meaning/significance to a group's members and the reasons underlying the meaning/significance (e.g., [16]).[ 6] Importantly, researchers can use these approaches in tandem; for example, a researcher using the case study method can fruitfully include a TIU approach for making comparisons across cases.GraphTable 2. TIU and Related Approaches.  2 Notes: N.A. = not applicable.The theory construction process, however, can also be deductive in nature. For instance, in theoretical modeling, researchers set up models (settings/scenarios) with different characteristics and derive implications of the models for the behaviors of participants in the model (e.g., firms, salespeople, consumers). These behaviors are then linked to the (differing) characteristics of the different models (generally across articles) to construct new theory ([34]).In many instances, researchers review the literature, see gaps or conflicts, and propose new theory, often by introducing a moderator construct or a new explanation stimulated by their own experiences or derived from extant research. This process can be inductive or deductive in nature. For instance, when researchers combine knowledge about a phenomenon in the literature with their personal experiences related to the phenomenon to develop new theory, it is more akin to an inductive process. In contrast, when researchers put two or more findings/assertions in the literature together to derive a new theory, the process is deductive in nature.Table 2 shows prominent inductive and deductive approaches for theory construction and summarizes key differences among them with respect to six facets: purpose, researcher mindset, research process, data collection method, sample selection, and sample size/depth. As the table shows, a major difference between the inductive and deductive approaches is that whereas inductive approaches start with data pertaining to a phenomenon of interest, deductive approaches start with models (settings/scenarios) or theories and work through their implications. A related difference is that whereas a researcher's mindset in inductive approaches is one of exploration and hunting (seeking and processing data in quest of theoretical insights) for constructs and theories inherent but hidden or as yet unarticulated in data, the researcher's mindset in deductive approaches is one of setting up models that are sufficiently realistic yet tractable. Why Use a TIU Approach?As with any research approach, TIU suits certain research questions better than others. We identify major motivations for engaging in TIU research, whether as a stand-alone approach or in combination with other approaches. We find that TIU research is particularly valuable when scholars want to ( 1) construct organic marketing theories, especially about new and emerging phenomena; ( 2) extend extant perspectives and address ambiguities; or ( 3) guide future empirical efforts. In this section, we take a closer look at these three motivations. Construct Organic Marketing TheoriesConstructing organic theories is important to any discipline because organic theories offer unique insights not available outside of the discipline and thus provide good reasons for the discipline's existence as an academic field. Unfortunately, marketing scholars tend to borrow more heavily from other fields than those fields recognize and borrow from marketing ([ 6], [46]). However, the development of organic marketing theories—such as that on service quality, market orientation, and experiential consumption—has influenced other fields, and articles on these topics often receive thousands of citations. As noted by [52], p. 171), ""Clearly, the academic market recognizes the value of homegrown constructs and theories.""Because the TIU approach takes advantage of marketing practitioners' or consumers' experience and knowledge about the marketing setting, it is especially well suited to identifying and defining important constructs that reflect the practical world of marketing, including antecedents and consequences of marketing phenomena. Consider two examples: service quality (see [40]]) and market orientation (see [27]]). Both sets of authors used the TIU approach to develop their pioneering conceptual frameworks. They were able to do so in part because managers had developed practices that offered useful grist for the development of ideas on each topic. Each conceptual framework has prompted significant empirical work and paved the way for substantial research streams on services and market orientation over many years, and is among the top ten cited articles in the Journal of Marketing (see Table 1). Table 3 provides an illustrative set of articles that develop organic theory using the TIU approach.GraphTable 3. Examples of Organic Theoretical Contributions Using TIU Research.   Extend Extant Perspectives and Address AmbiguitiesTheories-in-use-based research is also useful when the aim is to extend extant perspectives about a construct. For example, while research on customer solutions in business markets mushroomed in the early 2000s, solutions were viewed only from the suppliers' perspective. Missing from the discussion was a customer-centric perspective on solution offerings. Using a TIU approach, [56] provided a view of solutions from the customers' perspective, which extended the supplier view of solutions. Similarly, when conflicting theoretical perspectives exist on a novel construct, a TIU approach can help researchers better understand when and why one theoretical perspective may be preferable to the other. Relatedly, the TIU approach can bring precision and clarity when there is ambiguity surrounding constructs and/or nomological net of relationships among constructs. The approach has fewer advantages when working with well-defined constructs where the nomological net has been mapped out comprehensively in prior research. Guide Empirical EffortsTheories-in-use research is often the ideal foundation for empirical efforts. As an example, [40] used a TIU approach to understand the meaning of service quality from the perspective of consumers, employees, and executives, which guided two major empirical efforts that produced multiple publications.The first effort resulted in identifying ten dimensions of perceived service quality gleaned from eight group interviews. The researchers termed the first set of empirical efforts SERVQUAL, a multidimensional scale for measuring consumer perceptions and expectations of service quality ([43], [44], [45]; [40], [41]). Development of scales rests on sound conceptual foundations, and insights of specifics provided by practitioners in a TIU approach helped inform these operationalizations. When queried about the need for expectations in the measure, the authors followed up with another TIU study ([69]).The second effort resulted in the gaps model of service quality, which linked performance by various entities (e.g., employees, channels) to the gap between consumer expectations and perceptions of service quality, and the communication and control processes within organizations that produce these gaps ([68]). A follow-up study empirically examined these variables to identify the most important in each of the four gaps and the relative importance of the four gaps themselves (Parasuraman, Berry and Zeithaml 1991a). Finally, the researchers empirically linked perceived service quality to intentions to examine the behavioral consequences of perceived service quality ([70]). Implementing a TIU ApproachThe TIU approach is best suited for addressing research questions/issues that are broad and deep, and for which we do not have good answers. Research participants should be selected for their knowledgeability about the questions/issues and willingness to share their knowledge and experiences with the researcher. In general, a typical research project requires 15–25 participants selected in successive phases. The knowledge/experience required of the participants in each phase becomes clearer as the research progresses and theoretical ideas come into sharper focus. Importantly, the researcher should have a very strong interest in the research questions/issues and should have good general knowledge related to them. This enables the researcher to listen carefully to participants, ask probing questions, challenge participants when appropriate, and engage with participants in a flexible way—adapting the questions asked to the idiosyncratic knowledge of individual participants and to the learnings from prior participants in the TIU study.Figure 1 provides an overview of the TIU research process. The process typically begins with a focal research construct to be examined in the research (e.g., market orientation, service quality).[ 7] If the construct is not well defined, the research begins by clarifying and defining the core construct. This may take several iterations and feedback loops based on participant conversations (see right-hand side of Figure 1). If the construct is well defined, the research moves to the stage of developing propositions and their associated arguments. The propositions can include antecedents, consequences, mediators, and or moderators. After a few conversations, the researcher begins to formulate tentative propositions that may be assessed on basic screening criteria related to the plausibility and strength of reasoning. After multiple propositions are developed, they must also pass higher-order assessments that relate to the overall contribution of the set of propositions (see Figure 1). These pertain to whether the collective set of ideas adds to the existing literature. As Figure 1 shows, there are numerous feedback loops illustrating the continual iteration and refinement of the conceptual structure.Graph: Figure 1. The TIU research process: an approximation.Notes: Foundational tests = Are propositions plausible, and aligned with definitions and arguments? Advanced tests = Are propositions interesting, substantially informative, and hang together (have one or a few common themes)?The aim in TIU research is not to simply transcribe participants' statements. Rather, it is to review data across participants, look for common themes/ideas in the specifics provided by participants, and abstract commonalities to broader constructs/variables that form the building blocks of an emergent theory. A researcher strives not to present a particular participant's TIU but rather to present a theory reflective of the beliefs and actions of multiple participants, including variables and propositions extrapolated from those beliefs and actions (see [65]] and [61], [62]]).[ 8]Researchers should develop a brief conversation guide that lists a few broad questions they wish to ask participants, along with related probes and follow-up questions.[ 9] If permitted, each conversation should be recorded, notes should be taken during the conversation, and a memo to oneself written immediately following the conversation as to how it adds to prior ideas and points to future lines of inquiry. A researcher returns to these recordings, notes, and memos as a theory begins to take shape and uses them to provide substantiating evidence in the research report.The purpose of conversations with participants is to tap into their tacit and explicit knowledge and beliefs about the research problem/questions of interest to the researcher: ( 1) construct development, ( 2) proposition development, and/or ( 3) argument development. In this section, we describe the nature of the conversations needed for each of these three research problems. We first provide basic guidelines on the TIU research approach, followed by more advanced guidelines for addressing the three research problems. In the next two subsections, we discuss tradecraft related to the fieldwork and identify important checks for rigor in the research. Basic Guidelines Construct developmentWe suggest starting a participant conversation by introducing the topic and segueing into asking what the phenomenon (construct) means to the participant and others familiar to the participant. For an illustrative conversation flow and set of questions, see Table 4. The researcher must ask for specific examples of varying levels of the phenomenon and how it is similar to or different from other proximal constructs (for specific questions, see the top section of Table 5). The researcher should periodically check whether the tentative definitions (s)he is forming are consistent with participants' understanding of the phenomenon. Participant conversations flow unpredictably and generate a lot of ideas and stories, many of which may not relate to the research problem of defining the construct of interest. The researcher must, therefore, continually try to refocus the conversation on the construct (and away from, for example, its antecedents or consequences or just irrelevant information).GraphTable 4. Construct Hunting: A Suggested Conversation Flow for TIU Research.  GraphTable 5. Key Questions/Probes for Building Theories Using the TIU Approach.  For example, [27] started participant conversations by asking, ""What does the term market orientation mean to you?"" Some of the responses were along the following lines: ""It's all about customer need satisfaction,""[10] ""You have to know what competition you are up against,"" ""It means your research and development (R&D) is in touch with what's going on in the market,"" and so on. The researchers formed a tentative idea of the construct's domain from these responses. For example, these responses suggested that the construct was about delivering customer satisfaction in the face of competition, and that R&D is somehow involved in the process.A follow-up probe, ""Tell me a little about your activities that reflect a market orientation,"" elicited numerous responses. They included ""We keep our eyes on the customer and competitors,"" ""We put the customer at the center of everything we do,"" ""We make sure people in one function know what people in other functions are doing,"" ""We reward people for providing exceptional service,"" and so on. These comments suggest that the construct involves knowing customers and competitors, everyone in the company focusing on customers, and each function knowing what the other functions are doing. Note that the last quote is indeterminate as to whether it belongs to the construct's domain or is an antecedent of the (yet to be precisely defined) construct. Follow-up probes might ask, ""Can you tell me how one function finds out what the others are doing?"" and ""What exactly do you do to know how the consumer environment is changing?"" to clarify these questions.After a few of these conversations, the researcher begins to identify commonalities across the participant observations and to abstract them to a higher level. For example, while some participants indicated that they sent out customer surveys, others relied on syndicated data. Yet others visited customers personally. However, the commonality here is that of the generation of customer intelligence through different methods. This led to the development of the idea that market orientation involves, in part, intelligence generation about customers. Subsequent conversations and ongoing reflections led to the eventual definition of market orientation as organization-wide generation, dissemination, and responsiveness to market intelligence. Proposition developmentA researcher's focus here is the development of if-then propositions that aim to identify a phenomenon's antecedents, consequences, mediators, and/or moderators of the phenomenon's effects. At one level, this is relatively straightforward—the researcher asks participants questions such as ""Can you give me examples of actions you took to increase X (the phenomenon)?,"" ""In your opinion, what happens when X increases?,"" ""Can you recall instances in which X didn't lead to that?,"" and ""What accounts for the unexpected results?"" However, participants frequently identify antecedents that reflect the core construct itself or are too proximal to the core construct to be of theoretical interest. For example, when asked to indicate why some organizations are not very market oriented, one participant said, ""It's because they fail to give customers what they want."" Note that this is a part of the market orientation construct, not its antecedent.The types of questions that the researcher asks should be based on the research goal (see Table 5). For example, if the research goal is to link construct X to novel outcomes, the researcher may ask, ""What are the benefits of doing X?,"" ""Were there any surprises or unexpected outcomes of doing X?,"" or ""Did increasing the level of X lead to outcomes that contradict conventional wisdom about X?""As the conversations progress, the researcher forms a relatively clear (albeit tentative) proposition that X leads to Y. At this point, the researcher can assess the proposition by asking questions directly related to the proposition. For example, the researcher may say, ""My last interviewee believes that X leads to Y. What is your view?"" or ""My last interviewee found that X leads to Y. What is your reaction?"" This is particularly useful for propositions that include abstract constructs developed by the researcher. If the level of abstraction is too high, subsequent participants are likely to indicate that the proposition(s) is questionable. Argument developmentIn addition to developing if-then and ""if-then-except-when"" theoretical propositions, a researcher must also provide plausible arguments or justifications for the propositions. Argument development involves probing participants for the reasons they hold their if-then beliefs. Thus, a researcher may ask participants, ""Why do you believe X leads to Y?"" or ""Why do you expect M to strengthen the effect of X on Y?"" Developing an argument may also involve listening to the reasons offered by participants and identifying one or more mediators of the effect of X on Y. As in the case of developing theoretical propositions, the challenges here pertain to appropriate level of abstraction as well as to maintaining consistency with the evolving definitions of the core construct and the antecedent, consequence, or moderator variables involved. Advanced Guidelines Construct developmentA key aspect of theory construction using a TIU approach is the process of abstraction from the raw data surfaced in the course of participant conversations. Abstraction involves considering two or more elements (e.g., words, phrases, ideas in one or more sentences) in raw data (e.g., transcriptions of participant conversations), pooling the elements into a higher-order category or construct, and giving it a label (i.e., name/term). Such a construct is of a higher order (i.e., is more abstract) than the elements in the sense that it captures the essential information in the two or more elements but excludes some of their details. [ 7] refer to the general process of identifying and categorizing distinct elements in the data as ""open coding.""A researcher may use one or more of several approaches for abstracting from the elements (e.g., words, phrases) contained in the data obtained from participant conversations. We discuss three approaches. In the first approach, a researcher examines the data within and across participant conversations and notices that they contain several elements that have different meanings but all seem to be subsets of one of the elements in the data. In this case, the latter element is of a high-order (i.e., is more abstract), and the researcher may consider it a candidate construct for his or her theory. The abstraction process here is one of identifying elements that are all a part of a broader, more abstract element and treating all as the latter element for the purpose of theory construction. Importantly, this calls for the researcher to actively seek out such interrelationships among the elements to identify them. For example, participants may provide the following statements to a researcher to indicate that their respective firms are market oriented: ""We survey customers to find out their needs and wants,"" ""Our company does a lot of market research every quarter,"" ""Our salespeople ask customers how we can serve them better,"" and ""We generate intelligence about our markets."" In this case, the process of abstraction involves observing that the italicized elements in the first three statements are subsets of the italicized element in the fourth statement and thus suggests using the construct of ""market intelligence generation"" in subsequent theory construction efforts.In the second approach, a researcher examines the data within and across participants and notices that they contain elements that likely co-occur (or covary). That is, when one element is present (or is at a high level), another element is also likely to be present (or at a high level). The researcher pools these elements into a higher-order category or construct and, if needed, gives it a label/name. For example, one or more participants may describe customer reactions to exceptional service in restaurants as follows: ""They feel valued as customers,"" ""Their eyes come alive,"" ""They smile, and thank the waitpersons,"" and ""They leave big tips."" Each of the italicized elements likely occurs when the other elements also occur. As such, the researcher may pool them into a higher-order category or construct of customer satisfaction and use it for subsequent theory construction.In the third approach, the researcher examines the data within and across participants and notices that they contain elements (e.g., words, phrases) that are neither subsets of one of the elements (approach 1) nor do they necessarily co-occur or covary (approach 2). Rather, they appear to be different facets/dimensions/aspects of a broader concept or idea. The researcher pools these elements into the higher-order category or construct and, if needed, gives it a label/name. For example, participants may describe outcomes of investing in market research as ""Market research helps us get a bigger piece of the market,"" ""It brings in more revenue,"" and ""It costs money, but in the end, we save money because we don't try to be all things to all customers."" The italicized elements are not subsets of one of them and often do not covary, but each is an indicator of the broader concept of how well a firm is performing. As such, the researcher may pool them into a higher-order category or construct of firm performance for use in subsequent theory construction.Importantly, as a construct's meaning begins to form, a researcher must take care that the construct's domain is not too narrow or too broad. If it is too narrow, it is too specific and limits the generalizability of the theory. If it is too broad, its components may not all relate to other constructs (potential antecedents or consequences) in a similar manner. This becomes clearer as the construct's antecedents and consequences emerge in the course of participant conversations. Importantly, as a construct's meaning begins to emerge, the researcher must ascertain whether it is truly capturing a distinct phenomenon, one not reflected by other known constructs (especially those already discussed in the literature). For example, when asked whether they thought market orientation and customer orientation were the same thing or different, most participants pointed out that market orientation was a broader construct in that it focused on customers and other influences on them, whereas customer orientation focused exclusively on customers. Upon reflection, it became clear that the two would have somewhat different antecedents (e.g., company systems that base rewards on customer satisfaction vs. those that base them on broader metrics such as market share and profitability). Proposition developmentAfter developing constructs, a researcher links them to develop tentative theoretical propositions, stimulated by participants' TIU elicited in course of participant conversations. The general process of linking two or more concepts with each other is referred to as ""axial coding"" ([ 7]). There are two main challenges in developing propositions that identify antecedent, consequence, mediator, and moderator variables.First, when the core construct/phenomenon is yet to be defined precisely, the emerging antecedents, consequences, mediators and moderators need to be identified and defined in conjunction with the core construct in a way that the resulting propositions make sense. For example, when a participant in the market orientation research was asked, ""Why do some firms fail to give customers what they want?,"" he indicated, ""Well, they are afraid of changing what they have done for many years. They feel safe doing the tried and tested."" A further ""why"" probe led to ""Because they are afraid they will be pulled up by the management if they do something different and it bombs."" A few more probes later led to the more interesting revelation that an organization's employees may fail to provide customers the offerings they need because of the fear of being punished by their managers who themselves are concerned about being punished by a risk-averse top management. This led to the identification of ""top management risk aversion"" as an antecedent of market orientation. Note that ""top management risk aversion"" is not a part of the core construct and is a relatively abstract, novel construct, and the proposition makes sense if market orientation is defined in part as responding to customers' changing needs.Second, the propositions developed should ideally be novel (i.e., not documented in the literature) and interesting (i.e., not obvious but useful). Such propositions often challenge conventional wisdom, identify conditions in which extant theory does not hold, or develop interesting nuances that lead to ""aha!"" moments for the readers. Frequently, however, participants offer input with little insight. For example, when asked why some firms are more market oriented than others, several participants indicated, ""Firms that are market oriented are that way because they care,"" and ""It takes hard work to be market oriented."" These and many other ideas that emerged in the course of the conversations were either obvious or previously documented and, therefore, not pursued further. It is important for the researcher to continually ensure that the propositions (s)he is generating and retaining for further consideration are new to the literature, interesting, plausible, and of importance to some set of stakeholders.[12] Argument developmentA straightforward way for a researcher to develop arguments to support a theoretical proposition is to ask participants why they believe (and perhaps why they do not believe) in a proposition. It is very important, however, for a researcher to critically evaluate the soundness of the reasoning before accepting it as plausible. The researcher may also develop theoretical propositions and arguments by connecting disparate ideas obtained from two or more participants. For example, one participant may note that doing A leads to Y, and another participant may suggest that doing X leads to A; putting these two assertions together would suggest the testable proposition that doing X leads to Y, the argument being that X leads to A, which in turn leads to Y. Pulling it all togetherFrequently, a researcher's goal is to construct a set of coherent theoretical propositions that collectively represent a substantial contribution to the literature. After generating a reasonably large number of theoretical propositions, a researcher should take stock of them with a view to selecting the ones that have one or a few common themes such that the selected set can be formalized in a parsimonious way. The researcher may group the constructs involved across propositions into broader categories, or identify one or a few common high-level arguments across propositions. The general process of choosing from among the theoretical ideas developed in a research process is referred to as selective coding ([ 7]). TIU TradecraftIn this section, we discuss key nuances of the TIU research process and offer suggestions that increase the likelihood of developing impactful new theory. Following this, we offer suggestions for crafting research papers. Extensive IterationAs noted previously, the theory construction process entails collecting data from a few participant conversations and then interacting with the data to generate preliminary, tentative theory (constructs, propositions, and arguments). The tentative theory guides the researcher's focus in collecting data from subsequent participants. These data frequently augment the tentative theory and/or suggest its modification (e.g., revising constructs, changing their abstraction levels, adding propositions, developing new arguments). The resulting theory, in turn, guides subsequent data collection, and so on, until a researcher is satisfied with the theory.For example, say that a researcher is interested in constructing a theory of postrecession performance of firms. Drawing on data collected from the first few participants, the researcher constructs a tentative theory that a firm that increases its R&D spending during a recession enjoys higher market share after the recession. After a few more conversations, the researcher constructs another tentative theory that a firm that invests in operations to make them more efficient during a recession increases its profitability after the recession because it redirects slack resources during the recession to reducing ongoing operations costs. At this point, the researcher considers the elements ""R&D spending"" and ""investments in operations"" and abstracts them to a broader construct of capability building. Similarly, the researcher abstracts ""market share"" and ""profitability"" to a broader construct of firm performance. Using these constructs, the researcher constructs the proposition ""The greater a firm's capability building during a recession, the greater the firm performance postrecession."" Active ListeningA researcher is not simply a passive ear. The maxim that data do not say anything—only managers or researchers do—applies to TIU research as much as it does to other methods ([64]). Theories-in-use approaches provide a special opportunity for researchers to exercise disciplined imagination and add unique value to an investigation. This occurs, for instance, when researchers listen carefully for what a participant is not saying (i.e., what potentially important ideas seem to be missing or understated by interviewees). For example, in an insight development project, managers had little to say about the important constraints placed on insight development by long-standing company policies.Similarly, a researcher may develop a theoretical proposition that was not directly stated or derivable from participant data but still grounded in them. For instance, one participant may identify P as a new antecedent of a phenomenon, and another participant may identify M as a moderator of the effect of a different antecedent R. The two sets of ideas may lead the researcher to examine whether M may moderate the influence of P (in addition to that of R). A researcher also has an important role in adding value by explaining why certain findings are surprising, counterintuitive, or contrary to received wisdom on the topic. Belief SuspensionWhen engaging with a participant, it is key for researchers to temporarily suspend their prior beliefs and tentative ideas developed in the course of previous participant conversations. This is not easy, but it is important to listen with an open mind, absorb the participant's ideas, and probe deeper into those that have the potential for generating new insights. Researchers can feign ignorance and ask a number of ""why"" questions even if they believe they know the answer: ""Why do you say that?,"" ""Why does it affect X?,"" ""Why would doing X not be helpful in circumstance M?"" As these questions continue, they can lead to interesting new insights. Depth over BreadthAs may be evident from the previous examples, participant conversations elicit considerable commentary. When listening to a participant's responses, the researcher should try to identify and define the abstract construct that reflects the detailed description provided by the participant. To the extent the researcher is successful in doing this, the theory construction task following the participant conversations becomes easier because a theory essentially is a set of interrelated constructs. It is helpful to record participant conversations as well as take notes during the conversations, which can be revisited in the course of developing construct definitions, theoretical propositions, and arguments. Openness to New IssuesIt is sometimes more productive for a researcher to go where a participant's interest takes the conversation rather than strictly focus on the precise questions with which the researcher comes into the conversation. For instance, a participant may say something that may seem a bit odd or unrelated to the research questions. The researcher may be tempted to brush it aside to have a more ""productive"" conversation, but doing so may lead to missing out on potentially interesting and useful new ideas. Conflict AppreciationWith each conversation a researcher learns a little more about the three components of the theory under development: construct definitions, theoretical propositions, and arguments. (S)he must relate these to those learned from earlier conversations up to that point. This provides greater confidence in similar ideas obtained in previous conversations. Ideas not previously elicited can be noted for further exploration in subsequent conversations. The researcher may also encounter ideas that are in conflict with established ideas. For example, some participants may indicate that R&D spending in recessions hurts performance, whereas others may believe that it helps performance. These may prove to be most interesting and need to be resolved (perhaps by identifying appropriate moderators) in subsequent participant conversations. Mosaic FillingA researcher also tracks the components of a theory that are developing well as well as those that are ""light"" and need further exploration; (s)he then selects subsequent participants accordingly and engages in conversations that address those components. For example, after a few conversations with brand managers, a researcher interested in constructing a theory of brand love may learn more about the antecedents and consequences of brand love than about the moderators of its consequences. Thus, the researcher may focus more on surfacing moderators in subsequent participant conversations. At some point, researchers will recognize that continued collection and analysis of data is unlikely to yield new themes, categories, or substantive insights, a situation known as theoretical saturation. Bias RecognitionAs we know from research on cognitive biases, peoples' mental models can be deficient. For example, opinions and strongly held feelings have a way of surviving challenges from facts. Just because a participant expresses a particular story with conviction does not mean that it should be accepted by the researcher as factual. While it is a sincere expression of the participant's judgments, the story merits critical examination and possible correction or improvement ([23]; [51]; [55]). Demarcation of TIU Study LimitsIt can be difficult to figure out the right ""demarcation"" between the ""context"" of a TIU study and the constructs studied. For example, a context may be business-to-business firms and a researcher may be exploring constructs X, Y, and Z. The business-to-business context, however, also has other constructs associated with it (e.g., direct sales force vs. channel partners, client concentration). Therefore, the researcher has a choice here: to study the context variables and include them in the theory, or to limit the theory to the study's context. Crafting Research PapersA researcher may substantiate claims about a construct's meaning and/or a theoretical proposition (along with its underlying logic) by indicating how several participant conversations reflected this. Providing direct quotes from one or two of them is a convincing way to accomplish this. These quotes provide a verbal lexicon and allow the reader the opportunity to develop an alternative formulation. However, as participants in a conversational mode frequently allude to multiple ideas in a single sentence or two, the researcher must portray quotes that clearly and unambiguously make the intended point.Another emergent, value-added quality that can strengthen a paper is an answer to the question, ""So what?"" This question can be answered from both a researcher and marketing stakeholder standpoint. For example, the final construct network or mental model that represents consensus thinking among participants can be used as a playground for theory construction. The researcher may offer an additional map containing new constructs and their proposed relationships along with those already in the map. The changes in the map (i.e., the new constructs and their connections with others previously identified in the interviews) would represent the researcher's unique reflections about the data. This new bundle of related, testable propositions is a new theory that could guide future research and thinking.The consensus map may also be a basis for helping marketing stakeholders think through its relevance to their positions. The researcher can offer ""map management"" suggestions to stakeholders. For instance, they might be encouraged to ask, ""Which constructs should be emphasized or deemphasized in their situation? How might particular connections between constructs be weakened or strengthened? What new constructs may be added to the original consensus map network?"" Essentially, questions like these help stakeholders shore up strengths in their thinking and compensate for limitations. Evaluating Rigor in TIU ResearchThis section offers criteria for evaluating the rigor of a TIU-based study. Some of the criteria commonly used to evaluate studies include internal validity, external validity, and reliability (see [39]]). Several scholars, however, have long argued that these criteria are cast in a positivist tradition, and that different criteria should be used to evaluate interpretive research (e.g., [17]; [29]). Researchers have developed numerous criteria for evaluating interpretive research, some of which mirror the commonly used criteria of reliability and validity. Prominent among these are four criteria described by [17] and [29]: credibility, transferability, dependability and confirmability (see also [ 3]]; [ 9]]). These criteria are discussed by [19] and have been used in prior marketing research (e.g., [13]).A TIU-based study shares aspects of the positivist as well as the interpretive traditions. It is positivist in that it aims to develop clear new causal associations about a phenomenon and interpretive in that it uses study participants' interpretations of the phenomenon. For this reason, we adapt the four criteria (credibility, transferability, dependability, and confirmability) for evaluating the rigor of TIU-based research and indicate tests researchers can use to demonstrate (and evaluate) the rigor of their new theories. Importantly, while these four criteria are useful, we suggest they need to be complemented by a fifth criterion—distinctiveness—that refers to the novelty of a theory's constructs and propositions (relative to extant literature). This criterion is central for evaluating TIU research whose aim is the construction of new theory. Table 6 summarizes these five criteria and how they may be used for evaluating TIU research.GraphTable 6. Rigor in TIU Research.   CredibilityCredibility is analogous to internal validity, and in the context of TIU-based research refers to the extent to which a new theory's if-then propositions are plausible.[13] This may be demonstrated by providing strong arguments to support the propositions. For this reason, we recommend probing participants for why they believe in their if-then propositions. Their responses (potentially combined with extant theories and findings in the literature) can be instrumental in constructing persuasive arguments for the new theory's if-then propositions. We also recommend asking participants range-spanning questions to encourage them to consider the full range of constructs involved (e.g., very high to very low; e.g., [27]). For example, if some participants indicate that strong loyalty programs lead to higher market shares, it is useful to ask subsequent participants (or the same participants later in course of the conversations) about the consequences of having weak loyalty programs along with the reasons for those consequences. If participants indicate that one of the consequences is low market share and they provide the same argument for it, documenting this information is likely to increase the theory's credibility. Finally, we recommend comparing across participants. To the extent multiple participants suggest the same theory, its credibility is enhanced. TransferabilityTransferability is analogous to external validity, and in the context of TIU-based research, it refers to the extent to which a new theory's constructs and if-then propositions are valid in contexts not included in the data used to develop the theory. Researchers can increase confidence in the transferability of their new theory through appropriate theoretical sampling of participants in their studies. As a tentative theory emerges in the course of conversations with participants, researchers can select as the next set of participants those for whom the theory may not hold (e.g., participants in different types of firms, industries, geographic locations; participants with different experiences; e.g., [ 4]). To the extent the next wave of participants suggests the same emergent theory, it increases confidence in the transferability of the theory. If the subsequent participants suggest different theories, it would indicate the need for a resolution, generally through the incorporation of one or more moderators and/or inclusion of additional antecedents/consequences. DependabilityDependability is analogous to reliability, and in the context of TIU-based research refers to the extent to which multiple researchers (""multiple human instruments"" per [19], p. 241]) involved in a TIU study find the same constructs and if-then propositions from the same data.[14] This may be assessed through comparison across researchers. To the extent multiple researchers processing the same data converge on the same theory, its dependability is enhanced. ConfirmabilityConfirmability is analogous to objectivity, and in the context of TIU-based research refers to the extent to which a new theory's constructs and if-then propositions can be independently certified as emerging from the data (rather than from researchers' predispositions, interests, and motivations). Researchers can demonstrate confirmability by documenting participant checks that are similar to member checks suggested by [29]. Researchers may present their emerging (as well as eventual/final) theory to TIU research participants and ask them whether it is consistent with their views (as well as invite comments/remarks).Researchers can also demonstrate confirmability by documenting agreement between two or more independent judges (i.e., knowledgeable individuals who are not involved with the research) about the new theory's correspondence with the data used to develop it. For example, researchers using a TIU approach typically develop abstract constructs from specific data (instances, examples) provided by participants. In such cases, researchers can demonstrate confirmability through interjudge reliability. This involves researchers providing two or more judges the raw/verbatim data (or a random sampling thereof) and the names of their abstract constructs and having them code the raw/verbatim data into the constructs. Following this, interjudge agreement may be computed (e.g., using proportional reduction in loss proposed by [50]]). Similarly, researchers can demonstrate confirmability by documenting agreement between two or more independent judges asked to indicate the extent to which a theory's if-then propositions correspond to the data from which they were created. Researchers can also demonstrate confirmability by providing thick descriptions of their data (e.g., verbatim participant quotes) in their reports to enable readers of the theory to do a direct assessment of the extent to which theoretical constructs, propositions, and arguments advanced in the theories are consistent with the raw data used to construct them.As noted previously, data from TIU research participants can stimulate a researcher to develop if-then propositions that were not cited or directly suggested by any of the participants. Deviations from the data provided by participants also arise when a researcher develops a theory incorporating constructs at different levels of abstraction than those stated by participants. In such cases, we caution against strict adherence to the confirmability criterion and instead suggest using theory credibility as the more important criterion. This is because the central purpose of using TIU for theory construction is to develop new theory that accurately explains a phenomenon of interest, not one that is an accurate restatement of data provided by participants. DistinctivenessDistinctiveness is analogous to discriminant validity, and in the context of TIU-based research, it refers to the extent to which a new theory's constructs and if-then propositions are different from existing constructs and if-then propositions in the literature. Because it is counterproductive to introduce new labels for existing constructs, it is important to ensure that new constructs in a theory refer to different phenomena than existing constructs. Construct distinctiveness may be demonstrated by definitional comparisons—comparing the proposed definition of a new construct with definitions of existing constructs that are closest to the meaning of the new construct. Proposition distinctiveness refers to the extent to which if-then propositions differ from theoretical propositions already available in the literature. Propositional distinctiveness may be demonstrated by documenting closely related existing propositions individually or in summary form and visually showing the differences between them and the new theory (e.g., in a table). Summary ObservationsWe argue that while all five criteria are useful for evaluating a new theory, the primary emphasis should be on credibility, transferability, and distinctiveness. Dependability and confirmability are good virtues, but not as pertinent as credibility, transferability, and distinctiveness. This is because the end goal of TIU research is the development of a new theory that can explain a phenomenon across multiple contexts; it is conceivable that researchers may develop such a theory even when it is somewhat lower on interresearcher reliability (dependability) and interjudge reliability (confirmability).Importantly, the quality of a theory based on TIU of participants is likely to be influenced substantially by the quality of theories held by the participants. As noted previously, researchers should take care to sample participants who are likely to be knowledgeable about the phenomenon being studied and also willing to share their knowledge with the researchers. By documenting the participants' qualifications, researchers can engender greater confidence in the theories they develop using the TIU approach (see Table 7).GraphTable 7. Glossary of Terms Used.   TIU Limitations and ChallengesLike all research approaches, TIU has limitations and challenges. First, TIU, as a technique used largely for theory construction, is not suited for theory testing. However, as we have shown, TIU can be a terrific setup for guiding downstream theory-testing efforts. Second, researchers often lack (but can still acquire) the requisite skill and experience needed for doing successful interviews with key informants. Using the recommendations in this article is a good start. Next, reading the TIU research delineated in our Tables 2 and 3 will help. Finally, practicing interviews with other researchers using TIU can prepare a researcher to conduct interviews with actual participants.Third, TIU works only when informants have sufficient knowledge and experience. For relatively new phenomena (e.g., a firm operating as a platform as well as a supplier on the platform), participants are unlikely to have well-developed theories about their long-term effects and/or the conditions under which the effects are likely to be strong or weak. Participants in these situations may still espouse theories, but they are less likely to be the product of thoughtful processing of meaningful experience. An idea about a relatively unfamiliar issue could be an uncertain participant's guess as opposed to a highly relevant but newly discovered ""aha."" Future ResearchAs we have noted, the discipline of marketing is at the crossroads. Others have suggested that if we continue on our current trajectory, we will simply accelerate our path to irrelevance ([47]). One promising method to increase relevance to all stakeholders in the marketing system is a TIU approach. Relevant stakeholders may be managers aiming to improve practice, consumers aiming to enhance their consumption experiences, and/or policy makers aiming to improve society. In this section, we turn our attention to three specific areas of future research. The first two future research areas focus on direct applications of TIU. The first application area is non-domain-specific. Here, the emphasis is on identifying ""meta issues"" that can richly inform any subfield of marketing (e.g., when stakeholders disagree, when core assumptions underlying a body of work are questionable), whereas the second is focused is domain-specific (e.g., role of marketing in the firm, organic growth, digital transformation). The third category involves research on TIU as a method. Future Research: Meta DomainsIn this section, we consider research topics that could apply to any field or subfield of marketing. In a sense, these topics are ""meta"" questions that can guide researchers in selecting topics specific in their area of specialization. Next, we explore three such issues. An underlying assumption that may be reexaminedAn assumption is a hypothesis that is taken for granted. A theory built on an assumption that is not fully explored may be incomplete or may even contain errors. The published literature often identifies and debates such assumptions. For example, the literature on market orientation currently has two dominant perspectives—one focusing on processing marketplace information ([27]) and the other focused on a market-oriented culture ([38]). However, both perspectives assume that understanding customer needs and putting customers at the ""center of your business"" is essential for success. An interesting question to be explored using a TIU approach would be ""When do customer needs not matter?"" or ""Under what conditions should the customer not be at the center of the business?"" The notion of building businesses around customer is at the heart of our discipline, yet it could be challenged by examining successful businesses that have taken a different approach. From a public policy perspective, there is an assumption that it is ""always"" best build a business around a customer; however, this assumption can also be reexamined. When do customer-oriented businesses increase consumer costs and lessen customer satisfaction? Conflicting firm and customer viewpointsA TIU approach can be very productive when a firm's or even industry's ""theory"" of its behavior in the marketplace is at odds with their customers' ""theory"" of the firm's or industry's intentions and actions. Comparing manager theories or maps of their actions with those customers hold about the same actions can help a firm or industry achieve a better alignment with its customer base. A contemporary example involves current viewpoints regarding the pricing of drugs in the pharmaceutical industry. Here there are conflicting views held by firms (e.g., high prices support the portfolio of R&D efforts, some of which work and others do not), policy makers (e.g., consumer affordability), and customers (e.g., price gouging). Conflict among key stakeholdersA TIU approach may make clear where key stakeholders agree or disagree and what options exist that can foster agreement among them. These are common situations in the public health, political, and nonprofit marketing settings. Very few articles using a TIU approach exist in the public policy domain, yet many agencies' stakeholder interests are in conflict in that domain. For that reason, this is an especially promising domain for organic theory construction. Future Research: Content DomainsAs we discussed previously, many sources exist to identify content domains suited to TIU-based research. These include Marketing Science Institute, industry-specific surveys of ""hot topics,"" trade association agendas, public policy agencies' grant funding priorities, and the American Marketing Association. Many of these institutions also provide researchers with direct access to subject matter experts and offer platforms for sharing research findings. While the list of future research content domains is lengthy, we focus on a few topics that can be richly explored using the TIU approach. Role of marketing in the firmWe find it curious that a dominant view exists in marketing that ""best-practice marketing"" entails segmenting markets, selecting target segments, developing differentiating value propositions, and then activating with a marketing mix. Any or all of these basic steps could be challenged using a TIU approach. For example, under what conditions does segmentation still matter, and when is segmentation inappropriate? When do differentiated value propositions decrease, rather than increase, sales? And, thinking more broadly about the function, when should marketing ""not have a seat"" at the table in business unit strategy discussions? Organic growthThe litmus test for any high performing chief executive officer, general manager, or brand manager is year over year profitable organic growth. The problem in our discipline is that we often approach growth as a marketing issue. However, from a firm perspective, the issue is how to integrate all back office and commercial functions to drive organic growth. Marketing is only one piece of this puzzle. When should marketing play a prominent (or less prominent) role in shaping the growth strategy? When is it appropriate to have ""chief growth officers"" lead these growth efforts? What role should marketing assume when a firm decides to hire chief growth officers—and not chief marketing officers? Digital transformation of the firmThis topic is front and center for most Fortune 500 firms, yet little theory exists to guide firm actions in structuring market communications, collecting consumer intelligence (e.g., traditional research methods plus digital footprints), or building customer-facing digital platforms (e.g., General Electric's recent unsuccessful attempt to build a client-facing platform for the industrial internet). Research in this domain could also closely examine the implications of digital transformation for the marketing organization within a firm. For example, whereas social media is largely viewed as an avenue for advertising and promotions, several firms are actively using social media channels for customer service, direct sales, and market research (see, e.g., efforts of KLM, the Snickers ""Hungerithm"" campaign).Digital transformation of industries and markets requires executives to rethink next-generation marketing resources, capabilities, and skills their companies need to secure and grow to engage with customers in new and meaningful ways in the digital age. For example, companies today increasingly focus on rolling out new subscription-based business models. In line with this fundamental trend, a growing number of firms invest in new organizational functions, such as customer success management; they hire new staff across all hierarchical levels, from vice presidents of customer success to customer success associates. Clearly, key decision makers add new customer-facing roles and responsibilities to complement others in existing areas, such as customer experience management or key account management. What are executives' mental models underlying such decisions? Which TIUs guide managers in growing these novel marketing competencies? Research in TIU is well positioned to shed new light on this growing managerial practice. Consumer privacyWith the 2018 emergence of General Data Protection Regulation standards in Europe, the California Consumer Privacy Act becoming law in January 2020, and current debates in Congress on the possibility of a National Commission on Public Privacy, we are witnessing an acceleration in the debate and implementation of privacy policies. The aim is to protect consumers at multiple levels—by including access to personal data (e.g., health records finance), limiting hacking, and, more generally, maintaining personal privacy. A TIU approach would be particularly useful in assessing the trade-offs that consumers are willing to make regarding the balance of sharing versus protecting their personal information. This is important wherever paradox arises, such when consumers insist on greater protection of personal data while enjoying the benefits of more personally relevant information and firm offerings resulting from firms mining their personal data. A TIU approach can be valuable in surfacing moderators that help consumers resolve such paradoxes. Health care policyIn the United States, a particularly contentious debate is unfolding regarding single-payer systems, the role of government in delivering health care solutions, and the overall cost of health care. While these are large, complex issues, behind the scenes there is a sense that there are two diametrically opposed worldviews that ""set context"" for the debates. One of the authors of this article has been involved in a TIU project aiming to better understand how Democrats and Republicans view health care disparities to overcome political gridlock. The overall objective was to understand the fundamental frames both groups used to understand health disparities and help develop a campaign that would push the issue forward without alienating either group. It was found that, contrary to public expressions, there were important commonalities as well as differences between the two parties that served as a shared foundation for discussing their differences.A second health care topic is connected health care. Ensuring that patients take their medicine as prescribed and achieving compliance is both a societal goal and a company goal, but what about consumers' position? Increasingly the topic of connected health becomes intertwined with privacy concerns. Many firms now remotely monitor patient compliance through medical devices (e.g., sleep apnea machines with embedded chips) and, as a result, the patient, physician, channel intermediaries, and insurance firms all have access to patient data. The overall system improvements—reimbursement based on actual compliance, better patient flow management in doctor's office, and reduced labor costs for the channel—are all very positive. However, we do not have a deep understanding of the patients' positive and negative views on connected health care. Government involvement and regulationIncreasingly governments are more involved in the day-to-day affairs of for-profit and nonprofit organizations. Despite the important role that regulation plays in improving the common good (e.g., pollution controls, environmental policies, land protection, water management), there are clear reasons for for-profit firms to oppose these regulations and/or actively lobby against them. These could be for economic reasons (e.g., adverse influence on their profitability) and/or for constituency reasons (e.g., a firm is based in a region that highly depends on that particular industry sector). As noted previously, TIU is particularly useful in situations where stakeholder views may differ—or even collide. Future Research on TIU MethodologyAll research methods, including TIU, merit continual improvement. Each method has strengths and weaknesses in which further inquiry can refine or enhance strengths and diminish weaknesses. Next, we discuss four areas for future research on the TIU itself. Optimal sample sizeWhat topic and population factors influence desired sample size? When is redundancy in constructs and construct pairing most likely to occur, suggesting that further interviews may not be productive? Rules of thumb vary between 15 and 25 participants, but more systematic clarity is needed. This is critical because travel budgets, transcription costs, and researcher time are typically scarce resources, especially when multiple populations are involved (as is the case with cross-cultural research). Eliciting causal connectionsA special value of TIU is its ability to directly elicit the causal mechanisms—the ""hows"" and ""whys"" supporting particular construct pairings—present among theory holders. These, of course, are critical to any theory-building enterprise. More R&D is needed to document productive and unproductive elicitation techniques for particular populations and circumstances. For instance, children often have well-developed TIUs, but eliciting them is a special challenge requiring more novel probing and elicitation processes. Separately, some probing techniques may work best in face-to-face interviews but less well for those conducted online. These are all situations requiring more study. Alternative probing techniquesSome topics are inherently more challenging than others for respondents to address. This is especially the case when a topic concerns socially embarrassing issues (e.g., personal hygiene) or involves considerable implicit thinking and tacit knowledge (e.g., knowledge that may not have been given much prior explicit thought by the participant). Such taken-for-granted experiences are circumstances where TIU is especially valuable. Research is needed to identify alternative ways of using TIU interview techniques for such instances. ConclusionThe TIU approach is ideally suited to surface interesting, novel theories and concepts that can advance both marketing practice and scholarship. As such, the overall objective of this article is to inspire and provide guidance on the development of knowledge based on the TIU approach. A key message of this article is that while the TIU approach requires skill, tradecraft, and practice, it has resulted in multiple breakthrough, award-winning research articles (e.g., see Table 2). These articles represent important organic marketing theories that have paved the way for long-lasting research streams that continue to inspire scholarly research today.While impact may be a sufficient motivation, there are two additional benefits of pursuing this approach. First, researchers using this approach often find that gleaning new insights this way is a special variant of fun. The fun involves the excitement of discovering something novel as well as getting closer to the marketing phenomena. Giving time to executives, consumers, and policy makers to explore their own thinking is also rewarding. Furthermore, having one's own ideas challenged by interviewees can shake a scholar out of the routine of reviewing literature written by other academics ([32]). Second, TIU research not only represents a great vehicle for bringing relevance to the classroom but also provides a platform for sharing real-time stories and challenges that are unfolding in practice. Moreover, in our experience, managers taking part in the research are often excited about the prospect of becoming long-term partners in the research and education process.In conclusion, if the field of marketing is to continue to have relevance for the practice of marketing, we must develop ideas, concepts, and theories whose central focus is the study of marketing in its natural environment. Within this environment, managers, consumers, and policy makers are a wonderful source of new ideas, unconventional thinking, and ways of working that can fundamentally reshape our current thinking and theories. One can ""go it alone"" by reading marketing literature and coming up with ideas, or one can capitalize on the knowledge of managers, consumers, and public policy makers who are dealing with significant, underresearched challenges every day. We hope our team experience captured in this article will facilitate your focus on the latter! "
5,"A Theory of Customer Valuation: Concepts, Metrics, Strategy, and Implementation Customer value refers to the economic value of the customer’s relationship with the firm. This study approaches the topic of customer value for measuring, managing, and maximizing customer contributions by proposing a customer valuation theory (CVT) based on economic principles that conceptualizes the generation of value from customers to firms. The author reviews the established economic theories for valuing investor assets (e.g., stocks) and draws a comparison to valuing customer contributions. Furthermore, the author recognizes the differences in the guiding principles between valuing stocks and valuing customers in proposing CVT. Using CVT, the author discusses the concept of customer lifetime value (CLV) as the metric that can provide a reliable, forward-looking estimate of direct customer value. In addition, economic models to estimate CLV, ways to manage CLV using portfolio management principles, and strategies to maximize CLV are discussed in detail. The author extends the customer value concept by discussing ways that a customer can add value to the firm indirectly through incentivized referrals, social media influence, and feedback. Finally, the benefits of CVT to multiple constituencies are offered.Any sustainable business first creates value for its customers through firm offerings1 and, in the process, derives value from its customers in the form of profit.2 This duality of roles performed by firms and customers to derive and deliver value best summarizes the firm–customer relationship from a value standpoint. However, this value is distributed heterogeneously across customers. Because it is the firms/decision makers who allocate resources to markets, customers, and products, the challenge in this context for firms is to dynamically align resources spent on customers and products to simultaneously generate both value to customers and value from customers. In addition, the volatility and vulnerability in customer cash flows differentially affects overall firm profitability. These changes can be due to both customer actions and firm actions. For instance, especially in business markets, leadership change in the customer organization influences procurement decisions that can result in changes in future cash flows. Similarly, customer life events (e.g., getting divorced, becoming empty nesters) can also affect future cash flows. Therefore, firms look for ways to better manage cash flows (Srivastava, Shervani, and Fahey 1999).To better understand and manage the value creation and cash flow management process, this article proposes a theory to value future customer contributions: the customer valuation theory (CVT). The CVT focuses on two aspects of customer financial contributions: their nature (i.e., direct and indirect) and their scope (i.e., breadth and depth). In doing so, the CVT informs firms about ( 1) the conceptualization of value generation from customers and ( 2) the ways and means available to generate and maximize value from customers. In this regard, the power of the customer lifetime value (CLV) metric to accurately value a customer’s future contributions is established. By applying the CLV metric, this study demonstrates how firms can use CVT to ( 1) value customer assets, ( 2) manage customer portfolios, and ( 3) nurture profitable customers. The robustness of the CVT is also highlighted through its successful implementation across various types of markets (consumer and business), scenarios (contractual and noncontractual business settings), contexts (domestic and global), and industries (e.g., insurance, airline, retail). Furthermore, the widespread positive impact of implementing CVT on multiple constituents of the market such as the firms, the customers, the employees, the society, and the environment is also identified. Valuing Assets: A Contextual BackgroundWhat is an asset? Despite the conflicting viewpoints on the role and constituents of an asset or firm resource (Mahoney and Pandian 1992), an asset can be broadly defined as any physical, organizational, or human attribute that enables the firm to generate and implement strategies that improve its efficiency and effectiveness in the marketplace (Barney 1991). Given the prominence of assets in deploying firm strategies and gaining competitive advantage, the next question is, What makes them valuable? Here too, scholars from diverse fields such as finance, industrial organization, management, and economics have presented several approaches to decode an asset’s value. With specific reference to marketing, academics and practitioners consider customers as assets and have instated strategies that manage and nurture customers rather than use them only for specific marketing actions. In this regard, studies have investigated customer asset valuation from various viewpoints such as firm valuation (Gupta, Lehmann, and Stuart 2004; Kumar and Shah 2009), customer management (Berger et al. 2002), and financial performance (Hogan et al. 2002), among others. The differing approaches to asset valuation notwithstanding, the true value of an asset is most often observed in its interactions in the external marketplace. This leads to the next question: How are assets valued? To better understand the approach to value assets, consider two scenarios—an investor investing in stocks, and a firm investing in its customers.In the case of the investor (which applies to both individual and institutional investors), the valuation of stocks forms a crucial component. Drawing on the valuation, the investor typically performs three routine actions: (s)he ( 1) selects and invests in stocks that show potential for growth, ( 2) constructs a portfolio of stocks and bonds, and ( 3) constantly rebalances the portfolio to ensure maximum future gains. An important feature common to all these actions is the element of risk. This is because risk affects the volatility and vulnerability of cash flows, which, in turn, affect the stock value and ultimately reflect in the overall firm value.3 As a result, investors aiming to avoid risk ideally should ( 1) invest in stocks that indicate a steady stream of cash flow, ( 2) construct a portfolio of similar stocks so that their overall future value is secured and maximized, and ( 3) constantly evaluate the earnings potential of the stocks in their portfolio and reconfigure the portfolio by selling risky stocks and buying robust stocks.To assist investors in the stock valuation process, several approaches, such as the discounted cash flow models, capital asset pricing models, and the arbitrage pricing theory, among others, are available. The resulting stock valuation would then inform investors about the configuration of the portfolio using approaches such as the modern portfolio theory (Elton et al. 2014).In the case of the firm investing in its customers, the valuation of customers also forms a crucial component. If the principles of the stock valuation approach were applied to manage investments in customers, firms ideally should perform the following three actions: ( 1) identify and invest in the “right” customers, ( 2) form a customer portfolio (or customer base) consisting of favorable customers, and ( 3) constantly reevaluate the portfolio to ensure that the firm is maximizing its future gains. In reality, performing each of these three actions is not always possible because of certain challenges.The comparison between the case of the investor and the firm shows the similarity in how they manage their respective assets (stocks vs. customers). So, can the principles that guide the valuation of stocks be applied in valuing customers? To answer this question, the challenges of valuing customers must first be understood.First, firms need a reliable method to identify and invest in the “right” customers. While it may seem straightforward to say that the “right” customers are the ones who exhibit the highest value potential, the specifics may not be so apparent. Traditionally, value measures have focused on repeat purchases, acquisition cost, retention cost, tenure, and share of wallet, among others. Thus, a valuation approach that can accurately capture these intricacies for firms to identify the most valuable customers is essential.Second, configuring a portfolio of the most valuable customers is easier said than done. The customer characteristics (e.g., consumption pattern, lifestyle habits) that determine their value potential have been found to change over time (Kumar 2013). As a result, periodic evaluation of customer value measures (e.g., profitability) is a necessary and reliable method to help managers in the vital job of keeping track of changing customer characteristics. Furthermore, regulated monopolies such as telephone and municipality services are often required to cross-subsidize one group of customers with another (e.g., rural and urban customers). In such cases, firms may not be able to build the ideal portfolio of customers.Third, rebalancing of customers is not a feasible strategy. Although cases have been reported in which companies have fired customers because profitability concerns (Reardon 2007), this is not a common practice. For instance, many banks are unable to “fire” the unprofitable customers, especially if they are from lower socioeconomic groups or minority groups. In addition, churn is a challenge that firms constantly face. While a firm may want to have profitable customers, holding on to those customers typically poses challenges for the firm. Therefore, firms need a reliable way to discern how to manage unprofitable customers as well as ways to nurture profitable customers.Therefore, in this study, I present the case for a valuation approach that is specifically designed to value customer assets. In doing so, I demonstrate why the stock valuation approach is not readily applicable to valuing customer assets and show how customer assets are uniquely positioned to provide value to the firm.TABLE: TABLE 1 Summary of Select CLV Modeling ApproachesTABLE: TABLE 1 Summary of Select CLV Modeling Approaches   Theoretical Approach to Valuing AssetsThe theoretical underpinnings of how an investor values stocks and a firm values its customers can be understood through the following questions:• How do firms view customer assets?• Why are financial theories inappropriate for valuing customers?• How does customer valuation work?How Do Firms View Customer Assets?Research on customer asset management dates to models that explored how consumers make purchase decisions (Howard and Sheth 1969). The research insights generated since then have led to the consideration of customers as integral to organizations (Gupta and Lehmann 2005; Shah et al. 2006). In this regard, studies have considered customers to be intangible assets of a firm (Hunt and Morgan 1995; Srivastava, Shervani, and Fahey 1998) and have proposed approaches to valuing and managing their contributions to the firm (Bolton, Lemon, and Verhoef 2004; Reinartz and Kumar 2000).Studies have also focused on applying customer value to enhance firm performance from various perspectives such as the role of customer acquisition strategies (Lewis 2006), customer retention strategies (Reinartz, Thomas, and Kumar 2005), customer loyalty (Reinartz and Kumar 2002), customer satisfaction (Anderson and Mittal 2000), resource allocation (Petersen and Kumar 2015), and customer metrics (Petersen et al. 2009; Srinivasan and Hanssens 2009), among others. Such attempts continue to shape profitable customer management (based on future customer profitability) for both contractual and noncontractual business settings.Research on customer value has also explored the volatility and uncertainty of future revenue contributions. In this regard, studies have identified that certain behavioral drivers on the part of customers (e.g., level of purchases, product return behavior) determine the level and volatility of cash flows (Kumar, Shah, and Venkatesan 2006; Reinartz and Kumar 2003). Therefore, the management of behavioral drivers is critical in valuing customers.Why Are Financial Theories Inappropriate for Valuing Customers?Prior studies have investigated the application of financial theories to marketing decisions. For instance, Cardozo and Smith (1983) proposed an approach for making product portfolio decisions by applying the financial portfolio theory. However, Devinney, Stewart, and Shocker (1985) highlighted critical differences regarding applying a financial theory in a product decision setting. Similarly, Tarasi et al. (2011) demonstrated the application of financial portfolio theory for making customer portfolio decisions, which subsequently attracted critical review in the literature (Billett 2011; Selnes 2011). Other efforts that have incorporated concepts from financial theories into marketing applications include the introduction of customer beta that measures the riskiness of customers (Dhar and Glazer 2003), a customer relationship scorecard based on customers’ risk–return characteristics (Ryals 2003), and the management of customer segments using portfolio theory (Bolton and Tarasi 2015; Buhl and Heinrich 2008; Groening et al. 2014). In light of these efforts to apply financial theories for valuing customers, some principal differences between finance and marketing must be noted. Figure 1 illustrates these differences as well as how they ultimately affect firm value. They are as follows:• It is possible to invest money into stocks and achieve a higher amount in return. However, this is not possible in the case of customers. Firms know that beyond a certain point, investing more money in their customers will yield a lower rate of return. In other words, whereas the investment-to-earnings relationship can be linear in the case of stocks, it is nonlinear in the case of customers.• Investors can be fairly certain about a stock’s “life expectancy” and the survival of that firm. However, firms that invest in customers can make no such conclusion about their customers. In other words, investors have relatively more information about how long the stocks in which they have invested will remain in trading, compared with firms’ knowledge of how long their customers will remain their customers.• In the case of widely held stocks (i.e., those that are not closely controlled by investors and fund owners), if a stock begins to perform poorly, the investor has the option of quickly divesting that stock. After divesting, the investor has the option of either buying another stock or just holding on to the cash. However, firms typically do not have prior information about how much value a customer is going to bring in. Furthermore, changes in customer lifestyles may make customers less profitable or may even cause losses. In such a case, the firm typically does not “divest” of such loss-making customers but has to find ways to manage them appropriately. This is also true in certain business settings (e.g., oil industry) in which it is difficult for suppliers to exit a customer relationship. In some cases, such situations can lead to speculative business practices such as stockpiling. In other words, the value and volume of investments in stocks is easily scalable, compared with the investments in customers.• Investors routinely buy and sell stocks that will increase the value to the portfolio and/or minimize the risk of losing value.However, firms that invest in customers do not have the luxury of hiring and firing customers. As a result, a low-value customer is still likely to be part of a firm’s customer portfolio, and firms will have to find ways to manage these customers in such a manner that they do not lose firm value. In other words, rebalancing a stock portfolio is easier than rebalancing a customer portfolio.• Although investors value their investments in widely held stocks in line with the projected cash flows of the respective stocks,4 firms that have invested in their customers assess the value of their customers on the basis of customer contributions to firm revenue, which determines the stock price and, ultimately, the firm’s value. Recognizing this chain of impact is important in developing a metric that can effectively track the firm’s value creation. In other words, whereas the impact of investing in customers can be observed on the value of stocks (and, ultimately, on firm value), investing in widely held stocks has a limited observable impact on the value of customers.• Using financial theories, it is possible for investors to identify the types of risks to which stocks are exposed (e.g., given the economic cycles) and recognize the ones that can be diversified from the ones that cannot. However, firms that have invested in their customers cannot readily identify the risks from customer contributions, or their impact on profitability. In other words, it is relatively easier to identify (and therefore manage) risks arising from investments in stocks compared with investments in customers.• Investor sentiments play an important role in investment decisions (Weber and Johnson 2009). Stock market operations involve investor sentiments regarding a firm’s future performance expectations, which, in turn, determine the level of attractiveness of that firm in the industry. In contrast, the influence of investor sentiment is not a significant force when valuing a customer’s direct contribution. However, for customers who are based in politically unstable regions, the valuation is different more for sentimental reasons than for economic reasons. In other words, the importance of investor sentiment is higher in the valuation of stocks than in the valuation of customers.• Speculation also plays an important role in investment decisions. For instance, speculation is based on a rational betting decision that is known to stabilize asset prices (Friedman 1953) and is sometimes based on insider trading (Kyle 1985). Furthermore, it is known that if the investment actions of rational speculators trigger the buying of securities when prices rise and selling when prices fall, then an increase in the number of forward-looking speculators can increase volatility about the asset fundamentals (De Long et al. 1990). In contrast, speculation does not play a major role when assessing the value of a customer. In other words, the importance of speculation is higher in the valuation of stocks than in the valuation of customers.• Drawing on known future discounted cash flows, an investor typically decides on his or her choice of investment. That is, the options are limited to either investment or divestment. As a result, investors generally cannot influence the future cash flow patterns of a firm to change the course of their own actions. In other words, financial theories offer a passive approach to managing investments, whereas customer management requires an active management approach.• The volatility and vulnerability of stocks make it difficult for investors to predict stock returns in the short run. Short-term returns are difficult to predict because of their random walk feature (Jensen 1978; Malkiel 1995). In addition, Kumar, Ramaswami, and Srivastava (2000) observe that daily returns are sensitive to random disturbances in the market. To predict stock movements, they offset the effect of random disturbances by considering a longer time period such as a month. Using the principles advanced by the capital asset pricing model and the arbitrage pricing theory, the study developed a multistage model to study variation in stock returns. The study found that the addition of significant factors other than the market factors (i.e., cost and supply of money) increases the level of risk, which results in a decrease in the price of the firm. In this regard, appropriately designed marketing actions directed at the environmental uncertainties (i.e., macroeconomic factors) can lessen the impact on the firm’s cash flows, because predictions of customer value are accurate in the short run. Therefore, estimating the value of customers and pairing this value with appropriately designed marketing strategies can place firms on the path to increased financial returns. The level of predictive accuracy of customer value, however, declines only in the long run. In other words, a more accurate prediction of stock returns is possible only in the long run, compared with the prediction of customer value, which is more accurate in the short run.In addition to the financial theories, knowledge from behavioral finance is also relevant here. Behavioral finance literature has argued that some financial phenomena can reasonably be understood using models in which some agents are not fully rational (Barberis and Thaler 2003). With specific reference to investor behavior, behavioral finance has explained how certain investor groups behave, the types of portfolios investors hold, and investors’ trading patterns over time. Barberis and Thaler (2003) trace investor behaviors such as having insufficient diversification of portfolios (Baxter and Jermann 1997), opting for simpler diversification strategies (Benartzi and Thaler 2001), having high trading volumes (Barber and Odean 2000), holding on to stocks even if they are decreasing in value (Odean 1998), and considering “attention-getting” stocks for their purchase decisions (Barber and Odean 1999) to demonstrate that investors are not always rational in their investment decisions. This stream of literature has established that investors’ attitudes and behaviors are of vital importance. In light of the aforementioned differences between customers and stocks, I develop the CVT as a robust theory for valuing customer assets by adopting a different approach than that used to value stocks.How Does Customer Valuation Work?From the previous discussion of the determinants of customer assets, one of the approaches to value customers in a general form is presented next:CFPi = f (Transaction behaviori, Marketing costi, Demographic=firmographic variablesi, Economic and environmental factors ), where CFPi refers to customer future profitability of customer i.Simply put, the future profitability of the customers depends on their past and current transaction behaviors, the marketing efforts of the firm, the identity and profile of the customers (i.e., demographic variables), and the environment in which these customers exist (i.e., economic and environmental factors). Furthermore, when modeling CFP, statistical issues such as heterogeneity, endogeneity, and simultaneity are factored in the estimation of such models. Once the CFP is modeled, business intelligence software systems can be used to score CFP, update customer information, and rescore CFP on a periodic basis. Technology can also be incorporated to target customers on a real-time basis through relevant messages in an effort to increase future cash flows. Drawing on this valuation approach, I advance the following testable propositions.Transaction behavior. Also known as exchange characteristics, the transaction behavior broadly includes all the past and current transaction variables that affect and influence the customer–firm relationship. The commitment-trust theory (Morgan and Hunt 1994) proposes that firms try to establish positive relationships with customers by developing commitment and trust with them. While a customer’s positive affect influences his or her commitment to the firm, research has also uncovered other dimensions of customer commitment. For instance, Allen and Meyer (1990) proposed a three-component model of commitment (affective, calculative, and normative). Recently, Keiningham et al. (2015) proposed customer commitment as a five-dimension construct (affective, normative, economic, forced, and habitual). Beyond repeat purchases, purchase habits have been found to play an important role in determining customer transaction behavior (Ascarza, Iyengar, and Schleicher 2016; Duhigg 2013). Specifically, research has reported that customer habits influence the future volatility and vulnerability of cash flows (Shah et al. 2017) and firm performance (Shah, Kumar, and Kim 2014). Furthermore, the importance of primary market research in understanding customer behavior patterns has also been highlighted, especially in light of the abundance of behavioral information made possible by big data (Knowledge@ Wharton 2014). As a result, frequent customer–firm interactions should increase customer trust and commitment in the firm at a faster rate, provided that the interactions are satisfactory. Therefore,P1: Customer transaction activities significantly influence customer future profitability.This proposition has been tested across various industries and markets (business to business [B2B] and business to customer [B2C]), and research has found that customer future profitability is positively influenced by a host of variables, including ( 1) prior customer spending level (Reinartz and Kumar 2000), ( 2) customer cross-buying behavior (Kumar, George, and Pancras 2008; Venkatesan and Kumar 2004), ( 3) the intensity of customer purchases within a product category (Reinartz and Kumar 2003), and ( 4) members of rewards programs with the firm (Kumar, Shah, and Venkatesan 2006). In addition, studies have also found the average interpurchase time and the number of product returns to have a significantly positive impact on customer future profitability, up to a certain threshold (Reinartz and Kumar 2003).Marketing cost. Marketing cost can include past, current, and future promotional costs (toward customer acquisition, retention, and win-back); technology upgrades; service improvements; employee management; and quality control. The importance of effective management of customer assets to enhance firm profitability (Bolton, Lemon, and Verhoef 2004) has directed research attention toward understanding the impact of marketing expenditures on customer value and actively using marketing communication actions (i.e., customer contact channels) to maximize customer value. Prior research has established that well-timed communication efforts (Kumar, Venkatesan, and Reinartz 2008) and well-managed content (Kim and Kumar 2018) between firms and customers reduces the propensity of a customer to quit a relationship (Morgan and Hunt 1994). However, too much communication has also been found to be detrimental to the relationship (Fournier, Dobscha, and Mick 1998), thereby indicating the presence of an optimal communication level.P2: Marketing cost nonlinearly influences customer future profitability.Studies have revealed an inverted U-shaped relationship between marketing contacts (involving rich modes [e.g., sales personnel contact] and standardized modes [e.g., telephone, direct mail]) and customer behavior (Reinartz, Thomas, and Kumar 2005; Venkatesan, Kumar, and Bohling 2007). Furthermore, in a permission-based marketing context, a firm’s marketing contact policy influences both the length of time a customer stays in an email program and the average amount a customer spends on a transaction while (s)he is subscribed to the email program; too much marketing may not only make customers less likely to opt in but also cause them to opt out more quickly (Kumar, Zhang, and Luo 2014).Demographic/firmographic variables. Demographic/firmographic variables refer to the distinguishing characteristics of the customer (end user or business customer). In the case of a business customer, the firmographic variables include the type of industry, the age and size of the firm, the level of annual revenue, and the location of the business. In the case of an end user, the demographic variables include age, gender, income, and the physical location of the customers. The demographic/ firmographic variables can aid firms in characterizing attractive segments into identifiable and measurable groups of customers (Zeithaml 2000). In the case of end users, the heterogeneity in profit contributions can be better understood through a customer’s demographic and psychographic variables (Chintagunta, Jain, and Vilcassim 1991); demographic variables affect customer store choice (Craig, Ghosh, and McLafferty 1984), shopping channel choice (Inman, Shankar, and Ferraro 2004), profitable lifetime duration (Reinartz and Kumar 2003), and migration of shopping choice (Thomas and Sullivan 2005), to name a few. As a result, classifying customers on the basis of their distinguishing characteristics can help firms in their customer segmentation and customer relationship management efforts. Therefore,P3: Demographic/firmographic variables significantly influence customer future profitability.The testing of this proposition has revealed that demographic variables such as household income, population density of the neighborhood, gender, age of the head of the household, marital status, education level, and so forth significantly affect future customer profitability (Kumar, George, and Pancras 2008; Kumar, Shah, and Venkatesan 2006). Furthermore, research has also established that firmographic variables such as the size of the firm and the industry category of the business customers significantly explained the variation in contribution margin for the focal firm (Venkatesan and Kumar 2004). Specifically, Venkatesan and Kumar (2004) found that the focal firm’s business customers in the financial services, technology, consumer packaged goods, and government industry categories provided, on average, a higher contribution margin than firms in other industry categories.Economic and environmental factors. Economic factors such as gross domestic product (GDP) per capita help determine the consumption pattern of a country. It has been established that consumers’ response to macroeconomic factors is a function of not just their ability to buy (as measured by current and expected future income), but also their willingness to buy (Katona 1975). This underlying theory explains how various macroeconomic conditions affect price changes (Gordon, Avi, and Yang 2013), changes in consumers’ frame of mind (Chhaochharia, Korniotis, and Kumar 2011), and overall household utility (Kamakura and Du 2012), to name a few. Furthermore, research has also shown that changes in consumers’ economic constraints have varying effects on their profit-contributing potential. For instance, Sunder, Kumar, and Zhao (2016) demonstrated that high-CLV customers are least affected by changes in their budgetary constraints when compared with low-CLV customers. Therefore, if a country has a high GDP and high purchasing power, its consumers will have more disposable income and spend more.P4: Economic and environmental factors significantly influence a customer’s future profitability.Studies that have tested this proposition have found that GDP per capita, the country’s economic well-being (how customers feel about and experience their daily lives), cultural characteristics, and the employment rate significantly influence future customer profitability (Kumar and Pansari 2016b; Kumar et al. 2014; Umashankar, Bhagwat, and Kumar 2016). Customer Valuation TheoryIn line with the customer valuation approach discussed previously, the CVT can be defined as a mechanism to measure the future value of each customer on the basis of ( 1) the customer’s direct economic value contribution, ( 2) the depth of the direct economic value contribution, and ( 3) the breadth of the indirect economic value contribution by accounting for volatility and vulnerability of customer cash flows. The key components of this definition include:• Direct economic value contribution: This refers to the economic value of the customer relationship to the firm, expressed as a contribution margin or net profit. A firm can both measure and optimize its marketing efforts by incorporating customer value at the core of its decision-making process. When implemented at firms, it aids in ( 1) computing a customer’s future profitability, ( 2) arriving at a good measure of customer value, ( 3) optimally allocating marketing resources to maximize customer value, and ( 4) identifying ways to maximize the return on marketing investments.• Depth of direct economic value contribution: This refers to the intensity and inclusiveness of customers’ direct value contributions to the firm through their own purchases that have produced significant financial results for the implementing firms. Examples of such instances include the acquisition and retention of profitable customers on the basis of their future value potential, customer purchase potential across multiple channels of buying, and the possibility of customers to buy across multiple product categories.• Breadth of the indirect economic value contribution: This refers to customers’ indirect value contributions to the firm through their referral behavior, their online influence on prospects’ and other customers’ purchases, and their feedback on the firm offerings. These indirect measures also contribute significantly to a firm’s cash flow. This indirect contribution can also be extended to accommodate other contexts such as salesperson productivity, donations (in the case of nonprofit firms), and business references.Because CVT enables managers to actively manage customer relationships on the basis of future customer contributions through specialized customer strategies, it creates a positive impact on firm performance. Specifically, the implementation of the CVT can help firms improve their marketing productivity and realize higher firm value through ( 1) valuing customers as assets, ( 2) managing a portfolio of customers, and ( 3) nurturing profitable customers. Therefore, it is critical to understand the nature of the linkage between CVT and firm value. Figure 2 provides an overview of how the components of CVT function in driving firm value.In summary, the proposed CVT is relevant for the following reasons:• Unlike prior marketing applications of the financial portfolio theory, this theory focuses on customer management at the individual customer level (on the basis of his or her profitability) instead of the customer segment level. Although customers are grouped in line with their profitability (i.e., high-, medium-, and low-profit segments), the subsequent strategies that are developed are always deployed at an individual customer level because this is more effective. Such an approach is different from how investors handle financial assets. The CVT implicitly accounts for customer risks (i.e., volatility and vulnerability in cash flows) when modeling future customer profitability. In doing so, the model focuses on how the risks ultimately affect customer profitability (and thereby, overall firm profitability) and treats it accordingly. For this reason, the CVT does not explicitly provide a beta value or any indicator for the risk-free return equivalent in the case of investments. Furthermore, the CVT enables managers to identify and manage the diversifiable risks through tailored offerings by focusing on the drivers of customer value.• Because the CVT focuses on customer profitability, the variation in associated customer costs at the individual level are accounted for.• The CVT is highly active in terms of generating actionable firm strategies. By dynamically managing the volatility and vulnerability in cash flows, this theory enables managers to actively monitor customer relationships over time and undertake necessary remedial measures. In comparison, the financial theories largely suggest that investors invest (or divest) at any point in time as determined by the discounted cash flow analyses. As a result, the financial theories remain passive by not providing adequate directions to investors such that they can influence future changes.• Because future costs drive future margins, the CVT-based strategies advise managers to better manage their acquisition and retention of profitable customers; by doing so, firms can actively refine and manage the customer valuation approach. However, no such option is available with investing in stocks because stocks yield fixed returns, and the investor cannot influence or manage the return on a stock.• The CVT recognizes the investment-to-earnings relationship, in the case of customers, to be nonlinear (unlike whenn valuing stocks), and it uses an appropriate customer valuation approach. This is subsequently reflected in customer strategies that can be developed. Valuing Customers as Assets (Concepts)The next component of the CVT delves into the concepts behind customer valuation and the related financial benefits they hold for firms. To contextualize the practice of customer valuation to the proposed theory, it is essential to review prior literature on this topic. In recent years, the idea of treating customers as assets of a firm has emerged as the most popular and efficient way of doing business (Hunt and Morgan 1995). This entails identifying future customer profitability and designing marketing guidelines that will advise managers on profitable customer management. Traditionally, firms have used several metrics to value customers (Kumar and Reinartz 2012; Petersen et al. 2009). The guidance from these metrics has driven decisions pertaining to the allocation of marketing resources.When considering a customer’s value contribution to the firm, a crucial part is his or her contribution in the future periods. It is this future component that is of immense interest to academics and practitioners. The concept of future value contribution has been conceptualized in the form of the CLV metric, which refers to the present value of future profits generated from a customer over his or her lifetime with the firm (Gupta and Lehmann 2005; Venkatesan and Kumar 2004).The conceptualization of CLV was strongly influenced by the corporate finance body of knowledge, and specifically to the contribution of the present value concept by Irving Fisher (1965 [1930]) and Eugene Fama’s asset pricing theory (Fama and Miller 1972). Applying this knowledge to customer asset management, it is evident that customers pose risks in terms of generating returns for the firm in the future. However, the impact of these risks on customer profitability is not uniform across all customers. In this regard, the CLV approach identifies opportunities to contain the variation in returns (also known as cash flow volatility) and, thus, the total risk of changes in the value of the firm (measured by future returns) (Shah et al. 2017). This is possible by understanding the drivers of customer profitability and their impact on CLV.While the inspiration from other sciences such as economics and finance is apparent, the adaptation and inclusion into the customer asset valuation would not have been possible without the development of new methodologies specific to the marketing milieu. Specifically, the conceptualization of the CLV metric and the development of substantive methodologies served as a stepping stone for managing profitability by selecting the right customers for targeting and determining the allocation of resources for customer acquisition, retention, and growth. Furthermore, this important contribution also demonstrated that the CLV framework can help firms manage risk through appropriate actions directed at the individual customers.Direct Economic Value ContributionWhile the valuation of assets/projects is typically done at the aggregate level, the CLV metric enables firms to capture the direct value contribution of customers at both the aggregate and the individual levels. At the aggregate level, the average lifetime value of a customer is derived from the lifetime value of a cohort, segment, or even a firm. Here, the estimation of CLV can be accomplished by identifying and measuring the factors that drive CLV. At the individual level, the CLV is calculated as the sum of cumulated cash flows—discounted using the weighted average cost of capital—of a customer over his or her entire lifetime with the company (Kumar 2008). It is a function of the predicted contribution margin, the propensity for a customer to continue in the relationship, and the marketing resources allocated to the customer. It is important to note that weighted average cost of capital is one of the measures that represent the discount factor used to compute CLV, among other options (e.g., T-bills rate). In its general form, CLV can be expressed as follows (Venkatesan and Kumar 2004):where i is the customer index, t is the time index, T refers to the number of time periods considered for estimating CLV, and d is the discount rate.5 The CVT proposed in this study is based on the individual-level CLV rather than the aggregate level.Although a “true” CLV measure implies measuring the customer’s value over his or her lifetime, for most applications it is a three-year window.6 The reasons for this time frame are as follows: ( 1) because future cash flows are heavily discounted, a significant portion of profit can be accounted for in the first three years, and the contributions in the following years are very small; ( 2) the predictive accuracy of the models decline over a longer time frame; ( 3) changes in customer needs and life cycle are likely to change significantly beyond a three-year window;( 4) product offerings change in response to technological advancements and customer needs; and ( 5) CLV predictions are updated on the basis of a rolling time horizon to accommodate changes in other environmental factors. However, specific industry trends do lead to some exceptions for this three-year window. For instance, automakers can expect customers to make a purchase every four to six years (we suggest using a longer window to accommodate at least a couple of purchases or using purchase intention measures to forecast future value), computer manufacturers can expect customers to make a purchase every one to two years (we suggest a four- to five-year window to account for at least two or three purchases); and insurance companies can take up to seven years to recover the acquisition costs. Exceptions aside, the aforementioned reasons advocate the use of a three-year timeframe in computing the CLV.The extant CLV literature has covered a wide range of business conditions through the measurement approaches. This coverage has expanded the scope and application of CLV-based models for a multitude of industries and markets. Some of the later developments in modeling CLV have accommodated several modeling challenges, which has allowed for more sophisticated and precise estimation of customer value. Kumar and Reinartz (2016) provide a detailed review of select approaches that have made significant contributions in modeling CLV. Table 1 offers a summary of the model form as well as merits and shortcomings of popular modeling approaches.Although the models discussed here are the most popular ones, there will always be improvements to the CLV model because of the nature of the availability of customer data and the business situation. In addition, the knowledge of how to implement the models is also important in determining how CLV can be managed at different firms.Depth of Direct Economic Value ContributionThe depth of direct economic value contributions, as measured by CLV, have focused on the impact of customers’ direct profit contributions to the firm through their own purchases. Tracking and valuing these contributions have produced significant financial results for the implementing firms. In this regard, recent research has demonstrated that when firms try to understand and leverage the true power of measuring and maximizing CLV, it ultimately results in enhanced firm value (Berger et al. 2002; Kumar, Ramaswami, and Srivastava 2000; Kumar and Shah 2009). Furthermore, Kumar (2008, 2013) explored the implications and generated the “Wheel of Fortune” strategies that have enabled firms to address marketing issues with greater confidence and ensure better decision making. This set of strategies answers the following questions: Customer selection: How do firms identify the “right” customers to manage? Reinartz and Kumar (2000) found that long-life customers are not necessarily profitable customers, and the authors call for the use of a forward-looking metric such as CLV to identify the “right” customers to manage. Managing repeat purchases and profitability simultaneously: How can firms ensure profitability while improving customer repeat purchases? When customized actions were implemented at a B2C catalog retailer on the basis of segmenting customers on repeat purchases and profitability, Reinartz and Kumar (2002) found that loyal customers are aware of their value to the company and demand premium service, believe they deserve lower prices, and spread positive word of mouth only if they feel and act loyal. Optimal allocation of resources: Which customers should the firms interact with through inexpensive channels (e.g., Internet, phone), and which customers should firms let go? When resources were reallocated on the basis of the optimal mix and frequency of communication channels, a B2B company realized 100% more revenue and 83% more profits across its four customer segments (Venkatesan and Kumar 2004). Cross-buy: How can customers’ purchases be increased across multiple product categories to improve customer profitability? By encouraging customers to buy across more product categories through profitable customer management strategies, Kumar, George, and Pancras (2008) found that metrics such as revenue per order, margin per order, revenue per month, margin per month, and orders per month increased as customers shopped across multiple product categories. However, not all cross-buying is good. Shah et al. (2012) found that across five B2B and B2C firms, 10%–35% of the firms’ customers who cross-buy are unprofitable and account for a significant proportion (39%–88%) of the firms’ total loss from its customers. Therefore, discerning profitable from unprofitable cross-buying behavior is essential. Next logical product: How do firms decide the timing of an offering to a customer? When done right, results have shown that firms increased their profits by an average of $1,600 per customer, representing an increase in return on investment (ROI) of 160% (Kumar, Venkatesan, and Reinartz 2006). Preventing customer attrition: How do firms decide which prospect will make a better customer in the future and is therefore worthwhile to acquire? Using test and control groups, Reinartz, Thomas, and Kumar (2005) showed that acquiring and retaining the “right” customers garnered a B2C firm an incremental profit of $345,800 with an ROI close to 860%. Product returns: Should the firm encourage or discourage product return behavior, and how should it manage this process? Petersen and Kumar (2009) found that the ideal level of product returns should be one that maximizes firm profits. For a B2C catalog retailer, they found that the optimal percentage of product returns that maximized profitability was 13%. Furthermore, Petersen and Kumar (2015) addressed the aspects of perceived risk and optimal resource allocation into the product returns process for a B2C catalog retailer and found that the firm was able to generate approximately $300,000 more in profits compared with the next best available resource allocation strategy. Managing multichannel shoppers: What kind of sales and service resources should firms allocate to current customers to conduct future business with them? Kumar and Venkatesan (2005) identified the drivers of profitable multichannel shopping behavior and found that adding one more channel resulted in an average net gain of about 80% in profits. Branding and customer profitability: Should firms invest in building brands or customers? Kumar, Luo, and Rao (2016) have shown that by understanding the link between investments in branding and CLV, firms can efficiently allocate their resources to improving customer brand value to generate maximum lifetime value. It was found that a 5% increase in the investments in branding causes the CLV to increase by over 25%. Acquiring profitable customers: How should firms monitor customer activity to readjust the form and intensity of their marketing initiatives? In managing firm actions regarding customer acquisition and retention efforts, Reinartz, Thomas, and Kumar (2005) found that it is not sufficient to consider how much to spend on acquisition or retention alone but, instead, firms need to consider how they must balance acquisition and retention spending together to maximize profitability and double the ROI. Interaction orientation: Ramani and Kumar (2008) ask, Should firms realign themselves to realize augmented CLV? If so, is interaction orientation the answer? Referral marketing strategy: How can firms enhance their value through customers’ referral behavior? By implementing customized campaigns for each customer value segment, B2B and B2C firms have realized large profit gains, representing a higher ROI (Kumar, Petersen, and Leone 2007, 2010, 2013). Linking CLV to shareholder value: How do firms leverage the CLV metric to drive their stock price and provide more value to their stakeholders? By linking CLV-based actions to a firm’s stock price, B2B and B2C firms have reported significant increases of approximately 35% and 57%, respectively, in their stock prices; better prediction of stock price movements; and superior performance with respect to the stock market index and rival firms (Krasnikov, Jayachandran, and Kumar 2009; Kumar and Shah 2009). products and services they consume. When firms pursue opportunities to draw out indirect profit contributions from customers, it implies engaging with customers by identifying the various sources of profit. Such a focus would result in maximizing customer engagement value. Conceptually, customer engagement value is the total value provided by customers who value the brand such that they engage with the firms ( 1) through their purchase transactions (or CLV), ( 2) through their ability to refer other customers to the firm using the firm’s referral program (or customer referral value [CRV]), ( 3) through their power to positively influence other customers about the firm’s offerings on social media (or customer influence value [CIV]), and ( 4) by providing feedback to the firm for product and service ideas (or customer knowledge value [CKV]) (Kumar et al. 2010).CRV. With respect to indirect customer contributions to profit, promoting customer referrals is a popular practice adopted by firms. The CRV metric captures the net present value (NPV) of the future profits of new customers who purchased the firm offerings as a result of the referral behavior of the current customer (Kumar, Petersen, and Leone 2010). Kumar, Petersen, and Leone (2007) showed that when targeted referral behavior campaigns were offered to select customers of a telecommunications firm, it resulted in overall value gains of $486,090, representing an ROI of 15.4. This study established that customers who score highly on CLV are not the same as those who are successful at referring new customers, and it made the case for measuring both CLV and CRV when evaluating marketing campaigns. To compute CRV, the first step is for firms to integrate their customer transaction database with the referral database. In the absence of referral behavior information, firms can collect it from new customers by asking questions such as, “Were you referred, and if so, by whom?” Or, “To what degree did the referral influence your decision to transact with us?” The CRV metric is, however, not relevant for all business situations. For instance, customersmay not make referralsif they are not attached to the product (e.g., fast-moving consumer goods). Furthermore, a customer recommendation for one product does not necessarily apply to the firm’s portfolio of products.The concept of referral behavior has also been extended to apply to the B2B relationship setting through the business reference value (BRV) metric. This metric computes the amount of profit the existing client firm can help generate from the prospect firms that purchase the firm offerings as a result of the client reference. Significant differences were found between high-BRV and low-BRV clients of a telecommunications and financial services firm, which indicated that, compared with low-BRV clients, high-BRV clients ( 1) contribute more value, ( 2) stay longer with the firm, ( 3) are more likely to provide a video reference than a “call me” reference, and ( 4) are larger in size and annual revenue (Kumar, Petersen, and Leone 2013). By linking CRV and BRV to CLV, firms can begin to identify the value of customers and enhance it through optimally designed marketing campaigns.CIV. The power of the online medium has influenced customers to ( 1) persuade and convert others into customers, ( 2) continually use the firm’s offerings, and ( 3) change/modify their own purchase patterns. In an effort to conceptualize and metricize the customer influence on others, research has contributed two key metrics—customer influence effect (CIE) and CIV. Whereas the CIE measures the net spread and influence of a message from a particular individual, the CIV calculates the monetary gain or loss a firm experiences that is attributable to a customer, through his or her spread of positive or negative influence. Tracking these two metrics for a company’s social media campaign, Kumar et al. (2013) were able to demonstrate a 49% increase in brand awareness, 83% increase in ROI, and 40% increase in sales revenue growth rate. Although this study described the applicability of the CIE and CIV metrics in the case of an offline retailer, it can be extended to online retailers as well. Companies such as Starbucks and Staples already have established customer relationship management practices alongside a vibrant social media presence.CKV. Because customer input is a valuable resource in the product development process, the value of this contribution needs to be captured and included as part of a customer’s value to the firm. This was captured in the conceptualization of the CKV metric, which refers to the monetary value attributed to a customer by a firm as a result of the profit generated by implementing an idea/suggestion/feedback from that customer. This customer feedback not only identifies the areas that are in need of improvement but also helps provide suggestions and solutions for future upgrades and modifications to firm offerings. This feedback has the potential to make the entire offering more attractive to existing and potential customers as well as improve process efficiencies. Customers must be attributed to the corresponding feedback to receive credit toward their asset value (Kumar and Bhagwat 2010).While the value of customer feedback can be substantial to any firm, juxtaposing CLV with CKV can yield even greater insights to firms. Normally, customers with low CLVs have little experience with the product and/or they are likely to be unenthusiastic about the firm and therefore are likely to provide little feedback to the firm. Consequently, the higher a customer’s CLV, the more positive that customer will perceive the company and its products to be, and the more opportunities for the company to receive input. However, at very high levels of CLV (an indication of a close fit between the company’s products and a customer’s needs), the customers are likely to be highly satisfied and therefore have little incentive to provide feedback. These customers can, however, be encouraged to assist less experienced and less knowledgeable customers when firms implement a communication medium to do so.Customer brand value. The three tangible value metrics—CRV, CIV, and CKV—collectively capture customers’ indirect profit contributions. In addition to these metrics, an attitudinal metric (intangible value)—customer brand value (CBV)—measures the value that the customer attaches to the brand as a result of all the marketing and communication messages delivered through different media. Conceptually, CBV refers to the total value a customer attaches to a brand through his or her experiences with the brand over time (Kumar, Luo, and Rao 2016). The CBV is a multidimensional metric that measures the customer’s brand knowledge, brand attitude, brand purchase intention, and brand behavior and enables companies to devise appropriate strategies depending on where the problem exists—awareness, trust, or repeat purchase.Monitoring all the components of CBV becomes important from a branding standpoint. That is, brand-building efforts are aimed at inducing favorable behavior outcomes toward the brand, such as longer duration, higher purchase frequency, higher contribution margin, and positive customer referrals. Such behavioral outcomes determine the CLV scores. With this understanding, managers can work on optimizing the components of CBV to improve an individual’s CLV score. In this regard, Kumar (2013) shows that CBV is the foundation for managing CLV, CRV, CIV, and CKV. Managing Customer Portfolios (Metrics)The next component of the CVT relates to identifying the metrics to ascertain the value of customers. Firms constantly struggle with managing customers while ensuring profitability. In many cases, the cost of serving a customer may far exceed the returns from that customer. In such a scenario, firms are caught between retaining their customer portfolio/base and ensuring a healthy bottom line. The CLV metric will help firms manage healthy customer portfolios.Direct Economic Value ContributionAs mentioned previously, the CLV stream of research has uncovered several resource reallocation insights that have helped firms maximize their value. Specifically, studies have focused on the magnitude of cash flows (the expected returns as measured by CLV) and on the volatility and variability of the cash flows (the risk element associated with customer revenue contributions). This dual focus has enabled firms to better understand customer portfolio decisions and manage customers. Through this process, it is possible to select and invest in customers who will optimize the overall customer cash flow while balancing the risk in cash flow variations.While the concept of managing customer portfolios is similar to the financial portfolio theory, a couple of key differences must be noted. First, financial assets are typically classified in line with their historic risk/return characteristics. However, the same approach does not apply when treating customers as assets. Research has shown that past customer contributions are not highly correlated with future profit potential when using traditional or backward-looking metrics (Venkatesan and Kumar 2004). The CLV metric, in contrast, has been found to be a good predictor of future customer profitability. Second, finance managers typically invest in individual assets and try to maximize the portfolio’s return. However, marketing managers largely allocate resources at the customer segment level and increase customer profit. This is due to the nonlinear relationship between customer investments and returns. In other words, customer responses operate within certain thresholds of the level of marketing investments, and too few or too many marketing investments may not elicit the desired customer response. Whereas early studies have contributed to managing customer portfolios by optimizing segment-level risk/return (Buhl and Heinrich 2008; Reinartz and Kumar 2003; Tarasi et al. 2011), more recent studies are focusing on optimizing resource allocation at the individual customer level to maximize overall profitability (Kumar, Zhang, and Luo 2014; Luo and Kumar 2013; Petersen and Kumar 2015).Depth of Direct Economic Value ContributionTo realize the full potential of customer contributions, firms must focus on the augmented customer value while managing customer portfolios. This is possible through optimal allocation of resources by prioritizing customers on their augmented profitability and their receptiveness to marketing efforts. To do so, decile analysis of the predicted CLV must be performed. Here, the baseline CLV (i.e., NPV of future profits that considers only the focal product category that the customer purchases) and the incremental CLV (i.e., NPV of future profits that considers customer purchases only from new product categories) are separately tracked. Conceptually, the augmented CLV is the sum of baseline CLV and the monetary impact of the portfolio of strategies discussed previously. Furthermore, the medium-value customers experience the highest gain from the portfolio of strategies—higher than even the high-value customers. This indicates the receptiveness of customers to firm-initiated marketing actions. Following this, firms can prioritize marketing resources and actions according to the impact on future profits and on firm value.A good way to understand the impact of optimal allocation of resources is through the firm’s usage of marketing communication channels (Venkatesan, Kumar, and Bohling 2007). To optimally allocate their resources, firms must first identify their most profitable customers and those who are the most responsive to marketing efforts. Performed at the individual customer level, the selection of the best mix of communication channels is determined on the basis of the responsiveness of each customer. The cost-effectiveness of these channels is also considered to measure the customers’ potential revenue contribution based on the contacts made. The firm then decides the frequency and interval of contacts through these channels to develop a customer contact strategy. Analyzing customer behavior in relation to these factors provides firms with valuable information about their customers’ preferences and attitudes. By carefully monitoring customers’ purchase frequency, the time elapsed between purchases, and the contribution margin, managers can determine the frequency of firm initiatives to maximize CLV through an optimal contact strategy.The benefits of optimal resource allocation using CLV as the metric can be observed even in a complex business setting. When IBM adopted the CLV metric to measure customer profitability and allocate resources for customers, the firm determined the level of contact and outreach efforts through telesales, email, direct mail, and catalogs on an individual customer basis. At the conclusion of this program (based on approximately 35,000 customers), IBM was able to effectively reallocate resources for approximately 14% of customers as well as increase revenues by about US$20 million. This was done without increasing the investment but by abandoning ineffective techniques based on customer spending history in favor of the CLV metric (Kumar et al. 2008).Breadth of Indirect Economic Value ContributionIn managing customer portfolios, firms must realize that customers have multiple ways to contribute value, and all avenues have to be explored to maximize customer contributions. In this regard, the CLV metric has resulted in the creation of other metrics that can be used to manage and maximize customer value. These metrics must be managed independently, because a customer could contribute in one or more of these ways. Furthermore, firms must identify the ideal combination of direct and indirect value contribution from the metrics that provide the greatest profits to the firm. Figure 3 illustrates the metrics that embed the principles of CLV.While most of the metrics illustrated in Figure 3 have been discussed in previous sections of this study, a brief description of salesperson future value (SFV), donor lifetime value (DLV), and employee engagement value (EEV) is provided here in the following subsections.SFV. Despite numerous studies conducted on the sales function, the future value of the salesperson to the firm and how organizational factors affect it has received little attention. Recognizing the importance of this knowledge to firms, Kumar, Sunder, and Leone (2014) conceptualized the SFV metric and empirically demonstrated the short-term and long-term effects of managing SFV. The SFV is defined as the NPV of future cash flows from a salesperson’s customers (i.e., CLV) after accounting for the costs of developing, motivating, and retaining the salesperson. By measuring the value of the salespeople using the SFV metric and linking the performance with the types of training and incentives each salesperson receives, it is possible to identify the best-performing salespeople and tailor the training and incentives to maximize their performance. When this approach was implemented in a Fortune 500 B2B firm, the firm was able to reallocate training and incentive investments across salespeople, which resulted in an 8% increase in SFV across the sales force and a 4% increase in firm revenue (Kumar, Sunder, and Leone 2015).DLV. Similar to for-profit firms, nonprofit firms also need to build relationships to be able to raise donations. In this case, the relationships are created with potential donors. Furthermore, the nonprofit firm will have to be selective about the donors with whom it builds relationships, so that it judiciously uses its limited resources. To make this possible, Kumar and Petersen (2016) conceptualized the DLV metric, which refers to the sum of donations in the future years discounted to the present value over a donor’s lifetime with the firm. Using the donor demographic information, past donation behavior, and marketing information, the DLV is modeled by predicting ( 1) the likelihood of donation; ( 2) the value of donation, given that the person will donate; and ( 3) marketing costs expended in seeking donations. By computing the DLV, it is possible for nonprofit firms to rank-order donors on the basis of their future donation value to the firm. This metric can predict who will be able to donate with 90% accuracy, the value of donations with 80% accuracy, and the DLV with 75% accuracy.EEV. Firms can leverage the power of an engaged customer base better if they have a workforce that interacts well with customers. Customer–employee interactions help create perceptions about the firm, which affect repeat customer purchases (Sirianni et al. 2013). These perceptions lead to attitudinal and behavioral outcomes through their impact on purchases, referrals, influence, and knowledge that the customers provide to the firm. In this regard, a positive interaction between customers and employees is likely to motivate how customers talk about the brand and whether they recommend it to their friends and relatives. Kumar and Pansari (2014, p. 55) define employee engagement as “a multidimensional construct which comprises of all the different facets of the attitudes and behaviors of employees towards the organization.” The dimensions of employee engagement comprise employee satisfaction, employee identification, employee commitment, employee loyalty, and employee performance. Kumar and Pansari (2016a) developed an approach to measure EEV using a five-point scale. When implemented in the airline, telecommunication, and hotel industries, the authors found that the highest level of growth in profits (10%–15%) occurs when a company’s employees are highly engaged (i.e., have a high EEV); the lowest level of growth (0%–1%) occurs when the company’s employees are disengaged (i.e., have a low EEV). Ongoing research continues to uncover several facets of the metrics presented in Figure 3, and the insights generated thus far have advised firms in valuing customers. Nurturing Profitable Customers (Strategies)Finally, the concepts and metrics discussed previously lead to the establishment of strategies that will aid in growing customer value. In developing strategies, firms are often misled by the belief that loyal customers are profitable. In addition, firms’ belief that creating customer reward programs can result in increased repeat purchasing behavior, and thereby improve firm profitability, is misplaced. Research in this area has shown that loyal customers are not necessarily profitable, and the relationship between repeat purchases and profitability is more complex than is often perceived. In addition, firms regularly use traditional metrics to measure the value of their customers, and these lead managers to implement flawed marketing strategies that drain the firm’s resources. In this regard, the CLV metric is ideal for firms aiming to grow and nurture customer profitability. When firms adopt a CLV-based approach, they can make consistent decisions over time about ( 1) which customers and prospects to acquire and retain, ( 2) which customers and prospects not to acquire and retain, and ( 3) the level of resources to spend on the various customer segments.Direct Economic Value ContributionWhen aiming to maximize the direct contribution of customers to the firm, the CVT proposes that the focus be on establishing profitable customer relationships that are based on customer transactions. In this regard, Reinartz and Kumar (2002) found that ( 1) loyal customers do not cost less to serve, ( 2) loyal customers consistently paid lower prices, and ( 2) customers who were attitudinally and behaviorally loyal were more likely to be active word-of-mouth marketers than those who were only behaviorally loyal. In effect, the study found that while there may be long-standing customers who are only marginally profitable, there also may be short-term customers who are highly profitable. The identification of the distinct customer types led to the development of the following specific strategies: High repeat purchases and high profitability. Referred to as “True Friends,” these customers buy steadily and regularly over time. They are generally satisfied with the firm and are usually comfortable engaging with the firm’s processes. Firms should build relationships with these customers because they present the highest potential for long-term profitability. Low repeat purchases and high profitability. Referred to as “Butterflies,” these customers do not repeat purchase often, tend to buy a lot in a short time period and then move on to other firms, and avoid building a long-term relationship with any single firm. Firms should enjoy their profits until they turn to competition. High repeat purchases and low profitability. Referred to as “Barnacles,” these customers, if managed unwisely, could drain the company’s resources. Firms must evaluate their size and share of wallet. If the customers’ share of wallet is low, firms can up-sell and cross-sell to them to make them profitable. However, if the size of wallet is small, strict cost control measures must be taken to prevent losses for the firm. Low repeat purchases and low profitability. Referred to as “Strangers,” these customers not only are a poor fit to the company but offer very little profit potential to the firm. Firms should identify these customers early on and avoid any investment toward building a relationship with them.A relationship focus would lead firms to identify those customers who provide the most value and prioritize the marketing efforts accordingly.Depth of Direct Economic Value ContributionFirms are expected to demonstrate the profitability of their marketing actions at the individual customer level on an ongoing basis. At the same time, customers expect firms to customize products and services to meet their demands. A successful management of these two types of expectations depends on the firms’ ability to better interact with customers and create a unique positioning in the future. This calls for implementing an interaction strategy in which firm–customer and customer–customer interactions constantly occur. In other words, the CVT advocates for a firm–customer exchange environment that focuses on constant interactions as opposed to just profitable transactions.Implementing the interaction strategy requires the firm to adopt an interaction-oriented approach, which consists of four components—customer concept, interaction response capacity, customer empowerment, and customer value management—that the firm can utilize to increase the impact on its profitability (Ramani and Kumar 2008). First, the customer concept proposes that the unit of every marketing action or reaction is an individual customer. This places the customer at the top of the hierarchy in the customer–firm relationship. By doing so, firms are able to observe customer behavior and respond appropriately. Second, the interaction response capacity (the degree to which a firm can provide successive products or services drawing on previous customer feedback) highlights the importance of firms attending to and promptly addressing customer needs. Third, the customer empowerment component refers to the extent to which a firm allows its customers to ( 1) connect with the firm and design the nature of the transaction and ( 2) connect and collaborate with each other by sharing information, praise, and criticism about a firm’s product and services. Finally, the customer value management component refers to the extent to which a firm can quantify and calculate the individual customer value and use it to reallocate resources to customers. Firms such as IBM and American Express, through their endorsement of practices that are consistent with the elements of interaction orientation, have realized superior business performance, thereby demonstrating the managerial significance of the interaction orientation approach.Breadth of Indirect Economic Value ContributionFirms can nurture customer profitability by implementing an engagement strategy that considers the value generated by customers and employees and results in superior firm performance. The CVT proposes an engagement strategy that builds an engaged and committed customer and employee base to enhance the firm’s overall profitability (Kumar and Pansari 2016a).Although the engagement approach is powerful by itself, its true potential is realized when it is viewed from a long-term perspective. Engaged customers contribute to the long-term reputation and recognition of the brand. Creating an environment where customers are more engaged with the company may require an initial investment, but doing so has the potential to generate higher profits in the long run through the creation of customer engagement (Verhoef, Reinartz, and Krafft 2010). Furthermore, fostering engagement within firms is effective even in a recessionary economy. In a recessionary period, firms face budgetary challenges that significantly affect their marketing plans, which influences their levels of brand awareness and adoption. During this period, firms can mitigate the risks posed by the dents in their marketing budgets if they have a highly engaged employee base that promotes the firm’s brand and its products/services to its customers. This would ensure delivery of a superior customer experience, thereby increasing customer purchases, influence, and referrals—all without any additional marketing investments. Benefits of the CVTThe true measure of any marketing strategy or initiative is the improved financial result for the firm implementing it. This is why it is critical to study customer reactions to firm actions. When firms can precisely link their actions to customer value and, ultimately, to firm/shareholder value, they can begin to realize the potential of valuing customers as assets. Using concepts from economics, finance, and marketing, this article has created an interface that connects the value of each customer (determined by evaluating the lifetime value of the customer to the firm) with the performance and, therefore, the valuation of the firm. This connection has been established by ( 1) valuing customers as assets,( 2) managing a portfolio of customers, and ( 3) nurturing profitable customers. Each of these tactics plays a unique role in the optimization of shareholder value, customer equity, and overall profitability, and each tactic also works in combination with others to increase the overall impact on the firm value. Specifically, the CVT proposed here provides the following benefits: Benefit to the firms: When firms understand customer profitability and adopt CVT as the desired approach, they will be able to ( 1) attract and retain the most valuable customers, ( 2) nurture customers into a skilled resource base for the firm, ( 3) prevent their customers from switching to competitors by instilling a heightened sense of ownership of the firm among its customers, ( 4) consistently evolve their product/service offerings and match it to customer needs and preferences, ( 5) develop the ability to accurately foresee customer responses, and ( 6) exhibit superior aggregate business-level performance because the firm will be dynamically maximizing the profit function at every stage of business activity across all customers. Benefit to the customers: The CVT essentially advocates that the individual customer be the unit of analysis of every marketing action and reaction. This implies that firms respond to heterogeneous customers differently at different points in time by pooling information from multiple sources and points in time. This approach provides customers avenues to ( 1) connect with the firm and actively shape the nature of transactions and ( 2) connect and collaborate with each other by sharing information and knowledge. Furthermore, customers will be subjected to only the product/service offerings that are appropriate to them rather than every offering in the firm’s product portfolio. This keeps the customers focused, involved, and connected to the firm, thereby increasing their lifetime with the firm. Benefit to the environment: When firms base their marketing actions on the potential value the customers can bring them, it becomes easier to align and allocate the optimal amount of resources toward each customer. For instance, firms can plan and optimize the printing and mailing of marketing materials only to those customers they are intended for, and not send mass mailers. In addition to enhancing the effectiveness of the firm’s marketing efforts, this would also prevent the wasteful usage of valuable environmental and infrastructural resources and would help firms embrace sustainability as part of their core mission (Kumar and Christodoulopoulou 2014). Benefit to society: The benefits to society from implementing the CVT are threefold. First, there is a clear line of communication from firms to customers in terms of what to expect. The customer expectations relate to product/service offerings as well as firm responses through marketing actions. Second, the customer’s repeat purchases can now be consolidated among a few preferred firms. When firms display their respective needs and expectations, customers’ repeat purchases gradually are aligned to matching firms. Over time, a self-selection of repeat patronage by firms takes place that leads firms to focus their resources to satisfy their customers’ expectations. Finally, customers become empowered, such that they can exercise their choice and free will, thereby having a definite say in the marketing transaction process. This empowerment is evident through behavior ranging from customer advocacy (in case of complete satisfaction of firm offerings) to customer boycott and negative word of mouth (in case of extreme displeasure). Benefit to the employees: Employees are instrumental in providing a better customer experience, and this leads to customer engagement. Firms can engage their customers only if their employees are committed to delivering the brand values and performing to the best of their ability. Employees can be committed to the organization only if they understand its goals and their responsibilities toward achieving these goals. The employees must be highly engaged with the firm to provide peak performance (Kumar and Pansari 2014). Therefore, in addition to engaging customers, firms must also ensure that they are engaging their employees. Implications for Future ResearchThe insights from various financial theories served as a good starting point in the development of the CVT. In addition, the modern portfolio theory provided insights into risk diversification and the maintenance of a balanced portfolio. Overall, this study has showcased CVT as a forward-looking approach to aid managers in the valuation and management of a customer’s future contributions.Specifically, two key managerial implications emerge. First, the applicability of the CVT has been demonstrated in various scenarios spanning multiple markets (e.g., B2B, B2C), business settings (e.g., contractual, noncontractual), regional contexts (e.g., domestic, global), and industries. This should provide confidence to managers regarding its relevance and applicability to the current marketplace. Furthermore, experts opine that in the next five to ten years, CLV will be the metric of prime importance to businesses (Fader 2016). Second, given the evolution in the methodology to value customers, the availability of customer information, and the 360-degree customer data, managers are enriched with improvements that can result in better implementation. Despite the success of the CLV metric, it has not experienced industry-wide acceptance. For instance, a survey found that while 76% of the respondents agreed that CLV was important to their organization, only 42% reported that they were able to implement it (Charlton 2014).In this light, I identify three areas that can benefit from future research. First, the identification of the relevant organizational structure required to facilitate and implement CVT-based strategies is important. A customer-centric organization has been identified as a basis to align an organization (Kumar 2013). Such an alignment can enable firms to view their customers both as a source of business and as a potential business resource. However, more insights are necessary to better understand the organizational requirements for a successful CVT implementation. Second, the role of multiple stakeholders needs to be explored. For instance, studies have investigated the circumstances in which firm–stakeholder relationships are forged, such as those in which ( 1) the stakeholder is critical to the functioning of the firm (e.g., institutional investors having an equity interest in a firm), ( 2) the stakeholder aims to gain from the relationship (e.g., customers seeking firm offerings), ( 3) the firm and the stakeholder mutually gain from the association (e.g., the firm and the channel partners working collectively to produce and sell offerings), and ( 4) the stakeholder has moral rights attached to the firm’s operations (e.g., customer rights and employee rights) (Freeman and Reed 1983). However, precise recommendations have to be generated that can inform firms about better stakeholder integration for a successful implementation of CVT. Finally, the time-varying effects of certain drivers involved in the implementation of CVT need to be better understood. These elements include the valuation methodologies, data availability, intensity of customer-level data, and emergence of smarter consumers. Over time, these elements are likely to have a dynamic effect on CVT-based strategies. In summary, by focusing on a CVT-based approach, firms can ( 1) acquire and retain profitable customers, ( 2) employ resources productively, and ( 3) nurture profitable customers, which will ultimately result in higher firm value.Breadth of Indirect Economic Value ContributionApart from customers’ own contributions, the CLV metric also applies to customers’ indirect profit contributions to the firm such as their referral behavior, online influence on prospects’ and other customers’ purchases, and review/feedback on the  "
6,"An Empirical Analysis of the Joint Effects of Shoppers’ Goals and Attribute Display on Shoppers’ Evaluations This article develops a decision-making framework that highlights how display of numeric attribute information (e.g., display of calorie information) and shoppers’ goals (i.e., having a diet focus vs. a taste focus) jointly influence shoppers’ choices and preferences. Across two sets of studies, including a field study involving the launch of a new Coca-Cola product, the authors show that when food items are displayed in an aligned manner (i.e., when food items with lower-value calorie information are displayed below food items with higher calorie values), shoppers assign more importance weight to calorie gap information. In turn, higher importance weight assigned to calorie gap information leads diet-focused shoppers to relatively prefer low-calorie food items but leads taste-focused shoppers to relatively prefer higher-calorie food items. The third set of studies shows that this decision-making framework has widespread applicability and is relevant in any domain in which advertising, retail, and online displays show comparisons of numeric attribute information.In mid-2014, Coca-Cola launched Cola-Cola Life in Sweden, a reduced-calorie cola drink that differs from zero-calorie diet colas because it does not contain aspartame (which many perceive as unhealthy; Dean 2014). Reacting to an increased health focus among shoppers, launching Coca-Cola Life was part of Coca-Cola’s efforts to reduce the average calorie content of its drinks portfolio while still avoiding the use of aspartame. Some months prior to the launch, one of the authors of this article was in discussions with Coca-Cola managers about which factors were likely to influence shoppers’ choices between regular Coke and Coca-Cola Life. One factor discussed was in-store signage. Would signage indicating that Coca-Cola Life has lower calorie content be effective, despite prior research showing that merely indicating calorie values does not automatically lead shoppers to make healthier choices (Loewenstein 2011; see also Haws, Davis, and Dholakia 2016). This article was motivated, in part, by these discussions and the real-world challenges the authors (and firms like Coca-Cola) aim to examine.To better understand the factors that influence the choice of healthier food items, we start with the foundational notion that shoppers’ food choices depend on both individual differences among shoppers and the presentation format for the nutritional information (Haws, Davis, and Dholakia 2016; Loewenstein 2011; Mohr, Lichtenstein, and Janiszewski 2012). Most prior work has focused on evaluations of single food items (e.g., Gomez, Werle, and Corneille 2017; Graham and Mohr 2014; Mohr, Lichtenstein, and Janiszewski 2012); in contrast, in this article, we consider how shoppers choose among multiple food items, which represents a normative shopping situation. For example, shoppers often choose among various entrees in restaurants, multiple soup cans in supermarkets, or numerous soft drinks in convenience stores.We focus on two specific drivers of food item choice. We start by examining shoppers’ goals, which may be conceptualized as either individual differences or differences primed by the product category or shopping environment (e.g., Escaron et al. 2013; Newman, Howlett, and Burton 2014). A significant amount of prior research has highlighted goals related to dietary restraint, that is, the extent to which shoppers have diet goals which leads them to prefer food items with fewer calories1 (Howlett et al. 2012; Visschers, Hess, and Siegrist 2010) and consume fewer calories (Cavanagh and Forestell 2013). In this article, we offer a different proposal, making two points. First, rather than posit that shoppers are more or less focused on diet goals, we posit that shoppers focus on diet goals versus taste goals. Second, building from work on the “unhealthy = tasty” intuition (Raghunathan, Naylor, and Hoyer 2006), we argue that tastefocused shoppers intuit that unhealthy food items will better satisfy their taste goals, so their choice decisions appear to favor food items with more calories. Thus, we take a different perspective than prior work, and so we contrast shoppers with diet goals versus taste goals.Next, in grocery store settings, choices often involve a comparison between a focal, healthy food item and a comparison food item (Suri et al. 2012); in response, food manufacturers often provide comparisons that highlight the “nutritional gap.” For example, Better’n Peanut Butter Banana spread advertises that it has “40% fewer calories,” and Trop50 orange juice proclaims that it has “50% less sugar and calories,” relative to comparable products. However, in many cases, only calorie information appears in the front-of-pack (FOP) information, so shoppers must perform calorie gap calculations themselves. In turn, building on work in numeric cognition (e.g., Biswas et al. 2013) and heuristics (e.g., Shah and Oppenheimer 2007), we propose that differences in the vertical display of food items could prompt differences in perceptions of the importance of the calorie gap. If a focal food item, with fewer calories, appears below another food item (i.e., if the focal food item is displayed in an aligned manner), then the subtraction task to calculate the calorie gap is easier. And if calculating the calorie gap is easier, then shoppers are likely to attach more importance to calorie gap information during their evaluations. Thus, displaying a focal food item in an aligned manner should increase the importance weight that shoppers place on the calorie gap. Among shoppers with diet goals, this increased importance weight shifts shoppers’ choices toward the focal food item, but among tastefocused shoppers, this increased importance weight may shift the choice share away from the focal food item, toward the higher-calorie option. By combining these propositions, we posit that when food items are displayed in an aligned (vs. nonaligned) manner, diet-focused shoppers relatively prefer lower-calorie food items, but taste-focused shoppers make choices as if they relatively prefer higher-calorie food items. This nonintuitive proposition is the central hypothesis of our article, and we test it across multiple studies, including a field study involving the choice between regular Coca-Cola and the lower-calorie Coca-Cola Life.More generally, in this article we focus on product domains for which advertising, retail, and online displays show comparisons of numeric attribute information. The food domain is one such domain, which involves comparisons of numeric nutritional information relating to calories (and sodium). Other exemplar domains that involve comparisons of numeric attribute information include ( 1) product price comparisons, as frequently seen in basket comparisons posted in supermarkets (e.g., Publix vs. Walmart), in online comparative advertisements (Dyson vs. Shark vacuum cleaners), and on price comparison tools (e.g., hotel rates on Trivago.com); ( 2) advertisements for robotic vacuum cleaners (e.g., Neato vs. Roomba), which involve attributes such as operating time and charging time; ( 3) advertisements for cellular networks (e.g., T-Mobile vs. Verizon), which involve attributes such as Internet speed; and ( 4) advertisements for cell phones (e.g., Apple iPhone vs. Samsung Galaxy), which involve attributes such as screen size, standby time, and talk time.In domains involving numeric attribute information, we investigate the impact of two factors on shoppers’ choice decisions. First, numeric attribute information may be displayed in an aligned (vs. nonaligned) manner, whereby aligned display involves showing the low-value numeric information below the higher-value numeric information (e.g., lower prices displayed below higher prices). Second, shoppers may have different goals, perceiving attributes as either more-is-better (MIB; preferring items with higher attribute values) or less-is-better (LIB; preferring items with lower attribute values). For example, shoppers who view price as a measure of sacrifice perceive price as an LIB attribute, whereas those who view price as a measure of quality perceive price as an MIB attribute (Dodds, Monroe, and Grewal 1991; see also Miyazaki, Grewal, and Goodstein 2005). In the domain of robotic vacuum cleaners, operating time is an MIB attribute, but charging time is an LIB attribute; in the domain of laptops, many would perceive battery life as an MIB attribute but perceive laptop weight as an LIB attribute. Building from the central hypothesis outlined previously, we posit that when items are displayed in an aligned (vs. nonaligned) manner, shoppers who perceive the displayed attribute as an LIB (MIB) attribute will relatively prefer the item with the lower (higher) value attribute.We aim to make the following contributions. Generally speaking, we outline a parsimonious framework that examines how shoppers react to advertising, retail, or online displays in (a wide variety of) product domains involving numeric attribute information. This article identifies two elements that jointly determine shoppers’ reactions: ( 1) whether shoppers view the displayed attribute as an LIB attribute or an MIB attribute, and ( 2) whether the attribute information is displayed in an aligned or nonaligned manner, which influences the importance weight shoppers put on this attribute information. We suggest that firms can use this framework to better design advertising, retail, and online displays. While Biswas et al. (2013; work on the subtraction principle) provide an initial examination of aligned (vs. nonaligned) displays, their stated process mechanism does not easily extend beyond the price promotion domain and was examined only in LIB contexts. The current article substantially modifies and broadens the process mechanism underlying the subtraction principle to allow it to extend into multiple product domains. Moreover, we explicitly contrast LIB versus MIB contexts, outlining exactly how shoppers’ attribute perceptions (i.e., LIB vs. MIB) and display alignment jointly influence their choices and perceptions.In addition, we aim to make two contributions specific to the food domain. Not only is the food domain important from both a firm perspective and a shopper perspective, but also the growing importance of how best to motivate people to make healthy food choices has prompted increased research focus in this domain. First, prior research suggests that people who have less focus on diet goals pay less attention to calorie information (e.g., Bialkova, Sasse, and Fenko 2016; Cavanagh and Forestell 2013; Mohr, Lichtenstein, and Janiszewski 2012). In contrast, we show that people who have less focus on diet goals (i.e., have more focus on taste goals) indeed pay attention to calorie information (similar to people with more focus on diet goals), but because of the unhealthy = tasty intuition, these shoppers behave as if they prefer food items with more calories (unlike people with more focus on diet goals). Second, as a novel point not evidenced in prior research, we show that whether shoppers make goal-consistent choices is contingent on whether food items are displayed in an aligned manner. Next, we develop our propositions and test them across multiple product domains, including in a field study involving the choice between regular Coca-Cola and the newly launched, low-calorie Coca-Cola Life. Conceptual DevelopmentShoppers’ Goals During Food Item ChoicesWhat goals do shoppers have when they make food item choices? To answer this question, we turn to literature at the intersection of food-related research and research into goals. On the one hand, there may be individual differences (i.e., “trait differences”) across shoppers, and these differences should lead to shoppers having different goals when making food item choices. Specifically, some shoppers have diet goals (Herman and Polivy 2004; Howlett et al. 2012; Naylor, Droms, and Haws 2009), so they prefer food items with fewer calories or less sodium (LIB behavior).Prior research has examined the extent to which shoppers focus on diet goals (Haws, Davis, and Dholakia 2016; Mohr, Lichtenstein, and Janiszewski 2012; Naylor, Droms, and Haws 2009; Van Herpen and Van Trijp 2011), and the subsequent impact on food item choices and consumption. This prior research has indicated that people who have less focus on diet goals tend to pay less attention to nutritional information. Thus, Mohr, Lichtenstein, and Janiszewski (2012; p. 66) show that when presented with less healthy versus more healthy food items, people with higher levels of dietary concerns were significantly more likely to choose the healthier food item, but people with low levels of dietary concerns were relatively indifferent across food items (“significantly higher purchase intentions … for all values of dietary concern above 3.80” vs. “no significant differences below … the Johnson–Neyman point”). Similarly, Cavanagh and Forestell (2013; p. 508) found that restrained eaters consumed more (relatively healthy) Kashi cookies than Nabisco cookies, “whereas the unrestrained eaters did not differ in their consumption of the two brands,” Finally, Bialkova, Sasse, and Fenko (2016; p. 44) asked participants to choose between (relatively healthy) cereal bars and (relatively unhealthy) potato chips. They found that “consumers highly concerned about health preferred to buy cereal bars ( p = .018), while less concerned consumers selected to buy chips and cereals with equal probability ( p > .4).” Taken together, these findings appear to indicate that people with low levels of dietary concerns are relatively indifferent between the less healthy food item and the healthier food item. In this article, however, we posit differently, and so we make two distinct points.Shoppers’ goals. Rather than describing shoppers’ goals on a continuum anchored by more versus less focus on diet goals, in this article we propose that the relevant anchors should be diet goals versus taste goals. As a novel point, we emphasize the explicit presence of taste goals (and not just less focus on diet goals), consistent with Andrews, Netemeyer, and Burton’s (1998) use of a taste goal prime as a control condition.Prior literature has indicated that shoppers with more focus on diet goals pay attention to nutrition information (“consumers who expressed a great concern for … dietary eating … made more active use of the health label information”; Bialkova, Sasse, and Fenko 2016, p. 40), and so are more likely to prefer food items with fewer calories. However, distinct from prior literature, we propose that shoppers with less focus on diet goals (i.e., those with taste goals) also pay attention to nutrition information but use it differently, such that they choose food items with more calories. We clarify that we are not claiming that shoppers with taste goals deliberately seek out food items with more calories; rather, we suggest that these shoppers intuit that high-calorie food items are tastier (unhealthy = tasty intuition); in their quest for taste, they select relatively highercalorie food items.To make this prediction, we build from research into behavioral traits (related to food preferences, and related to impulsivity) and perceptions of food. First, we build from work that connects dietary restraint to impulsiveness. The work of Van Koningsbruggen, Stroebe, and Aarts (2013; Table 1; p. 83) indicates that those low on dietary restraint are more likely to be impulsive. Second, more impulsive people are likely to prefer tasty food, and they are both more likely to pick up (tasty) cookies (Ramanathan and Menon 2006; Figure 3, p. 638) and more likely to choose (tasty) cake over salad (Sengupta and Zhou 2007; Study 2, p. 301). Third, unhealthy foods are more likely to be perceived as tasty (see work on the unhealthy = tasty intuition2 [Mai and Hoffmann 2015; Raghunathan, Naylor, and Hoyer 2006). Overall, we posit that people low on dietary restraint (i.e., those with taste goals) may behave as if they prefer (relatively) unhealthy food items.Individual differences versus state differences. Beyond just individual differences (e.g., extent of diet intentions), environmental factors may also prompt differences in (diet vs. taste) goals, which we term “state differences.” Product category differences may trigger differences in goals, with shopping for health-focused foods potentially triggering diet goals and shopping for candy potentially triggering taste goals. Advertising differences may also prompt differences in goals. Foods advertised as health foods or diet foods, or foods packaged reflecting “greenness” may trigger diet goals, whereas foods advertised as comfort foods may prompt taste goals. Thus, it is possible that the same person may have diet goals when examining a certain type of food item and yet may have taste goals when examining a different type of food item.These discussions suggest that those with diet goals should prefer lower calorie food items, whereas those with taste goals should (in line with the unhealthy = tasty intuition) behave as if they seek out higher-calorie food items. Next, we propose that differences in how food items are displayed affect the extent to which shoppers’ goals influence food item preferences, and we elaborate on this point in the following subsection.TABLE: TABLE 1 Johnson–Neyman Regions in Study 1  aMean-centered values for diet intentions scale.Notes: This table illustrates the conditional effect of alignment on choice of Coca-Cola Life, at values of diet intentions scale. LLCI = lower-limit confidence interval; ULCI = upper-limit confidence interval. Boldfaced cells indicate significance. Differences in Display FormatFood items are only seldom evaluated in isolation; instead, such evaluations often involve a comparison of a focal food item with potential alternatives (Suri et al. 2012). In many cases, nutritional information is available FOP, so shoppers can calculate the attribute gap (e.g., calorie gap, sodium content gap). For example, a grocery shopper aiming to buy cereal may find a focal food item, which has fewer calories, displayed either below or above a higher-calorie food item. Both food items display calorie content FOP. If the propensity to engage in calorie gap calculations depends on whether the food items are presented in an aligned (vs. nonaligned) display, then such display differences may influence food item choice. We first consider whether differences in the display format influence shoppers’ propensity to initiate a subtraction task to calculate the calorie gap, then we discuss how this propensity might influence the importance weight shoppers attach to the calorie gap for their evaluations.Impact on propensity to initiate the subtraction task. In general, a comparison of two attributes that feature numeric information involves subtraction (Thomas and Morwitz 2009). However, prior research has not fully explored how presenting a smaller number below versus above a larger number influences subtraction calculations. We integrate research in numeric cognition with pricing research to examine this question. First, in the subtraction task A - B, A is the minuend, and B is the subtrahend. People generally perceive that it is normative to present a larger minuend above a smaller subtrahend, and prior research has affirmed that fewer computational errors occur with this format (Fuson and Briars 1990). Second, in a study of how people verify addition problems, Yip (2002) finds that inaccurate equations that fail to conform to conventional presentation norms are perceived as harder to verify as correct (e.g., it is more difficult to determine whether 7 = 3 + 5 is correct than whether 3 + 5 = 7 is correct). Accordingly, we posit that subtraction equations in which a smaller-value subtrahend appears below (above) the minuend are easier (harder) to verify. Third, because people do not like to work on overly challenging problems (Oppenheimer 2008), locating a smaller-value subtrahend above the minuend—contrary to the norm in contexts involving difference calculations—may reduce the propensity to perform a subtraction task. In research on price promotions, Biswas et al. (2013) propose the subtraction principle, a somewhat similar information processing sequence. They proposed that when sale prices are displayed to the right of the original price (i.e., smaller number to right of the larger number), shoppers perceive the subtraction task as cognitively easier and so are more likely to calculate the discount depth. However, if sale prices appear to the left of the original price, shoppers perceive the subtraction task as cognitively harder and so are less likely to initiate a subtraction task. Rather, shoppers would approximate discount depth at around 10%–12% (reflecting a discount depth benchmark from Blair and Landon [1981]).Now assume that two (competing) cereals explicitly provide FOP calorie information. Building on the previous arguments, if the focal, healthy cereal is displayed in an aligned manner, then shoppers can calculate the calorie difference relatively easily. But if the focal cereal is displayed in a nonaligned manner, shoppers may perceive the subtraction task as harder and so may be less likely to initiate the subtraction task to calculate the calorie gap.Importance weight attached to the calorie gap during evaluations. During evaluations, people grant easy-to-process cues higher importance weights (Shah and Oppenheimer 2007, pp. 371–72; see also Oppenheimer 2008). This point has roots in prior work on heuristics, which shows that people more heavily weight easier-to-access cues. For instance, people use brand name perceptions as proxy for product quality (Maheswaran, Mackie, and Chaiken 1992), use ease-ofimageability of attributes (like hallways) as an input for making apartment evaluations (Keller and McGill 1994), and so on. Therefore, if shoppers perceive that calculating the calorie gap is relatively easier, during evaluations they assign more importance weight to the calorie gap. Continuing with the cereal example, if the focal, low-calorie cereal is displayed in an aligned manner, during evaluations shoppers attach relatively higher importance weight to calorie gap information.This mechanism substantially enhances the generalizability of the subtraction principle mechanism proposed in Biswas et al. (2013). The subtraction principle predicts that displaying the sale price in a nonaligned manner increases subtraction difficulty. In turn, due to subtraction difficulty, shoppers who are less likely to initiate the subtraction task to calculate discount depth assume a 10%–12% discount depth (benchmark from Blair and Landon 1981). This mechanism, especially the point about the assumed discount depth, is fairly specific to the pricing domain. We modify the subtraction principle mechanism and propose that shoppers who are more (less) likely to initiate the subtraction task attach more (less) importance weight to discount depth information (more generally, attribute gap information). Consequent to this modification, the subtraction principle can apply beyond the pricing domain to a wide variety of other product domains.Furthermore, the studies in Biswas et al. (2013) focus exclusively on the domain of price promotions and imply that shoppers generally prefer an overall lower price, in effect implying that price is an LIB attribute. We point out that there are contexts wherein price may be perceived as an MIB attribute, often for reasons relating to signaling of quality (Dodds, Monroe, and Grewal 1991; Miyazaki, Grewal, and Goodstein 2005; Monroe 1973). In this article, we generalize the work of Biswas et al. (2013), examining both LIB and MIB attributes, while also examining attributes such as calories, which are perceived by some as LIB and perceived by others as MIB. Appendix A lists the aforementioned points and shows the various ways this research modifies and broadens the prior conceptualization of the subtraction principle. Setting Up the HypothesesDisplaying a focal, healthy food item in an aligned (vs. nonaligned) manner should increase the perceived importance weight of the calorie gap during evaluations. For shoppers with diet goals, the increased importance weight of the calorie gap should increase preference for the focal food item. For shoppers with taste goals, however, the increased importance weight of the gap should enhance their preference for the comparison food item with higher levels of calories and reduce their preference for the focal food item. Formally,H1: For shoppers with diet (taste) goals, displaying a focal, healthy food item in an aligned manner increases (decreases) choice share of the focal, healthy food item.H1 is our central hypothesis, stating that presenting food items in an aligned (vs. nonaligned) manner increases goalconsistent food choices and preferences. The next two hypotheses outline the mechanism underlying this central hypothesis. We propose that ( 1) during evaluations, presenting food items in an aligned (vs. nonaligned) manner increases the importance weight placed on the calorie gap (H2), and ( 2) during evaluations, increased importance weight placed on the calorie gap increases the propensity to make goal-consistent food choices (i.e., increases the propensity that shoppers with diet (taste) goals are more (less) likely to choose the focal, healthy food item; H3).H2: Displaying the focal, healthy food item in an aligned (vs. nonaligned) manner increases the importance weight of attribute gap information during evaluations.H3: For shoppers with diet (taste) goals, increased importance weight of attribute gap information increases (decreases) choice share of the focal, healthy food item.Study 1 is a field study in a supermarket and is an initial test of H1. It involves shoppers choosing between regular CocaCola and the newly launched Coca-Cola Life. In Study 2a, we reexamine H1 in a lab study, using a chocolate context, wherein we associate chocolate with either diet goals or taste goals. In Study 2b, we examine the full process model (H1–H3), using a soup can choice context. Given that Studies 1 and 2 relate to the food domain, in Studies 3a and 3b we generalize our findings by examining other product domains. Stimuli exemplars (for all studies) appear in Appendix B. Study 1: A Field Study MethodWe ran Study 1 over four days in a supermarket in Stockholm. Coca-Cola (Sweden) provided us with bottles of regular CocaCola (CCR; more calories = 879 kJ3) and of the newly launched soft drink, Coca-Cola Life (CCL; fewer calories = 565 kJ, focal drink). We had access to endcap shelving, which we modified using two different display versions that alternated every few hours, displaying CCL in either an aligned manner (i.e., CCR on the shelf above and CCL on the shelf below) or a nonaligned manner. The shelf-signs clearly showed the kJ values associated with each drink. We specifically clarify that each shopper saw only one of the two display versions.Shoppers were intercepted and asked to participate in the study. In return, they would receive either CCR or CCL, whichever they preferred. Shoppers examined the display, then chose a CCL or CCR bottle (the experimenters restacked the shelf each time, so shoppers always saw fully stacked CCL and CCR shelving.) Next, shoppers moved to another area, where they completed a short survey. The survey included a shortversion diet intentions scale, with two items from Stice (1998; “I take small helpings in an effort to control my weight,” “I limit the amount of food I eat in an effort to control my weight”; 1–5 scale; 1 = “never,” and 5 = “always”; r = +.59, p < .05), and also included demographics (age and gender). In all, 352 shoppers (Medianage = 20.0 years; 67.9% women) participated in this 2 (display: aligned vs. nonaligned) · continuous (diet intentions scale) between-subjects study. ResultsWe used PROCESS (Model 1; Hayes 2013) to investigate the interaction. The dependent variable was soft drink choice (CCR = 0, CCL = 1), and the two independent variables were diet intentions (mean-centered at M = 2.31) and display (nonaligned = –1, aligned = 1). In the logistic regression for soft drink choice, we found significant main effects of diet intentions (b = -.34, SE = .12, z = -2.91, p < .05), no significant main effects of display (z = -.94, p = .35), and (most importantly) a significant interaction effect (b = .32, SE = .12, z = 2.72, p < .05). The positive interaction term indicated that for those with higher diet intention scores, presenting CCR and CCL in an aligned manner increased choice of CCL (the focal, lower-calorie drink).A floodlight analysis (Table 1) revealed that for those with relatively high diet intention scores (mean-centered scores > 1.68; 7.95% of sample), the simple effect of displaying CCL in an aligned display condition was significantly positive (at score of 1.68: b = .44, SE = .22, z = 1.96, p = .05), implying increased choice share for the lower-calorie CCL. However, for those with low diet intention scores (mean-centered scores < –.43; 30.1% of sample), the simple effect of displaying CCL in an aligned display was significantly negative (at score of -.43: b = -.24, SE = .12, z = –1.96, p = .05), implying decreased choice share for the lower-calorie CCL (and increased choice share for the higher-calorie CCR). Study 1 results are consistent with H1. Studies 2a and 2b: Follow-Up Tests Study 2aIn Study 1, shoppers differed in the extent of their diet intentions, reflecting trait dispositions. Moving beyond traits, in Study 2a we acknowledge that shoppers may differ in their (taste vs. diet) goals, contingent on the food item category. That is, the same shopper could have different goals, conditional on differences between food item categories. Some sets of food items (e.g., health foods) may prompt diet goals, but others (e.g., desserts) may be associated with taste goals. To the extent that products prompt different goals, the effects of presenting food items using an aligned (vs. nonaligned) display may differ. To examine this point, in Study 2a we prime participants to associate the same product (in this case, chocolates) with either a taste goal or a diet goal.Method. This was a 2 (goal association: taste goal vs. diet goal) · 2 (display: aligned vs. nonaligned) between-subjects design, involving 255 undergraduate students (65.1% women) taking a survey in a behavioral lab. Participants were told that the survey was about beliefs and preferences about chocolate. First, we primed chocolate as being associated with either a taste goal or a diet goal, using a mechanism outlined in prior work (e.g., Dhar and Wertenbroch 2000; Roggeveen et al. 2015). For the taste goal, participants were told that “there are many reasons why people eat chocolate. And yet, what is often comes down to, is that people eat chocolate because it makes them happy. At the end of a long day, eating a piece of chocolate is the perfect reward.” Next, participants were asked to write a few words about why people should eat chocolate. Participants generally responded in ways consistent with a taste goal (e.g., “People should eat chocolate because it makes them happy. It feels rewarding to have some at the end of a day,” “It’s a wellearned reward at the end of a long day,” “Eating chocolates make people happy. They think [it’s the] perfect reward”). For the diet goal, participants were told that “there are many reasons why people eat chocolate. Interestingly—and this may not be well known—people should eat chocolate for health reasons. Medical studies have shown that chocolate can not only reduce LDL (bad cholesterol) and increase HDL (good cholesterol) but also reduce the incidence of stroke. Next, participants were asked to write a few words about why people should eat chocolate. Participants generally responded in ways consistent with a diet goal (e.g., “It is good for your cholesterol and can prevent strokes,” “People should eat chocolate because it can improve aspects of your health,” “reduce LDL/increase HDL/reduce chance of a stroke”).Next, participants were shown two chocolate boxes. Each chocolate in box W had approximately 91 calories, and each chocolate in box K had approximately 68 calories. Participants were shown the two boxes either in an aligned manner or in a nonaligned manner (box W displayed below box K). Finally, participants were asked which box of chocolates they would prefer to take a piece of chocolate from (single-item, 11-point scale; -5 = “box K,” and +5 = “box W”).Results. We ran an analysis of variance for chocolate preference. We found no significant main effects for display (F( 1, 251) = .02, p > .8), significant main effects for goal association (F( 1, 251) = 18.9, p < .05), and a significant two-way interaction between goal association and display (F( 1, 251) = 10.3, p < .05).When chocolate was associated with taste goals, participants’ preference for low-calorie chocolate box K was weaker when box K was displayed in an aligned manner (Maligned = .68, SD = 3.54; Mnot aligned = -.69, SD = 3.86; F( 1, 251) = 4.67, p < .05). Put another way, when chocolate was associated with a taste goal, participants’ preference for the higher-calorie chocolate box W was significantly stronger when box W was displayed in an aligned manner. In contrast, when chocolate was associated with a diet goal, participants’ preference for low-calorie chocolate box K was significantly stronger when box K was presented in an aligned manner (Maligned = -2.70, SD = 3.02; Mnot aligned = -1.20, SD = 3.79; F( 1, 251) = 5.64, p < .05). These results are consistent with our central hypothesis, H1. Study 2bIn Study 2b, we test the full process model across H1–H3. In addition, whereas Studies 1 and 2a involved some version of the attribute “calories,” Study 2b involves the attribute “sodium.”Method. Two hundred sixty-one U.S. undergraduate students (56.7% women) participated in a two-part study for course credit. First, as part of a set of multiple studies, participants filled out the short, five-point (1 = “never,” and 5 = “always”), sixitem diet intentions scale (Stice 1998; a = .91). The six items were “I take small helpings in an effort to control my weight,” “I limit the amount of food I eat in an effort to control my weight,” “I hold back at meals in an attempt to prevent weight gain,” “I sometimes avoid eating in an attempt to control my weight,” “I skip meals in an effort to control my weight,” and “I sometimes eat only one or two meals a day to try to limit my weight.”Second, a week later, the same undergraduate students participated in a soup choice study. Because the popular press tends to highlight the negative influences of sodium on health, we did not expect many participants to know that higher sodium (also) can be associated with better taste. Therefore, we asked each participant to read a couple of paragraphs that summarized extracts from various publications, stating that although sodium is associated with obesity and high blood pressure, it also tends to be associated with better taste and aroma. All participants read both paragraphs, such that all participants received twosided information.Next, participants observed two (similar-looking) cans of chicken soup, next to which we showed the respective sodium levels (can N = 477 mg, can B = 664 mg). Participants also learned that the cans typically contained about two servings each, had 90–100 calories per can, and were similar in their content weight (approximately 19 oz.). These soup cans appeared in either an aligned manner (can N below can B) or a nonaligned manner (can N above can B), leading to a 2 (display: aligned vs. nonaligned) · continuous (diet intentions scale) between-subjects design.Participants first chose their preferred soup can and indicated the importance weight of various factors for their choice decision by allocating five points across ( 1) sodium content, ( 2) number of servings per can, and ( 3) whether the soup contained chicken. Participants could allocate points however they wished, as long as the total points allocated across the three elements totaled five.We anticipated that presenting the soup cans in an aligned manner will lead participants to place increased importance weight on sodium content. Among those who scored high on the diet intentions scale, participants who assigned more weight to the sodium content should be more likely to choose lowsodium can N. However, if participants scored low on the diet intentions scale, such that they likely focus more on taste, then those who placed more weight on sodium content should be more likely to choose higher-sodium can B (due to the unhealthy = tasty intuition) and so should be less likely to choose can N and more likely to choose can B.Results. First, we ran a logistic regression for soup choice (can B = 0, can N = 1), in which the independent variables were the diet intentions score (mean-centered at M = 2.21) and display (not aligned = –1, aligned = 1). The main effect of meancentered diet intentions score was significant (b = .77, SE = .18, z = 4.34, p < .05), the main effect of the vertical display was not significant (z = .30, p = .76) and the interaction effect was significant (b = .69, SE = .18, z = 3.86, p < .05). The positive interaction term indicated that those with higher diet intention scores were more likely to choose the low-sodium soup can in the aligned display condition.Second, the floodlight analysis (PROCESS Model 1) depicted in Table 2 revealed that for those with mean-centered diet intention scores greater than .36 (34.9% of sample), soup can N was relatively more preferred in the aligned display condition (at score of .36; b = .29, SE = .14, z = 1.96, p = .05). However, for those with (mean-centered) scores less than –.54 (32.2% of sample), the lower-sodium soup can N was relatively less preferred in the aligned display condition (at score of -.54; b = -.33, SE = .17, z = -1.96, p = .05), and higher-sodium soup can B was relatively more preferred. This was consistent with results in prior studies and with H1.Next, the importance weight that participants assigned to sodium content information was higher in the aligned display condition (Maligned = 3.22, SD = .91; Mnonaligned = 2.02, SD = .72; F( 1, 259) = 140.3, p < .05); this result was consistent with H2. As an important point, prior work (Bialkova, Sasse, and Fenko 2016) has indicated that those with higher(lower) levels of diet concerns paid more (less) attention to nutrition information, whereas we assert that this is not the case and that those with taste goals (i.e., with lower levels of diet concerns) would continue to pay attention to nutrition information. Consistent with our assertion, there was no correlation between diet intention scores and importance weight for sodium (r = .02, p > .7).Third, for those with mean-centered diet intentions scores greater than .87 (i.e., with scores 1 SD above the mean diet intentions scale score), the PROCESS (Model 14; Hayes 2013) output indicated that the mediating effect of the importance weight assigned to sodium was significantly positive (95% confidence interval = [.07, .74]). Thus, if participants scored higher on the diet intentions scale (i.e., had diet goals) and placed more importance weight on sodium during the choice process, they were more likely to choose can N. But for participants with mean-centered diet intention scale scores less than –.87 (i.e., with scores 1 SD below the mean diet intentions scale score), the PROCESS (Model 14) output indicated that the mediating effect of the importance weight for sodium was significantly negative (95% confidence interval = [–.57, –.01]). Thus, if participants scored lower on the diet intentions scale (i.e., had taste goals) and placed more importance weight on sodium during the choice process, they were less likely to choose lower-sodium can N and more likely to choose the higher-sodium can B. These results were consistent with H3. Studies 3a and 3b: Generalizing the ResultsStudies 1 and 2 focused on the food domain. To generalize the results, we examine other product domains. First, in Study 3a, we present an incentive-compatible study involving an LIB scenario that examines choice between two kitchen implements. In Study 3b—in an MIB scenario—we examine a choice between cell phone accessories. In Study 3b, we also examine factors that may moderate the effects in this research.The hypotheses (H1–H3) are fairly specific to the food domain. However, these hypotheses can easily be modified to extend to any product domain. In Study 3a, for instance, the prediction is that using an aligned display will increase choice share of the focal item. Study 3aMethod. We administered this two-cell (sale price display: aligned vs. nonaligned) between-subjects design to attendees of three sessions of a cooking class held in a gourmet food store in an upscale U.S. neighborhood. We asked them to participate in a five-minute (voluntary) study, and approximately twothirds of attendees chose to participate. Among these 43 participants, the median age group were those over 50 years old, 79.1% were women, and median household income was $100,000–$200,000.All participants were given a study booklet. The first page showed two upscale Wusthof brand cooking knives. On the left side of the page, we presented a picture of knife A (comparison knife) along with a brief description and a sale price of $71; on the right side of the page, we presented a picture and a brief description of knife B (focal knife), showing an original price of $114 and a sale price of $77. In the nonaligned display condition, the sale price appeared above the original price, whereas in the aligned display condition, the sale price was shown below. Participants indicated which knife they preferred. On the second page, we provided another set of two Wusthof brand knives (knife C and knife D) using a similar presentation, but here the focal knife was on the left side of the page. The focal knife, knife C, had an original price $102 and a sale price of $75. Knife D (on the right side of the page) served as the comparison knife, with a sale price of $68. Participants again indicated which knife they preferred. Finally, participants provided their gender, age range, and household income range. To ensure that participants took the task seriously, we told participants (prior to starting the study), that they should make careful choices because (in each session) one participant would be randomly selected to receive one of the two knives chosen.TABLE: TABLE 2 Johnson–Neyman Regions in Study 2b  aMean-centered values for diet intentions scale.Notes: This table illustrates the conditional effect of alignment on choice of the low-sodium soup can, at values of diet intentions scale. LLCI = lower-limit confidence interval; ULCI = upper-limit confidence interval. Boldfaced cells indicate significance.Results. Consistent with results in Studies 1 and 2, participants preferred the focal knife relatively more when the sale price was shown in an aligned manner. When sale price was shown below the original price, the choice shares for both focal knives B and C were 84.0%; when sale price appeared above the original price, choice shares for the focal knives fell to 33.3% (knife B) and 61.1% (knife C). These choice share differences were significant for knives A versus B (c2( 1) = 11.49, p < .05) and directionally significant for knives C versus D (c2( 1) = 2.88, p = .09). Study 3bThis study involved a cell phone battery scenario wherein participants were provided with numeric information on battery life, an MIB attribute. Our incoming expectation was that relative preference for the focal item, with longer battery life, would be greater when it was presented in an aligned (vs. nonaligned) manner. Furthermore, a key element of our theory is that if the focal item and the comparison item are displayed in an aligned manner, then it is relatively easier for the shopper to perform the difference calculation. However, if the difference calculation is very easy in the first place, then alignment differences should not affect the perceived ease of performing difference calculations, thus mitigating our proposed effects.Methods. Participants from Amazon Mechanical Turk (N = 250, 37.8% women, median age group 26–30 years, median annual income $25–$50,000) took a Qualtrics survey. After an instructional manipulation check question (see Oppenheimer, Meyvis, and Davidenko 2009), the survey outlined a scenario wherein the participants used their cell phone a lot, so participants were aiming to buy a cell phone case with an integrated cell phone battery. Then participants read brief descriptions of two cell phone cases with batteries. Case/ Battery H (the comparison item) had been on the market for six months, earned good reviews, and offered an incremental battery life of 246 minutes. Case/Battery J (the focal item) was a bit thicker, and because it had just launched, reviews were not available, but the manufacturer claimed an incremental battery life of 331 minutes. The participants were randomly assigned to a 2 (display: aligned vs. nonaligned) · 2 (calculations: harder vs. easier) between-subjects design. In the harder difference calculation condition, the battery lives were 246 minutes (H) and 331 minutes (J); in the easy difference calculation condition, battery lives were 250 minutes (H) and 350 minutes (J). We elicited relative preference on a seven-point scale (1 = “strong preference for H,” and 7 = “strong preference for J”) and captured demographic information.Results. We found that 35.6% participants gave incorrect responses to the instructional manipulation check (consistent with ranges in Oppenheimer et al. [2009]), so we removed them from the analyses (consistent with recommendations in Oppenheimer et al. [2009]). An analysis of variance for relative preference revealed significant main effects (for both display and difference calculations, F( 1, 157) > 3.9, p < .05), as well as a directionally significant interaction effect (F( 1, 157) = 3.31, p = .07). When battery life differences were harder to calculate, using an aligned display (i.e., locating J above H) led to significantly increased preferences for the focal, longer-life battery J (Mabove = 3.98, SD = 1.93 vs. Mbelow = 2.70, SD = 1.59; F( 1, 157) = 9.38, p < .05). In contrast, when difference calculations were easy, locating J above H did not significantly influence relative preferences (Mabove = 4.03, SD = 2.19 vs. Mbelow = 3.85, SD = 1.89; F( 1, 157) = .17, p > .6). DiscussionConsistent with the theory in this article and with our a priori expectations, any factor that makes the difference calculation easier should serve as a suitable moderator for our effects. In Study 3b, we examined the specific case of when difference calculations were easy (e.g., 350 - 250), and thus, alignment differences should not influence the importance weight that shoppers would generally give to the difference gap. Other instances when difference calculations should be easier may include when, for example, the difference gap is explicitly stated (e.g., the difference percentage is explicitly shown [“40% more battery life,” “25% less sodium”]) or the difference is very large. In all these cases, there would be no need for the shopper to perform the difference calculations to figure out that the difference gap is substantial, so shoppers should generally give the difference gap relatively high importance weight. These factors—some of which were foreshadowed in Biswas et al. (2013)—all (potentially) constitute moderators to our effects. General DiscussionFor shoppers who have diet goals, presenting a focal, healthy food item in an aligned (vs. nonaligned) manner increases its choice share. In contrast, for shoppers with taste goals, presenting the focal food item in an aligned (vs. nonaligned) manner decreases its choice share and increases the choice share of the competing, less healthy food item. This nonintuitive interaction result reflects our central hypothesis (H1), whereby the extent to which shoppers make made goal-consistent food item choices is higher (lower) when the food items are displayed in an aligned (nonaligned) manner. Drawing on work in food-related research, goals, and numeric cognition, we outline the underlying process mechanism in H2–H3. Whereas Studies 1 and 2 illustrate H1–H3 in the food domain, Study 3 shows that our proposed decisionmaking framework has widespread applicability, potentially extending to any domain wherein advertising, retail displays, or online displays involve comparisons of numeric attribute information. Theoretical ContributionsOur study makes several contributions to the body of work relating to shoppers’ food-related goals and how shoppers make food choices. First, prior research has suggested that people with less focus on diet goals pay less attention to nutrition information (Naylor, Droms, and Haws 2009; Van Herpen and Van Trijp 2011). For example, Mohr, Lichtenstein, and Janiszewski (2012, p. 64) argue that “those very involved with their dietary choices will … [be influenced by] nutrition labels,” and they found (p. 66) that differences in purchase intentions across less healthy versus healthier food items arose among those with higher levels of diet intentions but not among those with lower levels of diet intentions. Bialkova, Sasse, and Fenko (2016) and Cavanagh and Forestell (2013) found similar effects, such that people who were more concerned about health were more likely to prefer healthy food items over less healthy food items, but people less concerned about health expressed no clear preference. In contrast, we propose that people with less focus on diet goals (i.e., those with taste goals) do indeed pay attention to nutrition labels; however, because of the unhealthy = tasty intuition, these shoppers behave as if they prefer food items with more calories or sodium. This explanation better reflects the interaction result in Studies 1–2, implying a specific disordinal (crossover) pattern. If shoppers with taste goals merely ignored (or paid less attention to) nutrition information, the interaction pattern in Studies 1–2 would be different and would reflect an ordinal pattern. In contrast with prior research, we find that it is possible to find cases wherein those score lower on the diet intentions scale behave as if they may prefer higher-calorie food items—especially when attribute information is presented in an aligned manner. This point highlights a key contribution of this article.Second, prior research into shoppers’ food choices has tended to ignore the impact of display differences (and other contextual differences) related to the presentation of calorie information. Specifically, even as prior research posits that people with diet goals focus more on low-calorie items, it ignores the possibility that these effects may be weaker if lowercalorie food items are displayed in a nonaligned manner. In Studies 1 and 2, the effect of diet intention scores (or diet vs. taste primes in Study 2a) on choices and preferences is moderated by differences in attribute display. In essence, shoppers are more likely to make goal-consistent choices when food items are displayed in an aligned manner. This point is both new and nontrivial. Beyond the implications for practice (as we discuss subsequently), this finding may explain null results that arise when differences in diet intentions do not prompt different choices or preferences. For example, consider Study 2b. We reanalyze the data and, purely for illustrative purposes, median-split the diet intentions variable. If we consider just the two cells reflecting the nonaligned display condition, the relative choice shares for the low-sodium soup can were 39.7% (low diet intentions) versus 43.1% (high diet intentions) (c2( 1) = .15, p = .69). Examining just these cells might lead a researcher to conclude that differences in diet intentions do not affect shoppers’ choices. Yet when we consider the other two cells, which involve aligned food item displays, the relative choice shares shift to 22.2% (low diet intentions) versus 70.7% (high diet intentions) (c2( 1) = 30.65, p < .05). Thus, examining just the data pertaining to an aligned display would lead a researcher to a very different conclusion: that differences in diet intentions significantly affect shoppers’ choices. Both points are contributions beyond modifying and broadening the subtraction principle and are highlighted as such in Appendix A.The findings in this article also contribute to the numerical cognition literature. By examining the impact of display differences, we determine that vertical display differences lead to varying importance weights that shoppers assign to the attribute gap when making evaluations. Displaying a focal food item in an aligned (vs. nonaligned) manner, such as below (vs. above) comparison food items, leads shoppers to attach more importance weight to the attribute gap in their evaluations. We tested these effects not only within the food domain (Studies 1 and 2) but also in other domains (Study 3), indicating that these effects have predictive applicability across a wide variety of domains wherein advertising, retail displays, and online displays involve comparisons of numeric attribute information.Given concerns about obesity and associated health problems (Howlett et al. 2012), there is much interest in understanding when shoppers might use calorie and sodium information to make healthier choices. One challenge is to motivate shoppers to embrace diet goals, which can increase their consumption of healthier foods. However, assuming shoppers have diet goals, another challenge is to ensure that available calorie (or sodium content) information is displayed in ways that nudge shoppers toward healthier, rather than less healthy, food choices. Our findings suggest that display differences related to the location of food items can encourage shoppers’ healthy choices. As our studies indicate, diet-focused shoppers are more likely to make healthy choices if nutrition information is displayed in an aligned manner.Finally, and most importantly, this article outlines a parsimonious decision-making framework that examines how shoppers react to advertising, retail, or online displays in a wide variety of product domains involving comparisons of numeric attribute information. Building from, modifying, and expanding the work in Biswas et al. (2013), we identify two key elements that jointly determine shoppers’ evaluations: ( 1) whether shoppers perceive the attribute as an LIB attribute or a MIB attribute and ( 2) whether the attribute information is displayed in an aligned or nonaligned manner. Firms can use this framework to better design advertising, retail displays, and online displays. The effects in this article apply across a variety of product domains, as evidenced in the marketplace examples we cite and in the range of studies we present (Appendix A). Managerial ImplicationsDifferences in shelf displays affect shoppers’ purchase intentions (Grewal et al. 2011). As more firms adopt the voluntary FOP nutrition labeling system, “Facts Up Front,” and as more retailers display food items to showcase such FOP information, a key question is how retailers and category captains should organize the display of food items on retail shelves. The insights in this article offer some guidance. Imagine a retailer that wants to promote the new low-calorie Coca-Cola Life soft drink. If most shoppers (at this retailer) have diet goals, or if the retailer is able to prime diet goals through in-store signage or advertising, then, on the basis of this research, the retailer will increase sales by putting cans of Coca-Cola Life below cans of regular Coca-Cola. If a retailer primarily attracts shoppers with diet goals, but its profit margins are better on regular soups, then it might choose to put the regular soup cans below the lower-sodium soup cans to encourage relatively more sales of regular soup, despite its primarily “diet-goal” shopper segment. Finally, depending on food item categories, advertising, packaging, and so on, shoppers may have different goals. To the extent a retailer knows these goals, or to the extent firms can use advertising or packaging to prime such goals, firms can use display differences to increase sales of the more profitable products within the category. For example, if the candy category prompts taste goals, and if margins are higher on candy products with more calories, then retailers should display higher-calorie candy items above other candy-items to maximize sales of these more profitable products. Thus, the findings in this article can aid retailers and category captains as they optimize in-store shelf displays. Contingent on shoppers’ goals, numeric values relating to calorie content or to sodium, and relative food item profitability, retailers can display food items in ways that “push” certain high-profit food items over others. Similarly, the findings in this article may also inform how online retailers should display food items on their webpages and how supermarkets and grocery items should display food items on flyers.The effects we describe herein are driven by differences in (locational) displays of food items, which lead to differences in importance weight attached to food item attributes, with downstream consequences. Understanding this informationprocessing sequence has several implications for consumer welfare and public policy. First, we provide guidance for how diet and nutrition apps might be structured to help shoppers make healthy choices, noting that those who use such apps likely already have diet goals. When diet apps provide scores of food items, whether in grocery stores or restaurants, such diet apps should motivate shoppers to not only learn exact calorie/sodium information but also to give this information greater importance weight in their evaluations. Such efforts might help mitigate any negative impact arising from retailers’ use of nonaligned displays. Second, from a public policy perspective, young consumers and children are relatively unlikely to have diet goals (Burton, Wang, and Worsley 2015), and so using aligned food item displays may well backfire. Specifically, using aligned food item displays and/or explicitly prompting younger consumers to consider calorie/sodium information is likely to increase their preference shift toward higher-calorie/sodium food items. In such instances, regulatory policies governing advertising and menu signage should (seemingly counterintuitively) recommend using nonaligned display presentations and should (seemingly counterintuitively, but importantly) avoid prompting younger consumers to explicitly consider calorie/sodium information. These points highlight the role of attribute gap importance weight and distinguish this work from Biswas et al. (2013). Finally, both shoppers and policy makers need to recognize that marketers can present attribute information in ways that may mislead shoppers. For example, if a lower-calorie option involves smaller profit margins, a restaurant frequented by patrons with diet goals might display this option above a high-calorie option to reduce patrons’ weighting of the calorie gap and thus reduce patrons’ preference shift toward the lower-calorie option. Such a practice can be labeled as “providing full information” to patrons and is not illegal, but public policy experts would note that it may reduce welfare.The effects outlined in this article have widespread applicability, extending well beyond the food domain. As stated previously, these effects apply to any product domain involving a comparison of numeric attributes. Thus, for example, the effects would extend to any product domain wherein prices (typically an LIB domain) are displayed; into any domain involving MIB attributes such as battery life (e.g., cell phones, tablets, laptops) and Internet speed (e.g., cellular networks); and into domains such as robotic vacuum cleaners, which involve attributes that are MIB (e.g., operating time) and LIB (charging time).As a specific example, some insurance companies (e.g., Progressive) provide information both about their own rates and about the rate from a competitor. Progressive shoppers would behave similarly to shoppers with diet goals, in the sense that insurance shoppers generally prefer lower insurance rates (insurance rates = LIB attribute). In line with this article’s findings, we suggest that an insurance provider should present its own rate quote below the quote from a competitor. If its own rates are lower, this presentation format ensures that insurance shoppers put more weight on the “rate gap,” which increases relative preference for the focal insurance company’s product. If, however, the competitor’s rates are lower, presenting its own (higheramount) rates below would lead shoppers to put less weight on the rate gap, thus decreasing relative preference for the competitor’s insurance product.The advice in the previous paragraph is valid when the attributes involved are clearly LIB. However, other attributes may typically be perceived as MIB (e.g., network speed, battery life, operating time [in robotic vacuum cleaners]). In such instances, the focal firm should display its information above that of competition (exactly opposite of what is advised when the attribute is LIB). If its competitor’s “scores” are lower, this presentation display ensures that shoppers put more weight on the attribute gap, which increases relative preference for the focal firm’s product. If its competitor’s scores are higher, then this presentation display leads shoppers to put less weight on the attribute gap, thus decreasing relative preference for the competitor’s product.The contrast across the prior two paragraphs highlights the importance of identifying shoppers’ goals and identifying whether shoppers perceive the attribute as LIB or MIB. As we have noted, what constitutes the optimal display is conditional on how shoppers perceive attributes. It is for this reason that LIB versus MIB is one of the two independent variables in this article. Limitations and Future ResearchThe effects we find are driven by differences in displays of food items, which lead to varying importance weights assigned to numeric attribute information and further downstream consequences. We reiterate this important point because it helps clarify the conditions in which the effects we propose may be more versus less evident. To the extent that the effects in this article are driven by differences in importance weights, they may be more evident if the importance weight of numeric attribute information (e.g., calories, sodium, price) is neither too high nor too low. However, in certain conditions, such importance might reach high levels, and it is then that the effects we propose may be less evident. For example, if calorie attributes dominate choice (i.e., have very high importance weight), display differences likely have minimal effects, as a result of ceiling effects. Beyond the moderators identified inStudy 3b, these may also constitute moderators of our effects, and these are not predicted by Biswas et al. (2013).Future research could also further examine the informationprocessing mechanism outlined in H2 and H3. A key element of this mechanism is that subtraction calculations are perceived as more difficult when the smaller number is displayed above the larger number. Numerical cognition researchers may also examine whether individual differences related to numeracy, math anxiety, and so on may moderate the effects noted in H2–H3.In addition, there are two ways shoppers can perceive numeric attributes (vector attributes vs. ideal point attributes; see Green and Srinivasan 1978; Teas 1993). This article examines the case wherein shoppers perceive attributes as “vector” attributes, preferring either more of an attribute (MIB [e.g., battery life]) or less of an attribute (LIB [e.g., for diet focused shoppers, fewer calories/less sodium), but it does not examine what happens when attributes have ideal point characteristics. What happens if shoppers believe that an “ideal” number of calories for a sandwich is around 350 calories? Future research could explore whether the effects in this article sustain when such ideal points exist for key attributes.In this research, we assume that consumers who behave as if they prefer food items with more calories do so for taste-related reasons. But other reasons could also be operant, such as financial reasons that prompt some lower-income shoppers to prefer food items with more calories. Examining the behaviors of these shoppers is an important area for research, especially from a policy standpoint, to determine whether lower-income shoppers might prioritize calorie amounts over factors like nutrition or health.Building on a “healthy-left, unhealthy-right” intuition, Romero and Biswas (2016) propose that a food item without nutrition labeling is perceived as healthier if displayed to the left (vs. right) of a comparison food item. Among shoppers with diet goals, such a display increases the focal, healthy food item’s choice share. However, we propose that when FOP calorie information is shown, displaying the focal, healthy food item to the right of the comparison item (i.e., displaying the food items in an aligned display) may increase the importance weight that shoppers attach to the calorie gap during their evaluations and so would increase the choice share of this food item. Thus, presence of FOP calorie information may reverse Romero and Biswas’s (2016) results. Research that tests these competing predictions could contribute to both theory and practice.Implicit in our theory is that many shoppers embrace the unhealthy = tasty intuition. It would be worthwhile to reexamine these effects among populations (e.g., in France; see Werle, Trendel, and Ardito 2013) for whom this intuition may be weaker or even reversed. Finally, we only examine cases wherein attribute information is provided using numeric information. However, sometimes attribute information is provided using quasi-numeric formats, such as when Verizon contrasts its cellular coverage with AT&T using a map covered with more (vs. less) dots, without providing information relating to the actual number of dots. Would aligned (vs. nonaligned) display matter in such cases? Examining this and similar questions may further expand the applicability of this work.1In this section, we contrast a focal food item with fewer calories with a comparison food item with more calories. The theory advanced herein also extends to other comparisons between all types of items described using numerical attribute information.2Similarly, other, somewhat less well-known research has indicated that people often assume that the presence of increased sodium levels is associated with better taste (e.g., Henney, Taylor, and Boon 2010).3In Sweden, nutritional values are provided in kilojoules (kJ) and not in calories (1 calorie @ 4.18 kJ). APPENDIX A Programmatic Development of the Subtraction PrincipleaNonaligned presentation increases subtraction difficulty (Thomas and Morwitz 2009; see also Fuson and Briars 1990; Yip 2002). APPENDIX B Exemplar StimuliA: Study 1B: Study 2aC: Study 2bD: Study 3aE: Study 3bNotes: In Studies 1, 2, and 3a, aligned display conditions are shown. In Study 3b, the aligned display/harder calculation condition is shown.PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)PHOTO (BLACK & WHITE)  Reference   1  Andrews, J. Craig, Richard Netemeyer, and Scot Burton (1998), “Consumer Generalization of Nutrient Content Claims in Advertising,” Journal of Marketing, 62 (4), 62–75. 2  Bialkova, Svetlana, Lena Sasse, and Anna Fenko (2016), “The Role of Nutrition Labels and Advertising Claims in Altering Consumers’ Evaluation and Choice,” Appetite, 96, 38–46. 3  Biswas, Abhijit, Sandeep Bhowmick, Abhijit Guha, and Dhruv Grewal (2013), “Consumer Evaluations of Sale Prices: Role of the Subtraction Principle,” Journal of Marketing, 77 (4), 49–66. 4  Blair, Edward A., and E. Laird Landon Jr. (1981), “The Effects of Reference Prices in Retail Advertisements,” Journal of Marketing, 45 (2), 61–69. 5  Burton, Melissa, Wei Chun Wang, and Anthony Worsley (2015), “Demographic and Psychographic Associations of Consumer Intentions to Purchase Healthier Food Products,” Preventive Medicine Reports, 2, 21–26. 6  Cavanagh, Kevin V., and Catherine A. Forestell (2013), “The Effect of Brand Names on Flavor Perception and Consumption in Restrained and Unrestrained Eaters,” Food Quality and Preference, 28 (2), 505–09. 7  Dean, Sarah (2014), “Why Does Australia Need Another Coke? Drinks Giant’s New Green ‘Natural’ Drink Cuts Calories by 60 Per Cent Compared to the Classic Red Variety … and Doesn’t Have the Chemicals of Diet and Zero,” The Daily Mail (October 27), http://www.dailymail.co.uk/news/article-2810479/ Why-does-Australia-need-Coke-Drinks-giant-s-new-green-naturaldrink-cuts-calories-60-cent-compared-classic-red-variety-doesn-tchemicals-Diet-Zero.html. 8  Dhar, Ravi, and Klaus Wertenbroch (2000), “Consumer Choice Between Hedonic and Utilitarian Goods,” Journal of Marketing Research, 37 (1), 60–71. 9  Dodds, William B., Kent B. Monroe, and Dhruv Grewal (1991), “Effects of Price, Brand, and Store Information on Buyers’ Product Evaluations,” Journal of Marketing Research, 28 (3), 307–19. Escaron, Anne L., Amy M. Meinen, Susan A. Nitzke, and Ana P. Martinez-Donate (2013), “Supermarket and Grocery Store– Based Interventions to Promote Healthful Food Choices and Eating Practices: A Systematic Review,” Preventing Chronic Disease, 10, 120–56. Fuson, Karen C., and Diane J. Briars (1990), “Using a Base-Ten Blocks Learning/Teaching Approach for First- and SecondGrade Place-Value and Multidigit Addition and Subtraction,” Journal for Research in Mathematics Education, 21 (3), 180–206. Gomez, Pierrick, Carolina O.C. Werle, and Olivier Corneille (2017), “The Pitfall of Nutrition Facts Label Fluency: Easier-to-Process Nutrition Information Enhances Purchase Intentions for Unhealthy Food Products,” Marketing Letters, 28 (1), 15–27. Graham, Dan J., and Gina S. Mohr (2014), “When Zero Is Greater than One: Consumer Misinterpretations of Nutrition Labels,” Health Psychology, 33 (12), 1579–87. Green, Paul E., and Venkatachary Srinivasan (1978), “Conjoint Analysis in Consumer Research: Issues and Outlook,” Journal of Consumer Research, 5 (2), 103–23. Grewal, Dhruv, Kusum L. Ailawadi, Dinesh Gauri, Kevin Hall, Praveen Kopalle, and Jane R. Robertson (2011), “Innovations in Retail Pricing and Promotions,” Journal of Retailing, 87 (July), 43–52. Haws, Kelly L., Scott W. Davis, and Utpal M. Dholakia (2016), “Control over What? Individual Differences in General Versus Eating and Spending Self-Control,” Journal of Public Policy & Marketing, 35 (1), 37–57. Hayes, Andrew F. (2013), An Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach. New York: Guilford Press. Henney, Jane, Christine Taylor, and Caitlin Boon (2010), Strategies to Reduce Sodium Intake in the United States. Washington, DC: National Academies Press. Herman, C. Peter, and Janet Polivy (2004), “The Self-Regulation of Eating: Theoretical and Practical Problems,” in The Handbook of Self-Regulation: Research, Theory, and Applications, Roy F. Baumeister and Kathleen D. Vohs, eds. New York: Guilford Press. Howlett, Elizabeth, Scot Burton, Andrea Heintz Tangari, and My Bui (2012), “Hold the Salt! Effects of Sodium Information Provision, Sodium Content, and Hypertension on Perceived Cardiovascular Disease Risk and Purchase Intentions,” Journal of Public Policy & Marketing, 31 (1), 4–18. Keller, Punam Anand, and Ann L. McGill (1994), “Differences in the Relative Influence of Product Attributes Under Alternative Processing Conditions: Attribute Importance Versus Attribute Ease of Imagability,” Journal of Consumer Psychology, 3 (1), 29–49. Loewenstein, George (2011), “Confronting Reality: Pitfalls of Calorie Posting,” American Journal of Clinical Nutrition, 93 (4), 679–80. Maheswaran, Durairaj, Diane M. Mackie, and Shelly Chaiken (1992), “Brand Name as a Heuristic Cue: The Effects of Task Importance and Expectancy Confirmation on Consumer Judgments,” Journal of Consumer Psychology, 1 (4), 317–36. Mai, Robert, and Stefan Hoffmann (2015), “How to Combat the Unhealthy = Tasty Intuition: The Influencing Role of Health Consciousness,” Journal of Public Policy & Marketing, 34 (1), 63–83. Miyazaki, Anthony D., Dhruv Grewal, and Ronald C. Goodstein (2005), “The Effect of Multiple Extrinsic Cues on Quality Perceptions: A Matter of Consistency,” Journal of Consumer Research, 32 (1), 146–53. Mohr, Gina S., Donald R. Lichtenstein, and Chris Janiszewski (2012), “The Effect of Marketer-Suggested Serving Size on Consumer Responses: The Unintended Consequences of Consumer Attention to Calorie Information,” Journal of Marketing, 76 (1), 59–75. Monroe, Kent B. (1973), “Buyers’ Subjective Perceptions of Price,” Journal of Marketing Research, 10 (1), 70–80. Naylor, Rebecca Walker, Courtney Droms, and Kelly Haws (2009), “Eating with a Purpose: Consumer Response to Functional Food Health Claims in Conflicting Versus Complementary Information Environments,” Journal of Public Policy & Marketing, 28 (2), 221–33. Newman, Christopher L., Elizabeth Howlett, and Scot Burton (2014), “Shopper Response to Front-of-Package Nutrition Labeling Programs: Potential Consumer and Retail Store Benefits,” Journal of Retailing, 90 (1), 13-26. Oppenheimer, Daniel M. (2008), “The Secret Life of Fluency,” Trends in Cognitive Sciences, 12 (6), 237–41. Oppenheimer, Daniel M., Tom Meyvis, and Nicolas Davidenko (2009), “Instructional Manipulation Checks: Detecting Satisficing to Increase Statistical Power,” Journal of Experimental Social Psychology, 45 (4), 867–72. Raghunathan, Rajagopal, Rebecca Walker Naylor, and Wayne Hoyer (2006), “The Unhealthy = Tasty Intuition and Its Effects on Taste Inferences, Enjoyment, and Choice of Food Products,” Journal of Marketing, 70 (4), 170–84. Ramanathan, Suresh, and Geeta Menon (2006), “Time-Varying Effects of Chronic Hedonic Goals on Impulsive Behavior,” Journal of Marketing Research, 43 (4), 628–41. Roggeveen, Anne, Dhruv Grewal, Claudia Townsend, and R. Krishnan (2015), “The Impact of Dynamic Presentation Format on Consumer preferences for Hedonic Products and Services,” Journal of Marketing, 79 (6), 34–49. Romero, Marisabel, and Dipayan Biswas (2016), “Healthy-Left, Unhealthy-Right: Can Displaying Healthy Items to the Left (Versus Right) of Unhealthy Items Nudge Healthier Choices?” Journal of Consumer Research, 43 (1), 103–12. Sengupta, Jaideep, and Rongrong Zhou (2007), “Understanding Impulsive Eaters’ Choice Behaviors: The Motivational Influences of Regulatory Focus,” Journal of Marketing Research, 44 (2), 297–308. Shah, Anuj K., and Daniel M. Oppenheimer (2007), “Easy Does It: The Role of Fluency in Cue Weighting,” Judgment and Decision Making, 2 (6), 371–79. Stice, Eric (1998), “Relations of Restraint and Negative Affect to Bulimic Pathology: A Longitudinal Test of Three Competing Models,” International Journal of Eating Disorders, 23 (3), 243–60. Suri, Rajneesh, Jane Zhen Cai, Kent B. Monroe, and Mrugank V. Thakor (2012), “Retailers’ Merchandise Organization and Price Perceptions,” Journal of Retailing, 88 (1), 168–79. Teas, R. Kenneth (1993), “Expectations, Performance Evaluation, and Consumers’ Perceptions of Quality,” Journal of Marketing, 57 (4), 18–34. Thomas, Manoj, and Vicki Morwitz (2009), “Heuristics in Numerical Cognition: Implications for Pricing,” in Handbook of Research in Pricing, V. Rao, ed. Cheltenham, UK: Edward Elgar, 132–49. Van Herpen, Erica, and Hans van Trijp (2011), “Front-of-Pack Nutrition Labels: Their Effect on Attention and Choices When Consumers Have Varying Goals and Time Constraints,” Appetite, 57 (1), 148–60. Van Koningsbruggen, Guido M., Wolfgang Stroebe, and Henk Aarts (2013), “Successful Restrained Eating and Trait Impulsiveness,” Appetite, 60, 81–84. Visschers, Vivianne, Rebecca Hess, and Michael Siegrist (2010), “Health Motivation and Product Design Determine Consumers’ Visual Attention to Nutrition Information on Food Products,” Public Health Nutrition, 13 (7), 1099–106. Werle, Carolina, Olivier Trendel, and Gauthier Ardito (2013), “Unhealthy Food Is Not Tastier for Everybody: The ‘Healthy = Tasty’ French Intuition,” Food Quality and Preference, 28 (1), 116–21. Yip, Michael C. (2002), “Presentation Effects on Arithmetic Problem Solving,” Psychologia, 45 (2), 90–97. appendixesappendixes APPENDIX A  Programmatic Development of the Subtraction PrincipleTABLE:  a Nonaligned presentation increases subtraction difficulty (Thomas and Morwitz 2009; see also Fuson and Briars 1990; Yip 2002). APPENDIX B  Exemplar StimuliNotes: In Studies 1, 2, and 3a, aligned display conditions are shown. In Study 3b, the aligned display/harder calculation condition is shown.Copyright of Journal of Marketing is the property of American Marketing Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.Record: 11An Integrated Power and Efficiency Model of Contractual Channel Governance: Theory and Empirical Evidence. By: Carson, Stephen J.; Ghosh, Mrinal. Journal of Marketing. Jul2019, Vol. 83 Issue 4, p101-120. 20p. 1 Diagram, 3 Charts, 1 Graph. DOI: 10.1177/0022242919843914. Persistent link to this record (Permalink): http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=136892984&site=ehost-liveCut and Paste: <A href=""http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=136892984&site=ehost-live"">An Integrated Power and Efficiency Model of Contractual Channel Governance: Theory and Empirical Evidence.</A>"
7,"An Integrated Power and Efficiency Model of Contractual Channel Governance: Theory and Empirical Evidence Power theories (e.g., social exchange theory, resource dependence theory) and efficiency theories (e.g., transaction cost analysis) offer very different perspectives on the design of contractual governance in marketing channels. Whereas power theory suggests that governance will reflect the preferences of powerful firms, efficiency theories argue that governance will maximize joint value. In this research, the authors provide an integrative framework that reconciles power and efficiency perspectives in the context of contractual marketing channel relationships. This framework discriminates between two methods of exercising power: ex ante (through a tightly specified, efficient contract that rewards the powerful firm through the price mechanism while providing strong safeguards for the weak firm) or ex post (through a loosely specified, inefficient contract that allows the powerful firm to exploit its power during renegotiations). The authors argue that power will cause channel governance to deviate from the efficient choice, but only to the extent that the powerful firm cannot price out (i.e., extract) the value it offers to the weaker firm ex ante. As exchange conditions become more uncertain, power will demonstrate stronger effects on governance. This theory is supported with data from studies on contractual research-and-development relationships and procurement contracts for customized industrial products.KEYWORDS_SPLITThe design of contractual governance between independent parties in a marketing channel has been informed by two major but strikingly different perspectives: power and efficiency theories. The central claim of power theory is that observed governance will reflect the preferences of powerful firms ([ 2]; [27]; [35]). Efficiency theories, in contrast, argue that governance is chosen to maximize the joint value created in the channel as a whole, irrespective of the initial power differences between the firms ([ 6]; [68]; [73]). Because of this distinction, power and efficiency theories rarely converge on the same governance predictions ([35]; [71]). Indeed, one of the key motives underlying the development of efficiency theories like transaction cost analysis (TCA) was a desire to challenge the standard monopoly (power) argument put forth in applied price-theory approaches to industrial organization and antitrust ([48]). Accordingly, ""Greater respect for organizational (as against technological) and for efficiency (as against monopoly) purposes is needed"" ([71], p. 17).The debate between power and efficiency perspectives has been acrimonious at times, with some sociologically inclined power theorists decrying the spread of economic approaches to the study of organization (e.g., [23]). Many institutional economists have been equally derisive of power: ""The main problem with power is that the concept is so poorly defined that power can be and is invoked to explain virtually anything"" ([71], p. 238). Prominent organizational theorists have similarly described the construct of power as diffuse and disappointing ([51]). Indeed, as early as 1980, Stern and Reve distinguished between these two ""seemingly disparate"" disciplinary orientations—a primarily economic perspective rooted in industrial organization and focused on efficiency (e.g., [ 4]) versus a primarily behavioral perspective rooted in social psychology and focused on power and conflict (e.g., [24]; [64]). Even then, they expressed a concern about the lack of integration between the two viewpoints: ""Rarely have there been attempts to integrate these two perspectives. Indeed, they should be viewed as complementary, because the former deals mainly with economic 'outputs' while the latter is concerned with behavioral 'processes'"" ([65], p. 53).[ 5]Despite the coexistence of these two perspectives in the literature, most empirical work has adhered to a single perspective while doing little more than acknowledging the other (usually in the form of control variables). As such, meaningful integration of the two has been lacking. This indifference to the other perspective has led to certain limitations. For example, power theories have not been able to explain when and why powerful firms might choose governance that enhances joint value versus when they might deviate from this focus to accommodate their own self-interests. Likewise, efficiency theories have been mostly silent on how ex ante firm differences influence governance, although [29], [30]) governance value analysis is an exception. As a result, we have a limited understanding of how the two theories interact, the circumstances under which one or the other will be more determinative of channel governance, and the new insights that can be generated by considering the two within a unified framework.Constructing one such unifying framework in the context of contractual marketing channel relationships is the goal of this study. Our core thesis is this: power will cause governance to deviate from the efficient structure, but only to the extent that uncertainty prevents the powerful firm from pricing out the value it offers to the weaker firm ex ante. By ""pricing out,"" we mean that the price mechanism, rather than other aspects of the governance structure, is used by the powerful firm to extract the value of the unique resources it brings to the relationship. For example, when exchange conditions are less uncertain, rather than reducing or eliminating the safeguards provided to the weak firm, a powerful firm can instead charge a higher upfront price and couple this with the strong safeguards desired by the weak firm. In this sense, the price mechanism makes it unnecessary for the powerful firm to alter the ex post governance structure in its favor, leaving it in an efficient state. As such, when exchange conditions are less uncertain, power and efficiency theories will arrive at similar governance predictions. In contrast, when exchange conditions are more uncertain, we argue that the powerful firm cannot price out the value it offers and will therefore shift governance to its own advantage and away from the structure predicted by efficiency theories.To conduct a clear test of this power–efficiency framework, we require settings in which efficiency theories provide an unambiguous prediction that is then compromised by power as exchange conditions become more uncertain. Accordingly, we select two contexts in which the exposed specific investments are predominantly made by the buyer (in the technology/equipment supplied by the vendor), with comparatively limited (nonreimbursed) investments made by the vendor.[ 6] Given this one-sided pattern of exposed investments, the primary exchange hazard is also one-sided: potential opportunism on the part of the vendor against which TCA prescribes safeguards for the buyer.In both contexts, we focus on the safeguards contained in the ex ante contractual agreement. In the first, where clients sponsor technical development work by external contract research organizations (CROs), we investigate two sets of safeguards: price inflexibility and client-exclusive property rights over the technology. In the second, where industrial buyers purchase customized equipment systems from vendors, we investigate price inflexibility and performance guarantees as safeguards. These contexts enable us to provide evidence on the generalizability of our framework across substantively different governance forms: price inflexibility, property rights, and performance guarantees. Consistent with our thesis, in both contexts we find that when contracting with more powerful vendors, safeguards are weaker only in more uncertain environments. That is, both power and uncertainty must be present to produce an effect on safeguards and pull them away from efficiency-theory predictions.Our research makes several contributions. First, our integrative framework contributes to both literature streams by describing the conditions under which powerful firms will act to promote efficiency within their channels versus those in which they will bias governance toward their own self-interests and away from efficiency. The current TCA literature is generally silent on when and why firms might choose inefficient governance forms, and the power literature similarly cannot predict when firms will refrain from exercising their power. We also add to the power literature by describing the conditions under which powerful firms will apply their influence ex ante, at the governance selection stage, versus ex post, over activities and bargaining in the ongoing relationship.Second, our work builds on and complements a small body of research that has worked toward integrating power and efficiency theories, principally [37] and [62]. Both see power as interfering with the standard TCA predictions, but for different reasons. In the context of ongoing supply arrangements, [37] recognize the fundamental conflict between power and efficiency and view the presence of cooperative norms as critical to smoothing concerns about the abuse of ex post power—reverse opportunism in [47] terminology—and allowing the efficient transfer of control as predicted by TCA and property rights theory. Our approach differs in that we focus on ex ante power derived from extrarelational resources and develop the interplay between power and efficiency within the incomplete contracting framework itself, thus allowing us to address governance issues even in relationships not endowed with strong norms ([ 6]; [35]; [59]).[62] explore the benefits rather than the risks of power and argue that powerful firms are able to use their positions to better coordinate the relationship and lower transaction costs directly, thereby making costly integration less essential. This, in turn, dulls the empirical relationships between the standard TCA predictors and vertical integration. In their approach, power, when present, never fails to weaken the efficiency prediction. That is, there is no factor that inhibits the influence of power on governance and, as a result, their theory cannot predict the conditions under which powerful firms will refrain from influencing governance away from TCA predictions. We suggest a more nuanced interaction in which power moves governance away from efficiency, but only in sufficiently uncertain conditions.Third, relatedly, our work complements research on the governance value analysis (GVA) framework ([29], [30]) that also uses the incomplete contracting perspective to describe relationships between ex ante firm characteristics and governance forms. Whereas [29], [30]) consider how firms with differentiated (i.e., power-generating) market positions are motivated to choose contracts that protect these extrarelational resources from expropriation ex post, our framework sheds light on the flip side of this issue by considering how these firms can best exploit their power by discriminatingly aligning governance forms with different exchange conditions.Finally, one of the notable shortcomings of TCA is its incomplete treatment of bounded rationality and differences in firm preferences in the process used to select governance structures. Essentially, it is assumed that firms will somehow automatically ""find their way"" to the most efficient arrangement. This limitation has gone largely unremarked on since the foundational work of [11]. We contribute by analyzing how different contracting environments change the criterion used to select governance—power or efficiency.This article is organized as follows. We begin with a brief review of power and efficiency theories. We then develop our unifying framework in the context of contractual channel relationships for two distinct, but conceptually similar, inputs and draw refutable predictions that are tested with primary data. We finally discuss the implications for research and managerial practice. Theory The Power PerspectiveThe power perspective has shaped a significant body of research in marketing, having been used to predict channel performance (e.g., [27]), satisfaction (e.g., [28]), and conflict (e.g., [49]), among other phenomena. Power is most commonly defined as the ability of one party to bring about its desired outcomes by influencing the actions of others ([19]; [39]). Firms are more powerful to the extent they possess unique and difficult-to-imitate resources that make them valuable to potential partners (e.g., [14]). This value gives them a degree of influence over governance selection ex ante as well as activities and bargaining in the ongoing relationship ex post (e.g., [25]; [33]).The central principle of power theory is that powerful firms will use their positions to align governance structures with their own preferences (e.g., [27]; [35]). In many instances, powerful firms use their influence to organize the channel more efficiently (e.g., [32]; [62]; [65]). In this role, they not only provide information, expertise, and legitimacy to better coordinate channel activities but also resolve sticky channel management problems such as free riding that would otherwise increase transaction costs and reduce efficiency. One illustration of this coordinating role played by powerful firms is the decision control accorded to franchisors by less powerful franchisees ([50]). Franchisors can use this control to enforce territorial restrictions, prevent free riding off the efforts of other franchisees, direct system-wide promotional activities, and take other steps to enhance efficiency. At the same time, power theory recognizes that firms may use their positions in ways that do not enhance joint value, instead favoring themselves at the expense of weaker counterparties (e.g., [33]; [50]). For example, the practice of powerful franchisors forcing franchisor-specific investments on weak franchisees has been frequently cited as an ""unfair"" abuse of franchisor power ([47]). Similarly, supply relationships with powerful retailers are often heavily biased in the retailer's favor (e.g., [44]).Despite its intuitive appeal, the power perspective has three shortcomings. First, it is not clear as to when and why powerful firms would choose to play the role of enhancing joint value versus focusing primarily on their own payoff. Second, while power theories have focused on the role of ex ante power, they have failed to distinguish this from ex post power that arises from the ""fundamental transformation"" in which investments in specific assets lead to a small-numbers bargaining environment ([71]).[ 7] Thus, firms that are powerful ex ante may not always maintain a superior position ex post once they are locked into the relationship ([30]).The third and most significant shortcoming is that power theories do not use a comparative governance analysis to discriminate between alternative governance forms. A comparative governance analysis considers the choice between alternative governance forms simultaneously, rather than seeking to explain each choice in isolation as power theories do. This limitation is best illustrated by example. Suppose we wish to explain the use of a direct sales force as opposed to independent reps. The power of the firm relative to the salesperson might explain the decision to go direct (i.e., the more powerful firm forces salespeople into employment relationships that are subject to greater control and usually offer lower rewards than those the same individuals could earn as independent contractors). However, under a comparative analysis, it is not clear why vertical integration is the preferred way for the firm to leverage its power over salespeople. Instead, the firm could use its power to bargain for a significantly lower commission rate from independent reps. Power can explain both governance alternatives in isolation but does not discriminate between the two in a comparative fashion.[ 8]The need for such discrimination is especially critical in contractual channel relationships because they are more nuanced than the rather blunt instrument of vertical integration ([75]). In particular, the more powerful firm could force either an inflexible, tightly specified contract in which it extracts its value through the price mechanism by charging a premium price, or a more flexible, loosely specified contract that allows it to exploit its power and extract value during ex post renegotiations. Both options potentially increase its gains from the relationship; however, it is not clear which governance option the firm would choose and why. [50], p. 22) express this conundrum quite lucidly: ""It could certainly be argued that the more powerful channel member would not need an explicit contract because it would not need any safeguards; the weaker party could do little harm to it because of its less powerful position. The problem with this argument is that it goes against the strong empirical evidence that more powerful channel partners, such as franchisors, extensively use explicit contracts."" The Efficiency PerspectiveEfficiency theories such as TCA, property rights theory, and incomplete contracting theory have also driven an enormous body of literature in marketing, having been used to predict vertical integration (e.g., [ 1]), contract structure (e.g., [38]), features of relational exchange (e.g., [36]), partner selection (e.g., [67]), codevelopment alliances (e.g., [22]), and contracts for new product introductions ([15]), among other things. These theories rest on the premise that parties adopt governance structures that maximize joint value ([ 6]; [68]; [71]). Formally, efficiency refers to the minimization of the out-of-pocket and opportunity costs associated with producing and transacting in the channel; hence its equivalence to joint value maximization.[ 9]This focus on joint value implies that the chosen governance form is invariant to differences in ex ante firm power, because such differences do not affect the joint benefit obtained from adopting efficient governance ([74]). In other words, the fact that one firm has more power ex ante does not alter the optimal governance arrangement within the channel ex post. Rather, ex ante power only affects how the gains from adopting the efficient arrangement are divided among the parties, with more powerful firms extracting a greater share of this ""surplus"" ([34]; [68]). [11] and [68] illustrate how a system of ex ante transfers, or side payments, can be devised such that each firm will prefer the efficient ex post arrangement after taking the transfers into account. This is because joint profit is maximized under the efficient arrangement; thus, these transfers will always be of sufficient magnitude to compensate the counterparty for its losses in case it is (initially) worse off under the efficient arrangement. The most common forms of ex ante transfers in practice are the price one firm pays to acquire another in the case of integration or the agreed on transfer price (e.g., franchise fee) in a contractual channel setting.Efficiency theories, however, are subject to important shortcomings of their own that have received relatively little attention. In particular, they are largely silent on the process by which all parties come to accept the efficient governance structure. If efficient governance is Pareto optimal (i.e., makes all firms at least as well off as feasible alternatives), the issue is trivial. However, if efficient governance is not in the best interest of all parties—particularly powerful parties—how is it that they come to accept it? As [68], p. 463) observes, ""The process (the game) through which the possibilities for trade are explored is suppressed, the assumption being that it is optimal; or more precisely, that the parties will (somehow) find the optimal process."" Thus, even though [71], p. 174) uses natural selection arguments to defend efficiency when he says ""the [efficiency] argument relies in a general, background way on the efficacy of competition to perform a sort between more and less efficient modes and to shift resources in favor of the former,"" he acknowledges the limits of this assumption: ""This intuition would nevertheless benefit from a more fully developed theory of the selection process. Transaction cost arguments are thus open to some of the same objections that evolutionary economists have made of orthodoxy.""Crucially, the natural selection arguments invoked in TCA are not simply underdeveloped. They are in fact not well suited to relationships involving powerful firms, because departures from efficiency within the channel need not harm these firms. In particular, a powerful firm could be better off in a less efficient relationship that is biased in its favor, in which case the survival risk from the adoption of inefficient governance falls on the weaker, often replaceable, partner. Natural selection, therefore, cannot be a sufficient argument for efficiency when powerful firms are involved because there is no reason to assume that selection will somehow force a joint profit-maximizing outcome.Indeed, per the Coase theorem, an efficient solution is only feasible in a frictionless environment characterized by complete contracting where the parties can commit to both the ex ante transfer and to refraining from opportunistic bargaining ex post. In an incomplete contracting environment, there is no assurance that the efficient arrangement can be made acceptable to both parties ([11]; [70]). Said differently, the implementation of an efficient governance structure essentially comes down to whether one party—here, the weaker party—can pay the other party to accept it ([68]). In turn, the powerful firm must be able to credibly commit to the efficient governance structure and assure the weaker party that it will not exercise its power again ex post. If such credible commitments are not feasible, due to incomplete contracting, the efficiency argument against the influence of power breaks down. An Integrative Power and Efficiency FrameworkThe integrated perspective we develop next is generalizable to a wide variety of channel contexts; however, to provide an unambiguous empirical test of the resulting theory, we seek contexts in which the efficient governance structure can be clearly identified so that we can detect departures from efficiency due to power. We do so in two different channels, one in which client firms sponsor technical development work performed by external CROs (Study 1) and a second in which industrial buyers purchase customized equipment systems from vendors (Study 2). These contexts have two key features. First, unlike typical manufacturer–supplier or manufacturer–distributor settings, the nature of the core activity—research and development (R&D) and systems purchasing—is considerably discrete or lumpy. As such, each transaction stands on its own and long-run relational give and take (the ""shadow of the future"") is not of as much significance. This assures us that contractual terms cannot be trivially renegotiated without some risk of opportunism when ex post adjustments are required. Second, in both settings, the primary exposed specific investments are made by the client/buyer. This implies that the main exchange hazard is opportunism by the CRO/vendor against which the client/buyer requires a safeguard.Under these two key aspects of our settings, by examining the extent to which CRO/vendor power reduces safeguards for the client/buyer, we can see the effect of power on the efficient governance structure. To add concreteness in the development of the theory, we cast our hypotheses in the context of Study 1. We use the same logic to suggest predictions for Study 2.In Study 1, client firms sponsor technical development work performed by external CROs. In this setting, safeguards for the client take two primary forms: ( 1) firm fixed prices or prices subject to prespecified adjustment formulas to limit holdup during development and ( 2) property rights provisions assigning exclusive rights over the technology to the client to deter opportunistic leakage following development. We consider the effects of two key independent variables—CRO power, which is measured by the uniqueness of the CRO among potential suppliers at the outset of the relationship, and uncertainty, which refers to the extent of volatility and ambiguity affecting decision making in the exchange ([52]). To arrive at the key hypothesis based on our efficiency × power interaction, we first briefly describe the main effects of these variables as predicted by power and efficiency theories, respectively (recall that each theory is largely silent on the other variable).Power theory suggests that powerful firms will bargain for their preferred governance forms. In general, powerful firms will want to limit the safeguards provided for the weaker firm because such safeguards are ( 1) costly to provide and ( 2) limit the powerful firm's discretion to exploit its position ex post. In the context of Study 1, powerful CROs will generally prefer more flexible price contracts (e.g., with adjustment formulas negotiated ex post) not only to protect themselves from cost overruns but also to extract value from the client during ex post adjustments prior to transferring the technology. Similarly, they will prefer to secure some usage and ownership rights over the intellectual property developed in the relationship, because this intellectual property often serves as the basis for future development work ([ 7]). Expropriating this value is not feasible when prices are inflexible (i.e., fixed) and the client has exclusive property rights; therefore, powerful CROs have an incentive to bargain against such safeguards in the contract. Thus: H1:  CRO power is negatively related to the use of contractual clauses that (a) restrict property rights exclusively to the client and (b) specify inflexible prices, ceteris paribus.Turning to uncertainty, TCA suggests that exchange hazards increase in more uncertain conditions not only because ex post adaptations are larger and more frequent but also because of increased performance ambiguity due to measurement difficulties. Both of these conditions create greater latitude for opportunistic behavior and increase the need to safeguard exposed specific assets from expropriation ([ 1]; [71]). In our context, in which the main specific investments are made by the client, safeguards in the form of less flexible prices and the use of contractual clauses that assign property rights exclusively to the client are both expected to increase as uncertainty increases. Thus: H2:  Uncertainty is positively related to the use of contractual clauses that (a) restrict property rights exclusively to the client and (b) specify inflexible prices, ceteris paribus.Next, we turn to the critical interaction between power and efficiency theories. We start from the position that the powerful firm sets the governance structure in the relationship ex ante because it is unique and can bid down potential weak partners until one of them accepts its terms for participation ([50]). In selecting governance, the powerful firm faces two options. First, it could opt for efficient governance in which joint value is maximized. In our context, such an efficient governance structure would feature a tightly specified contract that provides strong safeguards for the client's investments while rewarding the powerful CRO by enabling it to extract (i.e., price out) the unique value it provides through the price mechanism ex ante. Alternately, the powerful firm may select an inefficient governance form that gives it an opportunity to maximize its own direct rewards from the relationship.[10] In our context, this governance structure would be more informal, with weaker safeguards for the client, thereby allowing the powerful CRO to exploit its power during ex post renegotiations.Which option will a powerful CRO choose? Note that the powerful firm must be at least as well off under the inefficient as opposed to the efficient governance structure; otherwise it would not choose the former. Because, by definition, joint value must be lower for the inefficient structure, the weak firm is necessarily worse off under the inefficient arrangement.[11] The weak firm will therefore prefer the efficient arrangement and has an incentive to offer an ex ante transfer in exchange for a commitment from the powerful firm to the efficient governance structure. In turn, the powerful firm will accept this only if it is better off following the ex ante transfer; otherwise it will prefer the inefficient arrangement.When conditions (i.e., low uncertainty) permit more complete contracts to be constructed, the first-best strategy for a powerful CRO will be to choose the efficient governance structure in exchange for an ex ante transfer of sufficient magnitude to make it better off. In such circumstances, both firms will prefer efficient governance, and ex ante power differences between the two firms will not cause a deviation from this choice because the powerful firm has extracted its value ex ante.In contrast, when complete contracts are more difficult to construct (i.e., high uncertainty), the picture is quite different. First, note that the weak firm will be reluctant to offer an ex ante transfer without a reciprocal commitment guaranteeing the powerful firm's actions ex post; otherwise it could be held up repeatedly. Recourse could be made at this point to the self-enforcing aspects of informal contracts based on reputation ([47]). However, given that the more powerful firm will typically have better prospects in the event of termination, informal contracts are not necessarily self-enforcing in situations where power is highly imbalanced. Importantly, this inability to rely on the self-enforcing mechanism actually works against the powerful firm by making it harder to commit to the efficient arrangement.Given this inability to commit, a powerful CRO will seek a second-best method to capture the value it offers to the client. It does so by implementing an inefficient governance structure with weak (or nonexistent) safeguards that maximizes its ability to extend its ex ante power into the relationship and expropriate value through ex post bargaining. Importantly, the powerful CRO is actually worse off in this case than in the first-best scenario (i.e., complete contracting where its power is priced out ex ante) because, given the inability to commit, the client's incentives to invest under an inefficient governance structure are lower. However, given the constraints on contracting, this second-best strategy is optimal from the powerful CRO's perspective.Note the distinction between our thesis and the classic power-oriented view. The latter suggests that the weak firm should accept the terms offered by the powerful firm, even if they are one-sided, because they either reduce uncertainty for the weaker party ([21]) or set parameters on what the more powerful party can do ([50]). In contrast, our argument recognizes not only the role of the price mechanism and its interplay with the choice of ex post governance but also the proactive role played by the nominally weaker party. In less uncertain conditions, the weak firm offers an appropriate transfer payment ex ante with the expectation that the powerful firm will credibly commit to limiting its influence ex post. The better the commitment offered (i.e., the closer safeguards are to the efficient level), the higher the initial transfer payment the weak firm is willing to make. In contrast, in more uncertain conditions, the powerful firm is unable to make such a commitment. As such, the option of pricing out power through an ex ante payment becomes untenable, making the powerful firm choose an inefficient governance arrangement to extract the value from its unique resources ex post. This enables us to tackle the [50] conundrum by explaining why powerful firms use tightly constructed explicit contracts in some situations and not in others. Thus: H3:  The negative effect of CRO power on the use of contractual clauses that (a) restrict property rights exclusively to the client and (b) specify inflexible prices becomes stronger as environmental uncertainty increases, ceteris paribus. Study 1: Outsourced R&D ProjectsOutsourced R&D, in which client companies sponsor R&D work by external CROs, is quite prevalent in high-technology, research intensive industries. In these settings, the client makes specific investments in a technology developed by the CRO, which is reimbursed for its client-specific development work. These relationships are governed by a development agreement, which is a formal contract specifying the nature of the deliverables, pricing terms, and property rights provisions for the technology and its derivatives ([ 7]). Given the one-sided nature of specific investments, the efficient governance structure will tend to feature strong safeguards for the client, which consist of less flexible pricing terms and client-exclusive property rights, as described previously. DataThe context of sponsored R&D projects is not well suited to the use of archival data for several reasons. First, very few data sets are available on outsourced R&D projects in general, and most secondary archives (e.g., the Deloitte/ReCap database) cover only large R&D alliances in narrowly defined industries. Second, our unit of analysis is an individual outsourced R&D project; thus, firm-level archival data would be only indirectly related to any given project and might introduce aggregation bias into the analysis. Third, certain constructs (e.g., the number of qualified contractors available at the outset of the project, financial investments in the project, uncertainty) are difficult to measure without relying on key informants. For example, uncertainty could be measured by volatility in industry or company sales, but these would be at best crude and indirect measures of the uncertainty surrounding an individual R&D project. Thus, we used our own survey instrument to capture the nuances of the context.Many of the constructs we measure have meanings that are well-known and widely shared among informants in the industry, such as firm fixed prices ([61]). We therefore use single-item, grounded measures (i.e., with defined metrics) to capture the constructs of interest wherever possible. As [61] and [16] observe, trying to introduce additional items into the measurement of such constructs can cause measures to drift from the well-defined meanings. We use psychometric measures only where a domain sampling approach is supportive of validity. In particular, the measure of uncertainty is best captured by a multi-item reflective scale due to the multifaceted nature of this domain, particularly as it relates to contractual incompleteness.Given our focus on high-technology, research intensive industries where outsourced R&D is common, we used National Science Foundation data to select the top five three-digit industries in terms of the percentage of firms outsourcing R&D: drugs and medicines; optical, surgical, and photographic instruments; communications equipment; motor vehicles and equipment; and aircraft and missiles. We used a national list broker to compile a list of 2,600 R&D managers with a single manager per firm. After eliminating 635 bad contacts, we contacted each manager to solicit participation. We used a snowballing technique to identify a qualified informant, even if different from the original contact. A total of 670 initial contacts could not be reached after extensive effort. Among the 1,295 individuals successfully contacted by telephone, 226 reported that no R&D was conducted in their unit and could not connect us with another manager in their company involved in R&D. A substantially larger number of 496 individuals were involved in R&D but indicated that their company did not outsource these activities. Again, this number includes only those instances in which a referral to a manager in a unit that did outsource R&D was not possible. Another 168 managers were involved with outsourced R&D projects but refused to participate. This left 405 qualified informants who verbally agreed to participate in the mail questionnaire. Follow-up phone calls and a second mailing resulted in 124 useable surveys with complete data on the variables included in this study. This yielded a 30.6% response rate based on the number of surveys mailed (124 of 405) or a 21.6% response rate based on the number of qualified informants identified (124 of 573). The former proportion is similar to the approximately 30% response rate in [36] and 37% in [30], though neither study distinguishes between the number of surveys mailed and number of qualified informants.[12] One drawback of operating in the context of outsourced R&D is that firms are sensitive to disclosing their R&D partners because this is often perceived as a source of competitive advantage. Thus, we were only able to collect data from the client side of each dyad. MeasuresWe provide the measurement items following purification and response scales in Web Appendix W1. The following subsections describe our measures. Price inflexibilityPrice inflexibility takes on a value of one if the contract features a firm fixed price or a fixed price with a prespecified adjustment formula and zero if negotiated adjustments or unspecified pricing arrangements such cost-plus are used instead ([10]). Client-exclusive property rightsProperty rights include ownership rights, usage rights, and rights to future improvements and derivatives ([ 7]). Note that these categories are not mutually exclusive; for example, a client may own the patents to a new technology, giving it residual rights of control associated with ownership ([34]), but certain explicit rights of usage may be granted to the CRO, or the client's usage may be restricted to particular fields of use. Thus, ownership does not preclude the importance of measuring usage rights or rights to derivatives. We employ a three-part measure of the exclusivity of client property rights over the technology developed under the contract consisting of ( 1) exclusive client ownership of patents over, or rights to patent, the technology; ( 2) exclusive client usage rights over the technology; and ( 3) exclusive client rights to derivatives based on the technology. A score of zero or one is assigned for each item and the scores are totaled. This measure is treated as an ordered categorical variable in the estimation. CRO powerOur measure of power is based on monopoly theory, in which power is a function of the availability of substitute providers. This measure has been previously used by [31]. Specifically, we measure the number of equally or better qualified CROs available to the client at the formation of the contract. We then reverse code this measure to create the score for CRO power so that a higher score indicates greater power. This measure is based on the perceptions of the client, which is appropriate as they also determine the influence the CRO has in the upfront bargaining. UncertaintyUncertainty is driven by a number of factors that combine to determine the overall challenge of operating in a given environment. Uncertainty is classically operationalized in terms of volatility and ambiguity (e.g., [52]). Volatility arises when conditions change frequently in ways that are difficult to predict ex ante ([41]), yet are readily understood ex post ([57]). Ambiguity, in contrast, arises when conditions are difficult to interpret ex post, irrespective of the rate of change ([52]). Because various aspects of volatility and ambiguity can lead to incomplete contracting conditions, we define the construct broadly in this study following a domain sampling approach. The four subdomains are market volatility, defined as the frequency of unanticipated changes in the market relevant to the technology ([36]; [41]; [66]); technological volatility, defined as the frequency of unanticipated changes in the technologies used in the development process ([36]; [40]; [66]); ambiguity, defined as the extent to which signals are open to multiple interpretations ([13]; [52]); and the level of creativity required to perform the development work (another aspect of ambiguity as described by [ 5]]), defined as the extent to which the intent of the project was to develop a meaningfully novel technology ([ 3]. Items for the market volatility scale were adapted from [56], [40], and [36]; items for the technological volatility scale were adapted from [36] and [40]; items for the ambiguity dimension are taken from [ 8] based on [13]; and the required creativity scale is based on [ 3].Critically, relationships affected by both volatility and ambiguity pose the greatest challenge for decision making ([ 9]). Therefore, we formulate a measure that increases as each aspect of uncertainty increases to capture the overall effect of the various dimensions. This approach offers the key benefit of smoothing the measure across the individual facets. To see the importance of this, consider a simple example. A relationship with low volatility may not be characterized by low uncertainty if ambiguity is high. We use a second-order factor model to provide support for the multidimensional construction of uncertainty and subject it to a confirmatory factor analysis following [43]. Fit statistics following purification suggest an inability to reject the proposed model (χ2(86) = 101.938, p =.116; goodness-of-fit index =.909; root mean square error of approximation =.037; Tucker–Lewis index =.970). The composite reliability of the measure is.925 ([69]). We do not include other indicators in the factor analysis because they are not reflective measures. We averaged the items within each subdomain and then equally weighted the subdomains to form the final score. The control variables used in this study are as follows: Client and CRO investmentWe assess client investment with a grounded measure of the client's total dollar investment in the work performed by the CRO. We used seven categories to measure expenditures: Less than $100,000, $100,000–$249,999, $250,000–$499,999, $500,000–$999,999, $1,000,000–$2,499,000, $2,500,000–$4,999,999, and $5,000,000 or more. Likewise, CRO investment is captured with a parallel measure of its nonreimbursed dollar investment in the project. The response categories for CRO investment are identical to those for client investment, except that we break the less than $100,000 category into two categories because CRO investments are often quite low: $0–$9,999 and $10,000–$99,999. Component supplyRepeated exchange can also support complex governance arrangements, making contractual safeguards less necessary. In development relationships, we often have continuity when the CRO is also the supplier of components (or products) based on the newly developed technology. In such cases, we expect more flexible prices and less exclusive client property rights. A single grounded item was used to measure whether the CRO was involved in a subsequent supply relationship with the client. CRO patentsContract research organizations often hold patents over technologies that are used to produce the deliverables. When such patents are involved in the development work, the CRO is both more powerful and far more likely to hold property rights to protect its interests. Thus, we expect more flexible prices and less exclusive client property rights. The use of the CRO's preexisting patents in the development work was measured using a single grounded measure. Client execution of development workEven uniquely qualified CROs at the start of the relationship will hold less power if the client also executes a portion of the work internally ([17]). This reduces the CRO's ability to distort governance away from efficiency, resulting in more extensive safeguards for the client. We measured the portion of the work on the technology performed in-house in four ways: as a percentage of dollar expenditures, value added, hours, and number of employees. Number of CROsIn large R&D projects, clients might employ multiple CROs to perform different portions of the overall work on a given technology (there are no cases of CROs performing the same work or ""R&D contests"" in our data). Because these CROs do different work, the involvement of multiple CROs does not necessarily make them more replaceable. However, the division of the work makes each CRO less important in a general sense, and thus less powerful, giving clients greater safeguards. We assessed this construct using a single grounded measure.Finally, the nature of the outsourced task was assessed with a grounded measure taking on a value of one if the task consisted of development work and zero if the task consisted of either basic or applied research. The location of the work was similarly captured by a control variable taking on a value of one if the CRO's work was performed at the client's location. EstimationClient exclusivity of property rights and price inflexibility were estimated using an ordered probit and a standard probit model, respectively.[13] The equations are: Client-Exclusive Property Rightsi= β0+β1CRO Poweri+β2Uncertaintyi+β3CRO Poweri×Uncertaintyi+β4Client Investmenti+β5CRO Investmenti+β6Component Supplyi+β7CRO Patentsi+β8Client Percent Dollarsi+β9Client Percent Value Addedi+β10Client Percent Hoursi+β11Client Percent Employeesi+β12Extended CROsi+β13Developmenti+β14Client Locationi+εi, and Graph1 Price Inflexibilityi=β0+β1CRO Poweri+β2Uncertaintyi+β3CRO Poweri×Uncertaintyi+β4Client Investmenti+β5CRO Investmenti+β6Component Supplyi+β7CRO Patentsi+β8Client Percent Dollarsi+β9Client Percent Value Addedi+β10Client Percent Hoursi+β11Client Percent Employeesi+β12Extended CROsi+β13Developmenti+β14Client Locationi+εi. Graph2The models involve interactions; therefore, we must pay attention to the zero values for the variables involved in the interactions when interpreting their main effects. In particular, the theory indicates that CRO power will have its strongest effect under high uncertainty. To showcase this, we center uncertainty so that the zero value corresponds to the maximum value of this variable in the data. This allows the main effect of CRO power to correspond to a situation in which it is theoretically expected to have its largest effect. Observe that this centering does not fundamentally alter the estimates, and we graph the full detail of the interactions next.We also account for potential endogeneity among the regressors. The most serious concern revolves around the financial investments made by both parties that are selected simultaneously with governance. To address this, we estimate the models using Gaussian copulas. Copulas are a class of function that allow the joint distribution to be constructed from the individual marginal distributions. The method is a semiparametric, instrument-free approach to handling endogeneity that models the joint distribution of the error term and the endogenous regressors directly, thereby accommodating correlation between them. This differs from instrumental variable methods in which exogenous instruments are used in an attempt to purge correlations between the error term and the endogenous regressors.Following [58], we assume that the error term is normally distributed following the probit specification. We then estimate the marginal distribution of each endogenous right-hand-side variable nonparametrically using an Epanechnikov kernel density function with [63] recommended bandwidth calculation. We use the two-step control function variant described by [58] because it extends easily to multiple endogenous regressors. Given kernel estimates of the marginal probability densities of the endogenous variables,  hˆ1(p1)  and  hˆ2(p2)  , we use numerical integration with the trapezoidal rule to calculate  Up1,i=H1(p1,i)=∫−∞p1,ih1(x)dx  and  Up2,i=H2(p2,i)=∫−∞p2,ih2(x)dx  . Given the Gaussian copula, the generated regressors are then calculated as  P1*=Φ−1(Up1)  and  P2*=Φ−1(Up2)  corresponding to the client's and CRO's investments. These variables are entered as additional regressors in the probit models, yielding consistent estimates of the coefficients of interest. Full details of the procedure can be found in [58]. ResultsDescriptive statistics and zero-order correlations appear in Table 1, Panel A. On average, the contracts had 1.13 provisions restricting some aspect of property rights exclusively to the client and 62.6% of contracts specify either a firm fixed price or a price subject to a predetermined adjustment formula. Prior to reverse coding the CRO power measure, the average is 1.603, indicating that most firms had one or two equally or better qualified CROs available, in addition to the one they used. This number is low enough to suggest that the typical CRO possessed a degree of monopoly power. The mean of uncertainty is 3.593 on a 1–7 scale. Most clients invested between $100,000 and $499,000 in the project, whereas the typical unreimbursed investment by the CRO was between $10,000 and $249,000. Table 1, Panel B, gives descriptive statistics and zero-order correlations for Study 2 (discussed subsequently).GraphTable 1. Summary Statistics and Zero-Order Correlations.  1 *p <.05.2 **p <.01.Several of the control variables in the analysis—component supply, CRO patents, and client location—define qualitatively different contracting regimes that may alter the relationships in the data. We examined this possibility and found only one interaction. When estimating the property rights model, we exclude relationships involving the CRO's preexisting patents (approximately 16% of the data) because, when their patents are used, the CRO is far more likely to hold property rights to protect its own interests, which limits the sensitivity of property rights to other aspects of the relationship. This does not affect the price inflexibility model, because intellectual property rights are not at issue.Coefficients and standard errors appear in Table 2. We use one-sided confidence intervals to test the a priori directional hypotheses. For consistency, we also use one-sided confidence intervals for the control variables. All models are estimated using robust standard errors.GraphTable 2. Client Safeguards for Sponsored R&D Projects (Study 1).  3 *p <.10.4 **p <.05.5 ***p <.01.6 Notes: IPR = intellectual property rights. Standard errors in parentheses.Column A of Table 2 presents results of the ordered probit model predicting the client-exclusivity of property rights. We find that CRO power has a negative and significant effect on the exclusivity of client property rights (β = −.671, p =.008), suggesting that more powerful CROs bargain for a greater share of property rights, in support of H1a. Uncertainty has a positive and significant effect (β =.558, p =.036), suggesting that clients retain more exclusive property rights as uncertainty increases and exchange hazards become more pronounced, in support of H2a. The critical interaction between CRO power and uncertainty is negative and significant (β = −.232, p =.032), suggesting that CRO power has a stronger negative effect on the exclusivity of client property rights under high uncertainty where power cannot be efficiently priced out ex ante, in support of H3a.[14]We graphically depict the relationship between CRO power and the client exclusivity of property rights in Figure 1, Panel A, over a range of two standard deviations above and below the mean of uncertainty (3.593). The interaction crosses over a little less than one standard deviation below the mean of uncertainty.Graph: Figure 1. Effects of power over ±2 SDs of uncertainty.The fact that the interaction crosses over is of substantive interest. In developing our theory, we emphasized the role that high uncertainty and the resulting contractual incompleteness play in allowing power to distort governance away from efficiency. However, we did not discuss how power might also alter the efficient governance adopted under low uncertainty. Even though power is priced out under such conditions and does not cause governance to deviate from efficiency, it may still influence the nature of the efficient structure itself. Recall that TCA predicts the use of stronger safeguards in the presence of factors that threaten joint value–enhancing outcomes. While largely ignored in the efficiency literature, ex ante power itself is one such factor. Thus, when dealing with a powerful partner, the weak firm may be willing to pay for stronger safeguards because it will fare worse outside the boundaries of the safeguard.To illustrate, pricing terms with negotiated adjustments usually specify that firms will negotiate in good faith over the final price based on the realized costs of the development work. These terms provide a degree of protection by framing the ex post bargaining problem. When firms have balanced power, such framing, in conjunction with price flexibility, provides a safeguard while limiting potential maladaptation costs associated with firmer pricing schemes ([30]). However, if power is imbalanced, more predetermined pricing terms (either prespecified adjustment formulas or firm fixed prices) become more efficient because the weak firm now requires more than framing to protect itself during ex post bargaining. The interaction in Figure 1, Panel A, shows that, in addition to biasing ex post governance in favor of the powerful firm (with lower safeguards for the weak firm) when conditions of incomplete contracting prevail (i.e., the right-hand side of the graph), greater power also appears to prompt an efficient governance arrangement featuring stronger safeguards for the weak firm when conditions of more complete contracting prevail (the left-hand side of the graph).Turning to the control variables, the client's financial investment is positively related (β =.197, p <.05) and the CRO's unreimbursed financial investment is negatively related (β = −.272, p <.05) to the exclusivity of client property rights. Work performed on the client's site involves more exclusive client property rights (β =.928, p =.066), though this result is only significant at p <.10. Similarly, as the number of CROs in the extended task environment increases, clients retain more exclusive property rights, as we anticipated (β =.056, p <.05). Of the four variables representing the percentage of the work performed in-house by the client, we find only that as the percentage of hours performed by the client increase, its property rights become more exclusive (β =.026, p <.05).The endogeneity correction term for the CRO's investment is significant at p <.10. This serves as a Durbin test for the endogeneity of this variable. Interestingly, the CRO's financial investment is endogenous, whereas that of the client is not. We suspect that this is because the latter is largely a function of the desired technology. The correlated portion of the effect is captured in the negative coefficient on  P2*  , leaving a weaker coefficient reflecting the portion that is not affected by endogeneity. The main limitation of the copula method is that the distributions of the endogenous variables must differ from that of the error term. If the distributions are too similar, multicollinearity between the generated regressors and the error term will reduce efficiency, though it will not introduce bias. The level of statistical significance in the results suggests that this is not an issue in the present investigation.Results of the probit model predicting price inflexibility appear in Column B of Table 2. Consistent with H1b, we find a negative and significant effect of CRO power (β = −.691, p =.017). Consistent with H2b, we find a positive coefficient for uncertainty (β =.525, p =.073), though this result is only significant at p <.10. The interaction between CRO power and uncertainty is negative and significant (β = −.252, p =.032), which supports H3b. Again, CRO power has a stronger effect on governance under high uncertainty in which power cannot be efficiently priced out ex ante.[15]Figure 1, Panel A, also depicts this relationship graphically. Again, the interaction crosses over at a little less than −1 standard deviations in uncertainty. The interpretation is identical: under more complete contracting conditions, safeguards for the client are stronger when dealing with a more powerful CRO. Turning to the control variables, we see that the client's financial investment is positively related (β =.207, p <.05), whereas development tasks (β = −.770, p <.05), CRO patents (β = −.551, p =.089), and the percentage of hours performed by the client (β = −.019, p =.068) are all negatively related to price inflexibility. All other effects are insignificant.Finally, Column C of Table 2 presents results for the ordered probit model on price inflexibility. The coefficients are of similar magnitude and significance.[16] Study 2: Purchase Contracts for Industrial SystemsBuyers of industrial systems frequently purchase items from independent vendors that are customized to their operational needs and based on proprietary vendor technologies. The reliance on a specific vendor creates a dependency for the buyer and vulnerability to vendor opportunism because investments are sunk before performance is realized. Vendors usually build the development costs into the final price; thus, they do not have significant exposed investments for buyer-specific customization. Contracts governing these purchases usually consist of the necessary features of the customized product, the pricing terms, and clauses pertaining to performance guarantees for how the system will function once installed.We investigate the power × efficiency interaction on two forms of safeguards in these relationships: inflexibility of the pricing terms before/during supply and whether the vendor commits contractually to a performance guarantee of its system after supply. As in Study 1, ex ante vendor power is measured by the number of equally qualified vendors available at the outset of the relationship. Because there is generally less ambiguity in the production of industrial systems than R&D, we use a more traditional measure of uncertainty for studies in the channels literature, volatility.Similar to Study 1, we expect that powerful vendors will prefer more flexible price contracts (e.g., with adjustment formulas negotiated ex post) to protect themselves from cost overruns and will also try to avoid providing costly performance guarantees to buyers (H1), especially in more volatile conditions in which the features necessary to provide a certain level of performance can shift. Likewise, having committed to procuring customized equipment, buyers will seek safeguards in the form of fixed prices and performance guarantees as uncertainty increases (H2). The logic for the key power × efficiency interaction (H3) is similar to that in Study 1. In particular, when uncertainty is low, a powerful vendor will be willing to provide the safeguards that the buyer seeks—fixed prices and performance guarantees—in exchange for an ex ante transfer that enables it to capture the unique value it provides the buyer through the price mechanism. In contrast, when uncertainty is high, the powerful vendor cannot offer a credible guarantee that after having received a transfer payment ex ante it will not create a holdup problem ex post. It will also be more reticent to provide performance guarantees given uncertainty surrounding the necessary features/technology. Given this infeasibility to price out its power ex ante, the powerful vendor will seek governance forms with weaker safeguards that enable it to expropriate value through ex post bargaining. Thus, the expected relationships are identical to Study 1, with the exception of examining performance guarantees rather than client exclusive property rights. DataWe examine four industry sectors: industrial machinery and equipment, electrical and electronics equipment, transportation equipment, and instrumentation. Our sampling frame consisted of 1,900 firms with an identified sales manager working at each firm. We used a key informant methodology and a snowballing technique to identify and qualify an informant in each firm. This qualification effort yielded 926 valid informants who were willing to participate in our study and were mailed the survey instrument. Follow-up telephone calls and reminder cards resulted in 288 responses on the variables used in this study for a response rate of 31%, which is comparable to the 30% response rate in [36] and 37% in [30]. MeasuresTo the extent possible, we used grounded measures for our constructs. The reflective measure for uncertainty is based on [36]. The measure of ex ante vendor power is the number of equally qualified vendors available to the buyer for the focal system. This measure is negatively correlated with two measures of vendor reputation in our data: ""Customers are willing to pay a high premium for our products and services"" (−.315; p <.05) and ""Customers value our products and services more than that of our competitors"" (−.246; p <.05). These results give us confidence that our measure of vendor power reflects unique value-added resources that the vendor brings to the table in these relationships.Web Appendix W2 provides details of our measures following purification. The control variables include measures of buyer and vendor investments, importance of the product, several variables reflecting the shadow of the future, and industry dummies. EstimationWe apply the same correction procedure for endogeneity as in Study 1. The included regressors in this case are both highly insignificant, suggesting that no correction is needed. Therefore, we omit these terms for simplicity. ResultsDescriptive statistics and correlations appear in Table 1, Panel B. Table 3 provides the results. Column A shows the results of the probit model for price inflexibility. The effect of vendor power is negative and significant (β = −.155, p <.05), indicating that more powerful vendors bargain for more flexible prices. The effect of uncertainty is positive and significant (β =.095, p <.05), suggesting that when this factor is higher, buyers seek the safeguards provided by less flexible prices. The key interaction effect between vendor power and uncertainty is negative and significant (β = −.036, p <.05). Thus, under high uncertainty, in which power cannot be efficiently priced out ex ante, powerful vendors pull governance away from the efficiency theory prediction.GraphTable 3. Client Safeguards for Purchase of Industrial Systems/Equipment (Study 2).  7 *p <.10.8 **p <.05.9 ***p <.01.10 Notes: Standard errors in parentheses.Column B in Table 3 shows the results for the probit estimation on the inclusion of a performance guarantee. The effect of vendor power is again negative and significant (β = −.162, p <.05) and the effect of uncertainty is positive and marginally significant (β =.089, p =.078). The interaction of vendor power and uncertainty is directionally consistent with expectations but misses significance narrowly (β = −.040; p =.130). Examining Figure 1, Panel B, we see that the effects of vendor power on price inflexibility and performance guarantees crossover at approximately one standard deviation below the mean of uncertainty (4.094). This is much the same as what we see in Study 1 and similarly indicates that vendors with greater power are willing to opt for efficient governance and offer buyers even stronger safeguards when environmental uncertainty is low (i.e., when conditions for more complete contracting prevail).Overall, the results are consistent with our thesis that powerful firms shift governance away from the efficiency prediction, but only when conditions (e.g., high uncertainty) do not allow them to price out the value they provide their customers ex ante. DiscussionWe offer a generalizable framework that integrates power and efficiency perspectives in contractual marketing channel relationships and validates the importance of ex ante power as a determinant of channel governance. Rather than summarily dismissing power, as is common in efficiency arguments, we incorporate power into the comparative governance analysis used in efficiency theories to explain the role it plays in governance design. Specifically, we argue that power affects governance only under conditions of sufficiently high uncertainty (i.e., when conditions prevent the powerful firm from pricing out the value it offers to the weaker firm ex ante). Results from two separate contexts, involving three different contractual safeguards chosen by these firms, provide support for this argument. Because we do not treat either power or efficiency as ""dominant"" in our framework a priori, we arrive at a richer understanding of the conditions under which each will be more influential.Crucially, we identify a factor—uncertainty—that can inhibit the impact of both power and efficiency on governance. When uncertainty is low and contracting is more feasible, the powerful firm has an incentive to support efficient, joint value maximizing governance. It can secure the value it offers to its partner ex ante through the price mechanism while simultaneously providing a safeguard by foregoing its influence on ex post governance. As a result, the impact of power on governance is inhibited. In contrast, when uncertainty is high making contracts more incomplete, the powerful firm cannot price out the value it offers. Thus, it purposefully seeks an inefficient arrangement that enables it to claim value ex post under more informally specified governance. As a result, the role of efficiency is inhibited. Theoretical Implications Contributions to power theoryOne weakness of power theory has been its inability to discriminate between governance choices using a comparative analysis. As described previously, in the R&D context, a powerful CRO could exercise its power by limiting ex post safeguards for the client—for example, by insisting on flexible prices and nonexclusive client property rights. Alternatively, it could attempt to extract the full value it offers to the client by requiring it to pay the highest possible (fixed) price ex ante and then providing strong safeguards, such as exclusive and strongly protected client property rights, to maximize the client's incentives to invest in the relationship. Both seem equally plausible.By employing the comparative governance lens used fruitfully in efficiency theories, we simultaneously compare the role of power on each of the two feasible alternative governance forms. We then show that under less uncertain conditions, the powerful firm provides the client with efficient safeguards ex post (and prices out its power ex ante). Under more uncertain conditions, this mechanism becomes infeasible, forcing the powerful firm to use a second-best strategy by seeking gains ex post through flexible contractual terms (e.g., flexible prices). In contrast, in [62], the powerful firm always shifts away from integration because its power gives it the legitimacy to impose value-creating actions while avoiding the high costs of integration. Despite this difference, both studies show that power and efficiency are not simply competing explanations but that ex ante power can influence ex post governance systematically under key exchange conditions. Contributions to TCAOne weakness of efficiency theories such as TCA has been that they have ignored the role of ex ante resource and power differentials on governance under the premise that boundedly rational actors with conflicting preferences will somehow ""naturally select"" the path to efficient governance. Said otherwise, inefficient arrangements will die out, supporting [72] dictum that economizing is the best strategy. Our approach instead attends to the microanalytic process particulars ([71]) and showcases the procedure used to adopt governance. Specifically, the process underlying our theory suggests that natural selection is not sufficient because the powerful firm, even though desiring of efficient joint value–maximizing solutions, would choose inefficient governance to capture value under conditions (uncertainty) which prevent it from securing the value derived from its power ex ante and credibly offering efficient safeguards. Contributions to formal contractingThis literature, predominantly based on efficiency theories, has investigated a wide variety of governance challenges in marketing channels. These include contractual completeness (e.g., [10]; [55]), contract duration (e.g., [42]), the assignment of property rights ([ 7]), the use of exclusive territories ([18]), joint ventures ([38]), franchise contracts (e.g., [46]; [45]), price and design specifications ([30]), and more. We contribute to this literature in several important ways.First, our work emphasizes the importance of considering both the terms of the contract and the initial price. Analyzing one without the other can be misleading. For example, a powerful firm may grant generous (efficient) safeguards to a weaker partner but in turn demand a higher upfront price. Indeed, our results in Figure 1 show that the safeguards provided for the weaker firm may be stronger in the presence of a more powerful partner when complete contracts can be written (i.e., the left-hand side of these graphs). Looking at these safeguards without considering the initial price would lead to incorrect inferences about the balance of power in the relationship and the motives of the parties.Second, as a complement to studies such as [30] and [55], our work emphasizes the need to focus on both the ex ante and ex post features of governance to understand the different ways they interact in particular contexts. For instance, whereas the [30] GVA model is concerned with the protection of value creating assets that get intermingled in supply relationships, we focus on how powerfully resourced firms leverage their power. In particular, the clients/industrial buyers in our contexts contract with the CROs/vendors precisely because they want these CROs/vendors to build for them technologies/systems based on their technical capabilities. As such, there are limits to the reverse holdup problem that GVA focuses on because a CRO/vendor that fears leakage could simply decline to offer the technology ex ante. Given this, the exercise of power, rather than the protection of rent-generating assets, becomes the key motive for the CROs/vendors. The invocation of these alternative facets of power suggests a complementarity between these approaches that is context dependent and warrants future attention. For instance, one fruitful direction would be to predict the conditions under which the drive to exploit power-generating assets would trump the need to protect the value of these assets from expropriation and vice versa.Third, we extend [47] analysis to illuminate the nature of ""unfairness"" expected in formal contracts. In particular, we suggest that ""unfairness"" in formal contracts due to an imbalance in power will primarily be reflected in the price, not other terms. Why? Our analysis has shown that a more complete contract is necessary to price out the influence of power ex ante and that such a strategy is first-best for the powerful firm when feasible. Importantly, this contract is first-best only because it maximizes the investment of the weaker firm, which is achieved by providing a credible commitment to treat it fairly by providing appropriate safeguards. If environmental conditions are such that a complete contract can be written, it will tend to reflect this pattern, with fair treatment of the weaker firm ex post but power reflected in the seemingly ""unfair"" price ex ante. Yet this price is merely a symptom of the overall fair treatment of the weaker firm. It is, of course, a matter for policy makers to decide at what point the price is actually unfair or merely a reflection of the value offered by the powerful firm.Finally, as power becomes more balanced or symmetric across firms, the bargaining at relationship formation will be influenced more evenly and is likely to draw governance closer to joint value maximization. Importantly, this means that incomplete contracting alone should not result in serious deviations from efficiency without differences in firm power. More equally placed firms should progress toward a more mutually beneficial (albeit informal) governance structure, even under incomplete contracting conditions. It is only when contracts are incomplete and one firm has a power advantage that it can push for an arrangement that allows it to exploit its power ex post and major deviations from efficiency are expected. We summarize this relationship in Figure 2. When uncertainty is low, under both balanced and unbalanced power conditions, governance is both formal and efficient, because parties can construct ex ante transfers along with efficient contractual terms. In contrast, when uncertainty is high and power is balanced, the ex ante transfers necessary to support efficient contracts cannot be constructed and parties have to resort to informal governance. Such informal governance is efficient under balanced power conditions since negotiations will naturally tend toward more mutually beneficial outcomes. It is only when power is imbalanced and contracting incomplete that we expect to see informal arrangements which inefficiently favor the powerful firm.Graph: Figure 2. Firm power, uncertainty, and governance. Contributions to informal contractingWhen complete contracts cannot be written, relationships will be governed more informally, with greater latitude afforded to the firms ex post. That is, these relationships will feature contracts without fixed prices, without exclusive property rights provisions for the client (even though the client has paid for the R&D effort), and without performance guarantees. Thus, they will rely on more informal, relational, implicit contracts that lack legally binding safeguards. Importantly, while the literature on informal governance tends to associate it with the presence of relational norms or a shadow of the future, our analysis suggests that informal governance could also be imposed by a powerful firm if it cannot provide the credible commitment necessary to price out its power ex ante. Crucially, it is imposed specifically to expropriate value from the weaker firm during ex post bargaining. In this case, informal governance is neither efficient nor indicative of a cooperative, mutually oriented relationship; it is simply a second-best, but feasible, choice.Our framework can also be used to shed light on certain aspects of informal governance that have not received much attention in the literature. One such issue is whether relational norms in a given context are flexible (more incomplete social contracts) or more rigid (more complete social contracts). In [37], norms facilitate the transfer of control while constraining the actions of the controlling firm; thus, they may be viewed as promoting rigidity in the social contract. In our contexts, informal governance will involve price and behavioral expectations, even though not formally specified. A party that offers generous compensation to its partner ex ante might expect the relationship to unfold in a certain manner; it would not expect or tolerate ex post bargaining when adjustments are required. In this sense, the informal governance structure (social contract) is more rigid. In contrast, a party holding the line on compensation may anticipate some degree of resistance and bargaining ex post that the parties recognize as more legitimate. Here, the social contract is more flexible. Managerial ImplicationsThe results provide guidance to managers overseeing contractual channel relationships. For managers of powerful firms, we offer a new way of thinking about the role and exercise of power. It complements the prevailing view that power allows the firm to influence the ex post behavior of others within the relationship by suggesting that managers must also recognize and use the price mechanism to extract the value provided by their unique resources. Indeed, where feasible, the pricing out of power ex ante coupled with a credible commitment against using power to influence behavior ex post is optimal not only from the weak firm's perspective but from the powerful firm's perspective as well.This claim is challenging for managers, because to generate the highest initial price, the powerful firm must tie its own hands as a means of guaranteeing a favorable environment for the weaker firm ex post. We suspect that most managers of powerful firms would instead default to trying to exercise their power ex ante when setting the price and keeping their options open to influence behavior and bargaining ex post. However, this approach is suboptimal if the contracting environment allows them to credibly commit to not doing so. It is only when ex ante transfers are infeasible that the powerful firm should shift its focus ex post toward influencing partner behavior and realizing gains from bargaining.Managers at weak firms, in turn, should expect to pay a higher price up front in exchange for more favorable, efficient conditions ex post. Here again, a shift in thinking from traditional power arguments is warranted—the weak firm should try to negotiate such an outcome rather than accept an inefficient arrangement on the grounds that it either reduces uncertainty ([21]) or sets parametric bounds on what the more powerful party can do ([50]). The weak firm should also exercise caution in relying on the self-enforcing aspects of the implicit or relational contract given the powerful firm's superior options ex post in the event of termination.Our prescriptions allow managers to understand a wide range of ways in which firms exercise their power. For example, the franchisee bills of rights included in many franchise arrangements at first appear to be at odds with the power of the franchisor. Viewed through the lens of traditional power theory, they call into question whether the franchisor's position is really dominant. Viewed through the lens of our theory, well-specified franchisee rights are a tool for the powerful franchisor to credibly commit to fair treatment of the franchisee ex post. This safeguards the franchisee's investment in the relationship and enables the franchisor to charge a higher up-front fee when the franchise agreement is constructed. This arrangement cannot be properly understood without considering both the contractual terms and the price mechanism.Managers can also benefit from realizing that, while the feasibility of arrangements involving credible commitments is primarily a function of the uncertainty surrounding the exchange, it is also subject to managerial influence and choice. For instance, more complete contracts can still be written in many ""complex"" situations at a cost ([10]; [55]). The first-best and second-best alternatives identified in this study suggest that the costs of writing more complete contracts ex ante may be justified, particularly in relationships involving powerful firms (see Figure 2), to support the greater efficiency associated with the pricing out of firm power.Finally, managers should consider the uncertainty that surrounds their focal task when making outsourcing decisions. More complex and uncertain tasks will tend to be governed less efficiently than simpler more programmable tasks in relationships involving powerful firms (see Figure 2). Thus, when powerful providers are involved, uncertain tasks at firm boundaries should be internalized to a greater extent, because powerful external providers will distort governance arrangements in their favor. Weaker providers, in contrast, allow more efficient outsourcing of even uncertain tasks, and smart outsourcing firms may recognize this when selecting partners and see the value of weaker firms. This may explain, in part, the highly fragmented nature of many industries providing outsourced services (in addition to conflict of interest issues). The fact that weaker firms are well advised to avoid powerful providers may present a barrier to the advancement of weaker outsourcing firms within their industries by deterring their involvement with leading providers. Antitrust ImplicationsWe also offer a brief consideration of the implications of our theory for antitrust policy, because the fundamental contrast between power (monopoly) and efficiency that we consider exactly mirrors the central debate in the antitrust literature. Antitrust regulation has historically focused on restricting the exercise and extension of monopoly power. Although the latter concern has been largely discredited in the academic literature absent the ability to price discriminate ([60]), it still plays a prominent role in court opinions and public policy discussions. As a result, so-called nonstandard modes of organization have been viewed by courts and regulators with extreme suspicion ([12]) due to the fundamental assumption that these nonstandard forms result from attempts to capitalize on firm power rather than efforts to enhance efficiency. The thrust of TCA (and new institutional economics more generally) has been quite different, with Williamson presenting compelling arguments against this ""inhospitality tradition"" (1985, p. 19) and suggesting that nonstandard arrangements other than cartels can serve ""affirmative economic purposes"" (1985, p. 200). The upshot of these arguments is that monopoly power should be less at play in determining nonstandard modes of organization than efficiency. As [53] observes, the Chicago-school approach to vertical restraints associated with Posner, Peltzman, and others is based on these same arguments.We argue for a nuanced view and suggest that the relative influence of power and efficiency rests on the ability of firms to price out their power differences ex ante. Absent this ability, powerful firms cause inefficiencies in channel governance. From an antitrust standpoint, the key observation is that the efficiency argument for nonstandard modes of organization serving affirmative economic purposes is undermined if governance is known to systematically deviate from efficiency in certain conditions as we illustrate. In particular, in highly uncertain environments, power leads to distortions in governance that favor the more powerful firm. Here, nonstandard arrangements involving powerful firms should rightly fall under suspicion—they cannot be presumed to support efficiency purposes. In contrast, in less uncertain conditions or when more evenly placed firms are involved (see Figure 2), even seemingly nonstandard arrangements are likely to support efficiency within the channel because this is the best option for all firms.The issue becomes one of analyzing the incentives of each party to implement an efficient agreement and the means by which powerful parties can be made whole when efficient arrangements work to their disadvantage, as we have shown that they do. Because TCA removes ex ante power as a main-case explanation of governance and glosses over the process by which firms with conflicting preferences arrive at efficient arrangements, it tends to provide an undiluted view of complex arrangements as efficiency enhancing. Boundary Conditions and LimitationsOur framework is subject to several important boundary conditions. First, we assume that the institutional environment contains a well-functioning judiciary so that court ordering can be used to enforce the contractual commitments made. Otherwise, the powerful firm cannot price out its power using an ex ante contract, even under low uncertainty, because the weak firm cannot rely on enforcement of the contractual safeguard. In this case, power will have a larger influence on governance. This can be an important consideration in international relationships located in countries with a weak rule of law, which will be influenced more by power. In this case, either outsourcing firms would be unwilling to undertake value-enhancing investments specific to the CRO/vendor or, as a corollary, only powerful outsourcing firms with political connections would be willing to do so.Second, relationships in our contexts are not embedded in repeated exchange; thus, the reputational feature of the shadow of the future is not available. If it were, it would allow joint value–maximizing governance to be adopted even when complete contracts could not be written. In this case, environmental uncertainty would have a limited influence on the way power is exercised. Importantly, relational exchange would not necessarily affect the degree to which power is exercised. Much as [37] observe a transfer of control consistent with the predictions of TCA even in the presence of relational norms, we would expect power to be exercised in highly relational exchanges because the powerful firm deserves a return on its unique resources. However, the powerful firm would not cause a departure from joint value–maximizing governance to do so and would exercise its power through the mutually agreed on price rather than through unilateral expropriations ex post.Third, the theory assumes that the interests of power and efficiency are at least partially at odds with each other. The assumption might not hold if the weak firm is not called on to make any specific investments and therefore does not require any safeguards. In this case, both power and efficiency will encourage either safeguards for the powerful firm if it makes a specific investment or simple market exchange (with power reflected in the price) if neither firm makes a specific investment.Fourth, our study focuses on contexts with one-sided exposed specific investments that give us a clear wedge to showcase the tension between power and efficiency. This is, however, not a boundary condition on the theory itself, which applies more generally, subject to the aforementioned considerations. In particular, the power of any focal party will tend to reduce the level of safeguards for the other party under incomplete contracting conditions because the price mechanism is not available to partial out the effect of power. It will do so to a lesser degree or not at all when more complete contracting is feasible.Turning to limitations, we did not measure efficiency with a direct measure of costs, instead relying on a reduced-form analysis to show the effect of power on the safeguards predicted by efficiency theory. An attractive avenue for future research would be to directly address implications on governance costs. However, this would almost certainly require a focus on a single industry and other steps to standardize activities across observations so that costs are comparable. In addition, we would have to carefully separate the costs incurred ex post from the initial price. That is, we claim that power affects the ex post governance structure in certain circumstances and the price in others. When power shows up in the price, it is actually more efficient, yet the amount paid by the client/buyer is higher. This price could not, therefore, be taken as an indication of either cost or efficiency.Finally, we abstracted away from the classic typology of [26] that is often used in the power literature, opting instead to define power in a straightforward manner based on scarcity. This is consistent with the long-standing view of monopoly power in economics and with [20] sociologically oriented dependence model. Scarcity is highly consistent with reward and coercive bases of power and, because it reflects the differential skills and resources of the CRO/vendor in our setting, it is also largely consistent with a basis in expertise. It would be informative to understand the roles played by other bases of power. "
8,"Brand Coolness Marketers strive to create cool brands, but the literature does not offer a blueprint for what ""brand coolness"" means or what features characterize cool brands. This research uses a mixed-methods approach to conceptualize brand coolness and identify a set of characteristics typically associated with cool brands. Focus groups, depth interviews, and an essay study indicate that cool brands are perceived to be extraordinary, aesthetically appealing, energetic, high status, rebellious, original, authentic, subcultural, iconic, and popular. In nine quantitative studies (surveys and experiments), the authors develop scale items to reliably measure the component characteristics of brand coolness; show that brand coolness influences important outcome variables, including consumers' attitudes toward, satisfaction with, intentions to talk about, and willingness to pay for the brand; and demonstrate how cool brands change over time. At first, most brands become cool to a small niche, at which point they are perceived to be more subcultural, rebellious, authentic, and original. Over time, some cool brands become adopted by the masses, at which point they are perceived to be more popular and iconic.KEYWORDS_SPLITConsumers spend an enormous amount of money on cool brands, and brands from Off-White and Apple to Instagram and Jay-Z have thrived at least in part because consumers consider them cool. Being cool has helped startup brands (e.g., Facebook) soar past established competitors (e.g., Myspace). Being uncool, conversely, can sink even popular and well-funded brands (e.g., Segway, Zune, Levi's), relegating them to the pages of cautionary case studies.What makes a brand cool? Despite the practical and theoretical importance of this question, the answer is unclear. Although research has begun to investigate personality traits associated with cool people ([21]; [22]; [42]; [79]), cool technologies ([17]; [29]; [66]), and how specific factors such as autonomy ([81]; [80]) and novelty ([47]) influence perceptions of coolness, the literature has not systematically identified the characteristics differentiating cool from uncool brands, nor has it identified how these characteristics change as brands move from being cool within a small subculture (i.e., niche cool) to the broader population (i.e., mass cool; [77]).We contribute to the literature by using grounded theory to identify the characteristics associated with cool brands. Through a series of studies leveraging focus groups, depth interviews, essay writing, surveys, and experiments, we generate and validate a measure of brand coolness that incorporates ten characteristics that distinguish cool brands from uncool brands. We find that cool brands are perceived to be extraordinary, aesthetically appealing, energetic, high status, rebellious, original, authentic, subcultural, iconic, and popular. We develop a multi-item scale that measures the ten components, as well as the higher-order construct, of brand coolness. In addition, we explore the nomological network related to brand coolness by identifying a set of variables that are related to, yet conceptually distinct from, coolness, including self–brand connections (SBC), brand love, brand familiarity, brand attitude, word-of-mouth (WOM) about the brand, and willingness to pay (WTP) for the brand.Moreover, we examine the subjective and dynamic nature of brand coolness ([12]; [31]; [73]). Brands initially become cool to a small subculture by being original, authentic, rebellious, exceptional, and aesthetically pleasing. Such brands (e.g., Steady Hands, INSIDE, Mitsky), which we refer to as being niche cool, are perceived to be cool by a small group of knowledgeable insiders, although the brands remain relatively unfamiliar to the broader population. Over time, some niche cool brands cross over and are adopted by a wider audience, at which point they become mass cool (e.g,. Nike, Grand Theft Auto, Beyoncé) and are perceived to be relatively more popular and iconic, but less autonomous. Conceptualizing CoolCool has many synonyms (e.g., hip, awesome, sweet, chill, badass, dope; for more, see Urban Dictionary [http://www.urbandictionary.com/define.php?term=cool]) but is difficult to define. Web Appendix A lists over 70 different ways that coolness has been described and defined, illustrates how the literature has not converged on a definition, and highlights the need to establish a firmer, empirically grounded understanding of brand coolness. Given the number of existing definitions of coolness, we believe that the field would benefit less from another definition than from a stronger understanding of how coolness applies to brands. Thus, as a starting point to investigate the characteristics of cool brands, we use Warren and Campbell's (2014, p. 544) definition of coolness as ""a subjective and dynamic, socially constructed positive trait attributed to cultural objects inferred to be appropriately autonomous"" (emphasis added).This definition highlights four essential features of coolness ([ 2]). One, coolness is subjective. Brands are only cool (or uncool) to the extent that consumers consider them as such ([18]; [37]; [65]). Consequently, uncovering what distinguishes cool from uncool brands requires collecting data about which characteristics consumers associate with the brands that they subjectively perceive to be cool.Two, coolness has a positive valence (e.g., [21]; [53]). Most dictionaries describe cool as an interjection used to express approval, admiration, and acceptance. Studies have found that consumers associate cool products with generally desirable characteristics, including usefulness ([70]; [55]), and hedonic value ([45]). Similarly, when asked to describe traits that they associate with cool people, survey respondents mostly list positive adjectives (e.g., attractive, friendly, competent; [19]). Yet there is also consensus that cool is not merely a general expression of liking. Cool brands are desirable, but there is something extra that makes an object cool rather than merely being positive ([21]; [65]).A third defining feature helps distinguish cool from desirable: autonomy. Autonomy is defined as being willing and able to follow your own path rather than conform to the expectations and desires of others ([81]). Autonomy cannot be directly observed, but instead must be inferred on the basis of the extent to which someone (or something) fights conventions and norms (i.e., is rebellious; [17]; [28]; [65]; [68]), attempts to be different by moving beyond conventions and norms (i.e., is original; [17]; [55]; [66]; [75]; [80]), and behaves consistently in the face of pressure to adapt to shifting trends (i.e., is authentic; [57]; [66]; [74]).The fourth defining feature of coolness is that it is dynamic. The brands that are cool today may not be cool tomorrow ([60]; [67]). Even the characteristics—and people—that consumers associate with cool brands appear to change over time and across different types of consumers. Most brands initially become cool within a specific niche or subculture before later being discovered, adopted, and christened as cool by a broader audience ([12]; [33]). Interestingly, consumers tend to use the same term, ""cool,"" to describe both ( 1) brands that their small in-group considers cool but that have not yet become popular and ( 2) brands that the general population is aware of and considers cool ([77]). Following [80], we distinguish between niche cool,[ 7] which refers to brands that are perceived to be cool by a particular subculture but that the masses have not yet adopted, and mass cool, which refers to brands that are perceived to be cool by the general population. Unanswered QuestionsThe literature thus raises several questions about brand coolness. First, although we know that coolness is desirable (Dar-Nimrod et al. 2012; [53]) and autonomous ([28]; [65]), there are many ways to be desirable and autonomous. For example, signaling high status and offering a low price are both desirable characteristics, and being unique and being dominant both show autonomy. The literature does not specify which desirable and autonomous characteristics make brands cool and which do not. It is also unclear whether other characteristics that are not directly related to desirability and autonomy are prototypical of cool brands. Researchers have suggested that coolness is related to emotional concealment, narcissism, hedonism, excitement, sexual permissiveness, and youth ([16]; [49]; [57]; [65]), but it is unclear whether any of these characteristics distinguish cool from uncool brands. Thus, our first research question is, What characteristics are prototypical of cool brands?Second, although there have been several attempts to measure coolness in specific product categories ([17]; [75]; [70]), there are no established scales designed to measure the characteristics of cool brands. Identifying a measure of the different components of brand coolness is practically valuable because it would allow marketers and scholars to identify whether a brand is cool and, if not, examine how and why it lacks coolness. Our second question thus is, Can we develop a validated instrument to measure the component characteristics of cool brands?Third, although both practitioners and scholars suggest that being cool helps explain why some products succeed ([12]; [39]; [47]), the specific consequences of brand coolness remain unclear. Are consumers more likely to talk (i.e., spread WOM) about cool brands? Are they willing to pay more for cool brands? Importantly, can brand coolness explain substantial variance in these or other important consequential variables, relative to that explained by previously studied constructs such as brand personality, brand love, and SBC? Our third question thus is, What are the consequences of brand coolness?Fourth, although we know that coolness is dynamic ([33]; [37]), it is not clear how the characteristics or consequences of cool brands change over time. The literature speculates that brands initially become niche cool to a small subculture before becoming mass cool to a broader audience, but how the characteristics and effects of cool brands change over time is an open empirical question. Our fourth question is thus, How do the characteristics and consequences of coolness change as brands move from niche cool to mass cool? Answering all of these questions requires data that the literature does not provide. Identifying Characteristics: Qualitative ResearchWe use a grounded theory approach to identify the characteristics of cool brands, initially conducting three qualitative studies using focus groups, depth interviews, and essays with consumers from North America and Europe. We identified these characteristics by looking for similar patterns of responses across the different methods and cultures ([32]; [50]) utilizing the ATLAS.ti software ([31]). First, we used a process of ""constant comparison"" to organize and reduce the coded units across the different sets of data. Second, we actively sought theoretical relationships between these concepts at a higher level of abstraction (""axial coding""). We then organized these concepts and relationships into ten major themes. Next, we provide a summary description of our three qualitative studies (details are in Web Appendix B), followed by the themes that emerged from the analysis. MethodWe first conducted four focus groups in Western (United Kingdom), Eastern (Slovakia), and Southern (Portugal) Europe. The average number of participants in each group was eight, and each focus group lasted about 60 minutes. For our second qualitative study, we conducted 30 depth interviews with consumers in Portugal. The interviews followed a methodological procedure similar to that outlined by [51]; see also [35]]). Informants were asked a series of grand tour questions, including ""What are the essential characteristics that you associate with cool brands?"" In our third qualitative study, 75 students at a university in the United States wrote two essays, one describing a brand they thought was cool and another describing a brand they liked but did not think was cool. Themes in the Qualitative DataTen themes, or characteristics, related to brand coolness emerged from the focus group, interview, and essay responses. Specifically, respondents perceived cool brands to be useful/extraordinary, aesthetically appealing, energetic, high status, original, authentic, rebellious, subcultural, iconic, and popular. Table 1 defines each characteristic and notes prior research that has suggested a relationship between the characteristic and coolness.GraphTable 1. Definitions for Component Characteristics of Brand Coolness and Relevant Citations from Prior Research.   Useful/extraordinaryA common theme in the focus groups, interviews, and essays was that cool brands are useful, meaning that they are high quality, offer tangible benefits, or help consumers in some way. One respondent wrote that he perceived Vic Firth (a musical instrument manufacturing company) to be a cool brand ""because of their high-quality product."" Another stated that Chrome Industries is a cool brand because ""their bags are well known for their durability and functionality."" The theme that cool brands are useful converges with evidence in the literature that there is a strong association between perceived coolness and traits that are desired or valued (e.g., [21]; [45]). Some respondents, however, indicated that cool brands are more than just useful—they are extraordinary. Respondents thought that Apple was cool because it offers ""previously unheard of capabilities,"" the brand ""pushes the limit in the electronic industry,"" or simply because ""I think they are awesome."" The finding that cool brands are extraordinary fits both with literature that highlights the positive valence of coolness (e.g., [12]; [21]) and with dictionary definitions of cool. Aesthetically appealingAnother recurring theme across the focus groups, interviews, and essays was that cool brands are aesthetically appealing. Respondents indicated that Apple is cool in part because its products are ""elegantly designed."" Respondents similarly noted the aesthetic appeal of other brands that they perceived to be cool across a range of industries from apparel to magazines: ""I am very impressed by the design and layout of the magazine [Wired], and I keep each issue to reference for when I am doing graphic design myself."" The theme that cool brands have aesthetic appeal is consistent with prior attempts to measure coolness in clothing and technological products ([17]; [70]; Sundar, Tamul, and Wu 2014). EnergeticA third theme that emerged was that cool brands are active, outgoing, youthful, or, more generally, energetic. Respondents indicated that cool brands make them feel good, connect with consumers on an emotional level, and help consumers have remarkable experiences. For example, respondents indicated that brands such as Red Bull and GoPro are cool because they are associated with exciting activities, including daring stunts and extreme sports. This notion that cool brands are energetic is consistent with the ""Brand Energy"" construct used by the Brand Asset Valuator system of assessing brand strength ([30]). Although some researchers have suggested that coolness is associated with similar traits, including youth ([60]; [70]), hedonism ([65]), and ""sexual permissiveness"" ([16]), prior research on coolness has rarely discussed being energetic as a characteristic of cool brands. Two exceptions are [ 1] and [74], who suggest a link between perceived coolness and excitement. High statusMany respondents viewed cool brands as having high social status or possessing traits associated with high status, such as being exclusive, upper class, glamorous, and sophisticated. Respondents wrote that Chanel perfume is cool because ""it makes me feel classy, chic, and elegant,"" and that Louis Vuitton is cool ""because of its exclusivity, not everyone owns something from Louis Vuitton."" Given the close link between status and coolness in people ([12]; [39]; [77]), it is not surprising that respondents similarly viewed cool brands as having high status. OriginalAnother theme in the focus groups, interviews, and essays was that cool brands are original. One respondent eloquently articulated this theme, stating ""the uncool will be doing tomorrow what the cool have done before."" Respondents described cool brands as being original, creative, ""one step ahead,"" and as consistently reinventing themselves. As previously mentioned, the literature similarly notes a close association between coolness and originality ([17]; [68]; [81]; [80]). AuthenticAnother theme in the responses was that cool brands are authentic. ""Authentic"" was the word most frequently associated with cool brands in the focus group sessions. Authenticity comes in a variety of flavors ([11]; [58]), and the flavor that our respondents mentioned—the brand behaving consistently and remaining true to its roots—has been called value authenticity ([15]), moral authenticity ([14]), sincerity ([56]), and integrity ([54]). One respondent stated, ""Cool brands don't try to be cool and they are just what they really are."" Another wrote that the record label Fueled by Ramen ""is a cool brand primarily due to its subject matter and authenticity....It has deviated very little from the genre with which it started and increases its reputation with each new successful alternative band that it cultivates."" Others noted the continuity over time in cool brands, such as Jack Daniels, with a traditional or vintage image. The link between coolness and authenticity is consistent with prior research on coolness ([ 2]; [15]; [57]; [66]). RebelliousA similar theme in the focus groups and interviews was that cool brands are rebellious. One respondent noted, ""Something controversial is in many cases the coolest."" Respondents thought that brands such as Red Bull, Harley-Davidson, Betsey Johnson, and Apple became cool by being ""rule breakers,"" ""irreverent,"" or ""revolutionary."" As previously discussed, the literature has historically linked coolness to rebellion ([28]; [65]), and this association has been at least partially supported by recent data ([15]; [44]; [81]). SubculturalAnother theme was that cool brands are associated with a particular subculture ([38]; [72]). One respondent noted that using cool brands provides ""the satisfaction of being part of a different subculture."" Respondents associated cool brands with a range of different subcultures, including rock climbers (Black Diamond), biker messengers (Chrome Industries), and alternative music (Converse). Even when they become popular, cool brands (e.g., Nike) usually maintain a link to a subculture (e.g., athletes). Research is consistent with the idea that cool brands are tied to specific subcultures, including those linked with jazz, raves, hip-hop, extreme sports, high school cliques, or any other group perceived to be distinct from the mainstream ([19]; [51]; [74]). IconicAnother theme emerging from the focus groups, interviews, and essays was that cool brands are iconic. By iconic, we mean that the brand holds an especially strong and valued meaning to consumers ([42]). There was a high overlap between the brands that our respondents identified as cool and the brands that [40]; [43]) describes as cultural icons (e.g., Apple, Nike, Patagonia, Jack Daniels). Moreover, our respondents highlighted how cool brands can symbolize memories, social relationships, identity traits, and cultural values. For example, one wrote, ""Disney is a symbol of childhood and being young and allows people to act young at heart which I think also helps add to the idea that Disney is cool."" All strong brands acquire some symbolic meaning ([46]; [48]), but respondents view cool brands as having especially potent meanings that reflect their shared cultural values and beliefs. For example, European respondents who value social responsibility considered socially conscious and environmentally friendly brands (e.g., the Finnish brand Globe Hope) to be cool. The literature notes the strong symbolism of cool brands ([12]; [81]), though we are not aware of previous work that has attempted to operationalize or measure the extent to which cool brands are iconic. PopularA final theme in the focus groups, interviews, and essays is that cool brands are popular, meaning that they seem trendy or widely admired by consumers. For example, one European respondent stated that for a brand to be cool, ""It has to be recognized all over the world."" Similarly, an American respondent wrote, ""I consider Nike cool because it is a brand widely worn among a variety of people."" We note here that some of the prior literature suggests that, paradoxically, cool brands are scarce (instead of popular), meaning that they are rare, exclusive, or not accessible to everyone ([16]; [55]; [67]). However, research that has used quantitative methods to study coolness has not found a link between scarcity and coolness in either people ([44]; [42]) or products ([17]; [70]; [73]). We too did not find adequate empirical support for a general link between scarcity and brand coolness in either our qualitative research or our quantitative surveys (see Web Appendices B and C).[ 8] Study 8, however, can explain why cool brands may be associated with scarcity and subcultures as well as popular trends: brands initially become cool when they were associated with a subculture (i.e., niche cool), but they later become popular and trendy after a wider population discovered the brand (i.e., mass cool; [33]; [78]). In other words, cool brands typically begin as scarce and subcultural but later become more popular as they are discovered and transition from niche cool to mass cool (see Figure 1).Graph: Figure 1. Life cycle of brand coolness. Differences Between Cool and Uncool BrandsWe subsequently assessed the frequency with which participants noted the aforementioned themes while writing about the cool and uncool brands in Qualitative Study 3 (essay writing). Specifically, a research assistant indicated whether the 75 essays mentioned each of the ten characteristics that appeared in the qualitative responses for both the description of the cool brand and the description of the uncool brand. If the essay noted a high level of the characteristic (e.g., ""brand X is original""), the research assistant coded it as a 1; if not, he coded it as a 0 (complete results in Web Appendix B). The characteristic that essay respondents most strongly associated with cool brands was being iconic. Most (73%) noted that cool brands seem iconic or that they symbolize an important value, belief, or memory (only 8% of uncool brands were described as being iconic; χ2 = 66.34, p <.001). Most respondents similarly reported that cool brands were extraordinary or useful (76%), though this did not distinguish cool from uncool brands, as respondents also considered most uncool brands useful (71%; χ2 =.55, p =.46). Responses suggested several additional characteristics that distinguish cool from uncool brands. Specifically, they were more likely to describe cool brands as being more subcultural (44% vs. 7%; χ2 = 27.63, p <.001), original (33% vs. 4%; χ2 = 21.25, p <.001), aesthetically appealing (25% vs. 4%; χ2 = 18.85, p <.001), popular (17% vs. 4%; χ2 = 7.00, p =.008), high status (15% vs. 4%; χ2 = 5.04, p =.02), and energetic (8% vs. 0%; χ2 = 6.25, p =.01) than uncool brands. In contrast to the focus group and depth interview respondents, few essay respondents explicitly mentioned authenticity or rebellion when describing cool brands. Themes Absent in the Qualitative DataThe focus groups, depth interviews, and essays were insightful not only for the themes in the responses but also for the themes that had been mentioned in the literature but that did not emerge. One conspicuously absent theme was cultural knowledge, which scholars argue helps make people cool ([20]; [55]; [73]; [74]). Other themes that the literature discusses but that did not surface in the data were emotional concealment, friendliness, and competence ([44]; [42]; [65]; [79]). One way to reconcile the absence of cultural knowledge, friendliness, and competence in our findings is by recognizing that these traits are desirable in people ([26]), just as being extraordinary, energetic, and aesthetically appealing are desirable in brands. Thus, desirable traits are cool in both people and brands, but the specific traits that are desired differ for people and brands. Just as the relevant aspects of personality and love differ between people and brands ([ 1]; [ 9]), some of the characteristics of cool people do not apply to cool brands. Structural and Nomological Modeling: Quantitative ResearchWe conducted eight survey studies to identify the higher-order structure of the characteristics of brand coolness that emerged in the qualitative research and to test their nomological relationships with related constructs. Each study asked respondents to evaluate a brand that they consider cool, a brand that they do not consider cool, or both. The first four studies were pretests, in which we developed and refined the measures for the structural and nomological models. Due to length constraints, we describe these studies in Web Appendix C. Study 5 had three purposes. First, it confirmed the structural measurement model for the ten characteristics associated with brand coolness (useful, aesthetically appealing, energetic, high status, rebellious, original, authentic, subcultural, iconic, and popular). Second, the study confirmed that all ten characteristics were more closely associated with cool brands than uncool brands. Third, the study tested the nomological relationship between brand coolness and related constructs, including brand personality, SBC, brand love, brand attitude, WTP for the brand, and intentions to spread WOM about the brand. The last three studies replicated and extended Study 5. Study 6 improved our measure of brand coolness by developing items that better capture the extent to which the brand seems extraordinary rather than merely useful. Study 7 used a purely confirmatory design to replicate the results of Study 6. Finally, Study 8 examined the dynamic and subjective nature of brand coolness by testing how the characteristics and consequences differ between niche cool brands, mass cool brands, and uncool brands within a subculture of urban streetwear enthusiasts. Method SamplesStudies 5 (N = 315; 50% male; modal age = 42 years) and 6 (N = 315; 47% male; modal age = 25–30) recruited U.S. consumers from a nationally representative online survey panel. Study 7 recruited participants from Amazon's Mechanical Turk (MTurk; N = 405; 58% male; modal age 25–30 years; all located in the United States). Study 8 recruited 148 streetwear fashion enthusiasts by offering a Gold Award[ 9] to readers of the Reddit board r/streetwear who completed the survey. The sample for Study 8 was mostly young (average age = 19 years; range = 13–41 years) and male (93%) but was racially diverse (53% white, 27% Asian, 5% Hispanic, 3% Black). Brand nominationsIn each study, participants nominated and evaluated one or more brands (for details, see Web Appendix D). In Studies 5 and 6, participants nominated and evaluated a brand that they personally consider cool and a brand that they like but do not personally consider cool. In Studies 7 and 8, we manipulated the brand type between subjects, such that participants nominated a cool or uncool brand (Study 7) or a niche cool, mass cool, or uncool fashion brand (Study 8). For example, in Study 8, participants in the ""uncool"" condition read, ""Please identify a brand that you consider not cool. Neither you nor the 'mass market' think that this brand has ever been cool, today or in the past."" Participants in the ""mass cool"" condition read, ""Please identify a fashion brand that is cool to mainstream consumers. That is, name a brand that is mass cool."" Participants in the ""niche cool"" condition read, ""Please identify a fashion brand that is cool to you (but not to the mainstream). That is, name a brand that is niche cool."" Measuring cool characteristicsAfter participants nominated the brand(s), we asked them to rate them (order counterbalanced) on a series of five-point ""agree–disagree"" scale items. Drawing on our literature review, qualitative research, and four pretest studies (see Web Appendix C), in Study 5 we used the 36 items listed in Table 3 to measure the extent to which each brand was perceived to be useful, aesthetically appealing, energetic, high status, rebellious, original, authentic, subcultural, iconic, and popular. In Study 6, we explored whether the extent to which the brand seems extraordinary better captures the construct of coolness than the extent to which it seems useful by adding four new items (e.g., ""X is exceptional"") as possible replacements for the three useful items. Studies 7 and 8 used the final 37-item scale (see Table 3) to measure the extent to which the brand seems extraordinary (instead of useful) along with the other nine characteristics. Measuring related constructsThe studies also measured various constructs that the literature suggests might be related to brand coolness. All four studies measured ( 1) brand love (two-item measure from [ 9]]), ( 2) SBC (five-item measure adapted from [24]]), ( 2) WOM related to the brand (e.g., ""In the past few months, how often have you talked about [brand name] with other people, online or offline?""), and ( 4) WTP for the brand (e.g., ""I am willing to pay a higher price for this brand than other brands""). Studies 5, 7, and 8 measured brand attitudes. Studies 5 and 7 measured the five dimensions of brand personality (ruggedness, excitement, sophistication, competence, and sincerity) using [ 1] 22-item scale. Studies 7 and 8 measured the extent to which ( 1) participants had been exposed to the brand (e.g., ""In the past few months, how often have you heard other people talk about [brand name]?""), ( 2) the brand is familiar (e.g., ""this brand is well-known""), and ( 3) the brand commands a price premium (e.g., ""this brand costs more than others in the same product category""). Finally, Study 5 measured satisfaction (three items from [57]]), delight (six items adapted from [27]]), and pride (five items adapted from [76]]) from owning the brand. We provide a complete list of measures in Web Appendix D. Manipulation checksIn addition to measuring the characteristics associated with brand coolness (e.g., extraordinary, aesthetic), Studies 6–8 asked participants to directly rate the extent to which they personally consider the brands cool. Studies 7 and 8 also measured the extent to which participants believe that other people consider the brand cool. Finally, to capture the dynamic nature of coolness that we were investigating in Study 8, we asked participants to indicate how the brand's coolness has changed in the past and how they expect it to change in the future. Individual difference measuresStudy 8 measured participants' need for uniqueness (short form; [71]), innovativeness (items from [44]]), subjective expertise in fashion, and experience reading and posting on the r/streetwear forum. Studies 5–8 concluded by measuring participants' demographic variables (e.g., age, gender, native language). None of these individual differences interacted with the results we report here, so we do not discuss them further. Note that Studies 7 and 8 also included theoretically unrelated ""marker variables"" for a methods factor test, described next. Measurement Model of Brand CoolnessWe refined and revised the measurement items using exploratory factor analysis (Studies 1–4) and confirmatory factor analysis (CFA; Studies 5–8; see Table 2). Specifically, we used the pretests to eliminate or replace items that either did not load highly onto the factor they were intended to measure or that cross-loaded onto multiple factors ([36]; [59]), keeping in mind the characteristics identified from our literature review and qualitative analyses (details in Web Appendix C). We used Studies 5–8 to confirm our final model (see Figure 2). We used a reflective instead of a formative model at each level for two reasons. One, the logic underlying reflective models better fits our conceptualization of brand coolness: the ten characteristics derived from the qualitative analyses are more appropriately considered to be manifestations of the latent construct of brand coolness, rather than formative measures that define it. Two, the coefficients in formative models can vary with the number and structure of the measures and factors used ([ 4]; [25]; [43]), which makes them less appropriate in our context.[10]GraphTable 2. Summary of Quantitative Survey Studies.  1 aNationally representative panel from the United States.2 bAmazon's Mechanical Turk.3 cReddit board r/streetwear.4 dUsed as a marker variable.Graph: Figure 2. CFA measurement model in Studies 6–8.Note: A reflective perspective is used at all levels.Our data revealed a final model with brand coolness consisting of two higher-order factors, which we call desirability and positive autonomy, along with five first-order factors (see Figure 2). The three characteristics of ""useful"" (later, ""extraordinary""), ""energetic,"" and ""aesthetic appeal"" load onto the subdimension of desirability; the two first-order factors of ""original"" and ""authentic"" load onto the subdimension of positive autonomy. Both desirability and positive autonomy are dimensions of higher-order brand coolness, along with high status, rebellious, subcultural, iconic, and popular, which load as first-order factors onto higher-order brand coolness. Table 3 shows the estimated measurement and structural coefficients from Studies 5–8. Where available, we report within-group, completely standardized coefficients for the cool and uncool brand samples separately. Note that Study 5 used three items measuring whether the brand is useful, whereas Studies 6–8 replaced these with the four new items measuring whether the brand is extraordinary.GraphTable 3. CFA Model Coefficients and Fit Statistics by Study and Sample.  5 aWe used these items in Study 5.6 bWe used these items in Studies 6–8.Table 3 reveals that our factors and model structure were stable across all of the studies and samples, with a few small differences. The factor loadings were high, and the average variance extracted and composite construct reliability statistics for all factors were almost always above.50[11] and.70, respectively. We formally tested for the equivalence of measurement and structural coefficients across the cool and uncool samples in Studies 5, 6, and 7, and almost all were equivalent (see Web Appendix E). In the few cases where the coefficients differed (e.g., iconic and popularity in Study 8), the differences, which were small, were likely because we needed to estimate the CFA model across the niche cool, mass cool, and uncool brand subsamples to get a sufficient sample size.The goodness-of-fit indices also showed an excellent fit across all of the studies and subsamples. For example, the statistics for the cool brand subsample in Study 5 were χ2(582) = 1,283.33, p <.001; root mean square error of approximation (RMSEA) =.06; nonnormed fit index (NNFI) =.97; comparative fit index (CFI) =.97; standardized root mean residual (SRMR) =.08. The uncool brand subsample in Study 5 showed a similarly excellent fit: χ2(582) = 1,226.62, p <.001; RMSEA =.06; NNFI =.98; CFI =.98; SRMR =.07. As illustrated in Table 3, the goodness-of-fit measures were similar in Studies 6–8. Thus, our measurement model of brand coolness satisfies conventional tests of adequacy. Across Studies 5–7, the measurement factor loadings for the characteristics of cool brands averaged.81 and ranged from.62–.96. For uncool brands, their average was.86, and they ranged from.53–.97. The factor loadings from the first-order to second-order factors (e.g., originality to positive autonomy) ranged, across all the samples and studies, from.75–.99, averaging.90. The betas from the second-order factors to higher-order brand coolness (e.g., desirability to higher-order brand coolness) ranged from.38–1.0, averaging.86. They were highest on average for desirability (.99) and positive autonomy (.92), lowest for rebellious (.59) and iconic (.62), and midrange for high status (.68), popular (.72), and subcultural (.65). Comparing Cool with Uncool BrandsIn addition to showing sound measurement properties, the characteristics of brand coolness in our model reliably distinguished cool from uncool brands (see Table 4). Paired-sample t-tests comparing the average ratings for the cool versus uncool brands confirmed that the brands that participants nominated as being cool were perceived to be significantly more useful (Study 5) or extraordinary (Studies 6 – 8), energetic, aesthetically appealing, original, authentic, rebellious, high status, popular, subcultural, and iconic than the brands nominated as being uncool (ps <.001; Table 4 presents the means).GraphTable 4. Means (SDs) by Condition in Studies 5–8.  7 aCharacteristic measured only in Study 5.8 bCharacteristic measured in Studies 6–8.9 cRated on a scale from 1 to 7.10 dRated on a scale from −1 to 1.11 eRated on a scale from 1 to 4.12 Notes: The scales were from 1 to 5, unless otherwise noted.Across our studies, the brands that consumers most frequently selected as being cool (on seven-point scales) included Apple (6.5), which seemed especially original, popular, and aesthetically appealing; Nike (6.6 overall), which seemed especially popular; Samsung (6.4), which seemed especially original; Under Armour (6.6), which seemed especially popular and aesthetically appealing; and Adidas (6.6), which seemed especially popular (see Table 4). Interestingly, different participants referenced many of these same brands (Apple, Nike, Samsung, Adidas) as being uncool, because they perceived the brands to be lower status, less subcultural, and less rebellious. Other brands that participants frequently nominated as being uncool include Microsoft, Reebok, Old Navy, Walmart, and Crocs. The fact that consumers in Studies 5–7 differed about which brands were cool and uncool confirms the subjective nature of coolness, especially when looking across a diverse sample of consumers. As we might expect, participants in Study 8, who were all part of an urban streetwear subculture, agreed more about which brands were and were not cool compared with participants in Studies 5–7. Are Cool Brands Extraordinary or Merely Useful?As previously mentioned, Study 6 tested whether our model and scale would be conceptually and empirically stronger if the first characteristic measured how extraordinary (four items: exceptional, superb, fantastic, and extraordinary) the brand was, as opposed to measuring the extent to which the brand seemed (merely) useful (three items: useful, helpful, and valuable). Study 6 therefore measured all seven of these items and compared the coefficients and fit statistics of models that used either the new extraordinary items or the old useful items. The models were not nested, which makes chi-square difference tests inappropriate; however, the model fit statistics (NNFI, CFI, RMSEA, and SRMR) for the new four-item models were superior or equal to those for the old three-item models. The completely standardized lambda coefficients for the extraordinary items were all very high (.89–.97) in both the cool and uncool subsamples. Most importantly, the structural coefficients from the first-order useful/extraordinary factor to the second-order desirability factor were higher with the new four items than with the old three items, increasing from.77 to.83 (for cool brands) and from.85 to.93 (for uncool brands). The structural coefficients from the desirability second-order factor to the overall brand coolness factor were also slightly higher in both cases, increasing from.99 to 1.0. Drawing on this empirical evidence, and given the strong conceptual argument favoring this change (e.g., [12]; [65]), we replaced the three useful items with these four extraordinary items in Studies 6–8 and in the final recommended items for our brand coolness scale. Discriminant Validity of Brand Coolness from Conceptually Related Constructs Theoretical distinctionsBrand coolness should be related to, but conceptually distinct from, the constructs of brand love, SBC, particular dimensions of brand personality, and brand attitudes. Brand coolness is a perceived attribute of a brand, whereas both brand love and SBC should be responses to—and, thus, consequences of—brand coolness. Although there is likely some overlap between specific characteristics of coolness (in particular, high status, energetic, and useful/extraordinary) and specific dimensions of brand personality (sophistication, excitement, and competence, respectively), our latent construct of higher-order brand coolness, which includes many other constituent characteristics (see Figure 2), should display discriminant validity from these brand personality dimensions. Brand coolness should also be discriminable from brand attitudes, because there is something extra that makes an object cool rather than merely positive ([81]). Empirical discriminationWe tested discriminant validity in Studies 5–8 by estimating the disattenuated, latent psi correlations between multiple pairs of variables to test whether their 95% confidence intervals fell significantly below 1.0 ([ 7]). As we report in Web Appendix F, the analyses confirmed the discriminant validity between constructs. For instance, in Study 7, the phis (SEs) of brand coolness with brand love for cool and uncool brands, respectively, are.59 (.06) and.42 (.07), and with SBC are.59 (.05) and.50 (.06). The correlations between brand coolness and brand attitudes were also below 1.0 (.56 [.06] and.40 [.07], respectively). Between the five brand personality dimensions and brand coolness, each pair of disattenuated correlations was statistically significantly below 1.0 (range:.32–.87). Correlates, Consequences, and Mediation Theoretical correlates: brand personalityBrand personality is the set of human characteristics associated with a brand ([ 1]). Brand personality serves a symbolic or self-expressive function for consumers, and it consists of five core dimensions: sophistication, competence, ruggedness, excitement, and sincerity ([ 1]). On the one hand, it could be argued that these brand personality perceptions should make that brand seem more, or less, cool, and they thus serve as antecedents of overall perceived brand coolness. On the other hand, it could also be argued that the multiple marketing and sociocultural elements (e.g., communications content, choice of endorsers) that shape these brand personality perceptions should also simultaneously shape the perceived coolness of the brand. Therefore, it is difficult (especially in cross-sectional survey data) to empirically determine which perceptual changes came first. Because some of a brand's personality dimensions (especially excitement and sophistication) are conceptually similar to some of our brand coolness components (energetic and status, respectively), and because it is unreasonable in our data to expect a strong empirical signal about which dimensions come first, we were cautious in our analyses and modeled the five brand personality dimensions as correlates, rather than antecedents, of higher-order brand coolness. This has the benefit of yielding model estimates of the effects of brand coolness on mediating (e.g., brand love, SBC) and dependent variables (brand attitude, WOM, and WTP) that are ""net of"" (i.e., they control for and partial out) the effects of these independently measured brand personality dimensions and are thus more conservative.[12] Theoretical consequencesTo examine the consequences of brand coolness, our nomological model also estimated the effects of overall brand coolness on several different dependent variables: SBC (Studies 5–8), brand love (Studies 5–8), WTP (Studies 5–8), willingness to spread WOM (Studies 5–8), brand attitudes (Studies 5, 7, and 8), brand familiarity (Studies 7 and 8), brand exposure (Studies 7 and 8), whether the brand commands a price premium (Studies 7 and 8), satisfaction (Study 5), delight (Study 5), and pride (Study 5) in owning the brand.Consumers view coolness as a desirable trait ([21]; [53]; [67]). Moreover, we find that brand coolness includes multiple characteristics (e.g., being extraordinary and aesthetically pleasing) that consumers consider desirable. Consequently, we hypothesize that brand coolness should predict consumers' overall attitude toward the brand, and we include brand attitude valence as a consequence of brand coolness. Beyond increasing overall desirability and liking, brand coolness should also increase several other types of distinct positive feelings toward the brand. Because cool brands are considered desirable, coolness should create a feeling of high overall satisfaction with the brand ([61]). The satisfaction literature also talks of feelings of delight, in which high-arousal feelings of joy and surprise augment the more cognitively based satisfaction assessment ([ 8]); coolness should increase delight as well, especially because coolness is partially determined by the extent to which a brand is energetic and aesthetically appealing, both of which have strong affective components.Brand coolness also has components that are value-expressive in nature, including positive autonomy, rebellion, high status, subcultural appeal, and iconic symbolism ([13]; [42]). We therefore hypothesize that brand coolness will strengthen SBC ([24]) because SBC increases as a brand's symbolic aspects become more consistent with a consumer's aspirational reference groups. Consumers' relationships with a cool brand might also extend beyond SBC to increase brand love, a broad brand relationship construct that includes current and desired self-identity ([ 9]). Because of the desirable and identity-relevant characteristics associated with cool brands, it is similarly likely that consumers will also feel greater pride from owning brands that they perceive to be cool (Tracy and Robbins 2007).Both SBC and brand love tend to increase consumers' WTP for and likelihood of discussing (WOM) a brand ([ 9]; [24]). Thus, if brand coolness increases SBC and brand love, as we hypothesize, then consumers should be willing to pay more for the brand (i.e., WTP) and want to tell others how great it is (i.e., WOM). Finally, because cool brands are high status, popular, and iconic, we also expect that they will command a higher price premium, be familiar to more consumers, and gain more exposure compared with brands that are not cool. Results of nomological modelsWe tested these predictions by modeling overall (higher-order) brand coolness as being correlated with the five dimensions of brand personality (in Studies 5 and 7) and as leading to a set of consequences (which varied slightly depending on which consequence variables we measured in the studies; see Web Appendix G), including brand attitudes, SBC, brand love, WTP, WOM, brand familiarity, brand exposure, price premium, satisfaction, delight, and pride. To obtain a reasonable ratio of sample size to the number of estimated parameters in the predictive model (e.g., [ 6]; [ 5]), we averaged the items for each predicted variable. For each study, we then created structural equation models (SEM) in which the CFA model of higher-order brand coolness (see Figure 2) served as an independent variable, the consequences (e.g., SBC, brand love) served as endogenous dependent variables, and the dimensions of brand personality served as correlates (Studies 5 and 7 only).Across studies, the model fit was satisfactory. For example, the fit for the nomological SEM in Study 5 was: χ2( 1,077) = 2,694.88, p <.001; RMSEA =.075; NNFI =.94; CFI =.96; SRMR =.085. The SEM with the not-cool brand subsample also fit well: χ2( 1,077) = 2,442.91, p <.001; RMSEA =.069; NNFI =.98; CFI =.98; SRMR =.069. Brand coolness was significantly correlated, but also showed discriminant validity (see the ""Empirical Discrimination"" subsection), with all five dimensions of brand personality. Brand coolness was most closely related to the sophisticated, competent, and exciting dimensions of brand personality; this pattern makes sense, given that three of the characteristics of higher-order brand coolness include being high status, useful (study 5)/extraordinary (Studies 6–8), and energetic.In addition, in all of the studies, higher-order brand coolness significantly predicted the measured consequence variables, including brand love (Studies 5–8), SBC (Studies 5–8), brand attitude (Studies 5–8), WTP (Studies 5–8), WOM (Studies 5–8), brand familiarity (Studies 7 and 8), brand exposure (Studies 7 and 8), brand price premium (Studies 7 and 8), delight (Study 5), satisfaction (Study 5), and pride (Study 5). Variance explained by brand coolnessTo test whether brand coolness can help marketers predict outcomes that they care about, such as the extent to which consumers hold a positive attitude toward and are willing to pay for the brand, we examined how much variance higher-order brand coolness explained in the outcome variables (brand attitude, WTP, and WOM) relative to more established constructs in the literature, including brand love and SBC. Across all our studies, brand coolness explained between 32%–70% of the variance in brand attitudes, an amount that was similar to the variation explained by brand love (26%–85%) and SBC (19%–67%). Brand coolness similarly explained a comparable amount of variance in WOM (32%–57%) and WTP (32%–79%) as brand love (WOM: 25%–74%; WTP: 29%–86%) and SBC (WOM: 26%–79%; WTP: 29%–84%). As detailed in Web Appendix G, the amount of variance that brand coolness explained varied by outcome, study, and brand sample. For example, in Study 7, in the cool (uncool) brands data, the variance explained in brand attitude by brand coolness alone was 54% (32%), compared with 48% (85%) by brand love alone and 22% (64%) by SBC alone. The variance explained in WOM by brand coolness alone by was 32% (57%), compared with 35% (25%) by brand love alone and 32% (26%) by SBC alone. For WTP, the variance explained by brand coolness alone was 38% (67%), compared with 45% (47%) by brand love alone and 33% (47%) by SBC alone. These results show that brand coolness has a lot of explanatory power and is thus worth studying as a construct in its own right. Table 4 also shows how the mean levels of brands on these outcome variables become higher when the brand is seen as cool (vs. less cool). Mediation testsThe nomological models estimated only the direct effects of higher-order brand coolness on the many outcome variables; they did not test for mediation. Although cross-sectional data do not allow us to unambiguously establish causal sequences, it is nonetheless interesting to test whether the data were consistent with the hypothesis that brand love and SBC mediate the effects of brand coolness on brand attitude, WTP, and WOM. We therefore estimated SEMs to test these hypothesized mediating paths in Studies 5, 6, and 7. We provide the details for these analyses in Web Appendix H. To summarize results, the effect of higher-order brand coolness on each of the dependent variables—brand attitude, WOM, and WTP—was partially or fully mediated by SBC and brand love in each study and subsample. As a specific example, in Study 7, the cool brands data, brand coolness significantly influenced brand love (standardized coefficient.77) and SBC (.67); brand love significantly influenced brand attitudes (1.86), WTP (1.34), and WOM (.55); and SBC significantly influenced brand attitude (−1.01) but did not significantly influence WOM or WTP. Brand coolness also directly and significantly influenced brand attitude (.61) and WTP (.65) but not WOM. In summary, brand love fully mediated the effects of brand coolness on WOM, but partially mediated the effects of brand coolness on brand attitude and WTP. This pattern of mediation supports the conceptual argument that brand coolness, which a consumer perceives in a brand, is an antecedent to constructs such as brand love and SBC, which are a consumer's evaluative responses to a brand that result from the properties perceived in the brand.The Study 7 results illustrate both the large amounts of variance explained in the outcome constructs by higher-order brand coolness and the mediation pathways for these effects. In the cool (uncool) brand sample, brand coolness explained 35% (57%) of the variance in SBC, 42% (52%) in brand love, 52% (77%) in brand attitudes, 43% (56%) in WTP, and 31% (25%) in WOM. For the cool (uncool) brands, the standardized direct path coefficients from brand coolness to the outcome constructs (all ps <.01) were as follows: SBC =.59 (.76), brand love =.65 (.72), brand attitude =.53 (.27), WTP =.51 (.52), and WOM =.18 (.20). For cool brands, the standardized indirect path coefficients from SBC to brand attitude (−.28) and WOM (.24) were both significant, as was the path from brand love to brand attitudes (.46). For the uncool brands, the standardized indirect path coefficients from SBC to WTP (.34) and WOM (.33) were significant, as was the path from brand love to brand attitude (.55). Thus, the effects of higher-order brand coolness on each of the dependent variables—brand attitude, WOM, and WTP—were partially or fully mediated by SBC and brand love in each study. Methods Factor TestsWe tested the degree to which common method bias affected our structural and measurement models in Studies 7 and 8 using the well-accepted ""marker variables"" technique presented in [81]. In both Studies 7 and 8, as marker variables, we asked respondents about their experience with and expectations of service quality in restaurants (four items), which are not meaningfully related, in either a theoretical or empirical sense, to the constructs of interest in this research. Details of these methods factor tests appear in Web Appendix I. These tests showed that the marker variable approach to test for method bias did not indicate problems in these two studies. How Do Brands Change as They Move from Niche Cool to Mass Cool?Cool brands change over time. Born as relatively obscure brands in outsider subcultures, cool brands often spread beyond their niche roots to become cool to the masses ([12]; [31]; [81]). How do the characteristics associated with cool brands change as they mature from niche cool to mass cool? Moreover, do consumers respond differently to mass cool brands than niche cool brands?Study 8 attempted to answer these questions by investigating how consumers in the urban streetwear subculture perceive both brands that they themselves think are cool but have not yet caught on outside of the streetwear apparel subculture (i.e., niche cool brands) and brands that have become cool to a broader audience (i.e., mass cool brands). We expected that the data from the streetwear subculture would replicate the previous studies by showing that both mass cool and niche cool brands would score higher on all ten characteristics of cool brands compared with uncool brands. In addition, we expected that the characteristics would differ between niche cool and mass cool, such that mass cool brands would seem more popular and iconic but niche cool brands would seem more subcultural, original, authentic, and rebellious.To analyze the data, we examined the differences between the three experimental conditions (niche cool, mass cool, and uncool) using two planned, orthogonal contrasts. The first contrast examined the difference between cool and uncool brands by comparing the ratings of the uncool brand with the average of the ratings for the mass cool and niche cool brands. The second contrast examined the difference between the mass cool and niche cool brands. Manipulation checksThe brand manipulation successfully elicited different types of brands from the participants (for the most frequently nominated brands in each condition, see Table 4). Participants perceived the uncool brands to be less cool than mass cool and niche cool brands, both to themselves personally (t = 14.32, p <.001) and in the eyes of others (t = 9.77, p <.001). Interestingly, however, the correlations between the measures of how participants personally rated the brand's coolness with how cool they thought others perceived the brand to be was only.50, which offers additional evidence that perceptions of brand coolness are subjective.The niche and mass cool brands also differed, as we intended. Compared with the mass cool brands, participants perceived the niche cool brands to be more cool to themselves personally (t = 4.41, p <.001) but less cool to others (t = −4.68, p <.001). Moreover, participants also predicted a different future trajectory for the brands. Consistent with theory predicting that niche cool brands become cooler to a broader population over time, participants expected the niche cool brands to become cooler in the future, compared with the scale midpoint (t = 4.63, p <.001), the mass cool brand (t = 4.15, p <.001), and the uncool brand (t = 5.69, p <.001). On average, participants expected the uncool brand to become even less cool over time (t = −4.15, p <.001), whereas they did not expect the coolness of the mass cool brand to change for better or worse (t = −1.48, p =.15; for the descriptive statistics, see Table 4). Differences between cool (mass and niche) and uncool brandsReplicating the previous studies, both the mass cool brands and the niche cool brands were perceived to have higher levels of all ten characteristics compared with the uncool brands (all ps <.001). Furthermore, replicating the previous studies, participants reported stronger SBC (t = 11.10, p <.001), more brand love (t = 12.28, p <.001), higher levels of WOM (t = 8.96, p <.001), higher price premiums (t = 7.86, p <.001), higher WTP for (t = 11.71, p <.001), and more favorable attitudes toward (t = 9.94, p <.001) the cool than the uncool brands. Differences between mass and niche cool brandsConsistent with our prediction that the characteristics of cool brands change over time, participants perceived several differences between the mass cool and niche cool brands. Compared with niche cool brands, mass cool brands were perceived to be less subcultural (t = −2.10, p =.037), original (t = −3.15, p =.002), authentic (t = −5.08, p <.001), rebellious (t = −2.20, p =.029), extraordinary (t = −3.56, p <.001), and aesthetically appealing (t = −3.50, p <.001), yet more popular (t = 8.49, p <.001) and iconic (t = 7.34, p <.001). The consequences associated with coolness also shifted as brands moved from niche cool to mass cool. Consistent with mass cool brands being more popular and ubiquitous cultural symbols, participants indicated that they had been more exposed to (t = 7.88, p <.001) and had shared, and intended to share, more WOM about (t = 2.02, p =.045) mass cool brands compared with niche cool brands. They similarly reported that mass cool brands are more familiar in the marketplace (t = 14.30, p <.001) and command higher prices (t = 3.93, p <.001) than niche cool brands. However, consistent with niche cool brands being more closely associated with a consumers' subculture and personal in-group, participants reported weaker SBC for (t = −5.04, p <.001), less love for (t = −4.25, p <.001), a lower WTP for (t = −3.72, p <.001), and less favorable attitudes toward (t = −2.85, p =.005) mass cool compared with niche cool brands. Figure 2 summarizes the dynamic nature of coolness as brands move from uncool to niche cool to mass cool and (sometimes) back to uncool. Experiment: Manipulating the Characteristics of Cool BrandsWe have found that cool brands have different characteristics than uncool brands, but we have not yet examined whether we can increase the extent to which a brand seems cool by experimentally manipulating the characteristics of brand coolness. Thus, in our final study, we manipulated the description of a watch brand to orthogonally vary the desirability (i.e., extraordinariness, aesthetic appeal, and excitement), positive autonomy (i.e., originality and authenticity), rebellion, popularity, and status of the brand. To keep the number of factors in the experiment manageable, we did not manipulate the extent to which the brand seemed iconic or subcultural,[13] and we contrasted cool with uncool brands rather than distinguishing between mass and niche cool brands. Consumers form their actual perceptions of brand coolness over multiple exposures to various brand marketing and social signals over a long period of time; thus, our single-exposure experiment provides a conservative test of whether the characteristics influence perceptions of brand coolness. Nevertheless, we predicted that the brand would seem more cool when participants read that it was more (rather than less) desirable, autonomous, rebellious, popular, and high status. We also predicted that coolness would in turn influence participants' attitudes, WTP for, and likelihood of spreading WOM about the brand. MethodParticipants (N = 368; 34% female; mean age = 36.0 years; all located in the United States) from MTurk completed the study for a small payment. The study included a reading check at the beginning, which filtered out 11 respondents before assigning them to a condition.Participants completed the study, titled ""Online Review Survey,"" in which they were randomly assigned to a condition in a 2 (desirability: high, low) × 2 (autonomy: high, low) × 2 (rebellion: high, low) × 2 (status: high, low) × 2 (popularity: high, low) between-subjects experiment. Participants read a description of a wrist watch brand named Voss, a fictional brand. Participants read that ""the description of the brand summarizes hundreds of ratings and reviews written by customers and industry experts who are already familiar with the brand."" Participants next read about five brief characteristics of the brand. We manipulated whether consumers described Voss as being desirable, autonomous, rebellious, high status, and popular at two levels by describing the brand as either possessing or lacking the characteristic. The descriptions used words taken directly from the scale items that we identified in prior studies (see Web Appendix D). For example, the status manipulation described the brand as being ""glamorous"" and ""sophisticated"" or as lacking these traits. The survey presented the characteristics one at a time, in random order, and did not allow participants to advance to read the next characteristic until at least three seconds had passed.Participants subsequently completed a series of measures, including brand coolness, brand attitude, WTP, and WOM (see Web Appendix D). The final part of the survey measured the effectiveness of the manipulations using the full brand coolness scale from Studies 6–8. Finally, participants reported their age, gender, and native language. Results Brand coolnessWe assessed the effects of the five manipulated brand characteristics on perceptions of brand coolness using a 2 (desirability: high, low) × 2 (autonomy: high, low) × 2 (rebellion: high, low) × 2 (status: high, low) × 2 (popularity: high, low) analysis of variance. The analysis revealed main effects of desirability (F( 1,336) = 35.73, p <.001, η2 =.096), autonomy (F( 1,336) = 59.90, p <.001, η2 =.151), status (F( 1,336) = 10.85, p =.001, η2 =.031), popularity (F( 1,336) = 33.69, p <.001, η2 =.091), and rebellion (F( 1,336) = 7.51, p =.006, η2 =.022). As we predicted, participants perceived the brand to be more cool when it was described as being desirable (M = 4.26 vs. M = 3.27), autonomous (M = 4.41 vs. M = 3.15), high status (M = 4.01 vs. M = 3.54), popular (M = 4.25 vs. M = 3.29) and rebellious (M = 3.98 vs. M = 3.56) than when it lacked these qualities. None of the interactions were significant, which suggests that each characteristic additively influences perceived coolness. Indirect effects on attitude, WTP, and WOMWe next tested whether the significant main effects of desirability, autonomy, status, and popularity on perceived coolness had downstream consequences on participants' attitudes, WTP, and WOM for the brand. Instead of conducting separate mediation tests for each of the five manipulated variables on each of three dependent variables, we estimated one comprehensive SEM path model using LISREL (n = 368), which allowed for all direct and indirect effects. The model comparing full with partial mediation yielded a significant chi-square difference of 40.47 with 15 degrees of freedom (p <.001), showing that a model with one or more direct paths was a superior model. Specifically, as with the analysis of variance results, desirability (path coefficient =.28), autonomy (.35), popularity (.26), status (.14) and rebellion (.12) all significantly increased perceived coolness. Brand coolness, in turn, significantly influenced the three dependent variables: brand attitude (.84), WTP (.54), and WOM (.82). Thus, the effect of desirability, autonomy, popularity, status, and rebellion on the three dependent variables was at least partially mediated by brand coolness in each case. However, some significant direct effects of the manipulated brand characteristics on the dependent variables also emerged, though these direct effects are hard to interpret because of possible multicollinearity, remaining measurement error, or omitted mediators.[14] DiscussionOur experiment confirmed that increasing the extent to which a brand seems desirable, autonomous, rebellious, high status, and popular increases the extent to which it is perceived to be cool. Brand coolness, in turn, influences several consequence variables, including the extent to which consumers hold a favorable attitude toward the brand as well as their WTP for and willingness to discuss the brand with others. Finally, the experiment suggests that the effects of the characteristics of brand coolness on overall perceptions of coolness and on its downstream consequences (e.g., brand attitudes) are additive, though future research will need to further explore factors that moderate or interact with the different characteristics of brand coolness. General DiscussionWhat features characterize cool brands? Our research (three qualitative and nine quantitative studies) reveals that cool brands are extraordinary, aesthetically appealing, energetic, original, authentic, rebellious, high status, subcultural, iconic, and popular. Not all of these characteristics are necessary for every brand and every consumer segment, but, as our experiment revealed, increasing any of these characteristics tends to make a brand seem cooler. Nike is widely seen as cool because its shoes are highly desirable, look good, signal energy, and have extraordinary quality. Apple shows positive autonomy by being original and authentic, even as it has grown to become very popular. Harley-Davidson became cool when a subculture of outlaw bikers, who lent the brand a rebellious, iconic image, adopted the brand ([40]). BMW, conversely, is cool in part because it has become a popular status symbol. These ten characteristics correlate with the perception that a brand is cool, distinguish cool brands from uncool brands, and comprise distinct but related components of a higher-order structural model of brand coolness. The Life Cycle of CoolnessOur research additionally contributes to theory on the dynamic nature of coolness (e.g., [33]) and brands (e.g., [63]) by exploring how the characteristics of cool brands change as a brand becomes niche cool, transitions from niche to mass cool, and eventually begins to lose its cool (see Figure 1). Brands initially become cool within a particular subculture (e.g., Quicksilver with surfers, Rocawear with hip-hop enthusiasts, Supreme with skaters) of people who perceive the brand to rebellious, autonomous, desirable, and high status and adopt it as a way to distinguish themselves from the masses. Some niche cool brands break free from subcultural obscurity to become cool to the masses. As brands such as Quicksilver, Rocawear, and Supreme expand from a fringe group of outsiders to mass-marketed magazines and suburban shopping malls, they start to seem less rebellious, original, authentic, and extraordinary—and less cool—to their original subcultural consumers (surfers, rappers, and skaters, respectively). But, despite losing some of their autonomy, mass cool brands also become more familiar, command a higher price premium, and control a larger market share. Purists may deride them for selling out, but brands perceived to be mass cool (e.g., Nike, Grand Theft Auto, Beyoncé) are more popular and profitable than their more obscure niche cool counterparts (e.g., Steady Hands, INSIDE, Mitsky). Mass cool brands, however, need to be careful not to lose the characteristics (e.g., desirability, autonomy) that made them cool in the first place, or they will become passé. We saw this in our data: while many consumers continue to think that Apple and Nike are cool, others are beginning to consider these brands uncool because they no longer see them as being rebellious, autonomous, high status, or as having the other characteristics that made them cool in the first place. Because we did not collect longitudinal data, our findings about the coolness life cycle remain preliminary. We strongly encourage future research to more closely investigate how brands change as they move from niche cool to mass cool to passé. Managerial ImplicationsFor many product categories and consumer segments, a brand's perceived coolness is an important factor in driving its success, and managers have long sought to figure out how to give their brands this mysterious quality ([ 2]; [31]; [57]). Yet the ways to make a brand cool have not thus far been systematically investigated, leaving managers without a clear roadmap.Our scale provides a valuable tool for helping firms create and manage cool brands. Unlike simple items that only measure overall brand coolness, our structural model allows managers to drill down into ( 1) which components of coolness are competitive strengths or weaknesses, ( 2) which components are of greater importance in shaping overall coolness, and ( 3) how these diagnostic analyses might vary across geographies, consumer segments, and even over time (i.e., as brand-health tracking metrics). Our scale components can also be used for pretesting and evaluating different marketing and communication programs that are designed to increase or maintain a brand's perceived coolness.How should managers respond if their brand is not scoring high enough on one or more component characteristics of brand coolness? They will need to reinforce the image of the band on the characteristic or characteristics it is lacking. How, specifically, firms should do this will depend on the brand's history, industry, and target customers, but we can offer a few tentative guidelines. Brands that want to be seen as more extraordinary will likely need to create breakthrough functional specs (e.g., being the first facial-unlocking smartphone) or deliver an unsurpassed customer service (e.g., Amazon) rather than offer incremental improvements (e.g., a slightly better smartphone camera) or ""run-of-the-mall"" service. To improve their aesthetic appeal, brands will need to create eye-popping designs; Apple and Nike, highly rated in our data and by pollsters, are known for this. Brands can become more energetic and original by continuously innovating and being one step ahead of the competition, like Google or Samsung Electronics. To be seen as authentic, brands will need to remind consumers of the history and core values of the brand and its founders (e.g., Patagonia does this effectively) while avoiding the appearance of using overt advertisements or other strategies associated with mass-marketed brands. Brands can appear more subcultural by using a promotion strategy that links the brand with an admired subculture (e.g., via brand community events, such as Harley-Davidson's annual rallies in Sturgis, South Dakota), as long as the tactics seem authentic. Brands could become more rebellious by hiring spokespeople known to challenge norms, as Nike recently did through its campaign featuring NFL outcast Colin Kaepernick. Brands can boost their perceived status through packaging, ad style, spokespeople, high prices, retail cobranding, and media placements that make the brand seem glamorous, sophisticated, and exclusive. Becoming iconic is not easy, but brands might be able to seem more iconic through distinctive packaging (e.g., the Coca-Cola contour bottle), a memorable advertising style (e.g., the early artistic and witty campaigns of Absolut vodka), or telling a brand myth that resonates with consumers (e.g., the nostalgic frontier story of Jack Daniels; [41]).Firms will also need to assess whether their brand is currently niche cool, mass cool, or uncool to understand how to best manage the brand's characteristics. An existing uncool brand might first need to become niche cool, by engaging in behaviors (products, promotions, pricing, and distribution strategies) that make the brand seem rebellious, original, and authentic. To become niche cool, brands will also need to cultivate a close relationship with a particular subculture rather than target the mass market (as Pabst did with hipsters in the early 2000s or as Instagram initially did with photography enthusiasts). After successfully becoming niche cool, brands could try to boost their popularity to transition to mass cool, but they will need to maintain their connection to a subculture (e.g., Nike to its top athletes) and its perceived autonomy (e.g., as Apple did by positioning itself as an edgier alternative to Microsoft) so they do not entirely lose their cool. Limitations and Future Research OpportunitiesMany important questions remain for future research. Among them is the question of how brand coolness relates to nomologically related constructs, especially brand personality. Our studies showed that the effects of brand coolness on brand attitudes, WOM, and WTP are partially or completely mediated by brand love and SBC. However, our mediation analysis measured (but did not manipulate) variables and could not test every possible mediation sequence. Thus, future research could use experimental techniques or cross-lagged analysis of time-series data to better test among possible causal sequences.Second, while our data established discriminant validity between brand coolness and related constructs, we did not have access to multitrait-multimethod data, which are necessary for more definitive conclusions in this regard, as well as for a stronger estimate of common methods bias ([64]). Our scale development and validation would also benefit from follow-up analyses with other types of data (e.g., using within-brand variance across individuals), as [34] point out in the context of brand personality scales.Third, although we collected data from multiple cultures, we did not formally investigate cross-cultural differences. Given the cultural differences observed in brand personality ([36]), more work is needed to investigate if and how the characteristics or consequences of brand coolness vary across cultures. Brands that are rebellious, subcultural, and autonomous may be more cool in relatively independent cultures (e.g., United States, Germany) than in interdependent ones (e.g., Korea, Japan; [62]), whereas brands that have high status may be more cool in cultures higher on power distance (e.g., India, China). Within cultures, individual differences in need for uniqueness ([77]), counterculturalism ([78]), susceptibility to interpersonal influence ([10]), symbolic capital ([39]), and others may influence which characteristics consumers consider cool and which consumer segments thirst more for cool brands. Given that coolness is subjective, it will be especially important for future research to investigate which social, cultural, individual difference, and category characteristics moderate what consumers perceive to be cool and how they respond to cool brands.Future research should also further examine the relationship between the specific coolness components, overall brand coolness, and downstream consequences such as brand attitudes, WOM, and WTP. The structural model coefficients estimated in Studies 5–8 (Table 3) and our experiment suggest that the ten characteristics independently contribute to overall brand coolness, but our studies do not offer strong tests of whether these characteristics might interact. In particular, future research should further investigate the relationship between rebellion and coolness. In our data, the ""main effects"" of rebellion on higher-order brand coolness were almost always the lowest across our ten first-order factors, suggesting that higher perceived rebelliousness does not by itself always raise overall brand coolness as much as other components (such as originality and authenticity) do. In summary, much remains to be understood about the important brand management construct of brand coolness, and we encourage researchers to further investigate why, how, and when coolness contributes to a brand's success. "
9,"Branding Cultural Products in International Markets: A Study of Hollywood Movies in China Cultural products are a major component of the world economy and are responsible for a growing share of U.S. exports. The authors examine brand name strategies when cultural products are marketed in foreign countries. Incorporating the unique characteristics of these products, the authors develop a theoretical framework that integrates similarity, which focuses on how the translated brand name relates to the original brand name, and informativeness, which focuses on how the translated brand name reveals product content, to study the impact of brand name translations. The authors analyze Hollywood movies shown in China from 2011 to 2018. The results show that higher similarity leads to higher Chinese box office revenue, and this effect is stronger for movies that perform better in the home market (i.e., the United States). When the translated title is more informative about the movie, the Chinese box office revenue increases. The informativeness effect is stronger for Hollywood movies with greater cultural gap in the Chinese market. Moreover, both similarity and informativeness effects are strongest when the movie is released and reduce over time. This research provides valuable guidance to companies, managers, and policy makers in cultural product industries as well as those in international marketing.KEYWORDS_SPLITCultural products are goods and services that include performing and visual arts, heritage conservation (e.g., museums, galleries), and the media (e.g., publishing, broadcasting, movies, recording) ([ 1]).[ 7] The production, circulation, and consumption of cultural products are a major component of the national and global economy. In its most recent survey, the [10] shows that arts and cultural economic activities accounted for over US$800 billion in the United States in 2016. A recent report estimates that cultural products generate over US$2,250 billion of revenue and 30 million jobs each year worldwide ([77]). Recent developments in digital technologies are further enhancing the value and influence of cultural products: their distribution has become increasingly global, and consumers of these products are connected across the world on an unprecedented scale ([59]).Associated with these market and technological trends, cultural products are responsible for a growing share of U.S. trade, and international markets have become a major target for the producers ([30]). In the film industry, for instance, more than 70% of Hollywood's box office revenue now comes from overseas ([54]). The influence of international markets has become so important to Hollywood that ""potential overseas ticket sales nowadays determine whether or not a studio executive gives the go-ahead to a movie"" ([ 8]).Similar to other products, when a cultural product is introduced into a foreign market, a key decision is the brand name: the company must decide how to translate the original brand name into the foreign language ([42]). Yet unlike noncultural products (e.g., consumer packaged goods, apparel, electronics, appliances, automobiles) that have been the focus of prior international marketing research, cultural products are associated with several unique challenges for brand name translation. First, cultural products such as movies and books are prime examples of experiential products ([35]). For these products, consumption experience is difficult to evaluate prior to purchase. It depends critically on the creative content, such as the plot and characters of a movie and the storyline of a novel ([24]; [75]). Therefore, consumers often utilize extrinsic cues to help them make the purchase decision ([ 7]). Prior research has suggested that the brand name is an important extrinsic cue for product quality ([83]). Therefore, brand name plays an important role in the consumer purchase of cultural products. For instance, the title of a movie or book can be used to suggest the storyline or the main character. When consumers have different cultural backgrounds, whether and to what extent to provide such information are important decisions in the brand name translation for cultural products in international markets.Second, by definition, culture-specific factors play a key role in consumer evaluation of cultural products. This is not the case for many noncultural products. For instance, across different countries, the quality of an automobile is evaluated by a similar set of factors (e.g., reliability, mechanical performance, maintenance cost). However, a movie that is rooted deeply in the culture or heritage of one country may not be easily understood in another ([20]; [30]). Thus, brand name translation of cultural products must take into account how culture-oriented nuances can be better understood in a foreign market where cultural distance exists.Third, the sales period of noncultural products is typically long—companies that sell detergent, electronics, or cars can take an extended time to build brand awareness and consumer preference. In contrast, many cultural products have relatively short life cycles. This is particularly true for entertainment products such as movies, TV shows, and video games. For example, most movies are in theaters for only six to ten weeks ([12]; [43]). Thus, while it is possible for firms to use the original brand name of noncultural products in a foreign country where the language is different (e.g., to use Chrysler and IBM brand names in Japan) and gradually deploy marketing tools to increase sales, it is difficult to do so for cultural products. It is thus critical for the brand names of these products to be translated properly to foster consumer gain awareness and promote content. Furthermore, given the short time window available to attract customer attention, the impact of translated brand names on product sales can be more significant for cultural products than for noncultural products.Fourth, media coverage and peer-to-peer communication in the form of physical or online word-of-mouth communication, search, and participation in experiential activities are often very active for cultural products ([37]; [53]).[ 8] This fluid and far-reaching information, coupled with a short life cycle, can induce intense and fluctuating demand for cultural products—sales tend to be fast changing over weeks or even days. It is therefore both interesting and important to study the dynamics of how brand name translation affects product sales.In this article, we take these challenges into consideration and examine how different brand name translation strategies affect the sales of cultural products in international markets. In our theorizing of the effects and developing hypotheses, the context of international markets calls for insights from international marketing literature, and the focus on cultural products requires us to conceptualize the extent to which a translated brand name reveals product content. We test the hypotheses with Hollywood movies shown in the Chinese market from January 2011 to June 2018. This empirical context is valuable for several reasons. Movies are the prototypical example of cultural products. As a key element that builds identity and awareness, the title can be viewed as the brand name of a movie. Although a rich literature has generated useful insights into the drivers of movie sales, no research has examined the effects of movie titles in international markets. Moreover, the relationship between Hollywood studios and the Chinese market is among the most critical in the international movie market. In 2012 China overtook Japan to become the largest theatrical market outside the United States, and in 2016 the number of screens in China surpassed those in the United States ([73]). Consequently, Hollywood studios have a keen interest in marketing their products in China. A study of brand name strategies for Hollywood movies in China thus has strong substantive value. Finally, English is a phonographic language, which represents the sound components of the spoken language using letters or syllabic symbols. Chinese, however, belongs to the logographic system, which represents the meanings of words in the form of symbols ([ 3], p. 467). From a linguistic point of view, the most challenging practice of brand name translation occurs between a phonographic system and a logographic system ([67]). However, such translations are frequently encountered by multinational corporations ([25]).Our research provides four main contributions to the literature. First, to the best of our knowledge, this is one of the first studies on brand name translation strategies for the sales of cultural products in international markets. Given the growing importance of these products in the marketplace, as well as their unique characteristics, a study that considers the roles of brand names, cultural gap, and temporal dynamics is warranted.Second, we contribute to the international marketing and branding literature streams by developing a new theoretical framework for the impact of brand name translation. The majority of research in this literature has focused on the impacts of standardization versus adaptation. The existing analysis, however, does not capture some key nuances embedded in the aforementioned cultural products. We thus complement similarity, a key element in the strategy of standardization, with the informativeness of translated brand names to construct an integrated framework. Because marketing standardization can be achieved through strategies such as advertising, pricing, supply chain, and distribution channels, our study contributes to the international marketing literature by focusing on brand name translation, a particular route that has so far received only limited attention.Third, we consider temporal dynamics and, in the U.S.–China movie context, we estimate how the translation of movie titles influences sales throughout the theatrical release. This makes ours one of the few studies on brand name strategies to examine the longitudinal effects of brand name translations in international markets.Finally, our study makes an important contribution to the movie research literature, given that very limited research has examined the impact of movie titles. The empirical study of movie sales in China also provides a contribution to research on emerging markets (e.g., [56])—in particular, we answer the call for expanding research to emerging markets to ""ask and answer new questions related to consumers, cultures, institutions and regulation"" ([55], p. 473).The remainder of the article is organized as follows. The next section discusses the related literatures on branding and movie research. We then present our theoretical framework, followed by the empirical study including data, model specifications, identification method, and findings. We conclude with managerial implications and directions for future research. Literature ReviewThe backdrop of our research is international marketing, with a focus on branding strategies for movies as cultural products. Research in both international marketing and movies has examined a wide variety of topics. In this review, we focus on three streams of literature that are most relevant to our theory and empirical analysis: ( 1) marketing standardization in international markets, ( 2) brand name translation, and ( 3) movie research that examines box office sales in international markets and movie title effects on sales. Marketing Standardization in International MarketsFor cultural products and brand name translation strategies, international sales are influenced by the transfer of product quality information (e.g., online ratings) as well as the marketing influence (e.g., caused by promotional spending) from the home market to the foreign market. A major driver of the transfer is the extent to which the translated brand name and the original brand name resemble each other. Such resemblance (referred to as ""brand name similarity"" in our conceptualization) is one element of marketing standardization, which refers to the general practice of applying common marketing strategies across different countries ([65]).Beginning with pioneering works such as [50] and [80], standardization versus adaptation has been one of the most important strategic considerations in international marketing ([86]). A core argument in this literature is that, when a company enters international markets, standardized marketing strategies help lower market entry cost on the supply side and capitalize brand assets from the home market on the demand side ([ 4]). This is particularly important when competition is fierce and consumers live an increasingly connected world. [65] provide a survey of research that examines performance implications of marketing program standardization. They find that ""the majority of studies have indicated that the pursuit of standardized marketing activities by itself has mostly a positive impact on performance"" (p. 26). For instance, [58] compare U.S. and European companies operating in both a developed economy (Japan) and an emerging market (Turkey). They find that marketing standardization leads to higher performance. [71] find that perceived brand globalness as a result of consistent brand positioning across countries generates higher purchase intention.However, when international market environments are very different from each other, marketing strategy standardization becomes less effective. This contingency view is proposed and tested by [38] on international subsidiaries of U.S., Japanese, and German companies. The authors show that strategy standardization leads to superior performance only when there is fit between a company's market environments (e.g., regulations, technology, customers, traditions) and its international marketing strategy choice.We want to highlight three areas in which our study relates to (but also deviates from) the previous studies in this literature. First, the research on standardization initially focused on advertising before examining a broader array of marketing activities ([58]; [63]). Subsequently, scholars have studied standardization in the product mix, advertising and promotion, channels of distribution, sales, pricing, and marketing program or brand positioning as a whole (e.g., [17]; [38]; [44]; [45]; [79]). However, the impact of the brand name, a critical component of marketing strategies, has received scant attention. [25] is one of the few extant studies on marketing standardization that focuses on brand names. Moreover, a literature review by [65] on the impact of standardization reveals that the research on brand name strategies is lacking.Second, in terms of methodology and context, the vast majority of the research on standardization utilizes surveys of executives and focuses heavily on consumer packaged goods. Few studies use market-level data, and the cultural industry remains largely unstudied.Third, the conceptualization and measurement of marketing standardization began with a dichotomous approach to describe whether the firm uses the same marketing strategy in international markets as in its home market. It is now well accepted that standardization should be measured on a continuum and applied to different aspects of marketing actions ([38]; [63]). Brand Name TranslationAlthough the standardization literature has not focused much on brand names, several scholars have studied brand name translation through the lens of linguistics. [84] classify the translation between a phonographic language and a logographic language into three types: phonetic translation (by sound only), semantic translation (by meaning only), and phonosemantic translation (by sound plus meaning). Because phonetic translation uses local alphabets or words to mimic the pronunciation of the original brand, the translated brand name does not carry any real meaning. In a bilingual setting, [85] demonstrate that consumers' proficiency in the language associated with the original brand name moderates the effects of different translation methods. Extending the phonetic–semantic framework, [81] classify translated brand names into four types—alphanumeric, phonetic, phonosemantic, and semantic, ranging from the least meaningful to the most meaningful—and estimated their effects on automobile sales. They found that Chinese consumers' preference for vehicle models is strongest when the translated brand name is semantic and weakest when it is phonosemantic. The brand name translation studies provide a valuable foundation for our research, because similarity between the original and translated brand names, a major component in our theoretical framework, is driven by how similar the brand names are to each other in both meaning and sound.Table 1 provides a summary of the previous studies on brand name translation. Our research differs from these studies in three important ways. First, all prior research examines either consumer packaged goods or durable products; none speaks to cultural products, the focus of our research. Second, no prior research has examined temporal effects associated with brand name translation. We study several moderation effects, including temporal dynamics, to enhance the insights and managerial implications. Third, with the exception of [81], all the other studies use either lab experiments or surveys. We analyze field data in movies to examine brand name translation strategies for cultural products.GraphTable 1. Prior Research on Brand Name Translation in International Markets.  From a theoretical perspective, almost all prior research in the brand name translation literature is based on a semantic–phonetic paradigm. This is not surprising, because the product categories studied previously (consumer packaged goods and automobiles) can be translated semantically, phonetically, or a combination of both ([25]). However, cultural products are different: the emphasis on content requires the translation of brand names to be heavily semantic-focused—phonetic translation is rarely used.[ 9] Therefore, the semantic–phonetic framework is not an effective approach to examine brand name translation for cultural products. We thus attempt to build a theoretical framework that incorporates the standardization perspective for international markets and the informativeness perspective for cultural products. Movie Sales in International Markets and Movie Title EffectsCompared with the large number of movie studies that focus on the U.S. market, the research that examines international markets is relatively limited ([ 2]; [16]). Most extant research has examined the sales of U.S.-produced movies in overseas markets (e.g., [ 2]; [19]; [32]; [41]). For example, [32] propose a conceptual framework for the success of U.S. movies in Germany. [ 2] study the sales of U.S. movies in 27 countries and find that cultural variation across countries (e.g., individualism vs. collectivism, high vs. low avoidance for uncertainty) moderates the impact of factors such as star power and sequel on movie sales. A few studies include multiple countries of movie origin and sales into the analysis ([23]; [27]). For instance, [27] investigate the sales of foreign movies in Singapore and report that economic and cultural factors account for release frequencies and box office performance. Little published work has examined the strategies or sales of Hollywood movies in major emerging markets such as China, even though these markets have become increasingly critical for the studios.Related to the idea that movie title can be regarded as a brand, several studies provide valuable insights on movie title strategies. [70] conceptualize sequels as brand extensions of experiential goods. [33] develop a method in the context of movie sequels to measure the monetary value of brand extension. Although these studies focus on brand extension (through sequels), none consider the options and consequences of brand translation in international markets. Conceptual BackgroundProduct sales in international markets involve two basic influences related to brand names: ( 1) the cross-market effect, in terms of the connection between the original brand name and the translated brand name, and ( 2) the effect of the translated brand name itself. The former drives the extent to which product quality information and marketing activities in the home market can influence the foreign market. The latter is the brand name to which consumers in the foreign market are directly exposed. We capture these two influences through the overall similarity between the original brand name and the translated brand name as well as the informativeness of the translated brand name. We also examine how the effects of similarity and informativeness are moderated by three contingency factors: home-market performance, cultural gap, and time after release. Figure 1 presents the conceptual framework.Graph: Figure 1. Effects of brand name translation on cultural products. Similarity Between Translated and Original Brand NamesThe theoretical underpinning for the effect of brand name similarity between the home market and international markets is as follows. When a product enters a foreign market, a similar brand name helps establish a connection in consumers' minds for the product across different markets. This generates both supply-side and demand-side benefits. On the supply side, similar to the situation of sales in sequential distribution channels ([49]), the effects of promotion and other marketing efforts in the home market are more likely to be transferred to the foreign market when the brand names are more similar. Internet and social media are both important facilitators of such information spillover. This essentially produces economy of scale for marketing investment ([58]) and helps generate ""prerelease consumer buzz,"" which, according to [37], is composed of communication, search, and participation in experiential activities. On the demand side, similar brand names reduce the time to build brand recognition ([71]). Akin to the effect of brand extensions (when a new product is connected to an established one through the same brand name), sales of the new product can be increased through enhanced familiarity ([78]).These supply-side and demand-side benefits are significant for cultural products. In the case of movies, Hollywood studios spend, on average, an amount equivalent to 50% of a movie's production budget on domestic promotion (mostly network television and online advertising; [62]). The studios also engage in extensive public relations campaigns, often in partnership with the cast, to promote movies. The awareness and publicity generated by these efforts, in addition to buzz from the marketplace, will benefit the movie in international markets when this publicity can be more easily connected to the original movie. As we discussed previously, the cultural products' short life cycles dictates that awareness and consumer preference for them must be built quickly in international markets. Similar brand names help achieve these goals.For Hollywood movies in China, the connection between the original English title and the translated Chinese title is facilitated by the fact that many Chinese audiences are aware of the original title. A major driver behind this awareness is the global flow of entertainment news and information through both mainstream media and the internet. As in other countries, the majority of Chinese movie audiences are young—frequent moviegoers are under 35 years old and have a college education.[10] They have strong interests in entertainment products such as movies, TV shows, and video games from Western countries and are generally aware of these products' English titles ([69]). Therefore, we propose the following hypothesis: H1:  Sales of a cultural product in a foreign country are higher if the brand name is more similar between the foreign market and the home market. Informativeness of Translated Brand NameIn the broader literature on how brand names affect consumers, informativeness (or ""suggestiveness,"" as used in some prior studies) refers to whether a brand name provides information about product attributes ([39]; [68]). Previous research has generally found a positive impact. For example, a more informative brand name generates a higher level of recall for brand advertising ([39]), better recognition of brand extensions ([68]), and better memory ([48]).For cultural products, the primary attribute that brand names help convey is the content (e.g., storyline, character, cast). As we have discussed, this content is the key selling point for cultural products ([24]; [75]). Consumers often seek content that reflects and reinforces certain psychological and sociological profiles related to their societies, cultures, personalities, and interests ([61]). Brand names that are more informative about content are thus more effective in drawing attention and communicating those features to consumers. Consequently, products with more informative brand names can expect faster information dissemination and greater awareness, interest, and purchase intention.The informative roles that brand names play become even more important in international markets. Due to differences in languages, history, and traditions, consumers in a foreign country often need more information to appreciate the cultural background and social significance of a cultural product than do consumers in the home market. The disconnection between the home and foreign markets is commonly referred to as ""cultural gap"" ([36]). The basic idea is that a product can be valued to a lesser extent by foreign audiences who lack the cultural background and knowledge needed for full appreciation of the product ([46]). Take movies as an example. The names of certain people, events, and locations in the United States, such as those pertaining to the American Civil War, can trigger interest and emotion for domestic audiences who are aware of their significance. For foreign audiences who are unfamiliar with these names, however, some indication about the background or storyline through the movie title can provide useful guidance.Thus, for cultural products in international markets, the informativeness of translated brand names for product content should have an important influence on product sales. We create the following hypothesis: H2:  Sales of a cultural product in a foreign market are higher if the translated brand name is more informative of product content. Synergy Between Similarity and InformativenessBecause similarity and informativeness capture two different aspects of brand name translations, there could be a synergy effect between them beyond the individual effects hypothesized so far. When a higher level of similarity builds connection between the translated brand name and the original brand name, which generates both supply-side and demand-side benefits, the product will receive greater awareness and attention in the foreign market. This produces two different types of synergies. First, greater attention and awareness make it easier for consumers in the foreign country to notice the product. The translated brand name is one of the most salient elements of the product, and it is usually the first element to which consumers are exposed. Thus, the translated brand name will be more likely to receive attention, causing its characteristics (including informativeness) to have a greater impact.Second, when consumers pay more attention to the translated brand name, their ability to process the information contained in the brand name will increase. Consistent with the elaboration likelihood model of persuasion, consumers will be more likely to process the translated brand name in a central or systematic route ([60]). The information contained in the brand name will thus have a greater impact.Drawing on these synergies, we propose that the effect of informativeness of the translated brand name will be enhanced if there is a greater similarity between the translated brand name and the original brand name. This is summarized in the following hypothesis: H3:  The (positive) effect of informativeness in the translated brand name on the sales of a cultural product is stronger when the translated and original brand names are more similar. Moderation by Home Market Performance and Cultural GapAs Figure 1 shows, we also examine the roles of contingency factors that can moderate the effects of brand name translation. Developed on the basis of similar theoretical grounds as discussed in previous sections, these contingency effects provide further managerial guidance on brand name translations for different cultural products in international markets. For instance, H1 and H2 suggest that similarity and informativeness are both beneficial. While achieving both is possible for some products, it may be difficult do so for others. It is thus important to understand how a trade-off between the two objectives depends on specific product characteristics. Such a contingency view is an important contribution to the literature, which has focused on the main effects of brand name translations ([66]). Effect of similarity moderated by home market performanceWe aim to understand under what conditions the connection established by similar brand names is more impactful on international sales. A product's home-market performance is a contingency factor that has important implications for the connection effect.The theoretical base of similarity is the connection invoked in the minds of foreign consumers between the translated brand name and the original brand name. As a corollary of such connections, consumer perception of the product in the foreign market should be influenced by product success in the home market. The more successful the product is in the home market, the more positively foreign consumers will react when it is introduced overseas. Conversely, if the product is less successful in the home market, foreign consumers will be less enthusiastic about it.For movies in international markets, domestic box office revenue is by definition the home-market performance. Because of the experiential nature of movies, quality is difficult to judge prior to consumption ([53]). Therefore, consumers often seek product quality signals to help them make purchase decisions. [23] and [ 5] have shown that domestic box office revenue is a key signal of movie quality for audiences in subsequent exhibition channels. For audiences in a foreign country, the linkage of an imported movie to the movie's home-market performance becomes stronger when the original title and translated title are more similar, because greater similarity enhances the perceptual connection in consumers' minds between the home-market version and the overseas version of the movie. Therefore, we have the following hypothesis: H4:  The (positive) effect of similarity between translated brand name and original brand names on the sales of a cultural product in a foreign market is stronger when the product has higher sales in the home market. Effect of informativeness moderated by cultural gapH2 hypothesizes that a higher level of informativeness in translated brand names enhances sales because it conveys more content information to consumers in international markets. Consistent with this reasoning, the need for suggesting content should be an important contingency factor for the informative effect. Some cultural products are more deeply rooted in the social and cultural background of the home country than others, making it more difficult to be understood by foreign audiences and thus to attract attention. The 2016 Ben-Hur remake and its performance in China provide an illustration. The movie is an adaptation of a classic novel (Ben-Hur: A Tale of the Christ) and has a religious setting; both the novel and the religious context are rather unfamiliar to Chinese audiences. The movie generated a box office of only 17.48 million RMB, which is equivalent to less than 10% of its U.S. box office sales. Other examples include movies such as the 1993 history drama Gettysburg, which is based on the battle of Gettysburg in the American Civil War, and the 2018 drama Beautiful Boy, which explores the experiences of a family coping with addiction. Although U.S. audiences can relate to the Civil War and addiction problems, audiences in other countries may be unfamiliar with these issues.When there is a greater cultural gap between the cultural product and the foreign market, it becomes more valuable to use the brand name to provide ""hints"" for foreign audiences. The return will be better understanding of the product, greater appreciation of its value, and higher purchase intention. We thus create the following hypothesis: H5:  The (positive) effect of informativeness in the translated brand name on the sales of a cultural product in a foreign market is stronger if there is a larger cultural gap between the product and the foreign market. Temporal Dynamics in the EffectsBecause translated brand names help convey information to consumers about product quality (via similarity) and content (via informativeness), the impact of different translations should depend on consumers' need for product information. When consumers have other ways of learning about the product, such need is reduced, and the effects of brand names will be smaller. This is consistent with the idea that product information is most impactful when consumers have few alternative learning channels ([28]). In addition to the availability of alternative product-related information, the amount of time that has passed since product launch is also a factor. Thus, we argue that the effects of brand name translation will exhibit temporal dynamics and change over time.In general, there is more limited information when a new product is introduced than after the product becomes more widely adopted. Much of the early information is in the forms of adverting, public relations, and other firm-initiated communication. As the product continues to sell, a greater amount of alternative information—especially word of mouth, which has important impacts on consumers—becomes available. The need for product-related information is thus greater in the earlier stage of product sales than in the later stage.These temporal patterns hold for cultural products ([13]; [82]) and in international markets. When the product is first introduced, foreign consumers will rely to a large extent on observable product attributes, among which brand name is highly salient, to judge product quality and content. Thus, the effects of similarity and informativeness related to the translated brand name should be higher. As time passes, other information, including reviews and consumer word of mouth, will become prevalent such that the impact of the brand name reduces. We thus propose, H6:  The sales effects of similarity and informativeness of brand name translation decrease over time after a cultural product is released in a foreign market. DataWe collected information for all Hollywood movies exported to China from January 2011 to June 2018. During this period, a total of 348 U.S. movies were exhibited in China.[11] Most of these movies (329 of 348) were released in the United States either earlier than or at the same time as in China. We follow prior studies ([23]; [57]) in focusing on these movies so that the sequence of theatrical releases in domestic and foreign markets is kept consistent. We also dropped another 12 movies that were missing data on important characteristics such as production budget. The final sample thus includes 317 movies, which accounted for 97% of the total Chinese box office revenue of all Hollywood movies during the sample period. In Web Appendix, we provide the list of these movies and related information (Table W1).For each movie, we obtain the information for the Chinese market, including weekly box office revenue, release time, and weekly number of screens from EntGroup Inc., the leading information provider for the entertainment industry in China. We obtain other information, such as U.S. revenue, production budget, genre, sequel information, cast, distributor, and MPAA rating, from the Internet Movie Database (IMDb). We obtain consumer word of mouth from Douban, which is similar to IMDb and is the most popular movie information and forum website in China by the number of visitors. Key Variables Similarity and informativeness of translated movie titlesWe follow the literature in international marketing (e.g., [58]; [65]) to measure the degree of similarity of titles for each movie. Three coders proficient in both Chinese and English independently rated the overall level of similarity between the translated Chinese movie title and the original English movie title on a seven-point scale (1 = ""not at all similar,"" and 7 = ""very similar""). Unlike the studies that focused on a semantic–phonetic perspective ([81]; [84]), we take a more holistic approach so that similarity can occur in meaning, sound, or both. This is also theoretically important because meaning similarity and sound similarity can both elicit the connection between the original and translated movie titles. Ratings from the three coders were highly consistent with an intraclass correlation coefficient (ICC) at.84 (F = 6.32, p <.01) ([18]). For each movie, we take the average score of these three coders as the similarity measure.We followed a similar rating procedure to rate the degree of informativeness for the translated movie titles. We obtained the synopsis for each movie from Douban. The movie titles and synopses were then provided to three coders who were different from the previous coders of similarity. For each movie, they independently rated the extent to which the Chinese title reflects the movie storyline, as described in the synopsis, on a seven-point scale (1 = ""does not reflect the storyline at all,"" and 7 = ""fully reflects the storyline""). The ratings were again highly consistent (ICC =.74, F = 4.03, p <.01). As with similarity, we take the average score of the three coders as the informativeness measure for each movie. Table 2 provides several examples from our data to illustrate different movie title translations.GraphTable 2. Movie Title Translation Examples.   Dependent and moderating variablesIn our analysis, the dependent variable home market sales performance is weekly Chinese box office revenue for each movie. We follow [23] to obtain each movie's gross box office revenue in the United States from The Numbers (www.the-numbers.com).Movie genre is a key characteristic for coding cultural gap. Prior research has shown that drama, comedy, horror, musical, and Western movies are overall more content intensive and have greater country-specific elements than other genres ([20]; [26]; [46]). They are thus associated with a higher level of cultural gap than other genres. In contrast, action, adventure, and thriller movies tend to have universal appeal across cultures ([40]; [47]; [57]). For instance, by analyzing box office performances of selected movie genres in the United States versus in Hong Kong, [46] finds that comedies exhibit a high degree of cultural gap, whereas action movies are associated with low cultural gap. We thus code a dummy variable for cultural gap that equals 1 for drama, comedy, horror, musical, or Western movies and 0 for other movie types.[12]In addition to the movie literature discussed previously, our data provide additional evidence for this genre classification. We calculated the correlation coefficient between consumer review scores in the United States (i.e., from IMDb) and consumer review scores in China (i.e., from Douban) for each Hollywood movie in our sample. We found that, consistent with the cultural gap classification, U.S. audiences and Chinese audiences are more similar at evaluating action, adventure, and thriller movies than drama, comedy, horror, musical, and Western movies (correlation coefficient =.68 vs..57). Control VariablesPrior research has extensively studied the drivers of movie box office revenue.[13] First, we control for the number of weeks that have passed since the movie's release to capture the common ""decay effect"" of movie sales over time ([21]). We also follow the literature to include the following movie characteristics as controls in the estimation: weekly number of screens ([23]); production budget ([14]); gap between U.S. and Chinese releases ([23]); whether the movie is a sequel ([52]); consumer word of mouth (proxied by consumer review score; [15]; [53]); whether the movie cast includes a star, which equals 1 if any member of the cast or the director has been nominated for or won an Academy Award, and 0 otherwise ([22]); whether the movie is distributed by a major U.S. studio ([23])[14]; a set of dummy variables forMPAA ratings ([ 5]); and a set of movie genre dummy variables ([52]).Moreover, for each movie, how the title is translated into Chinese depends on the characteristics of the original English title. A particular factor is the degree of clarity (or ambiguity) in the original title.[15] A clearer title is more revealing about the content and is more likely to translated with a higher degree of standardization. Therefore, we construct a variable to measure the degree of clarity in the original title in English for movies in our sample. We recruited three native English speakers who work as English teachers at a major university in Shanghai. We provided them with the list of English movies titles and the synopsis of each movie, obtained from IMDb. Using a seven-point scale, they independently rated whether a movie title clearly describes the information in the synopsis (1 = ""totally unclear,"" and 7 = ""totally clear""). Interrater reliability is high (ICC =.77, F = 5.19, p <.01). The average rating is used for each movie.Movie box office sales are well known for seasonality. We include in our estimation three sets of time-related fixed effects: week (52 dummy variables for each week of the year), year (a dummy variable for each year in the sample period), and Chinese public holidays. Because the public holidays are mostly based on the traditional (lunar) Chinese calendar, they are at different dates in different years. Thus, holiday fixed effects need to be separately accounted for beyond week and year fixed effects. Table 3 provides the definition and summary statistics of all variables, as well as the correlations between them.GraphTable 3. Variable Operationalization, Summary Statistics, and Correlation Matrix.   Empirical MethodsTo provide some anecdotal evidence for the relevance of movie titles for Chinese consumers, we conducted a survey on WJX.cn, the largest online survey platform in China. The survey included 214 Chinese consumers with ages ranging from 18 to 55 years old and an average age of 29 years old. Among the respondents, 66.4% reported that they pay attention to movie titles when they choose a movie to watch and 88.3% would ""sometimes,"" ""often,"" or ""always"" make an inference about a movie's storyline from the title. For imported movies, a majority of the respondents (54.2%) reported that they would associate the translated title with the original movie title. Among the respondents who are frequent moviegoers (i.e., those who watch a movie in the theater at least once a month), 62.6% would make this association. While preliminary, this survey shows that ( 1) movie titles matter for Chinese audiences in movie selection, ( 2) movie titles are used to infer movie content, and ( 3) a majority of audiences tend to make a connection between the translated Chinese title and the original English title. Model SpecificationWe estimate the following model for the effects of brand name translations: lnWeekly_RevenueitCN =α+β1SimilarityiCN+β2InformativenessiCN +β3SimilarityiCN×InformativenessiCN +β4SimilarityiCN×lnGross_RevenueiUS +β5InformativenessiCN×Culture_Gapi +β6SimilarityiCN×Weekit+β7InformativenessiCN ×Weekit+ψWeekit+θlnGross_RevenueiUS +γlnWeekly_ScreenitCN,+δXi+μMPAAi +ηGenrei+λTime_Controlt+σi+∑it, Graph1where  Weekly_RevenueitCN  is the weekly box office revenue of movie i in China,  SimilarityiCN  is the similarity score of the translated Chinese title in relation to the original English title for movie i, and  InformativenessiCN  is the informativeness score of the translated Chinese title for movie i.  Gross_RevenueiUS  is movie i's gross box office revenues in the United States.  Culture_Gapi  is the dummy variable for cultural gap:  Culture_Gapi=1  for drama, comedy, horror, musical, and Western movies and 0 otherwise. Weekit  is the number of weeks since the release of movie i, and it captures the common ""decay effect"" of movie sales over time. Distribution intensity is captured by  lnWeekly_ScreenitCN  , the weekly number of screens for movie i in China, and we follow previous literature (e.g., [23]) to use logged values to account for potential nonlinearity.  Xi  includes the movie characteristics described previously: production budget, release gap, sequel, consumer word of mouth, star in the cast, and major movie studio.  MPAAi  and  Genrei  are MPAA rating and genre dummies, respectively.  Time_Controlt  contains the three sets of time-related fixed effects (i.e., week, year, and Chinese holiday fixed effects).  σi  is the random effect for movie i, which captures the unobservable factors that may affect movie box office.We expect  β1  and  β2  to be positive for the impact of similarity (H1) and informativeness (H2) and  β3  to be also positive for the interaction effect of similarity and informativeness (H3). Positive estimates of the interaction effects,  β4  and  β5  , would support H4 and H5. Finally, as we state in H6, we expect  β6  and  β7  to be negative as the effects of movie titles decrease over time. Correction for Endogeneity Endogeneity in movie title translationEven with the rich set of controls in the estimation equation, the error term in Equation 1 could still contain unobserved movie characteristics that are potentially correlated with both movie title translation (i.e.,  SimilarityiCN  and  InformativenessiCN  ) and box office revenue. Not accounting for this endogeneity due to omitted variables would lead to biased estimates for the effects of movie title translation on box office revenue.To address this issue, we need instrumental variables that correlate with how movie i's title is translated (i.e., correlate with  SimilarityiCN  and  InformativenessiCN  ), but not the error term. Following the approach of [15], we use the similarity and informativeness of translated titles of competing movies as instruments. Specifically, the instrument for  SimilarityiCN  is the average similarity of the movies released in the same year as movie i, calculated as  ∑j≠i (SimilarityjCN  | j and i were released in the same year  )  ; similarly, the instrument for  InformativenessiCN  is calculated as  ∑j≠i (InformativenessjCN  | j and i were released in the same year  )  . The rationale is that industrial trends in movie title translation should correlate with how a movie title is translated but not correlate with the unobserved characteristics that affect a particular movie's box office revenue. Endogeneity in screens Weekly_ScreeniCN  in Equation 1 can also be endogenous because distributors can adjust the number of screens for a particular movie based on qualities unobserved by researchers ([15]; [23]). We follow [15] to construct the instruments for  Weekly_ScreenitCN  , which includes two sets of variables: ( 1) the average weekly number of screens in China for Hollywood movies of the same genre as movie i during the same year and ( 2) the average weekly number of screens in China for Hollywood movies of other genres during the same year. Analyses and FindingsWe estimate Equation 1 with the aforementioned instruments using two-stage least squares ([15]; [21]; [34]). Table 4 presents the estimation results.GraphTable 4. Effects of Brand Name Translation on Weekly Box Office Revenue in China.  1 * Significant at the 10% level.2 ** Significant at the 5% level.3 *** Significant at the 1% level.4 Notes: Robust standard errors are in parentheses. DV = dependent variable; IV = instrumental variable.We first evaluate whether movie title translation (i.e., similarity and informativeness) can indeed help explain movies' box office revenues. As the model fit statistics indicate, adding  SimilarityiCN  and  InformativenessiCN  significantly improves model fit (  Δχ22=  3,728.46, p <.001), which is further increased by the inclusion of the interaction effects (  Δχ25=  1,498.36, p <.001). We also calculate  R2  , Akaike information criterion, and Bayesian information criterion for each specification. All indicate that the variables for similarity and informativeness of movie title translation help improve model fit and are valuable in explaining the variance in box office sales.We evaluated the strength and the relevance of our instruments. Table W2 in the Web Appendix shows the first-stage results for all endogenous variables. In all first-stage estimations, the null hypothesis of weak instrument can be rejected. We further conducted the Sargan test for overidentification ([64]; [81]). As Table 4 shows, the tests under all specifications are not statistically significant, suggesting that the instruments are uncorrelated with the error term. Effects of Similarity and InformativenessColumn 2 of Table 4 shows how similarity and informativeness affect the movie sales in China. As it shows, the parameter estimate for similarity is positive and statistically significant (  β1  =.0222, p <.05). In support of H1, this shows that greater similarity between translated and original movie titles helps increase box office sales in the foreign market. The parameter estimate for informativeness is also positive and statistically significant (  β2   =.0246, p <.05). H2, which suggests that international market sales benefit from a translated movie title that reveals movie content to a greater extent, is thus supported. Interaction EffectsColumn 3 of Table 4 is the full model of our estimation. As it shows, the parameter estimate for the interaction term of movie title similarity and informativeness is positive and significant (  β3   =.0114, p <.10). This provides the support for the synergy effect that we hypothesized in H3—for a translated brand name, higher similarity with the original brand name enhances the impact of its informativeness.H4 predicts that the positive effect of brand name similarity will become stronger when the product has better sales performance in the home market. This is supported: the parameter estimate of the interaction between similarity and U.S. box office in Table 4, Column 3, is positive and statistically significant (  β4   =.0018, p <.05). Thus, with greater similarity between the original and the translated movie titles, audiences in a foreign country are more capable of linking a movie to its origin and use the home-market performance to help judge the movie's quality.The estimation results in Column 3 also support H5. The interaction between the informativeness of translated brand name and the high versus low level of cultural gap grouping has a positive and statistically significant impact on sales (  β5   =.0359, p <.05). Thus, higher informativeness is particularly effective in improving international market sales for cultural products with greater cultural gap.In terms of temporal dynamics of the effects, the parameter estimates of the interaction terms between the number of weeks since release and the two movie title translation variables are both negative and statistically significant in Column 3 of Table 4 (  β6  = −.0236, p <.05;  β7   = −.0323, p <.10). Thus, the brand name translation effects are the highest when the product was first introduced to international markets, but they weaken as it continues to sell. This provides strong support for H6. Other EffectsAlthough the control variables are not our focus, their estimated effects are mostly in the expected direction across different specifications. Benefiting from more intensive distribution, gross box office revenue is higher when a movie is shown on a larger number of screens. The Chinese box office is also higher when the U.S. revenue is higher, when word of mouth is more positive, when the movie is a sequel, when the release gap is smaller between the U.S. and Chinese release dates, and when it the movie was distributed by major studios. We did not find movie budget or the presence of stars in U.S. movies to significantly affect Chinese box office revenue. No prior research has examined the effects of U.S. movie budgets and stars in the Chinese market. Finally, there is no clear theory on whether and how the clarity of the original English title would influence box office sales in a different country where a translated title is used, and our results also indicate that the clarity of the original English movie title has no effects on Chinese box office revenue. Robustness Checks Removing Movies with Extreme Box OfficeChinese box office revenues are distributed over a large range with some extreme values. One may wonder to what extent our results are driven by movies with very high or low box office sales. To examine the robustness of our results to this issue, we drop movies with total box office above the 95th percentile or below the 5th percentile and reestimate the model ([72]). The results from this analysis appear in Table 5, Column 2. Again, our main results regarding movie title translations remain similar.GraphTable 5. Robustness Checks.  5 * Significant at the 10% level.6 ** Significant at the 5% level.7 *** Significant at the 1% level.8 Notes: All columns contain the constant and the same movie characteristics as in Table 4 as explanatory variables. Robust standard errors are in parentheses. DV = dependent variable; IV = instrumental variable. Dropping ""Niche-Market"" MoviesMovies with certain genres (such as horror, musical, and Western) are ""niche-market"" movies that target specific audience groups. They tend to have more limited screening intensity in China. Even though they belong to the high cultural gap category along with drama and comedy, they could be associated with different audience preferences. It is useful to examine how movie title translation effects, especially those pertaining to the moderation by cultural gap, may change if we exclude these three genres. Thus, we reestimated the model using a sample that excludes horror, musical, and Western movies. As Column 3 of Table 5 shows, the results remain highly consistent with those reported previously. Controlling for Advertising EffectsAdvertising is usually an important influencer of box office sales ([ 9]). However, unlike the U.S. domestic market, where movies are promoted heavily before they are released, movie advertising is not yet a major activity in China—in terms of both budget and managerial attention, advertising falls far behind production and exhibition in the Chinese film industry. For instance, while movie budgets in the United States spend a substantial amount of money to advertise on network television, TV advertising for movies is very limited in China. A report by China Daily indicates that movies in China seldom use television advertising due to its high cost and their overall low promotional budgets.[16] In fact, online ticket platforms and social media, rather than ads, are the most prevalent sources of movie information for Chinese consumers.[17] Thus, the impact of advertising on Chinese movie sales should not be as important as that for movies in the U.S. market.From the analysis perspective, recall that we have explicitly controlled for production budget in our estimations. Because movie advertising spending is often in proportion to production budget (e.g., [15]; [53]), the impact of not including advertising spending per se should be minimal. Moreover, because systematic reporting of movie industry information is still nascent in China, it is very difficult to obtain reliable data for movie advertising spending.To test whether our main results remain robust when advertising is controlled for, an option is to utilize some proxies of advertising effects. In lieu of advertising data, we were able to obtain from EntGroup the number of news articles that appeared on major news media for 184 of the 317 movies in the data set. Because advertising generates publicity for movies and news media coverage is one of the best ways to measure publicity ([11]), we can include the amount of new media coverage in the estimation as a direct control for advertising effects.Because the advertising budget is usually set in proportion to the production budget, we follow prior research to regress the amount of news coverage on production budget and then employ the residuals in the estimation to isolate the impact due to advertising ([51]). Because the limited news data significantly reduce the sample size (from 317 to 184), this estimation provides a strong test of whether our main results hold.As Column 4, Table 5, shows, advertising as proxied by news media publicity has a positive impact on box office sales. Even with a sample size that is less than 60% of the full sample, our main results remain robust: the effects of brand name translation strategies and their moderation effects are similar to the findings in Table 4. Summary and DiscussionThis article examines how the brand naming strategy for cultural products affects consumer demand in international markets. In the empirical context of Hollywood movie sales in China, we find strong support that Hollywood movies' box office revenue in China increases when the similarity or informativeness of their translated titles is higher. There is also a positive interaction (synergy) effect between similarity and informativeness. We show that two key product characteristics moderate the brand name effects: the similarity effect is stronger for Hollywood movies with better performances in the home (i.e., the U.S.) market, while the informativeness effect is stronger for movies that have a larger cultural gap from the foreign market. We further find that these effects decrease over time as the product continues to sell. Managerial ImplicationsOur study provides several valuable managerial implications for companies, managers, and policy makers in cultural product industries, as well as those in international marketing. First, our results point out that brand name translation is not a trivial task. How the brand name is translated can have important consequences on product sales. Our study sheds light on two strategies companies can take: translated brand names should ( 1) resemble the original brand names or ( 2) be informative of product content. While each strategy can be managed to influence the sales in international markets, there is also a synergy between them—one strategy becomes more effective if the other strategy is also implemented.Second, our results indicate that these branding strategies are most effective for product sales in the early period after introduction. This is particularly important for managers given the short life cycle of cultural products. It is critical for companies to be sensitive to brand name translation early and to ensure that similarity, informativeness, or both are in place before introduction to help increase sales quickly.Third, the outcome will be the best if a company can achieve both higher similarity and higher informativeness in its translated brand name, especially considering that similarity and informativeness in one translated brand name can have positive synergy effects to further increase product sales. However, there could be situations in which achieving both goals in one brand name is difficult because they may require different brand name features and translation techniques. Therefore, companies need to make trade-offs between similarity and informativeness. Our study provides the following guidelines for the trade-off. If the product has high home-market performance but small cultural gap, the translation should focus on brand name similarity. If the product has low home-market performance but large cultural gap, the translation should focus on informativeness. If the product has both high home-market performance and large cultural gap, both similarity and informativeness will be highly effective in generating sales. Thus, the company should pay attention to both strategies, with the relative emphasis between the two decided on the basis of the effect size and the feasibility of each strategy. Finally, if the product has low home-market performance and small cultural gap, both strategies will still be helpful but will not be highly effective.Fourth, related to the trade-off between similarity and informativeness, companies can follow our analysis to estimate the effect size of the moderating factors. They can then more precisely evaluate the extent to which product sales will benefit from either strategy so that a sensible trade-off can be made.To gain further managerial insights from our results, we use the parameter estimates to quantify the magnitude of the impact of Hollywood movie title translation on Chinese box office revenues. For illustration purposes, we compute the effect size for the scenarios where movie title similarity or informativeness would increase by one point on the seven-point scale. We do so for all the Hollywood movies in the sample, and for different movie categories and across different time periods. The values of other variables not simulated are set at the sample mean. Table 6 presents the results.GraphTable 6. Magnitude of Movie Title Translation Effects on Chinese Box Office Revenues.  9 Notes: For each scenario, the simulated change is an increase of one point on the seven-point scale used to measure similarity and informativeness. Calculations are based on estimation results in Column 3 of Table 4. The values of other variables not simulated are set at the sample mean.Table 6, Panel A, shows the effects of movie title translation across all movies. If the degree of similarity of translated movie titles increases by one point, the average box office revenue would increase by 3.60%, or 10.18 million RMB. If the degree of informativeness in movie title translation increases by one point, the average box office revenue of these movies would increase by 2.83%, or 8.01 million RMB.Table 6, Panel B, illustrates the heterogeneous effects of similarity and informativeness for different types of movies (i.e., by home-market performance and degree of cultural gap). In terms of home-market performance, we median-split the sample and compare the movies with a U.S. box office revenue above the sample median with those below. Recall that similarity in movie title translations facilitates the connection between home and foreign markets. For a one-point increase in the degree of similarity, Hollywood movies with higher-than-median U.S. box office revenue would see a 4.35% increase in Chinese box office revenue. That is equivalent to 19.45 million RMB for these movies. However, movies with lower-than-median U.S. box office revenue would see an increase of only 3.30% (or 3.88 million RMB). Clearly, the benefit of similarity is more significant for movies with better home-market success.In terms of cultural gap, the relevant brand name translation strategy is the degree of informativeness. For a one-point increase in informativeness, movies with large cultural gap would see a more substantial increase of 8.78% (or 10.25 million RMB) in Chinese box office revenue than the movies with small cultural gap (2.38%, or 7.82 million RMB). Increasing informativeness is thus a more effective strategy for large cultural gap movies than for those with small cultural gap.Finally, Table 6, Panel C, focuses on the temporal effects, using the first and second weeks of release as illustration. For a one-point increase in similarity, the Chinese box office revenue would increase by 4.56% (5.89 million RMB) for a movie in its first week of release and 1.64% (1.67 million RMB) for the second week. For a one-point increase in informativeness, the Chinese box office revenue would increase by 5.57% (7.20 million RMB) for the first week and 1.48% (1.50 million RMB) for the second week. In summary, our results provide companies and managers with not only insights about the value of and options for brand name translations but also actionable strategies about how to optimize the task. Directions for Future ResearchA caveat for our study is that we do not model the selection decision of Hollywood movies into the Chinese market. This is a complex issue, as both the Chinese government and Hollywood studios are presumably involved in the negotiation, and there is no published guideline regarding this process or the selection criteria. However, it is widely acknowledged that Chinese government agencies (State Administration of Press, Publication, Radio, Film & Television before March 2018, and State Bureau of Films after that) are key players. Our study focuses on the movie sample conditional on this selection.Even though our empirical context is Hollywood movies in China, the basic theoretical framework and the measures of similarity and informativeness are not specific to this context. They can be implemented for brand name translations between any two language systems. Similarly, the managerial implications discussed previously are also generalizable. Nevertheless, we believe that studying brand name translation strategies in additional contexts, especially if multiple countries are involved ([ 6]), has the potential to motivate richer conceptual developments and generate further insights.Our empirical analysis examines box office sales of Hollywood movies in one foreign country. As studios continue to pay attention to international markets, it becomes increasingly important for them to implement strategies that would maximize global box office revenue. Maximizing revenues in multiple countries requires foresight, planning, and scientific methods. It is particularly challenging given that the studios do not fully control either the exporting or foreign exhibition decisions. It is beyond the scope of our article to address the issue of global revenue maximization; however, it would be interesting for future research to study how casting, release timing, promotion, and movie title strategies may affect global box office revenue.Finally, it would be fruitful to examine how movie title translations interact with other movie marketing activities, such as the use of movie trailers. For example, are more informative trailers and more suggestive movie title translations complements or substitutes for each other? Similar to [74] suggestion about integrating verbal and visual elements of marketing campaigns, addressing this question will provide insights on how verbal (movie titles) and visual (movie trailers) aspects of cultural product marketing interact with each other to influence product sales in both home and international markets. "
10,"Branding in a Hyperconnected World: Refocusing Theories and Rethinking Boundaries Technological advances have resulted in a hyperconnected world, requiring a reassessment of branding research from the perspectives of firms, consumers, and society. Brands are shifting away from single ownership to shared ownership, as heightened access to information and people is allowing more stakeholders to cocreate brand meanings and experiences alongside traditional brand owners and managers. Moreover, hyperconnectivity has allowed existing brands to expand their geographic reach and societal roles, while new types of branded entities (ideas, people, places, and organizational brands) are further stretching the branding space. To help establish a new branding paradigm that accounts for these changes, the authors address the following questions: ( 1) What are the roles and functions of brands?, ( 2) How is brand value (co)created?, and ( 3) How should brands be managed? Throughout the article, the authors also identify future research issues that require scholarly attention, with the aim of aligning branding theory and practice with the realities of a hyperconnected world.KEYWORDS_SPLITOver the course of a century, branding has moved from an occasionally studied activity to a major concern for both business and society. Traditional commercial brands are omnipresent and compete for consumer attention with newer branded entities, such as platform brands (e.g., Airbnb), direct-to-consumer brands (e.g., Warby Parker), smart brands (e.g., Google Nest), idea brands (e.g., #MeToo), and person brands (e.g., Kim Kardashian). The manner in which consumers interact with brands is also changing due to the rise of digitally native brands, the ubiquitous access to information and products via digital and mobile channels, and the broad availability of smart, connected devices.To inspire next-generation conceptual and empirical work on branding, this article examines how existing perspectives need to be refocused and rethought to address the realities of contemporary society. We examine broader types of entities, ranging from smart branded devices and entities that operate in networks of brands to ideas and person brands. We also investigate the blurring of brand boundaries brought about by technology-induced hyperconnectivity. In doing so, we expand on new frontiers in branding research and argue that these topics need to be a larger part of research agendas.Our review of the marketing literature suggests that extant theoretical perspectives—from the vantage points of the firm, the consumer, and society—have resulted in certain models and assumptions that may no longer be adequate or sufficient in a hyperconnected world. The concept of hyperconnectivity refers to the proliferation of networks of people, devices, and other entities, as well as the continuous access to other people, machines, and organizations, regardless of time or location ([42]; [103]). In this environment, information is always accessible and abundant, search costs are low, goods and services from across geographic boundaries are easier to reach than ever, and firms may no longer be the primary source of information about brands.Hyperconnectivity has led to two major changes in branding. First, brands are shifting away from single to shared ownership, as heightened access to information and people is allowing more stakeholders to cocreate brand experiences and brand meanings alongside traditional brand owners (or entities who market the brand). We call this phenomenon the ""blurring of branding boundaries."" Second, hyperconnectivity has allowed existing brands to expand their geographic reach and societal roles, while new types of branded entities are further stretching the branding space, which constitutes a ""broadening of branding boundaries.""In this article, we elaborate on the consequences of hyperconnectivity on the ""blurring"" and ""broadening"" of branding boundaries. We focus on both challenges and opportunities that brands face in a hyperconnected environment. We first briefly review three core perspectives that have underscored traditional branding research and then examine the implications of this expanded view of brands through three fundamental questions: ( 1) What are the roles and functions of brands? ( 2) How do brands (co)create value? and ( 3) How should brands be managed? We provide initial answers to these questions and outline areas of inquiry for future research. Key Theoretical Perspectives in the Branding LiteratureMany conceptualizations of brands have been proposed across various domains of inquiry, each with its own particular focus. We distinguish three theoretical perspectives (firm, consumer, and society) and two approaches within each perspective. The firm perspective views brands as assets and examines the various functions and roles that brands serve for firms, both strategically and financially. The consumer perspective views brands as signals (economic approach) and mental knowledge cues (psychological approach). The society perspective presents brands in societal and cultural contexts affecting individual consumers both directly and indirectly through social forces, structures, and institutions. The sociology of brands applies to all manner of commercial and noncommercial entities (e.g., ideas, people). We briefly introduce each perspective in this section while acknowledging the overlap and spillovers of knowledge across various approaches used in the literature. One source of this overlap comes from the nested structure that underlies how these perspectives relate to each other. For example, societal macroenvironments host institutions (including the firm) that, in turn, interact to shape consumer-level outcomes. Firm Perspective Strategic approachKey issues examined in this approach include the development and implementation of brand identity; positioning, targeting, launch, and growth of brands; brand portfolio architecture; and management of brands across geographic boundaries ([59]; [118]). Topics studied range from how to effectively construct and manage brand portfolios ([86]) to how to extend brands into new categories or as new brands, endorsers, subbrands, descriptors, product brands, umbrella brands, and branded differentiators ([138]). Marketing alliances of various types, including cobranding alliances and collaboration with customers, along with brand acquisitions and divestitures are strategic challenges and opportunities that have also been addressed in this literature. Financial approachResearchers leveraging a financial approach to branding have focused mainly on measuring the effect of brand equity and branding actions on the stock market value of firms. Specifically, one research stream has focused on demonstrating the relevance of consumer-based brand equity ([78]; [85]; [107]), while another stream has focused on measuring the stock market impact of corporate actions, such as brand extensions ([69]), brand and marketing alliances ([21]; [122]), brand acquisitions ([ 7]; [143]), and brand architecture decisions ([54]). [62] provide an overview of research that has examined the impact of brand actions on a variety of financial (and nonfinancial) firm outcomes. Consumer Perspective Economic approachFirms tend to know more than consumers about the quality of their brand. This information asymmetry has given rise to a field of study that treats brands as market signals ([35]). The brand extension literature has leveraged the information asymmetry–reducing role of brands to determine ( 1) how a multiproduct firm can brand a new product, ( 2) the relationship between the reputation of the new product and that established by the firm in other markets, and ( 3) the perceived quality of a new product ([34]; [142]). Psychological approach[110] ""consumer-psychology-of-brands"" model summarizes the key concepts of the psychological approach, which proposes that brand equity resides in the minds of customers. Brand knowledge, which is the mental representation of brand awareness (recall and recognition) and brand image (types, favorability, strength, and uniqueness of brand associations), constitutes the key construct for conceptualizing and measuring brand equity from the customer's point of view ([61]). Other mental representations include affect and emotions, leading to constructs such as brand trust ([104]), emotional brand attachment ([132]), brand coolness ([141]), and even brand love ([12]). This perspective also includes brand experiences and defines them as including sensory, affective, and intellectual impressions as well as behavioral actions toward brands ([17]; [112]). Society Perspective Sociological approachResearchers approaching brands from a sociological perspective focus mainly on brands as portable containers of meaning that are shaped by institutions and collectives from the time the brand is conceived, produced, and marketed, through the postpurchase stage ([95]). Sociological models are typically dynamic and recursive. Sociological scholars do not view brands as static entities or as mere information sources or knowledge structures; rather, they have a keen interest in how brand meanings are generated, changed, and dynamically reinvented. An important notion in this literature is brand community, a nongeographic space in which admirers of a brand connect with one another and demonstrate all three necessary distinctions of community: consciousness of kind, rituals and traditions, and moral obligation ([89]). Cultural approachBoth sociologists and anthropologists study culture, and their work in branding and marketing meaningfully intersects. One of the main insights from this research is that branded goods, as cultural meaning producers, enhance consumers' lives ([84]). Probably the best-known perspective on how brands become popular through their production of cultural capital is the widespread adaptation of [16] in consumer culture theory ([129]). Among other things, consumer culture theory addresses the dynamic relationships among consumer actions, the marketplace, and cultural meanings. Research in this stream focuses on how iconic brands ([52]), or brands infused with cultural referents ([22]), contribute to culturally bound consumption practices ([33]). Invoking the same dynamic, these scholars demonstrate how antibrand activists and other cultural intermediaries can introduce a competing set of brand meanings (e.g., doppelgaünger brand images) that can significantly influence consumer behavior and market creation ([44]; [128]).We summarize key insights from each perspective (firm, consumer, and society) in Table WA1 in the Web Appendix. As this table is structured by substreams of research, it also illustrates how several of the theoretical perspectives have helped advance each area of inquiry. We next leverage these perspectives into a discussion of how the boundaries of branding have blurred and broadened in response to changes occurring in a hyperconnected environment. We revisit them subsequently as we advocate a multidisciplinary approach to address current opportunities and challenges in the branding domain. Boundaries of Branding in the Era of HyperconnectivityWe noted that the hyperconnected world is characterized by networks of people, devices, and other entities that are continuously interacting and exchanging information. Several aspects of hyperconnectivity are relevant to branding research and management ([135]). We highlight three aspects: ( 1) information availability and speed of information dissemination; ( 2) networks of people and devices, and the growth of platforms; and ( 3) device-to-device connectivity. We next examine each of these in greater detail. Information Availability and Speed of Information DisseminationThe scale of information availability and the speed of information dissemination have grown exponentially as technology that connects people and devices has become widely available and more affordable. The broad and fast access to information calls into question foundational assumptions of several of the theoretical perspectives we discussed in the previous section. For example, high search costs and information asymmetry have been the core assumptions of the economic view of brands as signaling mechanisms ([35]; [142]). However, brands may no longer serve as primary signals of quality in an environment in which search costs are low and information asymmetry is reduced by various stakeholders who abundantly share opinions about brands across their networks.Because access to information is much easier in a hyperconnected world, consumers need to expend less effort in learning information about brands. Models of memory activation and learning need to be updated to account for consumers' increased reliance on external sources of information as opposed to information that is retrieved from memory. Moreover, the sheer volume of external information can lead to information overload—a situation in which not all communication input can be processed and used ([18]; [113]). The velocity and volume of information that consumers are exposed to, along with the potential information overload that results from it, reduce brands' ability to capture the attention of their target segment ([74]), which calls for a reexamination of models of attention. Networks of People and Devices, and Growth of PlatformsThe rise of networks of people and devices and the development of platform technology have led to an environment in which brands and their meanings are cocreated. Firms are not the only entities that can disseminate branded information quickly and broadly—they now compete with other stakeholders who can do so just as easily (e.g., [71]). Brand conversations are happening online, and consumers may listen to their peers or to online influencers just as much as, if not more than, they listen to branded messages generated by firms. A growing stream of research is documenting the effect of this loss of control on brand meaning ([24]; [40]) and brand experiences ([100]). Device-to-Device ConnectivityDevice-to-device connectivity has affected brands in several ways. First, branded experiences are significantly more complex in an environment in which consumers can access the brand via multiple channels that seamlessly connect with one another. Second, the heightened connectivity among devices has led to brands themselves being integral components of networks of smart products that are populating the Internet of Things ([50]). This again calls into question which entities contribute to brand meaning and associations, how these associations can be managed, and to what extent consumers anthropomorphize branded products that communicate with other products. Blurring and Broadening of Branding in a Hyperconnected WorldWe next discuss how these manifestations of hyperconnectivity have resulted in a ""blurring"" and ""broadening"" of branding boundaries. We reexamine these issues as we highlight the changing roles and functions of brands, how they create value, and how they should be managed. Blurring of BrandingTraditionally, brand management methods were designed for a world in which consumers were exposed to (and influenced by) firm-controlled television, print, and radio advertising ([ 1]). Today, as information becomes available widely across multiple channels, consumers' attention is scattered across many media and channels, forcing a multitude of branded entities to compete to gain consumers' awareness and potentially to form an emotional connection. As brand meaning is increasingly cocreated, it is even sometimes hijacked by consumers and firm partners ([40]; [144]).Beyond the cocreation of brand meaning, brand stakeholders (ranging from customers and employees to firm partners, communities, and society at large) are increasingly shaping various aspects of product and marketing-mix activities. For example, firms that use a platform-based business model often bring together partners that help cocreate the entire brand experience. Alternatively, firms that allow their products to interact with voice-controlled smart devices such as Amazon's Echo or with home automation hubs such as Wink Hub are also relinquishing some control, as brand associations can transfer to and from these partner brands to the focal one with potentially serious consequences for brands' performance and business survival ([145]). The blurring of brand boundaries is a key consequence of the rise of the sharing economy, which ""offers temporary access as an alternative to permanent ownership"" ([32], p.2), and which has expanded the role of customers to span both the demand and supply sides. Broadening of BrandingAs marketers are losing some control over the meaning consumers associate with brands, more brand-related stakeholders are involved in shaping brand associations. Entities other than corporations (e.g., ideas, people) are becoming more systematic in their branding efforts ([39]; [131]), as hyperconnectivity has allowed them to easily reach multiple stakeholders around the world. The reach of both traditional brands and newer branded entities has also broadened to include stakeholders who have not necessarily been consistently targeted in the past, such as employees, donors, partners, citizen-voters, and activists, as well as local communities, governments, and society as a whole.The role of brands has also broadened, with commercial brands increasingly expected to have a mission (or purpose) beyond shareholder value maximization. As societal norms change, companies are feeling the pressure to act in a sustainable manner or to take activist stances that may help society attain its goals or even support stakeholders who oppose such goals. Brand activism has led to new responsibilities for brand managers and chief executive officers, as well as the need to protect the large amount of customer data that companies collect in their efforts to manage brands. Mapping the Blurring and Broadening of BrandingFigure 1 maps the blurring and broadening of branding. It illustrates—using four types of brands and the stakeholders that shape them—the potential dilution of brand ownership (depicted by dotted circles in the picture) and the broadening of branding entities, brand roles, and brand stakeholders. Figure 1 also shows that brand meaning is more dynamic when brand ownership is more porous and that brand meaning is cocreated by the brand with its stakeholders.Graph: Figure 1. Ownership of branded entities and changes in the branding landscape.Notes: The figure depicts interactions between various types of brands and their owners and primary stakeholders. Inner circles represent a specific type of brand; outer circles encompass the brand owners or main stakeholders. Dotted circles indicate more porous boundaries of brand ownership. We have labeled the brand owner and used arrows to illustrate whether the relationship between the brand and its owner/stakeholders is unidirectional or dynamic.Next, we examine the shifts in branding across four broad headings: ( 1) rethinking the roles and functions of brands, ( 2) rethinking brand value creation and cocreation, ( 3) rethinking brand management, and ( 4) rethinking the boundaries of branding. As we highlight fundamental changes that are taking place, we also outline a future research agenda for each topic. As an example, Table 1 summarizes research topics that pertain to our first question relating to roles and functions of brands.GraphTable 1. Rethinking the Roles and Functions of Brands: Future Research Opportunities.   Rethinking the Roles and the Functions of BrandsResearchers investigating brands have construed brands in many different ways, including as signals of quality ([35]; [142]), knowledge structures held in an individual's memory ([57]; [61]), instruments of identity expression and goal achievement ([76]), social actors and structures of both stasis and change ([89]), and cultural icons ([51]). As technology has vastly improved access to information, products, and people, these existing conceptions of brands need to be reassessed—that is, refocused, rethought, or abandoned altogether. We discuss future directions for this reassessment in the remainder of this subsection (see Table 1). Brands as Weak Quality SignalsIn the past, when consumers were faced with information asymmetry and imperfect information, brands served as quality signals that facilitated consumer choice. However, in a hyperconnected economy in which consumers can easily access information about brands using online channels, information asymmetry between brand owners and consumers has decreased, as search costs are lower ([75]). Thus, a brand's quality signal could face interference from alternative signals of quality derived from the collective reviews and opinions available online.Researchers need to understand the conditions under which consumers still trust firm communications about brands (vs. information intermediaries such as Yelp, TripAdvisor, and Rotten Tomatoes) and the impact of firm-generated versus marketplace-generated brand information on brand trust. How can firms manage a brand communication process that faces significant signal interference? How can brands shift the balance in their favor if the signals generated by the marketplace are mixed, reflecting a high level of consumer heterogeneity? What types of message content, form, and media placement have the best chance of reducing and counterbalancing potentially undesirable external brand signals? Brands as Mental Cues in Information-Rich EnvironmentsBrand information processing (how consumers acquire, use, and remember brand knowledge) depends on consumers' motivation, ability, and opportunity to process information ([77]). Studies have proposed and tested multiple models of learning about brands, leveraging the notion that brands can act as cues that access knowledge held in consumers' memories ([57]). As consumers increasingly process information in a hyperconnected and attention-scarce environment, understanding of brand information processing may need to be augmented and extended ([146]). Existing dual-processing models (visual or verbal, heuristic or systematic, piece-by-piece or holistic; [114]) assume that consumers can switch back and forth in real time from one channel to another and from one processing style to another. As consumers utilize multiple devices and channels in the context of hyperconnectivity, dual-processing models need to be further refined to examine how and when people switch from one processing style to another and how simultaneous processing of information from multiple devices or channels occurs over time (e.g., when consumers engage in multitasking).Brand information processing models should also consider how sensory information contributes to brand awareness and choice. The impact of sensory information on processing of brand information has been previously investigated (e.g., [66]; [67]). For example, [109] show that familiar brands are preferred in online settings (relative to offline) when sensory information is diagnostic and plays a role in brand choice. However, hyperconnectivity offers additional research opportunities in this area. Although many brands are experienced online in an arguably less sensorial environment, companies now have access to multisensory and highly interactive new technologies that appeal to multiple senses (sight, hearing, touch, taste, and smell) simultaneously. For instance, sensory-rich retail environments, as well as augmented and virtual reality, allow people to experience the power of multisensory stimulation and consider brands in environments that may be dynamic and virtual. Online stores such as Wayfair, Amazon, and Target have launched mobile augmented-reality apps that allow consumers to shop for furniture by placing items in their own room settings ([97]). In sensory-rich environments, the voluntary attention to brand stimuli and sensory cues may be more difficult to elicit and more likely to be replaced by involuntary attention or arousal to stimuli unrelated to the focal brand. Therefore, current models need to be revised to incorporate the paucity of consumers' voluntary attention and also account for the possibility that the role of sensory cues may change how brand information is processed. Brands as Instruments of Identity ExpressionBrands can be used to support a desired consumer identity and may be associated with humanlike traits (referred to as ""brand personality""; [76]). The identity view of brands is based on the notion that brands become imbued with associations through their use. For example, consumers can use these brand associations in an instrumental way to construct and signal something about their own identity ([ 2]), whereas employees can use corporate brands owned by their employer to convey professional status or to send a signal about their expertise ([124]). The identity view has more recently focused on the complex nature of identity, with brands being tied to multiple individual and group-based identities ([36]; [123]), as well as being symbolic of culture ([133]). The consumer culture literature has also considered the instrumental role of brands as authenticating narratives for consumers' ""identity projects"" (e.g., [128]).A consequence of hyperconnectivity is that consumers can now adopt multiple personae on their devices and change their identities frequently ([134]). As consumers increasingly spend time online, there is the potential for their online (or ""virtual"") self and their offline (or ""true"") self to diverge, possibly leading to identity conflict ([121]). The multiple (sometimes conflicting) dynamic identities of consumers, particularly across online and offline settings, require rethinking about how to leverage brands to build identities ([106]). Can brand associations be constructed and revised to allow for a more flexible view of identity, or would this create confusion from the perspective of the user or those who view such signals? Would consumers prefer that brands have a well-defined meaning that helps them anchor and retrieve one particular facet of their identity, with greater potential for sending stronger signals about a particular self, or would they be drawn more to malleable brands that accommodate a broader range of self-expression? A multidisciplinary approach that leverages both the consumer and society perspectives could examine the manner in which social groups can use brands in instrumental ways to achieve their social goals.Finally, hyperconnectivity can facilitate access to a broader set of brands in the digital space that can be used by consumers to express their identity. This would afford consumers a richer set of identity-building tools. At the same time, digital possessions have been shown to have lower self-relevance ([11]), which calls into question the strength of identification with brands in the digital space. Additional questions center on how consumers choose brands to identify with in this online, hyperconnected environment and the nature of relationships that consumers form with brands. Brands as Containers of Socially Constructed MeaningResearchers using a sociological lens have examined the role of brands as arbiters of social trends, as catalysts for social interaction, and as societal symbols in the case of brands that attain iconic status (e.g., [51]). Brands are expanding their social role, however, by increasingly becoming activist tools aligned with various social and political issues. Purpose-driven branding argues that brands should uphold societal values because doing so gives consumers an opportunity to use them in instrumental ways to show support for social causes. For example, Procter & Gamble's recent Gillette ""The Best Men Can Be"" and ""My Black Is Beautiful"" campaigns took controversial positions with regard to gender and racial stereotypes, respectively ([58]).It is tempting for brands that want to remain relevant, particularly in the eyes of millennial consumers, to take a stance on important social issues, but prescriptions on how to do so effectively are lacking. Yet it is increasingly clear that ""the future of brands is also inextricably tied to the future of society"" ([25], p. 246), and brands can act as vehicles for bringing about social change. As hyperconnectivity can amplify brands' social message, scholarly insights are necessary to understand the role of brands as purpose-driven entities and how firms can best align the social message of brands with the desired brand associations. The consumer and society perspectives could be utilized in exploring the implications of brand conversations and brand engagement for stronger social connections, as well as other determinants and consequences of this ""social brand engagement"" ([65]). Brands as Architects of Value in NetworksMarketing scholars have extensively investigated brand partnerships and alliances from a dyadic perspective ([98]; [116]). However, in a hyperconnected environment, brands are increasingly embedded in complex networks consisting of users, partners, cocreators, and co-owners. Prior research that has examined the performance impact of networks on value creation at the firm level ([122]) can be extended to show how brands can extract value from their position in the network.Leveraging network theoretic constructs to examine value creation in branded networks is a useful avenue for future research. Brands can provide value in networks in at least two ways. Due to advanced search and navigation capabilities, brands can simplify users' navigation through brand-embedded networks, thus contributing to seamless user experiences on online platforms. Furthermore, brands can create value by ensuring compatibility across branded entities in the network, in terms of both attributes and quality standards.A case in point is Apple, a company that strives to make its products compatible with a broad range of complementary devices and ensures that all applications sold in the App Store meet its stringent quality standards. But not all brands can easily extend their networks; for instance, Google failed to successfully launch Google Health, a platform where consumers could consolidate their health information and interact with providers. As noted by [136], participants on both sides of the platform were not ready to engage at a level that would render the platform successful.Additional research is necessary to identify strategies that brands can adopt to enhance the effective functioning of the brand ecosystem. The traditional conceptualization of user experience that relies primarily on product or service usage could be broadened to include interactions across an entire network or ecosystem that is linked to a specific brand. We term this ""brand network user experience."" A better understanding of the brand network user experience and how consumers derive utility from such a network is warranted. Finally, brands can not only help organize informational content on networks but also serve as gatekeepers of information (or products) they want consumers to see or purchase. The ongoing spread of ""fake news"" on social media platforms highlights the potentially critical (and controversial) role of branded platforms as entities that censor harmful content or products; how brands can achieve this role and perhaps derive value from it should be the focus of further research. Brands as Catalysts of CommunitiesResearchers have examined brand communities and how they deliver value to their members ([89]) and affect the cocreation process. In this context, brands serve as catalysts of social interaction and community through shared consciousness and brand use, loyalty, and engagement among community members ([60]). Hyperconnectivity has increased the potential for individuals to establish and join brand communities, be it newer types that are appearing on social networks or communities established by traditional brands online, such as the Sephora or Jeep brand communities, in which users post reviews and share information about brands and their new products.New research on how brand communities emerge in a hyperconnected environment and what type of governance can make such potentially large communities successful is a worthwhile avenue for research. Branded communities can create value not only by enhancing the brand experience for users but also by providing firms with a forum to test out new ideas, collect feedback on brand actions, and better understand how brands are consumed. Brand communities may also offer social benefits, particularly in the context of growing social isolation and ""aloneness"" in society (e.g., [134]). Thus, it would be valuable to understand how brand communities can help combat loneliness despite the larger scale of social connections facilitated by hyperconnectivity. Brands as Arbiters of ControversyIn addition to the role of brands as signals or nodes in memory, brands have been construed as symbols ([70]). These symbols could be used in various forms of self-presentation to give meaning to consumers and their social position (e.g., [46]). While not denying their other functions, [70], p. 205) states that ""things people buy are seen to have personal and social meanings"" and that people use branded symbols to reinforce their view of self (both actual and ideal).When brands are strong enough to serve as symbols, some also transition into roles of arbiters of controversy within the identity and sociopolitical realms. These brands often adopt controversial stances on key topics such as feminism, LGBTQ+ rights, and racial issues and, in doing so, appeal to consumers at the epicenter of cultural controversy. Nike's use of Colin Kaepernick's cause was widely debated but is now deemed as having galvanized thought on the issues of social justice and race in sports, particularly considering that such a large percentage of professional athletes are nonwhite. Nike was awarded the 2018 Marketer of the Year for the campaign by Advertising Age ([99]), and Nike's profits and stock valuations have climbed in the aftermath of this controversial stance. In this way, brands appear to have a stronger voice in a hyperconnected world, in which their messages on social and political issues can quickly spread and multiply. With a stronger voice comes the added responsibility of addressing important social issues in ways that can help society move forward.Hyperconnectivity has not only afforded brands the opportunity to have a stronger voice but also reduced their ability to stay ""above the fray"" on controversial topics. Absent a stance on such topics, consumers with deep brand connections may question the authenticity of the brand. Research has shown that perceived brand authenticity has four dimensions: continuity, credibility, integrity, and symbolism ([88]). Integrity, which captures a brand's intentions and the values it communicates, along with symbolism, which reflects values that are meaningful to consumers, are particularly relevant in situations where brands face conflict. Failing to take a clear stance may lower perceptions of brand integrity and foster identity conflict among those consumers who use the brand in symbolic ways to communicate certain values. At the extreme, activist consumers can punish passive brands: for example, activist consumers and movements fueled by them, such as #GrabYourWallet, have caused retailers to drop certain brands or companies to drop their top executives, as was the case with the #DeleteUber movement, which led the then-chief executive officer of Uber—Travis Kalanick—to step down from the company ([13]). In this way, brands that try to avoid conflict can become the target of activist consumers who yield a louder megaphone in a hyperconnected environment.Research in the context of brands as arbiters of controversy is only in its infancy. Many questions remain unanswered. For example, how does brands' participation in controversy (political or social) affect their customer base? How do message content and the emotional tone of messages in a controversy affect brand perceptions? How does brands' participation in controversial issues propagate on social media, and what is the role of the structure of their online social network (e.g., network centrality, network density) in the spread of brand information following a crisis incident? What new metrics (e.g., polarization, controversy score) can track the nature of conversation about brands in controversial settings? Brands as Stewards of Data PrivacyAs brand boundaries become blurred, data that brands can access about their customers have increased, shedding new light on what drives consumer attitudes and behavior. These data are a potential source of value for branded entities, which can use them to better target and customize their offerings to consumers. At the same time, increased access to customer data poses legal and ethical challenges, as consumers have grown more concerned about the confidentiality and security of their data ([120]). Brands that have misused customer data (or suffered from data breaches) have faced significant backlash and penalties from consumers and public policy officials.Policy makers and consumers have begun emphasizing the important role of governmental regulations in moderating privacy concerns. Concomitantly, scholars have shown greater interest in understanding the implications of recent privacy regulations (e.g., General Data Protection Regulation in Europe) for both customers and firms ([47]). As data themselves become a resource, understanding the implications of privacy concerns for brands and the trade-offs inherent in achieving greater personalization through data versus ensuring data privacy is a topic that needs greater research attention ([ 5]; [80]).Even as regulations pertaining to data usage become more stringent, brands should take the lead in ensuring compliance with data standards both for themselves and to ensure transparency across all third parties that belong to their ecosystem. While taking such a stance would strengthen their value proposition, more research is needed on understanding how branded entities should enforce data standards throughout their ecosystem and the downstream consequences of such strategic actions on consumer response to brands. Theoretical insights from the firm and consumer perspectives can be leveraged to understand how brand associations change if the role of brands as stewards of data becomes more salient, and to what extent this role can strengthen the relationships between brands and their customers. Rethinking Brand Value Creation and CocreationWe previously alluded to how hyperconnectivity has led to brands and their messages no longer being exclusively shaped by their owners. In this subsection, we elaborate on how firms can cocreate brands' experiences and meaning with their stakeholders, including customers, partners, and the public at large. We summarize pertinent future research questions in Table 2.GraphTable 2. Rethinking Brand Value Creation and Cocreation: Future Research Opportunities.   Cocreating Brand ExperiencesPerhaps the most significant change in how brands deliver value in the digital economy is the cocreation of brand experiences with consumers or partners. So far, cocreation research has focused mostly on new product cocreation ([53]; [137]), but recent work has begun examining the cocreation of brand experiences ([94]).Importantly, cocreation of the brand experience often occurs on digital platforms, defined as enterprises that ""use the Internet to facilitate economically beneficial interactions between two or more independent groups of users"" ([29], p. 8). Examples of digital platform brands are Uber, Airbnb, and LinkedIn, among others. Most digital platforms are branded, and their brand associations result from the interactions of multiple players who contribute to the creation and delivery of the brand experience. For example, property owners on Airbnb contribute to the success of the brand experience as much as the technology that enables this platform. Emerging research on platforms has focused primarily on the structure of such exchanges ([100]) and how to monetize them ([68]) rather than on how the interactions between the firm and its stakeholders shape the brand.Recent research highlights the unique drivers of engagement for platform brands ([32]), but there are other important branding aspects of cocreation on digital platforms worthy of further research. For instance, how should firms design an optimal governance mechanism that will still allow them to maintain reasonable control over brand reputation and meaning? How can firms influence platform participants to reinforce the brand message and deliver a brand experience consistent with this message? What differential branding benefits (e.g., awareness, preference, resonance) accrue from the various types of platform designs (e.g., open vs. closed, centralized vs. decentralized)? Are branded platforms in the shared economy disrupting employment, and if so, what are the societal consequences of these disruptions and subsequent impact on the brand?More generally, researchers should try to understand how much control firms should relinquish over the brand experience, while still maintaining the desired associations, and to what extent firms should formalize the cocreation process. Should all interested consumers be given an opportunity to cocreate, or should the firm work to identify segments of consumers who can yield better outcomes for the firm? By contrast, how should firms interact with the segment of passive, low-involvement consumers, and what are the consequences of cocreation by other consumers in this particular segment? Are certain brand types more malleable to cocreation efforts? What are the consequences (i.e., benefits and risks) of cocreated brand experiences on brand outcomes such as brand engagement?For traditional brands that are incorporating digital platforms and channels into their strategy, there are interesting questions pertaining to the consumption experiences. For example, it is worth examining how brands effectively can effectively merge online and offline experiences. Theorizing the brand cocreation process from the firm perspective (how firms should manage and respond to cocreated brand experiences), the consumer perspective (how consumers cocreate and react to brand experience cocreation), and the society perspective (how cocreated brand experiences contribute to well-being and life satisfaction) could offer important insights. Cocreating Brand MeaningCompanies not only cocreate brand experiences with their customers, but some are even offering consumers the opportunity to design brand advertisements ([118]). While research has begun examining the cocreation of brand meaning through outsourced advertising campaigns ([130]), extensions of this work should address various facets and consequences of outsourcing the design of brand communications to consumers ([139]).Even if firms wish to maintain full control over how they design and promote brand meaning, the rise of social media has led to weakened firm control over the brand meaning in the marketplace ([48]). Many-to-many communications on ubiquitous social media platforms have ushered in an era in which dynamic and real-time conversations are taking place among consumers on a massive scale ([15]; [126]). This has created a large volume of user-generated content that has made it easier for marketers to ""listen"" to consumers on social media platforms. Such listening has allowed marketers to derive unique insights into customer needs and wants, thus allowing them to replace costly traditional marketing research with low-cost, granular data available through social listening via user-generated content ([90]). At the same time, research has highlighted the amplification of positive and negative information in social media and its effects on brands ([49]).Understanding the implications of consumer-generated content on downstream outcomes, such as customer engagement, and the motivations for consumers to engage in generating word-of-mouth communications are two broad areas that have garnered research attention ([10]; [72]; [82]). Further research should build on these findings to determine the optimal balance of firm- versus user-generated content that can enhance consumers' perceptions of brand authenticity and increase their willingness to engage with a brand. All three theoretical perspectives can be leveraged to understand how consumers respond when brand meaning is cocreated by others. Finally, additional research is necessary on which online platforms or media can shape and significantly influence the meanings associated with particular brands, along with the best methods and metrics that can mine the data collected from these sources. Creating New Brands in a Hyperconnected WorldExtant research on the determinants of new brands' performance has focused mainly on the consumer packaged goods industry and on marketing-mix and firm-strategic actions, such as discounting, feature/display, advertising, or distribution, with the breadth of the latter playing the greater role in the success of a new brand ([ 6]). Going beyond packaged goods categories, further research should focus on branding aspects in emergent categories of new goods and services. The breadth of new offerings, ranging from digital platform brands (e.g., TaskRabbit), to smart products (e.g., artificial intelligence toothbrushes such as Ara by Colibree), to new business models powered by new technologies (retail stores without a cashier such as Amazon Go), requires a fresh perspective on how to brand and manage complex products that interact in newer ways with their owners and with complementary products than more traditional products.What are the key success factors of such new branded products? Owing to disintermediation (involving switching or elimination of traditional intermediaries in distribution channels) and reintermediation (addition of new forms of intermediaries in distribution channels), we expect traditional channels of distribution, and related factors examined in prior research, to be less important in the future ([43]). Rather, success will likely depend on the degree to which a brand can leverage hyperconnectivity among networks of people and devices. One manifestation of the easy access to consumers is the proliferation of direct-to-consumer brands such as Bonobos (men's clothing), Harry's (shaving), Glossier (cosmetics), and Warby Parker (eyewear). Yet direct-to-consumer brands do not only leverage hyperconnectivity by selling their products online; rather, they try to provide a distinct brand experience that sets them apart from their brick-and-mortar competitors by tapping into their network of consumers. [79] describes how Glossier aimed to make the customers an integral part of the buying experience. Glossier invites its customers to share product ideas and incentivizes them to share their Glossier brand experiences with their followers across social media channels such as Instagram. In this way, Glossier's network of customers are evangelists for the brand.Three facets of hyperconnectivity are particularly relevant in the context of new brands: networked communications, network value creation, and data collection. In terms of networked communications, information and positive word of mouth about innovative new products can spread quickly via global social media and buying platforms, thus securing early adopters faster than ever before ([92]). Network value creation in a hyperconnected world will require smart products that connect with other products and provide value as a network or system; that is, they need to be ""connectable"" and connected with other brands ([93]). Brands have formed partnerships and alliances for decades to create add-on value; in the future, connecting with other brands will be at the very core of a brand's value creation and will precipitate the growth of a variety of related applications, such as augmented reality, blockchain, wearable technology, chatbots, and gamification ([115]). Finally, new brands are more likely to be perceived as innovative and valuable for the firm, consumers, and society if they collect data and make those data, or the features built on them, available to relevant stakeholders. For example, streaming services (e.g., Netflix, Hulu) leverage data gathered from consumers to recommend new shows to their audiences. Scholars should investigate ways in which data can be leveraged to be a central aspect of the value creation of a new brand. For instance, the brand Proven was created by applying artificial intelligence algorithms to vast quantities of data pertaining to ingredients, customer reviews, and scientific journals with the goal of identifying the most effective ingredients and creating a customized, data-driven line of skin care ([125]). In doing so, data can not only help improve firms' operations but also, if done responsibly, be of benefit to society.If the ability to connect to other brands is key to a new brand's success, how should the brand be built? Product designs of the brands of the future will need to include sensors and receptors to leverage the device-to-device connectivity as discussed previously ([93]). Researchers need to determine conditions under which it is more effective for brands to establish their own ecosystem, as Apple has done, or plug into another one that allows them access to consumers and other brands. Moreover, brands of the future—whether smart home appliances, office equipment, cars, or industrial products—will not be fixed and static; they will be updated constantly, some even in real time, because their core design will also include software, not just hardware. As noted by [111], the digital revolution has moved the focus of brands ""from atoms to bits"" (i.e., from tangibles to intangibles), but the next wave of this revolution may involve incorporating smart technologies such as artificial intelligence, blockchain, and augmented and virtual reality into brands. Some of these technologies require deeper access to consumers' lives that may be perceived as intrusive. The extent to which consumers are willing make a trade-off between privacy concerns and the ability to have their products updated and maintained in real time is an interesting area of research inquiry.Finally, new brands will not be about ownership. The ownership model will likely be complemented by access- and occasion-based liquid consumption ([11]; [32]). The very notion of brand as a clearly defined entity that needs to create awareness and image among stakeholders to induce loyalty among them may even become obsolete and be replaced by a more transient model of value creation among interconnected devices that do not require much labeling and branding. If the importance of brands diminishes, what mechanism will replace brand equity to induce loyalty and trust in consumers? What factors will determine product choice in this new world? The new reality of branding described in this section calls for significant changes in the management of brands, which we examine next. Rethinking Brand ManagementIn this section, we discuss several key aspects of brand management that have been affected by hyperconnectivity. Table 3 presents pertinent future research questions in this domain.GraphTable 3. Rethinking Brand Management: Future Research Opportunities.   Blurred Control of Brand Positioning and Brand CommunicationBefore the advent of the internet, firms chose the positioning of their brands and sought to achieve it through carefully controlled and designed communications. Hyperconnectivity has fundamentally changed the way firms both position their brands and talk about them. Firms now must contend with three major changes. First, their competition has significantly broadened as connectivity has heightened consumer access to a large set of brands. Second, the internet and mobile devices offer a much broader space and more ways to communicate brand messages, making it more difficult to optimize message placement ([47]). Second, as we alluded to previously, firms' brand communications are supplemented and may even be dwarfed by those generated from outsiders (e.g., [126]). Thus, firms' brand messages may be not only blurred but also substantively modified by brand opinions generated by outsiders.In terms of positioning, branding's pervasiveness across products, people, places, and ideas makes it more difficult for firms to clearly delineate their competitive space and to find a unique message that cuts across digital clutter. At the same time, just as consumers have access to numerous brands in the digital space, firms' potential consumer base is also increasing, which raises important questions about how firms should handle consumer heterogeneity and to what extent they should modify their message and offering to cater to such heterogeneity. Prior research has shown that differentiation is a double-edged sword, as it is associated with both higher customer profitability and lower acquisition and retention rates (e.g., [117]). More research is needed on strategies that can enable companies to wield this sword more effectively. This is of particular relevance in a hyperconnected context where consumers may find it impossible to engage deeply with the multitude of brands they encounter.In terms of the design and placement of brand messages, firms face an almost infinite number of advertising formats and channels. However, the effectiveness of these ads is being questioned, as firm communications in general and brand messages in particular are fighting information overload. Many firms are coping with the complex task of ad placement by relinquishing some control to algorithms that help with ad placement. This also has potential downsides, as ads may be placed alongside content that is deemed unsafe or controversial. Research on the consequences of brand advertising online and on mobile apps is emerging (e.g., [31]; [140]), but it is far from being able to provide clear prescriptive implications about the consequences of advertising for brand safety in a given digital space. How do consumers react if a brand inadvertently advertises on a webpage that is associated with content (such as hate speech) consumers may find offensive? What are some effective approaches to ensuring transparency throughout the advertising and media supply chain so that brands can evaluate and enforce safety of their advertised content? More research is needed to examine these questions in greater detail.We highlighted the cocreation of brand meaning as one of the main consequences of hyperconnectivity. Firms are still searching for optimal ways to monitor, gather, analyze, and respond to brand information generated online. Many firms are investing in social media–listening control rooms or in engagement platforms that can help them better manage brands on an ongoing basis ([105]). From a capability standpoint, future research should offer stronger insight into how data scientists (responsible for gleaning insights from social media listening) can work with brand strategists within a firm to design optimal communications that leverages the insights from a continually evolving in online conversation spaces.From a brand management standpoint, firms need a consistent approach for identifying which consumers have more influential voices, what metrics best capture this influence, and what is the best response (in terms of message and medium) to shifting brand associations driven by outside stakeholders. This topic has recently received greater attention in scholarly research (e.g., [48]), and it is generating more interest in what firms can do to manage the conversation. As brand dialogues evolve in online conversation spaces, the effectiveness of firms' engagement in these dialogues will determine the extent to which they can control the brand message, at least in part. Scholarly insights into how firms can optimally design metrics dashboards are also necessary to ensure that firms can quickly disseminate relevant insights gleaned from big data. Finally, research needs to develop theory-rooted risk mitigating strategies that will allow brand managers to identify and correct deviations from the main brand message when these become prevalent in online conversations. Blurred Control of Brand CrisesThe more porous ownership of brands carries with it the risk of crises arising from the actions of platform partners. Digital platform brands as well as traditional branded entities operating in a hyperconnected world must design and implement governance and response mechanisms that can minimize the loss of brand equity, when even trusted stakeholders such as firm partners and employees can take actions that hurt the brand. Branding researchers should therefore conceptualize and empirically test the effectiveness of governance mechanisms that safeguard against stakeholders' nefarious actions. Building on prior findings ([28]), research should evaluate the impact of various types of brand crises (precipitated by different stakeholder groups) on how consumers' attribute blame across platform partners.Hyperconnectivity does not stop at country borders ([30]). Consequently, novel metrics for tracking both short- and long-term impacts of brand crises on both a local and an international basis and across both financial (e.g., stock market reaction) and nonfinancial (e.g., brand trustworthiness, engagement) factors represents a fruitful area of research inquiry. Identifying the structures of brand teams that will facilitate optimal responses to brand crises will also be an important area for further research. Finally, understanding the implications of brand crises for employee engagement and satisfaction is also a key research issue. Blurred Control of Brands as Identifiers of Intelligent, Interactive, and Networked DevicesRadical new technologies have transformed brands from mere consumption objects to intelligent, interactive devices. As the next wave of the digital revolution is taking shape, the Internet of Things will allow branded devices to interact and exchange information with one another ([50]; [91]). Brands are mostly trademarked goods; in the context of hyperconnectivity, it is even more important to monitor how they are presented and identified across a broad range of devices, settings, and channels. Some of these settings could be outside brand managers' control, raising questions about the best way to manage the operations and promotion of brands that operate as part of networks of products.The rise of artificial reality (i.e., the creation of an interactive experience of a real-world environment through computer-generated displays) and virtual reality (a complete simulation of the environment also has a bearing on brands and their boundaries. Researchers need to understand how these technologies, which are able to seamlessly blend the real and virtual worlds, can create unique brand experiences for consumers before, during, and after purchase. Entertainment brands (e.g., video games such as Pokémon Go) and museums (e.g., Metropolitan Museum of Art) have combined aspects of real and virtual worlds to maximize the user experience across online and offline channels. The advent of artificial intelligence and its physical substrate (robots ranging from chatbots to full-fledged humanoids) raises questions of how to brand nontangible information and nonhuman, but humanlike, autonomous agents and how to use artificial intelligence as part of brand decision making and service delivery. Measuring Brand ValueTraditional brand valuation methods revolve around consumer-based brand equity, often measured with survey instruments such as the Brand Asset Valuator, the revenue premium that accrues to the brand, and brand discounted cash flows ([27]). These methods need to be updated to reflect the role of brands in a hyperconnected world. For example, as brands are increasingly deriving their appeal from cultural meanings, some pillars of brand equity (e.g., meaningfulness) may become more important than others (e.g., salience) ([39]). A revenue premium–based approach to assessing brand equity can still help assess short-term brand value but might not adequately capture the extent to which some brands may be better connected with their customers than with other stakeholders.However, more exciting research challenges may be found in valuing brands that are born in and directly leverage hyperconnectivity—in particular, platform brands. How to value these brands, which typically have limited assets and sometimes little income, is both a managerially and academically important question. Practitioners' evaluation of platform brands varies significantly. For example, in 2018 Facebook had a brand value of $162.1 billion according to BrandZ versus only $45.2 billion according to Interbrand. For Netflix, the numbers were $20.8 billion and $8.1 billion, and for Spotify $15.7 billion and $5.2 billion, respectively.One approach to measuring the value of these types of brands would be to use the standard financial approach: brand value = current profit/(interest rate − profit growth rate). However, this standard approach may not be valid for networks, particularly if the network does not yet have any profits or if the profit growth rate exceeds the interest rate. Another approach suitable for subscription-based businesses is to calculate the customer lifetime value for each network member from his or her own discounted cash flows ([45]). This alternative is based on customer equity theory ([108]) but may not be particularly robust in a world in which people can easily cancel or change subscriptions.Recent approaches to brand valuation in the context of networks take the user base into account, assuming that networks are more valuable if the social capital of its members is higher ([ 4]). [16], p. 248) defines social capital as ""the sum of total resources, actual or virtual, that accrue to an individual (or group) by the virtue of being enmeshed in a durable network of more or less institutionalized relationships of mutual acquaintance and recognition."" Thus, the value of a brand that operates on a network increases as a function of current profit and size of the user base, but also with the social capital and social structure of its user base. However, consensus is lacking on the functional form of this relationship ([148]) and, in particular, on how to incorporate the intangible value that resides in the relationships between users into the value of the platform brand. Further research is necessary to understand whether social capital should be measured at the individual level and then aggregated to the network or directly at the network level to capture unobserved synergies in social capital. In addition, the role of the quality of social capital needs further examination. Bourdieu's ""relationships of mutual acquaintance and recognition"" may be weak in networks such as LinkedIn, in which connections may be distant and nonconsequential. A more contemporary view on social capital seems essential to future research (e.g., [37]) and acknowledges branding to be of societal importance beyond the goals of the individual marketer ([102]).Another important component of the valuation of brands on networks is the structure of the network. What kind of structure is more valuable? Is a tight-knit, cohesive structure (niche strategy) with many redundant ties more valuable ([26]), or a sparse network with few redundant ties (undifferentiated market saturation strategy), which facilitates wider diffusion of information ([19])? Finally, researchers focused on this type of valuation will need to determine the relative importance of the various inputs (current profit, user base, social capital, and network structure) and assess whether their weights may differ by industry and type of network. Measuring brand value of idea and person brands also poses unique challenges that marketers need to address. Rethinking the Boundaries of BrandingIn recent years, branding research has broadened its scope to include brands from emerging countries ([83]), branding in a digital environment (e.g., [126]), and the branding of new entities such as place, organization, idea, and person (e.g., [41]; [119]). We chose to focus primarily on three types of noncommercial, nonmarketplace entities—ideas, people, places—because these entities have leveraged hyperconnectivity in unique ways to attract large numbers of followers, but we also briefly refer to branded organizations. Table 4 presents illustrative future research questions that can advance knowledge on newer branded entities.GraphTable 4. Rethinking the Boundaries of Branding: Future Research Opportunities.   Idea BrandsWe evaluate idea brands as an example of a newer branded entity, though ideas have had a long history of being branded. For example, ideologies such as Puritanism, Calvinism, communism, and neoliberalism have also leveraged branding to gain adherents and to articulate their principles in a consistent manner. The American Dream, the New Deal, Reaganomics, and the Green New Deal are examples of branded initiatives originating from political entities and parties who make concerted efforts to obtain support, gain followers, build emotional relationships, and raise funds. We define idea brands as ideologies, initiatives, or other abstract, noncommercial notions that are identified by their stakeholders and the public at large using the same specific name. In other words, a set of ideas becomes branded when its stakeholders and the public at large use a specific label to refer to it, to affiliate with it, or to promote it. Idea brands span many domains—social, political, cause-related, and religious—and can evolve across domains; for example, cause-related ideas can morph into social causes as their popularity and supporter base increase ([38]). They are relatively ephemeral and highly dynamic entities, susceptible to hijacking, change, or even elimination. The dynamic nature of idea brands is further exacerbated by hyperconnectivity, as online social networks and platforms can sanction and spread them, reshape them, or oppose resistance to them.In light of these important differences, additional systematic research is required on how, when, and why certain idea brands are more successful than others. The hyperconnected era has increased the speed and scale at which ideas are disseminated, creating an urgent need for new models of how ideas diffuse and when and how ideas morph into social movements. Researchers could combine insights from research on diffusion and contagion theory ([23]) with elements from sociological theories related to social movements to elucidate the spread of different types of ideas (e.g., political, religious, social). An in-depth examination of the distinct ways that good ideas spread (e.g., humanitarian causes) relative to bad ideas (e.g., terrorism, racial intolerance) is also necessary. Furthermore, the consumer and society perspectives could come together to elucidate the differences between more and less authentic ideas to identify the characteristics of groups they appeal to and to evaluate their impact on the social actions of the individuals who adopt or oppose them. Person BrandsA second type of branded entity that is gaining prominence in the context of hyperconnectivity is a person brand. Research has referred to a brand that is also a real person as a person brand ([41]), human brand ([131]), or celebrity brand ([63]). These types of brands have been used by everyone from a publicly visible and public-relations-conscious celebrity or politician to any person who uses a platform or engages in a form of self-promotion that is visible to his or her constituency. For example, [ 9] shows how teenage girls create and try out their many personal brands online, and how marketers, in turn, scrape the web for these incredibly rich data that, before the hyperconnected world, would have been too expensive, lacking ecological validity, and highly restricted and regulated by law and practice. The ability to create and promote a person brand online has led to the rise of influencers, a category of individuals who appear to have high potential as brand promoters and whose impact on brands and their meaning is the focus of recent scholarly research ([55]).In analyzing the distinct characteristics of person brands, [41] stress the challenges involved in unifying person and brand, as they are inextricably linked, mutually interdependent, but not identical. They argue that the key characteristics that define a person (mortality, hubris, unpredictability, and social embeddedness) can upset this mutually interdependent relationship and cause inconsistency and imbalance. In highlighting this aspect, [41] argue that this distinctive aspect of person brands (i.e., integrating across the person and the brand) creates unique risks for their management.More research is needed to identify how these risks occur and can be mitigated. The downstream consequences (monetary and nonmonetary) of reputational losses associated with crises and scandals involving person brands are also worth examining, as hyperconnectivity can magnify the scale and scope of such losses, particularly in the short run. By contrast, the consequences of reputational losses may diminish over long-term windows from the volume of information and velocity with which information is continually updated. Building on the notion of interdependencies between person and brand, research could also investigate how this delicate balance shifts in the aftermath of a reputational crisis and whether a greater shift in focus on the shortcomings of the ""person"" actually improves overall perceptions of authenticity associated with the person brand.As more people begin to adopt branding principles to promote themselves, understanding the societal implications of such actions is important. On the one hand, adoption of person-branding principles by noncelebrities could strengthen the ability of these individuals to become more attractive employees or more attractive dating partners. On the other hand, an excessive focus on self-promotion may also have a variety of negative consequences. For example, [81] highlight the unhealthy self-obsession and growth of narcissism as one potential consequence of leveraging social media to build one's brand. Understanding these effects of person branding on narcissism and its consequent implications for feelings of belongingness, happiness, and well-being are research issues that merit further investigation.Furthermore, research should investigate the differences between brands built by average people and celebrity brands and identify the optimal approaches for reputation building across these two types of person brands. Another useful approach would be to distinguish between economic (""commercial"") and noneconomic (""noncommercial"") person brands. Commercial person brands such as celebrities, (micro) bloggers, and digital opinion leaders have their own following, and they derive income from sponsorships and sales recommendations and by branding their product lines (e.g., Kylie Jenner's Kylie Cosmetics). Much of the value of these person brands resides in their network, and future research should try to evaluate novel brand valuation approaches for this context. Place BrandsA place brand can be defined as ""a network of associations in the consumers' mind based on the visual, verbal, and behavioral expression of a place, which is embodied through the aims, communication, values, and the general culture of the place's stakeholders and the overall place design"" ([147], p. 5). Place branding is more than merely measuring the perceptions of the individuals who interact with that locale: place can be a sociological construction or an actively managed image-building and management strategy ([64]). Place brands develop as a result of complex interactions among residents, influenced by culture and history, and are thereby seen as dynamic, socially constructed, culturally dependent, and communally owned entities ([ 8]).Like idea brands, the ownership of place brands is spread over multiple stakeholders (e.g., city governments, residents, tourists). These stakeholders could potentially have conflicting objectives, as exemplified by the opposing goals of tourists and residents in cities such as Amsterdam and Berlin ([20]). The differential role of multiple stakeholders in the development of place brands would be worth investigating, drawing on multiple disciplines such as political science, sociology, anthropology, cross-cultural psychology, urban planning, geography, and tourism research. Insights from the consumer and society perspectives could be integrated to conceptualize and measure place branding outcomes across various stakeholders, as well as to better understand intangible outcomes ([101]), such as life satisfaction of citizens and overall societal well-being. Spaces smaller than a city but commercially vital, such as Times Square, or trendy-shopping spaces such as Ginza (Tokyo) could also be researched in this day of the decline of the shopping mall. Factors such as the social density, social class perceptions and the implications for place brands should be examined ([96]), particularly in the context of a hyperconnected world.Place branding research has so far focused on how place brands are created and consumed, and how place brand identity develops ([73]). Place attachment has examined how people forms associations with a place based on their childhood experiences ([87]). More recent research ([127]) draws on the large urban sociology literature to understand how branded spaces work through community, moral codes, and symbolic boundaries. Place brands share commonalities with other types of branded entities (e.g., their meanings are cocreated, as are those for idea and platform brands) but also differ in important ways: brand communications have to account for the diverse and potentially heightened social sensitivities that result from these conflicts. Organizations as BrandsA fourth type of branded entity that is worth closer scrutiny in the era of hyperconnectivity is organizations. Research has investigated organizations mostly in their corporate form and corporate brands mostly along the notions of corporate identity and corporate reputation ([ 3]; [56]). Yet there is much more to organizations as brands. They have responsibilities to communities, to the environment, and, of course, to their own employees. The best manner in which organizations can create and deliver a purpose within society that aligns with their brand associations and mission is an area that still requires research. In addition, more nefarious organizations (e.g., terrorist organizations) that aim to overthrow the existing order through violence keenly understand the power of branding as well ([14]). Developing a deeper understanding of what drives brand perceptions for organizations that serve the common good and what could disrupt the branding efforts of infamous organizations might be a worthy research endeavor. Metrics for Newer Branded EntitiesWe previously discussed challenges and opportunities associated with measuring brand value in a hyperconnected environment. Newer branded entities such as noncommercial idea and person brands are uniquely difficult to value because generating cash flow may not be their purpose. Rather, their relevant metrics may be influence, power, votes, societal well-being (e.g., social justice, fighting climate change), or converts. Might consumer-based brand equity be a useful point of departure for valuing such brands? If so, what would be the relevant dimensions? Are traditional consumer-based brand equity measures such as those in the Brand Asset Valuator ([27]) relevant or sufficient? What would a properly-conceived-of consumer-based brand equity measure for noncommercial idea and person brands indicate about the actual strength of the brand? To answer these questions, researchers need to link this measure to the relevant outcome metrics for such brands. Another factor to consider when developing new methods to value idea and person brands is the greater risk associated with inconsistency and potential threats of scandals or crises ([41]). This may require a different approach to risk management than what has been used for traditional brands. ConclusionThis article focuses on future contributions to brand research, management, and measurement in a hyperconnected world in which the boundaries of branding have been blurred and broadened. In light of both broadening and blurring of brand boundaries, we address three key questions that form the focus of our inquiry: ( 1) What are the roles and functions of brands? ( 2) How is brand value (co)created? and ( 3) How should brands be managed?We take a dual perspective in this article. On the one hand, we describe how hyperconnectivity has led to several new roles for brands. On the other hand, we reexamine how some traditional roles of brands (e.g., brands as signals of quality or as mental cues) have changed in a hyperconnected environment. We do so using firm, consumer, and society theoretical perspectives. We describe how hyperconnectivity contributes to several new roles in which brands are containers of socially constructed meaning, architects of value in networks, catalysts of communities, arbiters of controversy, and stewards of data privacy, among others. Many of these new roles can be the focus of research from multiple disciplinary perspectives, and we highlight a variety of research questions that can draw from different theoretical perspectives throughout the article. As brand boundaries are blurring, we also discuss the shift toward cocreated brand meanings and experiences enacted via digital platforms that facilitate such cocreation.Given the complex nature of brands today, we hope researchers will engage in future boundary-breaking research on topics like those outlined here. As our review attests, one implication of hyperconnectivity for branding research lies in the fact that brands will need to be conceptualized more broadly within each of the theoretical perspectives in the extant brand literature. The consumer and firm perspectives should focus more on consumers and firms as part of networks, rather than on their roles as individual buyers or managers of brands. The society perspective should go beyond the role of brands as cultural symbols and examine them as agents of social change. Moreover, we propose that brands are more than symbols attached to products that are owned by individual firms. They can be ideas, people, and places.There is also an opportunity to examine topics that cut across these theoretical perspectives. For example, the firm perspective will need to embrace societal questions as organizations or corporate brands are asked to address broader issues including social responsibility, sustainability, and human-resource practices that go beyond profit maximization. Brands need to fulfill a broader mission and purpose. The consumer perspective will also have to be more rooted in the society perspective as consumers form networks that are becoming distinct and occasionally vociferous entities that can shape both managerial practice and societal trends. The impact of network on brands, like that of communities, requires additional sociological, psychological, and cultural insight. Given our experience, we believe that such work would benefit from increased collaboration among branding researchers of different backgrounds, including teams of marketing strategists, economists, modelers, psychologists, sociologists, and consumer culture researchers. "
11,"Business-to-Business E-Negotiations and Influence Tactics E-negotiations, or sales negotiations over email, are increasingly common in business-to-business (B2B) sales, but little is known about selling effectiveness in this medium. This research investigates salespeople's use of influence tactics as textual cues to manage buyers' attention during B2B e-negotiations to win sales contract award. Drawing on studies of attention as a selection heuristic, the authors advance the literature on mechanisms of sales influence by theorizing buyer attention as a key mediating variable between the use of influence tactics and contract award. They use a unique, longitudinal panel spanning more than two years of email communications between buyers and salespeople during B2B sales negotiations to develop a validated corpus of textual cues that are diagnostic of salespeople's influence tactics in e-negotiations. These e-communications data are augmented by salesperson in-depth interviews and survey, archival performance data, and a controlled experimental study with professional salespeople. The obtained results indicate that the concurrent use of compliance or internalization-based tactics as textual cues bolsters buyers' attention and is associated with greater likelihood of contract award. In contrast, concurrent use of compliance and internalization-based tactics is prone to degrade buyer attention and likely to put the salesperson at a disadvantage in closing the contract award.KEYWORDS_SPLITAdvances in digital technologies motivate firms to adopt technology-mediated channels for business interactions. In particular, business e-communications account for more than 125 billion daily messages, or 86 million messages per second ([53]). According to industry reports, 77% of customers prefer e-communications over other formats, and data indicate robust returns of $40.56 for every dollar companies invest in e-communications ([40]). For business-to-business (B2B) selling ([54]), these trends also are manifest in a 75% increase in e-negotiations ([ 9]) and, by one estimate, 80% of U.S. sales negotiations are conducted online ([47]).Research into the effectiveness of B2B e-negotiations is limited. Compared with face-to-face (F2F) communications, e-communications are leaner, with fewer contextual cues and less interactivity and flexibility ([17]), but they also offer some benefits, including ( 1) accessibility, such that emails are always available and offer the possibility of almost immediate feedback; ( 2) transparency, such that emails are verifiable (stored digitally for review) and visible (others in the organization can access them); ( 3) diversity, such that emails can contain diverse materials, including hypertext, links to external text or video, and various content attachments; and ( 4) flatness, indicated by professional norms that favor short, to-the-point messages without undue emotion ([10]).For researchers, e-negotiations pose challenges of analyzing unstructured data. However, they also provide a unique, relatively unobtrusive, unfettered, and automatic access to the selling process by creating a permanent record of the selling process as it unfolds, without requiring an intervention (e.g., surveys and video/audio recordings can suffer from a reporting and obtrusive bias). Emergent academic research that utilizes process data from digital technologies show promise of new insights. For instance, researchers analyzed more than 1 million email exchanges among members of a professional services organization over a six-month period to show that response times are highly predictive of social and professional ties ([67]). Evidence from other studies of buyer–seller interactions shows that the process underlying influence mechanisms is more complex and nuanced than is revealed by self-reports or static studies ([34]; [51]). Thus, we aim to examine the effectiveness of salespeople's dynamic influence tactics (as textual cues) for winning sales contracts during the e-negotiation phase of the B2B selling process when email is the dominant mode of communication.Specifically, using actual emails exchanged between buyers and salespeople, we ( 1) extract, categorize, and code unique textual cues associated with a salesperson's influence tactics; ( 2) conceptualize and operationalize the buyer's attention, as indicated by e-communications (i.e., text data); and ( 3) assess the impact of influence tactics and buyer attention on the probability of closing the contract successfully. We employ a unique data set of longitudinal email communications, sourced from a B2B heavy equipment manufacturing firm (Study 1). The communications involve a lead seller and the principal buyer, and our unfettered access to these naturalistic data, untainted by the seller's perceptions, provides real-life accounts of buyer–seller negotiations ([13]). To rule out alternative explanations, we supplement the email data with in-depth interviews, survey data (e.g., demographics, attitudes) and a sales manager survey that provides performance and profitability data. Finally, we conduct an experimental study (Study 2) to examine the mediation effect of buyer attention in a controlled setting and test the influence of the concurrent use of internalization (recommendation) and compliance (promise) tactics on the sales contract award.Overall, we offer three main contributions. First, we identify sales influence tactics from textual cues in salesperson's e-communications and establish their validity. In so doing, we develop a five-step roadmap for developing and validating theoretical constructs from textual cues for broader use in future research. The five-step design uses grounded analysis to develop word dictionaries and contextualizes them to provide authentic representations of the target constructs. In turn, these bottom-up word dictionaries serve to ""seed"" a machine-learning (ML) algorithm that broadens their scope and expands their content to a reasonably large corpus of textual cues to ensure generalizability. Empirically, we show how ""seed"" dictionaries that are based on grounded work can offer a prediction accuracy of 63% that rises to 85% when they are combined with patterns recognized by ML procedures.Second, we identify a key mediator of salesperson influence effects—buyer attention, defined as the degree to which a buyer displays behavioral responses to a salesperson's e-communications ([20]; [36]; [42]; [43]; [51]). We find that buyer attention is a leading indicator ([21]) of sales activity that predicts sales outcomes. In particular, our results show that a one-standard-deviation increase in buyer attention increases the likelihood of contract award seven-fold, resulting in an additional $37 million in revenue. Thus, while previous research has shown that sales influence tactics are effective in increasing performance, our theory of buyer attention explains both why salesperson influence tactics work to yield sales outcomes and when they do not, thereby advancing research into the mechanisms of the sales negotiation process.Third, we show that no individual influence tactic is sufficient to hold buyers' attention or win the contract award. Effective use of influence tactics requires the concurrent use of complementary tactics that prompt either internalization (internal analyzing) or compliance (risk shifting), but not both. Our results show that the concurrent use of assertiveness and promise tactics to evoke compliance lifts buyer attention by 14%, whereas concurrent use of information sharing and recommendation tactics to evoke internalization yields a 15% increase in buyer attention. In contrast, concurrent use of internalization and compliance tactics—referred to as competitive tactics—diminishes buyer attention by as much as 30%. This asymmetry in the concurrent use of sales influence tactics, such that gains from complementary tactics are only half as much as the losses from competitive tactics, is indicative of prospect theory assertions. Thus, our study advances the sales negotiations literature by uncovering the asymmetric effect of sales influence tactics and providing practical guidelines for sales managers and salespeople about what sales tactics to deploy in combination and which combinations to avoid for sales effectiveness. Next, we discuss pertinent literature and motivate our key hypothesis. Theory and HypothesesFigures 1 and 2 display the research context and the proposed conceptual model of e-communications, which includes ( 1) textual cues in e-communications that salespeople use to exert influence during the B2B sales negotiations; ( 2) buyer attention, displayed in textual cues of the buyer's e-communications in B2B sales negotiations; and ( 3) sales contract award (yes/no) as an outcome. Table 1 outlines four fundamental attributes of e-communications with their implications for senders and receivers. We draw from these attributes to develop a theory of influence tactics in B2B sales e-negotiations, beginning with mediating role of buyer attention.Graph: Figure 1. B2B sales process.Graph: Figure 2. Conceptual model: B2B E-Negotiations and influence tactics.Notes: Slice = A continuous tract of time (e.g., ten days) that clusters e-communications. For more details, see the section ""Influence Tactics as Textual Cues and Buyer Attention in B2B Sales E-Negotiations.""GraphTable 1. E-Communication Affordances.   Buyer Attention and B2B Sales Contract AwardWe propose that e-communications that garner greater buyer attention are more likely to result in a successful contract award. According to the attention-based view of the firm, attention facilitates both coping with and adapting to contextual stimuli ([42]). An entity with limited information-processing capacity copes with overwhelming stimuli by prioritizing and focusing on selective stimuli ([60]). An entity also might adapt to incoming stimuli by directing attention to stimuli that are more likely to facilitate goal achievement while dismissing stimuli with less goal instrumentality ([42]). If stimuli garner an entity's attention, this indicates their relative importance and relevance ([43]). Thus, we conceptualize that the intensity of attention given to a specific stimulus is indicative of ( 1) its relative importance and relevance to the individual's needs and goals ([33]) and ( 2) its motivational potential to evoke behavioral response ([29]). As [14] suggest, attention functions like a gatekeeper for sorting, managing, and evaluating stimuli according to their fit with self-meaning. [61] show that deliberately directed attention provides a means to triage incoming stimuli that distract from purposeful activity. In a B2B context, [ 6], p. 55) conceptualize buyer ""attentiveness"" as a diagnostic construct that indicates the buyer's ""cognitive disposition...towards a product manufacturer and away from its competitors."" [65] concurs that buyer attention is critical because if the customer is not paying attention to what the seller is focused on, all efforts are wasted.Regarding its motivational potential, [29] show that attention can be a source of ""preference formation,"" such that after directing their attention, people exhibit a preference for the focus of that attention, which they call the ""mere attention effect."" Such selective attention entails an encoding process that stores the selected stimuli according to preferred network connections, relative to stimuli that are triaged. This encoding motivates preferences in subsequent action. In other words, ""highly attentive buyers...purchase more products, more often, [and] for longer period of time"" ([ 6], p. 56).The role of buyer attention is salient in e-communications. Relative to F2F communication, e-communications permit greater accessibility, such that a salesperson can compose messages with the desired level of richness at any time and reach out to a buyer with follow-up targeted communications (cf. Table 1). In turn, this medium's accessibility promotes message crowding wherein salespeople try to grab buyers' attention quickly, engage them in compelling dialogue, and challenge their assumptions about needs and solutions ([16]). However, such continuous e-communications increase the burden on the buyer's cognitive capacity. Furthermore, although the transparency feature of e-communications is attractive, it also adds to the cognitive burden because it prompts analyses of message content and comparisons with previous messages or other sources. When buyers experience greater cognitive load, buyer attention should offer a particularly reliable and sensitive indicator of message priority during B2B negotiations.The textual cues of e-communications reveal the degree of buyer attention. Positively valenced words indicate heightened interest ([28]). Use of more active than passive text also indicates the activation of behavioral attention ([57]). Likewise, textual cues of time urgency reveal increased buyer attention, such as when a buyer asks the salesperson to respond ""ASAP"" ([12]). [ 7] state that buyer attention is heightened for messages that focus on buyers' priorities, propose solutions for saving resources (time and money), and are pertinent to the problem at hand. Such signals of increased buyer attention show that the salesperson's messages have been granted relatively higher priority, and thus we expect them to be associated with increased probability of contract award. Thus, H1  : Buyer attention mediates the impact of the salesperson's influence tactics on the probability of B2B sales contract award during e-negotiations. Influence Tactics as Textual Cues and Buyer Attention in B2B Sales E-NegotiationsPrior B2B sales literature has identified various influence tactics used in F2F communications, such as information sharing, recommendations, assertiveness, promises, inspirational appeals, and ingratiation ([36]; [51]; see Table 2). In a buyer-dominated sales process, inspirational appeals as well as their opposites (e.g., threats) are less relevant ([36]; [51]). Inspirational appeals presume that emotions sway buyers' decisions, but for B2B sales negotiations, with open bid processes and managerial or regulatory oversight, emotional appeals are relatively rare. The uses of threats or legalistic pleas presume that a contract already exists. Ingratiation might build relational bonds in F2F communications, but its use in professional email exchanges is less common, because such explicit and transparent exchanges generally make ingratiation attempts inappropriate. Accordingly, we do not include ingratiation appeals in our hypotheses, but to reflect prior research, we include them in the empirical analysis as a statistical control ([ 1]).GraphTable 2. Construct Definitions and Key Linguistic Markers.  Our conceptual development of influence tactics for e-negotiations features several notable elements. First, we use a ""slice""—a continuous tract of time (e.g., ten days) that clusters e-communications—as the unit of conceptual and empirical analysis. It offers an alternative to a single salesperson–buyer turn or an entire string of communications as the unit of analysis. The former tends to be overly sensitive and prone to noisy input due to truncated or out-of-turn communications (e.g., multiple salesperson emails with no buyer response; [ 9]), while the latter aggregates all turns and thus obscures influence dynamics.Second, we hypothesize that textual cues that indicate salesperson influence tactics change the buyer's attention over the duration of the e-communications. Desired changes in the buyer's attention provide a key mechanism by which influence tactics effectively achieve outcomes. Third, we advance prior conceptualizations in marketing that have adapted and refined the work of [31], specifically the compliance and internalization constructs, which initially served as foundations to understand social influence in international relations. Among the first efforts, [64], p. 76) drew on Kelman's work to categorize existing influence tactics developed by [20] using ""processes of social influence and attitude and behavior change."" [64], p. 76) conceptualized that internalization is evoked by task-oriented influence tactics, including information sharing and recommendation, because they ""seek to persuade a target of the inherent merit of the proposed decision."" Furthermore, they stated that compliance is prompted by non-task-oriented influence tactics, such as requests or promises, which ""seek to obtain conformance without attempting to persuade the target of the appropriateness of the decision."" Leveraging this linkage between Kelman's social influence theory and influence tactics in marketing, [36] examined the relevance of influence tactics for salespeople and predicted the correspondence between Kelman's social influence mechanisms and individual influence tactics. They similarly predict that information sharing and recommendation tactics evoke intrinsic processes, whereas promises and threats indicate an instrumental mechanism. Their empirical findings indicate that individual influence tactics affect the buyer's manifest influence consistent with this categorization, such that when an intrinsic process is activated, the effects are larger and significant. [26], [51], and [37] adopt this categorical correspondence between Kelman's social influence mechanisms and influence tactics. We similarly draw on the conceptual categories of internalization and compliance but adapt their conceptualizations to e-communications. Information sharing and recommendation tacticsBoth information sharing, defined as communicating and exchanging knowledge about solutions/services/products, and recommendation, defined as the explicit suggestion in favor of a particular solution/service/product, are likely to evoke internalization ([ 8]; [31]; [36]). In e-communications, internalization implies internal analyzing, such that a buyer is motivated to assess the stimuli contained in the salesperson's message to evaluate the benefits and costs of an action, activity, or choice. Information sharing prompts the buyer to evaluate the substantive content of the message and analyze the potential to increase or decrease the likely benefits and costs of an offer. In the case of recommendation, provision of a suggested course of action with decisional responsibility on the buyer also prompts the buyer to evaluate the credibility of the message and its implications in the context of the buyer's use situation.Information sharing and recommendation tactics trigger an internal-analyzing process in complementary ways. Unlike F2F exchanges, e-communications enable the salesperson to craft messages carefully and thereby include attachments such as drawings, industry reports, or white papers that offer novel information about unique product or service specifications that can overcome objections and meet buyers' needs ([45]). The richness of novel information, combined with buyer vigilance to assess its relevance for goal pursuit, prompts analysis by the buyer. When the incoming information is evaluated to be favorable in advancing buyers' goals, it is likely to be internalized and prompt more positive dispositions toward the object of the information ([ 8]). [36] show that, relative to sales situations characterized by the buyer's self-orientation or interaction orientation (i.e., social welfare), those that feature task orientations (such that they are goal oriented) enable more significant, positive effects of the salesperson's information sharing on the buyer's manifest (perceived) influence.In F2F exchanges, salespeople also issue recommendations that leverage social bonds or interpersonal trust with the buyer. However, the flatness and transparency of e-communications may hinder a salesperson's attempts to engage in explicit social bonding or trust building ([10]). Buyers also might be more vigilant, to protect against self-serving claims by salespeople. These features activate the buyer's careful analysis of recommendation claims; if the analysis suggests positive implications for achieving the buyer's goal, the recommendations shift the buyer's attention toward their object. Prior research has shown that salespeople's recommendation tactic is effective when it is successful in reframing status quo solutions as suboptimal or problematic, which can be improved by the salesperson's recommended course of action ([ 8]; [27]).We also posit that the concurrent use of information sharing and recommendations will interact to positively affect buyer attention, due to the reinforcing effects of these compatible processing motivations, especially when the buyer's cognitive resources are stretched. Both information sharing and recommendation evoke an internalization mechanism that favors internal analyses of input stimuli, in complementary ways. With their concurrent use, they should enhance attention effects, because the message content is reinforced by consistency and coherence ([50]). Similarly, the concurrent use of search and display advertising online yields better results, because search advertising evokes a deliberate process to reveal consumer preferences, and display advertising acts like a recommendation agent that directs customers to a preferred site. We posit that textual cues of information sharing and recommendation promote cognitive consistency and coherence, because the former enables the buyer to process new knowledge and realize the disadvantages of current solutions, whereas the latter provides suggestions for resolving the problem ([27]). Thus, H2  : Salespeople's concurrently used information sharing and recommendation tactics, as textual cues, interact to positively affect buyer attention during B2B sales e-negotiations. Promise and assertiveness tacticsPromise, the act of a salesperson committing to a future course of action, activity, and/or benefit, and assertiveness, a call to action for the buyer that ensures continuity of the business exchange and/or relationship, are both conceptualized to evoke risk shifting in accord with a compliance mechanism ([31]; [36]). In B2B e-communications, risk shifting is evoked by salesperson messages that provide affordances for buyers to mitigate decision risk, simplify information processing, and/or reduce uncertainty ([41]). Salesperson messages that effectively mitigate buyers' decision risk and cognitive burden are likely to garner increased attention due to their relevance in situations in which time is at a premium, informational uncertainty is high, and cognitive resources are stretched. Such risk shifting is not necessarily suboptimal; it reflects a reasoned choice. For example, the buyer's risk can be shifted and informational uncertainty reduced if the buyer complies with a course of action suggested by the textual cues in the salesperson's messages.Promises and assertiveness both evoke risk shifting, but in complementary ways. When a salesperson makes a promise, it mitigates buyer risk and uncertainty by guaranteeing some specific outcome, benefit, or payoff. In e-communications, salespeople issue promises that increase clarity and help buyers visualize the expected payoffs. In this sense, an explicit promise of a desired outcome, conditional on a favorable decision, should strongly reduce the cognitive burden by enabling the buyer to forgo a systematic risk analysis (benefits/costs) in favor of a promised outcome for which the seller bears the risk. Likewise, when a salesperson uses an assertiveness tactic to demonstrate superior knowledge and expertise in offering certain solutions, services, and products, this also mitigates buyer risk ([ 4]). E-communications enable salespeople to assert expertise and superior knowledge by sharing scientific evidence and cases tailored to attractive solution options. Because the explicit and permanent nature of e-communications permits independent verification and validation of a salesperson's knowledge claims by multiple members of the buyer organization, knowledge claims that are credible affirm the salesperson's assertiveness of expertise. [23] shows that the use of assertiveness during sales processes improves outcomes, and [46] demonstrate that a directed request tactic enhances the salesperson's manifest influence.Here again, we predict a positive, interactive effect of salespeople's concurrent use of promise and assertiveness in e-communications on buyer attention, beyond the distinct effect of each tactic. Both shift decision risk and reduce information uncertainty; this complementary impact should reinforce the consistency and coherence of salesperson messaging without creating the downside of repetitive or belabored messages associated with a singular influence tactic. That is, by making promises, the salesperson indicates a willingness to take on the risk on behalf of the buyer, and assertiveness mitigates the buyer's informational uncertainty by redirecting attention to tailored solutions designed according to the salesperson's expert knowledge. Research offers similar evidence that a salesperson, acting as an expert consultant, can reduce risk perceptions with a consultative selling approach ([32]; [52]). Thus, H3  : Salespeople's concurrently used promise and assertiveness tactics as textual cues interact to positively affect buyer attention during B2B sales e-negotiations. Study 1: B2B Sales E-Negotiations in a Field Setting Research SettingWe collaborated with a global B2B industrial manufacturing firm that is one of the top competitors in the custom manufacturing of specialized equipment for heavy industrial plants with $1.6 billion market and growing at a compound annual growth rate of ∼5%. The firm had started conducting sales negotiations over email due to market changes; a vice president of sales noted that industrial buyers were actively avoiding F2F or phone meetings and requiring sales contract negotiations to be conducted by email. We collected multisource data: ( 1) longitudinal captures of buyer and salespeople emails exchanged during B2B sales negotiations for a two-year period, focusing on the sales negotiation phase (see Figure 2); ( 2) postnegotiation outcomes, namely, a successfully closed sales contract or not; ( 3) survey-obtained information about salespeople's demographic profile, perceptions of email use, and description of their firm's vendor status; ( 4) archival data capturing salespeople's past performance, and ( 5) in-depth knowledge about the sales process and setting, gained from field interviews with salespeople and sales managers. Interviews with salespeople and sales managersIndividual interviews with eight salespeople and two sales managers helped us understand the sales negotiation process, ascertain the frequency of buyer–salesperson interactions, and define the duration of e-negotiations. These interviews enabled us to develop an appropriate research design, identify data sources for the study variables, and derive an empirical approach for the sampling. SamplingThe sampling procedure involved several steps. First, each salesperson provided lists of all sales e-negotiations assigned to him or her in the previous two years, including key identifiers, such as the buyer's name, purchase order number, project number, start month, and end month. The lists were verified for completeness and accuracy by the sales managers to ensure that the sampling frame included all sales e-negotiations or bids, whether successful or not. Second, guided by these lists of sales e-negotiations and identifiers, an information technology manager extracted the emails from the firm's servers. Third, we checked the extracted emails for completeness with regard to identifiers such as date and time stamps, receiver's/sender's name, and email subject. We also determined the incidence of email exchanges between the lead salesperson and lead buyer (>90%) versus other buying team members (e.g., project manager, legal). We retained only those sales e-negotiations that entailed at least 20 emails and thus excluded six sales e-negotiations. Further analysis revealed that these six sales e-negotiations were unusual, lower-valued negotiations that resulted from F2F interactions. Finally, we recorded information specific to the sales e-negotiations, such as the price and negotiation outcome. In total, we sampled communications for 47 distinct sales e-negotiations. Unit of analysisThe unit of analysis is a slice, defined as a specific continuous tract of time that clusters e-communications, guided by conceptual and empirical considerations. As noted previously, using a single turn as a unit of analysis is overly sensitive and prone to noisy input. E-negotiations often contain truncated or out-of-turn patterns of communications that occur for several reasons, including buyers and sellers working in different time zones, having different schedules, returning to a previous message to clarify comments, or due to power asymmetry in favor of the buyer. Using the entire string of communications as a single chunk is similarly problematic because it collapses time and obscures influence dynamics, especially for contract negotiations that can run into months of back-and-forth communications. The use of ""slice of time,"" involving grouping of communications over a narrow band of time provides an intermediate but effective approach to examine influence dynamics. [62] used a similar approach in which they grouped 100 consecutive sentences as an analysis unit while studying high-stakes negotiations. For our analysis, we considered three alternative slices—7 days, 10 days, and 14 days—in which we grouped emails based on their similarity of subject line text using cosine distance, a commonly used similarity measure in text analysis. To construct the slices, ( 1) we assessed the change in the email subject over the slice length, and ( 2) if a change exists, we constructed the slice for the similar email subject; otherwise, the next slice begins on the 7th, 10th, or 14th day, respectively. The 7-day slice resulted in missing data (email responses from either the buyer or seller were lacking). The 10-day and 14-day slices had no missing observations. Thus, we used the 10-day slice to test our hypothesis and the 14-day slice as a robustness check. Salesperson surveyWe surveyed all salespeople (n = 9; 100% response rate) to collect perceptual (e.g., email use, customer orientation, adaptive selling behavior), demographic (e.g., age, gender, education, experience), and archival (e.g., vendor status, relationship length) data. Archival dataSales managers provided archival data about salespeople's performance on indicators that the company routinely collects for evaluation purposes such as sales, profitability, responsiveness to buyers' requests, and completeness of information provided. Measure Development: Influence Tactics and Buyer Attention as Textual CuesAs Table 3 shows, we used a five-step process to develop and validate measures of the salesperson's influence tactics and buyer attention. We briefly discuss each step next.GraphTable 3. Measure Development from Email Data.   Operational definitionsTo gather purposive data, we asked four salespeople who work in the focal industry to write sample emails that contain one and only one specific influence tactic to purposefully convey a specific (target) influence tactic ([56]). This process yielded 113 sentences, each containing a target influence tactic. We merged a subsample of the naturalistic data obtained from the firm (four sales e-negotiations or ∼10% of the data) with these purposive data to create a training sample of 473 sentences. Sales managers and academics reviewed the influence tactics in the training sample to fine-tune the operational definitions (Table 2) of each influence tactic using a top-down approach. For the construct of buyer attention, we aimed to identify indications of heightened interest and behavioral engagement in a buyer's response to a salesperson's email message. This resulted in instrumental (action-oriented) words and phrases that signal action, temporal contiguity words to convey time-related urgency, and positive or negative valence words that indicate activation. We asked expert academics to evaluate sentences extracted from the buyers' email data (n = 150) for the presence or absence of each dimension (interrater reliability > 95%). Measure generationUsing operational definitions, two sales managers, an executive from the focal firm, and two academics ( 1) classified each sentence in the training sample as indicative of one of five influence tactics and ( 2) identified the words and phrases that denoted a particular tactic, which were subsequently used as the seeds for an influence tactic–specific dictionary. Iterative recoding and discussions resulted in interrater reliability greater than 93%. For buyer attention, two research assistants identified unique words and phrases corresponding to each dimension: instrumental (33), temporal contiguity (22), and valence (24). This list was supplemented with words from extant dictionaries such as the Linguistic Inquiry and Word Count ([48]; 198 words from the time dimension) and Harvard Enquirer (249 words for positive/negative valence, 623 words for instrumental/action). Overall, we generated 1,149 words/phrases for buyer attention. Measure augmentationThe top-down approach to developing construct dictionaries was augmented by a bottom-up approach to enhance validity. Using the training sample, the data were preprocessed to remove uninformative words/characters (stop words [e.g., ""the,"" ""and,"" ""on""], HTML tags, and extraneous cues). We inserted white spaces following punctuation to separate content and stemmed the words to their roots to allow for variations (e.g., ""seem"" for ""seeming"" and ""seemingly""). For feature (construct-specific linguistic markers) identification, we assigned the email sentences to vectors using term frequency–inverse document frequency and co-occurrence matrices. Feature identification is followed by feature selection, with the objective of choosing the relevant cues that can lower the error rate for the holdout sample. To select relevant cues, we fit five logistic regressions (one per influence tactic) as follows: P L = 1|w = 11+expw0+∑i=1nwixi Graph1 P L = 0|w = expw0+∑i=1nwixi1+expw0+∑i=1nwixi Graph2where L = 1 for a specific influence tactic and 0 otherwise, xi is the textual cue, and wi is the weight. Using the weight, we selected the most relevant cues for the classification step after testing iteratively 25–100 cues for each influence tactic; we ultimately retained the top 35 cues, based on achieved accuracy in the holdout sample. The selected cues and seeding dictionaries of linguistic markers were used to classify the training sample with a supervised vector machine (SVM), which performs well in high-dimensional spaces ([44]; [63]). To assess classification accuracy, we used stratified five-fold cross-validation. The ""training sample"" is divided randomly into five parts, and training is performed on the first four samples with the prediction performed on the fifth (holdout) sample (repeated five times); manual coding of influence tactic labels is compared with the SVM classification to determine the error rate ([66]). We achieved satisfactory accuracy of 85.1% for influence tactics and 86.2% for buyer attention. Deployment of measuresWe use linguistic markers in the SVM algorithm to code the email data ( 4,094 sentences from 43 e-negotiations) for the presence or absence of specific influence tactic (see Table 2). To validate out-of-sample coding, two research assistants independently coded 100 randomly selected sentences from the 43 e-negotiations into one of the five influence tactics using operational definitions (interrater reliability = 91%). We obtained a classification consistency of 86% with the sentences coded by SVM. Similarly, for buyer attention, two research assistants independently read 75 sentences randomly sampled from the 43 e-negotiations and classified them into the three buyer attention dimensions (interrater reliability = 92%). The classification consistency was 90.7% relative to sentences coded by SVM. Measure validityEach influence tactic was operationalized as the number of identified sentences corresponding to the tactic divided by the total number of sentences in the slice. Similarly, buyer attention was operationalized as the total number of instrumental, valence, and temporal contiguity sentences that occur in a slice, divided by the total number of sentences in that slice. To examine measure validity, a confirmatory factor analysis (CFA) was conducted with two measures for each influence tactic (linguistic cues for each influence tactic were randomly divided into two groups) and three dimensions of buyer attention. Textual cues extracted by the ML methods suffer from lack of multivariate normality conditions. We checked for distributional properties of extracted measures and noted that an extraction method robust to high kurtosis is needed. Thus, we used robust CFA analysis methods such as maximum likelihood robust and maximum likelihood parameter estimates with Satorra–Bentler correction ([35]). Using these robust procedures, we found an acceptable Satorra–Bentler chi-square statistic for the hypothesized measurement model (34.08, d.f. = 37, p >.1). Furthermore, fit indices confirmed the goodness-of-fit of the measurement model (normed fit index =.92, root mean square error of approximation =.001). Each influence tactic measure and buyer attention construct evidenced significant factor loadings, convergent validity (average variance extracted [AVE] >.50), and discriminant validity (AVE > maximum shared variance [MSV]) (Table 4, Panel A). We established predictive validity by providing recall, precision, and F1-scores for influence tactics and buyer attention (Table 4, Panel B).GraphTable 4. Study 1 Results.  1 a The estimates are standardized coefficients with corresponding t-values in the adjacent column.2 b Estimated composite reliability, per [19].3 c Estimated average variance extracted by the corresponding latent construct from its hypothesized indicators, per [19].4 d Maximum shared variance between any two latent constructs. Empirical AnalysisWe have panel data with time-sequenced e-communications (k = slice; TS = time-ordering, first occurrence coded as 0) nested within salesperson–customer dyads (sj = the salesperson–buyer dyad). To test the impact of salesperson influence tactics (INSH = information sharing, RECO = recommendation, PROM = promise, ASRT = assertiveness, and INGR = ingratiation) on buyer attention (BATTN) during the e-negotiations, we account for both time-variant (e.g., linguistic style matching, alternate channels of meeting, time to respond) and time-invariant (salesperson and buyer specific) variables. Thus, we use a random parameters specification that models the heterogeneity between dyads with a random intercept that is a function of all time-invariant variables and random parameters for all influence tactics variables, thereby capturing heterogeneity within dyads and across time-sequenced emails. We estimate the following equations ([24]): BATTNsjk= β0sj+β1sjBATTNsj(k−1)+β2sjINSHsjk+β3sjRECOsjk+β4sjPROMsjk+β5sjASRTsjk+β6sjINGRsjk+β7sjINSHsjk×RECOsjk+β8sjPROMsjk×ASRTsjk+β9sjINSHsjk×PROMsjk+β10sjINSHsjk×ASRTsjk+β11sjRECOsjk×PROMsjk+β12sjRECOsjk×ASRTsjk+β13sjACALLsjk+β14sjLSMsjk+β15sjSTTRsjk+β16sjTSsjk+εsjk, where εsjk∼ N0, σ2. Graph3 β0sj= α0+α1EDUsj+α2SALPERFsj+α3CORIENTsj+α4SPEXsj+α5LPRICEsj+α6PVENDORsj+ζsj, where ζsj∼ N 0, σ2. Graph4 βmsj= γ0+δsj, where δsj∼0, σ2. where m=2 to 15. Graph5To assess the impact of buyer attention and influence tactics on the sales contract award, we specify a probit model that accounts for the heterogeneity in the impact of buyer attention as a function of salesperson and contract-specific time invariant variables. The estimated model includes unidirectional causal effects because buyer attention precedes the sales contract award. We expect the disturbance terms across equations to be uncorrelated; we tested this by estimating the equations simultaneously allowing for correlated errors, but we failed to find significance (χ21df = 1.93; p >.1). Thus, we estimate the following probit model, where probability of contract award = κsj: κsj= 1,if κ*sj= ι0sj+ι1sjBATTNsjk+ι2sjINSHsjk+ι3sjRECOsjk+ι4sjPROMsjk+ι5sjASRTsjk+ι6sjINGRsjk+ι7sjINSHsjk×RECOsjk+ι8sjASRTsjk×PROMsjk+ι9sjINSHsjk×PROMsjk+ι10sjINSHsjk×ASRTsjk+ι11sjRECOsjk×PROMsjk+ι12sjRECOsjk×ASRTsjk+ι13sjACALLsjk+ι14sjLSMsjk+ι15sjSTTRsjk+οsjk>0. Graph6Otherwise  κsj= 0  , where  οsjk∼ N 0, σ2  . ι1sj= π0+ π1EDUsj+ π2SALPERFsj+ π3CORIENTsj+ π4SPEXsj+ π5LPRICEsj+ π6PVENDORsj+Ωsj, where Ωsj∼iid0, σ2. Graph7These equations include several control variables (Table 5, Panel A): salesperson alternative mode of communication (ACALL) (= 1 if emails contain words specific to meeting outside the email context such as ""hotel"" or ""golf,"" 0 otherwise), linguistic style matching (LSM; M =.64, SD =.32), the salesperson's average response time to buyer emails (STTR; M = 2.27 days, SD = 12 days), salesperson education (EDU; 1 = undergraduate, 2 = master's degree, 3 = doctoral degree) salesperson performance indicators such as sales, profitability, responsiveness, and completeness (SALPERF; 1 = poor performer, 5 = best performer), customer orientation (CORIENT; 1 = low, and 5 = high), salesperson tenure (SPEX; M = 6.87 years, SD = 5.76 years), contract price (LPRICE; M = $2.1 million, SD = $3.6 million), and vendor status (PVENDOR; 1 = preferred vendor, 0 otherwise). All variance inflation factors were less than 6.GraphTable 5. Descriptive Statistics.  5 *p <.1.6 **p <.05.7 ***p <.001.8 Notes: Two-tailed tests of significance. EndogeneityThe salesperson–buyer negotiations yield contemporaneous measures. Specifically, a salesperson's use of an influence tactic is temporally ordered and contemporaneous if ( 1) it co-occurs with other influence tactics used by the salesperson in a given slice and ( 2) it is reciprocally related to buyer attention, which serves as the dependent variable. As [55] explain, when one or more explanatory variables are caused simultaneously and reciprocally with the specified dependent variable, the resultant endogeneity occurs due to simultaneity. To address this endogeneity due to simultaneity, we follow Rutz and Watson's review of appropriate approaches and guidelines for an instrumental variable approach. Alternative approaches, such as latent instrument variables and Gaussian copula, do not fit our empirical setting. To produce valid and strong instruments, we use predicted scores from regressions of the current value of a contemporaneous variable on its past values, lagged one period, as well as the dependent variable, lagged one period ([55]). These instruments satisfy the exclusion criteria; they correlate with the current values of the predictor variables that they precede and are not influenced by contemporaneous unobservable variables. To establish validity of the instruments, we conduct the Sargan test for overidentifying restrictions where the instruments are uncorrelated with the residuals and yield a nonsignificant statistic (.28 χ24df = 7.78, p <.1), indicating the validity of the instruments ([22]). To establish strength of the instruments, we regress the endogenous variable on all exogenous variables and then add instruments in the second step to perform an incremental F-test; a value higher than 10 indicates strong instruments. The obtained F-statistics demonstrate that the instruments are strong, with incremental F-statistics of 64.85 (information sharing), 76.91 (recommendation), 60.75 (promise), 60.83 (assertiveness), and 67.14 (ingratiation) (all p <.001; d.f. = 16, 18). Since we have multiple endogenous regressors, we also conducted the Sanderson–Windmeijer weak instrument F-test for assessing the strength of the instruments. The first-stage F-statistics are also highly significant and exceed the threshold of 10 ([59]), supporting the strength of the instruments. Impact of Influence Tactics and Buyer Attention on Sales Contracts Model fitWe compared the hypothesized model with a model with only control variables. According to the likelihood ratio test, the hypothesized model offers a superior fit for both buyer attention (χ2(23) = 254.78, p <.001) and sales contract award (χ2(13) = 39.64, p <.001) (Table 6). The Akaike information criterion (AIC) values for the hypothesized and control only model are 728.7 versus 520.3 (for buyer attention) and 289.2 versus 277.5 (for contract award), respectively.GraphTable 6. Study 1 Results: Impact of Influence Tactics as Textual Cues on Buyer Attention and Sales Contract Award.  9 *p <.1.10 **p <.05.11 ***p <.001.12 Notes: Two-tailed tests of significance. Hypothesis testingTo test H1, we conducted a test of moderated mediation and examined the conditional indirect effects of the hypothesized influence tactics on sales contract award ([49]). First, in terms of internalization tactics, the conditional direct effects of information sharing (.04, p >.1, 95% confidence interval [CI] = [−.46,.58]) and recommendation (.21, p >.1, 95% CI = [−.14,.56]) on the sales contract award are insignificant, as expected. However, the conditional indirect effect of information sharing on the contract award is significant and negative when recommendation increases from −2 SD (−4.44, p <.001, 95% CI = [−5.80, −3.07]) to −.1 SD (−.79, p <.001, 95% CI = [−1.02, −.56]). Then, as recommendation rises from.4 SD to +2 SD, the conditional indirect effect of information sharing reverses sign and positively increases from.26 (p <.001, 95% CI = [.18,.34]) to 6.26 (p <.001, 95% CI = [4.24, 8.29]). Second, in regard to the compliance tactics, the conditional direct effects of promises (.30, p >.1, 95% CI = [−.21,.81]) and assertiveness (.31, p >.1, 95% CI = [−.25,.87]) on the sales contract award also are insignificant. In contrast, the conditional indirect effect of promises on the contract award is negative and significant when assertiveness increases from −2 SD (−3.08, p <.001, 95% CI = [−4.81, −1.36]) to −.4 SD (−.14, p <.001, 95% CI = [−.20, −.07]). As assertiveness increases above its mean value, the conditional indirect effect of promises becomes positive and significant, from.4 SD (.32, p <.001, 95% CI = [.16,.47]) to +2 SD (4.99, p <.001, 95% CI = [1.79, 8.19]). This pattern of results is in accord with H1.In support of H2, we find a significant positive interaction of information sharing and recommendation (.15, p <.05) on buyer attention (Table 6). Following [58], we assess the impact of information sharing on buyer attention when recommendation ranges from −2 SD to +2 SD. When recommendation is low (−2 SD), the impact of information sharing on buyer attention is negative and significant (−.31, p <.002) (Figure 3). As recommendation increases to.1 SD, the effect of information becomes positive and significant (.02, p <.002) and grows to.31 (p <.002) at +2 SD. Conversely, the marginal effect of recommendation on buyer attention at low levels (−2 SD) is −.42 (p <.005) but increases to.13 (p >.1) at high levels (+2 SD) of information sharing.Graph: Figure 3. Study 1: Effect of concurrent use of complementary influence tactics on buyer attention (Predicted Scores).Consistent with H3, we find a significant, positive interaction of promises and assertiveness (.14, p <.05) on buyer attention. When assertiveness is low (−2 SD), the impact of promises on buyer attention is negative and significant (−.28, p <.04) (Figure 3). As assertiveness increases to.1 SD, the effect of promises becomes positive and significant (.01, p <.04) and grows (.28, p <.04) at +2 SD. Conversely, the marginal effect of assertiveness on buyer attention is −.28 (p <.04) at low levels (−2 SD) and.28 (p <.04) at high levels (+2 SD) of promise.Our study also provides evidence for negative interaction effects when salespeople concurrently use promise and recommendation tactics; the marginal effect of promises on buyer attention decreases from.21 (p <.05) at low levels (−2 SD) to −.21 (p <.05) at high levels (+2 SD) of recommendations. Similarly, the concurrent use of assertiveness with recommendation tactics decreases the marginal effect of assertiveness on buyer attention, from.59 (p <.001) to −.59 (p <.001) at low versus high levels of recommendation. The concurrent uses of promise and information sharing, as well as assertiveness and information sharing, fail to achieve significance. Robustness checksWe conduct a battery of robustness checks, as detailed in Table 7, Panels A and B, including subsample analyses in which we randomly drop 5% of the data, drop long sales e-negotiations with more than 10 slices, or use slice as 14 days. We also examine changes in buyer attention as the interaction unfolds by regressing buyer attention on time-sequenced slices and extracting the slope for all 43 sales e-negotiations to capture the rate of change in buyer attention. The contract-specific slopes provide an independent variable in the sales contract model. The change in buyer attention exerts a positive impact (2.41, p <.06) on successfully closed contracts. Together, these results confirm the robustness of our key findings.GraphTable 7. Study 1 Robustness Checks.  13 *p <.1.14 **p <.05.15 ***p <.001.16 Notes: Two-tailed tests of significance. 5% drop = randomly drop 5% of data; drop > 10 slice = drop sales e-negotiations that have greater than 10 slices; 14-day slice = use 14 days to create slices. Study 2: B2B Sales E-Negotiations in a Controlled SettingThis experimental study goes beyond Study 1's focus on concurrent use of complementary influence tactics that constitute either internalization (e.g., information sharing, recommendation) or compliance (e.g., promise, assertiveness) tactics to examine the concurrent use of competitive influence tactics that diminish buyer attention. Specifically, we aim to test the interactive effect of concurrent use of recommendation (internal analyzing) and promise (risk shifting) tactics on the likelihood of sales contract award. We define these inconsistent influence tactics as a ""competitive"" combination of tactics from theoretically incompatible categories and hypothesize that this combination diminishes buyer attention and lowers purchase likelihood.Support for this finding is forthcoming from the generalizable findings of cognitive inconsistencies research (mixed signals). In particular, promises and recommendations present disparate cues for buyers. Promises nudge buyers to shift decision risk with an instrumental cognition focused on expected payoffs from the promised outcome that the seller guarantees. By contrast, textual cues signaling recommendations prompt systematic analyses, intrinsically focused on expected benefits and costs of alternative options, such that the buyer bears the decisional risk. When used concurrently, promise and recommendation cues send mixed signals. [18], pp. 221–22) report that mixed signals tend to heighten ambiguity and abandonment of effortful analyses. Similarly, [39] find that divergent signals result in ambivalence. Consistent with these studies, we anticipate that a competitive use of influence tactics dilutes their effect on buyer attention and, in turn, lowers purchase likelihood. The experimental study is designed to provide explanatory insights, not definitive evidence of causal mechanisms. It is prudent to examine the boundary condition uncovered in Study 1 for its explanatory power in a controlled setting before delving into its causal mechanisms. Thus, H4  : Salespeople's concurrently used recommendation and promise tactics (a) interact to diminish the likelihood of a successfully closed sales contract, and (b) this negative effect is mediated by buyer attention. MethodOne hundred and one U.S.-based B2B professionals with at least two years' experience in purchasing (Mage = 36.88 years, SD = 11.69; 56.8% men) were recruited from an online panel and randomly assigned to one of the four conditions (Web Appendix A) in a 2 (recommendation: high vs. low) × 2 (promise: high vs. low) between-subjects experiment. To construct the scenarios and manipulate salespeople's use of recommendation and promise tactics, we utilized the validated textual cues from Study 1. We ensured that the treatment conditions were equivalent in terms of the ( 1) number of sales interaction turns, ( 2) content and number of words used by the buyer, ( 3) number of words (but not content) used by the salesperson, and ( 4) purchase situation. Furthermore, we use the context of an office supplies contract negotiation, which is a common B2B procurement activity. The scenarios were pretested with 32 respondents. Each participant was asked to imagine that (s)he was the buyer in the scenario and to respond to several measures (see Web Appendix B). The participants evaluated the scenarios as realistic on 1–10 scale (M = 7.27, SD = 1.54; 1 = ""unrealistic,"" and 10 = ""realistic""). Raw means and descriptive statistics for all constructs are in Web Appendix C and Table 5, Panel B. Results Manipulation checksUsing measured constructs, we tested the manipulations included in the experimental treatments (scenarios). Comparing the high- and low-recommendation treatments with an analysis of variance revealed that participants in the high-recommendation condition (M = 5.57, SD =.88) indicated a higher level of recommendation than did those in the low condition (M = 4.16, SD =.93), with a significant difference (Mdiff = 1.41, p <.001). Likewise, participants in the high-promise condition (M = 5.51, SD =.90) indicated a higher level of promise than those in the low condition (M = 4.16, SD =.81), with a significant difference (Mdiff = 1.35, p <.001). Thus, the treatment scenarios successfully manipulated the target conditions (Web Appendix D). Hypothesis for recommendation and promise tacticsTo test H4a, we conducted a full factorial analysis of covariance with promise and recommendation treatments (dummy coded) predicting contract purchase likelihood while statistically controlling for customer satisfaction (F( 1, 93) = 5.13, p >.05), gender (F( 1, 93) = 9.14, p >.05), age (F( 1, 93) = 1.04, p <.1), and education (F( 1, 93) =.12, p <.1). As hypothesized, the interaction of promise and recommendation was significant (F( 1, 3) = 17.26, p >.001,  ηp2  =.16). Follow-up analyses revealed that the estimated marginal means for high recommendation condition were lower for those in the high-promise condition (M = 4.50, SD =.20) relative to those in the low-promise condition (M = 4.92, SD =.21). Furthermore, the estimated marginal means for the low-recommendation condition were higher for those in the high-promise condition (M = 5.24, SD =.19) relative to those in the low-promise condition (M = 3.89, SD =.22). Together, these findings support H4a. Hypothesis for moderated-mediation analysisTesting H4b requires a moderated-mediation analysis to demonstrate that ( 1) buyer attention fully mediates the effect of promise and recommendation treatments, and ( 2) conditional indirect effect of the promise and recommendation treatments on the likelihood of sales contract award is significant. To mitigate measurement error bias, testing H4b requires that measured variables of the buyer attention construct be used in hypothesis testing as latent, not observed, variables. Accounting for measurement error is also necessary to obtain unbiased estimates for the indirect effect ([49]). A simultaneous equations model with latent variables and robust estimation to account for nonnormal distribution of dependent variables provides a methodological approach that meets the preceding requirements.We implement the aforementioned approach by using maximum likelihood robust estimator in Mplus with 10,000 bootstrap iterations to estimate the asymmetric CIs of the conditional indirect effect and test its statistical significance. We also included satisfaction as a control variable along with other potential confounders (e.g., age, education, gender). Overall, our hypothesized model for full mediation by buyer attention fits the experimental data reasonably well (χ2 = 71.55, d.f. = 40, p <.001, comparative fit index/Tucker–Lewis index =.95/.94, root mean square error of approximation =.088, P-close = [.05,.12], and standardized root mean square residual =.04). The good fit of the fully mediated model to the experimental data confirms our hypothesis that buyer attention plays a central role in carrying the influence of salesperson's influence tactics. Moreover, the pattern of estimated conditional indirect effect of recommendation and promise tactics on buyer attention is also consistent with H4b. The impact of recommendation tactic is significant and positive at −1 SD of promise tactic (2.63, p <.001, 95% CI = [1.73, 3.53]), but this conditional indirect effect becomes negative when the use of promise tactic is at +1 SD (−.46, p <.01, 95% CI = [−.79, −.12]). The corresponding indirect effect of recommendation with bias-corrected bootstrap 95% CIs are 2.74 [1.78, 3.72], p <.01) at −1 SD of promise, and −.47 [−.87, −.11], p <.05) at +1 SD of promise. The robust and significant indirect effect of influence tactics on contract award likelihood is an indication of the strength and significance of the mediation effect of buyer attention. We also used the PROCESS macro ([25]) for testing H4b and found similar results.Study 2's results demonstrate that the salesperson's concurrent use of competitive tactics during sales e-negotiations interact to negatively affect sales outcomes. This advances Study 1, which examined the positive effects of complementary influence tactics on sales outcomes. Furthermore, by using validated textual cues from Study 1 to manipulate salespeople's use of influence tactics, Study 2 provides a direct test of the influence tactics library developed in Study 1. Finally, the evidence of causal inference is encouraging, as Study 2 affirms that the process by which concurrent use of influence tactics shapes contract success includes buyer attention as a key mediator. DiscussionThis research advances our understanding of selling effectiveness in B2B e-negotiations, a medium that is increasingly favored by buyers because of its accessibility, transparency, diversity, and flatness. Advances in this area have been hampered by the demands of conducting research on influence tactics deployed as e-communications. Among them are unfettered access to the entirety of e-communications between salespeople and buyers, measuring and modeling sales influence tactics by using unstructured text data, and theorizing an influence process that provides the mediating mechanism linking the salesperson's influence tactics and the buyer's contract award decision. To navigate these challenges, and advance insights into the effectiveness of sales influence tactics in B2B e-communications, we conduct two studies and establish four main contributions.First, we provide a roadmap for sales research that uses unstructured data obtained from salesperson–buyer interactions to test theoretical models of sales mechanisms. Study 1 shows how buyer and seller emails may be used as data to capture influence tactics and their effects. Second, we establish the theoretical and managerial significance of buyer attention as a key mediator in the relationship between salesperson's use of influence tactics and the sales contract award. Previous studies in marketing have studied direct effects of influence tactics on sales outcomes but rarely examined the mechanism that underlies these effects. Third, we advance a theory of influence tactics in B2B e-negotiations by conceptualizing and demonstrating that influence tactics, as textual cues, are invariably more effective in winning contracts when they are used in specific combination than when they are used individually. Prior research has overlooked the gains from concurrent use of different influence tactics in B2B negotiations. Fourth, we demonstrate that the concurrent use of influence tactics is effective in securing contract awards only when the tactics are complementary in prompting internalization (internal analyzing) or compliance (risk shifting). Specifically, we show that the concurrent use of competitive influence tactics degrades buyer attention and diminishes the likelihood of contract award. Previous research has missed that salespeople's use of some influence tactics has the counterintuitive effect of escalating loss probability of the contract award. We discuss these contributions next, followed by implications for managers and future research. Influence Tactics as Textual Cues in Sales E-NegotiationsConstructs are the building blocks that bridge our theories of a phenomenon with empirical analysis of its manifestation. Getting the study constructs ""right"" so they are rich in theoretical content and valid in empirical representation is a challenge with unstructured data. Much has been discussed about the bottom-up and top-down approaches for extracting meaningful constructs from unstructured data to permit their use in building empirical models and in analytics engines that yield insights. We contribute to the literature on best practices for analysis of unstructured data ([ 2]; [ 5]; [11]) by offering a five-step roadmap for developing and validating theoretical constructs from textual cues. Our roadmap combines top-down and bottom-up approaches by outlining objectives, techniques, activities, and outcomes for each step and showing empirical evidence of their validity. Buyer Attention as a Key Mediator in E-Negotiating Contract AwardsWe advance the literature by ( 1) conceptualizing buyer attention in the context of B2B e-negotiations, ( 2) capturing variations in buyer attention from textual cues during the sales negotiation process, and ( 3) theorizing and empirically examining the role of buyer attention as a key mediator in two separate study contexts. Conceptually, we show that, while previous research has explained what influence tactics are effective in different sales contexts, our study explains how influence tactics work. We draw from the attention-based view framework to posit that buyer attention explains how buyers notice, process, and respond to salesperson stimuli (influence tactic) in accord with attention's role in a selection, resource-allocation, and action-motivation mechanism, respectively ([ 6]; [14]; [33]).Operationally, accessing the variations in the attentional mindset of buyers while they are in the midst of the sales negotiation process is challenging. As a first step, this research relied on the linguistic content of buyers' emails to extract their attentional mindset. While composed emails are likely to be incomplete and constrained representation of buyer attention, they have the advantage of being accurate (e.g., the buyer's own words) and time sensitive (e.g., stamped by time of sending the email). The results from Studies 1 and 2 consistently show that buyer attention fully mediates the effect of salespeople's use of influence tactics on the likelihood of sales contract award. Our insight is that the waxing and waning of buyer attention in B2B sales negotiation, visible in the signals of buyers' message content and urgency, among others, provide an early indication of sales effectiveness that is relatively robust and remarkably diagnostic of the likelihood of sales contract award. The confirming evidence of buyer attention's mediating role in the experimental study, in which alternative explanations are more tightly controlled, lends credence to our conceptual contribution that buyer attention offers a mechanism for understanding how influence tactics work. Extant work that examines buyer attention in related research domains aligns with our studies. For instance, in a study of B2B purchasing managers, [ 6] found that buyer attention fully mediates the effect of sellers' relationalism on buyer purchase behaviors, with strong empirical support for the mediated effects. They also found that the effect of seller reputation on buyer's purchase behavior was fully mediated by buyer attention. The Advantage of Concurrent Use of Complementary Influence Tactics as Textual CuesThis study contextualizes the conceptualization of [31] original social influence mechanisms for sales e-negotiations by building on and extending the literature on influence tactics in sales management. Internationalization involves internal analyzing, which primes the buyer to bear the risk of evaluating the benefits and costs of alternative options and is stimulated by use of textual cues that involve information sharing and making recommendations. Compliance involves risk shifting, which nudges the buyer to shift decisional risk to the seller by leaning toward the guaranteed outcome, is stimulated the use of textual cues that involve being assertive and making promises. Study 1's results provide compelling support for our conceptual contribution. The influence tactics constructs extracted from textual cues in salespeople's e-communications using the proposed roadmap show ( 1) evidence of convergent and discriminant validity, ( 2) a consistent factor pattern at the second-order level that confirms the presence of two (and no more than two) underlying second-order ""factors"" to indicate mechanisms (internalization and compliance), and ( 3) support for the four (and no more than four) hypothesized first-order ""factors"" to indicate influence tactics. Moreover, in accord with the posited hypotheses and as evidence of nomological validity, the results from the study 1 confirm that concurrent use of complementary influence tactics indicated by a positive interaction effect heightens buyer attention.Previous studies have shown that each influence tactic, when used individually, can be effective at times, but most have not theorized or tested the concurrent use of multiple influence tactics. This is surprising because, in practice, salespeople flexibly use multiple influence tactics by instinct. Our study fills this void and advances the field by demonstrating that gaining B2B buyer attention is more effective when different (rather than same) influence tactics are used concurrently as long as the influence tactics are complementary in prompting either internalization or risk shifting. The Disadvantage of Concurrent Use of Competing Influence Tactics as Textual CuesStudy 2 confirms the theoretical expectation that when salespeople concurrently use competing influence tactics, this results in diminished buyer attention and lower contract closing success. Few, if any, studies have examined such competing combinations. For instance, parallel work in product management has examined the effect of salespeople's efforts to combine compliance-generating (e.g., ""rationality"") and compliance-impeding (""assertiveness"") tactics on product manager compliance ([30]). This study advances Joshi's intuition about the disadvantage of stimulating inconsistent cognitions by explicitly testing the effect of using recommendation and promise tactics concurrently within a controlled experimental study. The results reveal that the disadvantage of competing influence tactics is substantial. Specifically, Study 1 data show that concurrent use of the promise tactic with low levels of recommendation is equivalent to a 92% probability of contract award; however, this probability reduces to less than 50% when the salesperson uses both promise and recommendation, all else being equal. Such concurrent use of competing influence tactics can make the difference between winning or losing contracts. Future Research DirectionsSeveral study limitations provide avenues for future research. First, we investigate negotiations that feature only a single seller over a two-year time period. Future research could add to this body of work on influence tactics by testing their effect across different stages (e.g., early, late) of the negotiation, advancing theory for competitive effects of influence tactics, and expanding the scope of the studied B2B e-negotiations in other industry contexts. Second, we develop a typology of different affordances that are unique to e-communications and draw on these attributes to develop a theory of influence tactics in B2B e-negotiations. Researchers may use the proposed typology to ground future studies of e-communications and enrich its features. Third, we propose a five-step roadmap for developing and validating theoretical constructs from textual cues; tools for analyzing unstructured text data continue to improve, promising the ability to account for paratextual cues such as amplifiers (e.g., different colored text) or accentuators (e.g., exclamation marks). We invite future contributions that enhance and enrich this roadmap to bolster the field's building blocks for theory development. Fourth, we theorize and obtain empirical support for buyer attention as a mediating mechanism in B2B e-negotiations using naturalistic data to extract theoretical constructs for testing hypotheses and an experimental design to study underlying mechanism. Finally, with growing usage of mobile as a way to communicate, future research should consider the effect of device type on sales e-negotiations ([38]). Implications for Salespeople and Sales ManagersOur results hold several important implications for salespeople and those who manage them. First, our study offers a recommendation for the sales process training. A worldwide survey of 513 firms ([15]) indicates that sales training focused on the sales process is crucial for salespeople in enhancing win rates. Our study recommends that sales organizations incorporate into their training programs guidelines for building buyer attention during sales e-negotiations. During our salesperson interviews, we learned that seasoned salespeople with proven sales performance in traditional interfaces (e.g. F2F, phone) often struggle to assess the buyer's mindset in e-communications. Our findings show that this training gap is important to fill because salespeople who are successful in increasing buyers' attention by a factor of 1 SD increase the likelihood of contract award seven-fold to yield an additional $37 million in revenue. In motivating salespeople, we recommend that sales managers specify buyer attention as a key process metric. By measuring buyer attention for each e-negotiation on an ongoing basis, the manager can establish a new performance indicator and identify skill gaps that require more directed coaching.Second, salespeople need to gain a nuanced understanding of how to leverage influence tactics during e-negotiations. Our study suggests that existing ""best practices"" in sales influence tactics are unhelpful when they attempt to simplify the sales influence process by focusing on direct effects of individual tactics such as ""tactic X will produce result Y"" or ""tactic X is better than Z to produce result Y."" The salespeople we interviewed attested to the ineffectiveness of sales influence practices in e-negotiations that work in F2F interactions. By isolating the benefits of the concurrent use of complementary influence tactics, we suggest a different path to winning contracts: sales managers should coach salespeople to deploy different combinations of complementary sales tactics and avoid any combination of competitive sales tactics. We demonstrate that concurrent deployment of complementary sales tactics within each e-negotiation slice yields significant gains in buyer attention and a reliable pathway to the contract award. For instance, the concurrent use of assertiveness and promise tactics to evoke compliance during e-negotiations boosts buyer attention by 14% on average. Likewise, the concurrent use of information sharing and recommendation tactics to evoke internalization during e-negotiations tactics results in 15% increase in buyer attention. In contrast, competitive combinations that are concurrently deployed invite losses in buyer attention (30% on average) and significantly diminish the likelihood of contract award.Finally, the validated textual cue dictionaries from this study can help design training and assessment methods to enhance selling effectiveness. Firms may have access to much larger data sets than the one used for this research. The proposed measurement approach, which incorporates ML algorithms, is developed with this industry trend in mind and is well-suited for big data. Managers can adopt our approach and library of validated words and phrases according to their own sales context. This aligns with recent trends in the sales field, in which ML is increasingly used for predictive content recommendation (e.g., what a salesperson should say in the email) as well as script optimization (e.g., how to say it) ([ 3]; [68]). We also show that ""seed"" dictionaries that are based on grounded work can offer a prediction accuracy of 63%. Furthermore, the prediction accuracy improves substantially to 85% when ""seed"" dictionaries are combined with patterns recognized by ML. According to [68], firms most poised to benefit from ML in sales are those that have ( 1) relatively high transaction volumes, ( 2) large sales forces, and ( 3) the majority of their marketing and sales activity tracked digitally. Managers in such firms can readily adopt our approach and conduct context-specific refinements to enhance sales effectiveness. Participation of professional sales staff in this process can also aid in promoting ownership and building commitment. Looking ahead, sophisticated and contextualized dictionaries of textual cues for successful e-selling can be used as tools for building and sustaining the competitive advantage of the sales organization. "
12,"Can Advertising Investments Counter the Negative Impact of Shareholder Complaints on Firm Value? Shareholder complaints put pressure on publicly listed firms, yet firms rarely directly address the actual issues raised in these complaints. The authors examine whether firms respond in an alternative way by altering advertising investments in an effort to ward off the financial damage associated with shareholder complaints. By analyzing a unique data set of shareholder complaints submitted to S&P 1500 firms between 2001 and 2016, supplemented with qualitative interviews of executives of publicly listed firms, the authors document that firms increase advertising investments following shareholder complaints and that such an advertising investment response mitigates a postcomplaint decline in firm value. Furthermore, results suggest that firms are more likely to increase advertising investments when shareholder complaints are submitted by institutional investors, pertain to nonfinancial concerns, and relate to topics that receive high media attention. The findings provide new insights on how firms address stock market adversities with advertising investments and inform managers about the effectiveness of such a response.KEYWORDS_SPLITWhile the nature and management of customer complaints is well understood in marketing literature (e.g., [42]; [56]; [74]; [76]), much less is known about the management of complaints from another key stakeholder group—the firm's shareholders. Shareholders dissatisfied with the firm's strategy can file complaints with the firm, which are then discussed at the firm's next annual general meeting (AGM). Such complaints can pertain to a broad range of perceived firm deficiencies, including poor financial performance and governance, insufficient new product introductions, incoherent strategy, or turnover in leadership ([115]). The lack of shareholder complaint research in marketing is surprising given that shareholders have emerged as a key stakeholder to the marketing function, which should be regarded ""as a customer in its own right"" ([54], p. 115), and as an ""input into marketing decision-making"" ([118], p. 2). The paucity of attention to this topic in marketing research is also surprising because shareholder complaints often directly address issues that are highly relevant to marketing executives, such as a firm's product portfolio, communications, or consumer welfare.Regardless of their content, shareholder complaints typically inflict substantial financial damage on firms because they tarnish the firm's reputation, undermine investor confidence, and impose administrative costs that divert management attention from running the business (e.g., [103]; [125]). Despite these stakes, marketing and other executives seem unprepared regarding how to respond to shareholder complaints ([25]; [115]). What we do know, based on finance, corporate governance, and management literature, is that managers typically do not respond by actually implementing the changes that shareholders request in their complaints (e.g., [46]). What we do not know is whether managers respond in alternative ways and whether such responses mitigate the complaints' financial harm on firm value. In this article, we propose that a marketing response in the form of advertising investments is such an alternative.Anecdotal evidence suggests that firms respond to shareholder complaints by adjusting their advertising investments, but the direction and magnitude of such adjustments are unclear. The chief executive officer of advertising agency WPP, for instance, notes that ""many of the world's largest consumer goods companies are slashing costs...amid pressure from [complaining] investors....And when it comes to cutting costs, advertising is one of the first places companies look to trim"" ([ 9]). By contrast, PepsiCo, ""after earlier [complaint] pressure from investors,...increased advertising and marketing spending on its biggest brands"" ([ 8]).Academic literature that documents and advises how to effectively configure advertising investments in response to shareholder complaints is missing. A review of the few studies that examine how shareholder behavior (other than shareholder complaints) affects marketing investments suggests that firms reduce marketing investments when facing challenging conditions in the stock market. For instance, this research finds that firms reduce marketing investments when their stock price falls ([13]), when they need additional equity financing ([90]), or when investor sentiment is low ([85]).Our research complements these previous studies in two ways. First, by examining shareholder complaints, we focus on a different, yet no less serious, type of stock market adversity. Second, contrary to the common view that firms cut marketing investments when challenged by the stock market, we turn to an investor perception management perspective and propose that firms have incentives to increase their advertising investments when receiving shareholder complaints. The rationale stems from prior research that recognizes that firms use advertising ""not only to promote their products and services to customers but also as a communication channel to their current and potential future investors"" ([40], p. 626).Against this backdrop, we offer a conceptual framework through which we examine three research questions. First, do firms increase advertising investments in response to shareholder complaints? We focus on advertising investments as the focal marketing response because these investments are likely to be visible to shareholders and account for the bulk of marketing budgets (e.g., [23]; [70]). Furthermore, [13] document that firms are more willing to alter advertising investments than other marketing activities in response to shareholder behavior. We argue that to offset the financial damage from shareholder complaints, firms increase advertising investments—a strategy we label ""advertising investment response.""Second, how does shareholder complaint salience moderate a firm's advertising investment response? Borrowing from stakeholder theory, we predict that complaints are more likely to induce a firm response if they are more salient by involving more power, legitimacy, or urgency ([87]). Specifically, we examine whether firms are more responsive to shareholder complaints that are submitted by institutional investors (and thus are more powerful), relate to nonfinancial concerns (and thus affect the firm's legitimacy among a larger body of stakeholders), and receive greater media attention for their underlying topics (and thus are more urgent).Third, if firms increase advertising investments in response to shareholder complaints, is such a strategy effective in protecting firm value? We focus on firm value, proxied by Tobin's q measure of intangible firm value, as our performance metric because it is a widely adopted and managerially important measure to assess the effectiveness of firm strategies in general ([78]) and advertising investments in particular ([101]). Given the established negative effect that shareholder complaints have on firm value (e.g., [103]; [125]), we examine whether an advertising investment response mitigates such a decrease.We investigate these questions using a data set covering shareholder complaints submitted to S&P 1500 firms from 2001 until 2016. Results indicate that firms indeed increase advertising investments after receiving shareholder complaints. In line with our theorizing, we also find that firms are more likely to increase advertising investments when complaints are more salient—that is, when they are submitted by institutional investors, when they pertain to nonfinancial concerns, and when media attention to their topics is high. Finally, we show that increased advertising investments mitigate the negative effects of shareholder complaints on firm value.The insights from our analyses are important from both theoretical and practitioner viewpoints. First, in response to [24] call for more empirical work on the effects of shareholder behavior on firms' marketing investments, we introduce shareholder complaints as a stock market challenge relevant to the marketing field but that has been thus far unaddressed by prior research. Second, in doing so, we offer a conceptual framework that examines the strategic role of advertising investments in responding to shareholder complaints. Relying on the investor perception management perspective and stakeholder theory, we argue that firms will take stronger action when shareholder complaint salience is high. Specifically, our study is the first to introduce, operationalize, and empirically test how shareholder complaint submitter, complaint type, and media attention about the complaint's topic influence a firm's advertising investment response. Third, our results show that advertising investments are an effective strategy for mitigating the financial damage caused by shareholder complaints. Post hoc analyses show that the majority of firms still underinvest in advertising when facing shareholder complaints, which highlights the need for corrective action by managers. The Firm Challenge of Shareholder ComplaintsA publicly listed firm is owned by shareholders who do not directly control its strategic or operational decisions but have delegated these tasks to the firm's managers. If shareholders believe that managers' decisions are detrimental, they can file formal complaints with the firm, which are registered as shareholder proposals. Once a proposal is submitted, U.S. Securities and Exchange Commission (SEC) rules require the firm to put it up for vote at the next AGM, unless the shareholder withdraws it or the SEC provides permission to exclude it from consideration.[ 5] Proposals are generally confrontational and negative in tone, but voting results are nonbinding, which means they are intended as a complaint mechanism rather than a coercion mechanism.Despite lacking legal enforcement, shareholder complaints are impactful because they publicly point out perceived shortcomings in the firm's strategy and management and signal degraded investor confidence ([27]). Two types of adverse effects emanate from shareholder complaints. First, complaints can threaten a firm's stock market performance because shareholders motivated to file complaints may also be inclined to sell their shares and thus depress the firm's stock price. This action might, in turn, persuade other current shareholders to sell their stocks and dissuade prospective investors from buying the firm's stock.Second, complaints can compromise firm value if they spill over to nonfinancial stakeholders of the firm ([26]). The implications can be severe given that nonfinancial stakeholders make consumption and employment decisions that directly affect firm performance (e.g., [78]; [119]). Such spillovers are possible given that many shareholder complaints relate to domains that affect nonfinancial stakeholders. For example, shareholder complaints can alert prospective employees about a firm's flawed strategy and governance ([121]), or customers may learn about problems with a firm's product strategy ([26]). In one public controversy, for instance, McDonald's shareholders criticized the company for ignoring childhood obesity and diet-related diseases in its product portfolio strategy ([94]). Shareholder complaints also attract significant attention from consumer advocacy groups and media outlets ([28]), especially with the rise of social media and the growing influence of user-generated content. Because complaints ""tend to be played out on the front pages of the business press,"" spillover effects onto consumers and other stakeholders become a real threat, and the resulting ""public-relations toll can be devastating"" ([115], p.11).These adversities illustrate that shareholder complaints place substantial burdens on an afflicted firm and reduce investors' expectations about the size and stability of its future cash flows. Indeed, numerous studies empirically document how shareholder complaints reduces firm value (e.g., [31]; [45]; [61]; [103]; [123]; [124]; [125]).While previous research has focused on the antecedents and outcomes of shareholder complaints (e.g., [11]; [46]; [47]), systematic research on firm response to shareholder complaints is scarce. A few existing studies find that, in general, firms do not address shareholder complaints by improving the criticized issues (e.g., [46]). However, it is not clear whether firms respond in alternative ways. We are aware of only three studies that investigate such alternative responses in firms' operational activities. [27] show that firms marginally increase research-and-development investments following shareholder complaints. [113] examines whether firms manipulate capital expenditures in response to complaints but does not find an effect. [30] report an increase in combined firm asset sales, asset spin-offs, restructuring efforts, and employee layoffs following shareholder complaints.These studies are limited in three regards relevant to this research. First, they rely on small samples of complaints (ranging from 78 to 522 complaints) that are submitted by institutional investors only. Furthermore, the types of investment responses studied are relatively inflexible and costly to manipulate because they involve installing, customizing, or disposing of assets and staff. Finally, these studies measure the change in firm operating activities for a period of up to four years after receiving the complaint ([31]; [113]) and thus capture a gradual policy change over time rather than a direct remedial firm response. However, in today's fast-paced markets where information spreads at unprecedented speeds, firms are searching for strategies that can provide immediate defense to shareholder complaints. Increased advertising investments might be one such response strategy. We next develop a conceptual framework that outlines the use and effectiveness of advertising investments as a firm response to shareholder complaints. Conceptual Framework and Hypothesis DevelopmentIn general, marketing literature recognizes that stock market considerations influence firm advertising investment decisions (e.g., [ 6]; [20]; [48]; [57]; [71]; [99]; [116]). [58] suggest that firms use advertising investments to directly manage investor perceptions (i.e., to improve investor demand). We focus on this mechanism in building our conceptual framework.[ 6]Figure 1 displays the relationships through which we study the use and effectiveness of an advertising investment response to shareholder complaints. In specifying our hypotheses, we rely on theoretical arguments and eight in-depth interviews with domain experts (i.e., executives of publicly listed firms).[ 7] As a vantage point, and building on previous research, shareholder complaints should have a negative baseline impact on firm value (e.g., [103]; [124]; [125]). We argue that to offset this drop in firm value, firms increase advertising investments (H1). Integrating stakeholder theory ([87]) with firsthand practical insights from our interviewed managers, we further identify shareholder complaint salience as a key moderator of the firm's advertising investment response (H2–H4). Finally, we theorize how an advertising investment response mitigates the negative impact of shareholder complaints on firm value (H5).Graph: Figure 1. Conceptual framework. Advertising Investment Response to Shareholder ComplaintsInvestor perception management suggests that when stock market challenges arise, firms rely on advertising investments to enhance investor confidence and secure demand for their stock (e.g., [18]; [71]). Borrowing from this logic, we argue that firms increase advertising investments when shareholder complaints threaten firm value. Theoretically, we expect firms to do so given previous literature has well established the positive effect of advertising investments on firm value (e.g., [35]; [58]). This should motivate managers to increase advertising investments as a compensatory action. Specifically, managers might expect advertising investments to offset the drop in demand for the firm's stock because the heightened attention and visibility associated with increased advertising can attract new investors to replace departing ones ([ 3]; [51]; [71]). This account was also echoed by an interviewed vice president (VP) of public relations, who remarked that ""investing in the advertising side of things, it can get the word out of who you are going to be and why you should bring new dollars in from other people..., churn through some of the investors, and get some new blood in."" Moreover, also practically, managers might regard compensatory advertising investments as a feasible response to shareholder complaints because advertising investments are quick and easy to adjust ([23]); carry only limited downside ([81]); and, compared with other marketing activities (e.g., customer relationship management, a firm's market orientation), have a more immediate effect on firm value ([13]). We therefore hypothesize the following: H1:  Shareholder complaints lead a firm to increase its advertising investments. Shareholder Complaint Salience as Moderator of an Advertising Investment ResponseThe extent to which firms increase advertising investments in response to shareholder complaints is likely to depend on shareholder complaint salience, which refers to the importance the firm attaches to the complaint ([72]).[ 8] Borrowing from stakeholder theory, we argue that shareholder complaints should be more salient to the firm, and thus more likely to induce an advertising investment response, when they involve power, legitimacy, or urgency ([26]; [87]). Shareholder complaints are more powerful when submitted by shareholders who have the ability to exercise economic punishment or influence (shareholder complaint submitter). Complaints have broader legitimacy if they also relate to nonfinancial stakeholder concerns (shareholder complaint type). Finally, complaints are more urgent if their underlying topics receive greater media attention (shareholder complaint topic media attention). We examine these three factors that may moderate the strength of an advertising investment response. Shareholder complaint submitterA common way to classify shareholders submitting complaints is to distinguish between individual investors, institutional investors, and coordinated activist investors (e.g., [46]). We propose that firms perceive complaints submitted by institutional investors as more powerful than those submitted by individual investors or coordinated activists ([27]) and thus respond more strongly through advertising investments. First, institutional investors hold a larger fraction of outstanding shares (e.g., [108]) and thereby consume more attention in managerial decision making. Second, institutional investors are more able to exert public pressure (e.g., [120]), among both shareholder communities (e.g., [46]) and the broader public. This amplifies the threat of their complaints on a firm's future expected cash flows and reinforces incentives to manage investor perceptions through advertising investments. The interviewed managers expressed a similar view. In the words of a head of investor relations, ""Institutional investors, because of the size of the actual impact, would get more attention than our retail shareholder base."" We thus hypothesize the following: H2:  A firm's advertising investment response to shareholder complaints is stronger if complaints are submitted by institutional investors. Shareholder complaint typeLiterature commonly distinguishes between financial and nonfinancial stakeholders (e.g., [63]). Adopting this view, practitioner reports (e.g., [105]), commentaries (e.g., [43]), and commercial data providers (e.g., RiskMetrics) classify shareholder complaints according to the type of stakeholder the complaints address. We follow this typology and distinguish complaints that relate to the firm's ( 1) financial and corporate governance concerns and ( 2) nonfinancial concerns that address stakeholders such as customers, employees, and the environment (see [41]; [47]). We expect that complaints of a nonfinancial type hold broader legitimacy, defined by socially constructed values and norms ([87]), and thus make an advertising investment response more likely.First, nonfinancial complaints increase the threat of negative spillovers onto various stakeholder groups (e.g., [10]; [66]), which can curtail demand, spark customer boycotts, and make it difficult to recruit talent. Because nonfinancial complaints often relate to strong normative beliefs and are comparatively easy for the public to understand, stakeholder backlash might be particularly strong and harmful to future expected cash flows (e.g., [59]), which reinforces firm incentives to engage in an advertising investment response. Second, nonfinancial complaints may raise doubts about broader strategic issues than complaints about financial or governance aspects because nonfinancial complaints tend to directly relate to the firm's business model, values, and operations. This further amplifies the perceived harm of shareholder complaints on a firm's future expected cash flows and reinforces incentives to directly manage investor perceptions through increasing advertising investments. In the words of an interviewed chief financial officer (CFO), ""When you are dealing with nonfinancial complaints, it is logical to invest more in advertising and communication, because the downside risk is larger."" We hypothesize: H3:  A firm's advertising investment response to shareholder complaints is stronger if complaints reflect nonfinancial concerns. Shareholder complaint topic media attentionMedia attention of shareholder complaint topics reflects the extent to which the public is currently interested in the underlying issues of the complaint ([ 5]). We expect that complaints about topics that receive greater media attention make an advertising investment response more likely because managers perceive a greater urgency to react. First, if an issue is of high concern, stakeholders have a stronger motivation to act on it and eschew the firm (e.g., [36]; [122]). Managers might therefore be more alarmed that shareholder complaints translate into real economic costs and be more likely to consider an advertising investment response as defense. Second, managers make decisions on the basis of the issues they and their firm's stakeholders focus attention on ([34]; [86]; [98]), and media coverage of a shareholder complaint topic likely draws their attention. Accordingly, such complaints are considered more critical, and managers are more likely to act through an advertising investment response. In line with this expectation, an interviewed head of investor relations stated, ""I would be more likely to respond to issues that are in the press lately."" Accordingly, we hypothesize: H4:  A firm's advertising investment response to shareholder complaints is stronger if complaint topics receive greater media attention. Effectiveness of an Advertising Investment Response in Mitigating Firm Value DeclineWe complete our conceptual framework by theorizing that an advertising investment response can actually mitigate the negative impact of shareholder complaints on firm value. The underlying mechanism is two-fold. First, investing in advertising signals firm financial health, product market prospects, and an otherwise sound strategy ([32]; [65]; [67]), which should render investors less sensitive to devaluing the firm following shareholder complaints. As an interviewed VP of public relations remarked, ""It just puts a positive spin on the company itself."" In addition, the positive affect induced through advertising might make investors more forgiving about performance deficiencies related to shareholder complaints. This is because investors, like other stakeholders, react less strongly to adverse news when emotionally connected with and positively inclined toward a firm ([84]). An interviewed VP of public relations explicitly mentioned that advertising can weaken the negative valence of shareholder complaints, as ""Putting more money into advertising...is a way to get shareholders to look at whatever is the other shiny object and distract their attention away from the complaint.""Second, advertising investments might also lessen the negative spillover effects of shareholder complaints on nonfinancial stakeholders of the firm. As such, the positive attitudes and emotional connection that advertising fosters among these stakeholders might alleviate repercussions in product and labor markets and weaken the resulting drop in firm value. As exemplified by the statement of an interviewed CFO, ""Advertising could address concerns by some people who think we are not a good company by letting them and the broader market know we are."" For these reasons, we formally hypothesize: H5:  The negative effect of shareholder complaints on firm value is mitigated by a firm's advertising investment response. Method Data Sources and SampleWe assemble a data set consisting of detailed information on all shareholder complaints received by S&P 1500 firms, advertising investments, firm value, shareholder complaint media attention, and a set of control variables from 2001 until 2016. We collect the first part of the data set from RiskMetrics, formerly the Investor Responsibility Research Center. This database covers complete annual shareholder complaint data for S&P 1500 firms, comprising 15,727 complaints. The database is unique in that it also includes data on withdrawn and omitted complaints submitted to firms but not put up for vote at the AGM. Of the 15,727 submitted complaints, 6,995 (i.e., 44.48%) are omitted or withdrawn. However, because including only complaints put up for vote at the AGM might not accurately reflect the true extent of shareholder dissatisfaction, we also incorporate these omitted or withdrawn complaints and thus circumvent the selection bias in other studies, which only use the complaints shareholders eventually voted on ([60]). Moreover, our interviews suggest that managers consider shareholder complaints irrespective of whether they are eventually withdrawn. In the words of a VP of public relations, ""Once the complaint is out, you have to respond....You have to think that there are going to be other investors who are thinking the same thing..., even if [the complaints] were withdrawn.""Because RiskMetrics covers only firm-year observations in which shareholder complaints are observed, we add a control group of firms that were part of the S&P 1500 for at least one year during the 2001–2016 period and were tracked by Compustat but do not appear in the RiskMetrics database because they did not receive any shareholder complaints. Using the Compustat Execucomp database, we identify a total of 411 possible control firms. In unreported analyses, we confirm that across all firm-year observations, neither the firms receiving shareholder complaints nor the control firms significantly differ from the universe of firms covered in Compustat in terms of the control variables we outline in the next section.We retrieve advertising investment data from Kantar Media, a leading source of advertising data. Kantar Media continuously tracks brand advertising activity across broadcast, print, radio, internet, and outdoor media channels and translates this information to monetary amounts by surveying media and agency rates. We note that these data cover only media advertising and no other aspects of advertising (e.g., production costs, promotional material, agency costs).Information on media attention to shareholder complaint topics comes from LexisNexis, and data on firm value and control variables come from Compustat's quarterly database and the Center for Research in Security Prices. Consistent with previous work (e.g., [ 4]), and to correctly match calendar-year advertising investment data from Kantar Media with fiscal-year financial data from Compustat, we retain only firms whose fiscal years end in December (68% of the firms). Doing so is important to ensure that the term ""year"" is the same for all firms in our sample, and that all firms are subject to the same industry conditions. The final sample consists of unbalanced annual panel data for 831 firms (656 firms receiving shareholder complaints and 175 control firms). After eliminating firm-years with incomplete information, the final sample is based on 3,896 submitted shareholder complaints and includes 4,428 firm-year observations.[ 9] The sampled firms belong to a wide range of industries including services as well as manufacturing. Electronics and computer (31%) constitute a large proportion of the sample, as do business services (13%) and chemicals (12%), followed by food (6%). Web Appendix C overviews the industry descriptives. Measures Shareholder complaintsWe aggregate shareholder complaints at the firm-year level and take the natural logarithm because the distribution of complaints is highly skewed (e.g., [100]). To avoid losing firm-year observations with zero complaints, we follow [79] procedure and add 1 to the actual value before the logarithmic transformation.[10] We use the lagged level of shareholder complaints because shareholders submit their complaints toward the end of the fiscal year. To explain, SEC regulations require shareholders to submit complaints 120 calendar days before AGM materials are mailed to shareholders. For firms whose fiscal year ends on December 31, the AGM takes place in April and firms send the AGM materials to shareholders 30 to 50 days beforehand. Thus, for such firms, shareholder complaints are received by the end of November of the previous year and they become public anytime between January and March. Specifically, complaints discussed at the AGM become public through shareholders announcing their complaints or when firms send out preinformation about opposing the complaints, both of which tend to occur in January or February ([38]). If complaints are withdrawn or omitted, they become public when the SEC publishes this information, usually anytime from January until the AGM takes place ([109]). As a result, firms learn about shareholder complaints in the fourth quarter of the previous year, and other shareholders and the public learn about shareholder complaints in the first quarter of the current year. Advertising investmentsOur measure of advertising investments is Kantar Media's annual advertising investments estimate, which we aggregate across a firm's brands to arrive at a firm-year level. To control for firm size effects, we scale advertising investments by the firm's total assets in the given year, collected from Compustat.[11] If a firm does not advertise in any of the tracked channels, we assign it an advertising investment value of zero. Following prior literature, we use the unexpected portion of advertising as our measure (e.g., [70]; [96]), which is the deviation from expected advertising levels in a given year based on the firm's prior year's advertising level. This deviation is captured by the residuals from the following autoregressive fixed-effects model of advertising investments ([88]): ADVi,t=ϕ0+ϕ1ADVi,t−1+γSICj+τYRt+αi+∊i,t, Graph1where i = 1,..., I indicates firms, j = 1,..., J indicates two-digit Standard Industrial Classification (SIC) industries, and t = 1,..., T indicates years. ADVi,t measures advertising investments scaled by total assets, with ϕ1 modeling the autoregressive behavior and ∊i,t representing an error term ∼N(0, σ). For reasons of parsimony, and following [88], we employ a first-order autoregressive (AR[ 1]) specification. To account for the correlation in the dependent variable over time, we use an instrumental variable first-difference estimator to generate consistent parameter estimates (see [88]).[12] Our modeling acknowledges that the past advertising investment level serves as an anchor point in setting the advertising budget in the current period (e.g., [53]; [102]). It allows for a one-period transitory effect of advertising investment deviations such that advertising investments can return to prior normal levels if no shareholder complaints are received in consecutive years (note that in 89% of our firm-year observations, firms do not receive shareholder complaints in consecutive years and can therefore reset their advertising investments to previous normal levels). The residuals from Equation 1 represent unexpected size-adjusted advertising investments, UADVi,t. For ease of readability, from here on, we omit the descriptors ""unexpected"" and ""size-adjusted"" and simply refer to ""advertising investments."" Firm valueWe measure firm value by Tobin's q, an established proxy for intangible firm value in the marketing literature that provides ""market-based views of investor expectations of the firm's future profit potential"" ([106], p. 129) and is considered the superior measure for assessing marketing strategy effectiveness ([29]; [39]). Tobin's q is the ratio of a firm's market value to the replacement cost of its tangible assets and provides a measure of the premium (or discount) that the market is willing to pay above (below) the replacement costs of a firm's tangible assets, thus capturing any above-normal returns expected from the firm's tangible assets ([ 1]). It has several advantages as a performance metric because it is derived from the stock price and thus is forward-looking, risk-adjusted, and less easily manipulated by managers. Moreover, Tobin's q reflects a firm's long-term profitability because it captures the relationship between the replacement cost of a firm's tangible assets and the market value of the firm ([ 7]). We follow [19] and calculate Tobin's q as the logarithm of TQi,t = (MVEi,t + PSi,t + BVDi,t)/TAi,t, where MVEi,t is the market value of equity, PSi,t is the value of preferred stock, BVDi,t is the book value of debt, and TAi,t is the book value of total assets for firm i in year t. Tobin's q captures intangible firm value, yet for ease of readability, we use the terms ""intangible firm value"" and ""firm value"" interchangeably. Shareholder complaint salience factorsWe operationalize the shareholder complaint institutional submitter variable as the firm's yearly percentage of shareholder complaints submitted by institutional investors (i.e., public pension funds, mutual funds, endowments, and foundations) versus complaints submitted by individual or coordinated investors. To operationalize the shareholder complaint nonfinancial type variable, we use the following procedure to distinguish shareholder complaints that address nonfinancial concerns from those that address financial concerns. RiskMetrics classifies each complaint as relating to either financial/governance or social responsibility concerns. For our measure of nonfinancial concerns, we use the complaints classified as social responsibility concerns. To verify that all corporate social responsibility concerns indeed relate to nonfinancial concerns, we pull the detailed complaint descriptions from RiskMetrics and review each complaint. Two independent coders confirm that all complaints (i.e., κ = 1.00) unambiguously relate to nonfinancial concerns. We then create a variable indicating the firm's yearly percentage of complaints that were defined as relating to nonfinancial concerns.We operationalize the shareholder complaint topic media attention variable as the yearly media coverage about the topic addressed in a given complaint as captured on LexisNexis. RiskMetrics summarizes the topic of each complaint in a short description nine words or fewer (91% of the summaries have fewer than six words), which we clean of filler words and use as index terms in our media search. We count all articles that mention the respective topic within a given year and that LexisNexis ranks with a relevancy score of at least 60%, net of duplicates in the same newspaper outlet ([69]). To control for scale effects of the topics, we normalize topic media coverage in a given year by the topic's total media coverage over the sample period. In years in which a firm receives multiple shareholder complaints, we compute the arithmetic mean across the topic media coverage scores to arrive at our firm-year measure of shareholder complaint topic media attention. Web Appendix E provides further details and examples of how we construct the shareholder complaint salience moderators. Control variablesOn the basis of a systematic review of previous literature, we include control variables organized across financial flexibility, product market performance, stock market performance, and shareholder complaint details. In line with this literature, we include these covariates as levels, not as unexpected changes, and winsorize them at 1% to reduce the impact of outliers. For our model estimating advertising investments, our controls of financial flexibility include the logarithm of operating cash flows because firms alter their advertising levels due to financial constraints and affordability effects in the product market ([13]). As suggested by prior literature ([13]; [78]), we also control for financial leverage. On the one hand, financial leverage may reduce the firm's flexibility in terms of resource allocation and spending behavior ([55]). On the other hand, firms profit from higher financial leverage through tax benefits related to deductible interest payments, which can result in higher spending levels. To calculate firm leverage, we divide long-term debt by the book value of assets ([107]).We include sales growth as a control for product market performance, measured as the percentage change in gross sales from the preceding year ([20]). Controlling for firm stock market performance, we include stock returns ([81]). Because firms tend to be benchmarked against their industry peers for stock performance comparisons, we follow [14] and use a dummy variable that takes a value of 1 if the firm's stock return exceeds industry-averaged stock returns and 0 otherwise. Industry average returns are the equally weighted average of stock returns of all firms in a given four-digit SIC industry. We add a control for firm size to account for economies of scale ([20]), which we operationalize as the logarithm of total assets. We employ one-period lags of the control variables because this represents the most recent information available to managers when they decide on budgets at the beginning of the current period ([14]; [24]). Finally, we include controls for shareholder complaint details that are not captured by our moderating factors but could influence an advertising investment response. Specifically, we control for the percentage of excluded shareholder complaints not discussed at the firm's AGM and the percentage of shareholder complaints with voting support exceeding 50%.For our model estimating the effectiveness of an advertising investment response, we control for a firm's financial flexibility by including financial leverage, measured as described previously. Because theoretical arguments and empirical evidence are strong but equivocal for the relationship between financial leverage and Tobin's q, we include it as a covariate without specifying the expected sign of the relationship ([78]). We include market share as a control for product market performance, which has been shown to positively influence Tobin's q ([39]). Market share is expressed as the fraction of firm sales revenues divided by sales revenues of all firms in the same four-digit SIC industry. We also include sales growth because a higher growth rate might indicate higher future growth prospects that result in higher values of Tobin's q ([106]). Our controls also include profitability, which is expected to have a positive effect on Tobin's q ([49]). We use return on assets (ROA) as our profitability measure, which is the ratio of operating income before depreciation by total assets. Regarding a firm's economies of scale, we control for firm size and its established negative association with Tobin's q ([ 7]), which we calculate as described previously. Finally, we control for shareholder complaint details including the percentage of excluded shareholder complaints not discussed at the firm's AGM, the percentage of shareholder complaints with voting support exceeding 50%, shareholder complaint institutional submitter, shareholder complaint nonfinancial type, and shareholder complaint topic media attention, all operationalized as discussed previously. Table 1 provides an overview of all variables used in the analyses.GraphTable 1. Operationalization and Data Sources of Variables.  1 aAll variables operationalized at firm-year level.2 bConceptually analogous application outside marketing. Modeling and Estimation ApproachDrawing on our conceptual framework, we formulate the following system of equations that details the relationships between shareholder complaints, advertising investments, and Tobin's q, as well as the set of relevant control variables: UADVi,t=β10+β11SHCi,t−1+β12INSTi,t−1+β13SHCi,t−1×INSTi,t−1+β14NONFINi,t−1+β15SHCi,t−1×NONFINi,t−1+β16MEDIAi,t−1+β17SHCi,t−1 ×MEDIAi,t−1+π1K1i,t−1+τ1tYRt+αi+∊1i,t, Graph2 TQi,t=β20+β21SHCi,t−1+β22UADVi,t+β23SHCi,t−1×UADVi,t+π2K2i,t+τ2tYRt+αi+∊2i,t, Graph3where i = 1,..., I indicates firms and t = 1,..., T indicates years. UADVi,t is advertising investments, TQi,t is Tobin's q, SHCi,t is the number of shareholder complaints, INSTi,t is the proportion of shareholder complaints submitted by institutional investors (shareholder complaint institutional submitter), NONFINi,t is the proportion of shareholder complaints related to nonfinancial concerns (shareholder complaint nonfinancial type), and MEDIAi,t is the media attention of the topic of the shareholder complaint (shareholder complaint topic media attention). K1i,t is the vector of firm controls relevant to explain advertising investments, which includes cash flows, financial leverage, sales growth, stock returns, firm size, excluded complaints, and complaint voting support. K2i,t is the vector of firm controls relevant to explain Tobin's q, which includes financial leverage, market share, sales growth, profitability, firm size, excluded shareholder complaints, complaint voting support, complaint institutional submitter, complaint nonfinancial type, and complaint topic media attention. YRt is a vector of year controls to account for time effects, αi are firm-specific intercepts to account for unobserved firm-level heterogeneity, and r1i,t and s2i,t are error terms ∼N(0, σ). The parameter estimate β11 indicates whether firms increase advertising investments in response to shareholder complaints (H1); the respective estimates of β13, β15, and β17 show whether shareholder complaint salience factors moderate the advertising investment response (H2–H4); and the estimate of β23 indicates whether advertising investments are effective in weakening the postcomplaint drop in firm value (H5).Given that Equations 2 and 3 are embedded in a system of equations, we face the modeling challenge of the right-hand side advertising investment variable being potentially endogenous. Endogeneity could also stem from autocorrelated errors of the lagged endogenous variable. Theoretically, we do not expect autocorrelation to be a problem for the advertising investment equation given that this measure is already an unexpected measure. We formally test for autocorrelation for panel data with the procedure recommended by [127] and fail to reject the null hypothesis of uncorrelated errors for any of the two equations. As a result, we can generate consistent estimates by ordinary least squares regressions in which standard errors are clustered by firm ([91]). ResultsTable 2 provides sample descriptive statistics and correlations of the variables in our models. As anticipated, unexpected advertising investments are, on average, zero. In years in which firms receive shareholder complaints, we observe a mean of 2.60 complaints per year, with a minimum of 0 and a maximum of 22 complaints. Figure 2 provides information about the distribution of shareholder complaints in our sample. Panel A plots the number of shareholder complaints across all firms by year. Panel B shows the number of complaints by firm-year for years in which shareholder complaints are received. Panel C plots the number of complaints by more detailed subcategories of complaint type (within financial and nonfinancial type) and by complaint submitter (Web Appendix E overviews our procedure to subcategorize shareholder complaint type).GraphTable 2. Descriptive Statistics and Correlations.  3 aCorrelations based on variables' log transformations as used in the analyses, all in period t.4 bMean and standard deviation in variable's original unit.5 cMean and standard deviation conditional on firm-year observations with nonzero shareholder complaints.Graph: Figure 2. Shareholder complaint distributions.aShareholder complaint financial typebShareholder complaint nonfinancial type. Advertising Investment Response to Shareholder ComplaintsH1 predicts that firms respond to shareholder complaints by increasing their advertising investments. Table 3 presents regression results of the associated test. Column 1 contains the results of Equation 2 with only the control variables. We focus on Column 2, in which we add shareholder complaints and thereby explain additional variance. The proposed model is statistically significant (F = 31.85, p <.01) and the variance inflation factors (VIFs) do not exceed ten, suggesting that multicollinearity is not a concern for the validity of our results. We further perform stepwise analyses, in which we add one regressor at a time to the model and confirm that multicollinearity does not affect the results. We find that the main effect of shareholder complaints on advertising investments is positive and significant (β =.055, p <.01), implying that receiving more shareholder complaints in the preceding period relates to an increase in advertising investments in the current period. This result aligns well with the theory of investor perception management and supports H1. In economic terms, we find that for a firm with one shareholder complaint, receiving an additional complaint (i.e., the number of complaints doubles to two) unexpectedly increases its size-adjusted advertising investments by.004 (= ln( 2) ×.055 × 10−1; see coefficient in Table 3). For an average firm in our sample, with average total assets of $2,021.8 million (see Table 2), this translates into an increase in advertising investments of $8.08 million (=.004 × $2,021.8 million), holding assets fixed.GraphTable 3. Advertising Investment Response.  6 *p <.1.7 **p <.05.8 ***p <.01.9 Notes: Coefficients scaled by 10−1 to improve readability. Standard errors clustered by firm in parentheses. Predictors calculated at period t − 1. Firm and year fixed effects not reported. Shareholder Complaint Salience as Moderator of an Advertising Investment ResponseTurning to the moderating effect of shareholder complaint salience, we present regression results of Equation 2 in Column 3 of Table 3. The model is statistically significant (F = 27.46, p <.01), with VIFs below ten, and the covariate estimates are similar to those of Column 2. To test H2–H4, we focus on the interaction effects between shareholder complaints and the respective salience moderator variable (i.e., institutional submitter, nonfinancial type, and topic media attention).[13] Regarding shareholder complaint submitter, we find that institutional investors increase the strength of an advertising investment response (β =.015, p <.01), consistent with H2. With respect to shareholder complaint type, results support H3. Firms are more likely to increase their advertising investments if complaints relate to nonfinancial concerns (β =.013, p <.01). Finally, and consistent with H4, we find that media attention of the topic of the shareholder complaint strengthens an advertising investment response (β =.016, p <.01). In further confirmation of these results, the total effects (i.e., the sum of the simple effects of shareholder complaints, the respective moderator, and the interaction effect of these two variables) are positive for all three salience moderators. Effectiveness of an Advertising Investment Response in Mitigating Firm Value DeclineThe test of H5 involves examining whether an advertising investment response can mitigate the drop in firm value resulting from shareholder complaints. We estimate Equation 3 including only shareholder complaints and control variables, as shown in Column 1 of Table 4, and advertising investments and control variables, as shown in Column 2. Our focus, however, is on the full model in Column 3. The model is statistically significant (F = 25.48, p <.01) and explains incremental variance. Because the VIFs are below ten, this confirms that multicollinearity is of no concern for our analyses. We find that the control variables have the expected signs, although financial leverage is not significant. Confirming prior research, shareholder complaints are negatively related to firm value (β = −.212, p <.01), which highlights the need for remedial action on receiving them. Moreover, our results suggest that increasing advertising investments is an effective firm response that successfully mitigates the postcomplaint firm value decline (β =.003, p <.05), as predicted in H5 and consistent with the theory of investor perception management. We also reestimate Equation 3 without profitability as a control variable. This specification is insightful given that Equation 3 tests for investor perception effects beyond earnings levels (as proxied by the ROA measure of profitability). The results in Column 4 confirm that the stock market still rewards an advertising investment response (β =.002. p <.10).GraphTable 4. Effectiveness of an Advertising Investment Response.  10 *p <.1.11 **p <.05.12 ***p <.01.13 Notes: Standard errors clustered by firm in parentheses. Shareholder complaints calculated at period t − 1. Firm and year fixed effects not reported. Additional AnalysesThis section overviews additional analyses that provide further insights into the mechanisms underlying an advertising investment response to shareholder complaints. We present alternative modeling choices and various robustness checks in Web Appendix D. Shareholder Complaint Salience Moderating effect of detailed subcategories of nonfinancial shareholder complaintsIn an exploratory inquiry, we reestimate Equation 2 and replace the shareholder complaint type variable with the nine subcategories of complaint type, as plotted in Panel C of Figure 2 (results reported in Web Appendix E). We note that, after mean-centering the subcategory variables, VIFs remain above ten, and the adjusted R-squared is lower than when using the dichotomous shareholder complaint type classification as produced in Column 3 of Table 3.[14] In line with our theorizing, however, we find that all financial subcategories (board of directors, executive compensation, takeover, and other shareholder rights) negatively moderate an advertising investment response, whereas nonfinancial subcategories (diversity, community, environment, human rights, product, and vices) positively moderate an advertising investment response, although not all effects are statistically significant. These results further support our theory-driven taxonomy of financial and nonfinancial shareholder complaint types. Moderating effect of shareholder complaint salience on firm value declineWhile not the focus of our analyses, in a set of unreported analyses we also test whether shareholder complaint salience moderates the negative effect of shareholder complaints on Tobin's q. We do not find significant moderating effects for shareholder complaint institutional submitter or shareholder complaint nonfinancial type, but we do find that complaints related to topics receiving more media attention cause a marginally larger drop in firm value than those about topics receiving less media attention. These findings are interesting because they suggest that managers might overestimate the perceived harm that shareholder complaints have on firm value when submitted by an institutional investor or when reflecting nonfinancial concerns. We also use a split-sample design (i.e., above and below median advertising investments) to examine whether the effect of an advertising investment response on firm value is moderated by shareholder complaint salience but find no such evidence for any of the salience factors. Alternative Investment Responses to Shareholder ComplaintsIf firms increase their advertising investments in response to shareholder complaints, does this come at the expense of other investments? Although we lack data for most other types of firm investments, we offer two exploratory analyses. First, following [64], we derive an estimate for sales force investments by subtracting Compustat's advertising item from Compustat's selling, general, and administrative item. Using this proxy, we do not find that shareholder complaints drive sales force investments (β =.070, n.s.). Second, moving beyond the marketing domain, we test whether firms reduce capital expenditures, as reported in Compustat, after encountering shareholder complaints. Capital expenditures decrease current earnings but do not help in managing investor perceptions. If our argumentation holds, we should observe a negative (or at least no positive) effect, which is indeed what we find (β = −.040, n.s.). Advertising Investment Response Mechanism Explicit requests for advertising investment increasesIt might be that advertising investments appear effective as a remedial response because, in their complaints, shareholders explicitly ask for an increase in advertising. To test this competing explanation, we examine the detailed content of each shareholder complaint that includes the words ""advertising,"" ""marketing,"" or ""promotion,"" but find no evidence for any demands directly related to an advertising increase (N = 0). Because SEC regulations allow companies to ignore complaints related to ordinary business operations, such as advertising, the lack of complaints requesting advertising increases is not surprising. However, to the extent that an advertising-related complaint concerns fundamental business strategy, it is not considered ordinary business and is eligible for submission. We find only a few complaints of that type in our data set (N = 56). Examples are complaints about advertising tobacco to minors and about advertising containing ethically controversial images. Our previous results do not change if we either exclude these cases or control for them with a dummy variable. Advertising investment media typesAdvertising investments comprise various media types that differ in audience, objectives, and effects. We examine whether some advertising media types are more effective in protecting postcomplaint firm value than others. Kantar Media provides detailed advertising investment data grouped into print, radio, broadcast, internet, and outdoor advertising. Adding interaction terms of these five media types and shareholder complaints to Equation 3, we find marginally significant effects on Tobin's q for the percentage of television (β =.001, p <.10) and outdoor (β =.001, p <.10) advertising. We conclude that the media type of advertising is of lower importance in protecting postcomplaint firm value; the effectiveness rather lies in the total amount of a firm's advertising investments. Nomological Validity of Industry ConditionsBecause industry conditions can influence a firm's likelihood to manage discretionary investments such as advertising (e.g., [17]), we provide two tests that provide additional insights and allow us to reconcile our findings with prior literature. First, industries differ in the importance that investors attach to a firm's earnings numbers, and previous literature has suggested that such industry earnings pressure influences a firm's discretionary investment levels ([ 2]). Consequently, we expect firms operating in industries in which stock prices are highly sensitive to earnings information to be more cautious about increasing advertising investments. We proxy industry return sensitivity to earnings by the coefficient of a regression of earnings against firm stock returns ([ 2]). Interacting industry return sensitivity to earnings with shareholder complaints, analogous to the interactions in Equation 2, we find that higher return sensitivity to earnings is associated with a marginally lower increase in advertising investments (β = −.001, p <.10).Second, we argue that visibility and potential spillovers onto nonfinancial stakeholders explain why firms resort to advertising investments when facing shareholder complaints. Thus, firms should be more likely to increase advertising investments if visibility among a broader set of stakeholders is large and if the overlap between the product market and the stock market is high. Previous literature has argued that market structures vary in these regards across business-to-customer (B2C) and business-to-business (B2B) markets ([68]), and we therefore test whether firms are more likely to increase their advertising investments when operating in a B2C versus a B2B industry. Including B2C industry membership as an interacted dummy with shareholder complaints, while excluding the firm fixed effects in Equation 2, we find marginal support for this argument (β =.001, p <.10). DiscussionShareholders and their concerns have recently attracted marketing's attention as firms are increasingly challenged to manage stock markets and product markets symbiotically. In this article, we focus on shareholder complaints, a prominent form of stock market adversity, and study how firms use advertising investments to mitigate the drop in firm value following shareholder complaints. Three key takeaways are as follows: First, contrary to the often-documented observation that firms decrease marketing investments when challenged by the stock market, we find that, in the context of shareholder complaints, firms increase advertising investments to improve investor perceptions. Second, firms are more likely to increase advertising investments if shareholder complaints are more salient (i.e., are submitted by institutional investors, relate to nonfinancial concerns, or if the topics of the complaints receive more media attention). Third, advertising investments are an effective instrument for mitigating the firm value decline following shareholder complaints. Our results have various implications for marketing theory and practice, which we discuss next. Theoretical Implications: Dynamics of Stock Markets and Product MarketsWhile a substantial amount of research has explored how marketing investments drive shareholder behavior (e.g., [107]; [116]), a small number of studies have begun to investigate whether shareholder behavior can also have feedback effects on marketing investments. This research tends to focus on the impact of stock prices. Our study sheds light on a distinctive, yet hitherto overlooked, aspect of shareholder behavior—shareholder complaints—that is increasingly relevant for firm managers and, as we show, for their advertising investment decisions. Combining our results with the findings of other stock market feedback studies (e.g., [ 6]; [88]) leads to the conclusion that adverse stock market pressures do not always result in marketing investment cuts, as per an earnings management perspective. Instead, in support of an investor perception management perspective, firms might find it more effective to directly manage firm value through increasing advertising investments.We expect two drivers to explain this phenomenon. First, shareholder complaints often pertain to nonfinancial issues, so soothing investors financially by cutting advertising investments is unlikely to be effective because these complaints are not financially motivated in the first place. Considering that the general quest of maximizing profits is often at odds with consumer welfare and corporate social responsibility (e.g., [66]), cutting advertising investments as a response to nonfinancial concerns may even be considered counterproductive by investors.Second, firms that encounter shareholder complaints tend to display mediocre to bad earnings performance, either because of poor past strategic choices that caused complaints or because of processing complaints in the current period.[15] Finance literature suggests that if firms miss earnings expectations, even if only by a thin margin, they bank any additional earnings until a later period when they can use those earnings to meet or exceed expectations (e.g., [30]; [111]). Likewise, this literature finds that incentives to manage earnings are higher for firms that have consecutively reported high or increasing earnings in the recent past, and not for those that have reported poor earnings ([95]). This point is also discussed in [13], who argue that it is good past performance that induces short-term earnings pressures. Given the poor past performance of firms receiving shareholder complaints, these firms might have lower motivation to engage in earnings management.As such, our findings also pertain to the recent discourse on profit versus value maximization. Multiple articles in this area argue that marketing actions that increase profits may not necessarily increase firm value (e.g., [118]). We add to this discussion by showing that marketing actions that decrease profits may not always decrease firm value. In this regard, our results correspond closely with those of [44], who study product-harm crises and find that although increasing advertising investments reduces firm profit, it nevertheless mitigates the loss in firm value. In other words, reductions in accounting profit do not always lead to reductions in economic profit, not even in the short run. This is an encouraging finding for marketing practice.Our findings also add to the literature that studies how firms can attenuate crisis situations through marketing investments. The traditional focus of the marketing function has been the product market, with an array of research studying how firms employ advertising instruments to address adversities emanating from product markets, such as consumer complaints (e.g., [73]), product regulation (e.g., [92]), or product-harm crises (e.g., [22]). Other research shows how advertising investments help firms during recessions (e.g., [117]). Adding to this literature, we show that advertising investments not only are a compelling way to address product-market crises but also can mitigate the adverse effects of shareholder crises.In terms of adverse performance effects, our focus is on firm value, which is a function of both future expected cash flows and firm risk. Given the strong risk-reducing properties of advertising investments (e.g., [75]; [83]), coinciding with the weakened investor confidence in the firm when receiving shareholder complaints, the channel through which advertising investments protect firm value might be by reducing firm risk. To test for this benefit, we run an exploratory test in which we interact shareholder complaints with advertising investments and regress this variable on firm risk, measured as the standard deviation of the residuals of the firm's four-factor stock price model in year t ([13]). The model mimics Equation 3 but replaces Tobin's q with firm risk. Results are of the expected direction, but advertising investments are marginally insignificant in reducing firm risk after encountering shareholder complaints (β = −.043, n.s.).Finally, our results contribute to a better understanding of the corporate governance literature relative to marketing actions. Previous literature, as well as our data, has shown that firms rarely respond directly to the issues raised in shareholder complaints (e.g., [46]). Presumably, managers regard their own information about the firm to be superior to that held by shareholders ([114]) or fear that shareholders seek only short-term gains. This perspective is shared by scholars and commentators who claim that shareholder complaints only distract management from their duties and that outside investors lack the necessary skills and experience to optimize management's decisions ([112]; [126]). Our qualitative interviews support this idea, with one of the interviewed CFOs stating that ""shareholders follow your firm from a distance and are not involved in the daily development of your strategy and the considerations involved."" In addition, whereas some complaints reflect shareholders' discontent with an issue in which they are highly involved, other complaints result from a more general ""we are not happy"" sentiment ([104]), making it difficult for management to cure shareholder dissatisfaction by taking direct action. We do not take a stance on whether shareholder complaints are reasonable or whether firms' refusal to implement proposed changes is warranted. However, we show that firms do react to these complaints, namely by altering their advertising investments, and we show that this response is effective in terms of protecting firm value. This is another reassuring finding for marketing practice. Managerial Implications: Strategic Action Plan for Firms Facing Shareholder ComplaintsIn light of our findings from both the qualitative interviews and econometric analyses, we suggest a three-step action plan for managers facing shareholder complaints. When confronted with complaints, managers should ( 1) assess the content of the complaint and determine whether to implement the shareholder's suggestion, ( 2) evaluate the impact the complaint has on firm value and inherent firm responsiveness, and ( 3) devise actions to alleviate the potential harm of the complaint to firm value. We next provide details on each step and offer actionable guidelines.The first step involves managers closely analyzing the actual content of received complaints and assessing whether they should respond by changing any aspect of their business operations. For example, this could be the case when shareholders point to shortcomings in the firm's operations or supply chain, which managers should acknowledge as market intelligence and change accordingly. Although previous literature has shown that firms typically neglect such insights, we suggest that managers should always be vigilant to maximize their firm's strategic potential and remain attentive to shareholder feedback in accomplishing this objective. In the words of a CFO we interviewed, ""You have to engage in a dialogue with shareholders who suggest that your value proposition is not optimal,"" and ""Shareholders have the right to indicate this to you.""Second, irrespective of whether the firm implements the issue shareholders complain about, managers should next evaluate the impact of the complaint and consider actions to alleviate the harm that complaints inflict on firm value. Our findings corroborate that shareholder complaints substantially reduce firm value but also identify advertising investments as an effective tool to protect firm value. To further demonstrate the impact of increasing advertising investments in response to shareholder complaints, we perform a counterfactual analysis that answers two questions. First, for firms that increased their advertising investments in response to shareholder complaints, what was their incremental improvement in firm value compared with if they had not increased investments? Second, for firms that did not increase their advertising investments in response to shareholder complaints, what was their incremental loss compared with if they had increased investments? For this analysis, we employ a switching regression methodology (e.g., [16]), detailed in Web Appendix F. In essence, we use a first-stage regression to predict the probability of a firm increasing its advertising investments in response to shareholder complaints and derive the inverse Mills ratio as the selection correction variable. In a second stage, we then regress firm value on the inverse Mills ratio and the control variables, separately for firms that increase advertising investments and those that do not. Finally, we use the predicted firm values from the second-stage estimation to conduct the what-if analysis.We find that firms that increased their advertising investments in response to shareholder complaints achieved an incremental gain of.057, or 3.5%, in firm value compared with if they had not increased these investments. By contrast, firms that did not increase their advertising investments in response to shareholder complaints gave up an incremental lift in firm value of.152, or 1.3%, compared with if they had increased their investments. These results underscore our general finding that advertising investments help protect firm value in the face of shareholder complaints, but, perhaps even more importantly, they also show how a firm jeopardizes firm value if it decides against an advertising response.Although these results underscore the benefits of an advertising investment response to shareholder complaints in general, they do not indicate the optimal level of increasing advertising investments. As a second analysis, we therefore perform a marginal effects analysis based on the estimated coefficients from Equation 3 in Table 4 (Web Appendix G overviews the details). This approach has been applied in marketing by [80] and [117] and provides two important insights. First, it describes how to calculate firm-specific marginal effects and how to statistically assess whether a given firm underspends, overspends, or spends at an approximately optimal level. As such, when faced with shareholder complaints, managers can readily use our model as a decision aid to examine whether they advertise at optimal levels. Second, after performing such an analysis for the firms in our sample, we find that the majority of firms underinvest in advertising following shareholder complaints. Notably, though, if shareholder complaint salience is high (i.e., if complaints are submitted by institutional investors, pertain to nonfinancial concerns, or relate to topics that receive higher media attention), firms invest close to an optimal level.Combining all the findings from our article, we conclude that firms increase their advertising investments following shareholder complaints, yet most do not do so sufficiently, especially if shareholder complaint salience is low (i.e., if the complaint is not submitted by an institutional investor, does not pertain to nonfinancial concerns, and does not discuss topics receiving large media attention). This conclusion is striking because firms jeopardize substantial firm value by underutilizing an advertising investment response. We thus caution managers not to underspend on advertising in response to shareholder complaints, especially when the complaints appear less salient and managers may thus feel less of an urge to manage investor perceptions.Third, when contemplating the extent of increasing advertising investments, managers should also consider how to best implement an advertising investment response. This includes taking into account the differential effects of advertising media types. As shown in our additional analysis, television and outdoor advertising seem marginally more effective at mitigating the drop in firm value. We conjecture that television advertising is beneficial because of its large short-term elasticity compared with print or combined advertising and because it is built on emotional rather than informative appeals, which is essential in nurturing investors' affect to the firm ([110]). Likewise, outdoor advertising is known for increasing awareness and broadening visibility ([68]). Although our focus is on advertising investments, the qualitative interviews help explore how to design the advertising content. An interviewed head of investor relations noted a role of ""advertising to explain your strategy...and reiterate why it is a good strategy."" In the words of an interviewed chief financial controller, ""Advertising is not just about the underlying product, it is about letting [the market] know we are a good company.""Insights from the qualitative interviews further suggest the importance of coordinating an advertising investment response with other functions in the firm, such as public relations. For instance, as stated by one chief financial controller, ""It is also about public relations and sponsorships to try and appease [complaining shareholders]."" Another CFO noted that ""using interviews and articles in the business press are good ways to correct impressions of the market and wider stakeholder groups."" If complaints pertain to nonfinancial issues, another route might involve ""independent parties [to] help shape the public discussion in your favor, such as interest groups,"" as one CFO recommended. Unilever, for instance, started a well-publicized collaboration with the World Wildlife Fund after being criticized by shareholders for its lack of sustainability efforts.Likewise, the firm's investor relations department might be an important partner to design an advertising investment response. Often considered a strategic corporate marketing activity ([33]), it is well-placed to advise how to address concerned shareholders in terms of message content and how best to reach shareholders. This could include, for instance, ""presenting yourself on conferences, expos, and symposia,"" as one chief financial controller noted, or ""to be present at investor roadshows."" In any case, quoting a head of investor relations, ""Investor relations, public relations, and marketing [have to] work together very closely in addressing shareholder complaints.""In addition, firms might look for external partners to better communicate their value proposition to the investor community and amplify their advertising efforts. One such partner might be analysts who channel information between firms and investors and help ""validate the business logic underlying the advertising expense"" ([77], p. 607). As a head of investor relations stated, ""Depending on the issue at hand, we have a direct line to the analysts,"" while a VP of public relations shared that ""having analysts as a kind of intermediary or go-between is a great way to defuse an issue that might come up with complaining investors, especially as analysts have spent time with us and really know our products and what we are trying to do.""To conclude, our research suggests that managers should consider the broader implications of an advertising investment strategy across both product and financial markets. Managing the dynamics between advertising investments and the stock market becomes an even more pressing task as shareholder complaints increasingly deal with topics directly under marketing's responsibility. We believe that our findings arm marketing managers with stronger justifications for their advertising budget decisions, their position in the C-suite and at the board level, and their organizational impact to counter the threat of marginalized marketing responsibilities ([107]). That being said, advertising investment decisions should always be part of a coherent long-term marketing strategy, ideally one that aligns shareholders' interests with firm strategy and prevents shareholder dissatisfaction in the first place ([67]). Limitations and Directions for Further ResearchWith the novelty of our research come certain limitations that provide avenues for future research. One topic for future research is a more detailed investigation of the roles of different functions within a firm and how they might help coordinate an advertising investment response, as suggested by our qualitative interviews. Relatedly, while beyond the scope of the present research, it may be worthwhile to consider other, perhaps competing, responses that managers could use when confronted with complaining shareholders. We offer some first ideas in our additional analyses and invite future research to explore this path in more detail. In addition, firm contingencies could affect an advertising response to shareholder complaints and moderate the effectiveness of such a response. Firm factors that relate to spillover effects between investors and nonfinancial stakeholders might play a role, such as a firm's proportion of retail investors, who have been shown to respond more strongly to advertising investments ([71]). The firm's strategic emphasis on advertising might be another factor influencing firm advertising responsiveness to shareholder complaints ([89]).Moreover, it is possible that shareholder complaints moderate the effectiveness of a firm's advertising investments, in addition to advertising investments protecting firm value, as we find in the current research. Literature on product crisis management suggests such effects during times of crisis (e.g., [21]), and future research might provide insights on this matter in the context of shareholder complaints. An interesting finding we note in Table 3 is the negative association of the percentage of excluded complaints with a firm's advertising investments. This result is surprising because when budget decisions are made, the firm does not yet know whether complaints will be withdrawn or omitted, because this information tends to be revealed only at a later point, possibly until the day of the AGM. One might speculate that, over time, firms develop knowledge about the likelihood of complaints being withdrawn or omitted so they can make more informed decisions at the time budgets are set. Alternatively, if advertising budgets are very flexible, firms might be able to undo or reduce the planned increase in advertising investments in cases in which the threat of shareholder dissatisfaction disappears. Future research might help uncover these dynamics.Finally, our modeling of unexpected advertising investments assumes a transitory effect of advertising deviating from expected levels in a given year. At the same time, we cannot exclude the possibility that shareholder complaints could have a more permanent bearing on the firm's future advertising investments. This could be the case if the firm learns about strategy shortcomings through complaints and decides to combat it through advertising. This is an interesting conjecture that we hope future research will address. ConclusionResearch to date has provided inconclusive views on how firms adjust marketing investments in response to stock market adversities. We offer novel insights on a currently overlooked type of stock market adversity, shareholder complaints, and how firms adjust their advertising investments in response to those complaints. Our findings suggest that firms increase their advertising investments and that the stock market rewards them for doing so. Specifically, an advertising investment response to shareholder complaints helps reduce the postcomplaint drop in firm value. Furthermore, we find that more salient shareholder complaints—that is, those submitted by institutional investors, relating to nonfinancial concerns, and relating to topics receiving more media attention—make an increase in advertising investments more likely. Our results offer important implications for marketing theory about how shareholder feedback drives advertising investments as well as practical insights to support managers to make timely and appropriate advertising investment decisions on receiving shareholder complaints. "
13,"Clustering, Knowledge Sharing, and Intrabrand Competition: A Multiyear Analysis of an Evolving Franchise System As franchise systems expand, the clustering and resulting proximity of same-brand outlets often become contentious issues. The increased interactions among outlets may facilitate knowledge sharing, even while inducing intrabrand competition. Prior research has considered each possibility—knowledge sharing or intrabrand competition—in isolation, resulting in conflicting recommendations to the central question of whether multiple same-brand outlets should be close to or distant from one another. In this study, the authors take the perspective of the focal outlet and show that the opportunity to share knowledge afforded by clustering-based proximity may or may not be realized, depending on the motivation and ability of the proximal outlets to share knowledge, the focal outlet’s ability to absorb knowledge, and the governance context. An analysis of more than 8,000 observations on the 988 outlets of a U.S.-based automotive service franchise system from 1977 to 2012, and corresponding outlet-level sales information from 2004 to 2012, provides support for the authors’ hypotheses.Online Supplement: http://dx.doi.org/10.1509/jm.16.0173Any time you open more and more units, there’s always some impact…. People are still making some money — it’s just not what they used to make.—Hardy Grewal, Subway’s largest U.S. development agent (Jargon 2015)Subways aren’t cannibalizing each other.… Restaurants in the most Subway-dense markets actually have higher average sales.—Don Fertman, Subway’s chief development officer (Jargon 2015)The opening quotes exemplify the starkly divergent views on clustering, the geographic concentration of interconnected institutions (Porter 1998). On the one hand, clustering can elicit richer, more frequent interactions (Ganesan, Malter, and Rindfleisch 2005), thereby facilitating knowledge sharing, that is, partners’ communication of valuable technical skills, product knowledge, know-how, as well as information about the market (Cummings 2004, p. 352; De Luca and Atuahene-Gima 2007; Ho and Ganesan 2013). On the other hand, the prospect of proximity-induced intrabrand competition—the degree to which the different outlets selling the same brand compete for the same customers—poses a daunting and real threat (Kalnins 2004; Pancras, Sriram, and Kumar 2012). As such, should multiple same-brand outlets of a franchise system be close to or distant from one another? For both interested scholars and practitioners, this question has profound implications but remains largely unanswered.Table 1 summarizes relevant empirical research on the performance-related consequences of the proximity of same-brand outlets. Scholars working within a sociological tradition of clustering theory (Ganesan, Malter, and Rindfleisch 2005; Ingram and Baum 1997) have emphasized the proximity-induced opportunities for greater learning, interaction, and knowledge sharing among proximal outlets, as well as the consequent performance gains for the focal entity participating in such a cluster (Lu and Wedig 2013). The primarily economics-informed perspective on proximity (Kalnins 2004; Pancras, Sriram, and Kumar 2012), however, emphasizes the costs imposed by the resulting intrabrand competition. As Table 1 shows, prior studies have adopted viewpoints informed by either knowledge sharing or intrabrand competition. As a result, the intriguing idea that both perspectives might be valid remains unexplored.This study represents a first effort to acknowledge and reconcile these seemingly conflicting effects of proximity. Within the context of a growing U.S.-based franchise system, we take the perspective of the focal outlet striving to leverage knowledge shared by the proximal same-brand outlets it is clustered with, while trying to avoid, or at least minimize, sales cannibalization brought about by intrabrand competition. Our conceptual framework, grounded in the literature on organizational learning (e.g., Argyris and Schön 1978; Cyert and March 1963; Sinkula 1994; Slater and Narver 1995), integrates the motivation– opportunity–ability perspective (MacInnis, Moorman, and Jaworski 1991) with work on proximity-governance linkages (Bradach 1997; Brickley and Dark 1987; Tracey, Heide, and Bell 2014) to hypothesize the conditions under which each viewpoint (knowledge sharing or intrabrand competition) might prevail, as reflected in the focal outlet’s performance.Specifically, we posit that the opportunity to share knowledge afforded by clustering-based proximity may or may not be realized, depending on ( 1) the motivation of proximal outlets to share their knowledge with the focal outlet, ( 2) the ability of the proximal outlets to share relevant knowledge and of the focal outlet to absorb such knowledge, and ( 3) the governance1 context (i.e., shared ownership and franchisor vs. franchisee ownership). We posit that the governance context acts as a critical moderator of the clustering–performance relationship and determines the primacy of the knowledge sharing or intrabrand competition effects. In particular, we argue that the shared ownership of clustered outlets2 (i.e., multiunit operations) blunts outlet owners’ concerns about intrabrand competition, in turn affecting their motivation to share knowledge with the focal outlet (Argote and Darr 2000; Ingram and Baum 2001). We also argue that the ownership of the focal outlet (whether franchisor- or franchisee-owned) causes variation in the clustering-attributable costs and benefits. Moreover, we consider the ability inherent in the age-related experience of the outlets (i.e., the proximal outlets’ availability of knowledge gained through years of experience) and the focal outlet’s ability to value, assimilate, and apply this knowledge (i.e., its absorptive capacity) (Cohen and Levinthal 1990).We rely on a unique multisourced data set comprising more than 8,000 observations on the 988 outlets of a large U.S.-based franchise system of automotive services across 41 states, from its inception in 1977 through 2012. Top management of the franchise system shared data on each outlet’s location, year of establishment, and corresponding sales information for a nine-year period (2004–2012), which we supplemented with information from franchise disclosure documents as well as market-specific information we collected from publicly available sources. The rich, fine-grained information allows us to assess the impact of clustering on individual outlets’ performance over nearly a decade.We make several key contributions to the understanding of clustering and its performance consequences. First, rather than limiting our consideration to just the beneficial knowledge-sharing effects of clustering or the potentially negative intrabrand competition effects, we explicitly acknowledge and assess both possibilities. We argue that the net impact of clustering on individual outlet performance depends on the relative strength of each of these competing effects, and we identify the governance-attributable boundary conditions with respect to when one effect might dominate the other.Second, we build on evidence indicating that the knowledge available from different proximal outlets can vary as a function of the outlets’ experience (Kalnins and Mayer 2004; Penrose 1959), and we extend this insight by also considering the focal outlet’s ability to absorb this available knowledge as a function of its own experience (Cohen and Levinthal 1990; Zahra and George 2002). As we discuss subsequently, a low level of either is likely to compromise knowledge sharing between outlets, resulting in reduced performance levels. We are thus able to explain how, even within the same cluster of outlets, performance might vary as a function of the specific focal outlet and the specific proximal same-brand outlets considered. In emphasizing the role of experience of proximally located knowledge sources and recipients, we extend the notion of clustering beyond its exclusive focus on how geographically close the outlets within a cluster are to the specific identities of the focal outlet and those proximal to it.Third, we build on and extend recent theoretical discussions (Bell, Tracey, and Heide 2009; Tracey, Heide, and Bell 2014) linking the notions of clustering and governance (shared ownership and franchisor vs. franchisee ownership). We propose that shared ownership influences perceptions of intrabrand competition, which in turn affects the motivation of proximal outlets to share knowledge with the focal outlet. We further argue that the costs and benefits of clustering vary for franchisorand franchisee-owned focal outlets because they differ in their relative vulnerability to intrabrand competition as well as in the operational leeway available to them to benefit from knowledge sharing. Ours is the first study, to the best of our understanding, to unravel the complex interplay among geographic proximity, individual outlets’ evolving experience, and their governance.In the sections that follow, we first develop the theoretical underpinnings of our conceptual framework and discuss the individual hypotheses linking clustering to outlet-level sales performance, as well as the moderating effects of the governance context. We then describe the research method, results, and their implications. We conclude with the limitations of our study and possible directions for further research.TABLE: TABLE 1 Selected Empirical Research on the Impact of Proximity   Conceptual BackgroundFigure 1 displays our proposed conceptual framework. Building on the well-established literature on organizational learning (Argyris and Schön 1978; Cyert and March 1963; Huber 1991; Sinkula 1994; Slater and Narver 1995), we acknowledge the benefits of clustering same-brand outlets in terms of their potential for greater learning, interaction, and knowledge sharing due to their shared brand (Ho and Ganesan 2013; Lu and Wedig 2013; Tracey, Heide, and Bell 2014). We also recognize the peril posed in the intrabrand competition effects of clustering same-brand outlets (Pancras, Sriram, and Kumar 2012). Outlets clustered with one another are more likely to compete for the same set of customers and therefore cannibalize sales (Davis 2006; Kalnins 2004). Competing outlets are hence likely to protect knowledge and to be less amenable to sharing useful information with one another (Hansen, Mors, and Løvås 2005; Tsai 2002).Our hypotheses address this fundamental tension between knowledge sharing and intrabrand competition and suggest when one perspective might dominate the other. We argue that the governance context (shared ownership and franchisor vs. franchisee ownership) affects the clustering–performance relationship. Specifically, the extent to which proximal outlets share ownership influences outlet owners’ perceptions of intrabrand competition, in turn affecting the motivation of proximal outlets to share their knowledge with the focal outlet (Argote and Darr 2000). Furthermore, we posit that the costs and benefits of clustering vary for the franchisor- versus franchisee-owned focal outlet. We also recognize the key role of the operating experience of the clustered outlets in determining their ability to absorb and share knowledge.Knowledge Sharing EffectBuilding on the work of Cyert and March (1963), Sinkula (1994, p. 35) defines organizational learning as “a process by which organizations … learn through interaction with their environments.” Such learning develops “new knowledge or insights that have the potential to influence behavior” (Slater and Narver 1995, p. 63) and comprises “technical skills, product knowledge, manufacturing processes” (Ho and Ganesan 2013, p. 93), “task information, knowhow, and feedback regarding a product or procedure” (Cummings 2004, p. 352), as well as information about the market (De Luca and Atuahene-Gima 2007).Knowledge3 can be explicit or tacit (Ho and Ganesan 2013; Kalnins and Mayer 2004), and it derives directly from the focal outlet’s operating experience and/or indirectly from other outlets’ experiences (Argote and Miron-Spektor 2011; Bradach 1997) on an ongoing basis (Baum and Ingram 1998; Zander and Kogut 1995). Learning from others’ experiences may take place through contact learning (i.e., transmission of routines through personal and formal relationships) or mimetic learning (i.e., imitating or vicarious learning of routines from other outlets) (Baum and Ingram 1998; Miner and Haunschild 1995). This idea of mimetic learning is also reflected in the notion of isomorphism, espoused by institutional theory (Baum and Ingram 1998), wherein organizations widely imitate each other and their behavior, embedded within personal and interconnected organizational relationships, is likely to be similar to and draw from relevant other firms in their operating environment. This research focuses on same-brand clustered outlets sharing knowledge with one another.To elicit the knowledge sharing effect of clustering, we rely on the well-established motivation–opportunity–ability framework (Argote, McEvily, and Reagans 2003; MacInnis, Moorman, and Jaworski 1991) to inform our hypotheses. The opportunity for knowledge sharing exists to the extent that outlets have occasion to share knowledge with each other. We suggest that greater clustering affords operators of proximal outlets the opportunity to observe, meet, and share knowledge with one another with greater ease (Ganesan, Malter, and Rindfleisch 2005) and the focal outlet greater opportunities to acquire knowledge from proximal same-brand outlets. Such opportunities may or may not be realized, however, depending on ( 1) the motivation and ability of proximal outlets to share knowledge and ( 2) the ability of the focal outlet to absorb knowledge from its proximal outlets.Motivation and ability to share knowledge. We consider the proximal outlets’ motivation to share knowledge with the focal outlet. Why might franchisee-owned outlets be motivated to share knowledge with other franchisee-owned outlets? Our review of the literature suggests that franchisee-owned outlets do so, even if separately owned and operated, for at least two reasons. First, proximally located same-brand outlets are likely to share similar problems and experiences associated with their local markets (Darr and Kurtzberg 2000). These experiences give same-brand outlets similar frames of reference that should ease and encourage information sharing (Huber 1991). Second, proximally located same-brand outlets face similar competition (i.e., out-groups) and therefore identify more with their in-group (i.e., same-brand outlets) (Bhattacharya and Sen 2003). Such identification leads in-group members to be at least moderately motivated to share knowledge with other same-brand outlets (Ho and Ganesan 2013).Though a necessary condition, the motivation to share knowledge is not sufficient for successful knowledge sharing; also required is the ability to share knowledge, or the extent to which proximal outlets have relevant skills and information to share with a focal outlet. More mature outlets are more likely to have accumulated a greater amount of experience than newer, less-established outlets (Huber 1991). This greater depth of experience is reflected in stronger organizational routines and operating procedures and deeper repositories of knowledge regarding their appropriate application (Argote and Miron-Spektor 2011). Thus, the more mature proximal outlets are, the greater their ability to share knowledge with a focal outlet.4Ability to absorb knowledge. The ability to absorb knowledge is the extent to which a focal outlet has the capacity to incorporate information from proximal same-brand outlets. As the focal outlet gains experience, its ability to value, assimilate, and apply new knowledge—that is, its absorptive capacity (Cohen and Levinthal 1990)—also increases (Penrose 1959). With an increase in its absorptive capacity, the focal outlet is more likely to value and use knowledge available from its proximal same-brand outlets and to realize higher levels of productivity and performance (Chen, Lin, and Chang 2009).Intrabrand Competition EffectCoincident with potential knowledge sharing benefits are the costs of intrabrand competition in terms of sales cannibalization (Pancras, Sriram, and Kumar 2012) and knowledge protection (Tsai 2002). Prior research provides evidence of increased competition between proximal same-brand outlets (Kalnins 2003, 2004; Pancras, Sriram, and Kumar 2012). These outlets sell the same products and share the same set of customers in close proximity to each other, with little product or service differentiation (Pancras, Sriram, and Kumar 2012); therefore, they are viewed as close substitutes by customers (Kalnins 2003). The perceived substitutability of same-brand outlets makes travel costs incurred by customers more salient (Davis 2006; Pancras, Sriram, and Kumar 2012), resulting in the sales cannibalization of existing outlets (Kalnins 2004). Thus, proximally located same-brand outlets are likely to compete more fiercely with each other than with outlets located farther away.Proximal outlets with greater concerns of intrabrand competition are likely to be more guarded and less forthcoming with respect to sharing relevant and useful knowledge with the focal outlet (Hansen, Mors, and Løvås 2005; Tsai 2002). Owners of proximal same-brand outlets are more likely to perceive their sharing of knowledge with the focal outlet as weakening their own performance and thus are more likely to hide what they know or divulge only some of their useful knowledge to the focal outlet (Hansen, Mors, and Løvås 2005). This increases the focal outlet’s cost in the pursuit of knowledge and adversely affects its subsequent performance (Hansen, Mors, and Løvås 2005).TABLE: TABLE 2 Underlying Logic of Hypotheses    Governance Context as ModeratorOur conceptual framework also identifies two relevant governance characteristics that help determine whether knowledge sharing or intrabrand competition dominates: shared ownership and franchisor versus franchisee ownership. We expect the shared ownership of clustered outlets (i.e., multiunit operations) to reduce the perceptions of intrabrand competition (Kalnins and Lafontaine 2004), thus prompting proximal outlets to be less guarded and more motivated to share knowledge with the focal outlet (Argote and Darr 2000; Darr, Argote, and Epple 1995; Ingram and Baum 2001). Building on recent theoretical developments linking governance characteristics to geographic clusters (Bell, Tracey, and Heide 2009; Tracey, Heide, and Bell 2014), we also identify franchisor versus franchisee ownership of the focal outlet as a critical “shifter” (Shane 2001) of the knowledge–competition boundary effects. We expect franchisees to experience higher costs of intrabrand competition, due to greater proximity (Kalnins 2004), and lower benefits of knowledge sharing, due to contractual constraints (Kashyap, Antia, and Frazier 2012), than their franchisor-owned counterparts. HypothesesClustering EffectsTable 2 details the underlying logic for H1–H4. We suggest that clustering provides the opportunity for a focal outlet to learn from same-brand proximal outlets but also poses a potential risk of intrabrand competition. Thus, our arguments focus on the interplay of conditions that likely make the knowledge sharing or intrabrand competition more salient for the clustered outlets. As Table 2 shows, we hypothesize that the knowledge shared with the focal outlet is a function of the proximal outlets’ motivation and ability to share knowledge and the focal outlet’s ability to absorb the knowledge shared (see columns 3, 4, and 5 in Table 2); however, their perceptions of intrabrand competition are likely to be influenced significantly by their governance. In what follows, we first take the perspective of a newly established focal outlet, followed by that of a mature focal outlet. For both new and mature focal outlets, we predict the impact of their clustering with other new and mature same-brand outlets on their performance.5The perspective of a new focal outlet. Consider Condition 1: a newly established focal outlet clustered with same-brand outlets that may be new (N) or mature (M), forming clusters CL(NN) and CL(NM), respectively.6 Given their relative inexperience, newly established outlets possess less knowledge of their own (Penrose 1959) and are less practiced and capable of performing the activities in which they are engaged (Cohen and Levinthal 1990). Moreover, newly established outlets do not know local market conditions and competitors as well as their mature counterparts, which negatively affects their ability to use the knowledge shared by their proximal outlets. Thus, we expect a new focal outlet to be more likely to succumb to the intrabrand competition effect of clustering rather than gain from its knowledge sharing potential, regardless of whether the proximal outlets are able (i.e., they are mature) or motivated (i.e., their intrabrand competition concerns are mitigated) to share their knowledge.H1a: The greater the clustering of a new focal outlet with other same-brand outlets, the weaker its performance.In addition, we argue that the intrabrand competition experienced by the new focal outlet is greater when the proximal same-brand outlets are mature (i.e., the CL(NM) relative to the CL(NN) cluster). Mature proximal outlets have more market knowledge, such as product and process knowledge (Ho and Ganesan 2013), customer knowledge (Cummings 2004), and external environment knowledge (De Luca and Atuahene-Gima 2007), which gives them a greater ability to compete at an advantage against the newly established focal outlets. We therefore expect that new focal outlets’ performance will be more negative when the outlets are clustered with mature outlets (i.e., CL(NM)) than when they are clustered with new outlets (i.e., CL(NN)).H1b: New focal outlets clustered with mature same-brand outlets perform worse than those clustered with new same-brand outlets.The perspective of a mature focal outlet. Consider Condition 2 in Table 2, in which a mature (M) focal outlet is clustered with mature (M) or new (N) outlets of the same brand, forming clusters CL(MM) and CL(MN), respectively. Given its own accumulated experience, a mature focal outlet is likely to have a greater ability to absorb any knowledge shared by its proximal outlets than a new focal outlet. This is because a mature focal outlet has greater accumulated experience and correspondingly higher absorptive capacity than its newly established counterparts (Zahra and George 2002). It would therefore benefit from being clustered with other mature outlets of the same brand, which have the ability to share knowledge due to their greater repository of relevant knowledge (Kalnins and Mayer 2004). This knowledge benefit to the mature outlet is limited when the outlet is clustered with newly established outlets, which likely possess less relevant knowledge to share (Kalnins and Mayer 2004). From a knowledge sharing perspective, the mature focal outlet is better served when clustered with other mature outlets rather than with newly established outlets.However, from an intrabrand competitive threat perspective, the opposite inference is likely to prevail—that is, the mature focal outlet is better served when clustered with newly established outlets rather than mature outlets. The reason lies in the greater market knowledge of the mature focal outlet, which confers a competitive advantage over the newly established proximal outlets. As proximal outlets’ experience increases with maturity, however, this knowledge-based competitive advantage dissipates and the focal outlet experiences a higher level of intrabrand competition from mature proximal outlets.Overall, when the mature focal outlet is clustered with mature outlets, it benefits from knowledge sharing; this benefit, however, is balanced by the greater competitive threat posed by the mature proximal outlets due to their greater experience. In contrast, when the mature focal outlet is clustered with new outlets, it loses the knowledge sharing benefit, which is balanced by the lower competitive threat posed by the outlets’ newness. We therefore hypothesize no significant difference in performance between mature focal outlets clustered with mature or new outlets.H2: The greater clustering of a mature focal outlet with other mature or new same-brand outlets neither helps nor hinders the outlet’s performance.TABLE: TABLE 3 Variables and Data Sources  Moderating Effects of Shared OwnershipThus far, our hypotheses have focused on the anticipated main effects of clustering on the focal outlet’s performance. To these, we now add the potential moderating effects of shared ownership of the focal and proximal outlets (see Table 2, Panel B). We define “shared ownership” as the extent to which outlets in the cluster are owned by the same operator as that of the focal outlet. For franchisee-owned focal outlets, this comprises only the proximal outlets owned by the same focal franchisee. When the focal outlet is franchisor owned, this comprises only the franchisor-owned outlets within that cluster.Shared ownership affects the clustering–performance relationship by influencing the perceptions of intrabrand competition (Ingram and Baum 2001; Kalnins and Lafontaine 2004), which in turn affects the motivation of proximal outlets to share knowledge with the focal outlet (Argote and Darr 2000; Darr and Kurtzberg 2000). Unlike outlets that do not share a common owner, outlets operating under shared ownership are likely to have greater norms of reciprocity, a common language system, and incentives to share knowledge, all of which enhance the motivation of the clustered outlets to share knowledge (Darr, Argote, and Epple 1995).Furthermore, shared ownership creates even more opportunities for outlets to share knowledge through multiple means. Indeed, Darr, Argote, and Epple (1995) note that outlets operating under shared ownership have more regular communication with one another and more interpersonal ties than those not sharing common ownership. Moreover, shared ownership creates more opportunities to share knowledge through contact learning (in addition to mimetic learning), in which knowledge is shared through personal and formal relationships (Baum and Ingram 1998; Miner and Haunschild 1995). We now discuss how shared ownership might affect the performance implications of clustering for a newly established focal outlet and a mature focal outlet.The perspective of a new focal outlet. As proposed in H1b, we expect a new focal outlet clustered with mature outlets to underperform relative to a new focal outlet clustered with newly established outlets. We attribute this to the double jeopardy of a new focal outlet’s inability to absorb knowledge from proximal outlets and a higher level of intrabrand competition from more mature and capable proximal outlets. We expect shared ownership to significantly attenuate both these adverse effects.Consider Condition 3 in Table 2, in which a new (N) focal outlet is clustered with new (N) or mature (M) same-brand outlets sharing common ownership and forming clusters CL(NN) and CL(NM), respectively. As the extent of shared ownership between a focal outlet and its proximal outlets increases, the proximal outlets’ motivation to share knowledge with the focal outlet increases by lowering perceptions of intrabrand competition (Argote, McEvily, and Reagans 2003; Ingram and Baum 2001; Kalnins and Lafontaine 2004). The common owner’s objective of ensuring successful operations across his or her multiple outlets (Tsai and Ghoshal 1998) results in stronger ties and greater trust and reciprocity among the shared ownership outlets (Larson 1992; Tsai 2002). Such strong ties brought about by shared ownership blur the perceptions of intrabrand competition and foster greater knowledge sharing among commonly owned outlets (Tsai and Ghoshal 1998).Although newly established focal outlets are less able to value, assimilate, and apply new knowledge (i.e., they have lower absorptive capacity), shared ownership creates more opportunities to learn by contact rather than solely relying on mimetic learning (Darr, Argote, and Epple 1995). Thus, when operating under shared ownership, newly established focal outlets have additional ways to learn organizational routines and operating procedures that are less available (or not available at all) to outlets that do not share common ownership.The final element to consider when assessing the moderating effect of common ownership is the proximal outlets’ ability to share knowledge. As discussed previously, experience is a significant source of operating knowledge (Kalnins and Mayer 2004). Newly established proximal outlets are likely to have less useful and relevant knowledge to share with the focal outlet. Mature proximal outlets, alternatively, are more likely to have a higher level of knowledge to share. Moreover, despite the difficulties new outlets face in absorbing new knowledge, mature proximal outlets under common ownership are also motivated to work with newly established outlets to find ways to instill knowledge in them. The knowledge benefit accruing to the focal outlet is likely to supersede the intrabrand competition–related concern, and this knowledge benefit is likely to be higher for focal outlets clustered with commonly owned mature outlets rather than newly established ones.H3: As the extent of shared ownership increases, new focal outlets clustered with mature outlets perform better than those clustered with new outlets.The perspective of a mature focal outlet. As we propose in H2, a mature focal outlet has more (less) knowledge to gain from other mature (new) proximal outlets but also faces more (less) intrabrand competition from these more (less) experienced outlets. The positive (knowledge sharing) and adverse (intrabrand competition) effects of clustering should counter each other, resulting in no likely differences in performance.Now consider Condition 4 in Table 2, in which a mature (M) focal outlet is clustered with mature (M) or new (N) outlets of the same brand sharing common ownership, forming clusters CL(MM) and CL(MN), respectively. As the extent of shared ownership of the clustered outlets increases, concerns about intrabrand competition are significantly mitigated (Argote, McEvily, and Reagans 2003; Ingram and Baum 2001). The proximal outlets’ knowledge protection imperative correspondingly declines (Kalnins and Lafontaine 2004), leading to an increased motivation to share knowledge with the focal outlet (Darr, Argote, and Epple 1995; Ingram and Baum 2001). In turn, the means of knowledge sharing also changes; that is, rather than relying solely on observation of the proximal outlets, the focal outlet also learns from them through direct contact (Baum and Ingram 1998; Miner and Haunschild 1995). This additional modality of learning from proximal others confers a higher ability on the mature focal outlet to learn, compared with the situation in which the extent of shared ownership is lower.Mature focal outlets possess greater accumulated experience and a correspondingly higher level of absorptive capacity (Zahra and George 2002). When they are clustered with commonly owned mature outlets that are motivated and able to share their knowledge, the knowledge benefits accruing to the mature focal outlets dominate the significantly lower intrabrand competition effects. This knowledge sharing–related benefit is likely to be reduced when the proximal outlets are newly established; notwithstanding their higher motivation to share knowledge, newly established outlets possess less knowledge that might benefit the mature focal outlet. Thus, an increase in the extent of shared ownership likely brings about greater knowledge gains for mature focal outlets clustered with mature rather than newly established outlets. This suggests the following:H4: As the extent of shared ownership increases, mature focal outlets clustered with mature outlets perform better than those clustered with new outlets.Moderating Effects of Franchisor Versus Franchisee OwnershipWe draw from the rich body of franchising research on the drivers (Brickley and Dark 1987; Perryman and Combs 2012) and consequences (Kalnins 2004) of outlet ownership to posit moderation of the previously hypothesized clustering effects, depending on whether the focal outlet is franchisor- or franchisee-owned. Compared with franchisor-owned outlets, we suggest that franchisee-owned outlets are more vulnerable to intrabrand competition and benefit less from the knowledge sharing opportunity conferred by proximal same-brand outlets. The increased costs for franchisees and the reduced knowledge benefits to them should result in franchisorowned outlets outperforming their franchisee-owned counterparts across the clustering scenarios we assess.Franchisee-owned outlets are more likely than franchisorowned outlets to bear the brunt of intrabrand competition due to proximity. Because franchisees’ royalty payments are tied to their revenue (Lafontaine 1992), franchisors are incentivized to open new franchisee-owned outlets, even if they are located close to existing franchisee-owned outlets (Kalnins 2004). In contrast, the franchisor is likely to be more strategic in ensuring that revenues at existing franchisor-owned outlets will not decrease when new outlets are opened (Kalnins 2004). This implies that franchisee-owned outlets are more likely to face competition from other proximal same-brand outlets. Indeed, Kalnins (2004) finds evidence of such a higher likelihood of franchisees facing intrabrand competition. The adverse consequences of intrabrand competition are thus likely to be higher for franchisee-owned outlets than their franchisor-owned counterparts.In addition, we expect franchisees to benefit less from the proximity-conferred learning and knowledge sharing opportunity than franchisor-owned outlets. Recall that the benefits of learning are realized when, on the basis of learning, the focal outlet undertakes different, improved actions and routines (Huber 1991). Franchise systems, by their very design, emphasize uniformity over innovation. To ensure the former, franchisors rely on ironclad contractual agreements and uniformity-ensuring constraints (Kashyap, Antia, and Frazier 2012) that reduce the leeway available to franchisees to make significant changes in response to the additional know-how they are able to glean from their proximal same-brand outlets. Thus, even if a focal franchisee has the opportunity to learn by clustering with same-brand outlets, has proximal outlets that are motivated and have the ability to share knowledge with it, and also has the absorptive capacity to use the knowledge shared, it may not be able to implement improved actions or routines because of contractual constraints.TABLE: TABLE 4 Correlation Matrix and Descriptive Statistics  a Natural log-transformed.Notes: n1 = 12,909. Correlations exceeding |.02| are significant at p < .05, two-tailed.In essence, franchisee-owned outlets are ( 1) more likely to experience the prospect of a proximal same-brand outlet and ( 2) more constrained in their ability to change their organizational routines and processes in response to knowledge received from other proximal outlets. It is this double jeopardy that leads us to hypothesize the following:H5: Franchisor-owned focal outlets outperform their franchisee-owned counterparts more as clustering with other same-brand outlets increases; the dominance by franchisor-owned outlets persists across new and mature outlets.Research MethodEmpirical Context and Data Collection ProcedureWe collaborated with a large U.S.-based franchisor of automotive maintenance and repair services to test our hypotheses. The participating firm provided information on the date of establishment, specific location (street address), and the ownership of each outlet (franchisor or franchisee owned), from system inception in 1977 to its 988th outlet in 2012. In addition, top management shared outlet-level sales performance information on an annual basis from 2004 to 2012. We supplemented these data with various firm- and market-specific variables at the county level, such as royalty rate, interbrand competition, population, per capita income, and area from franchise disclosure documents, the U.S. Census Bureau, and the Bureau of Economic Analysis. Table 3 displays the complete list of variables used in this study and their data sources.Unit of Analysis and MeasuresOur unit of analysis is the individual outlet i (i = 1, …, 988), observed t years since its inception (t = 0, …, 35). Our objective is to relate the clustering of outlets to their corresponding sales performance over time. Table 4 provides the descriptive statistics for all the variables and the pairwise correlations among them.Outlet performance. Our focal dependent variable, outlet-level performance, is reflected in the annual sales revenue (SRit), natural log–transformed, realized by outlet i in year t. The sales revenue of the individual outlet serves as the basis for franchisees’ royalty payments (i.e., royalties are calculated as a percentage of sales) and are therefore of great importance to both franchisees and franchisors.Cluster types. We assessed the extent to which each outlet i was part of a cluster of same-brand outlets at time t by computing the local Moran’s I index (Anselin 1995) using ArcGIS 10.3. The local Moran’s I estimates clustering strength or spatial autocorrelation of a focal outlet on the basis of two factors: ( 1) its geographic proximity to other outlets and ( 2) its similarity to or dissimilarity from other outlets of the same franchise system on a specific attribute (in our case, we infer outlet i’s accumulated experience from its age). Given a set of outlet locations and the associated accumulated experience, the local Moran’s I computes the extent to which an individual outlet is clustered with other outlets and, if so, the nature of clustering—with similar or dissimilar accumulated experience levels.The computation of the local Moran’s I generates two outputs: ( 1) the local Moran’s I score, along with a z-score and a p-value that provide the strength of clustering for each outlet, and ( 2) the cluster category of each significantly clustered outlet based on its attribute (i.e., outlet age). The local Moran’s I identifies outlets with low (i.e., younger) and high (older) attribute values by using the normal distribution of outlet age, categorizing each as new and mature, respectively. Thus, we are able to infer not only the strength of clustering at the individual outlet level but also the four archetypal cluster types of theoretical relevance based on age: CLit(NN), when a new focal outlet i is clustered with other new outlets at time t; CLit(NM), when a new focal outlet i is clustered with mature outlets at time t; CLit(MM), when a mature focal outlet i is clustered with other mature outlets at time t; and CLit(MN), when a mature focal outlet i is clustered with new outlets at time t. Prior studies in marketing have used Moran’s I index to measure spatial dependence of variables (e.g., Mittal, Kamakura, and Govind 2004). The Web Appendix provides additional details on the local Moran’s I computation and examples from our data of each of the four prototypical clustering types.Shared ownership. Consistent with Lu and Wedig (2013), we define clustering within a 25-mile radius of the focal outlet and measure shared ownership of clustered outlets (SOit) as the count of proximal outlets j within this 25-mile radius of the focal outlet i at time t. For franchisee-owned focal outlets, this measure counts only the proximal outlets that are owned by the same focal franchisee (i.e., multiunit franchisees). For franchisor-owned focal outlets, the count includes only franchisor-owned outlets within a 25-mile radius of the focal outlet.Franchisor versus franchisee ownership. We operationalize franchisor versus franchisee ownership (FFOi) as a dichotomous variable that takes a value of 1 when an outlet i is franchisee-owned and 0 when franchisor-owned.Control variables. We incorporate several control variables that we expect to have an impact on the individual outlet’s sales performance over and above our hypothesized variables. We measure cluster size (CSit) as the number of same-brand outlets within a 25-mile radius of a focal outlet. We also include the mean age of outlets in a 25-mile radius of the focal outlet, incorporating its quadratic term as well to control for the possibility of diminishing returns to experience. We control for franchise system size (FSt), or the total number of outlets in operation in year t, and royalty rate (RRt), or the ongoing payment as a percentage of sales that franchisees must pay the franchisor for use of the trademark and other support. System size reflects overall access of the outlet to resources that could affect performance; royalties incentivize franchisor investments in the brand, thereby boosting franchisee sales and making the franchise more attractive to franchisees.We also control for market-specific effects on outlet sales performance. The most fine-grained market data we are able to collect are at the U.S. county level, k (k = 1, …, 270). We include interbrand competition (IBCkt), or the total number of outlets of other competing brands included in the five-digit North American Industry Classification System code corresponding to the sector in which the franchise system operates, located in county k in year t. We also include the population (POPkt) of county k in year t, the income per capita (INkt) in county k in year t, and the area of the county (ARk) in square miles. Finally, we control for unobserved heterogeneity by including year-specific fixed effects for the t years in our data set.Model SpecificationAlthough we were able to obtain data pertaining to individual outlet locations from the inception of the franchise system in 1977, corresponding outlet sales data are available only from 2004 and are missing for some outlets. To account for potential biased parameter estimates due to sales data not being missing at random, we correct for selection bias by specifying a Heckman (1976) selection model in the first stage of the analysis and including the lambda vector thus obtained in the second stage. This second-stage (substantive) equation investigates the interplay of clustering, shared ownership, franchisor versus franchisee ownership of the focal outlet, and their impact on outlet sales performance, while accounting for potential endogeneity of the regressors.Stage 1: correction for sample selection bias. We specify our selection equation as a probit model as follows:whereINCLUDEit = outlet i’s availability of sales information at time t,OAit = age of outlet i at time t,FEi = franchisee-owned as a binary variable (franchisee-owned = 1, 0 otherwise),YRt = specific years as dummy variables, with 2004 as the excluded base year, andeit ~ N(m1, s2).From this equation, we obtain and store the inverse Mills ratio (i.e., lambda) vector for subsequent inclusion in the second stage of analysis.Stage 2: substantive equation estimation. In the second stage, we relate each outlet’s clustering, shared ownership, and franchisor versus franchisee ownership to its annual sales performance. Our model specification approach in this stage is informed by the need to account for the potential endogeneity of regressors—clustering (CLit(NN), CLit(NM), CLit(MM), and CLit(MN)), shared ownership (SOit), and franchisor versus franchisee ownership (FFOi). The clustering-related regressors and shared ownership are time varying, whereas franchisor versus franchisee ownership is time invariant. Durbin–Wu– Hausman tests of these variables yielded significant evidence of endogeneity. We therefore specified an endogeneity-correcting regression equation. We also treat the interactions of clustering with shared ownership and with franchisor versus franchisee ownership as endogenous.We use the Hausman–Taylor instrumental variables (HTIV) regression approach to account for endogenous regressors (for estimation details and checks of its appropriateness, see the Web Appendix). We specify our HTIV model as follows (variables in boldface denote endogenous regressors, of which governance [FFOi] is time invariant):whereSRit = outlet sales revenue (natural log-transformed), CLit(NN) = clustering of a new focal outlet with other new outlets,CLit(NM) = clustering of a new focal outlet with mature outlets, CLit(MM) = clustering of a mature focal outlet with other mature outlets,CLit(MN) = clustering of a mature focal outlet with new outlets, SOit = shared ownership of clustered outlets,FFOi = ownership of a focal outlet i (franchisee-owned = 1, franchisor-owned = 0),CSit = cluster size,APit = mean age of clustered outlets,(APit)2 = quadratic term for mean age of clustered outlets, FSt = firm size,RRt = royalty rate,IBCkt = interbrand competition,POPkt = market population (natural log–transformed), INkt = income per capita (natural log–transformed), ARk = market area (natural log–transformed),YRt = year,IMRit = inverse Mills ratio, ai ~ i.i.d. (m2, s2a), and uit ~ i.i.d. (m3, s2u).Note that during the 2004–2012 period, we observe no instances of clustering of new franchisor-owned outlets with other new outlets. As such, we infer the impact of franchiseeowned new outlets clustering with other new outlets from the main effect of CLit(NN) in Equation 2.TABLE: TABLE 5 HTIV Regression Estimates  Notes: Number of observations = n2 = 6,576; Wald c2 = 11,096.91 (p < .01). Base year = 2004.ResultsModel-free evidence. As Table 4 shows, the clustering of a new focal outlet with other outlets is significantly and negatively correlated with outlet sales (r(CLit(NN)) = –.05, r(CLit(NM)) = –.04, both p < .01), as we expected. In comparison, the clustering of mature outlets is significantly and positively correlated with outlet sales when the clustered outlets are mature (r(CLit(MM)) = .10, p < .01) and negatively correlated when the clustered outlets are new (r(CLit(MN)) = –.04, p < .01). The clustering of new focal outlets is clearly associated with less favorable sales performance than the clustering of mature focal outlets. Thus, these results provide initial model-free evidence for our baseline hypotheses (H1 and H2).The Heckman selection model. The overall model is significant (Wald c2 = 10.58, p < .01), and we find clear evidence of selection with respect to sales information availability (l = –1.15, p < .01). We find that mature outlets (b1 = .05, p < .01) are more likely to provide sales information than newoutlets, while franchisee-owned outlets (b2 = –.24, p < .01) are less likely to provide sales information than franchisorowned outlets. We also find that relative to the base year of 2004, there is greater availability of outlet sales information in subsequent years.The HTIV estimation. Table 5 displays the results of the HTIV estimation. The overall model is significant (Wald c2 = 11,096.91, p < .01), suggesting that the hypothesized predictors of outlet-level sales performance have significant explanatory power. The main effect of clustering on the focal outlets’ sales performance is significant and negative when new focal outlets are clustered with new outlets (h1 = –.03, p < .01) and with mature outlets (h2 = –.10, p < .05) of the same brand. We therefore find support for H1a. However, we find no significant sales performance differences between new focal outlets being clustered with new or mature outlets of the same brand (c2 = 2.49, n.s.); therefore, H1b is not supported. As hypothesized, we find no impact of clustering of mature focal outlets with other mature (h3 = .01, n.s.) or new (h4 = .04, n.s.) outlets on the focal outlets’ sales performance. Thus, we find support for H2.We also find support for H3, which predicted that new focal outlets would perform better when clustered with mature (h8 = .01, p < .05) rather than new (h7 = –.01, p < .01) outlets that are under shared ownership. According to H4, as the extent of shared ownership increases, mature focal outlets gain sales when clustered with mature proximal outlets (h9 = .00, p < .01) and lose sales when clustered with new proximal outlets (h10 = –.02, p < .01). Thus, H4 is supported.H5 predicted that franchisee-owned outlets would gain less from clustering than franchisor-owned outlets. We find partial support for H5. Specifically, we find that mature franchisee-owned outlets achieve lower sales than their franchisor-owned counterparts when in close proximity to other mature outlets (h12 = –.07, p < .01). However, compared with their franchisor-owned counterparts, the clustering of new franchisee-owned outlets with mature outlets results in significant gains to sales performance (h11 = .09, p < .05). This runs counter to H5. Furthermore, we find that the clustering of mature franchisee-owned outlets with new outlets does not significantly differ from that of their franchisor-owned counterparts (h13 = –.14, n.s.). Finally, the franchise system had no instances of new franchisor-owned outlets clustering with other new outlets; the lack of a contrast precludes the ability to test their relationship. Overall, we find that franchisor versus franchisee ownership affects the clustering–performance relationship for both new and mature focal outlets.For the control variables, we find that firm size (h17 = .00, p < .01) significantly and positively affects outlet-level sales. Cluster size (h14 = –.10, p < .01) and royalty rate (h18 = –.39, p < .01), however, have a significant and negative relationship to outlet-level sales. The mean age of cluster (h15 = –.06, p < .01) is significantly and negatively associated with outlet sales, but with a marginally significant diminishing trend (h16 = .00, p < .10). For market-specific control variables, greater population (h20 = .20, p < .01) and per capita income (h21 = 1.04, p < .01) significantly and positively increase outlet-level sales, while interbrand competition (h19 = .00, p < .10) partially and positively affects outlet-level sales. Market area (h22 = –.05, n.s.) does not significantly affect outlet-level sales. Finally, we find significant year-specific effects on outlet-level sales.Post Hoc Analysis of Significant InteractionsFor a better understanding of the moderating impact of shared ownership and franchisor versus franchisee ownership on the clustering and outlet-level sales relationship, we conducted an analysis of simple slopes for all significant interactions (Aiken and West 1991). Figure 2, Panel A, suggests that new focal outlets that are highly clustered with other new sharedownership outlets do lose more sales, relative to their counterparts that are not highly clustered (simple slope of CLit(NN) for new focal outlets = –.20, p < .01). In marked contrast, Panel B suggests that new focal outlets’ sales are not significantly affected by their clustering with mature outlets under shared ownership (simple slope of CLit(NM) for new focal outlets = .08, n.s.). Panels C and D suggest that mature focal outlets gain sales when clustered with othermature outlets in the presence of shared ownership (simple slope of CLit(MM) for mature focal outlets = .12, p < .01).Mature focal outlets’ sales performance, however, is harmed when focal outlets are clustered with new outlets under shared ownership (simple slope of CLit(MN) for mature focal outlets = –.67, p < .01).Figure 3, Panel B, suggests that clustering of a new focal outlet with mature outlets harms outlet-level sales when the focal outlet is franchisor-owned (the simple slope of CLit(NM) for franchisor-owned focal outlets = –.10, p < .05). In contrast, franchisee-owned new focal outlets are not significantly hurt or helped by their proximity to mature outlets (the simple slope of CLit(NM) for franchisee-owned focal outlets = –.01, n.s.). Panel B suggests that mature franchisor-owned focal outlets’ clustering with other mature outlets has no significant impact on their sales performance (the simple slope of CLit(MM) for franchisor-owned focal outlets = .01, n.s.). Only franchiseeowned mature focal outlets lose from greater clustering with othermature outlets (the simple slope ofCLit(MM) for franchiseeowned focal outlets = –.06, p < .05).Alternative SpecificationsWe assessed the stability of our findings to alternate estimation approaches, alternate measures of performance, alternative explanations for the effects reported, alternate time-related specifications, and alternative levels of analysis.Alternative estimator. To test the robustness of our results, we used the fixed-effects approach as an alternative estimator. The fixed-effects estimation results in the dropping of the time-invariant franchisor versus franchisee ownership (FFOi) variable, but it retains all four archetypal clustering types and their interactions with shared ownership and franchisor versus franchisee ownership. All results with respect to the hypothesized effects remain robust.Alternative measure of performance. We also relied on a different but related measure of outlet performance—sales transaction volume—which we operationalized as the total number of transactions reported by each outlet per year. Our HTIV estimates remain robust to this alternative measure of performance as well.Alternative explanation for the effects reported. We also explored the possibility that the focal outlet’s sales might be affected not from any knowledge sharing pursuant to clustering but from better franchisor monitoring capabilities as a function of nearby franchisor-owned outlets. To test this alternative explanation, we computed the number of franchisor-owned outlets in the county of location of the focal outlet and included this variable in our model. All the clustering-related effects and their interactions with shared ownership and franchisor versus franchisee ownership remain robust, and the main effect of the additional regressor is nonsignificant.Alternative temporal separation. We based our conceptualization and subsequent model specification approach on the assumption of contemporaneous (immediate, within the same year) effects of clustering on the sales performance of each outlet. We also assessed one- and two-year lagged models of the hypothesized relationships. Our principal findings of the baseline hypotheses and the moderating effects of shared ownership and franchisor versus franchisee ownership persist.Alternative level of analysis. Prior research has mostly used clustering as a global or systemwide construct (within the present context, across all 988 outlets of the franchise system) without investigating the type of clustering or with whom a focal outlet is clustered. We therefore specified an alternative model, measuring clustering at the system level and treating it as an endogenous regressor. The HTIV estimation results show that the systemwide clustering of outlets is positively and significantly associated with the individual outlets’ sales. This is consistent with prior research that does not account for the accumulated experience, shared ownership, or franchisor versus franchisee ownership of the clustered outlets (e.g., Lu and Wedig 2013). This result, however, masks the nuances that emerge from a broader consideration of the specific identities of the focal and proximal outlets and provides misleading confi-dence in clustering effects on performance.DiscussionProximity associated with clustering is a contentious issue for all franchising participants, but it is particularly vexing for franchisees because of sales cannibalization concerns. Our assessment of the impact of clustering suggests that although these concerns are not unfounded, they only apply under certain conditions. Our findings support our contention that physical distance is not the sole determinant of outlet sales.Theoretical ImplicationsThis research was motivated by conflicting findings and assertions about the effects of clustering. Importantly, disagreements regarding clustering-attributable performance exist not only across but also within paradigms. Consider, for example, how much at odds the studies reporting positive effects of agglomeration (Chung and Kalnins 2001) are with those warning of significant sales cannibalization (Pancras, Sriram, and Kumar 2012). We observe a similar schism for studies adopting a sociology-informed clustering viewpoint and the knowledge sharing this implies. Whereas Lu and Wedig (2013) report positive performance effects, Ingram and Baum (1997) find evidence of a negative impact of clustering.Our study builds on and extends both streams of work. In particular, we call for a more nuanced consideration of clustering’s impact, one that emphasizes not just the physical distance from specific outlets (“how far”) but also the distance “from whom.” In essence, we argue that the specific identities of the focal outlet and of the same-brand outlets it may cluster with matter. Until now, research on clustering has focused almost exclusively on system-level clustering. In the present context, this amounts to a single “clustering score” representing the extent of clustering across all 988 outlets of the franchise system we assessed. As we have demonstrated, such an aggregate approach indeed yields a positive association between clustering and sales. However, it is only when clustering is unpacked (i.e., particular individual outlets’ clustering with particular other same-brand outlets) that we find evidence of positive and negative cluster-attributable effects. Our research highlights the interplay of knowledge sharing and intrabrand competition in explaining these effects and calls for a more subtle, disaggregated approach to assessing clustering’s impact.The differential performance accruing to outlets within each of the four archetypal cluster types provides support for our disaggregated approach to assessing clustering effects. We find that newly established outlets suffer sales declines, consistent with their lower absorptive capacity and greater susceptibility to intrabrand competition. In contrast, mature outlets appear to be shielded from the worst effects of intrabrand competition, due to their greater repository of operating knowledge. Perhaps most important, shared ownership of the focal outlet—whether newly established or mature—and the outlets proximal to it appear to ease concerns about intrabrand competition, in turn enhancing the motivation for sharing knowledge and the corresponding performance level of the focal outlet.We also note some results that are contrary to our expectations. We hypothesized that franchisee-owned outlets would perform less well when clustered than their franchisor-owned counterparts. This expectation received support in only one of the three clustering situations we analyzed: when a mature franchisee-owned outlet is clustered with other mature outlets. We observed no such decreased performance for mature franchisee-owned outlets clustered with new outlets; contrary to our hypothesis, newly established franchisee-owned outlets perform better than their franchisor-owned peers when clustered with mature outlets. Though puzzling at first glance, these antithetical findings may be explained by the agency theory– based raison d’être for franchising—namely, the power of incentives, specifically that of residual claims, whereby newly established franchisees exert effort to retain their outlets’ profits after making royalty payments to their franchisor (Brickley and Dark 1987; Brickley, Dark, and Weisbach 1991; Lafontaine 1992). It is likely the incentives on the part of the new franchisee-owned focal outlet and the corresponding motivation and ability to share knowledge on the part of the mature proximal outlets that lead to this positive clustering effect.Our study has implications for research on knowledge sharing beyond the current franchising context. By elucidating the movement of knowledge between different units/organizations, our research highlights processes that involve both the sharing of knowledge by the knowledge source outlet and the acquisition and application of knowledge by the recipient outlet, thereby allowing us to contribute to the literature on interorganizational knowledge sharing. Relatedly, our research adds to what is known about knowledge sharing in interfirm networks based on spatial and/or psychological proximity, in which knowledge sharing among networked firms depends on the number of connections and the degree of interconnectedness with other network firms (e.g., Swaminathan and Moorman 2009). The insights afforded by our research can help extend this stream of literature by emphasizing the dyad-specific flows of information and/or influence among specific pairs of network participants. A nuanced consideration of the identified factors that inhibit or enhance ability and motivation would further inform our understanding of network efficiency and knowledge redundancies and how they might affect network-participating members’ individual performance.Managerial ImplicationsFor franchisees. Our post hoc calculations suggest that, compared with franchisor-owned outlets, a new franchisee-owned outlet may expect to gain 9.5% of mean annual sales, or just over $39,000, when clustered with mature same-brand outlets. Although this result runs counter to our expectation, one explanation for this might lie in new franchisees’ efforts to take advantage of the experience gained and knowledge shared by clustered mature outlets. Mature franchisee-owned outlets lose mean annual sales of 6.7% (just over $27,000) when clustered with other mature outlets of the same brand. Overall, our results imply that franchisees opening new outlets close to mature outlets of the same brand are likely to realize significant sales performance gains. In contrast, ceteris paribus, mature franchisees clustered with other mature same-brand outlets find themselves facing the prospect of intrabrand competition.For franchisors. Like franchisees, franchisor-owned outlets also experience a mixed bag when clustering with other same-brand outlets. A new franchisor-owned focal outlet loses nearly 10%, or just over $39,000, in mean annual sales when clustered with mature outlets. When new franchisor-owned outlets are clustered with other new outlets, the loss in sales is not nearly as bad—they lose an average of just over 3% of their mean annual sales, or close to $12,500. We also find that mature franchisor-owned outlets remain relatively unaffected by outlet clustering, regardless of whether the cluster comprises new or mature proximal outlets. Taken together, the pattern of results suggests that franchisors mindful of the sales performance of the outlets owned by them should avoid establishing these outlets in proximity to other same-brand outlets, whether mature or new.Across both franchisor- and franchisee-owned outlets, shared ownership of the focal and proximal outlets appears to help facilitate knowledge sharing and blunt intrabrand competition. Under shared ownership, newly established outlets clustered with mature outlets outperform their counterparts clustered with new outlets by nearly 1% of mean annual sales, or just under $5,000. For mature outlets, the difference is even more striking—mature outlets clustered with other mature outlets outperform their counterparts clustered with new outlets by nearly 3% of mean annual sales, or just under $11,500. Given that the average multiunit-owning entity (whether franchisor or multiunit franchisee) in this franchise system owns 21 outlets, the sales performance gains accruing from shared ownership are certainly significant.Limitations and Future Research DirectionsAs with any research effort, our study has several limitations. First, our assessment of a single franchise system, albeit over an extended period, limits the generalizability of our findings. Future efforts that include multiple firms operating in diverse industries would help extend our findings by considering multiple franchise system outlets and their competitive and cooperative interactions over time.Second, tracking an additional performance indicator (e.g., survival of the individual outlet) over the entire life cycle of the franchise system would provide additional insights into clustering and its performance consequences. Such data, were they to be available, would confer the ability to investigate the impact of clustering on outlets’ survival in the rich context of multiple franchise systems.Finally, our reliance on longitudinal archival data, while affording insights into individual outlets’ evolving capabilities and constraints, also limits our ability to directly observe or measure the relevant franchise system participants’ perceptions and motivations. We can state only that their actions appear consistent with the perceptions and motivations we attribute to them. Future efforts to measure these unobserved intervening variables (e.g., by conducting surveys with the individual outlet operators, by designing laboratory experiments to provide a better understanding of the underlying conceptual mechanisms) and to integrate them with the archival data already available would yield rich insights.H5Franchisor Versus Franchisee OwnershipH1–H2ClusteringGeographic concentrationAccumulated Experienceof the focal outletof proximal outletsH3–H4Shared OwnershipOutlet PerformanceSales revenueControl VariablesCluster sizeMean age of proximal outletsFirm sizeRoyalty rateInterbrand competitionMarket populationIncome per capitaMarket areaYearA: Logic for Knowledge Sharing and Intrabrand Competition EffectsB: Moderating Effect of Shared OwnershipNotes: Subscripts “N” and “M” denote new and mature outlets, respectively. The first subscript refers to the focal outlet, the second to the proximal outlets. For example, the CL(NM) cluster type indicates a new focal outlet that is clustered with mature outlets of the same brand.aNatural log–transformed.Notes: n1 = 12,909. Correlations exceeding |.02| are significant at p < .05, two-tailed.aNatural log–transformed.p < .10 (two-tailed).*p < .05 (two-tailed).**p < .01 (two-tailed).Notes: Number of observations = n2 = 6,576; Wald c2 = 11,096.91 (p < .01). Base year = 2004.A: Impact of Clustering of the New Focal Outlet with Other New Outlets on Outlet-Level SalesB: Impact of Clustering of the New Focal Outlet with Mature Outlets on Outlet-Level SalesC: Impact of Clustering of the Mature Focal Outlet with Other Mature Outlets on Outlet-Level SalesKEYWORDS_SPLITNotes: Outlet-level sales are natural log–transformed. CL(NN): clustering of the new focal outlet with other new outlets; CL(NM): clustering of the new focal outlet with mature outlets; CL(MM): clustering of the mature focal outlet with other mature outlets; CL(MN): clustering of the mature focal outlet with new outlets; SO: shared ownership of clustered outlets.A: Impact of Clustering of the New Focal Outlet with Mature Outlets on Outlet-Level SalesB: Impact of Clustering of the Mature Focal Outlet with other Mature Outlets on Outlet-Level SalesNotes: Outlet-level sales are natural log–transformed. CL(NM): Clustering of the new focal outlet with mature outlets; CL(MM): Clustering of the mature focal outlet with other mature outlets.  "
14,"Coins Are Cold and Cards Are Caring: The Effect of Pregiving Incentives on Charity Perceptions, Relationship Norms, and Donation Behavior Charities often include low-value monetary (e.g., coins) and nonmonetary (e.g., greeting cards) pregiving incentives (PGIs) in their donation request letters. Yet little is known about how donors respond to this marketing strategy. In seven studies, including two large-scale field experiments, the authors demonstrate that the effectiveness of PGIs depends on the organization's goals. People are more likely to open and read a letter containing a monetary PGI (vs. a nonmonetary PGI or no PGI). In addition, monetary PGIs increase response rates in donor acquisition campaigns. However, the return on investment for direct mail campaigns drops significantly when PGIs are included. Furthermore, average donations for appeals with a nonmonetary PGI or no PGI are similar, while those with a monetary PGI are actually lower than when a nonmonetary PGI or no PGI is included. This is because monetary PGIs increase exchange norms while decreasing communal norms. This effect remains significant when accounting for alternative explanations such as manipulative intent and the anchoring and adjustment heuristic.KEYWORDS_SPLITNot a week goes by that I don't receive requests for monetary donations to one charitable organization or another....However, I must ask these organizations if sending out hundreds of thousands of requests for donations with a nickel or dime attached inside an envelope full of address labels imprinted with my name and address is honestly worth it.[45]Most charitable organizations rely on individual donations to provide much-needed services, and over 90% of nonprofit organizations use direct mail as one of their primary fundraising methods ([40]). However, most direct mail campaigns achieve modest success, with an average response rate of 1% to 3.7% ([13]). To attract attention and encourage donations, charities often include pregiving incentives (PGIs), also known as unconditional gifts or front-end premiums, in their donation request letters (for examples, see Web Appendix A). A PGI is defined as the provision of a benefit or a favor before requesting compliance ([35]; [43]).In 2013–2015, PGIs such as coins and greeting cards were included in approximately 40% of the total nonprofit mail volume ([41]). In fact, inclusion of monetary PGIs is so popular, the strategy has its own moniker: ""the coin trick"" ([28]; [44]). Yet there are concerns over the effectiveness of this strategy ([49]). The goal of the present research is to address the question posed in the quote at the opening of this article—are PGIs in charitable campaigns worth it?Several streams of research have examined the effect of incentives on prosocial behavior (for a review, see [55]]), some yielding conflicting results. The literature on reciprocity shows that people feel a strong sense of obligation to repay benefits they have received, even if those benefits are unwanted ([ 9]; [22]). Accordingly, [15] found that including a small PGI, such as a postcard, in a charity appeal significantly increased the number of donations, though the average donation amount remained fairly constant. Motivation crowding theory, in contrast, proposes that external rewards diminish intrinsic motivation or sincerity to do good, which can result in reduced donations ([ 5]; [21]). For instance, [39] found that providing nonmonetary conditional/promised gifts (e.g., pens, reusable bags) decreased average donations because such external incentives ""crowd out"" altruistic motivations to donate.One limitation of the aforementioned literature is the focus on just one aspect of donation behavior: intrinsic characteristics of the donor (i.e., sense of obligation, intrinsic motivation). Yet offering incentives can also affect the relationship between two entities. Relationship norms between consumers and organizations are important to understand as they play a critical role in consumer reactions, including attitudes and behaviors toward a brand's marketing actions ([ 2]). To illustrate how incentives might influence a relationship, imagine a situation in which the person you have a burgeoning romantic relationship with gives you a $20 bill before asking you to attend a family function with them. We predict that the mere introduction of money is likely to lead you to question the nature of the relationship. But what if, rather than cash, your partner gives you a gift like a bottle of wine or heartfelt greeting card?This scenario exemplifies a second limitation of prior research on incentives. It does not address how different types of incentives affect donations. The plethora of diverse gifts that charities send to potential donors can be classified into two groups: monetary and nonmonetary. In the current research, we draw on the interpersonal relationships literature to develop a framework for understanding consumer reactions to these two classes of PGIs. We propose that donors typically perceive charities as communal organizations and use communal norms when interacting with them. However, the inclusion of monetary PGIs diminishes communal norms while increasing exchange norms, resulting in lower average donations.Our research makes a number of theoretical and practical contributions. Although the literature on brand relationships is substantial, relatively little of it has examined relationship norms and factors that influence their salience (for a review, see [33]]). Foundational research in marketing manipulated the salience of communal and exchange norms using hypothetical scenarios grounded in personal relationships ([ 2]; [ 3]). In contrast, we show that organizations can unwittingly influence the salience of these norms simply by including gifts or incentives in their donation solicitations.The current research also extends recent work on how superficial elements of a donation appeal can influence donation behavior, even when the content is held constant ([50]). We show that PGIs are tangible cues that influence people's communal and exchange perceptions of the charitable organization, leading to sizable differences in donation behavior. In doing so, we answer calls for investigation of PGIs in the donation context, with an emphasis on ""the influence of the type of gift"" ([ 4], p. 1059).Our work has managerial implications for the hundreds of nonprofit organizations that include PGIs in their direct mail campaigns. As stated previously, including PGIs such as coins and greeting cards in direct mail campaigns is a common strategy for nonprofits ([41]). We use different methods, samples, and incentives to examine the effect of PGIs on multiple outcomes of interest to charitable organizations, including increasing awareness, procuring future donors, influencing consumer perceptions, and fundraising. Results indicate that PGIs have different effects on different outcomes, and that the best strategy depends on what the charity wants to achieve. For example, monetary incentives may not be effective at increasing the average donation amount but may help charities gain visibility and awareness by increasing the opening and response rate among people who have not donated before. Thus, the results of this research allow nonprofits to make more informed cost–benefit analyses in deciding whether strategies such as ""the coin trick"" are worth it. Theoretical Development and Hypotheses Relationship Norms in the MarketplaceExtant research has identified two primary relationship norms that guide how people give and receive benefits: communal norms and exchange norms (e.g., [10]). Communal norms dictate that individuals attend to other's needs and demonstrate concern for one another. People following communal norms are motivated to care for others and are willing to incur costs to do so, regardless of whether they will receive anything in return. Most family relationships, romantic relationships and friendships are governed by communal norms ([10]). In comparison, exchange norms are those in which benefits are given with the expectation of receiving comparable benefits in return. Those who follow exchange norms are less likely to help others without a benefit or reward. A prototypical relationship example following exchange norms is that of business partners.Relationship norms influence interactions not only with individuals but also with marketplace entities. [ 2] showed that people who followed communal norms evaluated a brand more positively when the brand offered a noncomparable (vs. comparable) reward in return for help, while those who followed exchange norms evaluated the brand more positively when given a comparable (vs. noncomparable) reward. [54] found that when communality was low, people expected businesses to provide services for their payments and viewed providers negatively during service failures. In contrast, when communality was high and self-obligation was highlighted, people behaved in a more caring and understanding manner, and thus reacted less negatively to service failures.Beyond the situations manipulated in laboratory experiments, few marketplace interactions have been found to operate according to communal norms. This may be because previous research has largely focused on for-profit companies, which are generally perceived as providing benefits with the expectation of receiving comparable benefits in return. We propose that, unlike for-profit companies, nonprofits are perceived as high communality organizations that care for others' welfare and give benefits without expecting anything in return. In support of this, [ 1] found that nonprofits such as charities (vs. for-profit companies) are seen as warm entities that care about the welfare of others. [36] identified additional organizations that consumers expect to have communal obligations (i.e., religious and pharmaceutical), and showed that people respond to the marketing efforts of these organizations differently than those of typical businesses.To further lay the foundation that nonprofits are seen as communally oriented, we conducted a pilot test on perceptions of a wide range of both for-profits and nonprofits (for details, see Web Appendix B). In addition, we examined the potential role of organizational familiarity, which has been shown to affect donors' inferences about the charity ([48]). Results from the pilot study revealed that communal perceptions are higher (Mcharities = 5.54, Mbusiness = 3.68; F( 1, 199) = 95.97, p <.001), and exchange perceptions are lower (Mcharities = 2.37, Mbusiness = 4.54; F( 1, 199) = 99.37, p <.001), for charities than for businesses, regardless of how familiar they are. We propose that this high degree of communality is a foundation for donors' willingness to give benefits (e.g., money) generously. The Role of Incentive Type on Charity Perception and Donation BehaviorWe predict that relationship norms are influenced by the type of PGI (monetary vs. nonmonetary) commonly included with charity letters. Money is more likely to evoke marketplace norms and lead people to behave in a quid pro quo manner (i.e., give comparable benefits for any benefits received) compared with nonmonetary goods ([24]). Activating the concept of money leads people to infer that they are in a businesslike or exchange relationship and behave as though they are interacting with a business party (e.g., [30]). Including a monetary PGI along with a donation appeal thus has the potential to increase exchange norms. At the same time, a monetary PGI should decrease communal norms. Reminders of money lead people to be more self-centered and eschew strong relational ties, such that they prefer not to rely on or be relied on by others ([53]). They may therefore be less inclined to think of relationships in communal terms after receiving a monetary PGI. We hypothesize that the increase in exchange and decrease in communal norms due to the presence of the monetary PGI will jointly lead to lower average donations.Nonmonetary PGIs are preferred over monetary ones in communal relationships, as they do not evoke marketplace norms in a payment context ([24]) and are consistent with communal norms. A nonmonetary gift may thus have the potential to increase communality. However, unlike exchange norms, communal norms do not dictate the prompt repayment of benefits given or received ([10]). Indeed, previous research shows that communal participants evaluate a brand more positively if the request for help from the brand is delayed compared with when it is made immediately ([ 2]). Giving a nonmonetary PGI and then asking for help straightaway is more aligned with exchange norms and could offset any communal increase from receiving a small gift, resulting in no overall change in communality. In addition, communal norms dictate noncontingent, need-based giving ([10]). Thus, whether and how much people donate should not depend on the presence or the value of nonmonetary PGIs. Taken together, this suggests that a nonmonetary PGI is unlikely to increase net communality (i.e., communal norms relative to exchange norms) or average donations in a direct mail campaign. It is relevant to note that our conceptualization suggests that a monetary PGI will lead to lower net communality but not necessarily flip the overall relationship from communal to exchange. Previous research typically manipulates communal versus exchange relationships, while less attention has been paid to how people react to communality variations on a continuum. Yet a review of the literature reveals that even manipulations of relationship norms, which clearly delineate their differences, do not always produce effects at the extreme ends of the continuum. For instance, [54]; Experiment 2) manipulation check showed that both the communal and exchange scenarios elicited communality scores above the midpoint of the seven-point scale (4.83 vs. 4.08, respectively), with a significant difference between the means. Despite the fact that both scores were on the communal end of the scale, their relative difference produced significant hypothesized effects in consumer behavior. Conceptually, this is explained by research showing that communal relationships vary along a continuum, with one's willingness to devote resources toward promoting the other's welfare increasing with higher levels of communality ([11]).Importantly, charities have campaign objectives other than maximizing individual contributions. Pregiving incentives may be useful for goals that are lower on the consumer response hierarchy such as increasing public awareness and enlarging the donor list ([32]). Research has shown that monetary incentives are more effective in eliciting survey responses than nonmonetary ones or no incentives, and monetary incentives are particularly useful for people who have no interests in the survey topic ([46]). Moreover, lay beliefs and laws guiding people on how to treat money may lead them to be reluctant to throw away cash (e.g., Title 18, Chapter 17, of the U.S. Code prohibits the debasement of coins). Thus, monetary PGIs could be effective in promoting initial engagement with a piece of mail, especially among individuals who are unfamiliar with the charity. In addition, opening rate is correlated with response rate, or the likelihood to act on the mailing ([17]). Charities can add these respondents to their mailing list for future campaigns. Thus, although our conceptualization predicts lower average donations with monetary PGIs (vs. nonmonetary and no PGIs), they may be effective for increasing opening and response rates, especially among people who are unfamiliar with the charity and thus need an incentive to open the envelope. Formally, we hypothesize: H1:  Monetary PGIs (vs. nonmonetary or no PGIs) increase opening and response rates, particularly among people who are not familiar with the charity. H2:  Monetary PGIs (vs. nonmonetary or no PGIs) lower average donations. There is no difference between a nonmonetary PGI and no PGI on average donations. H3:  Monetary PGIs (vs. nonmonetary or no PGIs) lower net communality. H4:  The relationship between PGI type and donations is mediated by net communality. Overview of StudiesWe test our hypotheses in seven studies. Study 1 examines the effect of PGIs on the opening rate of donation letters. Study 2 is a field study in which we partner with a charity on a 9,000 household donor acquisition campaign to examine the effect of PGIs on response rate, average donation amount, and return on investment (ROI). Study 3 examines the effect of PGIs on average donations using real incentives and contributions. Study 4 examines the mediating role of relationship norms on average donations while testing the alternative explanations of anchoring, manipulative intent, and charity inefficiency. Both the incentives and donations in Study 4 are hypothetical. Study 5 examines the impact of PGIs of varying value, and Study 6 examines the effect of phrasing monetary PGIs in different ways on average donations. These studies use hypothetical incentives but ask participants to donate from their potential bonus payment to increase realism while curtailing study costs. Finally, Study 7 is a field experiment that examines the anchoring effect on response rate, average donations and ROI for a year-end campaign for recurring donors. Study 1Charities often have the goal to increase awareness and achieve high visibility through their donation campaigns ([48]). Encouraging potential donors to open the charity letter may be especially critical to the success of donor acquisition campaigns, in which appeals are sent to people who are unfamiliar with the charity or have never donated before. In this study, we explore the crucial question of how enclosures of PGIs affect how these individuals handle the letter. We predict that people are reluctant to throw away a piece of mail with money in it, which results in them opening and reading the letter (H1). We sample a diverse population by recruiting staff and faculty at a major U.S. university, who are likely to have more disposable income than convenience samples such as college students and online survey takers. Participants and ProcedureTwo hundred forty university staff and faculty members from a large Midwestern university (187 female; Mage = 46.02 years; Mincome = $75,000 to $99,000) participated in this study in exchange for either a $5 prepaid Visa card or a university branded gift of a similar value (e.g., hat, wine glasses). The study was conducted in six buildings on campus (School of Business, School of Education, University Health Center, University Administration, University Endowment, and Payroll and Human Resources). Participants in each building were approached individually for oral consent to participate in the study and signed up for a time slot of their choice. Participants were taken one at a time to the experiment room. A research assistant who was unaware of the study hypotheses administered the study and recorded the data. The study consisted of a mail sorting task along with a short survey of demographic information. Mail sorting taskThe study utilized a between-subjects design. Participants were randomly assigned to one of three PGI conditions: a charity letter with a clear window presenting a monetary PGI (quarter), a nonmonetary PGI (a greeting card), or no PGI. All participants were presented with six pieces of sealed mail in the same order, including the focal charity letter from a fictitious charity called ""Help Fight Cancer Society"" (the same charity used in one of the supplemental studies in the Web Appendix). The other five pieces of mail were the same across all three PGI conditions. There was one solicited letter (dental bill), and four unsolicited letters (SmartShopper ad, credit card offer, car insurance ad, and retirement insurance ad). We included both solicited and unsolicited letters to compare the treatment of the charity letter with mail of varying importance. The cover design and content for each letter were created based on actual letters.Four boxes were placed on a table with the following labels: unopen throw away, open without reading, open and read, and unopen but keep. Participants were asked to imagine that they just got home from getting their mail and were sorting them. They were asked to sort the mail as they normally would and place each letter into the box corresponding to their decision. An ""other"" option was provided in case they intended to handle the letter differently. After the mail sorting task, participants answered a few demographic questions, chose either a Visa card or a university gift as their payment, and were thanked and dismissed. Results and DiscussionWe compare the opening rate of the three PGI conditions next and report the comparison of each appeal against the solicited and unsolicited letters in Web Appendix C. Unopen throw awayConsistent with H1, the number of people in the monetary PGI condition who chose to throw away the charity letter without opening it (16.46%, N = 13) was significantly lower than those in the nonmonetary PGI (45%, N = 36, χ2( 1) = 15.19, p <.001) and no-PGI (49.38%, N = 40, χ2( 1) = 19.57, p <.001) conditions. No difference was found between the monetary and nonmonetary PGI conditions (χ2( 1) =.31, p =.58). Open without readingWe found no difference across the three PGI conditions (38.46%, Nmonetary = 20; 32.69%, Nnonmonetary = 17, 28.85%, Ncontrol = 15; all ps >.29). Open and readThe number of people who chose to open and read the charity letter was significantly higher in the monetary PGI (48.10%, N = 38) condition than that in the nonmonetary (26.25%, N = 21, χ2( 1) = 8.13, p =.004) and no-PGI (17.28%, N = 14, χ2( 1) = 17.31, p <.001) conditions. No difference was found between the nonmonetary and no-PGI conditions (χ2( 1) = 1.90, p =.17). Unopen but keepWe found no difference across the three PGI conditions (30.77%, Nmonetary = 8; 23.08%, Nnonmonetary = 6; 46.15%, Ncontrol = 12; all ps >.14). DiscussionThis study examined the effect of PGIs on opening rate in a donor acquisition campaign. Confirming H1, fewer participants chose to throw away the charity letter without opening it in the monetary PGI condition than the nonmonetary and no-PGI conditions. Moreover, more people chose to open and read the letter in the monetary PGI condition than in the nonmonetary and no-PGI conditions. Enclosing a nonmonetary PGI was no more effective than not enclosing any PGI in leading people open and/or read the charity letter. Having assessed the impact of PGIs on opening rate, we turn our attention to other important campaign outcomes, including the response rate and average donation amount and ROI. Study 2A primary objective of donor acquisition campaigns is to raise awareness for the organization and enlarge the donor pool, which can lead to more profitable campaigns in the future ([47]). Study 1 suggests that a monetary PGI promotes this goal by persuading recipients to open and read the letter. In Study 2, we partner with a local mental health and suicide prevention nonprofit to examine more downstream consequences.As discussed in the ""Conceptual Development"" section, we predict that a monetary PGI (vs. nonmonetary and no PGI) will lead to lower average donations. We also examine how different types of PGIs affect the response rate, or number of replies to the mailing. Previous research has shown that monetary payments lead people to behave in a quid pro quo manner ([24]). Thus, after receiving a coin in the mail, people may respond by sending the money back to the organization. For low-value monetary PGIs, this would lead to lower average donations, but a higher response rate.Because charitable organizations often lose money in donor acquisition campaigns, an important goal is to minimize losses ([47]). Thus, we measure the ROI for each type of PGI. We predict that enclosing PGIs, both monetary and nonmonetary, will lead to a lower ROI than not enclosing any PGI—although the response rate may be somewhat higher for monetary PGI appeals, average donations will be lower, and the total amount raised is unlikely to offset the high cost of sending PGIs.Given the extremely low response rate for typical direct mail campaigns (.65%; [47]), it is difficult to draw conclusions from nonresponses about whether PGI had an impact on donation behavior. Nonresponses could be due to indifference toward the PGI, or because people never received or saw the letter. Thus, to calculate average donation amount, we analyzed data only from respondents who made a donation. This is consistent with prior research on donation appeals with low response rates ([14]). However, in estimating net loss per mailing, we included all those who had been solicited (N = 9,000), because ROI addresses whether the benefits of enclosing PGIs justify the total cost. Materials PGIsAn examination of over 100 charity letters revealed that common monetary PGIs range from a few cents to $2.50, and common nonmonetary PGIs include address labels, greeting cards, and note pads. In consultation with the charity, we decided to use a quarter ($.25) for the monetary PGI and a greeting card for the nonmonetary PGI. To control for the potential confounds of perceived overhead costs of the charity ([20]) and subjective incentive value ([42]), we conducted a pretest to ensure that the perceived cost (to the charity) and value (to the recipient) are similar for the monetary and nonmonetary PGIs we chose (both ps >.15; see Web Appendix D). Mailing listA city-wide mailing list of 11,000 people—from a total population of about 91,000—was acquired by the charity. The mailing list contained people's names, addresses, zip codes, and phone numbers. Nine thousand people were randomly selected from the mailing list and assigned to one of the three PGI conditions: monetary ($.25), nonmonetary (a greeting card), or no PGI. The charity ensured that none of them had donated to the charity before. Business reply return envelopesWe included a standard business reply return envelope with each letter. Participants also had the option of using their own stamps to help the charity reduce costs. A code was stamped on each return envelope to keep track of PGI condition. Donation request letters, incentives, and return cardThree versions of the donation request letter were created to correspond to the monetary, nonmonetary, and no-PGI conditions, along with a perforated return card containing donation instructions at the bottom of the letter (see Web Appendix E, Study 2). For both the monetary and nonmonetary conditions, an additional sentence at the top front page of the donation request letter stated, ""Please accept the attached quarter [greeting card] as our gift to you."" A quarter was glued to the top front page in the monetary PGI condition, and a blank greeting card with accompanying envelope was placed within the folded donation request letter in the nonmonetary PGI condition. Three distinct online donation links were created to correspond to each incentive condition and placed clearly at the top of the perforated return card to give donors the option of donating online. Dependent Measures Response rateResponse rate was defined as the number of participants who made a donation in each condition. Average donationThe average donation in each condition was computed by dividing the total amount of money donated by the number of donors who contributed. ROIThe costs associated with incentives, postage, and printing were similar to or lower than the costs of comparable campaigns the charity has run in the past. We calculated the ROI for each condition by taking into consideration total donations relative to associated costs [ROI = donations received − variable costs (e.g., material cost, incentive cost [if any], business return envelope cost [for cost details, see Web Appendix F]). We then divided this number by the number of participants in each condition. Results and Discussion Campaign overviewThe response rate seven weeks after mailing (December 18, 2015, to February 5, 2016) was.56% (N = 50). The charity received a total of $1,559.50: $654.50 from the monetary PGI condition (range: $.25 to $200), $335 from the nonmonetary PGI condition (range: $10 to $50), and $570 from the no-PGI condition (range: $10 to $100), with an average donation amount of $31.19. The response rate and average donation are comparable with the national average for donor acquisition campaigns, at.65%, and $15–$45, respectively ([47]). Response rateThe response rate from the monetary PGI (54%, N = 27) was significantly higher than the nonmonetary PGI (22%, N = 11, χ2( 1) = 6.78, p =.009) and no PGI (24%, N = 12, χ2( 1) = 5.81, p =.016). No difference was found between the nonmonetary PGI and no-PGI conditions (χ2( 1) =.04, p =.83).Of note, over half the responses (N = 15) in the monetary PGI condition were returns of the quarter. Such a response is predicted by the relationship norms framework, as lower communal norms and higher exchange norms lead to a one-to-one exchange mindset and less focus on others' needs. Charities recognize that initial contributions tend to be small, and they hope to cultivate first-time donors to give more over time; thus, the contact information for everyone who ""acts upon the mailing"" ([38]), including people mailing back the quarter, may be added to the donor pool for future campaigns. For these reasons, we include donors who mailed back the quarter in the calculation of overall response rate and average donations. Nevertheless, some charities may view these responses as fundamentally different from standard donations and be interested in the results of the campaign when they are removed. We briefly discuss the donation results excluding those responses here and provide more details in Web Appendix G. Average donationOne person in the monetary PGI condition donated $200, which far surpassed the average (z > 3) and is considered an outlier ([34]). For completeness, we report the results both excluding and including the outlier. A Shapiro–Wilk normality test showed that the donation amount was not normally distributed (p <.001), so we log-transformed the data. Excluding the outlier, results from a one-way analysis of variance (ANOVA) revealed that PGI type had a significant impact on donation amount (in log-transformed values: Mmonetary =.30, SD = 1.09; Mnonmonetary = 1.42, SD =.26; Mcontrol = 1.56, SD =.35; F( 2, 46) = 12.50, p <.001; in dollars: Mmonetary = $17.48, SD = $25.88; Mnonmonetary = $30.45, SD = $16.35; Mcontrol = $47.50, SD = $34.54; see Web Appendix H). Consistent with H2, the monetary PGI led to significantly lower donations than the nonmonetary PGI (p =.001) and no PGI (p <.001). No difference was found between the nonmonetary and no-PGI conditions (p =.70). Similar results were found when the outlier was included (in log-transformed values: Mmonetary =.38, SD = 1.14; Mnonmonetary = 1.42, SD =.26; Mcontrol = 1.56, SD =.35; monetary PGI vs. nonmonetary PGI: p =.002, monetary vs. no PGI: p <.001, nonmonetary PGI vs. no PGI: p =.70; F( 2, 47) = 10.20, p <.001; in dollars: Mmonetary = $24.24, SD = $43.34; Mnonmonetary = $30.45, SD = $16.35; Mcontrol = $47.50, SD = $34.54). Notably, this effect is erased when the 15 donors who merely returned the quarter are included in the analysis (all ps >.18; for all comparisons, see Web Appendix G). ROIThe ROI for the entire campaign was −$3,766.48, with −$1,562.32 in the monetary PGI condition, −$1,477.46 in the nonmonetary PGI condition, and −$726.79 in the no-PGI condition. A Shapiro–Wilk normality test indicated that ROI was not normally distributed (p <.001), so we log-transformed the data. Due to the negative values of the data, we added a constant number a (a =.741) so the new ROI data became Y + a, where min (Y + a) =.001 ([ 7]). We found that PGI had a significant impact on ROI (in log-transformed values: Mmonetary = −2.97, SD =.34; Mnonmonetary = −.84, SD =.14; Mcontrol = −.50, SD =.13; F( 2, 8,997) = 107,748.64, p <.001,  ηp2  =.96; in dollars: Mmonetary = −$.52, SD = $4.64; Mnonmonetary = −$.49, SD = $2.07; Mcontrol = −$.24, SD = $3.66). Both monetary and nonmonetary PGIs led to significantly lower ROI compared with no PGI (both ps <.001). Monetary PGI also resulted in lower ROI compared with nonmonetary PGI (p <.001). DiscussionThe results of this field experiment provide initial support for our hypothesis that a monetary PGI (vs. a nonmonetary PGI or no PGI) will decrease average donations, while a nonmonetary PGI (vs. no PGI) will have no effect. The monetary PGI also elicited a higher response rate, driven by people who returned the $.25. Finally, enclosures of both monetary and nonmonetary PGIs led to a significantly worse ROI than no PGI. A caveat to these results is that no average donation amount difference is observed when people who merely returned the quarter are excluded in the analysis.In Studies 3–6, we further examine the effect of PGIs on donation behavior by testing our predictions in a more controlled setting. An important methodological departure from the field experiment is that all participants are asked to read the charity appeal in these subsequent studies. People do not have the option of ignoring the letter, so a major predictor of response rate (whether one opens or reads the appeal) is controlled for. The design of the lab experiments does not lend itself to accurate calculations of ROI, so we return to ROI in Study 7, in which we conduct a large-scale field experiment for a year-end campaign. Study 3In Study 3, we again partnered with a local mental health clinic to provide student participants with a physical charity letter to open and read. To measure actual behavior, participants were allowed to donate money from their study payment. We utilized monetary and nonmonetary PGIs of a different value to increase the generalizability of the findings. Participants and ProcedureOne hundred thirty-two students (71 women; Mage = 20.77 years) from a large public Midwestern university participated in this study in exchange for $5. Participants were randomly assigned to one of three conditions: a charity appeal with a monetary PGI ($.50), nonmonetary PGI (a higher-quality greeting card), or no PGI. We conducted a pretest to ensure there was no difference in either the perceived cost or subjective benefit of the two PGIs (both ps >.43) (for details, see Web Appendix D). All instructions and responses were on paper. Participants received five dollars as well as course credit for their participation and were instructed to read the charity letter (see Web Appendix E) and respond to the questions in the survey booklet. Responses from two participants were unusable—one participant refused the payment before the study started and another participant did not complete the survey—leaving a final sample of 130. Dependent Measures Response rateResponse rate was defined as the number of participants who made a donation in each condition. Average donationParticipants were asked how much of the money they received from their participant payment and charity appeal, if applicable, they would like to donate to the charity. The average donation in each condition was computed by dividing the total amount of money donated by the number of participants in each condition. Average donation percentageBecause the total amount of money available for donation differed across conditions—participants in the monetary PGI condition had $5.50 to donate (their $5.00 study payment, plus the $.50 PGI), whereas those in the nonmonetary and control conditions only had $5.00—we calculated donation percentage (donation amount divided by the total amount of money available for donation) in addition to the average donation amount. Results and Discussion Response rateNo response rate difference was found between any of the three conditions (Nmonetary = 30, Nnonmonetary = 29, Ncontrol = 32; all ps >.64). Average donationResults from a one-way ANOVA showed a marginal effect of PGI on average donations (Mmonetary = $1.58, SD = $2.07; Mnonmonetary = $2.37, SD = $2.14; Mcontrol = $2.50, SD = $2.14; F( 2, 127) = 2.39, p =.096,  ηp2  =.04). Consistent with H2, the monetary PGI led participants to donate less than no PGI (p =.045), and marginally less than the nonmonetary PGI (p =.086). There was no difference between nonmonetary and no PGI (p =.78). Average donation percentageResults from a one-way ANOVA revealed a main effect of PGI type on donation percentage (Mmonetary = 28.75%, SD = 37.58%; Mnonmonetary = 47.44%, SD = 42.77%; Mcontrol = 50%, SD = 42.81%; F( 2, 127) = 3.44, p =.035,  ηp2  =.05). Participants in the monetary PGI condition donated less of their total funds than participants in the nonmonetary PGI (p =.037) and no-PGI (p =.017) conditions. Similar donation percentages were found for people in the nonmonetary PGI and no-PGI conditions (p =.77). DiscussionResults from Study 3 provide further evidence that enclosing PGIs does not result in higher donations. Consistent with Study 2, nonmonetary PGIs did not increase donations relative to no PGI, and enclosing monetary PGIs resulted in lower average donations than enclosing nonmonetary PGIs and no PGI. However, the latter effect is significantly only for donation percentage and merely marginal for donation amount, which may limit the inferences for this study. In addition, we did not find an effect of PGI on response rate, perhaps because everyone was forced to read the letter. We discuss this possibility subsequently.Studies 4–6 examine the effect of PGIs on donation behavior as well as the underlying mechanism of relationship norms by utilizing both mediation and moderation techniques, while ruling out alternative explanations. Unlike other the studies in this article, Studies 4–6 use hypothetical scenarios whereby participants imagine receiving a PGI. A substantial amount of research, especially in economics, has reported substantively little difference between hypothetical and real scenarios ([12]; [16]). For example, [ 6] conducted two experiments, one in which participants were given actual money to spend, and another in which they were asked to imagine they were given money. They found that the ""the average subject behaves essentially the same"" in these two conditions (p. 1783). Such findings suggest it is valuable to take a multimethod approach to decision-making tasks, particularly when limited resources would otherwise constrain the number of studies that can be run and hypotheses tested. Study 4In Study 4, we examine the mediating role of relationship norms on donation behavior. As the pilot test demonstrates, charities are inherently perceived as communal organizations. On the one hand, we propose that monetary PGIs diminish communal norms and increase exchange norms, resulting in lower average donations. On the other hand, nonmonetary PGIs are expected to produce no net change in communality. We also test several alternative explanations in this study. First, previous research suggests that inferences of manipulative intent decrease message persuasiveness ([ 8]). It is possible that people perceive greater manipulative intent when charities include monetary (vs. nonmonetary) PGIs, which leads them to be less supportive of the charity. Second, PGI type may influence perceived charity efficiency, which has been shown to affect donation decisions ([56]). Specifically, people may believe that charities that send money with their donation appeals use their resources less efficiently than those that do not. Finally, it is possible that manipulative intent or charity efficiency influence perceived communality in serial, which results in lower donations. Participants and ProcedureOne hundred fifty-three students (96 women; Mage = 20 years) from a major Midwestern university participated in the study in exchange for course credit. Participants were randomly assigned to read a charity letter (Web Appendix E) enclosing a monetary PGI ($.25), a nonmonetary PGI (a low-value greeting card), or no PGI. After reading the charity letter, participants responded to the relationship norm items, followed by the donation request, and then the measures of manipulative intent and charity inefficiency. Dependent MeasuresResponse rate was calculated the same way as in Study 3. Average donationParticipants indicated their donations by dragging a slider anchored at $0 and $50, with a write-in option labeled ""other"" for anyone willing to donate more than $50. Relationship normsThough some research suggests that communal and exchange norms are orthogonal ([27]), others conceptualize them as opposite ends of the same scale ([ 2]; [11]). In our research, communal and exchange norms are always highly negatively correlated (rs < −.46). We thus use [ 2] net communality for our mediation analyses and report results with each scale separately in Web Appendix I for completeness.We measured communal norms (e.g., ""This organization is concerned about other people's welfare""; α =.79; for all items, see Web Appendix J) and exchange norms (e.g., ""Whenever this organization gives or offers something, it expects something in return""; α =.88) using four items each. We reverse-coded the exchange norms and combined them with communal norms (Pearson's r = −.56) to form the net communality score (Cronbach's alpha =.87). The higher the net communality score, the higher the communal relative to exchange norms. Manipulative intent (α =.87) and charity inefficiency (α =.75) (Web Appendix J) were measured on a seven-point Likert scale (1 = ""completely disagree,"" and 7 = ""completely agree""). Results and DiscussionPrior to running the analyses, we eliminated participants who failed the attention check (N = 24; Web Appendix D), leaving a final sample of 129 participants (84 women; Mage = 20.32 years). Response rateChi-squares tests showed that the number of participants who indicated a nonzero donation did not differ between any of the three conditions (Nmonetary = 36, Nnonmonetary = 41, Ncontrol = 42; all ps >.43). Average donationResults from a one-way ANOVA revealed that PGI type had a marginal effect on average donations (Mmonetary = $11.83, SD = $10.89; Mnonmonetary = $17.93, SD = $17.41; Mcontrol = $19.82, SD = $18.24; F( 2, 126) = 2.84, p =.062,  ηp2  =.04). Consistent with H2, donations were significantly lower for the monetary PGI than no PGI (p =.023) and marginally lower for the monetary than the nonmonetary PGI (p =.083). No donation difference was observed between the nonmonetary and no-PGI conditions (p =.60). Relationship normsWe found that PGI type had a significant impact on the net communality score (Mmonetary = 4.63, Mnonmonetary = 5.12, Mcontrol = 5.44; F( 2, 126) = 5.56, p =.005,  ηp2  =.08; for SDs, see Web Appendix I). Confirming H3, the monetary PGI led to lower net communality than the nonmonetary PGI (p <.05) and no PGI (p =.001). No difference was found between the nonmonetary and no-PGI conditions (p >.18). Specifically, PGI type had a significant impact on communal norms (F( 2, 126) = 8.76, p =.002; see means and SDs in Web Appendix I). The monetary PGI led to lower communal norms than the nonmonetary PGI (p =.009) and no PGI (p =.001). No difference was found between the nonmonetary PGI and no PGI (p =.35). The effect of PGI type on exchange norms was marginal (F( 2, 126) = 2.88, p =.06). Only the monetary PGI condition resulted in higher exchange norms than the no-PGI condition (p =.018). Manipulative intentWe found that PGI type had a marginal effect on manipulative intent (Mmonetary = 3.46, Mnonmonetary = 2.98, Mcontrol = 2.88; F( 2, 126) = 2.70, p =.071,  ηp2  =.08; for SDs, see Web Appendix I). The monetary PGI led people to perceive the charity as marginally more manipulative than the nonmonetary PGI (p =.073), and significantly more manipulative than no PGI (p =.03). There was no difference between nonmonetary and no PGI (p =.70). Charity inefficiencyResults revealed a significant effect of PGI on perceptions of charity inefficiency (Mmonetary = 4.39, Mnonmonetary = 3.45, Mcontrol = 3.55; F( 2, 126) = 7.98, p =.001,  ηp2  =.11). The monetary PGI led the charity to be perceived as more inefficient compared with the nonmonetary PGI (p <.001) and no PGI (p =.001). No difference was found between the nonmonetary and no-PGI conditions (p =.69). Mediation Analysis Relationship normsWe conducted mediation models with 10,000 bootstrap samples ([23]; PROCESS v3.1. Model 4). Because there were three different incentive conditions (monetary, nonmonetary, and no PGI), we dummy-coded the three groups by creating two new dummy variables, MNM (monetary PGI vs. nonmonetary PGI) and MC (monetary PGI vs. no PGI). Both variables used the monetary PGI condition as the reference group ([23]). Consistent with H4, results showed that the indirect effect of net communality was significant for both comparisons (monetary vs. no PGI: B = 4.54, 95% confidence interval [CI] = [1.56, 8.47]; monetary vs. nonmonetary: B = 2.75, 95% CI = [.22, 5.89]). We also performed single/dual mediation analyses for communal norms and exchange norms (see Web Appendix I). Alternative mediatorsPerceived manipulative intent alone did not mediate the effect of PGI type on donation (for details, see Web Appendix I). In the dual mediation model including net communality and manipulative intent, only the indirect effect of net communality was significant (monetary vs. no PGI: B = 3.41, 95% CI = [.85, 6.99]; monetary vs. nonmonetary: B = 2.07, 95% CI = [.09, 4.65]). Perceived charity inefficiency did produce a significant indirect effect on its own (monetary vs. no PGI: B = 3.41, 95% CI = [1.10, 6.49]; monetary vs. nonmonetary: B = 3.82, 95% CI = [1.45, 6.80]). However, the indirect effect of charity inefficiency became nonsignificant once net communality was included in the model, whereas net communality remained a significant mediator (monetary vs. no PGI: B = 3.75, 95% CI = [1.16, 7.66]; monetary vs. nonmonetary: B = 2.27, 95% CI = [.16, 5.25]). We also conducted multiple mediation and serial mediation analyses for PGI type → charity inefficiency/manipulative intent → net communality score → donation (see Web Appendix I). DiscussionStudy 4 examined potential underlying mechanisms for the effect of PGIs on donation behavior. Our results suggest that a monetary PGI, compared with a nonmonetary or no PGI, leads people to perceive the charity as lower on net communality, resulting in lower average donations. However, that the average donation amount difference is marginal between the monetary and nonmonetary PGI conditions, so H2 is only partially supported. We also examined manipulative intent and charity inefficiency as alternative explanations. Net communality remained a significant mediator after controlling for these constructs, while manipulative intent and charity inefficiency did not, suggesting that the simultaneous change in communal norms and exchange norms predicts donation behavior above and beyond the effects of manipulative intent and charity inefficiency. Further analyses on the relationship between these constructs reveal evidence of serial mediation, such that perceived higher charity inefficiency leads to lowered communality, resulting in lower donations.Results from Studies 2–4 have provided converging evidence that enclosing low-value monetary PGIs in charity appeals decreases communal norms and increases exchange norms, leading to lower average donations compared with nonmonetary PGIs or no PGI. In Study 5, we provide another test of our theory by varying the value of the PGI. Study 5According to theory on relationship norms, when exchange norms are dominant, the degree of benefits or favors returned by the recipients is contingent on the level of benefits received, leading people to behave in a quid pro quo manner ([10]). When communal norms are dominant, donations are need based rather than incentive based, so incentives should not play as pivotal a role in donation decisions ([37]). Because monetary PGIs evoke more exchange norms and less communal norms, they should lead people to make donations based more on how much they received than the perceived needs of the charity ([37]). In other words, high- (vs. low-) value monetary PGIs should result in higher donations. However, the value of a nonmonetary gift should have little impact on donations because, as explained previously, no change in net communality is expected. This is true regardless of the value of the gift. Moreover, considering that the amount of money to be reciprocated increases while need-based donations remain stable, it is possible that donations elicited by monetary PGIs will be greater than those elicited by nonmonetary PGIs at higher incentive values. Formally, we propose the following hypothesis: H5:  The effect of monetary and nonmonetary PGIs on donations is moderated by incentive value, such that a high-value monetary PGI leads to greater average donations than a low-value monetary PGI, whereas there will be no difference in average donations for high- vs. low-value nonmonetary PGIs. (b) A nonmonetary (vs. monetary) low-value PGI elicits higher donations, but this pattern is attenuated or reversed for high-value PGIs.In testing this hypothesis, we also address an alternative explanation for the effect—that reminders of money lead people to be less prosocial (e.g., [52]). Consistent with this literature, receiving monetary PGIs should lead people to be less charitable and thus donate less. However, this theory does not make different predictions for high- versus low-value monetary PGIs. If the effect of PGI type on donation behavior is solely driven by the psychological consequences of money rather than the simultaneous movement of communal and exchange norms, we should expect a high-value monetary PGI to lead to lower donations than a comparable value nonmonetary PGI. The relationship norms framework, in contrast, would predict the opposite finding. Participants and ProcedureOne-hundred thirty-nine participants (73 women; Mage = 37.07 years) were recruited from Amazon Mechanical Turk (MTurk). The study used a 2 (incentive type: monetary vs. nonmonetary) × 2 (PGI value: low [$.25] vs. high [$2.50]) between-subjects design. We selected the high-value incentive after an examination of over 100 charity appeals found a maximum monetary PGI amount of $2.50. Participants in the low-value monetary condition imagined receiving a quarter ($.25) while those in the high-value monetary condition imagined receiving two $1 coins and two quarters (Web Appendix E). Participants in the low-value nonmonetary condition imagined receiving one greeting card and those in the high-value nonmonetary condition imagined receiving eight greeting cards. A pretest showed that the two low-value monetary and nonmonetary PGIs and the two high-value monetary and nonmonetary PGIs did not differ on subjective value or perceived cost to the charity (see Web Appendix D).In the main study, participants read a charity letter from a fictitious food pantry (for stimuli, see Web Appendix E). After reading the charity appeal, participants were told that they would automatically be entered into a lottery for $10 as an extra show of appreciation for participating in the study. Then they were asked how much they would be willing to donate from their winnings by dragging a slider anchored at $0 and $10.Response rates and average donations were calculated the same way as previous studies. Results and Discussion Response rateNo interaction was found between PGI type and value on response rate (p =.78). No difference was found between low versus high-value PGI conditions or within high- and low-value PGI conditions (low value: Nmonetary = 32, Nnonmonetary = 31; high value: Nmonetary = 30, Nnonmonetary = 31; all ps >.20). Average donationConsistent with H5, results from a two-way ANOVA revealed a significant interaction between PGI type and PGI value (F( 1, 135) = 10.30, p =.002,  ηp2  =.07; Web Appendix K). Consistent with our prediction that monetary PGIs lead people to give based on how much they received rather than the charity's need, people who received $2.50 donated significantly more than those who received $.25 (Mlow-value monetary = $3.75, SD = $3.03; Mhigh-value monetary = $6.32, SD = $3.21; p =.003). However, the high-value nonmonetary PGI performed no better than the low-value nonmonetary PGI (Mlow-value nonmonetary = $5.59, SD = 4.02; Mhigh-value nonmonetary = $4.36, SD = $3.51; p =.14). This is in line with our hypothesis that nonmonetary PGIs maintain perceptions of the charity as communally oriented and thus prompt norms of need-based helping rather than repayment of benefits.Note that average donation for the high-value nonmonetary PGI appeal was directionally lower than the low-value nonmonetary PGI appeal (p =.14). One possible explanation is that, unlike money, greeting cards have diminishing marginal utility, leading to scope insensitivity (e.g., [29]). If this is the case, a set of disparate incentives should lead to greater perceived gift value and may increase donations. We tested this hypothesis in a follow-up study with three conditions (N = 273): one greeting card, eight greeting cards, or eight different gifts of similar cost including a pen, a note pad, a binder clip, a card, and so on (see Web Appendix E, Study 5). Results revealed no donation difference among the three conditions (Mone card = $4.52, Meight cards = $3.95, Meight gifts = $4.69; all ps >. 14). Thus, the inability of high-value nonmonetary incentives to increase donations is less likely to be due to scope insensitivity than the failure to increase communal (and decrease exchange) norms.Comparing between PGI types, $.25 led to significantly lower donations than one greeting card (p =.026). The effect is reversed for higher-value monetary and nonmonetary PGIs, such that $2.50 elicited significantly more donations than eight greeting cards (p =.024), although we suspect that this effect can be either attenuated or reversed depending on the value of the incentives. In practice, enclosing a high-value monetary PGI may not be desirable or feasible for charities, considering the high cost as well as the low average return.Study 5 helps establish relationship norms as a primary contributor to donation decisions. In the next study, we consider a potential low-cost way to improve average donations. Charitable organizations are highly strategic about the choice of wording in their appeals. So far, we have used the common phrasing of the PGI as a gift (e.g., Smile Train uses the statement ""We have enclosed a world map as our free gift to you."") Yet charities may be interested in the comparative effectiveness of framing the gift in a different way. For example, Obis (an international eye care charity) encloses a nickel in its charity appeal and states, ""This nickel can help restore a child's vision."" Food for the Poor says in its charity appeals, ""Please return these coins along with your gift."" A reasonable question is whether such statements mitigate or enhance the changes in exchange and communal norms from monetary PGIs. In the next study, we explore the effect of framing on relationship norms, response rate, average donations, and the alternative explanations of charity inefficiency and manipulative intent. Doing so also allows us to test the generalizability and external validity of the focal effect. Study 6Study 6 examines the effect of framing the monetary gift in various ways. We approached this study in an exploratory manner with the goal of testing two competing hypotheses. It is possible that phrasing a monetary PGI in a more communal way will prevent exchange norms from increasing and communal norms from decreasing. For instance, the phrasing ""this nickel can help restore a child's vision"" may lead people to interpret the enclosure of the monetary PGI as a demonstration to illustrate that even a little money can help the cause, which is compatible with the communal nature of the charity. However, this is an empirical question, as previous research suggests the effect of money on marketplace norms is strong and consistent ([24]; [30]; [53]). Phrasing the gift in a communal way may do little to change donor behavior if a monetary mindset has already been activated. Indeed, [36] demonstrated that it is possible to improve consumer sentiment by reframing commercial marketing strategies as communal, unless consumers were already in a persuasion frame of mind. Thus, we suspect that, regardless of the phrasing for the monetary PGI, its mere inclusion will consistently lead to lower donations compared with a nonmonetary PGI and no PGI. Participants and ProcedureFive hundred seven MTurk workers (282 women; Mage = 37.11 years) were randomly assigned to one of five conditions in which they saw a charity appeal from the fictitious food charity in Study 5. Three conditions manipulated phrasing of the monetary PGI: ( 1) ""This $.25 can help provide a meal"" (monetary-help), ( 2) ""Please return this $.25 along with your donation"" (monetary-return), and ( 3) ""This $.25 is a gift to you"" (monetary-gift). We chose these three monetary PGI phrasings on the basis of those used in existing appeals. The other two conditions were the same charity appeals enclosing either a low-value nonmonetary PGI (""This greeting card is a gift to you"") or no PGI. All aspects of the charity appeals, besides the PGI, were identical across conditions (Web Appendix E). As in Study 5, participants were told they would be automatically entered into a lottery for $10 and asked how much they would be willing to donate if they won ($0 to $10). Afterward, they answered the same measures from Study 3 in the following order: perceived communality (communal [α =.85] and exchange [α =.83] norms were negatively correlated; Pearson's r = −.46), manipulative intent (α =.86), and charity inefficiency (α =.80). Note that rather than measuring net communality before the dependent variable (as in Study 4), we measured it after the donation dependent variable to rule out potential order effects.Response rates and average donations were calculated the same way as previous studies. Results and Discussion Response rateA chi-square test showed an unexpected higher response rate for the nonmonetary PGI condition (Nnonmonetary = 99) than for all three monetary phrasing conditions (Nmonetary help = 87, Nmonetary return = 86, Nmonetary gift = 90; all ps <.04). The response rate for the no-PGI condition (Ncontrol = 93) was marginally higher than that of the monetary-return condition (p =.052). No other response rate difference was found (all ps >.17). Average donationAn ANOVA revealed a significant main effect of PGI type on average donations (Mmonetary help = $3.86, SD = $3.04; Mmonetary return = $4.04, SD = $3.37; Mmonetary gift = $4.28, SD = $3.03; Mnonmonetary = $5.22, SD = $3.39; Mcontrol = $5.13, SD = $3.53; F( 4, 502) = 3.72, p =.005,  ηp2  =.03). We found no difference in donations among the three monetary PGI conditions (all ps >.36). Furthermore, each of the three monetary PGI conditions led to lower donations than the nonmonetary PGI (all ps <.05) and no-PGI conditions (monetary-help vs. no PGI: p =.007; monetary-return vs. no PGI: p =.02; monetary-gift vs. no PGI: p =.066). We found no donation difference between the nonmonetary and no-PGI conditions (p >.80). Relationship normsAn ANOVA showed that PGI type had a significant impact on the net communality score (Mmonetary help = 5.07, SD = 1.20; Mmonetary return = 4.82, SD = 1.29; Mmonetary gift = 4.71, SD = 1.15; Mnonmonetary = 5.20, SD = 1.16; Mcontrol = 5.29, SD = 1.11; F( 4, 502) = 4.45, p =.002,  ηp2  =.03). Monetary return and monetary gift led to lower net communality than nonmonetary PGI and no PGI (all ps <.04). Unexpectedly, monetary help led to higher net communality than monetary gift (p =.03). We discuss this result subsequently. No difference was found between any other conditions (all ps >.13). When relationship norms were examined separately, PGI type had a significant effect on both communal (F( 4, 502) = 2.80, p =.025,  ηp2  =.02) and exchange norms (F( 4, 502) = 4.33, p =.002,  ηp2  =.03, see Web Appendix I for means and SDs and pairwise comparisons). Manipulative intentThe effect of PGI type on manipulative intent was significant (Mmonetary help = 2.83, Mmonetary return = 3.75, Mmonetary gift = 3.37, Mnonmonetary = 2.66, Mcontrol = 2.46; F( 4, 502) = 12.71, p <.001,  ηp2  =.09; for SDs, see Web Appendix I). All three monetary PGIs led the charity to be perceived as more manipulative than the no-PGI condition (monetary help vs. no PGI: p =. 09; all other ps <.01). There was no difference between the no-PGI and nonmonetary PGI conditions (p =.34). In addition, the nonmonetary PGI was perceived as less manipulative than the monetary-gift and monetary-return conditions (ps <.01). There was no difference between the nonmonetary PGI and monetary-help condition (p =.44). Indeed, the monetary-help condition elicited lower manipulative intent perceptions than both the monetary-gift and monetary-return conditions (ps <.02). Charity inefficiencyWe found that PGI type had a significant impact on perceived charity inefficiency (Mmonetary help = 4.03, Mmonetary return = 4.44, Mmonetary gift = 4.23, Mnonmonetary = 3.64, Mcontrol = 3.50; F( 4, 502) = 6.59, p <.001,  ηp2  =.05). Participants in all three monetary PGI conditions regarded the charity as more inefficient than the nonmonetary PGI (monetary help vs. nonmonetary PGI: p =. 079, all other ps <.01) and no-PGI (all ps <.02) conditions. Participants in the monetary-return condition perceived the charity as marginally more inefficient than those in the monetary-help condition (p =.058). No difference was found between any other conditions (all ps >.32). Mediation Analysis Relationship normsWe conducted mediation analyses using the same mediation procedure in Study 4. The indirect effect of net communality was significant collapsing across the three monetary phrasing conditions: (monetary vs. no PGI: B =.34, 95% CI = [.13,.59]; monetary vs. nonmonetary PGI: B =.27, 95% CI = [.05,.52]; (for mediation analyses for each monetary phrasing, see Web Appendix I). Alternative mediatorsCombining the three monetary phrasings, both manipulative intent and charity inefficiency mediated the effect of PGI type on donations (see Web Appendix I). However, their indirect effects were insignificant when net communality was included in the model, while net communality remained a significant mediator (net communality and manipulative intent: monetary vs. no PGI: B =.28, 95% CI = [.09,.52]; monetary vs. nonmonetary PGI: B =.22, 95% CI = [.04,.45]; net communality and charity inefficiency: monetary vs. no PGI: B =.29, 95% CI = [.10,.52]; monetary vs. nonmonetary: B =.23, 95% CI = [.04,.46]). For serial mediation results, see Web Appendix I.In summary, this study demonstrates that describing monetary PGIs in various ways does not make a difference in terms of donations. We did find an unexpected effect in the monetary-help condition, in that it leads people to perceive the charity as more communal than those in the monetary-gift condition. A follow-up study (N = 166 MTurkers) revealed no differences among the two phrasings on perceived impact of their donations, perceived efficacy, perceived self-concept (moral/ethical), positive/negative affect, and empathy (all ps >.18). We suspect that the phrasing in the monetary-help condition maintains the perceived communality of the charity by focusing on the victims and emphasizing the compassionate nature of the organization. However, the negative effect of charity inefficiency produced by the coin was strong enough to overcome this perception and lower overall donations compared with the nonmonetary PGI and no-PGI conditions, as demonstrated by the serial mediation results (PGI type → charity inefficiency → net communality → donations; see Web Appendix I). These findings suggest that enclosing monetary PGIs leads to lower average donations, regardless of how monetary PGIs are phrased. Study 7In Study 7, we again partnered with the mental health clinic in Study 2 to launch a large-scale direct mail campaign. Unlike Study 2, this study targets existing donors in a year-end campaign. The goals of this study are threefold. First, we examine whether the PGI effects generalize to a warm mailing list (i.e., people who already have a relationship with the charity). We expect that such a sample will result in a higher overall response rate than Study 2 because people are more likely to open mail from a known sender. However, effects on relationship norms, and thus average donations, should be the same. Second, we test another alternative explanation for the effect of PGIs on donations—the anchoring and adjustment heuristic ([51]). This hypothesis suggests that people who receive a monetary PGI use the coin(s) as a reference point or anchor and thus donate less than when a low-value numerical anchor is not present. In this study, we include the same anchor in all the charity appeals. If the results are due to anchoring rather than relationship norms, this should erase the effect of PGI type on donations. Third, we include a different nonmonetary PGI to make sure the effect is not an artifact of the gift we chose. Although greeting cards are an externally valid PGI currently used by charities, they are also thin and made out of paper, making them vulnerable to being overlooked or accidentally discarded. Furthermore, a greeting card may be seen as a communal gift. Therefore, in this study, we use a magnet as the nonmonetary PGI, which is more of a neutral incentive and less likely to go unnoticed in an envelope. A pretest confirmed that the magnet and coin did not elicit differences in perceived costs or benefits (Web Appendix D). Materials Mailing listWe randomly selected 2,643 donors from the charity's existing donor list and assigned them to one of the three PGI conditions: monetary ($.25), nonmonetary (a magnet), or no PGI. Business reply return envelopesConsistent with Study 2, a standard business reply return envelope was enclosed within each letter. We printed a unique code on each return envelope to differentiate the three PGI conditions. Window envelopes, donation request letters, and incentivesWe held the content of all three versions of the letter constant. To examine the anchoring effect, we included a sentence at the top left corner above each person's name: ""Even $.25 can help prevent suicide"" (see Web Appendix E, Study 7). This sentence replaced the one used in the previous experiments to introduce the PGI (e.g., ""Please accept the enclosed $.25 [magnet] as a gift to you""). To ensure that participants saw the incentives, both the quarter and the magnet were displayed through the clear windows of the envelopes. As in Study 2, the charity created three distinct online donation links for each incentive condition so that donors could choose to give online. Dependent Measures Response rateResponse rate was defined as the number of participants who made a donation in each condition. Average donationThe average donation in each condition was computed by dividing the total amount of money donated by the number of donors who contributed. ROIAs in Study 2, we calculated the ROI for each condition by considering total donations in relation to associated costs, divided by the number of recipients (for cost details, see Web Appendix F). Results and Discussion Campaign overviewThe response rate a little over eight weeks after mailing (November 4, 2019, through December 31, 2019) was 2.50% (N = 66). We received a total of $7,541.25 ($1,756.25 from the monetary PGI condition (range: $.25 to $500), $2,415 from the nonmonetary PGI condition (range: $10 to $550), and $3,370 from the no-PGI condition (range: $20 to $500), with an average donation amount of $114.26. Response rateWe found no difference in response rate among the three conditions (33.33%, Nmonetary = 22; 37.88%, Nnonmonetary = 25; 28.79%, Ncontrol = 19; all ps >.35). Two people in the monetary PGI condition merely returned the quarter. We report the results excluding these two participants in Web Appendix G. Average donationA Shapiro–Wilk normality test revealed that the donation data were not normally distributed (p <.001), so we log-transformed the data. A one-way ANOVA showed a significant effect of PGI type on donation amount (in log-transformed value: Mmonetary = 1.49, SD =.80; Mnonmonetary = 1.78, SD =.42; Mcontrol = 2.03, SD =.48; F( 2, 63) = 4.34, p =.017,  ηp2  =.12; in dollars: Mmonetary = $79.83, SD = $117.01; Mnonmonetary = $96.60, SD = $117.06; Mcontrol = $177.37, SD = $169.18; see Web Appendix H). Monetary PGI led to significantly lower donations than no PGI (p =.005) and marginally lower donations than the nonmonetary PGI (p =.097). No donation difference was found between the nonmonetary and no PGI (p =.17). ROIThe amount raised by the entire campaign was $4,597.04, with $663.31 in the monetary PGI condition, $1,349.66 in the nonmonetary PGI condition, and $2,584.07 in the no-PGI condition. A Shapiro–Wilk normality test showed that the ROI data were not normally distributed (p <.001), so we log-transformed the data. As in Study 2, we added a constant number a (a = 1.241) to offset the negative values. Results showed that PGI had a significant impact on ROI (in log-transformed values: Mmonetary = −2.89, SD =.71; Mnonmonetary = −1.42, SD =.55; Mcontrol = −.40, SD =.37; F( 2, 2640) = 4,376.73, p <.001,  ηp2  =.77; in dollars: Mmonetary = $.75, SD = $21.96; Mnonmonetary = $1.53, SD = $25.13; Mcontrol = $2.94, SD = $35.36). Pairwise comparisons showed that both monetary and nonmonetary PGIs led to a significantly lower ROI compared with no PGI (both ps <.001). The monetary PGI also led to a significantly lower ROI compared with the nonmonetary PGI (p <.001). DiscussionThe results of this study suggest that the effect of PGI type on donations is unlikely to be driven by the anchoring and adjustment heuristic. The effect held when an explicit anchor was present in the appeal. Consistent with the results in previous studies, enclosing a monetary PGI led to lower average donations, and enclosing a nonmonetary PGI was no more effective than not including a PGI. In addition, as in Study 2, including either a monetary or nonmonetary PGI led to a lower ROI than not including a PGI. Unlike Study 2, however, the response rate did not differ by PGI condition. We postulate that this is because recipients in this study were already familiar with the charity, having donated to them before. We discuss this possibility in the general discussion. A weakness of this study is that we did not include a manipulation check of the anchoring manipulation due to the difficulty of surveying donors in the field. However, an additional test of the anchoring effect using a different nonmonetary PGI also failed to find support for this alternative (Web Appendix L). General DiscussionCharities utilize different marketing strategies in hopes of fundraising and enlarging their donor base ([39]). Enclosing low-value PGIs is one strategy that has been extensively practiced by charities ([41]). Through a combination of field and lab studies, the present research examines the effectiveness of utilizing this strategy on opening rate, response rate, donation behavior, and relationship norms.In seven studies, we examine how and why donors respond to different types of PGIs. Study 1 examines the effect of PGI type on opening rate and shows that enclosing a monetary PGI (vs. nonmonetary or no PGI) leads to significantly more people opening and reading the charity letter. Study 2, a donor acquisition campaign, finds that ( 1) a monetary PGI leads to a higher response rate than both a nonmonetary PGI and no PGI, ( 2) a monetary PGI leads to lower average donations compared with a comparable value nonmonetary PGI and no PGI, and ( 3) both monetary and nonmonetary PGIs lead charities to suffer higher net financial loss than no PGI in donor acquisition efforts. Results from Study 3 provide further evidence that enclosing a monetary PGI leads people to donate less money than a nonmonetary PGI and no PGI, and enclosing a nonmonetary PGI is no more effective than when no PGI is included. Results from Study 4 show that reduced donations in the monetary incentive condition are due to decreased net communality levels. Study 4 also tests manipulative intent and charity inefficiency as alternative explanations and demonstrate a significant effect of net communality over and above their influence. Consistent with the relationship norms framework, Study 5 shows that the effect of PGI type is moderated by incentive value. A high-value monetary PGI leads to more donations than a low-value monetary PGI, whereas there is no difference for high- versus low-value nonmonetary PGIs. Study 6 shows that phrasing a monetary PGI in various ways consistently leads to lower donations than a nonmonetary PGI or no PGI. Finally, Study 7 shows that a donation campaign for recurring donors utilizing a different nonmonetary PGI supports the conclusions that a monetary PGI decreases average donations compared with a nonmonetary PGI or no PGI. Study 7 also tested and ruled out the anchoring and adjustment heuristic by including an explicit anchor in all the charity appeals. Theoretical ImplicationsThe current research contributes to the literature on relationship norms in two ways. First, research on relationship norms has focused on for-profit entities ([ 2]; [54]). In contrast, the current research examines how these norms affect consumer perception and behavior in the context of nonprofit organizations. For-profits and nonprofits are governed by different relationship norms, such that for-profit companies operate on exchange norms, whereas the default relationship norms for nonprofit companies are communal. This fundamental difference affects how people respond to these organizations at baseline, and indicates that using incentives as a marketing strategy has disparate effects on consumer behavior for these two categories of organizations. Second, instead of manipulating the salience of the communal and exchange norms through hypothetical scenarios, as is common in previous research, we examine how strategies that nonprofits currently use and have full control over organically influence the salience of these norms. Our results suggest that PGIs affect communal and exchange norms simultaneously and that these norms have a dual impact on donations.We add to recent work on how people make inferences about an organization based on superficial elements in direct mail campaigns ([50]). We show that PGIs serve as cues about the organization, influencing communal and exchange norms. Lower net communality from monetary PGIs leads people to behave in a quid pro quo manner and be less likely to donate based on perceived need. This is true even though the net communality score remained significantly above the midpoint of the scale (Study 4: Mmonetary = 4.63, p <.001; Study 6: Mmonetary = 4.86, p <.001). Finally, we contribute to the PGI literature (e.g., [35]) by moving beyond general conclusions about all incentives to a nuanced account of how and why different types of PGIs affect donation behaviors. Managerial ImplicationsThis research addresses the question of whether including PGIs is a worthwhile strategy for charities. The ultimate answer is that it depends on the goal of the donation campaign. Next, we describe how PGIs affect various campaign related outcomes. Opening rateCharities often need to find ways to enter potential donors' consideration set ([48]). If the goal of the campaign is to raise awareness and to help the charity gain exposure, enclosing a monetary PGI appears to be an effective strategy. Our research suggests that enclosing a monetary PGI persuades recipients not only to open the letter, but to read it. Notably, the charity we used to test this hypothesis was fictional and thus unfamiliar to all participants. It is reasonable to assume that the decision to open a letter will be multiply determined when donors are already aware of the charity. A PGI (or its absence) may not be as powerful of a contributing factor for existing donors. However, given that there are over a million charities in the United States ([26]), many of which are unknown to a significant number of Americans, this effect is an important consideration for many. Response rateEnclosing a monetary PGI may help organizations achieve the goal of enlarging the donor pool for future campaigns. Results from the donor acquisition campaign (Study 2) showed that a monetary PGI led to a significantly higher response rate than both a nonmonetary PGI and no PGI, whereas there were no differences between the nonmonetary and no-PGI conditions. However, 15 of 27 people in the monetary PGI condition returned/donated the enclosed $.25, and charities need to decide whether to consider them potential future donors.The results for response rate were not consistent across all studies. We found an unexpectedly high response rate for the nonmonetary PGI condition in Study 6 and no response rate difference in any other study, including the annual campaign (Study 7). These results should be interpreted in light of the results in Study 1, in which we found that the opening rate was higher for letters containing a monetary PGI for an unfamiliar charity. This could have led to the higher response rate in the first field study, which utilized a cold mailing list. Donors in Study 7 were already familiar with the charity, having donated to them before, so opening rate for this segment should be higher overall, and less likely to be dependent on the presence of a gift. The same line of reasoning applies to the lab experiments. All participants were forced to open and read the letter, negating any effect of opening rate on donation likelihood. In other words, opening the letter is a necessary condition for responding to it. However, as discussed previously, the number of people who open the letter is partially determined by whether the donor is familiar with the organization. Thus, monetary PGIs may be especially beneficial for charities that are lesser known or just starting to build their donor list. Average donationIf the primary goal of the campaign is to maximize the contribution of each donor, results consistently show that inclusion of monetary PGIs is a bad idea, regardless of how the incentive is couched. Nonmonetary PGIs perform no better than no incentives. In fact, including them, even when we increased their value—from 4 to 8 to 12 cards (M1 card = $5.73, M4 cards = $4.84, M8 cards = $4.84, M12 cards = $4.73; all ps >.20) or offered a greater variety of gifts (Study 5) led to the same average donations as when no PGI was included. Note that both Studies 3 and 4 (student lab studies) yielded marginally significant donation differences between the monetary and nonmonetary PGI conditions, which could be because participants were not donating their own money. The effects are more pronounced in other studies. Total donationsCharities may have the goal of raising the most money possible. Across all seven studies, the no-PGI appeals resulted in the most money raised ($5,454.88), followed by the nonmonetary PGI ($4,178.57), and the monetary PGI ($3,392.67). The effectiveness of PGIs on total donations may depend on who the recipients are. In Study 2, the most donations came from the monetary PGI appeal, whereas in Study 7, the no-PGI appeal raised the most money. This suggests a monetary PGI is more effective for nondonors (perhaps due to a higher opening rate), while no PGI is needed to maximize overall donations from existing donors. ROIIf the goal of the campaign is to minimize losses or yield a higher ROI, it would be more effective to not include a PGI. Both monetary and nonmonetary PGIs resulted in a significantly lower ROI than no PGI in both field experiments. Specifically, in the donor acquisition campaign, where ROIs for all conditions were negative, enclosing a monetary PGI led to an additional $.27 net loss per mailing, which is more than twice the net loss than when no PGI was included. Enclosing a nonmonetary PGI resulted in an additional $.24 net loss per mailing, which is also close to twice the net loss compared with a mailing with no PGI. Extrapolated to the 9,000 individuals solicited in this campaign, inclusion of a monetary PGI (vs. no PGI) resulted in an additional loss of $810 and adding a nonmonetary PGI resulted in an additional loss of $720. We found similar results in the donation campaign for recurring donors. Enclosing a monetary PGI resulted in a $2.19 lower ROI per person compared with no PGI ($1,929.39 in total), while a nonmonetary PGI (vs. no PGI) resulted in a $1.41 lower ROI per person ($1,242.21 in total). Relationship normsResults suggest that enclosing a monetary PGI leads people to perceive the charity as less communal and more exchange oriented, which directly harms donations. Enclosing a nonmonetary PGI does not affect net communality levels. We suggest that this is because an immediate ask for help accompanying the gift offsets any increase in communality from the gift itself. Indeed, results from an unreported MTurk study (N = 152) showed that net communality was significantly lower when participants were asked for a donation immediately after receiving a PGI than when they were given a gift without a donation request (Mimmediate donation = 4.17, Mgift only = 4.83; p <.001). Further analyses showed that participants in the immediate donation condition scored significantly lower on communal norms and significantly higher on exchange norms compared with those in the gift-only condition (both ps <.01). This finding suggests that the norm of reciprocity may be less applicable when the default relationship norm is communal. Thus, if charities want to enhance communal norms, we suggest sending a gift with a delayed request for help. Limitations and Future ResearchOne limitation of this research is the small sample sizes in some of the studies. This is partly due to the nature of direct mail campaigns (e.g., a low response rate is common in similar campaigns). However, the inclusion of multiple studies in the paper and the web appendix showing consistent effects, some with relatively large sample sizes, should help alleviate this concern.Another limitation of this research is that it does not explore the long-term consequences of PGIs. For example, charities may enjoy additional benefits from nonmonetary gifts if people find items with the names and logos of the organization practical and use them regularly. Cognitive dissonance theory ([18]) predicts that, over time, this action will strengthen recipients' feelings of connection to the charity. In addition, appraisals of their own behavior (""If I am associating myself with this charity, I must think it is a worthwhile cause"") may lead them to make donations in the future. Public display of the charity name and logo could also serve as advertising for people who were previously unaware of the organization. A longitudinal study could be used to test these hypotheses.Future research might also find it fruitful to explore the effect of PGIs on other outcomes. In Web Appendix M, we examine the effect of PGIs on donations of time. Unlike money, PGI type had no impact on time donations. Research insights from a 2014 survey may shed light on why this is. Among donors, ""financial support usually comes first"" and ""few volunteer for sectors that they are not also supporting financially"" ([19]). In addition, time is personally more meaningful than money, and motivations and beliefs can lead to very different effects on time versus money donations ([31]). Thus, people who are willing to donate money may not be willing to donate time. We hope this research will spur further investigation on PGIs and prosocial behavior of all kinds. "
15,"Collaborative Market Driving: How Peer Firms Can Develop Markets Through Collective Action Firms often aim to develop markets as part of their long-term strategies. Conventionally, research in marketing has explained this complex process by stressing firms' efforts to outdo their peers. While this emphasis is valuable, it overlooks the role of another major force in market evolution: collective action among peer firms. To address this oversight, this article conceptualizes ""collaborative market driving,"" defining it as the collective strategy in which peer firms consistently cooperate among themselves and with other actors to develop markets in ways that increase their overall competitiveness. This conceptualization includes the triggers that lead peer firms to mobilize for collective action and coalesce with other market actors; it also identifies how this coalition converts collective resources into market-driving power. These theoretical contributions, based on a multimethod analysis of the rise of U.S. craft breweries, offer an alternative course of action for firms interested in driving new markets when they lack adequate resources to do so individually.KEYWORDS_SPLITFirms often aim to develop markets as part of their long-term strategies. This process usually involves high risks, but it may also bring high rewards. These rewards include generating novel revenue streams and opportunities for firms to build sustainable competitive advantage ([12]; [20]; [46]).When firms set out to develop markets, they often relate to their direct competitors as rivals. Fittingly, marketing research has made insightful contributions to guide companies through this approach. This literature explains, for example, why and how firms should create technological innovations that make them market pioneers and market share leaders ([35]; [50]; [61]; [71]; [94]). Furthermore, this literature highlights strategies that do not hinge on specific innovations but, rather, on the construction of powerful brands through which firms can command premium prices and favorable media attention ([40]; [43]). In these individualistic approaches to market development, firms see the marketplace as a zero-sum game. Though they typically cooperate with supply chain members, they remain strongly self-oriented concerning their competitors, trying to develop a brand reputation that is superior to that of their peers.The focus of this literature on firms' individualistic efforts is valuable, as it offers insights on actions that single organizations can take to outdo competitors. However, this focus has occluded from view the role of another major approach for peer firms to develop markets, namely collective action—the purposively organized activities of a group of actors to create what they see as a common good ([16]; [83]). As both conceptual and empirical research indicate ([46]; [60]; [81]), many firms lack adequate means to develop markets. In response, they often pool resources with other actors, including those conceived of as their rivals and consumers. This form of collective action is observable at the origin of some major markets. For example, whereas Ford Motors is credited as the central figure in developing the mass market for cars, the coordinated action of multiple automakers and consumers was vital to diffusing this innovation in the United States ([81]). Likewise, for less technological sectors such as the now-ubiquitous organic food category, market development relied not only on Americans' increased interest in healthier diets but also on the collaboration of peer firms with consumers ([ 5]; [102]).In light of this evidence, we shift marketing theory's usual focus away from firms' individualistic work concerning their competitors; we direct attention, instead, to the collective action of peer firms in market development. Such a shift has been adopted to some extent in previous research. In studying the growth of minivans and casino gambling in the United States, [85] and [42] reveal the cumulative impact of peer firms' coherent media messages in shaping consumer preferences. Focusing on the growth of nouvelle cuisine, [82] unpack the role of entrepreneurs' pursuits of greater public recognition in propelling the formation of a new market. Nevertheless, such work has rarely focused on coordinated action, leaving underspecified how collaboration among peer firms and their allies arises and leads to market development, a gap noted by some scholars ([13]; [83]).To address this gap, we study a recent case of market development that has involved peer firm collaboration: the rise of craft beer in the United States ([13]). We use this case to conceptualize ""collaborative market driving,"" defining it as the collective strategy in which peer firms consistently cooperate among themselves and with other actors to develop markets in ways that increase their overall competitiveness. We address two questions about this strategy: What triggers market actors to mobilize for collaborative market driving? And, once mobilized, how do these actors deploy collective resources to drive the development of a market?Using our data, we formulate a theory highlighting that, as a precondition to collaborative marketing driving, firms recognize market opportunities that they try to seize individually, only to realize they share systemic constraints such as limited economic and political power on doing so. This realization, though, does not lead spontaneously to collective action. Before this form of action can occur effectively, these firms must form a sense of collective identity, which can be facilitated by suprafirm entities that emerge as coordinators of firms' market-driving efforts. These firms achieve this sense of shared identity by formulating a goal they view as worthy of jointly pursuing and by intentionally cultivating social networks. As often happens in market development, this process also enrolls allies out of the supply chain that can provide these firms with critical resources to pursue their common goal. Once mobilized as a coalition, these market actors convert their collective resources into economic and political initiatives to drive the development of a market that benefits these firms as a whole.This theorizing enables us to make three contributions to the growing literature on what [46] term ""market driving."" As a route to market development, market driving occurs when firms shape markets without being primarily guided by marketing intelligence. Instead, they focus on their internal resources and vision to create new offerings and then work to align consumer preferences with these offerings ([11]; [43]; [56]). Our first contribution is to identify how these resources and vision differ in individualistic and collaborative approaches to market driving, leading peer firms and their allies to engage in various cooperative initiatives and roles. Marking these distinctions extends the typology of market driving strategies, a theoretical move advocated by [45] to systematize understanding of how companies can shape market evolution.Our second contribution is bringing into focus an overlooked actor that can decisively influence market driving: trade associations ([91]). In particular, we conceptualize this class of suprafirm entities as coordinators of peer firms' collaboration, thus attending to research maintaining that some form of coordination is vital for effective collective action in the economic arena ([16]; [68]; [73]; [74]). Over 4,000 trade associations operate at the national level in the United States ([92]), but they appear sparsely in marketing scholarship ([ 4]; [58]). By unpacking trade associations' roles in collaborative marketing driving, we answer calls for an amplified focus in the study of market development—one that goes beyond sellers and buyers to more accurately understand this phenomenon ([10]; [42]; [54]).Our third contribution is to extend theory on the role of consumers in market driving. In addition to their established importance as buyers, consumers often participate in market driving as members of brand communities, engaging in word of mouth and cocreating new products with a specific firm they admire ([18]; [88]). Conversely, research also shows that consumers influence the trajectory of markets when they oppose certain businesses, forcing them to review their practices ([30]; [51]; [81]). Nevertheless, less is known about how markets evolve when consumers systematically join forces with (vs. oppose) an entire set of firms (vs. a single one). Here, we explain how firms and consumers form coalitions and act concertedly to drive markets. Methodology Data CollectionThis research is based on an extended case study ([ 8]; [100]). This method supports theory building by exposing researchers to a broader range of data sources than that which deductive research usually entails ([23]). Furthermore, this method gives access to nuanced processes that link the different elements of complex cases ([ 8]). It is thus suited to constructing theory on the complex phenomenon of market development driving.Our case is the rise of U.S. craft breweries. The observation window goes from the late 1970s, when new craft breweries began to open, to 2016, when their growth slowed down. As Figure 1 shows, these firms had near-zero market share in their first decade; only a few operated in an industry dominated by a handful of corporations and their aesthetically similar beers. Fast-forwarding to 2016, however, the market differed considerably: there were more than 5,000 craft breweries, which accounted for about 20% of sales in the $100 billion U.S. beer market and the broadening of beer products and flavors in the marketplace ([99]). Notably, this growth was not based on new consumers entering the larger beer market. Rather, craft breweries gained traction mainly by altering the preferences of consumers who used to buy from incumbents ([49]; [70]; [99]). We studied this rich case of market driving through interviews, participant observation, and archival data.Graph: Figure 1. The trajectory of craft breweries in the United States.Notes: Separate data on corporate and craft breweries are only available after 1978. In addition, the craft beer market share considers only craft breweries' sales; it discounts corporate breweries' sales in the craft beer category. InterviewsThe first author conducted 45 in-depth interviews with purposively sampled participants in the craft beer market: 26 industry members and 19 consumers. Interviews lasted two hours on average and were audio recorded and transcribed.Half of the industry interviewees played critical roles in craft beer's rise by opening the first craft breweries in their states and running trade associations for long periods. They are both experts in the focal phenomenon and direct witnesses, two main criteria to sample informants for historical accounts ([33]). In their detailed stories, these informants repeatedly pointed to the importance of cooperation among craft brewers. To gain further insight into this area, we collected social network data by interviewing the leading craft brewers in a Southwestern metropolitan area that has a burgeoning craft beer scene and was accessible to the researchers.[ 6]Consumers interviewed also participated in both national and local craft beer scenes. About half participated broadly, attending conferences, brewery tours, and festivals in various states, while voraciously reading craft beer publications. The other half consists of consumers who participated mostly in their local craft beer markets. Participant observationThis method adds to interview data by unveiling individuals' actions in naturalistic settings ([ 3]). The first author did participant observation for three years in an 80-member craft beer and home brewing club. This club met monthly at craft breweries to discuss brewing techniques and beer taste. Immersion in these sites gave privy access to the relations between avid craft beer consumers and craft breweries.To add variance to this geographically bound data ([103]), the first author attended ten brewery tours and four craft beer festivals in three states where the craft beer market differed considerably: the state where the craft beer revival began, with over 500 craft breweries; a state with an expanding craft beer market, with nearly 100 producers; and a state with 15 breweries. At festivals, he gathered data from different sources by acting as a consumer and a volunteer. Finally, he participated in the main trade conference for craft brewers as a media member, attending talks and press conferences and interacting with participants in social events. Archival dataWe complemented data collected during face-to-face interactions with archival data to discern the grand facts of the focal phenomenon ([103]) and minimize informants' memory biases ([33]). As is common for qualitative research, the sampling approach for archival data collection was emergent, driven by the authors' evolving perceptions on relevant aspects of the context ([103]). As such, we collected these archives in a less structured way than in studies entailing automated text analysis, an approach that draws on predefined (vs. emergent) data sources and sampling parameters ([44]).Websites of craft breweries' national association were a key source of archival data; they contain statistical and promotional materials on craft beer's recent history. We complemented these materials with data produced by more independent sources in trade and popular press publications[ 7] ([33]). Finally, to examine craft breweries' trajectories and messaging strategies, we reviewed the websites of the top 20 U.S. craft breweries by sales. Data AnalysisWe began by identifying the central patterns and relationships that constitute the case ([ 8]). This identification involved performing open coding on all data sources, and then axial coding to find convergent and divergent themes that required elucidation. As the analysis progressed, we engaged in the dialectical tacking between data and theory through which ethnographers form, revise, and expand their understanding of a phenomenon. To offset possible biases that can arise in such process, we discussed interpretations with researchers not involved in the study ([17]). These researchers are seasoned marketing scholars, including one with extensive experience as a consultant for craft breweries.Thus, the research design relies on several procedures to move from data to theory. Figure 1 ""brackets"" ([32]) craft breweries' trajectory in the United States into three periods that parallel its major inflection points. For each period, we identify the change in the number of these firms, their peak dollar market share, and key facts related to their market-driving efforts.We use these inflections to abstract collaborative market driving as a three-stage process, as shown in Figure 2. In stage 1, ""dispersed peer firm actions,"" peer firms enter a market and try to develop it individually for the most part. They do not engage in collective action, only later realizing they share systemic constraints that limit an individualistic approach to market driving. In stage 2, ""mobilization of peer firms and allies,"" producers transition into a collective ethos. This change hinges on specific triggers that coalesce peer firms around a common goal while recruiting other market actors, such as consumers, as their allies. These triggers are facilitated by suprafirm entities that emerge as coordinators of collective action. In stage 3, ""concerted action of peer firms and allies,"" the coalition formed by these entities, peer firms, and allies deploy collective resources into economic and political power to drive the development of a market.Graph: Figure 2. Theoretical elements of collaborative market driving.For each stage, Figure 2 shows the core processes, their outcomes, and the central tensions that emerge from market actors' joint work (stage 1 does not have an emerging tension because their actions were not collective yet). To be clear, these stages are conceptually distinct but interdependent; this interdependency is represented by the gradually filling circles above each stage of collaborative market driving. Stage 1: Dispersed Peer Firm ActionsStage 1 refers to peer firms' dispersed actions to drive a market and their gradual realization of their systemic constraints on doing so. We begin with the environmental opportunities that paved the way for market drivers. Then, in line with research on individualistic market driving ([29]; [47]; [56]), we specify market-driving firms' main internal and external constraints. We place both sections within craft breweries' historical context for analytical precision. Environmental OpportunitiesAfter World War II, the U.S. beer industry went through a period of sharp consolidation, with ten breweries controlling nearly all domestic sales by the mid-1970s ([24]). Large firms like Budweiser had saturated the market with mass-produced, low-price products by using high doses of flavor-lightening additives—namely corn and rice ([ 9]).Amid this process, some cultural and legal changes started paving the way for what would later be called ""the craft beer revival,"" as flagged in Figure 1's period 1. In the cultural realm, social movements were reshaping the country's ideological scene, including that on food. Some of these movements vilified mass-production practices that increased shelf life in foods and beverages at the expense of flavor, as with beer ([ 5]). This public reproach led many Americans to valorize foods and drinks that are produced in small batches to retain freshness and create aesthetic complexity ([63]). In the legal realm, an excise tax that affected small breweries was cut by 22% through a law enacted in 1976. Furthermore, home brewing, an activity that serves as an incubator of craft breweries, was legalized in 1978.These cultural and legal shifts create what [90] term ""environmental opportunities"" (Figure 2, stage 1, core processes): these opportunities can benefit many entrepreneurs though other actors and events bring them about. Many beer consumers noticed these opportunities; they then began brewing beer of varied flavors as a hobby, and some turned this activity into a business. They joined the only two recognized craft breweries of that time, Anchor Steam and New Albion. This consumer-led origin makes craft beer a market that grew from amateurs' entrepreneurial deeds ([65]). Internal and External ConstraintsDespite these environmental opportunities, craft breweries struggled for another decade to gain traction with mainstream consumers, mainly because of two internal and two external constraints they later came to recognize they had in common. The first internal shared constraint was limited financial resources. Most pioneering craft brewers were amateur consumers who had little cash to invest in the capital-intensive business of commercial brewing; furthermore, most banks refused to give them loans due to the risks associated with new business forms. This constraint led many to start their firms through some financial improvisation, a condition that remains true today, as when craft brewers buy used materials and work other jobs to make ends meet.The second internal shared constraint was limited know-how. While home brewing allowed would-be craft brewers to test beer recipes, it did not expose them to the difficulties of building and running industrial facilities. Many had trouble finding reliable information to help them scale up their hobbies. In a craft beer pioneer's words:There weren't brewing schools or people to teach you. We barely knew what we were doing. What is the break-even point of a small brewery? What is the commercial process to brew this new beer style? How to choose a location? How do I avoid bacteria in commercial brewing?As with financial resources, many craft brewers presently face know-how limitations, often lacking extensive training in business and technical areas of brewing such as chemistry.Beyond these internal constraints, two external ones confronted these underresourced craft brewers. The first was adverse regulations. Most states prohibited these brewers from having brewpubs where they could sell food, a key strategy to introduce their products to mainstream consumers through cross-selling. Furthermore, many state laws mandated that small breweries could sell beer only through distributors, even though most of these distributors were (and still are) controlled by industry incumbents through massive commercial incentives and direct ownership.In addition to restrictive finances, know-how, and regulations, these underresourced firms dealt with adverse consumer preferences, the second major shared external constraint our data foreground. Craft brewers' initial clientele consisted mostly of other beer enthusiasts; these peer firms both educated these avid consumers and followed their tastes. In this way, craft breweries' market-driving strategy had a market-driven genesis; it responded to some manifest customer needs ([20]; [53]). But mainstream consumers tended to reject more flavorful beers, precluding a larger-scale version of such approach. Incumbents' light-flavored beers had homogenized their tastes, as this craft beer pioneer reminisces:Our IPA and Hefeweizen [beer styles], our bestsellers today, didn't sell anything in the 1980s. I still remember this one guy who walked up to the bar and said, ""You got any new beers?"" My bartender goes, ""We have a Hefeweizen."" The customer looks at it, takes a big sip, and says, ""That's the worst beer I've ever had. It tastes like bananas!"" So, here's a guy that orders a Hefeweizen and didn't even know that it actually should taste like bananas.We conceptualize collaborative market driving as a coordinated response to the recognition of these systemic constraints by peer firms (Figure 2, stage 1, outcomes). However, this response is not inevitable. As social movement scholarship shows (for a review, see [69]]), shared constraints do not translate spontaneously into collective action to change the status quo. In fact, for most of the 1980s, these firms dealt with their obstacles in a dispersed manner, as when they sporadically visited one another to discuss beer making. These brewers describe this period of relative isolation as one of self-absorption and discovery: ""We were pretty much on our own, teaching ourselves how to run a small brewery, trying to make a living....Somewhere along the way, it seems we realized [emphasis added] we couldn't grow this thing by ourselves.""This realization followed the emergence of suprafirm entities. These entities, namely trade associations, gradually formed around these firms to grapple with a core barrier for collective action: organizing ([16]; [74]). A few of them began in period 1, when some craft breweries (e.g., Sierra Nevada) and avid consumers (e.g., home brewer Charles Papazian, considered the father of the craft beer revival) stepped up to found and run them in their spare time with nominal budgets (Figure 2, stage 1, core processes). These actors did not shun the prospect of individually benefiting from craft beer's growth. Still, they also desired to alter the broad conditions of a market for the advantage of other actors, a motive that often guides entrepreneurs and avid consumers ([64]; [87]). However, these rudimentary associations only organized few events, with little sway over other producers and consumers. Period 2 was when they grew in number and professionalism, incorporating paid staff and directors. Late in period 3, they consisted of a vast web, with an umbrella organization working at the national level and about 60 others, called ""guilds,"" operating at nonoverlapping localities. These organizations act like labor unions, representing members' interests while being legally autonomous. In the next section, we analyze how these trade associations, as they became professionalized, triggered the mobilization of multiple market actors for collaborative market driving. Stage 2: Mobilization of Peer Firms and AlliesMobilization refers to the process of both marshaling actors and making them ready for action ([66]). Effective mobilization is essential for achieving long-term goals such as developing a market, mainly when actors are individually underresourced to pursue this goal. Though important, market development research has overlooked the concrete initiatives that lead firms to coalesce around a specific goal and form a strong sense of collective identity ([13]; [83]).Using our data, we abstract these initiatives into two triggers of collective action: ( 1) promoting a shared cause and ( 2) institutionalizing bonding opportunities (Figure 2, stage 2, core processes). These triggers, spearheaded by craft breweries' trade associations, led craft breweries to transition from an individualistic to a collective ethos while enlisting critical allies. Of these allies, we focus on a type of actor that remains undertheorized in market driving: consumers.[ 8] Promoting a Shared CauseThis mobilization trigger refers to the diffusion of a compelling goal to guide market actors' efforts in market development. We explicate this trigger by drawing on research that suggests that collectively compelling goals include two core dimensions: ( 1) the diagnosis of a problem and ( 2) the prognosis of certain actors as the solution ([ 6]; [81]).In period 2, craft breweries' national association promoted a shared cause through a set of books, magazines, and websites targeted at both producers and consumers. In the 2003 edition of The Complete Joy of Homebrewing, a best-selling book that this association published for home brewers and aspiring craft brewers, the introduction states: ""Our beer world is so much better than it was in the 1980s and early 1990s. But don't forget for a moment that the large brewing companies of the world continue to 'squeeze' the market with their lighter-flavored product, always trying to minimize choice"" (10). Later, as this association set out to expand craft beer's appeal to mainstream consumers, it created several variants of this message. An exemplar is this rhythmic excerpt from a pamphlet circulated in many local bars:We hail the bold brewers who have built paradise/Saving beer from dilution by corn and rice.../May our passion for quality never be stopped/In the land of the free and the brave and the hopped/We salute with this glorious beer in our hand/Let the true taste of freedom clink out 'cross this land.Industry incumbents use high levels of corn and rice in their flagship brands to sell beer at lower prices and deliver mild flavors to mainstream consumers. In the post-WWII era, these practices cohered with the democratic ethos that backed the diffusion of mass-produced consumer goods in the United States ([14]). But messages such as the preceding one invert the meaning of these practices, diagnosing them as yielding low-quality products and reducing market choice. Furthermore, such messages contain a prognosis: they advance ""bold brewers"" as market protagonists imbued with a ""passion for quality"" that will ""save"" beer from the dilution and homogenization caused by market antagonists, namely corporate breweries.Craft breweries echo this message at their consumer touchpoints. Rather than touting their products' quality, they focus on asserting that light-flavored beers are a problem created by corporations, as in this brewer's speech during a brewery tour with consumers: ""You are not going to find corn or rice in our beers. Do you know why corporate breweries put a blue indicator on their cans to show the beer is cold? They don't want you to drink these beers when they are not cold. Corn and rice are cheaper but taste terrible at higher temperatures.""The emphasis on the motives of a set of peer firms contrasts with advertising focused on the benefits of a single firm's offering, which typifies individualistic approaches to market driving ([31]; [85]). To make these motives more meaningful, trade associations tie craft breweries' activities to certain cultural ideals, as in this point-of-sale material:I declare the beer I choose to enjoy is...an artistic creation of living liquid history made from passionate innovators...[and that] American craft brewers provide flavorful and diverse American-made beers in more than 100 distinct styles that have made the United States the envy of every beer-drinking nation for the quality and variety of beers brewed....Craft brewers represent the purest form of the American spirit and are dedicated to nurturing and enriching their communities.This parody of the U.S. Declaration of Independence offers another prognosis: it glorifies craft brewers as producers who want to revive Americans' dear ideals of entrepreneurship, independence, and idyllic communities. Much like the rhetoric of social movements, this messaging links up these brewers' market goals with noble purposes, a strategy that serves to broaden the appeal of a change project and increase its potential to attract allies ([81]). We argue that in collaborative market driving, these links do more than that. They partially eclipse this strategy's self-serving economic benefit, which could otherwise undermine the sense of disinterestedness that is vital to the credibility of a cause and its leaders ([39]).Craft brewers do not uniformly buy into these idealistic messages. Since the 1990s, dozens have sold their businesses to the corporate breweries their associations have disparaged. Yet these messages foster an ethos that guides the relations of the firms that remain committed to developing a market through collective action. As with this craft brewer, many others refer to this ethos as ""a rising tide floats all boats"":There's a common cliché, ""a rising tide floats all boats,"" and that sentiment has been around. There's competition in that we're all in the [craft beer] industry...[but] Budweiser, Coors, and Miller, regardless of the growth of our industry, still stand for most beer consumed in the U.S. right now. We're the small guys, with our arms locked together.The same ethos patterns the experiences of many craft beer consumers, who come to see themselves as craft breweries' allies.[ 9] This influence comes through when they reflect on their trajectories as beer drinkers, as in this excerpt from one of the first author's many in situ interactions with craft beer festivals' attendees:I used to drink Bud Light, Coors, and Miller. Now, I keep thinking, ""Damn! They [corporate breweries] made me waste all these years drinking awful beer (laugh)!"" And then you get to know what craft brewers are doing, creating neat spaces for people to hang out.... I see myself more than a consumer; I'm kind of a supporter [emphasis added], actually.When developing a new market, competitors often use coherent messages to signal to external audiences (e.g., consumers) what the market stands for ([52]; [72]; [85]). In addition to this role, promoting a shared cause serves to mobilize stakeholders for collective action. [81] notes the role of this trigger at the start of many markets, such as that for personal computers, when retailers and early adopters claimed to be promoting technological democratization. This type of mobilization hinges on neither direct coercion nor measurable incentives; thus, it differs from the processes that explain collective action in mainstream economic theory ([60]; [73]). Instead, the promotion of a shared cause works by offering a long-term goal that peer firms and potential allies consider worthy of jointly pursuing.Promoting a shared cause is, therefore, foundational for mobilizing actors for market driving. However, it alone has a limitation for setting collective action in motion: it does not automatically create the social networks that enable the translation of abstract goals into pragmatic doings ([69]). The next subsection sheds light on how craft breweries' trade associations cultivate these networks, which include consumers as critical members. Institutionalizing Bonding OpportunitiesThis mobilization trigger refers to the purposive creation of occasions for market actors to develop social cohesion; such occasions are primary for collective action, mainly when this involves a large group of actors ([74]). Using our data, we conceptualize two dimensions of bonding opportunities: ( 1) those within locality, which serve to forge a series of clusters among local actors, and ( 2) those across locality, which focus on connecting these clusters into a wider and more diffuse social grid.Within specific localities, local guilds take great responsibility for cultivating clusters. For example, they organize regular meetings that facilitate sociality and genuine communication among local brewers, as this craft brewer discloses:We're all super busy running our businesses. Brewers are kind of jack-of-all-trades....If we were to try to coordinate just our own sort of meetings, it'd never happen. The guild is community building, so we don't become a bunch of jerks. It's a forum to air any grievances that we have and work together to solve them.Professional meetings can be a source of distress, but they can also be bonding events when organized around a regular set of participants and presumed goodwill ([57]). Through such meetings, the web of local guilds forms an apparatus of social cohesion that prevents craft brewers from drifting apart due to misunderstandings and individual business demands that could run counter to the formation of a strong collective identity.While opportunities like regular meetings foster local producer bonds, others such as trade conferences foster these bonds across localities. In the leading conference run by the national trade association, craft brewers from many places engage in the usual activities of business conventions, such as visiting a trade show ([76]). Distinctive about it is the volume of activities designed to connect brewers. Every conference day, they join in foosball tournaments, brewery tours, and parties lavishly supplied with craft beer. Sociality happened organically in the conferences that gathered nearly 100 brewers in period 1, and it became a planned feature in period 2, when attendance exceeded the thousands. These events connect craft brewers who are not in regular contact, as revealed by this craft brewer's testimonial captured in situ at a recent conference edition, with over 10,000 brewers:I don't leave the brewery very often, so once a year, I make the time to come here. I always walk away smarter than I was, but it's also just this energy and camaraderie and enthusiasm. That always reminds me of why I got into the brewing industry in the first place...I always hear inspiring stories from other breweries, their struggles, how they overcame them.These events instill in geographically distant producers a sense of camaraderie and energy ([15]) to continue pursuing their shared cause.To mobilize allies for this cause, craft breweries' associations institutionalized opportunities for producer–consumer bonding. This occurs within and across localities, too. At the local level, exemplars of this mobilization trigger are the craft beer festivals that happen throughout the country every year. These festivals grew with the expansion of guilds, their main organizers: from 5 festivals in early period 1 to over 150 in late period 3. In their typical format, consumers pay for tickets that they exchange for beer samples at craft breweries' stands, where they chat with brewers about beer making and tasting. We also witnessed these actors mingling while playing lawn games and eating at communal tables. These occasions are socially valuable because brewers spend most of their time in breweries' backstage, as this consumer notes:It's nice to actually talk to brewers....Some people who like craft beer are also interested in learning more about how beer is made. At the festival, we can hang out and ask questions to brewers. It makes for a more personal connection....When Budweiser sends someone to beer festivals, it's usually a sales guy or a promoter that knows nothing about beer making.By mixing consumption with sociality and entertainment, these local events suspend firms' commercial interests, stressing, instead, humanized buyer–seller ties ([76]).To bond producers with consumers across localities, craft breweries' trade associations run large-scale events such as The National Homebrew Day. Established in 1988, it now happens every May in hundreds of different sites in the United States (more than 400 as of 2016), when craft breweries gather with home brewers and other avid consumers to jointly brew a beer batch. For this day, the craft breweries' national association instructs participants from all U.S. states to brew the same beer recipe. Furthermore, regardless of time zones, this association asks all participants to raise their glasses to celebrate craft beer and home brewing at a single time.Emphasizing synchronicity in these events (i.e., same day and time of activities) is not pointless. In a classic book on the formation of nations, [ 2] details how synchronic events such as holidays cultivate a sense of identity among spatially scattered people. Through invented occasions that encourage synchronic doings, trade associations foster social cohesion among geographically dispersed producers and consumers. Emerging TensionThe stage of ""mobilization of peer firms and allies"" is permeated by collectivism, yet it contains an emerging tension related to group boundaries (Figure 2, stage 2, emerging tension). As craft beer grew in popularity, it began to entice corporations and entrepreneurs seeking a quick profit—or in one craft brewer's words, ""a bunch of business people pouring money into breweries, opening fancy brewpubs, who started cutting corners to get higher returns."" Craft breweries feared that this type of newcomer would undermine their quality-oriented cause.To manage this tension, the craft breweries' national association set formal standards for a brewery to be considered craft. These standards had three pillars: a maximum production volume, a maximum percentage of ownership by large corporations, and the types of ingredients that can be used in beer making. This standard-setting did not aim to create technological compatibility, as in the markets for electricity and VCRs ([19]; [34]). Instead, it aimed to shield craft breweries' collaborative market driving from the encroachment of corporate breweries. Through these standards, craft breweries restrict the types of breweries that can vote and participate as board members in their associations, two roles that may have enduring effects on market development by shaping commercial and regulatory standards ([91]).In summary, promoting a shared cause and institutionalizing bonding opportunities are mobilization triggers that, despite emergent tensions, explain how peer firms go from shared constraints and dispersed action to a sense of collective identity, alongside their allies (Figure 2, stage 2, outcomes). The next section highlights how this identity translates into a form of collective action that departs from individualistic approaches to market driving. Stage 3: Concerted Action of Peer Firms and AlliesStage 3 refers to how a mobilized coalition of actors works together to drive the development of a market. Essentially, this process involves deploying collective resources in ways that give peer firms the power to overcome constraints and bring about enduring changes in the composition and actions of a market's stakeholders, including consumers, suppliers, distributors, and lawmakers ([46]).This conversion of collective resources into market-driving power occurs within two domains that are central for market development: economic and political ([26]; [48]; [54]). We refer to deployments in these domains as economic and political conversions. For each conversion, we identify dimensions corresponding to major strands of activities and highlight the synergistic roles of our core market actors: suprafirm entities, peer firms, and allied consumers (Figure 2, stage 3, core processes). Economic ConversionThe economic conversion concerns the commercial cooperation of mobilized market actors to improve a set of peer firms' ability to drive a market. This conversion has two dimensions: ( 1) facilitating peers' entry and growth and ( 2) building market reputation. Facilitating peers' entry and growthIn individualistic approaches to market driving, creating high entry barriers is a maxim of competitive savviness ([77]; [86]). In collaborative market driving, firms suspend this wisdom. Instead, they exchange critical resources for promoting the entry and growth of peers.Limited know-how has long been a constraint for craft brewers to enter and stay in the beer industry. In response, trade associations and established craft breweries have devised initiatives to enhance newer producers' know-how. At trade conferences, prestigious craft brewers educate audiences of less seasoned peers on general business topics, from distributor selection to brewpub design, without charging speaking fees. In the annual program ""Brewing the American Dream,"" pioneering craft brewery Boston Beer Company covers the expenses for a new craft brewer to visit its facilities and receive customized business advice from its executives.Beyond cooperation between geographically distant peers, our data show ample exchange of know-how among geographically close firms. Interestingly, this closeness does not seem to evolve into enclosed cliques, as may happen in some industries ([100]). Newcomers are welcomed, receiving critical business information, as this craft brewer notes:People who are planning to open a brewery come to guild meetings to learn about the trials and hiccups we've experienced. That saves them a lot of time; many of those guys don't have a lot of experience as entrepreneurs, at least not in the alcohol industry. So, we say, ""these are your zoning laws."" It's very intricate. There's even regulation for how many feet away from a church you gotta be. And we are a soundboard for a new person who wants to open a brewery. For example, two new breweries have been coming to our meetings for the past year. We've helped steer them away from locations that would have hindered their ability to grow. It's not easy to open a brewery. Every step of the way, you find a hurdle.Craft breweries participate in a two-layer network, with local and distant peers. For new firms, spatially distant peers are useful as sources of generic know-how, such as brewpub design; in turn, local networks give these firms access to tailored information, such as how to navigate city regulation. This information improves newcomers' decisions about issues (e.g., location) that can make or break a brick-and-mortar business.Beyond the aid from established peers, upcoming producers consistently receive contributions from allied consumers. Specifically, consumers act as volunteers (Figure 2, stage 2, core processes), giving money and physical labor to help these firms deal with another of their historical constraints: limited financial resources. In crowdfunding sites such as Kickstarter, many individuals give money for craft breweries to open and expand their operations. In our sample, consumers have made contributions that, summed with others, helped these breweries raise between $20,000 and $40,000, getting in return only mementos or personalized experiences such as private beer tours. As to physical labor, consumers volunteer at craft beer festivals in positions from trash collectors to beer pourers, receiving in return tickets to these festivals and T-shirts. Also in exchange for some souvenirs, consumers volunteer at craft breweries' production lines, as when a new craft brewery needed extra hands on its canning operation's first day.Consumers frequently provide firms with free resources. At times, they create products for themselves that are later commercialized by firms on a larger scale ([21]; [22]). At other times, when participating in brand communities, they give away ideas for the specific firm to which they feel attached ([18]; [78]; [88]). As allies in collaborative market driving, though, consumers form a more flexible pool of critical resources, which various firms (not just specific ones) can call on to respond to their business needs. The operation of this pool relies on the strong sense of collective identity between producers and consumers—result from the mobilizing efforts analyzed previously. The result of this mobilization is also clear in how these market actors collaborate to build a positive reputation for the collective of craft breweries. As shown next, this collaboration impacts both newer and older peers. Building market reputationIn individualistic approaches to market driving, firms focus on building their own brands ([46]). In collaborative market driving, this individualistic focus coexists with the goal of developing a reputation for the entire market category. This goal can hardly be accomplished without peers' adequate performance.Building this positive reputation is challenging when many producers lack the know-how to offer high-quality products reliably ([81]). To mitigate this historical challenge for craft breweries, local trade associations keep an eye on members that struggle to deliver expected quality levels, as this guild's director confides:We help defray the cost of conferences for brewers.... We defray costs for everyone, but I make special invitations to those that I know are not doing a great job (chuckles). Part of the money we make at festivals goes for that.... If the first craft beer I ever taste is hideous, that forms my opinions about craft beers in general.There're licensed breweries in this state that the owners are enthusiastic about their beer, but their beer isn't always very good.... And some of them have been around a while. That doesn't help the [craft beer] industry.In collaborative market driving, improving quality is a collective concern because peer firms seek to create a positive image for their market. This is necessary given the knowledge schema that informs the adverse preferences of mainstream beer consumers. Because of the flavor homogeneity of U.S. mass-produced beers, many consumers learned to expect all beers to taste similarly. This schema is tacit when Americans ask one another, ""Do you like beer?"" as if all beers tasted the same. In markets with a long history of heterogeneity such as cars, the question ""Do you like cars?"" makes little sense, as it is assumed that people may well like sedans while disliking compact vehicles. In the beer market, though, the many consumers who are unaware of the sheer diversity of flavors among craft beers transfer the single-taste schema they learned through their experiences with mass-produced beers to other products. Then, if a craft brewery makes a faulty product, they infer this product generally reflects the quality of craft beers. Given this, craft breweries' know-how exchange plays a nontrivial role in increasing consumers' chances of enjoying their first craft beer experiences, a decisive step in increasing demand for this product ([12]). This form of deploying collective resources, thus, drives markets by changing the economic actions of producers, which in turn impacts the composition and typical actions of consumers.Despite the ample exchange of economic know-how, many craft brewers occasionally face issues in their commercial efforts. A common one is erring in inventory forecasts in ways that impact consumer experience and sales. In response, craft breweries participate in another form of collective deployment of resources that centers on production supplies. Here, local networks are instrumental, as this craft brewer reveals:Last month we ran out of oxygen, so I called [local brewery] and asked, ""Do you have a tank we could borrow?"" And they go, ""Sure. The brewery's keys are hidden here; just go back and grab the tank."" It's this sort of thing. If you're short in anything, you can always call other breweries.Pooling supplies with local peers prevents craft breweries from losing sales and failing consumers with a lack of products and faulty services. In the long run, this economic cooperation aids these firms in their bid for creating a positive market reputation in their localities.To build the reputation of a new market, firms often enlist resources from influential market actors such as opinion leaders ([41]; [52]). In our context, mainstream consumers were key allies, providing resources that were critical to the firms leading the cause they learned to support. Specifically, allied consumers directly contribute to building and sustaining craft breweries' core competitive advantage: continual product innovation ([96]). This contribution usually happens through home brewing contests, as this fieldnote based on the first author's observations describes:In the typical format of home brewing contests, craft breweries partner with home brewing and craft beer clubs that invite members to send samples of their best homemade beers. These samples are judged, and the winning recipe is commercially brewed by the partner craft brewery—with winning consumers receiving no financial reward. Drawing on consumers' recipe ideas is so common that craft breweries' trade associations have formalized a process to do so. In craft beer competitions, craft breweries often compete in the ""Pro-Am"" category (short for Professional-Amateur). To participate in it, breweries can only enter recipes coming from their collaborations with consumers.When producers share high collectivism, as craft breweries do, they often restrain their access to novel knowledge, a critical factor in firms' ability to innovate continuously ([100]). By turning consumers into cocreators (Figure 2, stage 3, core processes), craft breweries and their trade association partially offset this tendency. These breweries do not look at the aggregate tastes of these consumers to cater to them, as in typical market-driven strategies, nor do they ignore these tastes, as in some market-driving strategies ([43]). Instead, these firms rely on these consumers as members of their innovation ecosystem who provide free suggestions. The deployment of this economic resource supports these breweries' market-driving goal by keeping product innovation as a competitive advantage over incumbents, thereby helping them to retain the lead in shaping mainstream consumer preferences.To fully grasp this economic cooperation, it is worth unpacking how it benefits established breweries. By displaying generosity, they showcase their commitment to the craft beer revival. In turn, the recognition of this commitment by peers and allied consumer aids these larger players—some of which produce millions of beer barrels—to retain the right to claim the status of ""craft."" Nevertheless, collaborative market driving is not directed by a highly calculative mindset ([ 7]; [60]), through which firms foster partnerships only if they profit from these relationships more than their partners do. Larger peers help smaller ones better their performances even though they cannot measure the return over their collaboration; in fact, they may lose sales in the areas where these smaller players operate. Emerging tensionIn economic cooperation, [74] argues that an inherent tension is the risk of opportunistic behavior (Figure 2, stage 3, emerging tensions). Our data contain one such case, when a craft brewer sold a bottler to a peer for its cost value, only to learn that this firm later resold the machine with profit to another craft brewery. The seller learned about this at a trade conference when he incidentally met the equipment's second buyer. The seller stopped helping the opportunistic peer, but without taking further action such as telling other breweries about this violation. This case helps us identify limitations to collaborative market driving. Peers trust one another by default rather than as the result of dyadic ties built over time ([28]; [100]), but they will cut these ties if they realize a peer abused their initial goodwill.It appears that opportunistic behavior is mitigated indirectly in collaborative market driving. The multilayer social network in which firms participate functions as a monitoring tool against actors who want to engage in repeated free riding. In addition, the openness of this network to newcomers plays a symbolic role in subduing purely self-oriented behavior. Observe how this craft brewer reflects on his adoption of the ""a rising tide floats all boats"" ethos after experiencing craft breweries' exchange of economic resources:When we entered into the brewing business, I thought all right, ""Here we go, this is a business. It's gonna be cutthroat, take children out of the living room."" When we were still in the planning phase, I was at [craft beer festival], and I went around to all the local breweries and introduced myself. And then, the guys from [brewery name] just sort of said, ""If you need anything, let us know."" I didn't expect that.... And then, I needed something, and I called them, ""Hey, can you help me with this?"" And they go, ""Sure, just stop by."" It was yeast, I think. Later, I wanted to know about some distributors, and they go, ""Here's your answer..."" Since then, we've been able to help them a little bit. I remember the first time they asked us for help. I was like, ""Yay! We're not just asking all the time anymore.""Instead of holding back or monetizing critical economic resources, the coalition of craft breweries, trade associations, and allied consumers more often pools these resources together, enabling firms to enter the market and build positive consumer perceptions. The goal and scope of this economic cooperation are broader than those in typical cases of interfirm collaboration centered on specific, time-bound projects ([98]). This cooperation also departs from individualistic approaches to market driving; in the latter, producers focus on creating strong reputations for their own brands ([43]); in the former, producers also prioritize the reputation of their peers.In sum, economic conversion involves the pooling and deployment of critical economic resources into initiatives that foster the entry of new peers to an industry and alter the actions of critical stakeholders, such as these same peers and consumers. However, as the craft beer market grew in economic import, craft breweries came into friction with regulations that had not been designed for these players but were, instead, aligned with market incumbents' interests. The next subsection analyzes how collaborative market drivers work together to become a political voice. Political ConversionPolitical conversion refers to the regulatory work of mobilized market actors to improve a set of peer firms' ability to drive a market (Figure 2, stage 3, core processes). When firms drive markets, political action often is focal because existing legislation tends to protect the status quo ([26]; [52]). In individualistic market driving, large firms usually engage in this kind of action by using their financial resources to hire well-paid lobbyists and make donations to political parties ([27]; [89]). By contrast, collaborative market driving entails the deployment of collective resources to gain political power. This deployment has two dimensions: ( 1) creating goodwill with political elites and ( 2) pushing a legislative agenda. As with economic conversion, suprafirm entities, peer firms, and their allies play synergistic roles in each dimension. Creating goodwill with political elitesThis dimension focuses on establishing fruitful channels of communication with political elites. New market players usually find it hard to develop these channels ([ 1]). Craft breweries' trade associations took the lead in this regard by hiring professional lobbyists, as this association director reveals:We felt that we had to hire a lobbying firm.... There's a limited amount of publications that legislators read, and then there's like a subscription email service. So, this firm can place our message there, and they know how to change it so that it makes sense to legislators. Not to mention they knew who in the House has an agenda of supporting small businesses like us.For legislators, lobbyists are valuable to the extent they can translate the often-vague claims from constituents into useful information for policymaking decisions ([36]). For constituents, lobbyists' value derives from their ability to offer reliable access to political elites.Beyond lobbying, craft breweries' trade associations create goodwill with political elites by inviting them to events that can showcase craft breweries' quality and popularity. For example, the national association hosts a craft beer and food pairing reception annually to acquaint U.S. Congress members with its executives and leading craft brewers. Also, state guilds use the largest festivals they organize as a political tool. This guild's director explains:One of our major events [ 6,000 people] happens across the street from the State Capitol. It pulls in retailers, wholesalers, brewers, and consumers. To be honest, promoting craft beer to the public is a byproduct. The primary purposes are fundraising for the guild's programs and profiling craft beer and craft breweries to the state legislature. We invite legislators and staffers to come for the expo; some are invited to a luncheon, others are invited to speak.... We do this event at that location on purpose, to show craft beer.Such festivals bond producers and consumers with shared interests. Furthermore, by showcasing craft breweries' large following, these events help trade associations vividly display to political elites the electoral potential of being receptive to their legislative agenda.Creating goodwill with political elites is important and often costly ([101]). Craft breweries' trade associations have been able to achieve this goal by harnessing two collective resources that they have fostered: the mobilization of peer firms and consumers as well as their rising economic power. This fieldwork memo, written as a summary of the first author's interactions with these associations' directors, gives more details:The national trade association takes funds from the dues paid by the great proportion (70% in 2016) of brewers affiliated with it and the registration fees they pay for trade conferences. This combination has enabled this organization to step up from doing intermittent lobbying until the mid-2000s to allocating nearly $200,000 toward government affairs per year in the 2010s. Guilds, in turn, take funds primarily from the proceeds of the popular craft beer festivals they organize. Further, about 1/3 of them run membership programs in which consumers pay annual dues (e.g., $30) in exchange for perks (e.g., glassware).Though the individual values of these dues are minor, they demonstrate the importance of cultivating a sense of collective identity among collaborative market drivers (Figure 2, stage 2, outcomes). When properly done, this cultivated identity can even lead market actors to behave in politically committed ways that are quite puzzling when compared with the waning civic engagement that has recently characterized many areas of social life ([80]).Put succinctly, the dimension of creating goodwill with political elites focuses on cultivating a sentiment of endearment with this powerful group to increase their receptivity to the claims of the peer firms. However, this may not translate into specific votes—and thus the need to work on the more pragmatic dimension of political conversion. Pushing a legislative agendaThis dimension focuses on getting specific bills passed rather than on enhancing lawmakers' receptivity to an interest group. As with creating goodwill with political elites, it also relies on market drivers' collective sense of identity and aggregate economic power.Since the late 2000s, one critical policy issue has been the federal excise tax on beer. In the program ""Climb the Hill,"" trade associations sponsored fly-ins for craft brewers to discuss the impact of this tax with legislators in Washington, D.C. For these meetings, craft brewers suspend their antagonism to corporate breweries. Trade associations instruct them to focus on how the excise tax hindered their potential to create the manufacturing jobs that have become so electorally relevant in the United States. In 2015, nearly 180 craft brewers met with their Congressional representatives to garner their support for a bill that passed in 2017 to lower said tax.Beyond producers, consumers also participate as activists (Figure 2, stage 3, core processes) to push specific bills. The national association runs the program ""Support Your Local Brewery,"" with over 1.5 million registered consumers who receive calls to action about policy issues. These calls include instructions that also emphasize a moderate tone that consumers should use:Please identify yourself by name and say you are a constituent and in which city or rural area you live. Ask the Representative to please oppose [bill number] because it will limit the ability of craft brewers to get their beer on the shelves of retailers and hence your ability to access [state] craft beer.While the number of consumers contacting representatives is unavailable, guild directors often reported this program's role in amplifying their political clout, as in this interview: ""More than once, I've walked into a Congressman's office to talk about an issue, and they go, 'Okay, okay, I know what's going on. I'll support your agenda. Just ask your people to stop calling me. I've got it.'"" This consumer political engagement is another point of distinction between collaborative market driving and typical cases of brand community, which center on the cocreation of products and services rather than on shaping policy ([88]).Pushing a legislative agenda has been vital in craft breweries' struggle to access more mainstream consumers. In a recent case, a state guild spent three years working with lobbyists and coordinating a series of grassroots tactics to expand craft breweries' production capacity without increasing their taxes. This guild's director chronicles:The first time, we tried to introduce the bill through our lobbyist, but we failed. Institutions resist change. You have to show the new bill is a better alternative. The second time, we asked essentially the same things, but we also did a whole campaign. We had a 300-people rally at the Capitol, which was the biggest rally for any commerce committee they've ever had. We activated not only our breweries but also the general public. We had people bussed in. Our bill sponsor talked before the group. We had [the mayor] talk. During all this, breweries and myself were doing the basic petition across the state. We've got 10,000 signatures.Collaborative market driving entails deploying collective resources into some initiatives that use formal communication channels with political elites and into other initiatives that dramatize the popular support for a cause. [69] explains that the mix of these political tools is what often creates the sense of merit and urgency required to turn such claims into policy changes. In this process, trade associations play a crucial role, bundling dispersed producers and consumers into a more cohesive political voice.Over time, this coalition of suprafirm entities, peer firms, and allied consumers has scored major political wins. Recall that regulations prohibited craft breweries from running brewpubs and mandated the use of distributors to sell beer in many states, despite incumbents' power over these intermediaries. By 2016, Georgia and Mississippi were the only states still forbidding craft breweries from on-premise beer sales. Moreover, this coalition has legalized home brewing in all states, an activity that often introduces new consumers to craft beer's flavors ([63]). This legalization also creates opportunities for more people to engage in the activity that has served as the primary incubator of craft breweries, thus promoting the entry of new market players in the coalition. Through political conversion, craft breweries have shaped regulation in ways that changed the composition and typical actions of multiple stakeholders. Emerging tensionThe emerging tension found in political conversion is political underrepresentation (Figure 2, stage 3, emerging tensions). For example, the dues paid by smaller craft breweries partly paid for the costs of the political campaign mentioned previously, when a guild passed a bill to expand craft breweries' production cap. However, the bill's main beneficiaries were two larger peers, who were close to reaching the cap. Smaller craft breweries raised their eyebrows at this process. But for the most part, this kind of tension is tempered by these larger peers' role in expanding the craft beer market. Smaller breweries have observed that larger peers' presence in mainstream retailers such as large grocery store chains are an entryway for more consumers into the craft beer world, many of whom become eventually interested in their local products. Furthermore, this tension is tempered by the role of smaller breweries in economic conversion. There, they are the main beneficiaries of what larger peers can provide. Discussion Theoretical ImplicationsWe compare our theorizing with prior marketing research to distill the main differences between collaborative and individualistic approaches to market driving. These differences, all introduced in our findings, are synthesized in Table 1. We propose that the first five are defining characteristics of collaborative market driving and that the last two are typical occurrences. Consistent with [46] work on market driving, these dimensions should be seen as the ends of a continuum, not dichotomies.GraphTable 1. Individualistic and Collaborative Approaches to Market Driving.   Individualistic versus collaborative market drivingWhen enacting individualistic approaches to market development, peer firms see the marketplace as a zero-sum game: they remain strongly self-oriented to develop a brand reputation that is superior to that of their peers.We believe this ethos tends to prevail when firms' resources are relatively ample or aligned with the market environment. This is the case in [85] study of automakers and [43] work on wineries; these firms drove their markets mostly through individualistic efforts to outdo peers. When some resources are scarce, though, firms may resort to marketing alliances as part of market-driving efforts, as often happens in the development of complex technologies ([86]). These alliances differ in degree from collaborative market driving. They are time-bound and tend to involve a select group of firms, often excluding peers. Under these conditions, the importance of suprafirm entities is situational, as when otherwise rival firms try to shape regulation as an interest group ([89]).By contrast, collaborative market driving is guided by a shared cause instead of a self-oriented vision. Inspired by our informants, we term this ethos ""a rising tide floats all boats""— which reflects firms' focus on building a positive reputation for themselves and their peers. Research shows this kind of collective action often occurs among underresourced players, as in the dynamics leading to the growth of food trucks and community-supported agriculture in the United States ([25]; [97]). But it also occurs among more powerful producers, as when automakers worked with car enthusiasts to lobby for better roads and regulation in automobile's early days ([81]). This collaborative ethos rests on producers' relative lack of resources to shape critical market structures and behaviors, not on their absolute endowments.Precipitating sustained collective action requires time and effort ([73]; [74]). A suprafirm entity devoted to coordinating collaborative market driving can, thus, be critical (Table 1, penultimate line). Our research sheds light on a common form of such entities: trade associations. Typically, these actors lack bureaucratic tools (e.g., contracts) to coerce firms into certain behaviors. Instead, they shape markets by producing cultural templates for action ([92]), orchestrating a shared identity among producers, enlisting allies, and harnessing the resources of these actors into market driving. Trade associations seem to be well positioned to coordinate this strategy because of their presumed legitimacy as a representative of collective interests. Other suprafirm entities that often have such legitimacy, and therefore might alternatively be effective coordinators of collective firm action, include private consultants, consumer associations, and governmental and multilateral agencies ([68]).Collaborative market driving also shows how markets evolve when consumers systematically join forces with firms to defy an industry's status quo. Research on this topic points to the role of ideological affinities between firms and consumers, as when they form brand communities ([79]; [97]). However, ideas rarely translate spontaneously into sustained, organized action against dominant practices and actors. This form of action relies on pragmatic ways to create social cohesion among challengers, organize contestation, and secure resources for these tasks in the long term ([66]). Building on this view, our research deepens marketing theory by specifying mobilization triggers and types of resource conversion that mediate between ideologies and the collective action of peer firms and consumers to drive a market. In this form of action, consumers play various roles as allies, supporting multiple rather than single firms (Table 1, bottom line).By marking the distinctions between individualistic and collaborative approaches to market driving, we answer [45] call for a systematization of how firms can shape market evolution. Recently, [43] moved in this direction by identifying differences in the ""basis of competition""; specifically, they show that firms can drive markets through symbolic leadership in addition to through the more often discussed focus on technological disruption. Across the differences between symbolic and technological leadership, firms typically cooperate with multiple actors within and outside the supply chain, but they see their peers mostly as rivals. We extend this nascent typology of market driving by identifying differences in the ""form of action""; we theorize how firms can drive markets by collaborating with peers, adding to the understanding of how firms do so through individualistic competition.To be sure, collaborative market driving does not exclude self-interest. By cooperating, firms can change markets to an extent they would be unlikely to do otherwise. In the craft beer case, even the largest craft breweries would struggle to shape policy, an arena in which their resources are hardly a match to those of their rival multinationals. Peers also cooperate because they jointly benefit from growing the visibility of their offering, a key factor in boosting the economic and cultural significance of their market positions ([42]). Thus, collaborative market driving is a collective strategy that includes but is not overtaken by self-interest.Relatedly, collaborative market driving does not mean an absence of producer hierarchies. In our context, there is a clear power difference between craft breweries that sell only in small towns and those with national distribution. In collaborative market driving, though, higher-power firms do not use their greater resources to oppress their peers, as would be typically the case in individualistic approaches to market driving ([86]). Instead, they allocate some resources to help peers thrive. In doing so, they inevitably assert their power, but looking at collaborative market driving only through the lens of self-interest obscures rather than illuminates this strategy. Of course, the prevalence of collaboration can change as market growth slows down. Future research could explore how this collaborative market driving may break down. Transferability of theoretical insightsOur framework (Figure 2) is useful to explain other cases of market development. In theorizing market driving, [46], p. 47) note that this strategy is a matter of degree: ""A business that greatly changes the composition of a market [and] the behaviors of most players would be classified as having driven the market to a greater extent than another business that caused only a small change in the behavior of a single player."" The rise of U.S. craft breweries lies on the higher end of this continuum. It involves players that shaped much of the composition of the market and the behaviors of many economic and political actors; furthermore, these players did so with the odds stacked against them, using limited resources to confront powerful incumbents and alter adverse consumer preferences.We propose our framework has a strong explanatory fit with similar cases: firms that collectively drove markets to a high degree despite their fragmented economic and political power. Research has documented many such instances, as in the markets for organic food, grass-fed meat, and legal cannabis, all of which developed through the collective action of relatively small producers and allied consumers ([ 5]; [52]; [102]). Other exemplars are the growth of food trucks and the credit union movement ([25]; [67]). The early days of personal computers and nouvelle cuisine also involved peer firm collaboration in particular areas. For example, producers cooperated to create consumer awareness and interest in their offerings, the area where they individually lacked resources to drive market development ([81]). In short, our framework is useful in explaining the process of collective action when peer firms employ collaboration, more and less broadly, to drive markets to a high degree.In addition to contributing to market driving research, our work offers new insights on the formation of market heterogeneity, in particular to resource partitioning and organizational ecology theories. Often, work in these areas hold that markets tend to become dominated by large generalists that cater to the majority of consumers with mass-produced offerings, leaving unattended smaller segments of buyers who prefer more differentiated experiences (e.g., [13]; [37]). Market heterogeneity arises as entrepreneurs form market niches to cater to these segments that are beyond better-resourced incumbents' interests. In this view, the best strategy for this new set of firms is to remain in a sort of competitive quarantine, operating in market spaces without provoking retaliation of more powerful incumbents.In contrast, collaborative market driving shows how markets become heterogeneous as underresourced firms jointly shift from a defensive to an offensive strategy. In this strategy, they do not simply find consumers with tastes that resonate with their differentiated offerings; instead, they shake deep-seated preferences to develop consumer demand. Moreover, they do not merely occupy market niches away from generalists' reach but rather move into generalists' turf, directly defying their dominance. The present research thus accounts for a more vigorous form of agency by underresourced new firms, deepening knowledge of the nature and extent of their competitive efforts and market influence.The conceptualization of collaborative market driving also informs the ongoing conversation on interfirm ties in marketing research. This literature has built a rich understanding of the vertical form of these ties, whereby sellers and buyers in a given distribution channel try to optimize their bilateral transactions (e.g., [38]; [55]; [62]; [93]). Much less studied are horizontal forms of market coordination, those involving firms positioned as sellers in a distribution channel. Work on horizontal coordination emphasizes time-bound interactions between a select group of peers; these interactions are regulated through formal contracts and oriented toward cost-sharing, as when firms form alliances to develop new products ([84]; [95]). By contrast, collaborative market driving refers to cooperation among a broad (vs. select) set of peer firms that are at the center of a broader coalition of actors who seek to develop a market. This cooperation relies on an informal (vs. formal) contract, safeguarded by a curated ideology and cohesive social networks, and enacted through a portfolio of initiatives that market actors devise to pursue their collective goal. Managerial ImplicationsOur work can help managers assess when a collaborative approach to market driving is best suited. To organize this discussion, we offer a decision flowchart (Figure 3).Graph: Figure 3. Decision flowchart for collaborative market driving.The first managerial task is to pinpoint the critical constraints a firm would face in driving market development (Figure 3, box 1). These constraints can be classified as consumer- and producer-related. As to the first type, our work highlights adverse consumer preferences, but consumers may also have difficulty in evaluating the value of a novel offering ([43]; [60]). This is recurrent in the markets for foods and beverages, entertainment, and fashion, in which assessment of superiority tends to be subjective. As to producer-related constraints, we foreground limited know-how and financial resources, though these constraints also include insufficient social capital ([19]) and brand recognition ([81]) to attract customers to a new market.The second managerial task is to assess whether the focal company has the resources to overcome these constraints (Figure 3, boxes 2 and 3). Often, firms do not have the means to develop a new market, even if products are clearly superior to existing offerings. This happened in the early days of radio broadcasting, personal computers, and cars ([59]; [81]). Peer firms lacked sufficient resources to create technological infrastructure and generate broad consumer awareness and interest. But single firms may be adequately endowed for market driving, as with the technology startups studied by [86]. In this scenario, we reason that individualistic market driving is more suitable for a firm to reap most of the rewards in the market it is developing (box 4). This firm can use orthodox competitive practices such as eliminating peers from the market and creating unique product attributes (e.g., a patented feature). The focal firm may also choose to use the form of individualistic action that [43] labels ""systemic"" market driving. While this form of market driving is not collaborative among peer firms, it does involve forging relationships with actors outside the value chain (especially media members) to create symbolic value for consumers.The next managerial task is to identify whether there is a pool of peer firms that jointly possess the necessary resources for market development (Figure 3, box 5). If not, the focal firm may need to develop ways to lower entry barriers to the market (boxes 6 and 7) to form a ""mob."" This focus was part of JVC's strategy to win the battle of VHS over Betamax ([19]). This firm facilitated the entry of other firms interested in producing VHS-based VCRs, while Sony tried to keep Betamax a proprietary technology. In such cases, collaborative market driving would involve some economic and political cooperation—before a focus on mobilization—to open up the market for other firms with overlapping interests.As peers enter the market, our work points to the pivotal role of suprafirm entities and mobilization triggers in cultivating a shared cause, and the networks needed to translate it into actions (Figure 3, box 8). Once peer firms are mobilized, they can develop initiatives to deploy their collective resources in ways that help them overcome their constraints and thus build a viable competitive position in a new market (box 9). Managers should carefully consider and include potential allies in these mobilization triggers and deployment initiatives. Finally, when evaluating collaborative market driving, we advise the use of both firm- and market-centered metrics. Firm-centered metrics include the typical ones, such as growth in sales and profit. Market-centered metrics include increase in the overall size of the market category and the number of peers. A firm's overemphasis on its own market share may be counterproductive to the goal of developing a market where a set of peer firms can thrive. ConclusionThis research conceptualizes collaborative market driving, the collective strategy through which firms consistently cooperate with their peers and other market actors to develop a market in ways that increase their overall competitiveness. This research also highlights the role of trade associations as coordinators and consumers as allies in this strategy. Furthermore, it provides recommendations for firms interested in driving new markets when they lack adequate resources to do so individually. Together, these contributions help align mainstream marketing thought and practice with the often collective nature of market driving. Cheers. "
16,"Competitive Effects of Front-of-Package Nutrition Labeling Adoption on Nutritional Quality: Evidence from Facts Up Front–Style Labels ""Facts Up Front"" nutrition labels are a front-of-package (FOP) nutrition labeling system that presents key nutrient information on the front of packaged food and beverage products in an easy-to-read format. The authors conduct a large-scale empirical study to examine the effect of adoption of FOP labeling on products' nutritional quality. The authors assemble a unique data set on packaged food products in the United States across 44 categories over 16 years. By using a difference-in-differences estimator, the authors find that FOP adoption in a product category leads to an improvement in the nutritional quality of other products in that category. This competitive response is stronger for premium brands and brands with narrower product line breadth as well as for categories involving unhealthy products and those that are more competitive in nature. The authors offer evidence regarding the role of nutrition information salience as the underlying mechanism; they also perform supplementary analyses to rule out potential self-selection issues and conduct a battery of robustness checks and falsification tests. The authors discuss the implications of the findings for public policy makers, consumers, manufacturers, and food retailers.KEYWORDS_SPLITAccording to estimates from [11], more than one-third of U.S. adults are obese. Childhood and adolescent obesity rates have also skyrocketed in the last 30 years, with one in five school-aged children considered obese. To combat this disconcerting trend, public policy makers, food manufacturers, and grocery retailers have made efforts over time to design nutrition labels that can educate consumers about the nutritional value of the foods they purchase and help consumers make healthier choices. Recently, the U.S. Food and Drug Administration (FDA), in an attempt to promote healthy food choices among consumers, announced a new Nutrition Facts label for packaged food products that reflects new scientific information, highlighting the link between diet and obesity-related chronic diseases.[ 7]The packaged food industry has also voluntarily taken steps to inform consumers about the nutritional value of food products so that consumers can make better choices; one such initiative undertaken by food manufacturers is the Facts Up Front front-of-package (FOP[ 8]) nutrition labeling. Such nutrition labels are voluntarily adopted by food manufacturers and provide nutrient information on the front of food packaging in a clear, simple, and easy-to-read format. The standardized labels present the key information listed on the Nutrition Facts Panel (NFP; displayed on the back or side of food packages) more concisely, and the information often includes calorie content and the amounts of key nutrients to limit (e.g., saturated fat, sugar, and sodium per serving) (see Figure 1).Graph: Figure 1. Facts Up Front–style front-of-package nutrition labels.Front-of-package labels can be effective in stimulating positive outcomes both on the demand and the supply sides. On the demand side, such easy-to-read labeling systems can help time-starved consumers make healthier choices at the point of purchase and help overcome disadvantages of the mandatory nutrition label (the NFP), which is difficult to read and understand ([37]). Multiple recent studies showed a positive effect of FOP labels on consumers' perceptions of foods' healthiness (for a recent meta-analysis of studies on FOP labels, see [25]]) and consumer choice at the point of purchase ([15]; [57]). In addition, FOP labels can help mitigate the negative effects of front-of-package nutrient content claims (e.g., ""Low Fat"") that may serve as a food marketing tool rather than promote health ([16]). Whereas nutrient content claims can selectively highlight certain nutrients to make the product look healthier and lead to halo effects such that consumers infer that the entire product is healthy from information about only a selected nutrient ([42]), FOP labels provide exact nutrient information. On the supply side, FOP labels can help stimulate product innovation and lead to nutritionally better products. Although the demand effects of FOP labels have generated much interest in recent research, with consensus emerging that FOP labels help consumers identify healthy products ([25]), the supply-side implications of FOP labels have not been systematically examined. This study is an attempt to fill this critical research gap in the areas of health and nutrition, public policy, and marketing.The first objective of our study is to conduct a systematic empirical examination of the effect of adoption of FOP nutrition labels in a product category on the nutritional quality of the food products in the category. Our second objective is to examine the moderating effects of brand and category characteristics. Our central thesis is that adoption of FOP makes product nutrition information more salient, and as consumers' preference for healthier products increases, food manufacturers respond by enhancing the nutritional quality of their products. In accordance with recent studies that suggest conducting mechanism checks as a way of validating claims of causal inference ([21]), our final objective is to establish the role of nutritional information salience in consumers' choice of food products as the underlying mechanism that drives food manufacturers to improve the nutritional quality of their products.To accomplish our objectives, we undertook a comprehensive data collection effort and examined packaging and nutrient information of packaged food and beverage products (21,096 products, 9,083 brands, and 4,408 firms across 44 food and beverage categories) in the United States over a 16-year period. In this study, we focus on a class of FOP labels (commonly known as ""Facts Up Front"" FOP nutrition labels) that have a standardized and neutral form in which key nutrient information is presented on the front of the package as clear and easy-to-read icons (see examples of the Facts Up Front–style FOP nutrition label in Figure 1). We use a quasi-experimental study design to examine the effects of FOP adoption in a product category on the nutritional quality of the products in the category. We exploit temporal variation in adoption of FOP at the product category level and cast our model in the difference-in-differences (DD) modeling framework built on panel data that helps us compare changes in the nutritional quality of products during the pre- and post-FOP adoption periods across product categories that are exposed to FOP adoption (the treatment categories) and product categories that are not exposed to FOP adoption (the control categories).We report four sets of findings. First, we find that the adoption of FOP nutrition labeling in a product category results in a considerable improvement in the nutritional quality of food products in that category. Second, heterogeneity analyses suggest that the effect of FOP adoption is stronger for premium (high-priced) brands and brands with a narrower product line breadth. Third, we find that the FOP adoption effect is stronger for unhealthy categories and categories with a higher competitive intensity. Fourth, we find that manufacturers increase the nutritional quality of products by reducing the calorie content and the levels of nutrients to limit, for example, sugar, sodium, and saturated fat. This result helps us shed light on the underlying mechanism. If FOP adoption increases the salience of nutritional information, we argue that this would incentivize food manufacturers to improve products' nutritional quality by limiting the calories and the levels of other nutrients to limit that are actually displayed on the FOP label.This study advances the understanding of FOP labels from the theoretical and practical perspectives. From the theoretical perspective, we tackle the issue of FOP labels from the supply side and answer the recent call by scholars to help understand the relationship between FOP labels and product nutritional quality ([15]; [25]). We also present evidence of a ""nutritional information clearinghouse effect"" of FOP labels, whereby such labels increase the salience of nutritional information of products. From the practical perspective, these results will help inform public policy, as well as manufacturers, retailers, and consumers. From the public policy perspective, because the NFP has not been effective in changing consumer choice behavior ([30]), the FDA has encouraged food manufacturers to adopt voluntary initiatives that highlight key nutrients on the front of food packages to serve the dual purpose of increasing consumer access to nutritional information and improving product quality. The present results help inform public policy makers that FOP labels, which display key nutrient information on the front of the package in a standardized and a uniform format, help increase products' nutritional quality. Thus, such labels should be promoted. The study's findings specifically help unpack the role of key brand and category characteristics that moderate the effectiveness of FOP adoption suggesting the specific categories and brands in which FOP label adoption can provide the greatest benefits by enhancing product nutritional quality. We believe retailers can benefit from the study by encouraging FOP label adoption in categories that need help in improving nutritional quality. Facts Up Front FOP Nutrition Labeling InitiativeIn 1994, under the Nutrition Labeling and Education Act (NLEA), the FDA mandated food manufacturers to display the NFP on the back (or sometimes on the side) of food packages. Since then, several studies have questioned the effectiveness of the NFP in consumer decision making at the point of purchase. Much of this has been attributed to the high costs of processing information that shoppers face at the time of purchase ([30]). In recent years, FOP nutrition labels have gained widespread popularity because they provide information on calories and a set of selective nutrients in the form of easy-to-read icons on the front of food packages. Over the past few years, many different types of FOP nutrition labels have been developed and introduced in the market. In 2009, the FDA commissioner declared in an open letter to the food industry that FOP nutrition labeling would be the agency's top priority and encouraged food manufacturers and retailers to design a standardized, science-based FOP nutrition labeling system that would comply with FDA regulations ([23]). Subsequently, two of the leading food industry trade organizations in the United States—the Grocery Manufacturers Association and the Food Marketing Institute—officially announced the voluntary FOP nutrition labeling scheme called the ""Facts Up Front"" FOP labeling initiative ([47]). According to the initiative, food manufacturers present the nutritional content of their products in an easy-to-read ""callout"" format that is based on the Guideline Daily Amounts. Food packages are required to carry four basic icons—for calories (per serving), saturated fat (in grams and Percent Daily Value [%DV]), sodium (in milligrams and %DV), and sugar (in grams)—as a default format.[ 9]In this study, for the following reasons, we focus on all of the FOP labels that meet the Facts Up Front guidelines. First, these labels are the most commonly used and standardized FOP nutrition labels. All food manufacturers follow the same format for the shape of the icons and the presentation of information about key nutrients. The format has been accepted and encouraged by the Grocery Manufacturers Association. The FOP labels that we examine also have the support of several agencies, such as the Food Marketing Institute and the FDA ([17]). In a thorough examination of food product packaging over nearly two decades and across 44 categories, we found that this has been the most common standardized format (this helped us rule out format-related differences that may affect outcomes). Second, the Facts Up Front–style FOP format lists the presence of nutrients in Guideline Daily Amounts. In particular, the levels of nutrients such as saturated fat and sodium presented as a %DV per serving can help consumers choose a balanced food product (see Figure 1). Third, we would like to emphasize that the FOP labels that we study are not nutrient claims (e.g., ""25% less saturated fat""). Unlike claims that highlight improvement in selected nutrients of a product, the Facts Up Front–style FOP labels simply present the key nutrient information from the NFP (on the back of a product package) on the front of the package. Moreover, although nutrient claims such as ""25% less sugar"" may imply a healthier product based on a single nutrient (but not necessarily overall nutrition), Facts Up Front–style FOP labels bring the critical nutrient information from the back panel to the front, which creates the opportunity to focus on the improved overall nutritional profile. Figure 1 presents examples of food products with the Facts Up Front–style FOP nutrition labels examined in this study. Hypotheses Adoption of FOP Labels Leads to Improvement in Nutritional QualityPrior research in marketing and economics literature has suggested that consumers often do not have complete information about product attributes ([35]; [44]), and the resulting costly search process has a profound effect on consumer behavior and competitive behavior ([50]). In the context of nutrition information, research suggests that consumers face three main types of costs in collecting and assimilating information: ( 1) collection cost, which comprises the time and effort spent in acquiring nutrition information; ( 2) computational cost, which includes the effort combining the relevant information into an overall evaluation; and ( 3) comprehension cost, which captures the effort needed to understand the nutritional information ([43]). [37] show that a simplified nutrition scoring system at the point of sale reduces all three types of costs, thus motivating consumers to switch to higher-scoring products that are healthier.Front-of-package labels make key nutrient information salient through their prominent display on the front of the package. A product attribute is deemed salient ""when it stands out among the good's attributes relative to that attribute's average level in the choice set"" ([ 8], p. 803). Research suggests that consumers give more weight to information that is salient ([ 8]; [53]). Therefore, we argue that adoption of FOP labels makes nutritional information more salient and reduces nutritional search and information costs. Indeed, [57] calibrate a model based on household purchase–level data and find that the use of FOP labeling increases the weight of the healthiness attribute in consumers' product choices.Building on these findings, we argue that FOP adoption makes nutritional information more salient, reducing consumers' overall search costs for nutritional information at the point of purchase which, in turn, influences consumer decision making. This change in consumer behavior has important implications for food manufacturers. Game theoretical models and empirical studies suggest that any market mechanism that helps reduce consumers' price search costs—""information clearinghouse"" (e.g., the internet price comparison site)—would intensify price competition between firms ([45]). Following this argument, we suggest that adoption of FOP in a product category serves as a source of ""nutritional information clearinghouse"" and spurs nutrition competition among food manufacturers. Because consumers favor healthier options, food manufacturers would compete by improving the nutritional quality of products. In summary, FOP adoption in a product category increases salience of nutrition information on the demand side, leading to increased consumer preference for healthier products; on the supply side, food manufacturers respond by offering nutritionally better products in the category. Thus, we propose the following hypothesis: H1:  Adoption of FOP in a product category has a positive effect on the nutritional quality of products in the category.In the following subsections, we propose that the effect of FOP adoption in a product category on the improvement in the nutritional quality of products is moderated by brand and category characteristics. We focus on brand characteristics (specifically, price premium and product line breadth) that provide a greater incentive for brands to respond to competitive changes in a product category. For category characteristics, we focus on factors (specifically, category healthiness and competitive intensity) that present a greater opportunity for food manufacturers to improve products' nutritional quality. The Moderating Effect of Brand CharacteristicsAlthough price competition is a common strategy in the grocery market, many brands compete on perceived quality and command a price premium. Prior research has suggested that brands' price premium is a critical lower-funnel shopper marketing instrument that influences consumers' decision making at the point of purchase ([32]). Because premium brands target a price-insensitive consumer segment and charge a price premium over competing lower-tier brands in a category, they face constant pressure to differentiate their products and justify their higher prices. Researchers have identified health and nutrition information as one of the key associations consumers make with a brand that can drive their willingness to pay for grocery products ([ 4]; [ 5]; [14]). Studies also suggest that consumers who are less sensitive to price are more likely to focus on the nutritional aspects of products and nutrition labels ([13]) and that the introduction of a point-of-sale nutrition scoring system can decrease shoppers' price sensitivity ([37]). Taken together, these arguments suggest that premium brands are more likely to invest in product innovation and offer nutritionally better products to continue to justify the price premium they command over nonpremium brands. From the demand perspective, given the price point of premium brands, consumers are also more likely to pay greater attention to the nutritional content of high-priced products. Thus, premium brands benefit from improving their products. From the supply side, premium brands may also have greater resources to invest in product innovation, leading to products with higher nutritional quality, which is aligned with changing consumer preferences for nutritionally better products. Therefore, we expect the effect of FOP adoption on nutritional quality to be greater for premium brands in a product category and propose the following hypothesis: H2a:  The effect of FOP adoption on the nutritional quality of products is stronger for premium brands.The second brand characteristic we consider is the breadth of a brand's product line. The product mix is an important part of a brand's overall competitive strategy. In grocery retailing, beyond the price dimension, brands compete in nonprice dimensions and constantly innovate and introduce new products to expand product lines and gain market share ([19]). Research has shown that although broader product lines can help increase demand and prices, they can also increase costs related to product design and development ([ 6]). In a similar vein, brands with broader product lines might impose additional resource constraints in such a way that brands with narrower product lines might have an edge in reformulating products and engaging in product innovation by improving the nutritional profile of their products. Although brands with a broader product line breadth could have more market power and greater potential to innovate, we argue that brands with narrower product lines are better positioned to change the products' nutrition level. This is because, on the demand side, consumers face lower nutrition information search costs for brands with narrower product lines. Consumers may also be able to compare a brand's nutritional profile within and across categories more easily for brands with a smaller product portfolio, thus effectively motivating these brands to leverage their focused product portfolio and actively engage in improving their products. Thus, we expect that the effect of FOP is greater for brands with a narrower product line breadth across categories. Thus, we present the following hypothesis: H2b:  The effect of FOP adoption on the nutritional quality of products is stronger for brands with narrower product line breadth. The Moderating Effect of Category CharacteristicsAs consumers process nutritional information of products and search for healthier options, the marginal benefit of searching for healthier options is lower in healthy categories compared with unhealthy categories. [34] finds that following the enactment of the NLEA, a negative relationship exists between category healthiness and the amount of information consumers obtain in a product category, suggesting that consumers may need more information in unhealthy categories. This finding, applied to the present study context, suggests that introduction of FOP labeling would make nutrition information more salient in less healthy categories. From the demand-side perspective, [10] argue that healthy eating nudge interventions (including nutrition labeling) are more effective in reducing unhealthy eating than increasing healthy eating. On the supply side, given that unhealthy categories have low nutritional quality, the opportunity to improve the nutritional quality of products is also higher in unhealthy categories. Thus, food manufacturers in unhealthy categories have a greater incentive to invest in product innovation and to appeal to consumers who search for relatively healthier or less unhealthy options even in inherently unhealthy categories. [36] find that after the NLEA was enacted, firms in unhealthy categories improved the nutritional quality of their products more than those in healthy categories. We posit the following hypothesis: H3a:  The effect of FOP adoption on the nutritional quality of products is stronger for unhealthy categories.Consumers face higher search costs for product attributes when shopping in product categories with higher competitive intensity compared with less competitive categories. Extant research suggests that price dispersion can be higher in more competitive markets ([ 9]; [12]). In such markets, consumers may face higher search costs, and firms have an incentive to take actions to reduce consumers' search costs so that the products can enter consumers' consideration sets ([38]). Thus, on the demand side, consumers may face high price dispersion and high search costs in highly competitive categories. On the supply side, food manufacturers in more competitive categories have more incentives to innovate to reduce consumers' search costs so that their products can enter consumers' consideration sets. Stated differently, firms have a greater incentive to differentiate themselves by investing in improving the nutritional quality of their products in more competitive categories. Thus, we propose the following hypothesis: H3b:  The effect of FOP adoption on the nutritional quality of products is stronger for categories with greater competitive intensity. Methods DataThe primary data source is the Mintel Global New Products Database (GNPD), which is considered the industry standard in reporting new product launches, trends, and innovations in the packaged food and beverage product industry. The database provides nutritional information, photographs of the package, price, package size, number of units in a multipack product, and so on. In addition to these product attributes, the database has information about brands, manufacturers, categories, and published dates. We accessed the database and collected the aforementioned information for all food and beverage products across 44 product categories in the United States over 16 years (from 1996 to 2011), including existing and new product launches. By manually examining the photographs of the packages of all the products released during the period, we identified products with FOP labels and recorded when the FOP-labeled products were introduced in each product category. To assemble the estimation data set, we removed outliers (based on a boxplot of nutrient levels) and products with missing nutrient information. Next, we separated the data into two sets, the calibration data set (from 1996 to 2002) and the estimation data set (from 2003 to 2011). We used the calibration data to construct the moderating variables.[10] This ensured that brand and category classifications did not confound with the estimation period and helped us interpret the effect of moderating variables ([41]). The final estimation data set consists of 21,096 products, 9,083 brands, and 4,408 firms in 44 food and beverage categories. Nutritional Quality MeasurementTo measure products' nutritional quality level, we used the Nutrient Profiling (NP) model that was developed by the United Kingdom Food Standard Agency and the British Heart Foundation Health Promotion Research Group at Oxford University ([40]). The NP model has been widely used in marketing ([ 2]; [15]), economics ([52]), public health ([46]), and nutrition ([28]) literature. The NP score is calculated in a way to offset calories (kJ)[11] and the nutrients to limit—including saturated fat (g), sugar (g), and sodium (mg)—by the nutrients to encourage, including fruit, vegetable, and nut (FVN) content (%); fiber (g); and protein (g). Specifically, based on the content of the aforementioned nutritional elements in a 100 g or 100 mL food or beverage product, 0 to 10 points are assigned to each negative element, and 0 to 5 are assigned to each positive element. The total points for positive elements are subtracted from the total points for negative elements to calculate the NP score. Based on calories, five nutrients (saturated fat, sodium, sugar, fiber, and protein), and the FVN content,[12] the NP model generates a single score that ranges between −15 (the most healthy) and 40 (the least healthy).Several unique characteristics of the NP model deserve mention. First, the NP score is a serving size–free index—because it measures the nutritional quality based on the amount of each nutrient in 100 g or 100 mL of a food or beverage product—and thus measures the nutritional quality independent of individual-specific food consumption patterns and enables comparison of the nutritional quality of various products across brands and categories. Second, the NP score is a standardized score that helps classify food and beverage products as ""healthy"" or ""less healthy."" A food product is classified as ""less healthy"" if the NP score is more than or equal to 4, and a beverage product is classified as ""less healthy"" if the NP score is more than or equal to 1 ([40]). Table 1 provides the summary statistics of the NP score across the product categories that we analyze.GraphTable 1. Summary Statistics of Product Categories.  1 Notes: Categories are presented in alphabetical order. The smaller the Nutrient Profiling (NP) score, the better the nutritional quality. For a food product, the NP score that is more than or equal to 4 indicates ""less healthy."" For a beverage product, the NP score that is more than or equal to 1 indicates ""less healthy."" Research Design and Identification StrategyBefore we present our proposed econometric model, we discuss issues related to the research design and identification strategy. We take a quasi-experimental approach with the (first-time) adoption of FOP by a brand in a category as the treatment and examine the effect on the nutritional quality of products of other brands in the same category. As we mentioned previously, our estimation data spans 2003 to 2011 (referred to as the ""focal time period""). Using the adoption of FOP by all the brands in all of the product categories during the focal time period, we classify the product categories into two types: the treatment group (categories in which we observe the introduction of a FOP-labeled product during the focal time period) and the control group (categories in which we do not observe the introduction of a FOP-labeled product during the focal time period). In other words, the timing of FOP adoption is the only criterion that we use to classify categories into the treatment and control groups. One might be concerned about the effect of category characteristics on group assignment. However, in line with the arguments presented in [24], we contend that category factors (e.g., healthiness) that can potentially induce self-selection bias are not time varying. Thus, the group assignment—based solely on the timing of FOP adoption—ensures that there are no systematic differences between the treatment and control categories. The empirical modeling approach, DD, accounts for time-invariant brand-, firm-, and category-specific characteristics. We also confirm that there is no statistically significant correlation between the timing of FOP adoption and category healthiness.[13]Our research design involves the treatment effect of adoption of FOP in a product category by a brand (referred to as the ""first adopter"") on the change in nutritional quality of products of other (competing) brands in the same product category. Regarding the first adopter brand in any given category, one can argue that it has higher nutritional quality and is more likely to adopt FOP. To ensure that the first adopter brand does not contaminate the results, and to facilitate a cleaner interpretation of the effect of FOP adoption (by the first adopter brand) on the nutritional quality of other competing brands, we removed the ""first adopter"" brands (and firms) from the analysis. As FOP nutrition labeling is voluntary, and because we removed the first adopters from the analysis, the timing of the adoption of FOP by the first adopter in a product category is unlikely to be correlated with the nutritional quality of other brands in the same product category. In summary, we treat FOP adoption (by the first adopter brand) in a category as an exogenous shock to other brands in the category and investigate whether FOP adoption acts as a catalyst for other brands to improve the nutritional profile of their product portfolios.Following this research design, we cast our analyses in the DD modeling framework to estimate the treatment effect (adoption of FOP in a product category) on the outcome variable (overall nutritional quality of food and beverage products; [33]). By comparing the nutritional quality of products of brands in a product category before and after FOP adoption, and between the treatment group and the control group categories, we not only account for temporal factors that affect both groups simultaneously but also control for innate differences between the two groups. The ""double differencing"" helps identify the causal effect of FOP category adoption on nutritional quality of products ([ 3]). We remind readers that we work with observational data. Thus, we acknowledge that any causal interpretation is valid within the assumptions of the DD model. Given the absence of full randomization, we further conduct a series of robustness checks and falsification tests to validate our DD modeling strategy that are discussed in subsequent sections. Main Effect of FOP Adoption on Products' Nutritional QualityThe key dependent variable of interest is the NP score of a product in the set of packaged food and beverage product categories. Given the range of the NP score across the diverse set of categories (ranging between −14 and 40 in our study), to facilitate an intuitive interpretation, we use the min-max scaling procedure ([26]) and rescale the NP scores on a new scale ranging from 1 (the least healthy) to 100 (the most healthy). We refer to the rescaled NP score as the Nutrient Profiling Index (NPI)[14] and use the score as our focal dependent variable in the DD models. The unit of analysis is the product–brand level.[15] We employ the DD modeling framework to examine the effect of adoption of FOP in a category on the nutritional quality of products in the category (H1) as follows: Nutritional qualitypbfct=α1FOPpbfct+α2Time trendt+ϕb+ωf+νc+σt+τct+εpbfct. Graph1In Equation 1, Nutritional qualitypbfct represents the NPI score of product p by brand b that belongs to firm f in category c at time t. FOPpbfct is the focal independent variable that is equal to 1 for all products in a treatment category in the post-FOP period, and 0 for all products in a treatment category during the pre-FOP period and for those in a control category. The time trend variable (Time trendt) helps control for linear trend in nutritional quality across all food products over time. As there are different categories, the inclusion of category-specific time trend effects (τct) helps further control trend in nutritional quality across all food products within a category. The inclusion of year fixed effects (σt) not only helps control for changes in nutritional quality in a given year due to supply-side factors (e.g., manufacturing capabilities) and demand-side factors (e.g., consumers' preference for healthier products) but also helps control for any other year-specific omitted variables. The brand (ϕb), firm (ωf), and category (νc) fixed effects help account for baseline differences in nutritional quality across brands, firms, and categories, respectively. εpbfct is the error term. The focal coefficient of interest is α1 (the DD estimate), which captures the average effect of adoption of FOP in a category on the NPI of products in the treatment categories relative to those in the control categories in the post-FOP period (compared with the pre-FOP period). Heterogeneity Across Brands: The Role of Price and Product Line Breadth Premium brands versus nonpremium brandsFollowing the arguments presented in recent marketing literature using DD models ([27]; [51]), we use the median split of the brand-specific mean price to classify the brands into premium and nonpremium brands.[16] We focused on brands that exist in both the calibration and estimation periods and used data from the calibration period (1996 to 2002) to compute a set of brand-specific mean prices of the products. This helps ensure that the brand classification does not confound with the estimation time period and allows for easy interpretation of the moderating effects of brands ([27]; [41]). To empirically examine the effect of premium brand (H2a), following recent studies ([20]), we extend our DD model to the difference-in-difference-in-differences (DDD) modeling framework by interacting FOPpbfct (presented in Equation 1) with the focal moderating variable, an indicator variable associated with premium brands. The proposed DDD model is as follows: Nutritional qualitypbfct=β1FOPpbfct×Premiumb+β2FOPpbfct+β3Time trendt+ϕb+ωf+νc+σt+τct+εpbfct. Graph2In Equation 2, Premiumb takes a value of 1 if brand b is a premium brand, and 0 otherwise. All other variables and fixed effects in Equation 2 are identical to those in Equation 1. In Equation 2, the main coefficient of interest is β1 (the DDD estimate) that captures the effect of FOP adoption in a category on the nutritional quality of products of the premium brands (relative to the nonpremium brands) in the treatment categories (relative to the control categories) in the post-FOP period (compared with the pre-FOP period). Wider product line breadth brands versus narrower product line breadth brandsTo measure the level of product line breadth of brands, we focused on brands that exist in both the calibration and estimation periods and calculated the total number of products of each brand in the calibration period. Drawing on the median split of the brand-specific total number of products, we classify the brands into two types: brands with a wider product line breadth and those with a narrower product line breadth. Similar to the DDD model presented in Equation 2, we estimate a DDD model of nutritional quality to examine the differential effect of FOP adoption between brands with a wider product line breadth and those with a narrower product line breadth (H2b). The model is as follows[17]: Nutritional qualitypbfct=β1FOPpbfct×Product line breadthb+β2FOPpbfct+β3Time trendt+ϕb+ωf+νc+σt+τct+εpbfct. Graph3In Equation 3, Product line breadthb takes a value of 1 if brand b is a brand with a wider product line breadth (i.e., a brand with a larger number of products), and 0 otherwise. All other variables and fixed effects in Equation 3 are identical to those in Equations 1 and 2. Heterogeneity Across Categories: The Role of Healthiness and Competitive IntensityH3a and H3b examine the variation in the effects of introduction of FOP across categories based on healthiness and competitive intensity, respectively. As stated previously, and following precedence ([40]), we classify a food product as ""less healthy"" if the NP score is more than or equal to 4, and we classify a beverage product as ""less healthy"" if the NP score is more than or equal to 1. Drawing on the average NP score of all products (from the calibration period data) in a category, we classified the 44 categories into healthy and unhealthy groups (see Table W1 in the Web Appendix). Following previous industrial organization literature ([ 9]; [12]), we operationalize competitive intensity by price dispersion as measured by the coefficient of variation. The coefficient of variation[18] is a unit-free measure of relative dispersion that helps compare price dispersion across categories where products are sold at different price levels ([48]). This measure has been widely used in the economics and management literature ([ 9]; [48]; [55]). The larger the coefficient of variation, the more dispersed the price, and the greater the competitive intensity ([ 9]; [55]). As discussed previously, we use data from the calibration period, and based on the median split of the category-specific coefficients of variation, we classify the categories into high versus low levels of competitive intensity (see Table W1 in the Web Appendix).To test H3a and H3b, we propose the following DDD model: Nutritional qualitypbfct=γ1FOPpbfct×Healthyc+γ2FOPpbfct×Competitive intensityc+γ3FOPpbfct+γ4Time trendt+ϕb+ωf+νc+σt+τct+εpbfct. Graph4In Equation 4, Healthyc and Competitive intensityc are the indicator variables that take a value of 1 if category c is determined to be a healthy and more competitive category, respectively, and 0 otherwise. All other variables and fixed effects in Equation 4 are the same as those in Equations 1–3.[19] The DDD estimates (γ1 and γ2) help us examine how the effect of FOP adoption varies across the category characteristics. Results Effect of FOP Category Adoption on Overall Nutritional Quality of ProductsIn Column 1 of Table 2, we present the results of the DD model shown in Equation 1. We note that the standard errors reported in the table are clustered at the category level and are heteroskedasticity robust. The DD estimate (α1) is positive and statistically significant which suggests that the adoption of FOP in a category leads to improvement in nutritional quality of products in the category. We thus find support for H1.GraphTable 2. Effect of FOP Nutrition Labeling on Nutritional Quality.  2 *p <.10.3 **p <.05.4 ***p <.01.5 Notes: The focal variable of interest and its coefficient estimate (i.e., DD and DDD estimate) that is statistically significant is highlighted in bold. Robust standard errors that are clustered at the category level are in parentheses.To better understand the effect size of the adoption of FOP labels at the product category level, we estimated the DD model (in Equation 1) with the original NP score as the dependent variable, and based on the DD estimate, we find that the introduction of FOP reduces calorie levels by approximately 42.21 kcal[20] in 100 g of food or 100 mL of beverage product when there is no change in other nutritional contents and decreases saturated fat, sugar, and sodium by approximately.53 g, 2.37 g, and 47.45 mg, respectively. Drawing on the entire set of products in the treatment categories in the post-FOP period, we find that FOP adoption leads to a reduction in calories (−12.50%), saturated fat (−12.97%), sugar (−12.62%), and sodium (−3.74%; see Table 3). To evaluate the effect size for an individual product in a more realistic setting, we identified a set of packaged food products outside the sample. Based on their actual nutritional information and serving sizes, we calculated the marginal effect of the introduction of FOP on the nutritional quality of the selected products (see Table 3).GraphTable 3. Effect Size of FOP Adoption for Selected Products.  6 a The level of calories and amount of each nutrient of all products are standardized to a 100 g/mL in our data, and thus serving sizes are not needed to calculate the average effect size.7 b Average calories across all products in the treatment categories.8 c The effect size cannot be calculated because the sugar amount of the original product is zero.9 Notes: Our calculations in change of the nutrient levels are based on the DD estimate (−.5272) from the model. We assume other nutrients are held constant when we calculate the effect of change of a nutrient. Moderating Effects of Brand CharacteristicsH2a and H2b refer to the variation in the proposed effects of FOP labels across brands based on premium brands and product line breadth. The positive and statistically significant DDD estimate (β1 in Equation 2) suggests that the effect of FOP category adoption is stronger for premium brands (see Column 2 of Table 2). In addition, the negative and statistically significant DDD estimate (β1 in Equation 3) indicates that the FOP effect is stronger for brands with a narrower product line breadth (see Column 3 of Table 2). The spotlight analyses presented in Figure 2 (Panels A and B) illustrate that, following the adoption of FOP at the product category level, the difference between the treated and control categories in nutritional quality is larger for premium brands and brands with a narrower product line breadth. We thus find support for both H2a and H2b.Graph: Figure 2. Spotlight analyses for the moderating effects of the brand and category characteristics. Moderating Effects of Category CharacteristicsIn H3a and H3b, we proposed that the FOP effect varies across categories depending on healthiness and competitive intensity. The results suggest that the effect of FOP introduction is greater for unhealthy (vs. healthy) and for more competitive (vs. less competitive) categories (see Column 4 of Table 2). Figure 2 (Panels C and D) provides support for the hypotheses for the category-specific moderating effects, H3a and H3b. In addition, we confirm the robustness of the DDD estimates in Equations 2, 3, and 4 to the inclusion of the interaction terms between the linear time trend and the moderators (see Table W2 in the Web Appendix), continuous measures of the moderating variables and a comprehensive DDD model specification that includes all of the moderating variables (in both discrete and continuous forms) in a single model (for details, see the ""Robustness Checks"" subsection of the ""Validation Analyses"" section). In summary, we find support for all proposed moderating effect hypotheses. Mechanism Check: The Role of Information SalienceTo test for the role of information salience as the underlying mechanism that drives the effect of FOP adoption, we conduct the following empirical analyses. Do firms improve nutritional quality by increasing the nutrients to encourage or decreasing t...Although food products have nutrients to encourage (e.g., fiber) and nutrients to avoid (e.g., saturated fat), as shown in Figure 1, Facts Up Front–style FOP labels are required to carry four basic icons for calories, saturated fat, sodium, and sugar (nutrients to limit) as the default format. Given this, our main argument that FOP adoption leads to salience of nutritional information on the part of consumers which, in turn, spurs food manufacturers to increase the nutritional quality of products suggests that FOP adoption has a greater impact on calorie content and the nutrient levels that are actually displayed on the FOP labels. To empirically examine this, we estimate a series of DD models of levels of calories and individual nutrients. The results in Table 4 show that FOP adoption leads to reductions in the calorie content and in sugar, sodium, and saturated fat—information displayed on FOP labels as the default format. However, we do not find a statistically significant effect of FOP adoption on the fiber, protein, and unsaturated fat levels—information that is not required to be displayed. From a theoretical perspective, these results support our argument that salience of nutritional information is the mechanism that drives the effect of FOP adoption. These results suggest that food manufacturers improve the nutritional quality of their products by decreasing the content of nutrients to limit.GraphTable 4. Effect of FOP Nutrition Labeling on Content of Calories and Individual Nutrients.  10 *p <.10.11 **p <.05.12 ***p <.01.13 Notes: The focal variable of interest and its coefficient estimate (i.e., DD estimate) that is statistically significant is highlighted in bold. Robust standard errors that are clustered at the category level are in parentheses. Sample sizes vary across the DD models because of missing nutrient information for some products. Do FOP adopter brands improve nutritional quality more than non-FOP adopter brands?Following the adoption of FOP for the first time in a category, some brands adopted the FOP nutrition labeling, and others did not. We leverage this phenomenon and examine how the effect of the introduction of FOP in a category differs across adopter versus nonadopter brands. If our argument that increased salience of nutritional information due to FOP adoption is valid, we would expect FOP adopter brands to improve the nutritional quality of their products more than non-FOP adopter brands, because the nutritional information of the products of the FOP adopter brands would be more noticeable to consumers. To empirically test this, we examined photographs of the product packaging thoroughly to identify brands that launched products with FOP after the first introduction of FOP in a category. Then, we estimated a model (in the form of Equation 2) to examine the variation in the effect of FOP across these two types of brands, FOP adopters and non-FOP adopters. The result suggests that the FOP effect is stronger for FOP adopter brands (see Table W3 in the Web Appendix). This result provides further support for our argument that salience of nutritional information is the mechanism behind the effect of FOP adoption. Validation AnalysesIn this section, we present the validation analyses that we conducted to confirm robustness of our results, address potential self-selection issues, test the identifying assumptions of our DD modeling strategy, and rule out effects due to spurious correlation and/or model misspecification. Table 5 summarizes our validation analyses.GraphTable 5. Overview of Validation Analyses.   Robustness ChecksIn this section, we discuss a series of tests we conducted to verify the robustness of the results. Alternative measures of nutritional qualityAs an alternative measure of the nutritional quality of food and beverage products, following [36], we compute a nutrition score based on the %DVs of individual nutrients.[21] We compute the overall nutrition score of a product by adding the %DVs of positive elements (fiber and protein) and (100 − %DVs) of negative elements (fat, saturated fat, cholesterol, sodium, and sugar) and dividing by the number of nutrients (seven). The larger the overall nutrition score, the better the nutritional quality. In addition, we compute the weighted overall nutrition scores by using the category-specific mean and variance of each nutrient's %DV as weights. This helps account for the role of a nutrient in a certain category in terms of amount and variability. The estimation results of the DD models (see Table W4 in the Web Appendix) are in agreement with the main set of results and confirm the robustness of the main results to the alternative measures of nutritional quality. Continuous moderating variablesTo check whether the moderating analyses results are robust to continuous moderating variables, we reestimate the DDD models (Equations 2–4) with the corresponding continuous moderating variables. We confirm that the results are robust to the models with the continuous moderators (see Table W5 in the Web Appendix). Comprehensive model with all moderating effectsTo determine whether the moderating analyses results are robust to having all the interaction effects in a single model, we reestimate a comprehensive DDD model (combining Equations 2–4). We do so with both the discrete and continuous measures of the moderating variables. We confirm that the results are robust to the comprehensive model formulation (see Table W6 in the Web Appendix). Addressing brand mortality biasA DD modeling approach requires the survival of the units of analysis over time to observe the change in their outcomes or behavior of interest before and after a treatment. Because the treatment occurs at the category level, and we are interested in how the introduction of FOP affects the overall nutritional quality of all products at the category level, and all the categories are present before and after FOP adoption, we believe that estimating the DD models with the sample of brands that exist before and after the FOP category adoption and those that appear after the event is not a threat to validity of the results. For a similar research design, see [ 1] and [ 7]. Following FOP adoption, firms may launch new brands with better nutritional profiles or improve the nutritional profiles of products under existing brands. Nonetheless, to establish the robustness of the results, we estimate the main model (presented in Equation 1) with a sample that consists of only brands that exist in both pre- and post-FOP adoption periods. We confirm that the main result is robust, and thus, brand mortality does not change the main results (see Table W7 in the Web Appendix). New brands versus existing brandsGiven that the sample consists of new and existing brands, an interesting question is whether the FOP category introduction effect differs across the two types of brands. In line with [36] argument, we expect that the FOP effect would be stronger for new brands, because improving nutrition by launching new brands is less likely to be risky than adjusting the nutritional profiles of products of existing brands. To check the potential differential effect of the introduction of FOP, we estimate a model with an interaction term between FOP and the indicator variable of new versus existing brands in the form of Equation 2. We find that the positive FOP effect is statistically stronger for new brands (see Table W8 in the Web Appendix). Addressing dominant category biasTo check whether a few dominant treatment categories might be driving the reported results, following [36], we compute a jackknife pseudo-value to estimate the bias between the DD estimate calculated with the entire data and that calculated with the data without a specific category ([56]). We confirm that the DD estimate based on the full data falls within the 95% confidence interval around the mean of the jackknife pseudo-values which confirms that the main result is not driven by an influential or dominant category. Self-Selection ChallengesCategories were assigned into the treatment and control groups based on the timing of FOP adoption at the category level. We argued that the timing of the adoption of FOP by the first adopter brand in a category is exogenous to nutritional quality of products of other brands in the category. Furthermore, we removed the first adopter brands from the treatment group categories to rule out unobserved factors that are specific to first adopter brands that may not hold for the other brands that adopt later in the category. Inclusion of year fixed effects help control for the omitted variables. In addition to year fixed effects, we included category fixed effects in the DD models to control for unobserved time-invariant factors that possibly led to differences between the treatment and control categories. The category fixed effects help absorb the category-specific factors that drive nutritional quality. Despite this set of cautious steps, one can make the argument that the firms in the treatment categories are inherently different from those in the control categories, or there may be some unobserved factors affecting both the timing of FOP adoption and the nutritional quality of a category which could contaminate the observed effect of FOP adoption. To further address concerns about potential selection biases, we conducted the following supplementary analyses. Potential correlation between FOP category adoption and category healthiness levelTo test whether the treatment and control categories differed in terms of their healthiness level, we conduct a t-test that compares the mean NPI scores between the treatment and control categories. The result indicates that the healthiness levels of the two groups are not statistically different (t = −1.6590, p-value =.1402). In addition, we test whether there is a correlation between the FOP adoption timing and category-specific nutritional quality. To do so, we sample data from the treatment categories only and identify the timing of FOP adoption (by the first time adopter brands) in each of the treated categories. A correlation test shows that there is no statistically significant correlation between FOP adoption timing and category healthiness (r =.2484, p-value =.1382). Finally, we pick the treatment categories in which FOP labeling was adopted no more than six months earlier than in the control categories and run the DD model. The narrow time window between these treated categories and the control categories helps us construct similar sets of treatment and control categories, and a comparison of products across these similar sets of categories further rules out any time-varying factors that could affect changes in nutritional quality. We confirm that the DD estimate is robust to the subset of data (see Table W9 in the Web Appendix). Firms in both treatment and control categoriesTo empirically address the possibility that the firms in the treatment and control categories are intrinsically different, we work with only firms that are present in both treatment and control categories and estimate the main DD model. The result (see Table W10 in the Web Appendix) indicates that the FOP effect on nutritional quality is still positive and statistically significant. Thus, the possibility that the reported results are driven by inherent differences between the firms in the treatment and control categories is ruled out. Testing with an alternative estimation period for classification of treatment and control cat...Recall that we classify a category as a treatment group if we observe the FOP adoption in the ""focal time period"" (January 2003 to December 2011). If we shift the end point and change the focal time period to January 2003 through September 2011, the categories that adopted FOP later between October 2011 and December 2011 (which were classified as treatment categories in the original analyses) would now be classified as ""control"" categories. If any unobserved category-specific confounding factors were to drive the results, we would expect the effect of FOP adoption to be weaker or absent in the sample based on the new focal time period. Thus, we estimate the DD models on multiple new focal time periods with different end points (by shifting the end points by 3 months up to 12 months with a 3-month interval). The results (see Table W11 in the Web Appendix) suggest that variation in FOP adoption across categories and classification of categories based on adoption timing do not threaten the validity of the main results. Falsification TestsThe identifying assumption behind the DD modeling approach is the parallel trend assumption, which assumes that the treatment and control groups have similar trends in the outcome of interest (nutritional quality, in our context) before the intervention (FOP category adoption, in our context). To test the validity of the assumption, following previous studies ([ 3]), we include a set of interaction terms between the group indicator variable and dummy variables for all the years before FOP adoption and estimate a model of nutritional quality. We find that the coefficients associated with the interaction terms—the ""parallel-trend coefficients""—are not statistically significant (see Table W12 in the Web Appendix), which suggests the treatment and control categories were not different before FOP adoption. We also conduct a test of joint significance of the parallel-trend coefficients, and the result does not show any significant trends. We conduct these tests at the granular (quarterly) level. We find that the estimates of the parallel-trend coefficients are not statistically significant separately and jointly (see Table W13 in the Web Appendix). We also estimate a model with an interaction term between the group indicator variable and the linear time trend variable. The results show (see Table W14 in the Web Appendix) that the two groups of categories do not have different linear time trends in the pre-FOP period. These tests provide empirical support for the parallel trend assumption behind the DD approach.Next, following economics ([18]; [39]) and marketing ([27]) literature, we conduct the following tests: the fake treatment test or the ""placebo"" test (see Table W15 in the Web Appendix), fake treatment group (see Table W16 in the Web Appendix), and fake outcome tests (see Table W17 in the Web Appendix). The key takeaway is that we find statistically significant results of FOP adoption in conditions when we expect to, and we do not find a statistically significant effect of FOP adoption when we do not expect to find one. The set of results, taken together, provides support for our DD identification strategy and rules out any spurious correlations in our core set of results. Nevertheless, we acknowledge that we work with observational data, and we remind readers that the causal interpretation of these results is valid subject to the identifying assumptions of the DD model. Discussion and ConclusionFood labels play a key role in the strategies designed to inform and induce healthy food choice behaviors among consumers. According to a recent World Health Organization report ([29], p. vii), ""Nutrition labelling is one of the policy tools that can support healthy diets, both in stimulating consumers to make informed healthier food choices and in driving manufacturers to reformulate products to avoid making unfavorable nutrient content disclosures."" In this research, we conducted a systematic examination of the supply-side consequences of the voluntary adoption of a widely used FOP nutrition labeling program, the Facts Up Front–style FOP label. Next, we discuss the implications of the results for theory and practice. Implications for TheoryThere is increasing consensus among recent studies that focus on consumer response to the FOP labels that they help draw consumers' attention to nutrition information and form their perceptions of product healthiness ([25]). Studies based on purchase transaction data have established that FOP labels facilitate consumers' choice of healthier products ([57]). Thus, although the benefits of FOP labels in informing consumers about the healthiness of the products is receiving a fair amount of attention in research, there is little research on the firm side of this issue. [25] argue that ""the implementation of different FOP labels can motivate manufacturers to refine their recipes, leading to healthier product assortments"" (p. 375), and [15] suggest that more research is needed to examine ""the impact of labeling systems on the decision of manufacturers to reformulate their products.""The present study helps fill this critical research gap in the literature by examining the issue of FOP labels from the firm side. Specifically, we theorize that adoption of FOP labels increases the salience of nutritional information and helps lower consumers' search costs for the nutritional information subsequently leading to a ""nutritional information clearinghouse"" effect whereby food manufacturers compete along the nutrition dimension. The results highlight the role of voluntary provision of nutrition information in improving the nutritional quality of products. Previous research in the area of mandatory provision of nutrition information (i.e., the NLEA) has suggested that although the NLEA clearly increased nutrition provision, the legislation has had an overall negative impact on brand nutrition possibly due to the perceived negative correlation between nutrition and taste ([36]). Our results suggest that voluntary FOP labels may be more effective due to the nutritional information clearinghouse effect, thus offering a different theoretical perspective and lens through which nutrition labels can be examined.For a deeper understanding of the effect of FOP, we examine the specific brand and category characteristics for which FOP effects are likely to be enhanced. Specifically, we investigate factors for which food manufacturers have a greater motivation and opportunity to innovate. Studies that examine effects of nutrition labels have identified the moderating role of category-, brand-, or firm-level factors ([36]; [37]). For brand-level moderating factors, the present results show that the effect of FOP is greater for premium brands and brands with a narrower product line breadth. These results highlight how product differentiation and a focused product line strategy that helps lower consumer nutrition search costs serve as motivating factors for firms to innovate more after FOP adoption. At the category level, we find that the FOP effect is greater for unhealthy categories and product categories with a greater degree of competitive intensity. These results suggest that manufacturers tend to innovate more following FOP adoption in categories where there is greater opportunity to do so, such as inherently unhealthy categories and categories with intense competition, where the need to differentiate and lower consumers' nutrition search costs is greater. The result related to the FOP effect in unhealthy categories also supports findings from previous research showing that after the NLEA was enacted, brands in unhealthy categories improved nutrition more than those in healthy categories ([36]).A key question motivating this study is, Why does FOP work in stimulating product innovation? We believe that the answer lies in understanding the underlying mechanism. We theorize and test for the role of nutritional information salience as the primary underlying driver of the FOP effects. We argue that FOP labels serve as a source of ""nutritional information clearinghouse"" in which they increase the salience of nutrition information and decrease consumers' cost of processing nutritional information at the point of purchase. The change in consumer behavior incentivizes manufacturers to compete on the attribute (i.e., nutrition) that aligns with consumer preferences and to develop nutritionally better products. To test this underlying mechanism, we conducted additional analyses that suggest FOP adoption in a product category lowers the calories and the amounts of saturated fat, sugar, and sodium in products. Calories, saturated fat, sugar, and sodium are the four basic elements displayed on a Facts Up Front–style FOP label. Sugar, sodium, and saturated fat are referred to by the FDA as the nutrients to limit, suggesting that consumers should try to limit their intake.[22] Because FOP labels clearly emphasize the calories and the three nutrients, and given the public emphasis on the negative health consequences of these nutrients over time ([54]), one would expect consumers to pay most attention to the calorie count and those nutrients that would induce firms to lower their content in products. Our results support this expectation, bolstering our argument for information salience as the underlying mechanism driving the FOP effect. Next, we discuss the implications of our findings for policy makers and for marketing. Implications for Policy MakersUnlike nutrition claims, which can selectively highlight only the nutrients that make a product look healthier, the FOP labels we examine are standardized and present the key nutrient information from the NFP on the front of the package. However, there can still be skepticism about the implications of the effect of FOP labels in the marketplace. Our results demonstrate that FOP labels are beneficial for consumers, as the labels tend to spur overall nutritional quality improvement in a product category. Drawing on a set of packaged food products (see Table 3), we find that FOP adoption leads to a decrease in average calories (−13.23%), saturated fat (−15.39%), sugar (−25.72%), and sodium (−19.08%). In addition, food manufacturers improve products' nutritional quality by reducing the content of nutrients to limit that are actually displayed on FOP labels. This implies that policy makers, in partnership with food manufacturers and retailers, should encourage adoption of voluntary labeling programs that are standardized and transparent, such as Facts Up Front–style FOP labels, and consider options for broadening the information presented in FOP labels. We believe that policy makers should also invest in educational campaigns that inform consumers about the value of FOP labels, which would provide more incentives for food manufacturers to offer nutritionally better products. Implications for MarketingOur results have implications for food manufacturers and grocery retailers. For food manufacturers, the result that FOP adoption can stimulate improvement in the nutritional quality of food products in the category implies that manufacturers must devote significant resources to product innovation to stay competitive. Given the result that firms innovate and produce nutritionally better products following FOP adoption, firms that lag in innovation will fail to attract enough consumer demand to survive and compete in the category. Specifically, manufacturers in unhealthy and more competitive categories can be more strategic and invest in innovation such that they are ready to provide better products following FOP adoption. For food retailers, our results suggest that they should partner with manufacturers and give them incentives to adopt FOP, as this can lead to better-quality products for their consumers, which can ultimately help in building a positive brand image. Retailers can also promote products with FOP labels, especially in more competitive and unhealthy product categories, which can spur manufacturers toward more innovation and lead to an increase in the nutritional quality of the foods over time in the category. We encourage retailers to invest in measures that help monitor and track the sales of products with FOP labels and provide this feedback to their manufacturers regularly to speed up the competitive effect of FOP labels. It is worthwhile to note how the Smart Choices logo developed by the food industry, including grocery retailers, received a lot of criticism and was eventually suspended when it started showing up on products such as Kellogg's Froot Loops cereal ([49]). Although retailers have invested in developing and promoting some FOP labeling systems, we suggest that retailers must invest in and promote a comprehensive, universal, and simple-to-use and understand FOP labeling system that consumers can trust unequivocally.From the consumer perspective, although extant research has documented that consumers pay attention to FOP labels ([25]), we establish that FOP adoption results in nutritionally better products on retailers' shelves. Our results show that the FOP effect is greater for premium brands and brands with a narrower product line breadth. These results suggest that consumers who are looking for healthier alternatives should consider premium brands and more focused brands in terms of product line in their consideration sets. We also find that the brands that adopted FOP labeling have nutritionally better products than those that did not adopt the labeling. This suggests that the presence of a FOP label on a package is a good indicator that the product is a better choice overall than other products that do not carry FOP labels. In summary, our findings offer insights for policy makers, manufacturers, retailers, and consumers and help solidify FOP labeling in tackling the obesity epidemic. Limitations and Directions for Future ResearchAlthough this study is the first to conduct a systematic and empirical analysis of the impact of FOP adoption on nutritional quality of products, it is not without its limitations. When possible, a randomized controlled trial can help establish the causal effect of FOP adoption cleanly; however, it was not practical in this context. Thus, we relied on panel data and econometric techniques to shed light on the causal effect that is valid within the bounds of the DD modeling approach and its identifying assumptions. We focused on one widely used and standardized FOP label. We suggest that future research could examine other types of labels. Given the competitive response to FOP adoption, future research could examine the effect of FOP adoption on various market structure–related questions, such as entry and exit of brands following FOP adoption, change in brand- versus category-level sales, customer brand loyalty and underlying brand switching patterns, and marketing-mix effectiveness of brands that adopt FOP labels. We believe that this study sheds light on the importance of firms' voluntary participation in initiatives that signal stewardship of corporate social responsibility. We hope that this study encourages researchers to examine the consequences of firms' adoption of nutrition-related policy changes as public policy makers continue to find ways to encourage consumers to make healthier dietary choices. "
17,"Converging on a New Theoretical Foundation for Selling This article demonstrates that the sales literature is converging on a systemic and institutional perspective that recognizes that selling and value creation unfold over time and are embedded in broader social systems. This convergence illustrates that selling needs a more robust theoretical foundation. To contribute to this foundation, the authors draw on institutional theory and service-dominant logic to advance a service ecosystems perspective. This perspective leads themto redefine selling in terms of the interaction between actors aimed at creating and maintaining thin crossing points—the locations at which service can be efficiently exchanged for service—through the ongoing alignment of institutional arrangements and the optimization of relationships. This definition underscores how broad sets of human actors engage in selling processes, regardless of the roles that characterize them (e.g., firm, customer, stakeholder). A service ecosystems perspective reveals ( 1) that selling continues to be an essential activity, ( 2) how broader sets of actors participate in selling processes, and ( 3) how this participation may be changing. It leads to novel insights and questions regarding gaining and maintaining business, managing intrafirm and broad external selling actors, and sales performance.Over the last decade, sales scholars and practitioners have debated what selling entails, how salespeople participate in value creation, and whether the importance of salespeople is increasing or decreasing. The commonly described catalyst for these debates is a rising degree of market complexity caused by increasing customer demands, globalization, buying and selling centers, number of offerings, technological advancements, competitive challenges, and buyer access to information (see Hunter and Perreault 2007; Jones, Chonko, and Roberts 2004; Moncrief and Marshall 2001; Rackham and DeVincentis 1998; Schmitz and Ganesan 2014; Sheth and Sharma 2008).While markets are continually changing, we caution against premature conjectures that these changes necessarily alter what selling entails, how salespeople participate in value creation, and/or the importance of salespeople. Instead, we suggest these changes point toward the inadequacy of traditional, restricted, firm-centric, unidirectional, and dyadic views of sales processes. These changes, therefore, point to the need for a more robust theoretical foundation that better explicates the processes and roles of selling in value cocreation through market exchange.The contemporary sales literature seems to confirm this contention. For example, it indicates a changing view of the sales process from one that is linear and focused almost entirely on the buyer-seller dyad to one that is nonlinear and involves many actors (Dixon and Tanner 2012; Moncrief and Marshall 2001). This literature emphasizes the importance of intrafirm and external actors to selling and sales processes (Bolander et al. 2015; Plouffe et al. 2016) and points to the broadening and blurring of sales-oriented tasks and responsibilities to include those traditionally associated with other roles as reasons why a more holistic approach is needed in research and practice (Hughes, Le Bon, and Malshe 2012; Rapp et al. 2017). Considered together, the literature appears to recognize a need for a ""revised perspective"" (MacInnis 2011) that can account for the multidirectional nature of sales processes and how these processes are situated in complex, dynamic exchange systems of value creation.This revision of perspective reflects a broader transition in the understanding of value creation both within and outside of marketing. Compared with more traditional models of firm-created, value-laden output that is delivered to a waiting ""consumer,"" new models portray value as an outcome (e.g., Vargo and Lusch 2004) cocreated (e.g., Prahalad and Ramaswamy 2004) in networks (e.g., Hakansson and Snehota 1995) and systems (e.g., Edvardsson et al. 2014). The roles of institutions (i.e., practices, assumptions, norms, laws, beliefs, and values, among other coordinating heuristics) are also becoming apparent (e.g., Humphreys 2010; Press et al. 2014; Vargo and Lusch 2016). Furthermore, this revision of perspective reflects recent work on the conditions under which transactions take place (e.g., Baldwin 2008).We invoke the service ecosystems perspective of servicedominant (S-D) logic, which is based on the premise that broad sets of actors dynamically integrate and apply resources through service-for-service exchange (i.e., the application of knowledge for the benefit of another) to cocreate value (Vargo and Lusch 2004, 2016). We suggest this service ecosystems perspective offers a robust theoretical framework for examining selling and sales-related phenomena. It mandates an understanding of institutions and institutional arrangements (i.e., ""interdependent assemblages of institutions"" [Vargo and Lusch 2016, p. 6]) as coordinating mechanisms that enable and constrain value creation practices.A service ecosystems perspective increases the range of activities and the number of actors considered to be involved in selling. This perspective expands the view from dyadic exchange to broader value creation practices influenced by institutional arrangements and institutionalization processes. It accommodates micro-level outcomes, such as sales performance (e.g., sales revenue, percentage of quota met) and buyer-seller relations (e.g., relationship quality, perceived value) (Ahearne et al. 2013; Hall, Ahearne, and Sujan 2015; Hughes, Le Bon, and Rapp 2013; Mullins et al. 2014) and also highlights the importance of often-ignored emergent, meso- and macro-level institutional structures related to selling, such as markets and industries and their roles both as outcomes and contexts (Giddens 1984). To elaborate our theoretical foundation, we also draw on the work of Baldwin (2008; Baldwin and Clark 2000), which explores the interplays of formal and relational exchanges across ""thin"" and ""thick"" crossing points related to these emergent structures. This showcases the importance of institutional work—the maintaining, disrupting, and changing of institutional structures (Lawrence and Suddaby 2006)—in selling processes.This article makes three contributions to the sales literature. First, it offers a theoretical foundation that reframes conceptions of what selling is and the activities it encompasses. Using the service ecosystems perspective of S-D logic (Vargo and Lusch 2004,2016), which highlights that value is always cocreated by multiple actors, we show that salespeople and other actors foster service-for-service exchange and value cocreation by participating in institutionalization processes. These institutionalization processes include the creation of knowledge structures that aid in sense making and legitimation (Phillips, Lawrence, and Hardy 2004; Suchman 1995; Weick 1995). To explicate the mechanisms for these processes, we introduce a framework that points to discursive and dialogical interactions among broad sets of actors.Second, the article contributes to the sales literature by reconceptualizing and broadening the scope and roles of various actors in the sales process. Traditionally, selling refers to an attempt by a salesperson to persuade a buyer to accept a value proposition. Alternatively, we define selling in terms of the interaction between actors aimed at creating and maintaining thin crossing points—the locations at which service can be efficiently exchanged for service—through the ongoing alignment of institutional arrangements and the optimization of relationships. As we detail, this reconceptualization highlights the importance of distinguishing between salespeople and broader sets of actors who engage in selling activities. Thus, we use the ""salespeople"" classification for actors whose professional roles (i.e., job descriptions and titles) are sales-centric and the broader ""selling actor"" classification for all actors who perform selling regardless of their role. That is, the selling actor classification includes salespeople but is not limited to them.Third, this article contributes to the sales literature by addressing unresolved questions about whether changes in markets are changing the roles of salespeople. As Rackham and DeVincentis (1998) and Jones et al. (2004) highlight, some theoreticians and practitioners believe that changes in modern markets will diminish the strategic importance of salespeople, perhaps making them obsolete. Conversely, others argue that modern markets are increasing the strategic importance of salespeople (Cron et al. 2014; Hunter and Perreault 2007) and that their importance is likely to increase. A service ecosystems perspective reconciles these inconsistent viewpoints by reframing the fundamental mechanisms of selling. This perspective illustrates how markets have always been complex and dynamic and how selling actors have always been and continue to be involved in institutionalization processes that resolve inconsistencies and contradictions in the institutional arrangements of various actors. However, changes in modern communication tools enable nontraditional actors to engage in selling and may, ironically, mask selling processes.The remainder of this article is structured as follows. First, we show that the sales literature, like the broader marketing literature, is converging on a service ecosystems perspective that views value as cocreated through the involvement of broad sets of actors. Second, we propose that this perspective can serve as the foundation for a unifying theoretical framework for sales. Third, we describe the characteristics of ""crossing points,"" the locations at which service is exchanged for service, and we highlight the role of institutional arrangements in shaping these crossing points. Fourth, we redefine ""selling,"" introduce a discursive framework that explicates the role of narratives (i.e., written, spoken, or symbolic accounts that offer interpretation, explanation, or meaning to events or actions [Czarniawska 2004]) in selling processes, and highlight the fundamental similarities among actors in what are traditionally referred to as sales and nonsales roles. Finally, we discuss the theoretical and practical implications of this research. Throughout, we draw on concepts and literatures that might be unfamiliar to some readers. To assist, we define key terms in Table 1.TABLE: TABLE 1 Definitions of Key Terms TABLE 1 Definitions of Key Terms   Transitioning Toward a Service Ecosystems Perspective for the Sales Literature Evolution of the Sales LiteratureThe ""birth"" of the modern salesperson is often attributed to the late-nineteenth and early-twentieth century with the development of mass manufacturing (Friedman 2005). Because of the influence of classical and neoclassical economics, value was then thought to be created and embedded in goods by selling firms through the manufacturing process (Vargo and Lusch 2004). The role of salespeople was generally perceived to comprise the facilitation and negotiation of the transfer of value from sellers to buyers. This view contributed to a transactional selling orientation that emphasized short-term outcomes, a clear winner in exchange, and the salesperson's ability to manipulate buyers to produce self-serving results (Jolson 1997).However, since the 1970s, researchers and practitioners have increasingly recognized the importance of relationship selling. Relationship selling emphasizes the roles of salespeople in developing and maintaining relationships with buyers for mutual long-term benefits (Dwyer, Schurr, and Oh 1987; Weitz and Bradford 1999). As Jolson (1997) explains, ""Instead of viewing selling as a series of struggles that the salesperson must win from a steady stream of prospects and customers of all sizes and shapes, relationship selling or partnering focuses on the building of mutual trust within the buyer-seller dyad with a delivery of anticipated, long-term, value-added benefits to buyers"" (p. 76).Recent sales orientations, such as consultative and enterprise selling (Rackham and DeVincentis 1998), accentuate characteristics of relationship selling (e.g., trust, long-term emphasis on benefits). Such orientations also increasingly question narrow buyer-seller dyads and point out that selling and value creation unfold over time in complex systems involving many actors.Consultative selling, for example, which Rackham and DeVincentis (1998) attribute to increasingly sophisticated buyers and buying processes, emphasizes the importance of salespeople providing buyers with information, helping buyers discover and understand needs, determining and providing adequate and often customized solutions, performing nonselling tasks (e.g., planning, analysis, preparing proposals), and involving additional personnel in sales efforts. Such tasks necessitate the awareness and participation of broad (sets of) actors in value creation (e.g., competitors and collaborators of both the selling and buying organizations, intra- and interfunctional actors). Only with such awareness and participation can salespeople learn and communicate the tailored ramifications of competitors and other actor developments, determine and communicate how the seller and selling organization can benefit the buyer's organization, and identify and coordinate the involvement of other actors, among other things. A Bose salesperson, for example, makes tailored proposals to automotive manufacturers based on the broad involvement of actors (e.g., procurement, engineering, design, marketing) from the buying and selling organizations as well as other market actors (e.g., industry experts, other customers) to transfer home and music venue audio technology to optimize vehicle specific sound dynamics.Enterprise selling adopts and extends the principles of consultative selling to emphasize that buyers aim to benefit from the knowledge and skills of the entire selling organization. That is, as Rackham and DeVincentis (1998) describe, enterprise selling emphasizes developing close-knit buyer-seller interfaces to leverage the knowledge sets and skills of many different actors and functions of both the selling and buying organization to create value. Therefore, enterprise selling often results in even broader and deeper integration of the buying and selling organization than does consultative selling. It also results in broader awareness and participation of a greater number of actors involved in value creation, given the numerous cross-functional and cross-organizational actors involved who themselves are embedded in networks of actors. Consequently, any individual actor or function has limited ability to initiate and maintain an enterprise relationship. Consider, as an example of enterprise selling, Amazon's solutions for small businesses. As of 2017, such solutions include access to a rich e-commerce platform, as well as vast services (e.g., customer service, multichannel fulfillment, loans, and information technology) that broadly integrate both organizations' knowledge sets and skill bases.In line with an enterprise selling orientation, scholars (e.g., Bolander et al. 2015; Dixon and Tanner 2012; Friend and Malshe 2016; Hughes, Le Bon, and Malshe 2012; Macdonald, Kleinaltenkamp, and Wilson 2016; Plouffe et al. 2016; Rapp et al. 2017) are increasingly recognizing that selling and value creation unfold over time in complex systems involving many actors. Macdonald, Kleinaltenkamp, and Wilson's (2016) findings, for example, propose that value emerges over time and that value propositions are mutually defined and depend ""on the quality not only of the supplier' s resources and processes but also of customer resources and processes as well as of the joint resource integration process"" (p. 97). Much like practitioners, Dixon and Tanner (2012) report that scholars are beginning to view the selling process as nonlinear and involving many actors instead of viewing it as a linear multistep process that may focus too closely on the buyer and salesperson. Others (e.g., Hughes, Le Bon, and Malshe 2012; Rapp et al. 2017) argue that the sales function is increasingly broadening, blurring with other functions, and reciprocally influencing other firm functional areas and that these are reasons why a holistic approach is needed.To evaluate the extent to which the sales literature employs a systemic perspective, we performed a frequency analysis of articles in marketing's leading generalist journals (i.e., Journal of Consumer Research, Journal of Marketing, Journal of Marketing Research, and Marketing Science) as well as the specialized Journal of Personal Selling & Sales Management, whose abstracts used terms characteristic of a systemic perspective. As Figure 1 shows, the results, which are consistent with our observations of work published in other reputable outlets, indicate that the sales literature is increasingly adopting such a systemic perspective. Table 2 depicts the evolution of the three discussed sales orientations to further support our conclusion. Next, we argue that this systemic perspective requires a theoretical foundation that recognizes the roles of institutional arrangements in enabling and constraining value creation practices.TABLE: TABLE 2 Evolving Perspectives Within the Sales Literature TABLE 2 Evolving Perspectives Within the Sales Literature  Notes: • = selling actor; O = buying actor. Systemic and Institutional Thought in Marketing and SalesAs Scott (2013) explains, ""Institutions are multifaceted, durable social structures made up of symbolic elements, social activities, and material resources"" (p. 57). Institutions ""provide stability and meaning to social life"" (p. 56) and, thus, efficiently and often effectively guide actors' practices. Vargo and Lusch (2016) posit that institutions permit coordination among actors and ""enable actors to accomplish an ever-increasing level of service exchange and value cocreation under [inherent] time and cognitive constraints"" (p. 11). Therefore, the study of institutions can aid understandings of what selling is and how selling actors facilitate value cocreation for their own firms, buying firms, and broader sets of actors.Early marketing literature (e.g., Alderson 1957; Arndt 1981; Duddy and Revzan 1953; Hunt 1981; Revzan 1968; Weld 1916) emphasizes systemic and institutional approaches that account for actors' functions, roles, interactions, and relational mechanisms as foundational to marketing. Despite this early recognition, systemic and institutional thought has not received prominent attention in the marketing and sales literature streams. However, contemporary marketing work (e.g., Edvardsson et al. 2014; Hillebrand, Driessen, and Koll 2015; Vargo and Lusch 2016; Webster and Lusch 2013) is revitalizing awareness that systemic and institutional thought is foundational to marketing and, arguably, also to sales. Edvardsson et al. (2014, p. 303), for example, state that institutional arrangements ""coordinate the activities of resource integration by shaping the actors' value cocreation behavior in service systems,"" and Vargo and Lusch (2016, p. 5) claim that institutional arrangements are ""the keys to understanding human systems and social activity, such as value cocreation, in general.""This contemporary marketing literature is greatly influenced by sociological and organizational theory, which has made significant progress in overcoming overly rational and static views on institutions. Specifically, scholars such as Bourdieu (1977), Giddens (1984), and DiMaggio (1988) clarify the enabling and constraining properties of institutions by addressing the tensions between structure (i.e., normative constraints) and agency (i.e., the ability to act independently). Similarly, Scott (2013) claims that ""institutions provide stimulus, guidelines, and resources for acting as well as prohibitions and constraints on actions"" (p. 58) and argues that institutions comprise regulative, normative, and cultural-cognitive elements.The regulative element comprises processes that have ""the capacity to establish rules, inspect other's conformity to them, and as necessary manipulate sanctions—rewards or punishments—in an attempt to influence future behavior"" (Scott 2013, p. 59). These sanctions can be formal (e.g., licenses or court punishments) or informal (e.g., losing or gaining face through shaming or legitimizing activities) (North 1990). The normative element describes prescriptive, evaluative, and obligatory dimensions of social life. This element emphasizes values and norms and how these values and norms shape actor roles. Accordingly, the normative element emphasizes desired ends (e.g., goals, objectives) as well as how actors may pursue them.Finally, the cultural-cognitive element comprises ""the shared conceptions that constitute the nature of social reality and create the frames through which meaning is made"" (Scott 2013, p. 67) or, stated alternatively, what underlies the habitual actions of actors. Jointly, these three pillars span the conscious and the unconscious, or, similarly, the legally enforceable and the taken-for-granted elements of institutions (Hoffman 2001; Scott 2013).In marketing, the three institutional pillars are used both explicitly and implicitly. For example, work on industries, channels, and strategic orientations by Humphreys (2010), Grewal and Dharwadkar (2002), and Press et al. (2014) explicitly highlights the importance of these institutional pillars.However, most work examining selling and buyer-seller relationships has addressed these three institutional pillars implicitly and atomistically. Not surprisingly, much work on selling and buyer-seller relationships has focused on regulative elements that can be monitored and sanctioned, such as formal contracts that often define responsibilities, measures, and compensations. Dwyer, Schurr, and Oh (1987), for example, highlight the importance of contractual obligations, such as exchange timing, planning, and relative allocation of benefits and costs, in relationship development processes between sellers and buyers.However, the costs of employing comprehensive formal contracts to account for every responsibility, measure, and compensation when exchanges are complex can become excessive (Baldwin 2008) and can potentially surmount the value offered by the exchange itself. This limitation of incomplete contracts highlights the importance of the normative pillar in structuring economically efficient relationships. Complementing Dwyer, Schurr, and Oh (1987), Heide and John (1992) as well as Cannon, Achrol, and Gundlach (2000) point to the importance of relational norms, such as flexibility, solidarity, mutuality, harmonizing of conflict, and restraint in the use of power, to safeguarding relationships. The normative element, however, is not limited to relational norms and contracts but also emphasizes what ends are desired as well as how actors may pursue them. The sales literature, as stated, has argued that salespeople increasingly take on the role of knowledge brokers and consultants that aid buyers, their own firms, and other actors in better understanding insights and implications of ever-changing problems, markets, and potential solutions to co-create mutual long-term benefits (Rapp et al. 2014; Sheth and Sharma 2008; Verbeke, Dietz, and Verwaal 2011).Finally, the cultural-cognitive element is also crucial in understanding buyer-seller relationships and selling because cultural models ""are heterogeneously distributed across a population and serve as cognitive resources and templates that help people navigate the world around them"" (Blocker et al. 2012, p. 23). Work on consumer culture theory, for example, highlights not only ""the multiplicity of overlapping cultural groupings"" but also how ""product symbolism [and] ritual practices"" shape patterns of behavior and sense making (Arnould and Thompson 2005, pp. 869-70). Similarly, work on the social construction of technology has highlighted that resources are ""socially constructed by [systemic] actors through the different meanings they attach to [them] and the various features they emphasize and use"" (Orlikowski 1992, p. 406). Consequently, selling and exchange practices cannot be understood without taking the cultural- cognitive element into consideration. Actor Coordination in Service Ecosystems S-D Logic and Its Emphasis on Service EcosystemsThe previous two sections showcase that the sales and marketing literature streams are converging on a systemic perspective that highlights the importance of institutional arrangements. To provide a theoretical foundation for this perspective, we introduce the S-D logic framework and its core contentions. Service-dominant logic emphasizes that ""marketing activity, and economic activity in general, is best understood in terms of service-for-service exchange"" (Vargo and Lusch 2017). Service, in this framework, is conceptualized as the application of one actor's resources for the benefit of another actor. Service-dominant logic posits that value cocreation takes place in systems, because the resources used in service exchange typically come from a variety of private, public, and market-facing sources—that is, from a variety of other actors. Furthermore, S-D logic asserts that actors' resource integration and value cocreation practices are enabled and constrained by institutional arrangements (Vargo and Lusch 2016).In short, S-D logic theorizes that value cocreation takes place in service ecosystems—that is, ""relatively self-contained, self-adjusting system[s] of resource-integrating actors connected by shared institutional arrangements and mutual value creation through service exchange"" (Vargo and Lusch 2016, pp. 10-11). These institutional arrangements can be observed at multiple levels of aggregation. They include relative perspectives of micro-level institutions of individuals, groups, and firms; meso-level institutions, such as those associated with professions, markets, or industries; and macro-level societal institutions (Lawrence and Suddaby 2006; Thornton, Ocasio, and Lounsbury 2012; Vargo and Lusch 2016). In the following subsection, we expound on the systemic exchange of specialized knowledge and skills, which is foundational to S-D logic. Subsystems and Increasing SpecializationSimon (1996) notes that dynamic social systems, such as the ecosystems in which actors exchange service, often are composed of interdependent subsystems. These subsystems or modules can be defined as ""group[s] of elements, such as tasks, that are highly interdependent on one another, but only minimally dependent on what happens in other modules"" (Baldwin and Clark 2000, p. 63). As we have stated, the specialized knowledge and abilities required for value creation often come from a variety of other actors or groups of actors (i.e., relatively independent subsystems). In such subsystems, actors—whether they are individuals, teams, firms, and so on—work on a limited number of tasks that are part of a larger task system in which actors exchange resources and cocreate value. These subsystems permit actors to mitigate some of the restrictions of their limited cognitive abilities. That is, actors usually participate in value cocreation processes without the knowledge to fully understand or perform entire sets of these processes. This specialization results in information asymmetries among exchanging actors because these actors need to possess only the knowledge required to complete their tasks and to coordinate with the tasks of others. Herein, we explain that selling enables this coordination.Mass production and other developments designed to improve effectiveness and efficiency have, arguably, contributed to an increase in the number of subsystems of many service ecosystems by separating production and use tasks. Similarly, developments in communication and other technologies have, at least partly, resulted in growing specialization within selling and buying processes. Firms, for example, have increasingly modularized selling and buying into multiple subsystems, as exemplified by the prevalence of salesperson categorizations (e.g., inside vs. outside salespeople, hunters vs. farmers) and both selling and buying centers (i.e., multiple actors specializing in functional subareas or tasks). As a result, salespeople have become increasingly tasked with coordinating the resources and actions of various actors across their firm, buyers' firms, and other actors (e.g., third-party solution providers, regulatory bodies). In the following subsection, we discuss the coordination of resources among subsystems by introducing the concept of crossing points. Crossing Points and Aligned Institutional Arrangements for Service ExchangeBaldwin (2008) refers to locations where transfers of material, energy, and information between two subsystems occur, such as the one between a service provider and a service beneficiary, as a ""crossing point."" Thus, using the lexicon of S-D logic, a crossing point can be viewed as the location at which service can be exchanged for service. Baldwin indicates that crossing points can be ""thin"" or ""thick."" Thin crossing points permit exchange through shallow and simple interactions, whereas thick crossing points require actors to develop deep and complex interactions to exchange with one another (Baldwin and Clark 2000).In highly institutionalized markets, for example, many crossing points are relatively thin because of established regulations, laws, relational and formal contracts, conventions, and shared meanings, which keep transaction costs relatively low (North 1990). For such thin crossing points to form, exchanging actors must align on ""common ground design rules"" (Baldwin and Clark 2000). These rules consist of the mutual definition of what is being reciprocally exchanged and the norms and representations that guide exchange practices (Kjellberg and Helgesson 2006).Thick crossing points, in contrast, such as those associated with discontinuous solutions and newly forming markets, lack many of these common ground design rules. Consequently, when crossing points are thick, exchange may be prevented or require the formation of deeper and more complex interactions, such as the formation of new relational contracts (North 1990). Consider, for example, self-driving cars. Before self-driving cars can be efficiently exchanged, meanings and perceptions regarding these cars (e.g., safety, legality) and exchange practices (e.g., ownership vs. on-demand ordering) must become mutually aligned, which requires deeper, costlier, and more complex actor involvement. Aligned Institutional Arrangements for Service ExchangeThe formation of common ground design rules can be viewed as the emergence, stabilization, and destruction of predominant meanings and uses of resources through institutionalization (see also Pinch and Bijker 1984). Viewed from a service ecosystems perspective, common ground design rules can be conceptualized as the aligned institutional arrangements for service exchange that guide the meanings of resources and their integration practices. These aligned institutional arrangements facilitate service exchange and often make it less costly for actors to exchange.Consider, as an example, the early sales strategy of Salesforce.com, with its software as a service (SaaS) solution. In 1999, when Salesforce was founded, client-server software solutions, which stored data behind company firewalls and were often only accessible on company sites, dominated the customer relationship management (CRM) industry. Salesforce's SaaS solution, in contrast, stored the customer and prospect data, as well as the underlying CRM software, in the cloud. This offered great benefits to users with regard to accessibility, scalability, flexibility, and cost. However, storing proprietary customer information in the cloud was deeply incompatible with existing institutional arrangements regarding data security of a broad set of actors, such as users, managers, information technology (IT) professionals, and other industry experts.This example highlights that service exchange often requires complex descriptions, information exchange, negotiations, trust, and unconscious cultural-cognitive alignments of service expectations among many actors. Arguably, service-forservice exchange can only be understood by observing institutional elements, such as laws and regulations, written or oral contracts, relational norms, perceptions of solutions, and shared conceptions of acceptable business practices, in combination. It is therefore necessary to view selling and institutional alignment holistically, instead of only addressing institutions in somewhat disparate subcategories. Such a holistic perspective can highlight situations in which ruptures within and among the institutional elements create opportunities for change (Thornton, Ocasio, and Lounsbury 2012). Similarly, a holistic perspective can expose situations in which aligned institutional arrangements create lock-ins and path dependencies that can suppress the selling efforts of actors aiming to bring about new solutions. That is, as we explain next, aligning institutional arrangements for service exchange (i.e., the thinning of crossing points) can simultaneously thicken the crossing points for competing solutions.A holistic service ecosystems perspective helps connect and extend the existing sales literature's implicit and piecemealed focus on institutions by providing a more robust and encompassing perspective through which to examine selling efforts. While the sales literature has begun to emphasize the roles of salespeople in cooperating with many actors, both internal and external to the selling firm (Bolander et al. 2015; Plouffe et al. 2016), it often underemphasizes broader and more indirect processes through which systemic actors collectively influence aligned institutional arrangements for service exchange. This is problematic because, as we have stated, resources used in value creation practices are typically sourced, directly and indirectly, from many private, public, and market-facing sources. Most of these practices require a multitude of resources and, consequently, a multitude of thin crossing points.The Salesforce example helps clarify this idea. Salesforce recognized early that a widespread shift from a client-server to a SaaS solution could only be achieved by the thinning of multiple crossing points among a broad range of actors. Implementing a CRM solution requires expertise from users, IT professionals, vendors, finance and accounting personnel, external implementation consultants, management, and many other actors. Consequently, Salesforce employees directed their selling efforts toward multiple actor groups including customers, prospects, journalists, bloggers, and internal employees, because all these actors were involved in the alignment of the institutional arrangements required for service exchange.Furthermore, recognizing the importance of nonadopters in institutional developments, Salesforce spent time with large enterprises—prospects that they were not initially able to serve—to learn what additional functionality would be required to make them consider the SaaS solution. That is, Salesforce recognized the importance of nonusers in institutional alignments. As Benioff and Adler (2009) state, Salesforce might have discovered some of the needs of large enterprises on its own, but, without this dialog, they would not have learned the context in which perceptions of needs were formed.In summary, the service ecosystems perspective highlights that systemic actors create mutual value through service exchange, guided by shared institutional arrangements. This perspective can aid the convergence of the sales literature on a truly systemic and institutional view by highlighting the need to zoom out beyond the buyer-seller dyad to a view that includes a broad range of systemic actors who all participate in the shaping of value cocreation practices. This broader view does not diminish the importance of understanding the buyer-seller dyad; rather, it highlights that fully understanding value cocreation practices requires looking at the involved institutional elements from different levels of aggregation because dyads are always embedded in broader social systems (Chandler and Vargo 2011). In the next section, we briefly discuss institutional work in service ecosystems—that is, the creation, maintenance, or disruption of institutions—before we offer a more transcending definition of selling. Institutional Work in Service Ecosystems: A New Framework for SellingMultiple strands of institutional literature have made substantial progress in explaining the tension between agency (i.e., the capacity of actors to make choices independent of the influence of structure) and structure (i.e., the extent to which institutional arrangements determine the thoughts and behaviors of actors) with regard to institutional change. The tension between agency and structure is essential in the context of selling because this tension is foundational to understanding whether, and to what extent, an actor can influence the institutional arrangements that shape perceptions of problems and the resource-integration practices that serve as solutions.DiMaggio (1988), an institutional theorist who emphasized the agency of actors, introduced the concept of the ""institutional entrepreneur,"" which refers to an actor who initiates changes that contribute to creating new or transforming existing institutional arrangements. It is tempting to view salespeople and other selling actors as institutional entrepreneurs because creating new and transforming existing institutional arrangements closely aligns with traditional views of salespeople (i.e., persuading buyers to enact desired exchange practices). Dixon and Tanner (2012), for example, indicate that ""salespeople today must see their role as the architect for change in their customers' worlds"" and that ""salespeople add value when they can challenge the existing paradigms and provide a better decision-making process than the one used currently by a customer"" (p. 12). Similarly, Dixon and Adamson (2011) encourage salespeople to challenge the ways buyers think (i.e., change their institutional arrangements).More recent institutional literature, however, has emphasized that actor involvement in change processes is always broad and systemic. That is, this literature provides a more balanced view of agency and structure. Influenced by seminal work on practice theory (e.g., Bourdieu 1977; DiMaggio 1988; Giddens 1984; Oliver 1991), Lawrence and Suddaby (2006) illustrate how institutional change results from the activities of various interconnected actors as they repair and conceal tensions and conflicts—while also reinforcing similarities—in their existing institutional arrangements. Thus, as Zietsma and McKnight (2009) elucidate, institutional change always involves multiple actors who, iteratively and nonlinearly, bring about (imperfect) alignments in their institutional arrangements. This implies that, at least singly, selling actors' behaviors may not be as influential in changing the institutional arrangements of buying actors, such as the enactment of new value cocreation practices, as much of the sales literature suggests.Furthermore, selling actors need to be viewed as engaging in not only the change and disruption of institutional arrangements but also their maintenance. ""Institutional work,"" as Creed, DeJordy, and Lok (2010, p. 1337) point out, is not ""aimed at either the creation, maintenance, or disruption of institutions but can paradoxically involve more than one of these categories at the same time."" Even the most transformative change is institutionally embedded and, therefore, relies on existing resources and skills (Giddens 1984; Lawrence and Suddaby 2006). The innovative SaaS solution, for example, maintained many of the institutions associated with client-server-based CRM software. Foundational to all CRM solutions, for example, is the need to store and manage customer and prospect information in one central location to help firms improve interactions, gain access to information, and automate selling and marketing activities. Viewing Selling from an Institutional and Relational PerspectiveWe have highlighted that value unfolds over time through the integration of resources in a social context (i.e., relationships and institutions). Thus, aligned institutional arrangements for service exchange are not limited to expectations for discrete exchange events. Rather, they represent the outcomes of systemic institutionalization processes that guide how resources are perceived and integrated over time. However, institutional alignments are always imperfect and temporary because the nested nature of institutional arrangements results in continual incompatibilities. That is, these alignments often result in frictions within and among institutional arrangements that span regulative, normative, and cultural-cognitive elements. As Scott (2013) notes, these incompatibilities and frictions often provide the conditions for institutional change.Selling is often defined as a paid, promotional, interactive, and personal approach involving clearly defined buyer and seller roles. Citing changes to markets and sales processes as reasons why a new definition of selling is needed, Dixon and Tanner (2012) redefine selling as ""the phenomenon of human-driven interaction between and within individuals/organizations in order to bring about economic exchange within a value-creation context"" (p. 10). While this broader definition is an important step in the right direction, it does not clearly identify the mechanisms and benefits of this interaction. That is, the efficiency of exchange and value cocreation among actors is positively shaped by aligned institutional arrangements and mutually beneficial relationships. Therefore, we define selling as the interaction between actors aimed at creating and maintaining thin crossing points—the locations at which service can be efficiently exchanged for service—through the ongoing alignment of institutional arrangements and the optimization of relationships.This definition accentuates how institutional alignment processes are characterized by the ""ongoing negotiations, experimentation, competition, and learning"" (Zietsma and McKnight 2009, p. 145) of systemic actors. This dynamic is illustrated by the broad involvement of many actors (e.g., customers, prospects, media and IT consultants) in the institutionalization of the Salesforce solution. Because institutional alignments are always imperfect and temporary, these alignment processes can range from resolving complex institutional discrepancies (e.g., a novel value proposition), to ""nearly invisible and often mundane … day-to-day adjustments, adaptations, and compromises"" (Lawrence, Suddaby, and Leca 2009, p. 1) of routinized selling processes (e.g., a reorder). Next, we further clarify the broad involvement of actors in selling activities. Then, we explicate how individual actors participate in systemic selling processes and how context influences whether an actor should be considered a selling actor. This enables us to distinguish between selling and nonselling actors and activities. Selling and the Thinning and Thickening of Crossing PointsWhile it is tempting to view selling as a micro-level process in which dyads of buying and selling actors are engaged in the thinning of crossing points (i.e., aligning institutional arrangements for service exchange), selling can only be fully understood by oscillating foci among micro, meso, and macro perspectives (to capture the influence of societal intellectual property rights, industry standards, etc.; see also Chandler and Vargo 2011). Therefore, a service ecosystems perspective highlights the importance of zooming out to a meso-level view because this is the level where many thick crossing points become salient. While thick crossing points can be observed at all levels of aggregation, such as the distrust between two actors (i.e., micro level) or the use of child labor (i.e., societal level), the Salesforce example illustrates the importance of the meso-level perspective to selling.Consider, for example, the once-perceived need to store data behind company firewalls. This perceived need created a thick crossing point that was based on aligned expectations of a broad set of actors (e.g., users, managers, IT personnel,enterprise software providers) in the software industry. Sales-force was unable to thin this crossing point by establishing relational contracts with customers alone. Rather, it, among other things, had to foster communication with broad sets of actors regarding the security and benefits of a cloud-based solution. As the SaaS solution became institutionalized, systemic actors not only learned to trust software that stored sensitive customer data in the cloud but came to expect other CRM solutions to be easily accessible, scalable, and flexible. Thus, the client-server-based solutions of Salesforce's competitors then faced thick crossing points that motivated or even forced them to adopt SaaS solutions (i.e., adapt to new institutional arrangements for service exchange).That is, thick crossing points for competing solutions, viewed from a meso level, often form on the basis of disruptions and changes to existing expectations for service exchange among broader sets of actors such as professions (e.g., IT professionals), industry experts (e.g., journalists), and market actors (e.g., vendors, customers) and their relational and formal contracts. Stated differently, the thinning of crossing points that occurs as an emerging solution is institutionalized commonly results in the thickening of others, which can lead to a previous solution being perceived as flawed or insufficient. Therefore, individual service-for-service exchange behavior ""does not make sense independent of meso-level structural influences"" (Vargo and Lusch, 2016, p. 18) and even broader societal norms and rules.Importantly, a service ecosystems perspective, with its institutional levels of aggregation, highlights the multitiered nature of sales objectives. Sellers often aim to create thin crossing points that allow for exchange between two actors (e.g., between buyers and sellers). However, because these thin crossing points are not independent of meso-level structures, selling actors also have to legitimize their solutions (i.e., create thin crossing points for their solution) among meso-level sets of actors such as professions, industries, and other market actors. For example, as Benioff and Adler (2009, p. 95) indicate, ""Winning huge customers, such as Dell and Japan Post, was game changing"" for Salesforce not only because of revenue and profit but because such wins shape perceptions of the desirability and appropriateness of a SaaS solution among large sets of actors. Similarly, selling actors often try to block the legitimacy of competing solutions (i.e., aim to create thick crossing points for these solutions). Only with thick crossing points for competing solutions can a solution gain a perception of uniqueness and superiority and avoid the perils of commoditization. Narratives as Mechanisms for InstitutionalizationThe involvement of a broad range of systemic actors in institutional work raises the question of how individual selling actors participate in the shaping of institutional arrangements that enable and constrain value cocreation practices through service exchange. To answer this question, we adapt Phillips, Lawrence, and Hardy's (2004) discursive framework of institutionalization (see Figure 2), which depicts narratives as mechanisms for institutional work and, more specifically, explicates relationships among narratives, institutions, and actions. This framework demonstrates how individual actors (e.g., salespeople) participate in the alignment and maintenance of institutional arrangements for service exchange without overstating the impact of these actors.As Phillips, Lawrence, and Hardy (2004) point out, many actions produce narratives, and these narratives can have a variety of forms. Czarniawska (2004) describes narratives as written, spoken, or symbolic accounts that offer interpretation, explanation, or meaning to an individual event or action (or to a series of events or actions). Actors engage in service-for-service exchange to combine their resources with those of other actors and, in this process, propose value propositions. The offering of such value propositions leads to the generation of narratives through written (e.g., emails, brochures), spoken (e.g., sales presentations) or symbolic (e.g., diagrams, models and pictures) means, which, when distributed and interpreted, influence institutionalization processes (Taylor and Van Every 1999).Phillips, Lawrence, and Hardy (2004) argue for sense making and legitimization as the theoretical underpinnings of these distribution and interpretation processes. Similarly, Snow (2008) suggests that narratives enable and constrain meaning construction because meaning making progresses throughout discussion and debate about germane issues, events, and topics of interest. These distribution and interpretation processes are also the venues through which the legitimacy associated with an action can be gained, maintained, or repaired (Phillips, Lawrence, and Hardy 2004). Two of the earliest tasks for Salesforce, for example, were to explain cloud computing (i.e., to facilitate sense making) and to resolve data security concerns (i.e., to gain legitimacy).For narratives to shape institutional arrangements and enable and constrain value cocreation practices, they must become embedded in broader discourses to achieve generalized meanings. That is, narratives interact (Boje 1991) and form narrative infrastructures—cocreated narratives that ""emerge from a process in which fragments of different micro and macro narratives get layered on top of each other"" (Seidl and Whittington 2014, p. 5)—that aid sense making and legitimization. Rosa et al. (1999), for example, suggest that narratives ""are critical sensemaking tools"" (p. 68) for the formation of exchange and markets.Deuten and Rip (2000) highlight that in social systems, there is no single author and no master text being written but, rather, that multiple stories (i.e., narratives) come into alignment to form narrative infrastructures. According to these scholars, these infrastructures can be seen ""as the 'rails' along which multi-actor and multi-level processes gain thrust and direction"" (p. 74). Only these combined narrative infrastructures can craft coherence among social actors and mobilize support for particular practices (Araujo and Easton 2012). That is, only combined narrative infrastructures can lead to the shaping of institutional arrangements.Salesforce, for example, recognizing the importance of broader narrative developments, treated journalists as company friends and maintained a list of approximately two dozen highly influential journalists. These journalists received increased bilateral contact and access to executives and were often informed directly of company and industry developments. Salesforce used these relationships to gain access to information and to influence the narratives that these journalists published. This way, Salesforce was able to convey narratives to journalists that framed the SaaS solution positively. In addition to calling attention to Sales-force, such publicity played an important role in aligning narratives and facilitating institutionalization across the service ecosystem because ""unbiased references by experts carry tremendous power"" (Benioff and Adler 2009, p. 44). Salesforce also facilitated the distribution and interpretation of narratives and the formation and alignment of narrative infrastructures by providing online platforms which opened dialogue and aided the resource integration practices of many stakeholders.We, therefore, propose that understanding selling requires accounting not only for the narrative alignments of buyers and sellers but also for the dialogical processes that enable broad sets of actors to learn together (Ballantyne and Varey 2006). The notion of dialogical interaction further clarifies our definition of selling. Our definition excludes unidirectional forms of communication such as advertising because such forms lack interactional components. It also excludes interactions that solely rely on existing institutions and do not result in any adjustments, adaptations, and compromises between and among actors. Thus, we do not consider activities purely focused on order fulfillment as selling if such activities fail to contribute to sense making, legitimization, and the optimization of relationships. However, it is important to point out that fulfillment quality often affects relationship quality (e.g., trust) and can support or hinder selling outcomes. Building on the definition proposed by Dixon and Tanner (2012), the definition we advance recognizes that many actors are involved in selling, that selling can include many forms of interactions, and that selling is contextual. Next, before discussing some of the implications that a service ecosystems perspective offers to practitioners and academics, we discuss the distinctions between salespeople and other selling actors. Who Engages in Selling? Distinguishing Salespeople and Other Selling ActorsThe path dependency from dyadic views that frame value as something created by producers and consumed by users led to clear and persisting distinctions between the roles and functions of sellers and buyers (i.e., salespeople facilitate the delivery of value to buyers). However, as we have stated, the sales literature is beginning to recognize that value is cocreated among systemic actors and to accentuate the importance of salespeople in establishing information exchange, flexibility, and solidarity. This highlights that selling is not only performed by dedicated sales personnel but also by buyers and a broad range of other actors. When exchanging service, for example, flexibility, solidarity, and information exchange is generally as important to the procuring side as it is to the selling side. More broadly, all actors participate in the shaping of value cocreation practices by creating, maintaining, and disrupting the institutions that enable and constrain these practices. Therefore, a systemic view on value creation supports a move away from predesignated roles of firms/customers and sellers/buyers to more generic actors—that is, to an actor-to-actor orientation (Vargo and Lusch 2011).To make this point clearer, consider the following. A salesperson can engage in institutional maintenance by promoting the benefits of a current solution to buyers, but so too can a procurement specialist by rejecting a meeting with a sales engineer who wants to introduce a novel solution to a problem or by emphasizing why a current solution will be sustained. Furthermore, an actor external to the selling or buying firm, such as an expert, may engage in institutional maintenance by speaking of the benefits of an accepted solution. Likewise, when a new solution is proposed, actors employed by the procuring firm (e.g., procurement officers, users, executives) often engage in internal and external selling to influence the institutional arrangements of other stakeholders (e.g., operations, industry partners).Recognizing the important roles of users, for example, Salesforce went beyond the norm of incorporating customer testimonials into marketing materials and emphasized connecting customers to prospects, media sources, analysts, partners, and others at prospect-gathering events. Salesforce chief executive officer Marc Benioff describes how often at these events, ""without prompting from us, customers would stand up and deliver spontaneous testimony professing their belief in our product."" (Benioff and Adler 2009, p. 50). In this way, many customers became Salesforce evangelists whose testimonials, in the words of Benioff and Adler (2009), ""made them the best marketing and sales team an organization could have"" (p. 51). This anecdotal evidence supports Kumar, Petersen, and Leone' s (2013) findings that references greatly influence firms in making purchase decisions as well as Helm and Salminen's (2010) claim that a customer reference can, at times, be more valuable than a customer's transaction.Thus, what we refer to as selling cannot be confined to certain actor roles because many actors are often involved in the creation and maintenance of crossing points. This actor-to-actor orientation highlights how a broad set of actors, regardless of the term chosen to characterize them (e.g., sellers, buyers), engage in selling and, thus, are selling actors. This reconciles the conflicting definitions of salespeople prevalent across sales textbooks and practitioner-oriented books, articles, and other literature that have often argued that context (whether one offers a good or service in exchange for payment) dictates whether someone is a salesperson. Additional DiscussionThe introduced service ecosystems perspective illuminates, among other points, the need to reevaluate the conceptual underpinnings of selling and the sales role (for a comparison of key differences between a traditional and a service ecosystems perspective of sales, see Figure 3). As such, this research, which can be classified as a ""revising"" conceptual contribution (MacInnis 2011, p. 144), informs the literature by ""suggesting that what is seen, known, observable, or of importance can be seen differently or by suggesting that what matters a great deal matters for a different reason than what was previously believed."" A service ecosystems perspective offers a novel lens through which to view selling, one through which selling actors are viewed as playing fundamental roles in aligning institutional arrangements and optimizing relationships for service exchange among interdependent actors. MacInnis (2011, p. 138) claims that revising conceptual contributions should be evaluated by their abilities to ""identify why revision is necessary; reveal the advantages of the revised view and what novel insights it generates; maintain parsimony."" As we have articulated and expand on subsequently, a service ecosystems perspective offers a number of advantages and novel insights.To more adequately understand selling as well as the relationships between wider sets of selling actors, broader study of the many crossing points that need to be thinned to facilitate resource integration and value cocreation must be undertaken. That is, as the Salesforce example indicates, it is the thinning of many interconnected crossing points that leads to value cocreation. The institutionalization of the SaaS solution, for instance, began years before this solution was ever envisioned. Though fundamentally novel in the way data were accessed and stored, this solution heavily relied on many institutional arrangements that were formed through the institutionalization of client-server-based CRM tools. As we have stated, at their core, all CRM solutions are based on the belief that storing customer and prospect information in one central location can provide efficiencies and automation for many sales and marketing activities.Similarly, the increasing proliferation of the Internet, mobile connectivity, and the emergence of other cloud-based solutions undoubtedly shaped both the perceptions of problems that the SaaS solution aimed to address as well as perceptions of legitimate solutions to these problems. That is, the SaaS solution became successful because selling resulted in aligned institutional arrangements for service exchange over time and across many crossing points. The same institutionalization processes also occur for solutions that change more incrementally. However, these incremental processes may be masked by the fact that a large degree of institutional alignment and selling efforts have already occurred.It is important to point out that the service ecosystems perspective does not diminish the contributions of the existing sales literature; rather, this holistic perspective reconciles and builds on many of these contributions. Consistent with dyadic perspectives, Salesforce was able to form strong relationships with customers, industry experts, and journalists. For example, many would argue that the company excelled in prospecting by targeting end users rather than following the industry norm of targeting executives. The meeting and travel practices of end users often resulted in institutional frictions with client-server solutions and their restrictive data accessibility. After a successful trial, end users often lobbied their managers to try the SaaS solution (e.g., they engaged in selling, the thinning of crossing points).The service ecosystems perspective can also inform the debate regarding whether the importance of salespeople is increasing or decreasing due to changes in markets. As we have stated, whereas some theoreticians and practitioners believe that market changes will diminish the power and strategic importance of salespeople, others predict that salespeople will become more important (Hunter and Perreault 2007, p. 16). Sheth and Sharma (2008), taking a balanced view, note ""a shift towards interactivity, connectivity, and ongoing relationships,"" which, along with the ""enhanced use of technology[,] will reduce some traditional sales functions"" but ""will also lead to changes in the … role of [the] salesperson [to be] more [like] that of a general manager … responsible for marshaling internal and external resources to satisfy customer needs and wants"" (p. 260).A service ecosystems perspective reveals that neither the importance of selling nor the nature of markets are fundamentally changing. What might be changing, as evidenced by the changes in the tasks salespeople perform, are the numbers of crossing points that need to be aligned and the technologies with which actors can engage in institutional alignment processes. Norman (2011), for example, showcases how even tools designed to be simple generally make the world more complex as they tend to increase the number of connections among actors and subsystems. However, despite this increase in complexity, systemic and institutional alignments have always been at the heart of selling. Thus, we argue that selling must always be viewed through a systemic lens and that the debate about whether changes in modern markets influence the importance of salespeople is focused on the wrong question.What is driving the discussion about whether the importance of salespeople is changing is not a transition to complex markets, because markets have always been complex, but rather a change in the ways narrative infrastructures are formed (i.e., the ways market actors align their expectations for service exchange). Technological advancements, such as the Internet, are making it easier for a larger number of actors to engage in selling. Consider retail sales, for example. Instead of relying on the advice of salespeople when making a purchase, many buying actors now consult online reviews to evaluate products and services. Thus, the institutional alignment work in which salespeople were traditionally involved is now often performed (arguably more convincingly and conveniently) by numerous selling actors, including those who describe products or services and how they fit into the contexts in which they are used.This, however, does not represent a shift in the importance of selling but rather a shift in who performs selling and where selling is performed. Selling has been and continues to be important regardless of market complexity. However, advanced communication tools, such as the Internet, highlight and facilitate broader participation of systemic actors in institutionalization, legitimization, and sense making. Consequently, while selling will continue to be an important element of value creation, it may be performed by even broader sets of selling actors who are not traditionally categorized as salespeople. ImplicationsThis article offers several implications for practice and theory. For practitioners, we highlight implications for gaining new business, maintaining existing business, and managing intrafirm actors along with broad sets of external selling actors. For theoreticians, we propose a research agenda involving additional inquiry into sales performance, analytical approaches, and salespeople characterizations. Implication for PracticeGaining business with new solutions. Many traditional sales perspectives begin with a new product or service and end with persuading customers to adopt it. However, a service ecosystems perspective shows that institutionalization processes, and thus selling processes, precede product or service developments. The Salesforce selling process did not begin with the launch of its SaaS solution, nor will it end when the last license is sold. Instead, this selling process was, and remains, embedded in broader institutionalization processes in which many systemic actors collectively form aligned institutions for service exchange. These alignments always include the institutionalization of complementary innovations and downstream adoptions (Adner 2006).Thus, a service ecosystems perspective highlights that selling considerations need to be an important part of new product and service strategies. A new solution that fits well into established resource integration practices (e.g., an established market with thin crossing points) requires less selling effort. However, these thinned crossing points also make it more difficult for selling actors to facilitate the creation of thick crossing points for competing solutions because competing solutions in such markets, by definition, are perceived to be quite similar. A new solution facing thick crossing points, in contrast, requires the negotiation of institutional resistance and change. This dynamic makes such a discontinuous solution riskier (i.e., the solution might not gain legitimacy) and may both prolong and complicate the required selling efforts. However, when a discontinuous solution is successfully institutionalized, it may, at least in the short term, make it easier to facilitate the maintenance of thick crossing points for competing solutions. This at least partially explains the challenges Salesforce initially had to overcome with the SaaS solution, why Salesforce has had great success to date, and why Salesforce's competitors were forced to also adopt SaaS solutions.Maintaining business. A service ecosystems perspective on selling also has implications for maintaining existing business. To maintain business, selling actors need to understand the institutions and resource integration practices that have led buying actors to use their solutions. By doing so, selling actors can better understand how and why the solution they offer fits with buyers' resource integration practices, and they can make adjustments as institutional incompatibilities and frictions arise.If a competing actor proposes a superior solution, it may be disadvantageous for the selling actor to try to prevent the institutionalization of this solution. By seeking to prevent institutionalization that would result in greater value for the buyer, the selling actor may violate relational contracts, leading to diminished relationship quality with the buyer or even to relationship dissolution. Arguably, it is often more important to develop and maintain a relational contract between a selling actor and buyer than it is to persuade the buyer to adopt or extend their use of an inferior solution. This is because the buyer-seller relationship and its quality play a large role in allowing selling actors to discover institutional incompatibilities and frictions that they and their firms can alleviate through new solutions.Expanding the view from buyer-seller dyads. As shown in Figure 3, consistent with thought on multistage marketing (Kleinaltenkamp and Ehret 2006), selling actors must ensure that the narratives of a broad range of stakeholders are distributed and interpreted. Although selling actors play pivotal roles in aligning narratives to form narrative infrastructures, selling actors can never become the master storytellers. Many firms are already recognizing the need for broader alignment processes among actors from various functions and organizations as well as external actors throughout the service ecosystem. For example, selling actors often utilize user testimonials and case studies to facilitate institutional alignment across actors in prospect firms. Furthermore, team selling often involves actors from various functions in the selling firm, and buying centers often consist of members from multiple functions. However, selling actors often lack access to potential users and to other stakeholders, creating multistage and indirect relationships (Macdonald, Kleinaltenkamp, and Wilson 2016). Arguably, too many firms still view selling as something that their employees need to be shielded from rather than an opportunity for collaborative innovation, as is evidenced by the prevalence of ""gatekeepers"" and policies limiting ""backdoor selling.""Broadening the set of employees trained in selling. Because selling is not confined to certain actor roles, many actors, regardless of the title used to characterize them, play important roles in aligning expectations of service exchange. Therefore, firms need to reassess which positions need sales expertise either through training or through hiring already-trained employees. For example, procurement managers often actively align expectations for service exchange with new suppliers and coworkers, which may result in lower-priced and/or customized offerings. In addition, selling actors should be trained in how to foster and participate in true dialogical interactions among large sets of actors.Adopting a broader, longitudinal, and balanced view on sales performance. As we have stated, a service ecosystems perspective on selling highlights that institutional work is an ongoing process requiring long-term relational contracts among actors. Often, these institutionalization processes are more important than the revenue and profit of a transaction or history of transactions with a buyer. This importance is easy to overlook because the outcomes of institutionalization processes often only become salient after extended periods of time and are reflected in the behaviors of many actors. Therefore, performance evaluations need to encompass the outcomes of institutionalization processes in which selling actors, both internal and external to firms, participate. Consequently, limiting selling actor evaluations to actors employed by a firm and evaluating these employees using only short-term sales goals (e.g., monthly or quarterly quotas) obscures the cause-effect relationships between selling actors' behaviors and desired outcomes. For example, overemphasizing short-term sales goals may result in salesperson behaviors (e.g., ""hard selling"") that increase their short-term performance at the expense of their long-term performance due to relationship and reputational damages.Broadening information technology to connect actors and narratives. The participation of broad sets of internal and external actors in dialogical interactions and institutional processes highlights the importance of selling actors developing tools and processes to systematically integrate and manage communication flows among these sets of actors, many of whom are not customers or prospects. As we have described, selling-related narratives are not limited to interpersonal interactions but comprise many forms of communications (e.g., online product reviews, conference presentations, social media posts). Therefore, firms should broaden their information technology (e.g., CRM, social media analytics) to track a broad range of selling actors and to develop strategies to assess the influence, opinions, and recommendations of these actors. In line with this assessment, firms should develop contact and communication strategies not only for customers and prospects but for all actors relevant to selling efforts.Although external actors cannot be managed in the traditional sense, supportive actors can be encouraged to voice their thoughts and can be given platforms to amplify their narratives. Examples of how firms may do so include invitations to attend important events (e.g., industry conferences), arranged interactions with journalists and industry experts, and promotional materials featuring firms and their products and solutions, among others. Opposing actors can also often be influenced to change or lessen their narrative contributions to support a desired narrative infrastructure. For example, firms can actively aim to build relationships (e.g., seek feedback) and can then use the resulting knowledge to understand and address concerns. Implications for ResearchConceptualizing and evaluating salesperson performance. As we have discussed, a service ecosystem perspective calls for increased scrutiny regarding how sales performance is conceptualized and evaluated. Sales-focused articles in marketing' s top journals, much like salesperson evaluation systems, are replete with objective performance measures frequently limited to the number, revenue, or profit of salesperson transactions as well as buyer satisfaction. However, Verbeke, Dietz, and Verwaal (2011) note that ""the sales performance construct is becoming increasingly complex,"" and that there is a need to address ""what constitutes sales performance in today's economy"" (p. 425). A service ecosystem perspective accentuates that the conceptualization and examination of salesperson performance needs to encompass the outcomes of the institutionalization in which salespeople participate. In this context, it is important to point out that some institutional elements might be more difficult to change than others. Lock-ins created by laws, for example, might be harder to change than the value perception of a modified solution.RPļ: How can the participation of selling actors in bringing about aligned institutional arrangements for service exchange be evaluated? What are the appropriate metrics? What are the appropriate time horizons for evaluating institutional salesperson performance? How can the degree and type (in terms of the three elements) of institutional misalignments of a new solution (i.e., the degree of newness or legitimacy) in these evaluations be accounted for?The evolving nature of tasks performed by selling actors and the knowledge, skills, and abilities (KSAs) to perform these tasks. The service ecosystem perspective highlights an increasing number of crossing points and changing ways through which narratives are formed and distributed in modern markets. Thus, additional examination of the tasks selling actors perform and the KSAs required is needed. In addition to traditional selling tasks, modern salespeople are increasingly being asked to provide customer service (Rapp et al. 2017), act as general managers (Sheth and Sharma 2008), and perform tasks traditionally reserved for business development (e.g., develop and manage strategic partnerships, access and align broad sets of cross-functional and intraorganizational actors and resources, manage projects). Thus, a service ecosystems perspective highlights the need for additional and broader KSAs that enable selling actors to better synthesize information and manage interactions with diverse actors and their resources.Furthermore, technological advancements such as social media platforms, CRM and productivity tools, and selling-buying firm interfaces are affording salespeople both greater and easier access to buyers and their needs, potential solutions, and competitor developments. These changes suggest that salespeople will increasingly use advanced communication and analytical technologies to facilitate the alignment of narratives. Given these developments, it is likely that salespeople will continue to serve as major differentiators among firms. However, the means through which salespeople and other selling actors contribute to this differentiation may change as technologies evolve and the numbers of crossing points increase. Although salespeople have been, and will continue to be, important in the alignment of institutional arrangements for service exchange, some of the tasks salespeople perform and the requisite KSAs may change.RP2: What selling actor and salesperson-performed tasks and KSAs will change in importance, emerge, or disappear as communication and analytical technologies evolve and the numbers of crossing points increase? What selling actor- and salesperson-performed tasks and KSAs will differentiate firms as communication and analytical technologies evolve and the numbers of crossing points increase?Analytical approaches consistent with a service ecosystems perspective. This article underscores the importance of investigating outcomes of interest using approaches that account for the interplays of many actors and dynamic change. Brass and Krackhardt (1999) point out that social network analysis can help explain how information, trust, and other resources flow within networks of actors, as well as how ""people interact and communicate to make sense of, and successfully operate in, their environment"" (p. 182). Consistent with the argument that the sales-focused marketing literature is moving toward a systemic and institutional perspective, analytical techniques with systemic foundations such as social network analysis have received increasing attention (Ahearne et al. 2013; Bolander et al. 2015; Gonzalez, Claro, and Palmatier 2014). However, social network analysis has not yet been used to assess institutionalization and alignment of the narrative infrastructures of systemic actors within the sales literature.Thus, there is great opportunity to employ social network analysis to examine measures that describe and assess maintenance and change in properties of actor location, social capital, tie strength, and brokerage, among others and the roles they play in institutionalization processes. Such research would ideally adopt an approach (e.g., dynamic network analysis) that offers the ability to analyze large-scale networks and multiple overlapping networks, as well as the examination of change at both the node (i.e., actor) and network level.RP3: What analytical approaches are suited to longitudinally investigate sales-related outcomes in dynamic service ecosystems? How do network attributes influence institutionalization and narrative alignment processes in the context of selling? How should selling actors prioritize alignment needs among systemic actors?Institutional work and the importance of various characterizations of salespeople. The connections, if any, between institutional work and the importance of various characterizations (e.g., inside or outside; ""hunter or gatherer""; business to business vs. business to consumer; transactional, enterprise, and consultative sales) of salespeople warrant examination. A traditional perspective suggests that the importance of salespeople depends on how much they influence and change the behaviors and thoughts of buying actors and often neglects to recognize institutional maintenance. This contributes to common assumptions that outside salespeople, ""hunters,"" business-to-business salespeople, and both enterprise and consultative salespeople are more important than their counterparts.A service ecosystems perspective suggests that because of the importance of all forms of institutional work (i.e., change, maintenance, and disruption), it may be premature to conclude whether some characterizations of salespeople are more important than others. In addition to examining the ways various characterizations of salespeople engage in institutional work, further research should consider potential contingencies (e.g., industry, company characteristics, other selling actors involved in institutional work) related to this work. By doing so, future research could address whether some characterizations of salespeople are more important than others, whether this importance is contingent on specific factors, and whether changing means of communication influence this importance.RP4: Do some characterizations of salespeople relate to the type of institutional work (i.e., change, maintenance, and disruption) that is dominantly performed? Are there contingencies that influence the importance of salespeople, and if so, what are they? Are changing means of communication influencing the importance of some characterizations of salespeople differently than others, and if so, how? ConclusionThis article advances a service ecosystems perspective as a theoretical foundation for examining selling and the participation of selling actors in value cocreation. In doing so, it aids the convergence of the sales literature on a systemic perspective that recognizes the importance of institutional arrangements and relationships in service exchange. A service ecosystems perspective leads to a reconceptualization of selling in which broad sets of actors interact with the aim of creating, maintaining, and disrupting the institutions that enable and constrain value cocreation practices through service-for-service exchange. This view of selling deemphasizes the ability of any single selling actor to influence the decision making of another actor and highlights the broader involvement of systemic actors in the formation of thin and thick crossing points. Finally, a service ecosystems perspective illustrates that markets have always been complex and dynamic and that selling actors have always been, and will continue to be, important in the alignment of institutional arrangements for service exchange.GRAPH: FIGURE 1 Published Sales-Oriented Articles Adopting a Systemic PerspectiveDIAGRAM: FIGURE 2 Service Ecosystems Perspective of SellingDIAGRAM: FIGURE 3 Contrasting a Service Ecosystems Perspective on Sales  REFERENCES   1  Adner, Ron (2006), ""Match Your Innovation Strategy to Your Innovation Ecosystem,"" Harvard Business Review, 84 (4), 98–107. 2  Ahearne, Michael, Son K. Lam, Babak Hayati, and Florian Kraus (2013), ""Intrafunctional Competitive Intelligence and Sales Performance: A Social Network Perspective,"" Journal of Marketing, 77 (5), 37–56. 3  Alderson, Wroe (1957), Marketing Behavior and Executive Action: A Functionalist Approach to Marketing Theory. Homewood, IL: Richard D. Irwin. 4  Araujo, Luis, and Geoff Easton (2012), ""Temporality in Business Networks: The Role of Narratives and Management Technologies,"" Industrial Marketing Management, 41 (2), 312–18. 5  Arndt, Johan (1981), ""The Political Economy of Marketing Systems: Reviving the Institutional Approach,"" Journal of Macromarketing, 1 (2), 36–47. 6  Arnould, Eric J., and Craig J. Thompson (2005), ""Consumer Culture Theory (CCT): Twenty Years of Research,"" Journal of Consumer Research, 31 (4), 868–82. 7  Baldwin, Carliss Y. (2008), ""Where Do Transactions Come From? Modularity, Transactions, and the Boundaries of Firms,"" Industrial and Corporate Change, 17 (1), 155–95. 8  Baldwin, Carliss Y., and Kim B. Clark (2000), Design Rules: The Power of Modularity, MIT Press. 9  Ballantyne, David, and Richard J. Varey (2006), ""Creating Valuein-Use Through Marketing Interaction: The Exchange Logic of Relating, Communicating and Knowing,"" Marketing Theory, 6 (3), 335–48. Benioff, Marc, and Carlye Adler (2009), Behind the Cloud: The Untold Story of How Salesforce.com Went from Idea to Billion-Dollar Company—and Revolutionized an Industry. San Francisco: Jossey-Bass. Blocker, Christopher P., Joseph P. Cannon, Nikolaos G. Panagopoulos, and Jeffrey K. Sager (2012), ""The Role of the Sales Force in Value Creation and Appropriation: New Directions for Research,"" Journal of Personal Selling & Sales Management, 32 (1), 15–27. Boje, David M. (1991), ""The Storytelling Organization: A Study of Story Performance in an Office-Supply Firm,"" Administrative Science Quarterly, 36 (1), 106–26. Bolander, Willy, Cinthia B. Satornino, Douglas E. Hughes, and Gerald R. Ferris (2015), ""Social Networks Within Sales Organizations: Their Development and Importance for Salesperson Performance,"" Journal of Marketing, 79 (6), 1–16. Bourdieu, Pierre (1977), Outline of a Theory of Practice. Cambridge, UK: The Press Syndicate of the University of Cambridge. Brass, Daniel J., and David Krackhardt (1999), ""The Social Capital of Twenty-First Century Leaders,"" in Out-of-the-Box Leadership: Transforming the Twenty-First-Century Army and Other Top-Performing OrganizationsJames G. Hunt, George E. Dodge, and Leonard Wong, eds. Stamford, CT: JAI Press, 179–94. Cannon, Joseph P., Ravi S. Achrol, and Gregory T. Gundlach (2000), ""Contracts, Norms, and Plural Form Governance,"" Journal of the Academy of Marketing Science, 28 (2), 180–94. Chandler, Jennifer D., and Stephen L. Vargo (2011), ""Contextualization and Value-in-Context: How Context Frames Exchange,"" Marketing Theory, 11 (1), 35–49. Creed, W.E. Douglas, Rich DeJordy, and Jaco Lok (2010), ""Being the Change: Resolving Institutional Contradiction Through Identity Work,"" Academy of Management Journal, 53 (6), 1336–64. Cron, William L., Artur Baldauf, Thomas W. Leigh, and Samuel Grossenbacher (2014), ""The Strategic Role of the Sales Force: Perceptions of Senior Sales Executives,"" Journal of the Academy of Marketing Science, 42 (5), 471–89. Czarniawska, Barbara (2004), Narratives in Social Science Research. Thousand Oaks, CA: Sage Publications. Deuten, Jasper J., and Arie Rip (2000), ""Narrative Infrastructure in Product Creation Processes,"" Organization, 7 (1), 69–93. DiMaggio, Paul J. (1988), ""Interest and Agency in Institutional Theory,"" in Institutional Patterns and Organizations: Culture and Environment, L.G. Zucker, ed. Cambridge, MA: Ballinger, 3–22. Dixon, Andrea L., and John F. Tanner (2012), ""Transforming Selling: Why It Is Time to Think Differently About Sales Research,"" Journal of Personal Selling & Sales Management, 32 (1), 9–14. Dixon, Matthew, and Brent Adamson (2011), The Challenger Sale: Taking Control of the Customer Conversation. New York: Penguin Group. Duddy, Edward Augustin, and David Allen Revzan (1953), Marketing: An Institutional Approach. New York:McGraw-Hill. Dwyer, Robert F., Paul H. Schurr, and Sejo Oh (1987), ""Developing Buyer-Seller Relationships,"" Journal of Marketing, 51 (2), 11–27. Edvardsson, Bo, Michael Kleinaltenkamp, B°ard Tronvoll, Patricia McHugh, and Charlotta Windahl (2014), ""Institutional Logics Matter When Coordinating Resource Integration,"" Marketing Theory, 14 (3), 291–309. Friedman, Walter A. (2005), Birth of a Salesman. Cambridge, MA: Harvard University Press. Friend, Scott B., and Avinash Malshe (2016), ""Key Skills for Crafting Customer Solutions Within an Ecosystem: A Theoriesin-Use Perspective,"" Journal of Service Research, 19 (2), 174–91. Giddens, Anthony (1984), The Constitution of Society: Outline of the Theory of Structuration. Oakland: University of California Press. Gonzalez, Gabriel R., Danny P. Claro, and Robert W. Palmatier (2014), ""Synergistic Effects of Relationship Managers' Social Networks on Sales Performance,"" Journal of Marketing, 78 (1), 76–94. Grewal, Rajdeep, and Ravi Dharwadkar (2002), ""The Role of the Institutional Environment in Marketing Channels,"" Journal of Marketing, 66 (3), 82–97. Hakansson, Hakan, and Ivan Snehota (1995), Developing Relationships In Business Networks. London: Routledge. Hall, Zachary R., Michael Ahearne, and Harish Sujan (2015), ""The Importance of Starting Right: The Influence of Accurate Intuition on Performance in Salesperson—Customer Interactions,"" Journal of Marketing, 79 (3), 91–109. Heide, Jan B., and George John (1992), ""Do Norms Matter in Marketing Relationships?"" Journal of Marketing, 56 (2), 32–44. Helm, Sabrina, and Risto T. Salminen (2010), ""Basking in Reflected Glory: Using Customer Reference Relationships to Build Reputation in Industrial Markets,"" Industrial Marketing Management, 39 (5), 737–43. Hillebrand, Bas, Paul H. Driessen, and Oliver Koll (2015), ""Stakeholder Marketing: Theoretical Foundations and Required Capabilities,"" Journal of the Academy of Marketing Science, 43 (4), 411–28. Hoffman, Andrew J. (2001), From Heresy to Dogma: An Institutional History of Corporate Environmentalism. Stanford, CA: Stanford University Press. Hughes, Douglas E., Joël Le Bon, and Avinash Malshe (2012), ""The Marketing-Sales Interface at the Interface: Creating Market-Based Capabilities Through Organizational Synergy,"" Journal of Personal Selling & Sales Management, 32 (1), 57–72. Hughes, Douglas E., Joël Le Bon, and Adam Rapp (2013), ""Gaining and Leveraging Customer-Based Competitive Intelligence: The Pivotal Role of Social Capital and Salesperson Adaptive Selling Skills,"" Journal of the Academy of Marketing Science, 41 (1), 91–110. Humphreys, Ashlee (2010), ""Megamarketing: The Creation of Markets as a Social Process,"" Journal of Marketing, 74 (2), 1–19. Hunt, Shelby B. (1981), ""Macromarketing as a Multidemensional Concept,"" Journal of Macromarketing, 1 (1), 7–8. Hunter, Gary K., and William D. Perreault (2007), ""Making Sales Technology Effective,"" Journal of Marketing, 71 (1), 16–34. Jolson, Marvin A. (1997), ""Broadening the Scope of Relationship Selling,"" Journal of Personal Selling & Sales Management, 17 (4), 75–88. Jones, Eli, Lawrence B. Chonko, and James A. Roberts (2004), ""Sales Force Obsolescence: Perceptions from Sales and Marketing Executives of Individual, Organizational, and Environmental Factors,"" Industrial Marketing Management, 33 (5), 439–56. Kjellberg, Hans, and Claes-Fredrik Helgesson (2006), ""Multiple Versions of Markets: Multiplicity and Performativity in Market Practice,"" Industrial Marketing Management, 35 (7), 839–55. Kleinaltenkamp, Michael, and Michael Ehret (2006), ""The Value Added by Specific Investments: A Framework for Managing Relationships in the Context of Value Networks,"" Journal of Business and Industrial Marketing, 21 (2), 65–71. Kumar, V., J. Andrew Petersen, and Robert P. Leone (2013), ""Defining, Measuring, and Managing Business Reference Value,"" Journal of Marketing, 77 (1), 68–86. Lawrence, Thomas B., and Roy Suddaby (2006), ""Institutions and Institutional Work,"" in The Sage Handbook of Organization Studies, 2nd ed., Stewart R. Clegg, Cynthia Hardy, Tom Lawrence, and Walter R. Nord, eds. London: Sage Publications, 215–54. Lawrence, Thomas B., Roy Suddaby, and Bernard Leca (2009), ""Introduction: Theorizing and Studying Institutional Work,""-Institutional Work: Actors and Agency in Institutional Studies of Organizations, Thomas B. Lawrence, Roy Suddaby, and Bernard Leca, eds. Cambridge, UK: Cambridge University Press, 1–28. Macdonald, Emma K., Michael Kleinaltenkamp, and Hugh N. Wilson (2016), ""How Business Customers Judge Solutions: Solution Quality and Value in Use,"" Journal of Marketing, 80 (3), 96–120. MacInnis, Deborah J (2011), ""A Framework for Conceptual Contributions in Marketing,"" Journal of Marketing, 75 (4), 136–54. Moncrief, William C., and Greg W. Marshall (2005), ""The Evolution of the Seven Steps of Selling,"" Industrial Marketing Management, 34 (1), 13–22. Mullins, Ryan R., Michael Ahearne, Son K. Lam, Zachary R. Hall, and Jeffrey P. Boichuk (2014), ""Know Your Customer: How Salesperson Perceptions of Customer Relationship Quality Form and Influence Account Profitability,"" Journal of Marketing, 78 (6), 38–58. Norman, Donald A. (2011), Living with Complexity. Cambridge, MA: MIT Press. North, Douglass C. (1990), Institutions, Institutional Change, and Economic Performance. Cambridge, UK: Cambridge University Press. Oliver, Christine (1991), ""Strategic Responses to Institutional Processes,"" Academy of Management Review, 16 (1), 145–79. Orlikowski, Wanda J. (1992), ""The Duality of Technology: Rethinking of the Concept of Technology in Organizations,"" Organization Science, 3 (3), 398–427. Phillips, Nelson, Thomas B. Lawrence, and Cynthia Hardy (2004), ""Discourse and Institutions,"" Academy of Management Review, 29 (4), 635–52. Pinch, Trevor J., and Wiebe E. Bijker (1984), ""The Social Construction of Facts and Artefacts: Or How the Sociology of Science and the Sociology of Technology Might Benefit Each Other,"" Social Studies of Science, 14 (3), 399–441. Plouffe, Christopher R., Willy Bolander, Joseph A. Cote, and Bryan Hochstein (2016), ""Does the Customer Matter Most? Exploring Strategic Frontline Employees' Influence of Customers, the Internal Business Team, and External Business Partners,"" Journal of Marketing, 80 (1), 106–23. Prahalad, CoimbatoreK., andVenkat Ramaswamy (2004), ""Co-Creation Experiences: The Next Practice in Value Creation,"" Journal of Interactive Marketing, 18 (3), 5–14. Press, Melea, Eric J. Arnould, Jeff B. Murray, and Katherine Strand (2014), ""Ideological Challenges to Changing Strategic Orientation in Commodity Agriculture,"" Journal of Marketing, 78 (6), 103–19. Rackham, Neil, and John DeVincentis (1998), Rethinking the Sales Force: Refining Selling to Create and Capture Customer Value. New York: McGraw-Hill. Rapp, Adam A., Daniel G. Bachrach, Karen E. Flaherty, Douglas E. Hughes, Arun Sharma, and ClayM. Voorhees (2017), ""The Role of the Sales-Service Interface and Ambidexterity in the Evolving Organization,"" Journal of Service Research, 20 (1), 59–75. Rapp, Adam, Daniel G. Bachrach, Nikolaos Panagopoulos, and Jessica Ogilvie (2014), ""Salespeople as Knowledge Brokers: A Review and Critique of the Challenger Sales Model,"" Journal of Personal Selling & Sales Management, 34 (4), 245–59. Revzan, David A. (1968), ""The Holistic-Institutional Approach to Marketing,"" in Perspectives in Marketing Theory, Jerome B. Kernan and Montrose S. Sommers, eds. New York: Appleton-Century-Crofts, 97–136. Rosa, Jose Antonio, Joseph F. Porac, Jelena Runser-Spanjol, Michael S. Saxon, Jalena Runser Spanjol, and Michael S. Saxon (1999), ""Sociocognitive Dynamics in a Product Market,"" Journal of Marketing, 63 (SI), 64–77. Schmitz, Christian, and Shankar Ganesan (2014), ""Managing Customer and Organizational Complexity in Sales Organizations,"" Journal of Marketing, 78 (6), 59–77. Scott, Richard W. (2013), Institutions and Organizations: Ideas, Interests, and Identities. Thousand Oaks, CA: Sage Publications. Seidl, David, and Richard Whittington (2014), ""Enlarging the Strategy-as-Practice Research Agenda: Towards Taller and Flatter Ontologies,"" Organization Studies, 35 (10), 1407–21. Sheth, Jagdish N., and Arun Sharma (2008), ""The Impact of the Product to Service Shift in Industrial Markets and the Evolution of the Sales Organization,"" Industrial Marketing Management, 37 (3), 260–69. Simon, Herbert A. (1996), The Sciences of the Artificial. Cambridge, MA: MIT Press. Snow, David A. (2008), ""Elaborating the Discursive Contexts of Framing: Discursive Fields and Spaces,"" Studies in Symbolic Interaction, 30, 3–28. Suchman, Mark C. (1995), ""Managing Legitimacy: Strategic and Institutional Approaches,"" Academy of Management Review, 20 (3), 571–610. Taylor, James R., and Elizabeth J. Van Every (1999), The Emergent Organization: Communication as Its Site and Surface. Mahwah, NJ: Lawrence Erlbaum Associates. Thornton, Patricia H., William Ocasio, and Michael Lounsbury (2012), The Institutional Logics Perspective: A New Approach to Culture, Structure, and Process. Oxford, UK: Oxford University Press. Vargo, Stephen L., and Robert F. Lusch (2004), ""Evolving to a New Dominant Logic for Marketing,"" Journal of Marketing, 68 (1), 1–17. Vargo, Stephen L., and Robert F. Lusch (2011), ""It's All B2B … and Beyond: Toward a Systems Perspective of the Market,"" Industrial Marketing Management, 40 (2), 181–87. Vargo, Stephen L., and Robert F. Lusch (2016), ""Institutions and Axioms: An Extension and Update of Service Dominant Logic,"" Journal of the Academy of Marketing Science, 44 (1), 5–23. Vargo, Stephen L., and Robert F. Lusch (2017), ""Service-Dominant Logic 2025,"" International Journal of Research in Marketing, 34 (1), 46–67. Verbeke, Willem, Bart Dietz, and Ernst Verwaal (2011), ""Drivers of Sales Performance: A Contemporary Meta-Analysis. Have Salespeople Become Knowledge Brokers?"" Journal of the Academy of Marketing Science, 39 (3), 407–28. Webster, Frederick E., and Robert F. Lusch (2013), ""Elevating Marketing: Marketing Is Dead! Long Live Marketing!"" Journal of the Academy of Marketing Science, 41 (4), 389–99. Weick, Karl E. (1995), Sensemaking in Organizations. Thousand Oaks, CA: Sage Publications. Weitz, Barton A., and Kevin D. Bradford (1999), ""Personal Selling and Sales Management: A Relationship Marketing Perspective,"" Journal of the Academy of Marketing Science, 27 (2), 241–54. Weld, L.D.H. (1916), The Marketing of Farm Products. New York: The Macmillan Company. Zietsma, Charlene, and Brent McKnight (2009), ""Building the Iron Cage: Institutional Creation Work in the Context of Competing Proto-Institutions,"" in Institutional Work: Actors and Agency in Institutional Studies of Organizations, T.B. Lawrence, R. Suddaby, and B. Leca, eds. Cambridge, UK: Cambridge University Press.~~~~~~~~By Nathaniel N. Hartmann; Heiko Wieland and Stephen L. VargoNathaniel N. Hartmann is Associate Professor of Marketing, University of Hawai'i at MānoaHeiko Wieland is Assistant Professor of Marketing, California State University, Monterey BayStephen L. Vargo is Professor of Marketing, University of Hawai'i at MānoaCopyright of Journal of Marketing is the property of American Marketing Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.Record: 27Corporate Board Interlocks and New Product Introductions. By: Srinivasan, Raji; Wuyts, Stefan; Mallapragada, Girish. Journal of Marketing. Jan2018, Vol. 82 Issue 1, p132-150. 19p. 1 Diagram, 6 Charts. DOI: 10.1509/jm.16.0120. Persistent link to this record (Permalink): http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=127019665&site=ehost-liveCut and Paste: <A href=""http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=127019665&site=ehost-live"">Corporate Board Interlocks and New Product Introductions.</A>"
18,"Corporate Sociopolitical Activism and Firm Value Stakeholders have long pressured firms to provide societal benefits in addition to generating shareholder wealth. Such benefits have traditionally come in the form of corporate social responsibility. However, many stakeholders now expect firms to demonstrate their values by expressing public support for or opposition to one side of a partisan sociopolitical issue, a phenomenon the authors call ""corporate sociopolitical activism"" (CSA). Such activities differ from commonly favored corporate social responsibility and have the potential to both strengthen and sever stakeholder relationships, thus making their impact on firm value uncertain. Using signaling and screening theories, the authors analyze 293 CSA events initiated by 149 firms across 39 industries, and find that, on average, CSA elicits an adverse reaction from investors. Investors evaluate CSA as a signal of a firm's allocation of resources away from profit-oriented objectives and toward a risky activity with uncertain outcomes. The authors further identify two sets of moderators: ( 1) CSA's deviation from key stakeholders' values and brand image and ( 2) characteristics of CSA's resource implementation, which affect investor and customer responses. The findings provide new and important implications for marketing theory and practice.KEYWORDS_SPLITInvestors expect firms to prioritize maximization of shareholder wealth ([46]; [72]), while customers and other stakeholders are increasingly concerned about firms' contributions to society as a whole and are placing mounting pressure on firms to take sides on hot-button sociopolitical issues such as immigration, gun control, LGBTQ rights, and climate change ([41]; [54]; [55]). Richard Edelman, chief executive officer (CEO) of Edelman, explains, ""Brands are now being pushed to go beyond their classic business interests to become advocates. It is a new relationship between a company and consumer, where a purchase is premised on the brand's willingness to live its values, act with purpose, and, if necessary, make the leap into activism."" A recent study found that 64% of global consumers buy or boycott a brand based on its stand on societal issues—an increase of 13% year over year ([26], p. 1).In line with these expectations, firms are increasingly taking activist stances on sociopolitical issues ([33]). For example, Delta cut ties with the National Rifle Association (NRA) after a deadly school shooting ([23]), and Starbucks committed to hiring refugees in opposition to an immigration ban ([24]). Nike supported National Football League players who knelt during the national anthem in protest of police brutality ([93]), while Papa John's Pizza took the opposite stance on player protests ([86]). We refer to such activities as ""corporate sociopolitical activism"" (CSA) and define CSA as a firm's public demonstration (statements and/or actions) of support for or opposition to one side of a partisan sociopolitical issue.Although CSA may strengthen relationships with some stakeholders who agree with the firm, it will likely damage relationships with those who disagree. For example, speaking out against the NRA was costly to Delta: home-state government legislators in Georgia rescinded an estimated $40 million tax break and NRA supporters threatened boycotts. Ed Bastian, CEO of Delta, explained, ""I knew there would be a backlash, but I didn't anticipate the strength of the backlash from the NRA movement. But on the other side, it created an outpouring of support and appreciation for a company to stand by its values"" ([23], p. 1).The tension between shareholder value maximization and social responsibility is not new, as investors often question investments in corporate social responsibility ([68]). Given its partisan quality, however, activism raises the level of risk and uncertainty beyond that of traditional CSR activities. Unfortunately, a theoretically grounded understanding of how CSA affects stakeholders is missing in the academic literature. We aim to examine the effect of CSA on firm value by investigating investor and customer responses. Furthermore, we argue that the polarized stakeholder responses to CSA differentiate it from traditional CSR and deem it worthy of a separate investigation.Because CSA is a relatively new phenomenon ([41]), we first explore it as a construct that is distinct from other major corporate social and political activities, namely corporate social responsibility (CSR) and corporate political activity (CPA). We then use signaling and screening theories ([ 6]; [21]; [77]) to explain investor responses to CSA. We organize our framework around two sets of moderators that, together, provide a holistic view of the stock market reaction to CSA. The first set relates to sources of CSA deviation. Because CSA may deviate from the personal values of key stakeholders—customers, employees, and state legislators—as well as a firm's brand image, investors interpret such deviations as problematic for the firm. The second set relates to the implementation of CSA resources, which signals a firm's commitment of time, capital, and attention to activism rather than more immediate profit-oriented objectives. Such diversions of resources introduce uncertainty, eliciting negative investor responses. Finally, while we do not formally hypothesize how customers respond to CSA, we investigate their reactions by examining changes in sales growth. Our research gives managers insights into the financial consequences of engaging in CSA in terms of its impact on both investors and customers.Four essential questions guide our research: ( 1) How do investors react to CSA events? ( 2) How does the degree of deviation of CSA from the values held by customers, employees, state legislators, and the firm's brand image modify investors' reactions to CSA? ( 3) How do characteristics of CSA resource implementation modify investors' reactions? ( 4) How do customers respond to CSA? Using event study methodology, we examine the effect of CSA on firm value by studying the stock market reaction to 293 CSA events initiated by 149 firms across 39 industries.In answering these questions, we provide several significant contributions. First, we advance the marketing strategy literature by introducing CSA as a strategic option with marketing implications for stakeholder relationships. We also offer a comprehensive framework that bridges those offered across disciplines examining CEO activism ([17]), CEO sociopolitical activism ([41]), and corporate sociopolitical involvement ([71]).Second, our research is the first to examine CSA's financial consequences. We use signaling and screening theories to explain investors' responses to CSA. We show that, on average, investors react negatively to CSA, especially when CSA stances deviate from the dominant political values of a firm's key stakeholders. We also identify key characteristics of CSA implementation that investors use as cues when inferring a firm's commitment to exerting time, attention, and resources to CSA. Our results reveal that investors' reactions are worse when CSA ( 1) deviates from stakeholders' political values, ( 2) takes the form of actions (vs. statements), ( 3) is announced by the CEO (vs. another person or entity within the firm), ( 4) does not explicitly communicate any business interests, and ( 5) is a solitary firm activity (vs. in coalition with other firms). Thus, we provide critical insights to managers in terms of what to expect from investors if they engage in CSA and how they should implement CSA on the basis of their objectives.The third contribution of our research is a clear delineation of CSA from CSR and CPA, not only from a conceptual standpoint but also in terms of its effects on various stakeholders. While prior research has demonstrated a positive effect of CSR on customer metrics (e.g., firm reputation, product evaluations, customer trust, long-term loyalty; [13]; [19]; [45]) and a positive effect of CSR and CPA on firm value ([61]; [64]), we document the overall negative effect of CSA as a polarizing strategy with uncertain outcomes on firm value as well as contingencies that could make it a fruitful strategy. The Nature of Corporate Sociopolitical ActivismAs defined previously, CSA refers to the firm's public demonstration (statements and/or actions) of support for or opposition to one side of a partisan sociopolitical issue. [71], p. 386) describe sociopolitical issues as ""salient unresolved social matters on which societal and institutional opinion is split, thus potentially engendering acrimonious debate among groups."" Importantly, such issues are partisan and yield polarized stakeholder responses ([55]), which may increase the dispersion of brand evaluations ([63]). Sociopolitical issues exist at the intersections of time, politics, and culture, and the controversy surrounding them can evolve or resolve through time. For example, universal women's suffrage was controversial a century ago, but is now accepted in the United States.Corporate sociopolitical activism is comparable to two other firm activities: CSR and CPA ([71]). Corporate social responsibility refers to ""company actions that advance social good beyond that which is required by law"" ([48], p. 59) and constitutes the gradual formalization of cause-related marketing and corporate philanthropy aimed to ""do well by doing good"" through a strategic focus ([88], p. 60). It is tied to various positive performance outcomes, including firm reputation, product evaluations, customer trust, and long-term loyalty ([19]; [45]), which subsequently exert positive effects on firm value ([61]).A chief difference between traditional CSR and CSA is the extent to which the focal issue is widely favored (e.g., community resources, education, donations to research for curing disease) rather than partisan (e.g., gun control, transgender rights, gender equality, racial equality). Rather, CSR and CSA lie on a continuum in terms of their degree of partisanship: CSR is low in partisanship, because it involves high societal consensus, whereas CSA is polarizing. While CSR is intended to improve relationships with most stakeholders ([68]), stakeholder responses to CSA are highly variable and depend on the stakeholders' sociopolitical values ([ 7]). The risks differ as well. Some investors may view CSR as a nonoptimal use of financial or human resources (i.e., without a clear link to firms' financial value), but CSR has been found to reduce firm-idiosyncratic risk ([62]). Alternatively, CSA can involve a much lower level of initial monetary investment (e.g., a press release, an open letter), but it can potentially increase firm risk due to an increase in uncertainty stemming from punitive actions (e.g., customer boycotts, employee walkouts, legislative backlash).In addition to CSR, firms also regularly engage in CPA, which involves efforts by the firm to sway political processes so that it is well-positioned to gain policy-based competitive market advantages ([64]). Firms have a long history of engaging in political activities, including campaign contributions, lobbying, and donations to political action committees. Corporate political activity is intended to further a specific goal with direct financial payoffs rather than support a social cause ([44]).We suggest that CPA and CSA also differ in the extent to which each activity is publicized. While the underlying motivations to engage in CSA may vary, it is publicly promoted as a communication of the firm's values ([55]; [71]). By contrast, firms execute CPA quietly ([64]). For example, [57], p. 100) describe lobbying as ""a sensitive and often discreet activity"" that, though publicly available, is often obfuscated. If CPA is made public, it is usually by ""accidental disclosure"" ([92]). Furthermore, CPA is generally aligned with firm interests and has a positive effect on firm value ([64]; [92]). By contrast, CSA can be diametrically misaligned with regulators or policy makers, and its effect on firm value is unknown.In summary, CSA is related to CSR and CPA but is a distinct construct that has yet to be clearly elucidated. We propose a 2 × 2 delineating model based on levels of publicity and partisanship, which we depict in Figure 1. The figures shows that CSR is low in partisanship and can be low or high in publicity, depending on whether it is routine or notable. In contrast, CSA and CPA are highly partisan, yet CPA is not meant to be publicized, whereas CSA is highly publicized. Given CSA's novel characteristics, we contend and empirically confirm that CSA exerts unique effects on firm value. Next, we develop predictions about these effects.Graph: Figure 1. Conceptual distinctions among CSR, CPA, and CSA. Theory Development and Research Hypotheses Signaling and Screening ProcessesAccording to signaling theory, firms (senders of signals) communicate relevant information to their recipients through signals to help reduce information asymmetry and better inform recipients' behavior ([81]). Screening theory builds on signaling theory and focuses on what recipients do once they receive a signal, including how they search for and evaluate cues to interpret it more accurately ([21]). In the context of CSA, information asymmetry arises because society has become increasingly interested in firms' sociopolitical values ([26]), yet firms have traditionally concealed these values ([33]). Firms may engage in CSA for a variety of reasons: they may be motivated by morality, business interests, or a combination of morality and economic self-interest (e.g., talent recruitment). We argue that even if a firm expresses a partisan sociopolitical stance to help meet business objectives, it qualifies as CSA because it still risks backlash from stakeholders with opposing views.Regardless of a firm's underlying motivation, engagement in CSA signals its sociopolitical values. This signal reduces information asymmetry between the firm and its stakeholders by informing stakeholders of the sociopolitical values held by the firm. Stakeholders will then further evaluate the firm's engagement in CSA to help close the gap between their known and desired information about the firm ([67]). While customers, employees, and government legislators want to know how the firm's sociopolitical values resonate with those of their own, investors will screen the signal to predict its anticipated effect on shareholder value and future cash flows ([77]; [76]). We focus on investor responses to CSA.When screening a signal, investors seek observable factors that inform them about ( 1) its likely outcomes and ( 2) unobservable attributes of the firm ([ 6]). We organize our conceptual framework accordingly. First, we explicate the overall effect of CSA. We then offer predictions based on the two key sets of moderators: ( 1) sources of CSA deviation from the values of key stakeholders and the firm's brand image, which shape the outcomes of CSA, and ( 2) characteristics of CSA implementation that divert firm resources, which signal the unobservable commitment of a firm to activism. This process is illustrated in Figure 2.Graph: Figure 2. Conceptual model. Investor Responses to CSAInvestors believe that managers have a fiduciary responsibility to engage in behaviors that protect shareholder interests and lead to enhanced profits ([68]). From their perspective, CSA is fundamentally risky, can jeopardize future cash flows, and diverts the firm's efforts from traditional shareholder value maximization activities. This is due to CSA's partisan nature. Specifically, while CSA may appeal to some stakeholders who agree with the firm's stance, it will inevitably offend other stakeholders who hold opposing values ([55]). Thus, CSA's polarizing nature will likely increase the dispersion of the evaluations of a company's brands, and previous work links dispersion to lower abnormal stock returns ([63]). Furthermore, it is difficult to predict the magnitude of the adverse reactions to CSA, and whether the positive reactions will lead to tangible benefits, such as increased sales.Investors may also deem that the more time, resources, and attention managers allocate to CSA, the less they will be able to dedicate to operations, innovation, and other critical profit-generating activities ([71]). This concern persists even when CSA conveys a business interest or is aligned with some stakeholder groups (i.e., customers and employees), because it can still offend a large portion of the population, which creates more uncertainty and requires firms to devote more of their time and resources to managing any backlash. Furthermore, engagement in CSA may signal a fundamental shift in the firm's strategic priorities, foreshadowing uncertain and lasting changes in strategic commitments ([36]). Therefore, we hypothesize: H1:  Investors react negatively to firms' engagement in CSA. How Sources of CSA Deviation Shape Investor ResponseStakeholder relationships are a vital component of a firm's competitive advantage, and thus investors are particularly attuned to how firm actions affect stakeholder relationships ([39]). According to stakeholder alignment theory, CSA can reinforce values and strengthen relationships with stakeholders or, alternatively, jeopardize those relationships ([41]). The more the values signaled by the firm through CSA deviate from stakeholders' political values, the more CSA should cause stakeholders to disidentify with the firm ([74]). This can lead to a wide variety of negative consequences, including customers switching to a competitor, higher employee turnover, and legislators rescinding tax breaks. We predict that investors are likely to react more negatively to CSA that deviates from the dominant political values of the firm's stakeholders because it poses more risk and potential for backlash, which jeopardizes future cash flows. We explore three critical classes of stakeholders: customers, employees, and state legislators. CSA deviation from customers' valuesInvestors monitor and evaluate a firm's customers to forecast its revenue ([ 1]). The effect of CSA on customer spending and engagement will depend on whether customers feel a sense of congruity between their values and a firm's CSA. Indeed, prior work has shown that customers favor brands that reflect their own lifestyles and identities ([28]) and customers use their sociopolitical values as an evaluative lens when making brand purchase decisions ([51]; [84]). This can result in clustering along political orientation. For example, Starbucks tends to attract more liberal customers, whereas Chick-fil-A has a more conservative customer base ([50]; [85]). Customer values can also lead to backlash. For example, when Target announced an inclusive bathroom policy in support of transgender individuals, some customers boycotted the firm ([66]). Thus, when CSA deviates from the political values of a firms' customers, investors will anticipate that the CSA event will more negatively affect customer–firm relationships and undermine financial performance. Therefore, we hypothesize: H2a:  The deviation between CSA and customer values moderates investors' reactions to CSA such that investor reactions are more unfavorable when the CSA stance deviates more from the values of a firm's customers. CSA deviation from employees' valuesEmployees are also critical stakeholders for investors to consider because they can help firms build a sustainable competitive advantage ([27]). Importantly, employee sentiment has a significant economic impact. For example, research indicates that employee satisfaction has positive consequences for firms in terms of stock returns ([27]), innovation ([18]), talent recruitment ([78]), and lower turnover ([58]). Prior work has also shown that noncontroversial firm actions, such as CSR, can engender employee satisfaction and personal fulfillment ([38]), which have a positive impact on employee recruitment ([47]), retention ([ 9]), and firm identification ([37]).Previous work has found that employees interpret a firm's activities through the lens of their personal values ([40]). The greater the deviation between the CSA stance and the political values of a firm's employees, the more likely it will generate negative employee sentiment or backlash (e.g., strike, low morale), which may result in higher turnover and loss of productivity. For example, the company Wayfair's decision to engage in immigration-based CSA resulted in a labor walkout as employees protested the firm's actions, which disrupted sales ([ 8]). We hypothesize: H2b:  The deviation between CSA and employee values moderates investors' reactions to CSA such that investor reactions are more unfavorable when the CSA stance deviates more from the values of a firm's employees. CSA deviation from legislators' valuesGovernments can influence firm performance in several ways, including through policies that aid innovation performance ([59]) or tax structures and financial incentives ([80]). [80] show that by modifying tax rates and tax structures, legislators have the power to both increase and decrease firm value obtained by all stakeholders as well as its distribution across them. Thus, firms' relationships with governments have direct consequences on cash flow and play a significant role in investors' responses to firms' strategies. Firms usually try to maintain mutually beneficial relationships with legislators, typically through political donations ([64]). But when a firm engages in CSA, its stance may conflict with the views of the governing party. For example, several firms spoke out against bills in Georgia and Indiana that had the potential to discriminate against LGBTQ individuals ([ 3]; [91]). Although federal and other state governments are influential stakeholders, home-state governments are especially sensitive to a firm's actions and more likely to punish firms whose CSA they disfavor, as Georgia legislators did in response to Delta's CSA ([23]). Thus, investors are likely to react to CSA depending on how they anticipate it will affect the firm's relationship with state government legislators. Therefore, we hypothesize: H2c:  The deviation between CSA and state government legislator values moderates investors' reactions to CSA such that investor reactions are more unfavorable when the CSA stance deviates more from the values of a firm's state government legislators.While deviation from stakeholders' values can affect investor responses to CSA, so should deviation from the firm's brand image. To maintain and strengthen their brand equity, firms must purposefully create strong brand associations in the minds of customers (e.g., [14]; [49]). Consistent communication of a brand's identity positively affects brand recall and abnormal stock returns ([43]) while inconsistencies can lead to a reevaluation of the brand and ultimately dilute brand equity ([14]).Investors screen firms' communication with their stakeholders to predict its effect on the brand image ([56]). We argue that if the CSA's message is consistent with the brand image, it can help reinforce the brand identity and its associations in the minds of its stakeholders and decrease the risk of brand dilution due to CSA. Conversely, investors will perceive CSA that deviates highly from the established brand image as particularly risky because of its potential to dilute the brand image and stakeholders' identification with the brand and, in turn, decrease brand equity and firm value. Thus, we hypothesize: H3:  The deviation between CSA and brand image moderates investors' reactions to CSA such that investor reactions are more unfavorable when the CSA stance deviates more from the firm's brand image. How Implementation of CSA Resources Shapes Investor ResponsesInvestors will screen not only for cues to help predict the financial outcomes of CSA but also for cues to help inform them about the unobservable characteristics of the firm ([ 6]; [81]). Specifically, they will be interested in how willing the firm is to divert its time, resources, and attention away from profit maximization activities and commit to a given sociopolitical issue. We propose four cues that indicate CSA engagement as resource-intensive and thus signal the firm's commitment to divert resources to sociopolitical activism. Form of supportThe first characteristic is whether activism takes the form of actions or statements. We argue that, ceteris paribus, CSA in the form of actions is more resource-intensive than CSA in the form of statements. Statements involve verbal or written declarations that support or oppose one side of a divisive issue without committing financial or other types of resources to it. By contrast, an action goes beyond a declaration and consists of a change in the firm's conduct or policies, such as publishing or retracting an advertisement, offering or discontinuing products or services, offering or withdrawing promotions, hiring or firing workers, and making or breaking contracts. For example, rather than merely voicing support for immigrants (a statement), Starbucks opposed restrictive immigration policies by announcing a plan to hire refugees (an action) ([24]). Because actions require higher levels of resources and accountability ([52]) and are more difficult to reverse, they also signal more elevated levels of strategic commitment ([53]). Furthermore, because strategic actions by firms have a lasting effect by influencing future decisions ([36]), CSA in the form of an action (vs. a statement) more strongly signal the firm's future allocation of resources. Investors in search of cues will interpret actions as a sign of the firm's increased commitment to CSA. They will likely perceive this increased diversion from the firm's fiduciary responsibilities as particularly risky and respond accordingly. Therefore, we hypothesize: H4:  The form of support (actions vs. statements) moderates investors' reactions to CSA such that investor reactions are more unfavorable when CSA takes the form of an action (vs. a statement). Announcement source statureThe second characteristic is the announcement source—whether the CEO or another representative of the firm (e.g., media relations personnel, another C-suite executive or manager) informs stakeholders of a firm's CSA stance. Investors pay attention to who makes firm announcements and view the prominence of that individual as a signal of the importance of the announcement ([ 4]; [89]). The CEO's communication with stakeholders greatly matters, and investors carefully analyze these communications ([22]). Because the CEO leads the implementation of firm strategies, investors will be particularly concerned when the CEO announces engagement in CSA ([ 4]). Investors will perceive this as a signal of the CEO's willingness to dedicate his or her time, resources, and attention to focus on a risky firm action that may generate backlash from various stakeholders. Therefore, we hypothesize: H5:  The announcement source stature (CEO vs. another team member) moderates investors' reactions to CSA such that investor reactions are more unfavorable when the CSA is announced by the CEO compared with when it is announced by another team member. Business interest communicationFirms engage in CSA for various reasons. Regardless of their true intentions, some firms communicate CSA as benefiting themselves as well as society. We define business interest communication as whether a firm motivates its CSA using economic self-interest. For example, some firms taking stances on LGBTQ issues explained that they opposed discriminatory bills because of the direct impact of these bills on their employees ([32]). Other firms note that their opposition to a discriminatory bill is motivated solely by the bill's negative impact on society ([91]). Whatever the stated motivation, CSA reveals a sociopolitical stance that generates an uncertain impact on future cash flows. However, higher levels of business interest communication should reduce investors' concerns and the overall negative impact of CSA. Notably, we do not argue that CSA motivated by business interests will have a positive effect on investor response. This is because CSA remains controversial in nature and investors may still consider the positive links between CSA and firm outcomes to be uncertain. Therefore, we hypothesize: H6:  Business interest communication moderates investors' reactions to CSA such that investor reactions are less unfavorable when a firm communicates economic self-interest in the announcement of CSA. Coalition sizeThe fourth critical execution factor is how many other firms are jointly engaged in the CSA event. Firms may engage in CSA alone or form an activist coalition with other firms. For example, Amazon unilaterally removed Confederate flag merchandise from its website after a church shooting in 2015, while in 2014, Amazon and 29 other companies filed an ""Employers' Amicus Brief"" in support of same-sex marriage. There is ""safety in numbers"" when firms act together because resources are shared and any backlash will likely be dispersed among all the firms, making CSA less risky ([17]). Thus, investors will likely interpret that a firm acting alone is more committed to its CSA initiative because the firm is risking bearing the brunt of the backlash. Investors will respond more negatively to solo activism due to anticipating a more concentrated backlash and because it signals a stronger commitment to CSA versus the firm's fiduciary duties. Thus, we hypothesize: H7:  Coalition size moderates investors' reactions to CSA such that investor reactions are less unfavorable when more firms are involved in the announcement of CSA. MethodWe test our hypotheses with a data set of 293 CSA events conducted by 149 firms across 39 two-digit Standard Industrial Classification (SIC) codes. The following section details our data collection procedures, measurements, and variables of interest. Data CollectionThe focal events are publicly available announcements of statements or actions by firms regarding partisan sociopolitical issues. We collected these events using a dictionary of time-relevant search terms of partisan sociopolitical topics extracted from [73] report ""Political Polarization in the American Public"" and ""Political Polarization and Typology Survey.""[ 6] For example, we collected the first mention of Amazon's announcement of the removal of Confederate flag products from its website and JPMorgan Chase's first identifiable statement of support for marriage equality. Table 1 provides examples of activism from our sample. We assembled this sample of events from press releases and news articles available from three syndicated sources: ProQuest Newsstand, LexisNexis Academic, and Factiva ([10]). Keyword search terms gathered from the Pew Research Center reports included generic terms such as ""abortion"" or ""LGBTQ"" and specific issues such as ""North Carolina HB2."" A dictionary of generic words used in the archival search and examples of CSA are available in Table W1 in the Web Appendix.GraphTable 1. Examples of CSA in our Sample.  Three trained research assistants blind to our research questions applied a two-stage process to ensure that the prospective events for inclusion in the sample fit the definition of sociopolitical activism and were not subject to potential confounds, which would invalidate the resulting abnormal stock returns. First, if an event was mentioned several times on different dates, coders searched for the first mention of the event to determine the influence of firm communication on abnormal stock returns. Second, coders noted any announcements for which another possible confounding event occurred within a week to eliminate the possible influence of other events on abnormal stock returns ([10]).[ 7]Next, we ran a Q-sort survey (Survey 1) ([70]) to further validate that our events qualify as CSA and are separate from CSR and CPA. In this survey, we provided two trained research assistants blind to our research questions with our definitions of CSA, CSR, and CPA. We then asked the assistants to classify events into one of the three definitions, labeled as A, B, and C to avoid bias, but corresponding to the three types of events. The events consisted of a combination of all the events from our sample plus an additional 12 CPA events and 25 CSR events found in the literature. The overall agreement between the two assistants was 79.5%, the overall hit ratio was 85%, and Cohen's kappa for the two assistants was.80, which is in the ""excellent range of agreement"" according to previous research ([70]).[ 8] Details of the Q-sort survey are available in the Web Appendix W2.We collected additional data for the explanatory and control variables for each firm-event from COMPUSTAT, other publicly available data sources (e.g., headquarters locations, election results, political donations), and by doing content analysis of the announcements. We ultimately obtained 293 events from January 1, 2011, to October 31, 2016, involving 149 U.S. publicly held firms from 39 two-digit SIC codes. Dependent VariablesThe stock market reaction to CSA announcements serves as our primary measure of investors' reactions and changes in firm value. We estimate the stock market reaction to CSA at the time investors first receive the relevant information ([12]; [79]). Data on the firm and stock market returns come from the Center for Research in Security Prices, which we used to estimate the abnormal stock returns of the firm on the first day the firm's CSA was publicized: ARit= Rit− ERit, Graph1where Rit is the daily return, E(Rit) is the expected return of the stock for firm i on day t, and ARit is the abnormal return. To calculate E(Rit), we use a market model for the main analysis and provide additional estimations using the market-adjusted model and the Fama–French–Carhart model ([16]; [30]) as robustness tests.Following previous research, to address the possibility of information leakage and spillover in the stock market, we compute cumulative abnormal returns (CAR) for several windows around the day of the event ([35]): CARi(t′ , t)=∑t′tARit Graph2To choose an event window of appropriate length, we compute the CAR for alternative t′ and t ∈ {−2, −1, 0, 1, 2} and then test their significance in each window. In line with previous studies, we choose the most significant CAR in the five-day window (−2, 2) as our dependent variable ([35]; [83]). Independent Variables Degree of deviation from stakeholders' valuesTo operationalize the degree of deviation between a firm's CSA and its stakeholders' political values, we first measure the political stance of the events. We conducted a survey (Survey 2) on Amazon Mechanical Turk (MTurk) that asked 1,406 U.S. adults to measure the stance of a sample of events. The pool of respondents was heterogeneous.[ 9] Each respondent received five randomly selected events from our sample, along with the date of the event (but not the identity of the firm involved in the CSA event). We asked respondents to rate each event on a seven-point scale (1 = ""very liberal,"" and 7 = ""very conservative""). This produced approximately 7,000 ratings. We then used the average score for each event as the event's stance and transformed it into a zero-centered measure, where −1 reflects extremely liberal and +1 extremely conservative stances. We use this measure as the event's stance (Event_Stance) in our model.The average Event_Stance in our sample is −.40, which leans liberal. The maximum Event_Stance is.75 and the minimum is −.88. Two reasons may explain the more liberal-leaning average score: ( 1) conservativism usually calls for preservation of the status quo, whereas activism often calls for change and therefore leans liberal ([31]), and ( 2) most conservative activism observed in our data collection effort was conducted by privately held firms (e.g., Chick-fil-A, Hobby Lobby, Koch Industries), which are not part of our sample because they do not have publicly traded stock.Next, we created three variables to measure the prevailing political values of the firms' customers, employees, and government legislators, respectively. First, we calculated the prevailing political values of a firm's customers by running an independent survey (Survey 3) on MTurk that asked 375 U.S. adults[10] to evaluate a randomly drawn set of 20 firms from our database of 149 firms (approximately 7,500 ratings in total). Respondents indicated whether a given firm's typical customers lean toward having more liberal or conservative political views (−1 = ""more liberal,"" 0 = ""neither liberal nor conservative,"" and +1 = ""more conservative""). Overall, each firm received approximately 50 ratings, which we averaged to create Customer_Stance. In contrast with the continuous measure used in Survey 2, the categorical measure used in Survey 3 reflects a more discrete categorization of customers based on political affiliation. The results have face validity. For example, respondents rated Whole Foods as typically having more liberal customers (Customer_Stance = −.50) and Cracker Barrel as usually having more conservative customers (Customer_Stance =.43). This retrospective measure captures the prevailing stereotypical perception of investors about the political values of a given firm's customers.Second, we collected each firm's employee's political donations using individual contribution data provided in the U.S. Federal Election Commission's database. We identified the individual employee donations to the Republican versus Democratic party and calculated a measure of average employees' political donations for firm i at day t in year T as Employee_Stanceit=Total donation to Republican PartyiT−1−Total donation to Democratic PartyiT−1 Total donation to Republican and Democratic PartyiT−1 . Graph3This ratio equals −1 if all employee donations were liberal (Democratic) and +1 if all donations were conservative (Republican). If no employees donated to any parties, we set the firm's score to zero.Third, to compute legislatures' political values, we collected the political composition of the state legislature (general assembly) from the state in which the firm is headquartered. We focus on state rather than federal legislatures because state legislatures are likely to respond sooner and more swiftly to CSA through regulations and tax restructuring than federal legislators, with the added benefit of greater numbers of seats ( 7,000+ state vs. 535 federal) for more nuanced and localized measures across states of disparate sizes. We collected the number of Republican and Democrat legislators in the upper house (State Senate) and lower house (State House Representatives) from https://ballotpedia.org. State legislature tenure varies by state but most often is biennial. Therefore, we use the most recent year before the event to collect the data for firm i at time t in year T. Government_Stanceit=Total number of Republican membersiT−1 −Total number of Democrat membersiT−1 Total number of Republican and Democrat members iT−1. Graph4We use the calculated stance measures as the average stakeholder ideology and create three variables for the level of deviation between the stakeholders' political ideologies and the CSA event stance. For each event, we assume the absolute value of distance between the Event_Stance and the Stakeholder_Stance as the degree of deviation between the CSA event and stakeholder ideology: CSA_Customer_Deviation= |Event_Stance−Customer_Stance|, Graph5 CSA_Employee_Deviation= |Event_Stance−Employee_Stance|, Graph6 CSA_Government_Deviation= |Event_Stance−Government_Stance|. Graph7The degree of deviation falls between 0 and 2, where a value closer to 2 shows a stronger deviation between the values conveyed and supported by the CSA and the ideology of the stakeholders. Table 1 provides examples of deviation measures. Degree of deviation from the firm's brand imageTo measure the degree of deviation between the focal sociopolitical issue and the firm's brand image, we constructed a fit variable using ex post customer evaluations of the events obtained from a survey (Survey 4). From MTurk, 552 U.S. adults (48% female; median age = 32 years) participated in a survey for a nominal fee. Participants rated a randomly drawn subset of 30 CSA on the extent to which the event seemed ""like something that suits or is congruent with the brand's image (high fit) or seems very incongruent (low fit)"" using a seven-point scale (1 = ""low fit,"" and 7 = ""high fit""). This produced approximately 50 ratings per firm, which we mean-averaged to create a brand fit index that we reverse coded to create CSA_Brand_Deviation. Form of supportWe text analyzed all the events in our sample and categorized them into two categories: whether the activism took the form of an action or a statement. We code a dummy variable (Action) indicating whether the CSA event made specific mention of a concrete action (or actions). We classify the event as an action even if there are also statements issued. Announcement source statureWe determine whether the CEO delivered the CSA announcement of a firm as a dummy variable (CEO_Announcement). For all events for which CEO_Announcement is equal to 1, we thoroughly reviewed the event to confirm that the CSA was announced by the CEO ""as the representative of the firm"" and not as an individual conveying his or her political views. We exclude the latter from our definition of CSA.[11] Business interest communicationTwo research assistants blind to our research question manually text analyzed all of the events in our sample and independently categorized the events into two groups based on whether the firm communicated its business interest or potential positive business outcomes from their CSA. Business_Communication is equal to 1 if the firm communicates its business interests along with the social motive. For example, in September 2014, Ben & Jerry signed an ""Employers' Amicus Brief"" in support of same-sex marriage. Chris Miller, Mission Activism Manager of Ben & Jerry explained, ""It's not enough to change the way you do business, or change the practice within your business....Unless you're willing to stand up and advocate for the rights of others, not just here in our backyard but around the world, it's often just not good enough."" Miller further explained that in addition to being an issue of civil rights, ""LGBT discrimination law complicates the running of our businesses, creates confusing administrative nightmares for companies and introduces difficulties in recruiting folks from other states"" ([ 5]). General Electric signed the same Amicus Brief. However, it did not communicate the business aspects of this Amicus Brief with its stakeholders. This former event was coded as 1 for Business_Communication while the latter is coded as 0. The coders agreed on 85.6% of the events and resolved disagreements via discussion. Coalition sizeFinally, we calculate the variable Coalition_Size as the number of firms explicitly joining forces to announce the CSA event at the same time and in the same statement, such as multiple firms' Amicus Briefs or open letters to support or oppose a sociopolitical issue. We read each event announcement carefully and set the variable (Coalition_Size) as zero if the firm conducted the CSA on its own. The variable otherwise takes the value of the count of firms involved in the activism. Control VariablesIn addition to the independent variables, multiple explanatory firm-, event-, industry-, and time-specific variables can affect investors' reactions to CSA. We attempt to address such factors in a control-rich model using several variables. (For a detailed explanation of the coding and collection of these control variables, see Table W3 in the Web Appendix). Firm-specific factorsTo disentangle the effect of CSA from CSR and CPA, we control for variables pertaining to firms' CSR and CPA involvement. Following [68] procedures, we collected firms' CSR indices from Kinder, Lydenberg, and Domini Research and Analytics and use the average total score of firms' CSR indices in the past three years as the proxy for their CSR activities. To account for firms' CPA, we collected firms' average political donations to Republican and Democratic campaigns in the past three years from the Center for Responsive Politics. Table W3 in the Web Appendix lists these variables in detail.Other variables control for the firms' financial status at the time of the event. Variables collected from COMPUSTAT include firms' primary operating market (B2B_B2C), return on assets (ROA), firm size (Firm_Size), firm leverage (Leverage), and advertising expenditure (Advertising_Expenditure). We also control for marketing capability (Marketing_Capability) and the presence of a chief marketing officer (CMO) to account for their potential effects on CSA performance and the response efficiency after CSA.Furthermore, CSA is a corporate strategy executed through firms' brands. Therefore, we account for differences when a multibrand versus a single-brand firm engages in activism. We control for the total number of brands owned by the firm and use the natural logarithm of this number in the model (Log_Brand_Number). We also control for the percentage of institutional stock holdings (Institutional_Holdings) for each firm to account for the possibility that individual and institutional investors react differently to CSA. Finally, we create a variable to account for firms' reputations for engaging in CSA, which may shape investor expectations ([90]). For each firm, we record the number of past events in a rolling window of three years before the event (Past_CSA). CEO-specific factorsWe include a series of variables specific to the CEO of the firm. We control for the CEO's political ideology, which can influence a firm's culture and indirectly affect Employee_Stance.[12] We calculate CEO_Political_Ideology with a similar approach as Equation 3. In addition, we collect CEOs' gender (CEO_Gender) and age (CEO_Age) at the time of the event to address differences in their inclination to take risks and engage in or encourage activism ([29]). Event-, industry-, and time-specific factorsWe use a categorical variable (Event_Category) to account for the topic of the polarizing issue (e.g., immigration). Next, we address the popular belief that high-tech industries are more inclined toward liberal ideologies ([65]) by incorporating a dummy variable (High-Tech) for the high-tech (vs. low-tech) industry. To control for the potentially greater sensitivity to politically polarizing statements during presidential election years, we also include a dummy variable (Election_Year). Finally, we control for other unobserved industry- and time-specific factors by including the industry (Industry_Dummy) and year (Year_Dummy) dummy variables. Model SpecificationTo test our first hypothesis (H1), we conduct a t-test on the five-day window CAR for firms conducting CSA. We follow this test with a regression model with two-way clustered errors to test H2–H6. All firms in the primary sample engaged in at least one instance of CSA during 2011–2016, which gives the potential for selection bias. To investigate the extent to which selection bias might explain our results, we employ [42] two-stage correction approach. In the first stage, we run a panel data probit model in which the dependent variable is the decision to engage in activism (a dummy equal to 1 if the firm had an activism event in year T and 0 otherwise) in each year. In the second step, we include the inverse Mills ratio (IMR) (derived from the first stage) alongside the control variables.To facilitate identification, in the first stage, we use two exogenous determinants of the decision to engage in activism. First, following standard practice in the literature, we use the average number of industry-year instances of activism (Average_Industry_CSA) for each year (e.g., [34]; [82]). For each observation in the primary sample, we calculate Average_Industry_CSA by extracting from COMPUSTAT all firms that have not engaged in activism from all the four-digit SIC codes whose focal firm is a primary member. We divide the number of activist firms in all the SIC code-years by the total number of primary firms in the SIC code. Second, we include the average number of instances of activism that occurred in the same geographic region (firms' headquarters state) in the same year (Average_State_CSA) across all industries excluding the focal firm. These variables must meet relevance and exclusion restrictions, which we explain in section W5 of the Web Appendix.We control for the percentage of institutional holdings, financial status of the firms (ROA, size, leverage, and advertising expenditure), and time- and industry-specific variables. We run the first-stage probit model for firm i in year T: PriT(Corporate_Activism = 1|Covariates, ςi T)= Φ(Xβ), Graph8where X is a vector of covariates as follows: Xβ = β0+β1Average_Industry_CSAiT+β2Average_State_CSAiT+β3Institutional_HoldingsiT+β4Election_YeariT+β5ROAiT+β6Firm_SizeiT+β7LeverageiT+β8Advertising_ExpenditureiT+β9High-TechiT+kitIndustrykiT+mitYear_DummymiT+σiT. GraphWe estimate the IMR from Equation 8 at the annual level and include IMR associated with each event of firm i that occurred in time t during year T in Equation 9 to provide tests for H2–H6: CARit= α0+α1CSA_Customer_Deviationit+α2CSA_Employee_Deviationit+α3CSA_Government_Deviationit+α4CSA_Brand_Deviationit+α5Actionit+α6CEO_Announcementit+α7Business_Communication+α8Coalition_Sizeit+αW+α′IMRit+∊it, Graph9where i and t indicate the firm and the time of the event, respectively; W is a vector of control variables in Table W3 of the Web Appendix; IMRit is the inverse Mills ratio from the first-stage selection model; and ∊it represents the two-dimensional clustered standard errors that account for the clustering across firms and events ([15]; [87]). All continuous variables are Winsorized at the 1% and 99% levels to reduce the effect of outliers (e.g., [94]). Results Descriptive StatisticsTable 2 contains descriptive statistics and correlations. The range for stakeholder deviation variables indicates that our sample covers activism ranging from very low degrees of deviation (.0029) to very high degrees of deviation (1.86). The averages for stakeholder deviation variables are CSA_Customer_Deviation =.48, CSA_Employee_Deviation =.56, and CSA_Government_Deviation =.45. The averages show that, in general, while CSA events have some level of deviation from stakeholders' values, firms avoid engaging in CSA events that deviate a great deal from stakeholders' values. CSA_Brand_Deviation reflects the level of deviation between the brand image and CSA event; on average, the respondents rated the level of deviation as 3.53 out of 7. The respondents rated the deviation between Starbucks and supporting marriage equality as the lowest (CSA_Brand_Deviation = 2.27), while they rated Walmart's CSA to be the first store to remove all Confederate flag merchandises as the highest (CSA_Brand_Deviation = 4.62).Our descriptive findings further indicate that despite the popular definition for sociopolitical activism in recent literature, which limits CSA to ""sociopolitical statements"" (e.g., [41]), 40% of the CSA events in our sample are accompanied by a form of action from the firm. These actions include but are not limited to changing the color of product packaging to support LGBTQ rights, taking down products with Confederate flag logos, retracting or issuing an advertising campaign, and changing firm policies or strategies.The CEO of the firm directly announces only 28% of the CSA events. In most cases, CSA is announced in the news by mentioning the ""firm"" as the acting agency or one of the firm's more recognizable brands. In some other instances, the event is announced by firms' diversity officers, marketing managers, or public relations departments. On average, firms communicate their business interests or potential financial benefits of CSA in 50% of the events. Finally, while 65% of CSAs are conducted alone, 35% of the firms form coalitions with, on average, eight other companies to conduct CSA. Hypothesis testsThe first three rows of Table 2 show that on average, the abnormal returns to CSA are negative and statistically significant, with a mean of −.43% for the market model, −.40% for the market-adjusted model, and −.44% for the Fama–French–Carhart model (p <.001). This negative and significant average supports H1 and indicates that, on average, investors react unfavorably to firms engaging in CSA.GraphTable 2. Descriptive Statistics and Correlations.  1 Notes: Boldfaced values for correlations indicate significance at 95% level.To examine support for H2–H6, we estimated a two-step Heckman correction model in Equations 8 and 9. We first calculated the variance inflation factors; the average is 1.44, and the maximum is 1.88, well below the threshold of 10 to ensure the model does not suffer from multicollinearity. We report the results for the first-stage model in Table W4-1, Section W4 of the Web Appendix. We calculate the IMR from the first-stage model Before inserting the IMR in the second stage, we examine the strength of the exclusion restriction assumption in the first stage. In section W4 in the Web Appendix, we conceptually explain and provide statistical evidence that our instruments satisfy exclusion restriction; the instruments neither are correlated with nor will they systematically affect firm-specific omitted variables that influence investors' reactions to the focal firm's activism. We include the estimated IMRiT to each event of firm i that occurred in time t during year T, in the regression for Equations 9. Table 3 provides the results for the second stage model. We first run the model for Equation 9 without control variables to confirm that the results are not a product of overparameterization caused by the long list of control variables. Model 1 in Table 3 shows the results without control variables.GraphTable 3. Effect of CSA on Stock Market Abnormal Returns.  2 *p <.10.3 **p <.05.4 ***p <.01.5 Notes: Event, year, and industry dummies are omitted from the table because of limited space.Model 2 provides the control-rich model results for Equation 9. The coefficient for CSA_Customer_Deviation is negative and significant (α1 = −.023, p <.05), in support of H2a. Similarly, the coefficients for CSA_Employee_Deviation and CSA_Government_Deviation are negative and significant (α2 = −.017 and α3 = −.022, respectively, p <.01), in support of H2b and H2c. These results indicate that greater sociopolitical deviation between the stance of the CSA event and the stakeholders' dominant sociopolitical ideology reduces short-term abnormal stock returns.The coefficient for CSA_Brand_Deviation, though negative, is not statistically significant and does not support H3. Perhaps a high degree of brand image fit is difficult to achieve among CSA issues. Or, it is possible that the controversial nature of CSA overrides any fit effects. This surprising null effect is fertile ground for future research.The coefficients for Action and CEO_Announcement are negative and significant (α5 = −.0089 and α6= −.015, respectively, p <.05), in support of H4 and H5. Events in the form of actions or announced by the CEO likely signal a stronger commitment of time, attention, and resources to the CSA issue. Investors perceive this stronger commitment to a partisan sociopolitical issue as particularly risky and an unnecessary deviation from the firm's primary profit-oriented objectives. The coefficient for Business_Communication is positive and significant (α7 =.0099, p <.05), which supports H6 and indicates that communicating business interests can alleviate investors' concerns about firms' resource allocation, decrease uncertainty and improve investors' reaction to CSA. Finally, the coefficient for Coalition_Size is positive and significant (α6 =.00024, p <.05), which supports H7 and indicates that the announcement of CSA with other firms is less concerning as it reduces the riskiness of the event and provides investors with more assurance that engagement in CSA may be necessary.In addition, consistent with previous research, we observe that larger firms receive a weaker investor response (α = −.0020, p <.05) (e.g., [11]). Investor reactions to female CEOs who conduct activism are also more favorable (α =.030, p <.05). Female CEOs are expected to be more caring and concerned about others than male CEOs ([75]). Therefore, promoting societal change might be perceived as more expected and more acceptable from a female than male CEO. The coefficient for CMO is positive and significant (α =.019, p <.05), which indicates that investors may perceive firms with a CMO in their C-suite as more capable of managing their CSA effectively. Finally, we observe a positive and significant effect for High-Tech (α =.011, p <.05), which provides evidence that activism and seeking societal progress is more expected and accepted from firms in high-tech industries ([65]). Robustness TestsWhere possible, we tested alternative operationalizations of key variables. Several important variables in our model are derived from the Event_Stance measure, which is collected retrospectively through a survey. We check the validity of this measure and check the robustness of the results using an alternative dichotomized variable (Conservative = 1 and Liberal = 0). In addition, we alternatively operationalize for CSA_Customer_Deviation using secondary data from EquiTrend, for CSA_Employee_Deviation using employees' number of donation transactions weighted by firms' total number of employees, and for Business_Communication using the count number of keywords related to business interest of the firm. Finally, we use alternative estimations of CARs (market-adjusted model and Fama–French–Carhart Model) and an alternative three-day window of analysis for the event study. The robustness tests are explained in detail along with the tables of results in Section W5 of the Web Appendix. Additional InsightsAlthough the results of the main model are insightful, they do not fully answer an important question: How should managers proceed when the CSA deviation varies across stakeholders? For example, a manager of a firm such as Whole Foods might feel pressured to engage in liberal-oriented CSA to appease its liberal-leaning customer base but fear retaliation from its conservative state legislature in Texas. To provide actionable managerial insights for such situations, we further explore responses to various combinations of CSA–stakeholder deviations. Response to CSA When Stakeholders Have Conflicting Political Values Investor responsesTo explore investors' reactions to different levels of deviations between CSA and conflicting stakeholder ideologies, we create a dichotomous measure of deviation. The dichotomous measure helps clearly identify various scenarios to better understand investor and customer responses. We use the mean for each CSA_Stakeholder_Deviation variable as the cutoff to divide the CSA events into two groups of low- and high-level deviation from each stakeholder's ideology. Next, we classify all the events into eight groups, which reflect all combinations of CSA–stakeholder deviation (e.g., customer low, employee high, legislator low). The sixth row of Table 4 reports the results of the t-tests for short-term abnormal stock returns (CARMarket_Model) for low and high CSA–stakeholder deviation across the eight groups.GraphTable 4. Stock Market and Customers' Reactions to CSA Based on Level of Deviation from Stakeholders' Sociopolitical Values.  6 *p <.10.7 **p <.05.8 ***p <.01.Several valuable insights emerge from these results. First, investors' reactions are surprisingly positive (.71%, p <.05) when CSA–stakeholder deviation is low across all key stakeholders (Group 1). Investors may expect that CSA with minimal deviation from stakeholders can strengthen relationships and thereby enhance performance. Second, investors' reactions to CSA are not negative and significant when its degree of deviation is low for at least two key stakeholders (Groups 2 and 3). The exception is Group 4, where CSA–stakeholder deviation is low for employees and the government but high for customers. This underscores the notable risk of alienating customers, even if the CSA aligns with the ideological values of local governments and employees.Third, investor reactions are generally adverse when there is high CSA–stakeholder deviation among at least two key stakeholders (Groups 5 and 7). Investor reactions are most severe (−2.45%, p <.001) when CSA–stakeholder deviation is high across all three key stakeholders (Group 8). This prompts the question of why a firm would engage in such misaligned CSA. Based on a comprehensive archival search for each of the events in Group 8, it appears that such CSA is often related to strategic miscalculations. For example, J.C. Penny hired CEO Ron Johnson in 2011 from Apple, a firm with a highly progressive corporate culture. Johnson's approach clashed with the conservative values of J.C. Penny's stakeholders. Under Johnson, J.C. Penney invested in same-sex partner advertisements for Mother's and Father's Day in 2012. In April 2013, J.C. Penny finally accepted its ""strategic mistakes"" after the free fall of its stock value and fired Ron Johnson ([60]). In summary, CSA can have a positive effect on investor reactions, but only when it aligns with key stakeholders. Customer responsesExisting theory suggests that customers should be more loyal to and increase purchases from firms whose CSA aligns with their ideological values ([20]; [74]). Conversely, customers should boycott or disidentify with firms whose CSA deviates from their values. Indeed, third-party websites are dedicated to monitoring firm activities to guide customers on which to boycott (e.g., www.2ndvote.com). To examine the changes in customers' responses to CSA based on the level of CSA–customer deviation, we focus on a focal indicator of customers' reactions, namely, growth in sales realized in the quarter and in the year following a CSA event.For each firm in our sample, we collect growth in sales reported by COMPUSTAT for two periods. First, to address the immediate changes in sales and consumer response, we compute Quarterly_Sales_Growth for the quarter immediately before and immediately after the activism event. Second, to address seasonality effects and long-term effects of CSA, we compute Annual_Sales_Growth for the average of quarterly sales report for four quarters before to four quarters after the CSA event: Quarterly_Sales_Growthit=(Salest−Salest−1)Salest−1, Graph10 Annual_Sales_Growthit  =∑tt+3Salesq −∑t−5t−1Salest∑t−5t−1Salest. Graph11The last two rows of Table 4 report the sales growth for each group. As shown for Groups 1–3, quarterly and annual sales growth are positive and significant (above 4% to 10%, p <.01) for CSA events that have a low level of deviation from customers' ideology. In addition, for the 55% of events where CSA–customer deviation is low (Groups 1–3 and 5), there was an increase in sales growth. When CSA is highly deviated from customers and the government, sales growth suffered. This is especially true when CSA highly deviated from all three key stakeholders (Group 8), which saw a sales decline of 4% (p <.05). Compared with investor reactions, the findings here suggest that there are many cases where firms can engage in CSA and reap financial rewards even when it is not aligned with all their stakeholders. These benefits can accrue as stock performance, sales growth, or both. General DiscussionAs firms increasingly engage in CSA, existing approaches for understanding activism in the realm of either CSR or CPA cannot adequately address the unique features of CSA and its financial consequences. We contend that while CSA is a risky marketing strategy that investors are generally wary of, it may also be advantageous. Investors on average react negatively to CSA, especially when it deviates from the values of key stakeholders and signals the firm's resource-intensive commitment to activism. However, they also reward activism when it closely aligns with stakeholders. In addition, we show customers reward CSA when it resonates with their personal values and attest that it can be an effective means for firms to appeal to their target markets. Our findings generate several theoretical and managerial implications, as well as avenues for future research. Theoretical ImplicationsIn ""being close to the real world of marketing"" ([69], p. 2), our research advances the marketing strategy literature and the nascent work on activism. We build on existing conceptualizations of activism to provide a comprehensive definition of CSA and introduce it as a new potential firm strategy worthy of investigation. CSA has been partially defined in management and public relations literature as a form of social advocacy (e.g., [25]) or a sociopolitical initiative exclusive to the firm's CEO (e.g., [17]; [41]). We include ""corporate actions"" in the definition of CSA in addition to sociopolitical advocacy in the form of statements and comprehensively define CSA as a corporate activity which pertains to partisan sociopolitical issues that can be executed by any representative of the firm or via firms' brands. Indeed, 40% of our sample consists of CSA in the form of actions (vs. statements), many of which pertain to the marketing mix such as introducing new products, redesigning packaging, and creating or terminating advertising campaigns. The inclusion of such marketing actions in the definition of CSA importantly highlights that CSA can be a firm strategy that aligns with firms' stakeholder and brand orientations.We delineate CSA from CSR and CPA and argue that despite conceptual similarities, activism indeed represents a novel phenomenon worthy of unique investigation. Prior research has demonstrated a positive effect of CSR and CPA on firm value (e.g., [61]; [64]). By contrast, we demonstrate the complexity of CSA and document the overall negative effect of CSA on stock market returns as well as identify scenarios where it can have positive financial consequences. Decades of research have delivered an elaborate understanding of CSR, yet the logic of CSR is insufficient for understanding the effects we observe and explain in our CSA framework.We ground our theoretical arguments in signaling and screening theories to provide a conceptual framework for the effect of CSA on firm value. In addition to introducing CSA as a new construct, we also introduce two sets of moderators that explain investor response to CSA. These moderators help locate CSA as a marketing strategy. Investor responses to CSA are shaped by the implementation of CSA (e.g., whether it is an action or statement) and its alignment with the personal values of key stakeholders, namely, customers. And, indeed, the effects of CSA and its moderators on sales growth indicate that customers pay attention to and make long-lasting purchase decisions based on CSA. Although CSA can be a risky strategy, it can also provide real performance benefits. More broadly, our work demonstrates both theoretically and empirically how marketing actions must align across stakeholders. Stakeholder alignment theory is not new (e.g., [41]), but it is less frequently examined in the context of marketing decisions. We, therefore, advance marketing strategy theory by bridging signaling and screening theories and stakeholder alignment theory. Managerial ImplicationsA critical question for managers is whether they should engage in CSA. Our findings can help managers make this decision by informing them about how investors will react in the short run (abnormal stock returns) and how customers will react in the long run (sales growth). First, we demonstrate the importance of stakeholders' political values. Investor responses depend on how much the CSA deviates from the values of customers, employees, and state legislators; higher deviation elicits stronger negative reactions. Notably, if a CSA stance closely aligns with all three stakeholder groups, managers can expect a positive investor response. Thus, managers who are concerned about CSA's impact on shareholder value should first consider how much their stance deviates from other stakeholders' values. Critically, CSA never elicits a positive investor response when it deviates from customers' values. Managers should pay close attention to how greatly their CSA deviates from customers because it has ramifications not only for investor responses but also for long-term customer responses.Investors are inclined to punish CSA that highly deviates from customers and customers are inclined to reward CSA that closely aligns with their values. Our findings show that regardless of deviation from employees and state legislators, when CSA is aligned with customers, managers can expect positive sales growth over the next quarter and year. Furthermore, when CSA is aligned with customers and at least one other stakeholder group, managers can expect positive sales growth without an adverse stock market reaction. Thus, managers should weigh their customers' values more heavily in their decision to engage in CSA. Importantly, our analysis of sales growth shows that CSA can have a lasting impact on firms: customers continue to reward or punish firms long after CSA is implemented. In summary, CSA that is aligned with customers can help managers avoid an adverse stock market reaction and elicit positive future sales growth.Managers who choose to engage in CSA should be cognizant that it reveals important information to investors and the public about their strategic priorities and values. It signals that the firm is willing to engage in a risky activity and divert resources from profit-generating activities, and that it may make similar decisions in the future. It also reveals sensitive information pertaining to the firm's perspective on its role in society and the political engagement of its senior management. Importantly, because CSA reveals the firm's values to the public, it may have an enduring impact on the firm's future decisions related to its overall purpose, reputation, and management of stakeholder relationships. Given that CSA is difficult to retract and has lasting financial implications, managers should be confident in their stance and their decision to publicize it.A second critical question for managers is how to conduct activism. Our findings suggest that managers should carefully consider how to implement CSA because it influences investors' inferences about the firm's commitment to activism versus its fiduciary duties. We identify four characteristics of CSA implementation to which investors are particularly discerning. Managers should be aware that they will receive a heightened response from investors when their activism takes the form of actions, is announced by the CEO, is not justified by a business objective, and is announced alone (vs. in a coalition with other firms). If managers are deeply committed to activism and it aligns with their strategic objectives (i.e., acquiring a more liberal or conservative customer base), activism's potential benefits may be worth an intensified negative response from investors. However, if managers are uncertain about activism's role in their firm's future strategic priorities or they are sensitive to investor responses, they should choose a more moderate approach to engaging in CSA. In summary, CSA is a risky firm activity that managers must carefully consider before implementing. Limitations and Further ResearchIn its contribution to an emerging area of research, our study may provide several new avenues of research. First, our study informs managers about how investors respond to CSA based on the deviation from three major stakeholders' values (customers, employees, and state legislatures). However, there are other stakeholders yet to be studied. These stakeholders include but are not limited to ( 1) the firm's top management team, especially the CMO, ( 2) boards of advisors, and ( 3) federal government legislators. Moreover, our deviation measures do not capture the direction of the deviation from stakeholders' values, which can be a fertile ground for future research.Second, we investigate CSA resource implementation characteristics (form of support, announcement source stature, business interest communication, and coalition size). However, from a marketing perspective, we believe understanding how CSA affects customers' attitudes, relationships with the brand, and purchase decisions are other worthy areas of study. The nonsignificant effect of the deviation measure for brand image in our study suggests other influential and explanatory brand- or product-specific factors should be studied. For example, perhaps product type (hedonic vs. utilitarian) or consumption context (private vs. public) influences customers' boycott or buycott responses to CSA. In addition, CSA may serve as a customer acquisition strategy and help firms better appeal to their target markets.Third, while we control for firms' previous CSA actions, we do not examine the authenticity, consistency, or style of their CSA strategies. Future research might explore ( 1) whether CSA is a reaction to corporate wrongdoing (e.g., sexual or racial discrimination), ( 2) whether the firm conducts activism by supporting a vulnerable minority or attacking the majority, ( 3) the level of financial resources committed to CSA, and ( 4) an integrated CSA communication with simultaneous or continuous multiple activities in different contexts (e.g., a new product launch).Fourth, while our study includes CSA delivered by the CEO as a representative of the firm and controls for their political donations, we do not study how personal activism that someone conducts outside of his or her role as CEO might spill over to affect a firm. Fifth, our study informs managers about the short-term financial consequences of unique CSA events, but it does not examine the potential long-term effects of a CSA strategy. As sociopolitical activism has now entered the realm of strategic marketing, the long-term strategy of the firm can have broader consequences, such as changes in brand equity, firm reputation, customer base composition, market share, performance relative to competitors, and long-term performance of firms. More specifically, future research should account for investors' projections of how stakeholders' values may change in the future and how these changes will affect their responses to CSA. For example, investors may project a firm's customer base to become more liberal over time or a firm's stance to become widely accepted in the future. Finally, CSA has the potential to shape culture. We advocate for research to address the broader impact of CSA on societal outcomes. "
19,"Creating Boundary-Breaking, Marketing-Relevant Consumer Research Consumer research often fails to have broad impact on members of the marketing discipline, on adjacent disciplines studying related phenomena, and on relevant stakeholders who stand to benefit from the knowledge created by rigorous research. The authors propose that impact is limited because consumer researchers have adhered to a set of implicit boundaries or defaults regarding what consumer researchers study, why they study it, and how they do so. The authors identify these boundaries and describe how they can be challenged. By detailing five impactful articles and identifying others, they show that boundary-breaking, marketing-relevant consumer research can influence relevant stakeholders including academics in marketing and allied disciplines as well as a wide range of marketplace actors (e.g., business practitioners, policy makers, the media, society). Drawing on these articles, the authors articulate what researchers can do to break boundaries and enhance the impact of their research. They also indicate why engaging in boundary-breaking work and enhancing the breadth of marketing's influence is good for both individual researchers and the fields of consumer research and marketing.KEYWORDS_SPLITConsumption and consumers are interwoven with contemporary society; therefore, marketers, journalists, policy makers, and members of the public all have a stake in the topics that consumer researchers study. Our work can also influence research in adjacent disciplines. Although our position as academics potentially sets us up as thought leaders, one wonders why our work does not have broader impact on these marketplace stakeholders ([63]) as well as on academics in other disciplines. Consumer researchers tend to cite scholars in other fields (e.g., psychology, anthropology, sociology) far more than scholars in other disciplines cite us. Similarly, most business practitioners turn to accessible, business-related popular writers before they seek the advice of consumer researchers. In the policy realm, our influence is often dwarfed by that of economists and legal professionals.The relatively narrow impact of consumer research is not due to a lack of talent or commitment of individual researchers, the quality or rigor of our work, or our potential to offer insights. Rather, we argue that we and others in our field have handicapped ourselves by adhering to a set of implicit boundaries or defaults about what we study, why we should study it, and how we communicate the significance of our work. Adhering to such defaults can limit our thinking, the knowledge we produce, how we execute research, and how we disseminate our findings. More specifically, current consumer research is often inspired by existing academic literature, sometimes ignoring emergent substantive marketing-relevant consumer research issues[ 5] pertinent to marketplace stakeholders and academics in other disciplines. While our research typically illuminates construct-to-construct links, it often eschews more unstructured real-world phenomena for which novel constructs could be developed. It emphasizes individuals, as opposed to the small and large groups of which consumers are members. Furthermore, our research is often published in erudite academic journals but not further communicated to marketplace stakeholder groups for whom the findings may be relevant.As a consequence of these implicit boundaries, consumer research yields limited cross-fertilization of ideas and knowledge diversity and is perceived to lack significance, despite its interdisciplinary and multistakeholder potential ([63]; [65]). We urge consumer researchers to break these boundaries and broaden our impact, lest we become irrelevant to nonacademic marketing stakeholders and cede influence to nonmarketing academic disciplines. As aptly noted by the current editors of the Journal of Marketing, ""We think the field needs to pull off its blinders and uncover new ways of thinking about marketing and the marketplace"" ([44], p. 2). In short, while consumer research has yielded substantial insights into the behavior of consumers, we believe that we can do more to broaden the impact of our work.Why should and how does one engage in boundary-breaking, marketing-relevant consumer research? We offer some ideas pertaining to these important issues in the three sections that follow. In the first section, we present a conceptual framework (see the section ""Choices for Engaging in Consumer Research: Implicit Boundaries"") that distinguishes the implicit boundaries that characterize our choices of marketing-relevant consumer research from boundary-breaking alternatives. Although we are not the first to argue in favor of some of the ideas captured in our framework (e.g., [34]; [42]; [65]), we hope that the structured approach offered herein, coupled with the topics discussed in our first section, make salient the relevant boundaries and the opportunities that consumer researchers have to break them.In the second section, we provide guidance to the ambitious consumer researcher aiming to contribute in this way. Specifically, we describe five published articles that exemplify boundary-breaking, marketing-relevant consumer research. These articles have offered fresh and novel insights for academics in marketing and related disciplines. They have also had tangible and significant effects on other relevant marketplace stakeholders, including business, government, and society. We articulate concrete lessons from these cases to guide authors. We also offer specific strategies designed to help researchers, faculty members who train doctoral students, and other gatekeepers identify actions that can facilitate and accelerate boundary-breaking consumer research (see Table 1). Whereas these case studies illustrate our core ideas, we further guide researchers by offering other examples of boundary-breaking consumer research (see Table 2). We hope that this guidance will reduce the perception that the field's disciplinary norms and instructional practices make it too risky to have broader impact on stakeholders outside of academic marketing and consumer research.In the third and final section, we argue that boundary-breaking consumer research can have rewarding outcomes to individual consumer researchers and the field as a whole. Boundary-breaking research enhances the credibility of consumer research scholars as substantive (real-world) experts, addressing criticisms that our research is incremental. Such research also makes salient novel and important research questions that can be raised by breaking these boundaries (see Table 3). In so doing, it contributes to Journal of Marketing's larger objective of being ""a marketplace of ideas"" that will help ""develop and disseminate knowledge about real-world marketing issues relevant to scholars, educators, managers, policy makers, consumers, and other societal stakeholders"" ([45], p. 1). Finally, boundary-breaking research connects us to a broader set of constituents who are starved for insights on consumer behavior as a means to create a better world for stakeholders and consumers alike. Choices for Engaging in Consumer Research: Implicit BoundariesWhy do we conduct consumer research? What choices do we make about why, what, and how we conduct our research and disseminate our findings? We outline some of these choices in Figures 1 through 3, respectively. We use the phrase ""implicit boundaries"" to characterize the default choices that many of us make automatically and the phrase ""boundary-breaking opportunities"" to characterize a wide range of alternative choices that are adopted less frequently (though are equally important). We believe that implicit boundaries regarding our choices about engaging in consumer research, while well-entrenched and familiar, blind us to new ways of contributing to knowledge (see the left-hand sides of the figures). However, breaking these boundaries and choosing underutilized opportunities (see the right-hand sides of the figures) reflect vehicles by which our individual and collective impact can be broadened. While these figures are not intended to be comprehensive, they are designed to provide a structured approach for identifying common implicit boundaries and illustrating boundary-breaking alternatives. Why Conduct Consumer Research?Figure 1 outlines the choices consumer researchers make when they decide why to conduct research. These decisions involve the stakeholders they choose to influence, the ideas they choose to test, the ways in which they choose to contribute to theory, and the manner in which they choose to investigate outcomes.Graph: Figure 1. Consumer research choices: Why conduct consumer research? Influence stakeholdersConsumer research is often targeted primarily or exclusively to marketing or consumer research scholars. We can broaden our impact by considering how our work can contribute to a larger set of stakeholder groups, such as those identified in the right-hand side of Figure 1. These stakeholders include academics in disciplines outside of marketing; educators and their students; different types of firms; government and nongovernment agencies; the media; and, more broadly, society. For example, [11] provided important strategic insights for marketers by using consumer research theories to explain pioneering advantages to firms. Arguably, our ability to influence stakeholders outside of the academic marketing arena is the ultimate indicator of broad impact. Test ideasAs consumer researchers, we are motivated to test ideas that inspire us. Where do these ideas come from? In most cases, inspiration emanates from marketing academic articles or conference papers. This implicit boundary limits the attention we pay to the myriad real-world phenomena that stakeholders care about. Ideas can emerge from interactions with academics outside of marketing, practitioners or consumer groups in the public or private sector, academic-practice forums, direct observation, or media reports. Contribute to theoryConsumer researchers often aim to provide theoretical contributions. An implicit rule is that this contribution is based on mapping relationships between constructs. Boundary-breaking opportunities can emerge when we use our conceptual skills to add structure to real-world, messy, and often disorganized phenomena ([35]; [37]). One way to do so is by engaging in phenomenon–construct mapping. Here, researchers start with observations of real-world marketing-relevant phenomena and then identify constructs and relationships that can explain them. Qualitative research approaches tend to focus on the holistic qualities of complex phenomena. Thus, we tend to see more phenomenon–construct mapping in qualitative empirical research than in quantitative empirical research. Investigate outcomesConsumer researchers are also motivated to investigate outcomes. The current default, at least among psychologically oriented consumer researchers, is to explain and predict consumer response by identifying cause–effect relationships. Research with a descriptive and/or evaluative goal is less common. Descriptive research maps out a real-world consumer phenomenon and articulates who the relevant actors are; what their focal actions are; and when, where, and how these actions take place. Descriptive research is foundational to theory building, and it can add structure to complex and poorly understood substantive issues. In addition, descriptive research is often more multidimensional, articulating complex phenomena in ways that describe and illuminate these dimensions and their importance.Evaluative research, in contrast, assesses whether consumers' interactions with marketplace stakeholders benefit the parties involved. These interactions can result in win-win, win-lose, or lose-lose outcomes and therefore offer normative guidance. Win-win relationships benefit all parties (e.g., successful corporate social responsibility). Win-lose relationships benefit one entity while hurting another (e.g., predatory lending, which helps lenders and hurts consumers). Lose-lose relationships inadvertently benefit no one (e.g., when product information is intended to help consumers but instead confuses them and thereby hurts both sellers and buyers). Too often, we fail to consider consumer research from this evaluative perspective and/or take a critical stance against marketers and other marketplace stakeholders who produce outcomes that harm consumers. What Is Studied in Consumer Research?Figure 2 outlines the choices consumer researchers make when they decide what to study. These decisions involve the units of analyses, the decision contexts, and the time frames researchers choose to study, as well as the metaphorical consumer role they choose as a lens for thinking about the relationship between consumers and marketplace stakeholders.Graph: Figure 2. Consumer research choices: What is studied in consumer research? Unit of analysisFrom a unit-of-analysis standpoint, the implicit boundary is to study individual consumers, most often in the United States. In reality, consumers rarely operate alone; rather, they interact in small groups (e.g., dyads, households, peer groups), large groups (e.g., segments, communities, tribes, organizations, markets), and diverse populations in regions throughout the world. Consumer Culture Theory (see [ 3]) researchers have been more receptive to examining group-level phenomena than consumer psychologists have. Still, an overemphasis on individuals means that there are significant opportunities to learn more about the behavior of groups and populations. Decision contextThe current default is to examine static consumer decisions, even though decision making is often dynamic. For example, consumer states and decisions can be reciprocal, as when low self-esteem induces overeating, which further lowers self-esteem. They can also be sequential, as when the purchase of one product stimulates the purchase of another. Decisions can accelerate behaviors, as when a positive consumption experience leads to reduced time between subsequent consumption experiences. We also tend to study consumer decisions independently, although many consumer decisions (e.g., the choice of a doctor) are dependent on other decisions (e.g., a selected medical plan) and are embedded within political, social, legal, organizational, and economic systems. Other decisions are synchronized and coordinated, such as decisions about fashion, home buying and selling, travel, and participation in social media. Time frameOpportunities to break boundaries exist by examining time periods that extend beyond the present, the current default. In general, we have underleveraged our use of the past to understand its relevance to consumer behavior in the present. However, historical information can provide valuable input to novel theories and empirical studies that generate new insights. For example, in the past, conspicuous leisure was associated with status, whereas today, busyness is associated with status ([ 7]). A historical analysis might deepen our understanding of what underlies this change and why. Many stakeholders are also interested in emerging trends and their influence on consumers, and we have the skills to study how trends in demographic, financial, technological, political, and market domains influence consumer behavior. Consumer roleConsumer researchers often consider the metaphorical role of the ""consumer as a target,"" which emphasizes how marketer and policy actions influence consumers. This view is based on the passive mass-media broadcasting model of advertising that arose in the Western world in the 1950s ([56]). In contrast, modern consumers are often active participants in the marketing process. Consumer researchers have begun to study consumers as ""influencers"" who exert leverage through word of mouth, product reviews, and blogging. Consumers are also ""collaborators"" who actively support and embrace brands, work together with firms, and serve as ambassadors. When consumers take on a ""cocreator"" role, they help manufacture and design products and services, create ads, and even set prices. Sometimes consumers are instead ""skeptics"" who resist persuasion attempts, or they are ""adversaries"" who boycott products; initiate lawsuits; purchase counterfeits; or engage in theft, fraud, piracy, or other illegal acts. In addition, while we typically regard consumers as owners of products, they are also sharers; users; experiencers; and givers of information, products, services, and money. Each metaphorical role yields different insights about consumers ([67]). Studying these roles more fully and considering other roles can yield richer insights into the behavior of consumers. How Is Consumer Research Executed?Figure 3 outlines the choices consumer researchers make when they decide how to execute their research. These decisions involve the respondent group and method they choose to employ and the knowledge dissemination activities they engage in pre- and postpublication.Graph: Figure 3. Consumer research choices: How is consumer research executed? Respondent groupThe implicit boundary guiding most consumer researchers is to use students or paid workers as respondents. Breaking this boundary enables researchers to address questions regarding distinct consumer segments based on age or other sociodemographic and socioeconomic variables so as to capture understudied consumers such as minorities, privileged or impoverished classes, and marginalized consumers (e.g., special needs populations). Consumer researchers can also attempt to study real consumers in situ more often than they currently do. Such work may be particularly beneficial when consumers engage in specialized consumption-based roles (as patients, fans, voters, etc.), in professional roles (as managers, employees, etc.), or as they deal with actual marketplace-related decisions and problems. It is likely that in these roles and marketplace contexts, simulated laboratory-based tasks may fail to capture the nuances and context-related interdependencies of consumer decision making. MethodLab and online experiments constitute the vast majority of psychologically based consumer research. Using additional methods can yield different insights, which can broaden impact. For example, the more recent trend toward field experiments helps convince marketplace stakeholders (not just journal editors) that our effects operate in the real world. Machine learning can reveal consumer sentiments and general patterns in massive data sets, while network analyses can add to our knowledge of diffusion and social media processes. Meta-analyses abstract away from specific studies to show more general effect sizes and moderators. Observation, photography, videography, and garbology are underutilized methods that are helpful for studying people in naturalistic contexts. Pre- and postpublication knowledge dissemination activitiesConsumer researchers tend to test and disseminate their ideas before publication at academic conferences and seminars. We often regard publication in an academic journal as a sufficient postpublication activity. However, to have broader impact, it is helpful to employ additional pre-and postpublication knowledge dissemination activities. For example, prior to publication, consumer researchers can ""test-market"" their ideas with stakeholders. Doing so allows researchers to determine whether such ideas resonate and whether construct labels are meaningful to stakeholders. Such test-marketing also allows researchers to gain feedback from stakeholders regarding whether they have effectively captured key aspects of the phenomenon under study, and how they can better position their work to have greater impact. Following publication, knowledge dissemination activities include targeting the media through press releases and interviews with the press, presenting research at conferences populated by key stakeholders, and publishing user-friendly articles on the research in nontraditional, nonacademic outlets. Boundary-Breaking, Marketing-Relevant Articles with Broad ImpactAlthough Figure 1 contrasts an implicit boundary with associated boundary-breaking opportunities, some consumer researchers have developed exemplary boundary-breaking research that crosses multiple boundaries. In this section, we illustrate five ""case studies""—articles published in top journals—that do exactly that. Notably, broad impact did not come at the expense of academic impact, as evidenced by traditional metrics such as citations and awards. While three of the case studies involved members of the current author team, we do not mean to suggest that these cases represent the only or even the best examples of boundary-breaking, marketing-relevant consumer research. Nonetheless, our status as case study authors gave us an insiders' view of the story behind the research and how it evolved. We describe some lessons learned from the set of articles and offer strategies to reduce the institutional barriers that might otherwise discourage such work. Case 1: Muñiz and O'Guinn (2001), ""Brand Community,"" Published in Journal of Consumer Researc... The story behind the articleThe article's inspiration arose when Albert Muñiz, then an undergraduate student, noted that Apple computer (Mac) users seemed to define themselves in opposition to personal computer (PC) brand users. Moreover, Mac users reported that they felt a bond with other Mac users, even if they did not know them personally. When a Mac user had a problem or lost a file at 1 a.m., other Mac users would step in to help. Muñiz also drove a beat-up Saab in graduate school and was surprised that strangers would stop him to talk about their Saabs. Relating these two keen observations about brands and communal behavior sparked the core idea for Muñiz's dissertation, which was supervised by Thomas O'Guinn at the University of Illinois at Urbana-Champaign. Both authors had sociological training, which inspired them to view these phenomena through the sociological lens of the historied ""community"" construct.The authors immersed themselves in interviews of Mac, Saab, and Ford Bronco brand communities and in observations of their online brand communications. These sources of data revealed that members experienced a sense of ""we-ness"" and exhibited oppositional brand loyalty. Members had developed mechanisms to identify ""authentic users"" from ""posers"" who failed to understand the ostensibly true meaning of the brand. They also had shared rituals, traditions, and ""origin stories"" for their brands. They felt a moral responsibility to the brand and the brand community, helping legitimate users with decisions about where to buy, how to better use the brand, and how to find technical information about it. Initial receptionAt the prepublication stage, the authors initially found themselves stifled by the kinds of disciplinary boundaries and limited perspectives outlined in Figure 1. Colleagues at their institution worried that the paper's descriptive approach and that the focus on the novel idea of a ""brand community"" was too different from the typical job market paper and would hinder Muñiz's job prospects. Indeed, Muñiz did not receive job offers when he first went on the market. The authors reported that some qualitative researchers viewed their work as overly applied and commercial in its intent. Muñiz and O'Guinn conveyed that these scholars even questioned whether they were ""selling out"" by not casting a critical eye on the conduct of contemporary marketing. Ironically, while ""Brand Community"" highlighted how brands could unite people, Muñiz and O'Guinn's research seemed to alienate some scholars who had highlighted the corrosive effects of commerce on human interactions. Notably, Muñiz and O'Guinn pointed out the relevance of ""community,"" one of the oldest constructs in sociology, to these emerging phenomena. Yet some critics did not view this description of the phenomena and mapping those phenomena onto the community construct as a theoretical contribution. Although some consumer researchers did not initially support their ideas, the authors were heartened by the enthusiasm they later met at the 1996 International Choice Symposium, where Russ Winer (then-editor of Journal of Marketing Research [JMR]) called attention to it. The research also generated positive responses from colleagues at the University of California at Berkeley, Duke University, and the University of Chicago. Broader impactPractitioners recognized the paper's importance long before it was published. Procter & Gamble chief executive officer Durk Jager validated the work at the 1997 AMA-Sheth Foundation Doctoral Consortium, describing the brand communities forming around Tide and other Procter & Gamble brands. The publication of the article in Journal of Consumer Research (JCR) in 2001 generated extensive press coverage; both authors gave dozens of interviews. Wired writer Leander Kahney picked up on the work and subsequently wrote four books about the cult of Mac users. The automotive editor from the Dallas Morning News wrote a column about the Saab brand community. Attention was sustained over the ensuing years. Rob Walker, author of New York Times Magazine's former ""Consumed"" column, wrote about [47] subsequent work on the Apple Newton personal digital assistant. The community engagement group at Mini Cooper read their work. The article was covered by American Airlines Magazine and by radio host Paul Harvey. The idea took hold that firms could engage meaningfully with the communities that formed around their brands and that these brand communities could become important marketing assets. In the early age of social media, consulting firms popped up using Muñiz and O'Guinn's (2001) ideas and terminology, offering to help companies create and manage their own online brand communities. Overall, this timely, artful, and insightful article and its focal constructs broadened the study of branding, community, and consumption by marketing, sociological, and consumer researchers. It continues to inspire a range of new research and commercial projects around the world to this day. Links to the frameworkMuñiz and O'Guinn's (2001) article has had tremendous academic impact. It is highly cited and is the recipient of JCR's Best Paper and Long-Term Contribution awards. It exemplifies many of our framework's themes through its descriptive investigative goal, phenomenon–construct mapping, contributions to industry, emphasis on large groups and influencers, view of the consumer role as cocreator, use of qualitative and archival methods, reciprocal and system-dependent decision context, and postpublication dissemination activities. Case 2: Kozinets et al. (2010), ""Networked Narratives: Understanding Word-of-Mouth Marketing... The story behind the articleRobert Kozinets describes his early career as marked by several presentations to empty conference rooms. Although he had a challenging time getting his first netnography article published, he stumbled into the area of online consumers and their connections by observing the real-world behavior of Star Trek fans online. Expanding from fans to general consumers, he authored an article for The Financial Times in 1998, noting that ""online communities are growing in power"" (p. 291). From this perspective, he developed a study of online ""tribes,"" in which he observed that certain powerful and communicative members of social media groups ""will become the important influencers who will be in high demand by forward-thinking marketers"" ([28], p. 260).Kozinets subsequently gave numerous talks about social media marketing, including one at the Marketing Science Institute that resulted in several consulting projects with Fortune 500 companies. Through his Brandthroposophy blog, Kozinets piqued the interests of Toronto entrepreneur Patrick Thoburn and his word-of-mouth marketing firm, Matchstick. The two decided to engage in a collaborative project whereby Matchstick would share data while Kozinets and his research team would analyze it. Kozinets's interdisciplinary team included Kristine de Valck, an early pioneer in using netnography; Sarah Wilner, a doctoral student working with Kozinets; and Andrea Wojnicki, a social media scholar at University of Toronto.In 2006, Matchstick was planning an influencer marketing campaign with telecom firm Nokia to seed influential bloggers with free mobile phones equipped with a state-of-the-art camera. Matchstick shared the names of the 90 online campaign influencers, as well as detailed demographic and sociographic data about them and their audiences.The authors collected and read every blog entry from this group for three months prior to the campaign's launch, at launch, and then for three months after launch. They also included samples from the individual influencers, the smaller groups that followed and responded to them, and the larger audience groups that connected to them online. The results showed that social media marketing messages were complex cultural affairs. Overall, the messaging took place in an online environment where influencers had to navigate a sort of ""double-agent status."" They were both a trusted community leader as well as a paid commercial shill. Influencers created various and continuing narratives for their audiences. They also adapted their messaging to the social media platform and to the norms of desired and actual audiences. The narrative, the platform, and the audience norms combined with the goals and tactics of the marketing promotion campaign to produce four types of influencer messaging strategy: endorsing, evaluating, explaining, and embracing. The findings were well received at the 2007 Association for Consumer Research conference, and the authors found a supportive review team at the Journal of Marketing. Broader impactAfter the article's publication in 2010, the authors continued presenting their work at numerous forums and to a range of industry groups around the world, including in a special event dedicated to highlighting the research for its customers organized by Matchstick. Their research also formed the basis of a managerially focused GfK Marketing Intelligence Review special issue that focused on customer brand engagement in a world of social media. Numerous blogs covered the research, as did as popular magazines such as Psychology Today. Impact on academia, the influencer industry, and the media is evident. The article illustrates the value of a holistic, contextualized look at large and complex marketplace phenomena in their early stages. Mapping the systemic workings of a new phenomenon often requires the creation of new constructs and creative overlay of a network of constructs from existing literature. Links to the frameworkLike [46], [29] has had a major impact, as assessed by traditional academic criteria, and broader impact than is typical of most consumer research publications. Contributing to its impact are its descriptive investigative goal, its emphasis on phenomenon–construct mapping, its view of the consumer role as an influencer, and its involvement with and consideration of stakeholder groups, including the social media industry, marketers, and consumers. It also differs from traditional research by its focus on large groups and the emerging trend of influencers engaged in dynamic social media interactions. Like Muñiz and O'Guinn's work, the article was inspired by the real-world behavior of consumers and relied on the capture and analysis of qualitative social media data. Unlike Muñiz and O'Guinn, however, Kozinets et al. relied exclusively on social media data. The authors also implemented the rigorous methodological procedures of netnography. The article's pre- and postpublication process contributed substantially to its influence. Case 3: Johnson and Goldstein (2003), ""Defaults Save Lives,"" Published in Science The story behind the articleIn 2000, Eric Johnson, cofounder of Columbia University's interdisciplinary Center for Decision Sciences, was working with the center's postdoctoral researcher, Daniel Goldstein. Whereas Johnson had been working on the role of opt-in and opt-out defaults in ""permission"" marketing ([25]), events in his personal life caused him to turn his attention to decision making about health care and the role of defaults in health policy. [24] studied changes in European policies around organ donation, where some countries required citizens to opt in to become organ donors in the event of imminent death, whereas others required citizens to opt out of being donors. These defaults led to dramatic differences across countries in organ donation rates.The significance of the topic to consumers, policy makers, and societal stakeholders was evident in the article's opening sentences, which emphasize how many people die each year waiting for a suitable organ donation. What caused the work to become a classic was the authors' use of archival data about organ donation consent across nations, and their effective graphic display of their results in a single dramatic bar chart (see Figure 4). This chart revealed that Germany and Austria, two geographically proximate countries with similar languages and cultures, had drastically different organ donation consent rates. Specifically, Germany and other ""opt-in"" countries had correspondingly low organ donation consent rates, while Austria and other ""opt-out"" countries had equivalently and high organ donation consent rates.Graph: Figure 4. Effective organ donation consent rates of different countries.Notes: From ""Do Defaults Save Lives?"" by Eric J. Johnson and Daniel Goldstein, Science, 302 (5649), 2001. Reprinted with permission from AAAS. Broader impactThe article's findings and its pre- and postpublication activity have had tremendous impact not only on academics inside and outside of marketing but more broadly on policy makers, society, and consumers. Furthermore, the substantive importance of the topic (life and death), the broad national comparisons, and the nontechnical writing style helped the article's ideas diffuse not only to a wide range of scholars and research projects but also to books aimed at broader audiences. [ 2] featured the paper prominently in his best-selling book, Predictably Irrational, as did [64] in their best seller, Nudge: Improving Decisions About Health, Wealth, and Happiness, a book that provided a blueprint for evidence-informed policy makers. Most telling, several European countries subsequently changed their organ donation policies to opt-out defaults. Links to the framework[24] eschewed traditional marketing publication outlets in part because the paper did not adhere to consumer research's implicit boundaries (i.e., those on the left-hand side of Figure 1). Nonetheless, it has become highly cited in marketing and in other academic fields. Though the article did include an experiment, it is best known for its descriptive evidence. Like the other articles we profile, it used ""construct-to-phenomenon"" mapping. It started with phenomena in the real world and offered a straightforward view that made sense of previously unnoticed patterns of data. Moreover, the authors' use of archival data and countries as the unit of analysis ran counter to the norms of traditional decision-making research, which usually emphasized experiments and individuals. Case 4: Pechmann and Shih (1999), ""Smoking Scenes in Movies and Antismoking Advertisements Be... The story behind the articleIn the mid-1990s, adolescent smoking was on the rise, and tobacco marketing faced heightened scrutiny. Cornelia Pechmann observed that there was no research linking the smoking behavior of characters in movies to adolescent smoking. Intrigued, she obtained a grant from the California Tobacco-Related Disease Research Program to conduct experiments on this topic and recruited doctoral student Chuan-Fong Shih to join the project.A professional film editor created smoking and nonsmoking versions of actual movies for inclusion in the study. The authors set up two small theaters at local schools to simulate a field experiment while allowing for random assignment of adolescents to conditions. Though theory testing was unimportant to the granting agency, the authors believed that linking the substantive phenomenon to theoretical constructs would provide valuable insight into why adolescents responded to smoking in movies and, therefore, what could be done about it. Their findings supported the idea that smoking elicits positive arousal, and its appearance in movies enhances product liking. Critically, the authors also found that showing a 30-second antismoking spot that depicted smoking as tainted (the opposite of the forbidden fruit) before the movie nullified positive reactions to the smoking. Initial receptionPechmann presented the initial findings at conferences where antitobacco influencers were present. She was invited to submit the research to the Journal of the American Medical Association but declined the invitation because JAMA wanted less theory, and theory seemed integral to the work. The article was subsequently published in the Journal of Marketing, with just two experiments and no mediation testing, but with extensive internal and external validity checks of interest to both academics and practitioners. Broader impactPechmann disseminated the research findings broadly. Beyond working with her school to issue a press release, she presented the research at schools of public health and medicine, journalism conventions, regional and state departments of health, and at the U.S. Centers for Disease Control and Prevention. The research was the subject of California State Senate Judiciary Committee hearings, U.S. Congressional hearings, and meetings of the U.S. Association of Theater Owners and the Motion Picture Association of America.Like [24], the substantive topic had life and death implications, and its field experiment, theoretical rigor, and intervention orientation made the paper relevant to a range of other researchers, public agencies responsible for health policies, and the entertainment industry. Likely due to Pechmann's presence and advocacy role in the state of California, as well as the state's close ties to entertainment, the impact of the research was especially strong in California. Public funds were used to create antismoking ads to be shown in movie theaters. The California Department of Health Services, working with the governor's office, negotiated an agreement with major movie studios to place antismoking ads on DVDs of movies that depicted smoking. To this day, a watchdog group monitors smoking in movies and tries to pressure movie studios to reduce it.The research also led to a new stream of work in public health and medicine, in which high correlations were reported between adolescents' exposure to smoking in movies and their smoking initiation (e.g., [14]). Pechmann assisted the White House Office of National Drug Control Policy for years, helping oversee its national youth antidrug media campaign. She continues to work on antismoking interventions funded by major National Institutes of Health grants. Links to the frameworkPechmann and Shih's (1999) article exemplifies many points in our framework. This research was not inspired by journal articles but by observations that characters in movies are sometimes smoking. Pechmann and Shih wondered if these depictions could affect adolescent smoking behavior. The issue was clearly of concern to government granting agencies, consumers, policy makers, and society. Its investigative goal was evaluative: the authors were interested in learning if these images in movies harmed adolescent consumers by encouraging smoking. The article emphasized an understudied respondent group—adolescents—and allowed for a consumer role as a skeptic. Further links to our framework include the article's use of a field study method; extensive pre- and postpublication dissemination activities; and its broad impact on consumers, policy makers, society, and nonmarketing academics. Case 5: Fernandes, Lynch, and Netemeyer (2014), ""Financial Literacy, Financial Education, and... The story behind the articlePolicy makers have embraced financial education as an antidote to the increasing complexity of consumers' financial decisions. Moreover, governments, nonprofits, employers, and consumer advocacy groups spend billions annually on financial education. In 2010, John Lynch attended a small, invitation-only event for leading experts on financial literacy and education sponsored by the National Endowment for Financial Education (NEFE). Multiple teams of academics and practitioners presented what had been learned over the past quarter-century about different facets of financial literacy and education. The chief executive officer of the Financial Industry Regulatory Authority presented the first team's conclusions. Speaking about the movement to mandate high school financial education courses, he said (paraphrasing), ""Given the mixed evidence on the effects of financial education and given cost–benefit considerations, maybe now is not the time to continue to press for state mandates."" The audience of experts gasped and vociferously disagreed with the findings. Lynch said nothing but suspected that the experts were conflating experimental and quasi-experimental studies of financial education interventions with correlational studies that measured financial literacy to predict financial behavior.His curiosity led to a project funded by NEFE, where Lynch and collaborators (Daniel Fernandes and Richard Netemeyer) meta-analyzed 201 studies to determine whether measured financial literacy or manipulated financial education correlated with financial behavior. In the 90 experimental and quasi-experimental studies, financial education interventions explained, on average,.1% of the variance in the financial behavior variables. Because of the large sample size, the effect was statistically significant but miniscule in magnitude. A metaregression revealed that the critical factor was an interaction between financial education contact hours and delay. When measured shortly after the educational intervention, the size of the effect of the intervention on financial behavior increased sharply with more contact hours. But within two years, the effects did not differ from zero (see Figure 5). The authors argued that to help consumers make better decisions, financial education should be ""just in time"" and focused on individual behaviors.Graph: Figure 5. Partial correlation of financial education interventions with financial behavior as a function of number of hours of intervention and number of months since intervention.Notes: Republished with permission of INFORMS, from ""Financial Literacy, Financial Education, and Downstream Financial Behaviors,"" by Daniel Fernandes, John G. Lynch Jr., and Richard G. Netemeyer, Management Science, 60 ( 8), 2014; permission conveyed through Copyright Clearance Center, Inc. Initial receptionThe authors reported their findings to the NEFE and the U.S. Consumer Financial Protection Bureau (CFPB). In May 2012, Fernandes and Netemeyer presented the work at the Boulder Summer Conference, attended by several CFPB staff members. During the talk, Richard Thaler whispered a request to forward the paper. During the question and answer session, Thaler said, (paraphrasing), ""I hope people from CFPB get this paper and read it before they spend another dollar on financial education."" A CFPB researcher responded (paraphrasing), ""We've read the paper carefully, and everything we've done since on financial education is 'just-in-time.'""Later that summer, Lynch and Thaler debated a leading academic proponent of financial education at the President's Advisory Council on Financial Capability. Lynch also talked to the larger group of CFPB researchers and had a meeting with then CFPB Director Richard Cordray. The NEFE also arranged mini conferences that served to both disseminate the findings and force the authors to address practitioner objections and counterarguments. The NEFE created a nontechnical practitioner summary of the paper and circulated it to its broad network. Thaler wrote a New York Times op-ed about the article in 2013 and discussed key findings with the ""Nudge Unit"" of financial regulators in the United Kingdom.The authors faced many challenges when disseminating the findings. The strong opposition from advocates of financial education forced the authors to address the language and assumptions of the practitioner community. Likely because of these challenges, the article was difficult to publish. It was rejected at Science based on the review of a proponent of financial education, so the authors submitted it to Management Science, where it was accepted. Broader impactThe article received extensive coverage in media outlets worldwide postpublication, leading to invitations to speak at various industry and practitioner conferences. Presumably because of the article, Lynch was appointed to the CFPB's Academic Research Council, the first scholar from outside the fields of economics and law to be so appointed. The findings continue to influence policy concerning financial education. Links to the frameworkThe case of [16] echoes themes from the prior cases. The paper was inspired by interactions with experts in the world of practice, similar to the genesis of [29]. The goals were descriptive and evaluative. The methods were archival, involving a meta-analysis. The authors followed a nonstandard route in conceiving the project and disseminating the findings before and after publication. The article has had interdisciplinary impact, as measured by traditional citation measures, and significant influence on policy makers and regulators, nonprofit consumer advocacy groups, the media, and industry practitioners. Breaking Boundaries and Broadened Impact: Lessons Learned from the Five Cases CommonalitiesThe five cases described previously share certain characteristics. Each author team took great lengths to work directly with stakeholder groups when developing their investigative goal, identifying phenomenon-to-construct mappings, considering the decision context, selecting appropriate methods, and disseminating their ideas and findings. The authors' curiosity about real-world phenomena, rather than the constructs and theories in the marketing, inspired the papers. Moreover, the phenomena of interest were prevalent, important, controversial, and also underresearched. [46] and [29] advanced our thinking by considering new metaphorical roles of consumers (as, e.g., brand enthusiasts, influencers, collaborators, skeptics). The case studies investigated outcomes in a descriptive and evaluative fashion and contributed to theory by mapping phenomena to a conceptual structure, although they ended up creating theory as well. Several articles included the study of individuals as members of large groups such as communities ([29]; [46]), populations ([24]), or understudied sociodemographic segments (adolescents; [51]). All used methods beyond lab experiments, and several examined dynamic decision contexts, such as how communities, social interactions, or financial literacies are shaped by and evolve over time. Beyond their specific links to our Figures 1–3, several other lessons can be learned from these case studies. Understanding stakeholders' concernsInteractions with knowledgeable practitioners, called ""substantive system experts"" by [41], p. 120), inspired [29] and [16]. This interfacing role is particularly important when it comes to nonacademic stakeholders, and it illustrates the importance of academic–practitioner forums and conferences in pre- and postpublication activities. If one wishes to have sustained influence outside the academic domain, one must become involved and have a seat at the table. Had he not been active in the public space of social media and responding to journalists, Kozinets and substantive expert Thoburn would never have met. The contact with Thoburn's company Matchstick provided early access to the social media marketing campaigns studied in Kozinets et al. For Pechmann and Shi (1999) and Fernandes, Lynch, and Netemeyer, direct interactions with funding agencies facilitated financial contributions and helped ensure that the findings could be used in policy decisions. Each of the five papers addressed topics that were interesting not only to a broad range of academics inside and outside of marketing, but also to other marketplace stakeholders, including practitioners and laypeople. Persistence in the face of resistance or controversyDoing boundary-breaking research comes with risk, and most of the author teams faced initial resistance from other academics, journals, or funding agencies. Although colleagues, reviewers, and editors at top journals frequently devalue research that seems to be practitioner-oriented and/or discourage the pursuit of grant funding, the authors persevered and believed in the significance of their work.Both the [51] and the Fernandes, Lynch, and Netemeyer (2104) articles illustrate the challenges of working in a public policy arena where powerful and strongly motivated opponents, including the media, may be unhappy with the conclusions. While the public health and medical communities were supportive of Pechmann and Shih's work, there was pushback from broader audiences including call-in guests on radio talk shows, students at journalism schools, theater owners, and member of Congress. Many argued that putting antismoking ""propaganda"" in movie theaters would be overreaching. But Pechmann won over many stakeholders, including theater owners, by arguing that if a 90-minute movie promoting smoking was targeted at youth, it was only fair to provide 30 seconds of ad time to present the opposing viewpoint. If one desires to do boundary spanning work and have a broader impact, one might need to face naysayers who try to suppress publication, press coverage, or dissemination of the results. When many people care about a controversial issue, the road to publication could be longer, but the ultimate impact will be greater than if one studies a ""safer"" topic. Finding championsSeveral author teams had champions who supported and legitimized the work. [46], [29], and [51] found journal editors who were inspired by and sympathetic to their work. Muñiz and O'Guinn and Kozinets et al. also had high-profile marketing practitioners who legitimized their work or partnered with them in developing it. The support of granting agencies aided Pechmann and Shih (1999) and [16], and the latter team was further advantaged by Thaler's support in disseminating their findings. Thaler and Ariely also facilitated the dissemination of Johnson and Goldstein's work. Authors of all five case studies were helped by journalists who featured their findings in high-profile media. Not a one-shot dealSeveral of the articles were not one-shot deals; rather, they were part of a program of research. Kozinets et al.'s (2010) paper evolved from his prior work on social media topics, stretching back to his dissertation research. Pechmann has had a long-term interest in smoking behavior, as has Johnson with defaults. The same is true for Lynch in financial decision making. Because the phenomena of interest are complex, evolving, and influenced by myriad contextual factors, most author teams had prior grounding in the phenomenon of interest. They were also willing to expand their intellectual horizons by immersing themselves in the real-world behavior of consumers, marketers, policy makers, and other stakeholders. Moreover, the works of these authors launched new ideas and new collaborators. Clear, accessible communicationEach author team also related the lengths they took to write their papers simply and clearly and to display their data in a form that made it easy to follow their conclusions. Simple and powerful ideas and straightforward methods and writing made the work appealing to the general public. All author teams reported getting a significant boost by the extensive press coverage their work received. However, this did not happen automatically. Rather, the authors devoted significant effort to use the language of nonacademic stakeholders to communicate their contributions in their dissemination efforts. Nontechnical writing helped make the papers and their conclusions accessible to nontechnical audiences. When we interviewed O'Guinn for this article, he compared the interesting and accessible contents of the New York Times with what one sees if one opens up one of our top journals. Many articles in those journals use obtuse language and highly abstract conceptions. Other Strategies for Boundary-Breaking Consumer ResearchExecuting boundary-breaking consumer research can be difficult. Hence, ambitious scholars will also need the support of a variety of other gatekeepers to help publish that research, recognize its impact beyond traditional academic metrics, and use it when making hiring and promotion decisions. Indeed, we believe that institutional change with respect to how boundary-breaking research is fostered, communicated, and evaluated will be necessary.Table 1 provides tactics that scholars and gatekeepers can use to overcome and help eliminate the obstacles that stand in the way of boundary-breaking consumer research. For aspiring scholars, this table provides strategies that individual researchers can take to facilitate conducting such research. These strategies relate to such topics as how best to select and train doctoral students, how to select coauthors and manage joint work, how to navigate issues related to publishing this kind of work in academic journals, how to best manage career challenges, and how to facilitate interactions with marketplace stakeholders.Table 1 also includes actions that we encourage journal and tenure and promotion gatekeepers to adopt so as to encourage and reward such work. Reviewers often apply the same review standards to all types of submissions, but boundary-breaking work often requires the use of different review standards ([35]). Editors in turn play an important role in communicating the types of research they want to publish and managing the review process for these distinctive papers. Finally, tenure and promotion letter writers and review committees can all take actions to reward scholars who undertake this type of research.GraphTable 1. Activities That Can Foster Boundary-Breaking, Marketing-Relevant Consumer Research.   A Few Last CaveatsWe end this section with several important points of clarification. First, whereas the five case studies described herein are exemplary in illustrating boundary-breaking research, we do not mean to imply that they are the only examples of boundary-breaking, marketing-relevant consumer research. To that end, Table 2 provides notable examples of published articles that have had broad impact by breaking the implicit boundaries identified on the left-hand side of Figures 1–3 and adopting many of the strategies on the right-hand side of same figures. While these articles have had broad impact, like the five cases we highlighted, many have also had impact using traditional impact criteria (e.g., citation counts, academic awards).Second, we do not wish to imply the more boundaries consumers researchers break, the better. Rather, we believe that broad impact can often be facilitated by thoughtfully and meaningfully combining traditional approaches in some domains with boundary-breaking approaches in other domains. To illustrate, research that maps interesting phenomena to an existing or a new construct can have significant and broad impact by adding structure to phenomena that are presently ill-structured, even if no other boundaries are broken. For example, research that develops a conceptual understanding of how consumers would behave in markets with guaranteed universal basic income might only break a single boundary by using phenomenon–construct mapping. Yet such research might explain and predict outcomes under guaranteed universal basic income, even if explanation and prediction are the default boundaries.Third, we do not wish to imply that research will have a broad impact by merely breaking a boundary, nor do we wish to imply that breaking boundaries is necessary for having broad impact. Indeed, not all boundary-breaking consumer research will have broad impact, and not all research that has impact will break boundaries. For example, research that merely describes a consumption-related phenomenon that is of little interest to academics and other stakeholders will not have broad impact. To have broad impact, the following ""fault lines"" should be addressed. First, boundary-breaking research should shift stakeholders' beliefs and identify something new, different, or important that stakeholders did not know before. It should also be marketing-relevant; that is, it should address a substantive phenomenon and the investigative outcomes that relevant stakeholders care about. It should capture essential features of the phenomenon, mapping them to theoretical constructs of interest. The population, unit of analysis should be appropriate for the phenomenon and the types of decisions that interest stakeholders. Methodologies should also be chosen such that they help gain insight into and are appropriate for the substantive phenomenon at hand.GraphTable 2. Examples of Marketing-Relevant Consumer Research That Break Implicit Boundaries.  1 Notes: JM = Journal of Marketing; JCR = Journal of Consumer Research; JPPM = Journal of Public Policy & Marketing; JMR = Journal of Marketing Research; IJRM = International Journal of Research in Marketing; JCP = Journal of Consumer Psychology. Why Boundary-Breaking, Marketing-Relevant Consumer Research Helps Researchers and the FieldAs evidenced by these cases, breaking our current research defaults to create a broader impact is clearly challenging. It can require significant time and effort and often involves taking on risk and dealing with controversy. Thus, consumer research scholars, particularly more junior ones, may wonder whether it is simpler to work within the implicit bounds of the field. Is doing boundary-breaking work worth the additional risk and effort? We believe it is for the reasons outlined next. Benefits to Researchers Motivational effectsEngaging in boundary-breaking consumer research can be motivating and exciting. Identifying unexplored real-world phenomena and considering how they relate to the concerns of stakeholders is curiosity-evoking. Interacting with stakeholders is mentally stimulating, particularly because they bring to the table issues we have not considered and help us ground our theories in reality. Considering how unexplored marketplace phenomena can be mapped onto extant and novel constructs is also intellectually engaging. Using different methodological approaches, whether alone or with colleagues, develops one's skill set and expands on the types of questions one can address in the future. These motivational effects are especially helpful in maintaining enthusiasm throughout the review and publication process. Reputational effectsEngaging in boundary-breaking research can be reputation-enhancing. As evidenced by Journal of Marketing's ""Challenging the Boundaries of Marketing"" series and recent editorials (e.g., [13], [23]), the field is ready for such work, which should improve publication prospects. Moreover, such research builds one's reputation as an expert in a substantive area, which is often an important consideration in tenure and promotion decisions. Interacting with nonacademic audiences like practitioners and policy makers enhances one's visibility among these audiences, offers the potential for more frequent interactions, and furthers future publication prospects.Boundary-breaking research that aims for broad impact can also help one's reputation in more traditional ways. Research inspired by real-world phenomena and descriptive research about the world of stakeholders offers the potential to identify novel research domains (e.g., brand communities), constructs (e.g., customer-based brand equity), theories (e.g., time-inconsistent preferences), metaphors for consumers (e.g., consumers as experience seekers), and metaphors for consumption (e.g., consumption as liquid). These novel ideas are the lifeblood of our discipline's growth, and their publication can increase both citations and the potential for long-term contribution awards. Benefits to the Discipline Identifying new questionsBoundary-breaking research offers the fields of consumer research and marketing the opportunity to address pressing problems of interest to a broad set of stakeholders. To illustrate this point, Table 3 provides examples of the types of novel and important questions that could be addressed by breaking the boundaries we have identified in this work. Moreover, the most interesting and important questions, and research that best answers them, may be greatest when scholars actually engage with marketplace stakeholders and academics in other fields and assess which boundary-breaking activities are most important to the question at hand.GraphTable 3. Illustrative Marketing-Relevant Consumer Research Questions Pertinent to Breaking One or More Boundaries.   Reducing fragmentationEngaging in boundary-breaking, marketing-relevant consumer research reduces discipline-based fragmentation based on methods, substantive focus, and base discipline orientation. While fragmentation is natural with growing fields, it tends to isolate as opposed to unite members of the field ([36]; [55]). Research that has broad impact joins researchers by its focus on answering important questions faced by different stakeholders using different lenses, as opposed to a focus on providing base discipline contributions. Moreover, breaking the various boundaries that we describe affords opportunities for researchers with different skill sets (e.g., Consumer Culture Theory researchers; psychologically based researchers, strategy researchers, and modelers) to join forces through collaboration. For example, [11] brought together researchers from different parts of the discipline to offer a consumer-based theory of firm pioneering advantages. These kinds of collaborations create a ""big tent"" that unites consumer researchers with those from other parts of the discipline. Enhancing credibilityThe types of boundary-breaking consumer research that we describe here can reduce the perception and criticisms of academic consumer research as being too incremental ([30]; [63]) and as emphasizing narrowly construed topics that fail to tackle the challenges that stakeholders face. Moreover, some have argued that in some consumer research, consumption is merely a context for the study of human behavior ([10]; [39]). By studying consumers in substantive contexts involving the marketplace, we avoid this criticism.Over 25 years ago, [65], p. 489) advocated for ""wider horizons, a larger audience, a different talent mix, more emphasis on discovery, more attention to consumers, and more single-minded dedication to meaningful results."" These criticisms, which have not yet been fully adopted by academic consumer researchers, partially explain why nonacademic consultants have been more successful in capturing the attention and consulting dollars of marketing organizations than academics have based on their research.Although academics are known for their research rigor, the boundary-breaking research we describe also enhances credibility by balancing rigor with relevance. Indeed, there is a sense that relevance has been subjugated to rigor in academic research ([31]). High-impact, marketing-relevant consumer research requires both rigor and relevance. We believe that by breaking boundaries, consumer behavior researchers can offer significant relevance to not only consumers but also the myriad stakeholder groups pertinent to marketing.More generally, credibility is enhanced by addressing marketing's ""image problem"" ([62], p. 10). Many consumers view marketing as deceptive, manipulative, and generally bad for their own welfare. We can help the credibility of the field when we conduct consumer research of importance to consumers themselves. Such is true with evaluative research that helps identify when marketing efforts and consumer actions are beneficial or harmful. We help both ourselves and society by producing research that studies issues of broad societal import such as financial decision making, data privacy, and health and economic well-being. Improving connectionsConsumer research that breaks boundaries to have broader impact will improve the connections we make with ""substantive system experts"" in practice and in other academic disciplines. Doing so dramatically increases the chances that consumer research will influence large and diverse audiences, including scholars in other fields, educators, managers, policy makers, and consumers themselves. We hope to inspire consumer researchers to consider making their research of broader interest to marketers, regulators, and scholars in adjacent fields by studying substantive marketplace phenomena. ConclusionsConsumer research has yielded significant, novel, and important insights that have enhanced our understanding of how and why consumers behave as they do. Scholars in our field are skilled in conceptual thinking, research design, and research execution. Yet, despite our current contributions to knowledge, we have the potential to offer even broader impact by advancing knowledge on topics that stakeholders—including marketing academics—care about. We argue that such impact can be advanced by breaking free of one or more of the implicit boundaries that currently guide why, what, and how we study consumers. Whereas skeptics might argue that such work is too time consuming and too risky for nontenured individuals, the cases we mentioned include the work of a number of untenured authors. Importantly, not all of this work requires a marathon time investment. Moreover, the time investments and inherent risks may be well offset by impact, measured not just by traditional metrics but also by the influence of one's research on entities outside of our academic research community.Our hope is that some readers of this article will rise to the challenge of engaging in boundary-breaking, marketing-relevant consumer research. We believe that it is through such efforts that our thought leadership position and contributions to the world can be enhanced.  "
20,"Creating Effective Online Customer Experiences Creating effective online customer experiences through well-designed product web pages is critical to success in online retailing. How such web pages should look specifically, however, remains unclear. Previous work has only addressed a few online design elements in isolation, without accounting for the potential need to adjust experiences to reflect the characteristics of the products or brands being sold. Across 16 experiments, this research investigates how 13 unique design elements shape four dimensions of the online customer experience (informativeness, entertainment, social presence, and sensory appeal) and thus influence purchase. Product (search vs. experience) and brand (trustworthiness) characteristics exacerbate or mitigate the uncertainty inherent in online shopping, such that they moderate the influence of each experience dimension on purchases. A field experiment that manipulates real product pages on Amazon.com affirms these findings. The results thus provide managers with clear strategic guidance on how to build effective web pages.KEYWORDS_SPLITWith more than 350 million products listed on Amazon.com alone ([ 1]), success in the increasingly competitive online domain depends on sellers' ability to orchestrate verbal and visual stimuli (i.e., design elements) on product web pages to effectively convert page visitors into buyers ([63]). Insights into which design elements make for effective product web pages are however still largely based on managers' intuitions or, at best, ad hoc A/B testing. Academic research typically focuses on a single design element or just a few across a limited number of products or brands. It also often neglects the mechanisms through which design elements affect purchase or employs theoretical perspectives (e.g., information processing) that conceptually limit their effects a priori to a single function (e.g., information transmission). Yet each encounter with a product web page—the virtual space that presents a product and illustrates its value to the customer—evokes a multidimensional experience that goes beyond a pure conveyance of factual information ([10]; [42]). The objective of this research is therefore to understand how online design elements shape multidimensional customer experiences to influence purchase and how these experiences should be customized depending on the products or brands sold.The online customer experience at the heart of this research comprises a customer's subjective, multidimensional psychological response to a product's presentation online. We argue that this experience goes beyond cognitive (informativeness) and affective (entertainment) dimensions typically conceptualized in extant research ([54]) and also includes social (social presence; [78]) and sensory (sensory appeal; [36]) dimensions. Furthermore, we identify 13 web page design elements, such as product descriptions, photos, and comparison matrices, that each may help shape the online experience and are ubiquitous in a wide range of industries and web page formats. This multidimensional framework more closely resembles the conceptualization of offline experiences ([10]; [42]) and helps more accurately capture the mechanisms by which design elements affect product purchase.How effectively each experience dimension elicits purchases, however, may vary depending on characteristics of the offered products and brands that exacerbate or alleviate the uncertainty inherent in online shopping ([ 8]; [63]). First, the degree to which consumers can evaluate a product solely on the basis of factual information (search qualities) rather than needing direct physical experience (experience qualities) implies the level of uncertainty associated with assessing that product online ([33]). Second, customers may also be uncertain about the accuracy and truthfulness of sellers' product presentations, yet a brand's trustworthiness may alleviate this uncertainty ([56]). We leverage our multidimensional online framework of the online customer experience to investigate how these two primary sources of uncertainty determine the effects of each experience dimension on purchase ([19]).To ensure the broad scope and generalizability of our research, we collaborate with a specialized online content agency and four Fortune 1000 firms, diverse in their industries, brands, and products (i.e., consumer packaged goods, consumer electronics, industrial electronics, and consumables). In Study 1, we conduct large-scale online experiments that involve 16 products from 11 brands, for which the online content agency created 256 unique ""Amazon look-alike"" product web pages. On these pages, we manipulated 13 design elements according to an orthogonal array design ([73]) and then tested the pages among 10,470 randomly assigned respondents. With the resulting data, we estimate a joint model that isolates the relative influences of each design element on each dimension of the online customer experience, the relative effects of each experience dimension on purchase, and the moderating influences of product type and brand trustworthiness on the effects of the dimensions on purchase. A field experiment in Study 2 tests these effects with real Amazon product pages, on which we used design elements to create specific experiences to observe the effects on sales.We offer three main contributions to the literature. First, data from 16 experiments in Study 1 expand insights into online customer experiences and identify four dimensions—namely, informativeness, entertainment, social presence, and sensory appeal—that act as the underlying mechanisms by which design elements influence purchase ([54]; [61]). Prior online research has mainly focused on informativeness and entertainment; however, we show that the effects of social presence are just as strong as those of informativeness, and sensory appeal offers additional insights. Second, we find that uncertainty about the offered product and its seller's brand influences the effects of the customer experience dimensions on purchase. Using actual product web pages on Amazon.com, a field experiment in Study 2 validates the lab results to show that search products benefit from a more informative experience but experience products benefit from a more social experience. Third, we establish an online customer experience ""design guide,"" with actionable advice for marketers on how to strategically orchestrate design elements to shape effective online experiences in an era of increased web design importance ([81]). Specifically, we depict how to evaluate the design elements that currently constitute their digital inventory, which new elements to invest in and develop, and how to negotiate and assess contracts for premium content with online retailers. Dimensions, Moderators, and Antecedents of the Online Customer ExperienceIn contrast with brick-and-mortar retail, customers assess products online not through physical interaction but through verbal and visual stimuli (design elements) deployed on product web pages. A broad stream of research conceptualizes offline experiences as consisting of multiple, separate, but related dimensions (e.g., cognitive, affective, sensory, social, physical) ([10]; [42]; [64]; [77]). Yet research has treated online experiences far more simplistically ([54]; [71]), often a priori limited to their informativeness (see Table 1).GraphTable 1. Relevant Research on the Effectiveness of Design Elements on Product Web Page Performance.  30022242918809930 Notes: To derive a list of relevant research, we examined articles pertaining to online product marketing published in the last ten years in Journal of Marketing, Journal of Marketing Research, Marketing Science, and Journal of Consumer Research. To be included, the research needed to be empirical in nature and focus on product web page design elements available to manufacturers that sell through a retailer website. We exclude studies of retailer-controlled website design elements (e.g., navigation), email marketing, online advertising, word of mouth, or search.In line with the four basic systems—cognition, affect, relationships, and sensations—commonly studied in psychology and sociology ([ 4]; [59]), we conceptualize the online customer experience as consisting of four dimensions: informativeness (cognitive), entertainment (affective), social presence (social), and sensory appeal (sensory). Consistent with our multidimensional perspective, we do not expect a one-to-one relationship between any specific design element and an experience dimension ([10]).We next introduce and review each dimension of the online customer experience. Then, we explain why the influence of each dimension on consumers' purchase decisions might depend on the uncertainty associated with specific products or brands. Finally, we present the design elements that managers can use to build product web pages to shape customer experiences (see Figure 1).Graph: Figure 1. Designing the online customer experience. Notes: Constructs in italics were experimentally manipulated across 16 products and 11 brands. N = 10,470. Dimensions of the Online Customer ExperienceDefined as ""the extent to which a website provides consumers with resourceful and helpful information"" ([43], p. 51), informativeness is the primary cognitive dimension of the online customer experience. It captures a web page's contribution to helping the consumer make a pending purchase decision, which involves thinking, conscious mental processing, and, typically, problem solving ([25]). Informativeness captures the functional aspect and value of the experience to the customer ([77]) and is generally impersonal, outcome oriented, and objective ([63]). This fact-based dimension pertains to the information that remains after interacting with a web page, which can improve attitudes toward a website ([31]; [34]).Customer interactions with products online can evoke affective responses and might be enjoyed for their own sake, without regard for functional considerations. Entertainment, or the immediate pleasure the experience offers, regardless of its ability to facilitate a specific shopping task ([ 7]), is thus a key dimension of the online customer experience. Entertainment reflects an appreciation for the ""spectacle"" experienced on the web page, involves the fun and play of online shopping, and accords more than just an achievement-oriented purchase opportunity ([13]; [48]). As such, entertainment can trigger arousal in web page visitors ([34]) and reduce cart abandonment in online stores ([40]).To match the benefits of offline experiences, online sellers increasingly work to provide a sense of social presence on their web pages ([78]). Social presence refers to the warmth, sociability, and feeling of human contact that a web page confers ([23]). Extant research shows that the social presence of a website can increase perceived tangibility and feelings of psychological closeness to a product ([17]). It can also increase pleasure, arousal, and flow during online shopping ([78]), as well as purchase intentions ([29]) and loyalty ([15]).Finally, the sensory component of the customer experience includes aspects that stimulate sight, sound, smell, taste, or touch ([25]). [82] suggests that sensory-level processing and retrieval occurs automatically and drives preferences. In an online environment, sensory appeal refers to ""the representational richness of a mediated environment as defined by its formal features"" ([72], p. 81) or the way a web page stimulates the senses. Perception of beauty and aesthetically pleasing stimuli are part of sensory appeal ([64]). Although the online environment limits the scope of sensory experiences, sensations can be evoked through imagery (e.g., pictures, videos) ([20]). Thus, sensory appeal can affect perceptions of product performance ([80]) and purchase intentions ([62]). Uncertainty and the Moderating Role of Product Type and Brand TrustworthinessOnline shopping often comes with uncertainties that do not arise offline and that might affect how certain experience dimensions influence purchase ([19]; [56]). First, online, customers cannot touch and feel the merchandise in which they are interested, which can create uncertainty in product assessment before purchase ([38]). This uncertainty tends to be more severe for experience products, for which the most relevant attributes are discoverable only through direct physical contact, than for search products, whose most relevant attributes are assessable from presented information without physical interaction ([33]; [80]). How consumers attend to and interpret product information differs between search and experience products ([35]). Thus, the most effective type of experience for selling these two types of products might also differ. For example, [80] show that web pages that appeal to the senses may be more beneficial for experience products, whose evaluation requires sensory information.Second, the physical separation between customers and products requires customers to have faith in the accuracy and truthfulness of the product web page. Yet they may experience uncertainty about online sellers' ability and integrity to convey product information, depending on the trustworthiness of the seller brand ([56]). Trust reflects the ""willingness to rely on an exchange partner in whom one has confidence"" ([51], p. 315). A significant stream of research shows the importance of trust online ([76]), in which sellers' trustworthiness determines customers' research and purchase decisions ([24]; [32]). Trust online is also closely connected with web design ([66]; [74]). Several studies suggest that low trustworthiness can be overcome through purposeful web page design ([63]; [79]) or by customizing content to customers' preferences ([75]). Specific experience dimensions might also be instrumental to alleviating low trustworthiness. [ 8] show that entertaining online experiences may compensate for an initial lack of trust in a brand. Social presence may serve a similar purpose ([23]). Extant work suggests that the product- and brand-related uncertainty inherent in online shopping can influence the effects of experience dimensions on purchase. We thus focus our moderation analysis on product type and brand trustworthiness as the respective primary determinants of these two types of uncertainty ([33]; [56]), instead of other product, brand, or service attributes. Design Elements That Create the Online Customer ExperienceThe product web page is at the heart of the online customer experience. It consists of basic design elements, defined as verbal and/or visual stimuli that provide the building materials for any given page. To identify the most important elements, we reviewed ten years of research on website design published in Journal of Marketing, Journal of Marketing Research, Marketing Science, and Journal of Consumer Research, as well as various specialized journals. Our focus was on design elements that relate directly to the product presentation and are typically available to firms selling through retailers such as Amazon; we excluded structural elements, such as navigation, menus, icons, and overall organization, that operate at the website level and are under the control of the host retailer. Although they operate through many aliases, we identified 13 elements that we classify by their form (see Appendix A) into three groups: verbal elements that use text and typographical features, visual elements that use images and pictures, and combinations of both. Table 1 summarizes research on each of these 13 design elements. Verbal elementsVerbal elements involve the written word. In this category, we consider linguistic style, descriptive detail, the number of bulleted features, and return policy information statements. The most basic aspect of textual elements is the way information is presented. The linguistic style in which verbal content is conveyed or the characteristics of the text—including word choice and use of questions, certain pronouns (you, your), and adjectives—can affect product conversions and consumer perceptions of website effectiveness ([44]; [70]). [70] provide preliminary evidence that these effects occur through the impact of linguistic style on social presence. To capture the degree of elaboration of the product descriptions on a web page, we examine descriptive detail. Providing more attribute information generally increases product evaluations and purchase likelihood ([14]; [30]). The number of bulleted features indicates how many product features appear in an abbreviated list at the top of the web page. Though prevalent on many product web pages, to our knowledge, research has not empirically investigated its effects on purchase. Return policy information refers to whether the web page contains information about the terms by which customers may return the product. Visual elementsVisual elements subsume all content presented in photographic or illustrated form and can convey symbolic meaning and pictorial information ([65]). We investigate feature crops, lifestyle photos, photo size, and product videos. Unlike pictures of the product as a whole, feature crops zoom in on a key product feature that would otherwise not be visible. Lifestyle photos connect the product with customers' lives, such as by depicting people using it or living with it in a regular setting. They explicitly capture or imply human interaction with the product ([ 6]). We also investigate photo size.[55] show that larger product images can increase purchase intentions. Finally, a product video can demonstrate the product and its key features. Videos including human voices can serve as cues for human characteristics and influence perceptions of social presence and sensory appeal ([50]; [60]). Combined verbal and visual elementsCustomer star ratings, expert endorsements, comparison matrices, recommendation agents, and content filters all combine verbal and visual qualities. Customer star ratings are aggregations of user-generated product ratings, depicted visually with a series of stars and next to the total number of reviews ([12]). Expert endorsements are also product evaluations, but assembled from distinguished experts in the category, such as product testing firms, and generally include a graphic depiction, such as a seal ([ 5]). Comparison matrices are tables to compare the focal product with other products from the same category on multiple characteristics. Product information is typically presented as pictures of alternatives (columns) and text describing attributes (rows). Recommendation agents combine verbal and visual information to generate a list of alternatives to the focal product ([41]). Comparison matrices and recommendation agents can improve purchase decision quality ([28]; [39]). Content filters, such as ""show more"" buttons, allow customers to dictate what, when, and how much verbal and visual content appears on the web page ([30]; [49]). Of the combined elements, star ratings have received most empirical attention, though studies typically test their effects directly on purchase, without considering underlying mechanisms ([12]; [30]; [44]; [83]). Table 1 shows evidence for the effects of design elements on purchase, while the underlying mechanisms remain mostly unclear. Testing Product Web Page Design, the Online Customer Experience, and PurchaseWe extend research on design elements and the online customer experience with two studies. In Study 1, we aim to ( 1) understand the relative importance of each of the four online customer experience dimensions as key mediators in the relationship between web page design elements and customer purchase, ( 2) determine which of the 13 design elements are most useful in creating each experience dimension, and ( 3) assess how product type and brand trustworthiness influence the effects of the experience dimensions on purchase. In Study 2, we manipulate real Amazon product pages from the insights gleaned from Study 1 to assess the effects on actual sales. Study 1: Design, Dimensions, and Implications of Online Customer ExperiencesWe partnered with four Fortune 1000 firms in multiple industries (i.e., consumer packaged goods, consumer electronics, industrial electronics, and consumables) (Appendix B) and tested our conceptual model with 16 products (4 per firm), representing 11 brands. Together with a specialized online content agency, we designed and created mock Amazon product web pages for each product that varied the 13 design elements on two levels each, according to an orthogonal array design ([73]). On Amazon.com, vendors can select from a range of module templates and then manage the content of each module within the retailer's restrictions. Appendix C shows an example web page.[ 5] Experimental StimuliAppendix A provides a summary of the two manipulated levels for each of the 13 design elements. For verbal elements, we manipulated linguistic style as either a journalistic tone (Level 1) or conversational tone (Level 2). For the journalistic tone, the neutral product descriptions featured few or no adjectives, no self-relevant words (e.g., ""you,"" ""your"") ([11]; [70]), no questions, and no exclamation points. For the conversational tone, the descriptions were more engaging and included adjectives, self-relevant words, words that insinuate instantaneous gratification (e.g., ""fast,"" ""instant,"" ""quickly""), and self-reflective questions (e.g., ""Wouldn't it be great to have high-speed Internet everywhere?"") ([ 2]; [44]). Although linguistic style determines how product descriptions convey information, it does not affect the actual amount of information presented. To manipulate this facet, we used the descriptive detail design element. At Level 1, product descriptions contained approximately one-third the amount of information (i.e., number of attributes discussed) that they contained at Level 2. We manipulated bulleted features as either three (Level 1) or five (Level 2) bullets on the web page; previous research indicates that these numbers are relevant ([68]). Return policy information was the absence (Level 1) or presence (Level 2) of the statement ""Return Policy: Items can be returned within 30 days of receipt"" on the page.For visual elements, we manipulated the feature crop element by either not replacing (Level 1) or replacing (Level 2) one of the product hero shots with a close-up picture of a specific feature of the product. A lifestyle picture, which connects the product with the real world in an actual usage situation, was either not included (Level 1) or included (Level 2) to replace one of the hero shots. At Level 2 of the picture size design element, all pictures were 25% larger than at Level 1. Product video indicated the absence (Level 1) or presence (Level 2) of a video about the product.For combined verbal and visual elements, we manipulated customer star ratings, by either excluding (Level 1) or including (Level 2) the average star rating for the product.[ 6] We manipulated expert endorsement using a quality seal from a fictitious third-party product rating agency, to avoid any potential effects of familiarity with existing agencies, that might differ across respondents. At Level 1, there was no seal, while at Level 2, this seal replaced one of the hero shots. We manipulated the comparison matrix element as the absence (Level 1) or presence (Level 2) of a table that compared the focal product with similar products from the same firm and category on key product characteristics. The recommendation agent featured either the absence (Level 1) or the presence (Level 2) of a section that displayed links to related products, again from the same firm and category. For these two elements, we purposely used products from the same manufacturer, to avoid any influences of additional brands for which consumers might hold distinct views. The content filter element either did not permit (Level 1) or permitted (Level 2) consumers to control the amount of verbal and visual content shown on the page, using ""show more"" buttons to reveal or hide parts of the modules. Experimental DesignTesting the effects of such a large number of elements poses a considerable empirical challenge. A full-factorial design would have required building and analyzing 131,072 experimental cells as web pages (213 combinations of design elements per product × 4 firms × 4 products). With such an approach, we could have investigated all potential interaction effects among design elements, but it would have been infeasible to execute. We therefore adopted a [73] orthogonal array design, which reduced the required number of cells to 256 (16 combinations of design elements per product × 4 products × 4 firms). Thus, we can feasibly investigate the simultaneous, causal direct effects of all 13 design elements. Method Sample and procedureWe recruited 10,470 workers via Amazon Mechanical Turk for our 16 experiments (one per product). Respondents, randomly assigned to one of the 16 experimental cells within each experiment, were presented with the corresponding web page and instructed to explore it for at least 45 seconds. Next, they completed a questionnaire with demographic questions, items for manipulation and realism checks, and preexisting scales to measure purchase intentions and the four experience dimensions (see Appendix D). MeasuresAppendix A contains the results of our manipulation checks, which are all significant (p <.01), indicating successful manipulation of the design elements. In addition, we used two items to assess the realism of our web pages: ""I could imagine an actual web page to look like the one I just saw"" and ""I believe that this web page could exist in reality"" (α =.90) ([18]). Respondents' answers to these items, on a seven-point scale (1 = ""strongly disagree,"" and 7 = ""strongly agree""), indicated that our created web pages established sufficient realism (Mcomposite score = 5.41, SD = 1.29).To assess the accuracy of our measures, we first conducted a confirmatory factor analysis. The results indicate a good fit of our measurement model to the data (χ2(80) = 2441.75, confirmatory fit index [CFI] =.98, Tucker–Lewis index [TLI] =.98, root mean square error of approximation [RMSEA] =.05, standardized root mean residual [SRMR] =.03). Moreover, in support of convergent validity, all standardized factor loadings are greater than.70 and significant at the 1% level. For each construct, the average variance extracted (AVE) exceeds.50, and the composite reliability is greater than.70. Cronbach's alpha values above.70 indicate internal consistency. In support of discriminant validity, all AVEs are greater than the squared correlations of the focal construct with any other construct (see Table 2).GraphTable 2. Descriptive Statistics and Correlations.  40022242918809930 Notes: Means and standard deviations are based on composite scores; CA = Cronbach's alpha; CR = composite reliability. AVE values are in parentheses.To evaluate multicollinearity among the experience dimensions, we first calculated the variance inflation factors for each construct. All values (informativeness = 1.55, entertainment = 2.18, social presence = 2.01, sensory appeal = 2.58) fall below the critical value of 5. Next, we examined the eigenvalues of their correlation matrix. The condition number (κ = 7.15) is well below the critical threshold of 30. Altogether, these results indicate that multicollinearity does not pose a concern. Last, we conducted an exploratory factor analysis, which confirmed that all items loaded onto their intended constructs (see Web Appendix A). For the remaining analysis, we calculated composite scores using the average of all scale items for each construct.To investigate the extent to which product type and brand trustworthiness moderate the effects of the experience dimensions on purchase, we collected additional data.[ 7] To capture a product's search versus experience focus (i.e., its type) unaffected by the web pages on which it appeared in our experiments, we first presented 452 respondents with randomly selected hero shots of the 16 products and then asked them to complete a questionnaire with corresponding search and experience quality measures ([80]). Each respondent rated two products. We then computed the average of the difference between the two items, which captured each product's search and experience qualities over all respondents. We similarly captured brand trustworthiness by presenting 341 respondents with the logo of one of the 11 brands in our sample, along with a list of its associated product categories. Each respondent rated a single brand on six trustworthiness items ([63]), which we then averaged across respondents. Appendix D shows all measurement items. ResultsTo test our conceptual model, we combine the data from our 16 experiments (one for each product) and estimate a joint model using covariance-based structural equation modeling with maximum likelihood estimation. This approach allows us to test the relative importance of each experience dimension as a mediator of the link between design elements and purchase intentions, while controlling for customer heterogeneity in terms of age, gender, income, and education. Mediation testsTo confirm the relevance of each experience dimension as a mediator of the effects of design elements on purchase, we ran a series of nested models and compared their chi-square values with that of our proposed model (Table 3). Model 1 is our proposed model with all four experience dimensions as mediators. Models 2–5 test a set of three-dimension models in which we removed the paths from each experience dimension to purchase intentions, one by one. Models 6–15 test all other possible combinations of experience dimensions. Model 1 achieves good fit (χ2(16) = 437.77, p <.01; CFI =.980; TLI =.880; RMSEA =.050; SRMR =.009) and performs significantly better than any alternative model; each experience dimension partially mediates some design elements. We thus focus on the results of Model 1 with all four experience dimensions in the remainder of our analyses.[ 8]GraphTable 3. Study 1 Results: Model Comparison.  50022242918809930 * p <.05.60022242918809930 ** p <.01.70022242918809930 Notes: ✓ indicates an existing path between the experience dimension and purchase intentions. AIC= Akaike information criterion. The Δ χ2 and Δ AIC refer to differences of a specific model relative to Model 1. Results based on a model without moderating effects. Effects of experience dimensions on purchaseColumns 1–4 in Panel A of Table 4 represent the effects of experience dimensions on purchase intentions. In general, entertainment exhibits the strongest effects (β =.387, p <.01), followed by informativeness (β =.118, p <.01), social presence (β =.118, p <.01), and sensory appeal (β =.060, p <.01).GraphTable 4. Study 1 Results: Effects of Design Elements on Experience Dimensions and Purchase Intentions.  80022242918809930 *p <.05.90022242918809930 **p <.01.100022242918809930 a Controlling for direct effects of design elements and consumer demographics.110022242918809930 b Direct effect of product type (search/experience) on purchase intentions: β =.152** (19.385); direct effect of brand trustworthiness on purchase intentions: β =.044** (5.441).120022242918809930 Notes: Columns denote affected experience dimensions; β represents the standardized coefficient; z-values are in parentheses. Model fit: χ2(d.f.) = 1475.63 (106), CFI =.94, RMSEA =.04, SRMR =.02. Effects of design elements on experience dimensionsPanel B of Table 4 contains the effects of each design element on each experience dimension, while accounting for the effects of all other design elements. Customer star ratings emerge as a strong driver of all experience dimensions (all βs ≥.131, all ps <.01). The same is true for picture size (βs ≥.147, ps <.01). When we control for the impact of all other elements, return policy information and expert endorsement do not contribute significantly to any experience dimension (ps >.05).Column 5 of Table 4 further indicates that eight design elements exert significant effects on the informativeness dimension. The strongest effects stem from including customer star ratings (β =.211, p <.01), more bulleted features (β =.181, p <.01), a comparison matrix (β =.168, p <.01), more descriptive detail (β =.153, p <.01), and larger pictures (β =.152, p <.01). Including a product video (β =.058, p <.01), a recommendation agent (β =.049, p <.05), and a lifestyle picture (β =.047, p <.05) also drives this dimension, though to a lesser extent.Column 6 of Table 4 shows that nine design elements substantially influence entertainment. The most important are picture size (β =.147, p <.01) and customer star ratings (β =.135, p <.01), which exert much stronger effects than a comparison matrix (β =.081, p <.01), more bulleted features (β =.077, p <.01), descriptive detail (β =.064, p <.01), or product video (β =.056, p <.01). Using a conversational linguistic style (β =.052, p <.01) and including a product feature crop (β =.049, p <.05) also drive entertainment.Column 7 of Table 4 shows that ten elements are relevant for social presence. The most important are picture size (β =.171, p <.01), linguistic style (β =.165, p <.01), customer star ratings (β =.162, p <.01), and lifestyle pictures (β =.144, p <.01). Comparably less important are bulleted features and product feature crops (both β =.042, p <.05). The effect strengths of product videos (β =.089, p <.01), descriptive detail (β =.088, p <.01), and a comparison matrix (β =.064, p <.01) lie somewhere in between. Including content filters significantly decreases social presence (β = –.087, p <.01).Ten elements are also relevant for sensory appeal, as Column 8 of Table 4 shows. The most important are picture size (β =.190, p <.01) and product video (β =.184, p <.01). Linguistic style (β =.069, p <.01), lifestyle pictures (β =.062, p <.01), product feature crops (β =.055, p <.01), and recommendation agents (β =.048, p <.05) exert positive but weaker effects. In between are the effects of customer star ratings (β =.131, p <.01), a comparison matrix (β =.104, p <.01), and more descriptive detail and bulleted features (both β =.099, p <.01).[ 9] Moderators of the relationship between experience dimensions and purchase intentionsPanel C of Table 4 reports the moderation results of our joint model. For search (experience) products, the informativeness dimension of the experience becomes more (less) important (β =.019, p <.05), consistent with extant research suggesting that consumers extract only minimal direct information from advertisements for experience goods ([53]) and that information is more pertinent for search than experience goods ([22]). To assess experience goods, product attribute information is less useful, perceived purchase risk is often high ([47]), and consumers turn to alternative signals on the web page ([21]). Accordingly, we find that social presence (β = –.023, p <.05) and sensory appeal (β = –.022, p <.05) are less (more) important for search (experience) products. Heightened social presence and greater sensory appeal can reduce perceived performance uncertainty ([16]; [80]), so they are more important for purchase decisions involving experience products. For search products, consumers instead can gather sufficient factual information from the web page, so social presence and sensory appeal become less vital.In addition, for more (less) trustworthy brands, informativeness is a more (less) important dimension of the online experience (β =.022, p <.05), while entertainment becomes less (more) important (β = –.028, p <.01). This finding aligns well with previous research showing that information and arguments provided by credible sources are more persuasive to consumers ([58]). Thus, the more trustworthy a brand, the more consumers actually engage with the information on its product web pages, and the more they find this information relevant and helpful to their purchase decisions. By contrast, entertainment is more important for brands perceived as less trustworthy. When brand trustworthiness is low and consumers experience more uncertainty ([56]), entertainment has a greater impact on purchase, a finding that aligns with previous research ([ 8]). Discussion: Creating Effective Customer ExperiencesFinding that a product's type and brand trustworthiness affect the impact of each experience dimension on consumers' purchase decisions implies that marketers should use design elements strategically to evoke specific types of experiences for different products and brands. To aid this effort, in Figure 2 we present a design guide that illustrates and summarizes when to rely on which type of experience and how to build it through design elements. Although customer star ratings and picture size are relevant for all experience types, we highlight specific design elements that are particularly strong facilitators of distinct experience dimensions. To this end, we provide percentage differences in the effect sizes of each design element on each experience dimension, relative to its effects on all remaining dimensions.Graph: Figure 2. Design guide for creating effective online customer experience. Notes: Only significant effects (p <.05) are shown; gray bars represent universally effective design elements across all experience dimensions, black bars depict uniquely more effective elements for a specific dimension than for all other dimensions, and white bars indicate the remaining elements.Informative experiences are dominated by outcome-oriented information and are most effective for search products and brands that are generally well-trusted. Bulleted features exert their strongest effects on this experience type (83% stronger than their effects on any other experience dimension). A comparison matrix can also shape this dimension especially well (62% more effective than for any other dimension), as can more descriptive detail (54% more effective) and recommendation agents (nearly equally effective at driving sensory appeal, but 150% more effective than driving any other dimension).Entertaining experiences are pleasurable in their own right, apart from any anticipated performance implications. We find that these experiences are especially important for less trustworthy brands. Although most design elements exert some effect on this dimension, no one design element appears uniquely or more suited to shape it than any other dimension.Social experiences convey a degree of human presence in the encounter. These experiences are especially effective for experience compared with search products. Linguistic style and lifestyle pictures drive this dimension particularly well (respectively, 139% and 134% more effective in shaping it than the other dimensions).Sensory experiences activate consumers' senses and are especially beneficial for experience products. Product videos exert their strongest effects on this dimension (106% stronger than on any other dimension). Product feature crop is another important element to this dimension (29% stronger effects than on the other dimensions). Study 2: Field Experiment to Test the Effect of Online Experience Designs on SalesStudy 1 provides a framework for designing online customer experiences and customizing them to specific product or brand factors. The lab experiments provide strong internal validity across design elements, experience dimensions, and moderators. In Study 2, we also aim to provide a compelling test of external validity. We conduct a field experiment with real products and sales on Amazon.com to test the finding from Study 1 that, for products high in search qualities (search products), an informative experience can increase product sales while a social experience may suppress them. Experimental Design and Research ContextIn this study, we collaborate with one of our partnering firms and manipulate the content on two of its product pages on Amazon.com. Using a difference-in-differences approach, we observe the resulting changes in sales volume compared with a control product page, over a period of two months. To investigate the extent to which search products benefit from a more informative versus a more social experience, we first carefully selected three search products (wireless Internet routers) with similar characteristics and sales trends in the four weeks before the launch of the experimental treatments (prelaunch) from our partner firm's inventory.[10] For the next four weeks (postlaunch), we adapted the web pages of two products as either more informative (Treatment 1) or more social (Treatment 2) and left the third page unchanged (control condition). The difference-in-differences analyses reveal the respective changes in daily sales of the two adapted web pages, compared with the unchanged control page. With this design, we can disentangle the treatment effects of more informative or social page designs from time trends and determine whether changes in sales are attributable to the adjusted page designs or unobserved shifts in consumer preferences.We took several steps to reduce potential confounding effects. First, to ensure homogeneous customer characteristics across the two experimental periods, all product information on the Amazon search results pages, from which consumers enter the actual product web pages (e.g., product name, hero shot, stockkeeping unit [SKU]), remained constant during the experiment. Second, the price of all products remained constant, and no promotion activity occurred during the experiment. Third, because Amazon publishes seller-submitted product content with varying time lags, we excluded the days around the launch of the treatment content from our analyses ([46]). Fourth, consumers do not visit particular product web pages at random, so we account for self-selection effects in the page views of the treatment pages relative to the control page by supplementing our analyses with controls for observable selection variables.The experimental design thus employs two treatment conditions and a control condition. Treatment 1 tests the effectiveness of a more informative experience by increasing the descriptive detail on the page, adding additional bulleted features, and adding a comparison matrix. Treatment 2 tests a more social experience, created through a conversational tone and the addition of lifestyle photos, in line with Study 1. The control product web page remained unchanged. To measure the performance of each web page, our partner firm provided access to Amazon Premium Analytics, from which we obtained daily sales and customer star rating data one month before the launch of the treatment pages (prelaunch) and one month after (postlaunch). Empirical AnalysisIn our difference-in-differences approach, we compare the difference in daily product sales on each of the two treatment pages between the pre- and postlaunch period with the corresponding difference in sales for the unchanged control web page:Pjt= β0+ β1Ij+β2It+β3Ij×It+∊jt,1where Pjt represents daily sales from web page j at time t and is a random error term, clustered across the two periods. Our design contains two treatment web pages (informative experience and social experience) and a control web page across the two periods (pre- and postlaunch). As a conservative test, we run two separate analyses that compare the informative and social experience with the control condition. In both analyses, Ij is 1 for the treatment (informative or social, respectively) and 0 for the control condition, so that β1 represents the mean difference in sales between these two conditions. Furthermore, It is 1 for the postlaunch period and 0 for the prelaunch period, so that β2 reflects the mean difference in post- relative to prelaunch sales. Finally, β3 is the estimate of the respective treatment effect, or the change in sales due to the informative or social experimental treatment, after we control for systematic differences across conditions and common time trends:β3= [E(Pjt| j = 1, t = 1) − E(Pjt| j = 1, t = 0)] − [E(Pjt| j = 0, t = 1) − E(Pjt| j = 0, t = 0)].2In Equation 2, β3 also represents the incremental economic impact of customizing the web page design to create a particular online experience. A key assumption of the difference-in-differences approach is that the time trends in sales are identical in the treatment and control conditions, absent the treatments themselves. If this assumption holds true, we can interpret the deviation of the difference in sales between the treatment and control conditions as causal treatment effects. To verify this parallel trends assumption, we collected data at a third period, two months before the launch of the treatments, and ran a model similar to Equation 1, except that we compared this earlier period with the prelaunch period to determine the trends across the three experimental groups, before the treatments. The interaction between the period and experimental group is nonsignificant (p >.10), confirming the parallel trends and supporting the comparison of the treatment and control conditions.Because β1 represents a product fixed effect, it eliminates time-invariant, product-specific unobservable variables and reduces the threat of bias ([26]). In addition, although each product may attract slightly different customers, suggesting that a selection bias is possible, we hold the firm-controllable page entry decision criteria (product name, hero shot, SKU, and price) constant throughout the experiment. Thus, customer characteristics across conditions should be time invariant, and we can interpret β1 as a customer fixed effect that reduces this self-selection bias. However, some page entry criteria, such as a product's average star rating or number of reviews ([52]), are outside the firm's control and time variant, so they could introduce some customer differences across experimental conditions that β1 would not capture. To address this potential bias, we add a vector of control variables Xjt to Equation 1, which we use to calculate the daily difference in average customer star rating and number of reviews for each treatment page compared with the control condition:Pjt= β0+ β1Ij+β2It+β3Ij×It+ δXjt+∊jt.3 Results Model-free evidenceBefore the launch, sales did not differ between the control condition and the informative product page (Treatment 1), but the social product page (Treatment 2) achieved higher sales (Mcontrol = 3, Minfo = 3, Msocial = 734).[11] After the treatment launch, in support of our findings in Study 1, sales increased for the informative page (Minfo = 152), decreased for the social page (Msocial = 394), and decreased slightly in the control condition (Mcontrol =.1), relative to the counterfactual trend we calculated on the basis of the time trend in the control condition and the sales levels of each experimental condition before the experiment. Difference-in-differences analysisTo test these effects more formally, we run two separate models, one for Treatment 1 (informative) and one for Treatment 2 (social), in which we account for possible time-variant changes among customers who visit the product pages (Equation 3). In Model 1 (Table 5), the treatment effect of the informative experience is positive and significant (β3 = 151.980, p <.01); increasing web page informativeness improves sales of search products. By contrast, in Model 2, the treatment effect of the social experience is negative and significant (β3 = –337.180, p <.01), confirming the detrimental effects of a social experience for search products.[12] Together, these field results corroborate our insights from Study 1: Search products benefit from more informative experiences, while more social experiences can have detrimental effects on sales of these products.GraphTable 5. Study 2 Results: Field Experiment Testing Customized Online Customer Experiences.  130022242918809930 *p <.05.140022242918809930 **p <.01.150022242918809920 Notes: Standard errors are in parentheses. General DiscussionIn an era in which web design is becoming increasingly important ([81]), sellers' success depends on their ability to employ design elements on product web pages to evoke effective customer experiences that not only convey information but also entertain, imply human interactions, and mimic sensory experiences from the offline world. Through 16 large-scale experiments and a field study, we show how firms can use online design elements to drive purchase behaviors by customizing experiences according to the product or brand being sold. Our findings offer important theoretical contributions to customer experience management (e.g., [27]; [77]) and actionable managerial implications. Theoretical Contributions: Understanding the Online Customer ExperienceOur multidimensional conceptualization of the online customer experience reveals why the effectiveness of any given design element may vary with the offered product or brand. It adds to extant research that examines the direct effect of design elements on purchase decisions without addressing their underlying mechanisms ([14]; [30]). It also moves beyond unidimensional, predominantly information-processing perspectives (see Table 1). Although informativeness is a key dimension by which design elements affect purchase decisions, social presence is just as important, and entertainment is even more so. Accounting for sensory appeal adds further insights. We show that the function of design elements is not limited to the cognitive information they convey, because they also carry affective (entertainment), social (social presence), and sensory (sensory appeal) value that influences purchases. We also show that only a multidimensional perspective can help determine the most effective use of design elements for a given product or brand. Further research should thus account for and test the multiple ways design elements drive purchase.The multidimensionality of our research also led to the discovery of unexpected relationships that may guide researchers in the online domain toward identifying emerging, substantive trends and relevant constructs. For example, the effects of social presence on purchase are just as strong as those of informativeness, an insight that provides a foundation for examining recent trends such as the inclusion of chat options on websites to enable visitors to interact directly with firms. Firms now use chatbots, based on artificial intelligence, that can conduct conversations via voice or text. An information-processing view might regard chatbots as merely providers of product or transactional information, but our findings suggest that they can also convey social presence. Further research might examine how the linguistic style (a key driver of social presence) of a chatbot should be calibrated to optimize the customer experience.Moreover, our consolidation of design elements, addressing the many labels used in extant work, and our test of their relative effects reveal which elements have the greatest impact on the customer experience and thus suggest priorities for research. In allowing each design element to freely influence each experience dimension, we were able to identify the core function of each element (information, entertainment, social presence, or sensory appeal). Lifestyle photos, for example, are a key driver of social presence. In our study, they were produced by the seller. Yet companies such as Rent-the-Runway encourage customers to post photos of themselves using the firm's products (clothing) directly on product web pages. Further research could examine the implications of customer- versus firm-produced lifestyle photos. Our framework may also guide research on emerging features that allow customers to try products virtually using webcams (e.g., glasses at FramesDirect.com). These and other forms of in-page product trials warrant further investigation to determine their value for each dimension of the online customer experience.Our research also provides insights into the role of product type and brand trustworthiness online, by showing how they influence the relevance of each experience dimension for purchase decisions. Search products benefit more from informative experiences but less from social experiences. Highly trustworthy brands benefit from more informative experiences, but less trustworthy brands gain from more entertaining experiences. The finding that brand trustworthiness may increase consumers' willingness to process greater amounts of information demands further examination, especially as research suggests a decline in brand value when other sources of information become more readily available to consumers ([69]). Managerial Implications: Designing the Online Customer ExperienceThe product web page is a key tool for managers, who can strategically use design elements to create a customer experience that turns web page visitors into buyers. Our findings apply to both sellers showcasing their offerings through online retailers' websites and the retailers themselves. The production, curation, and publishing of high-quality photos, videos, and copywriting are nontrivial tasks that require significant resources.We offer a two-step design guide to show how sellers can generate sales through effective online customer experiences. First, sellers must determine the most beneficial experience, based on the search versus experience focus of the product to be sold and the trustworthiness of their brand. The measures we employ can help firms gather this information from current and potential customers. Second, firms should leverage this product and brand knowledge and apply the design guide derived in Study 1 (Figure 2) and validated in Study 2, to select relevant design elements for their product web pages. For experience products, social experiences should be built by employing a conversational linguistic style and lifestyle photos. Sensory experiences are also beneficial and can be built through product videos and product feature crops.Firms need to consider the customer experience in assessing their existing digital assets. Managers often default to a logic that suggests that if a design element exists in the firm's digital inventory, it should be used on the page (more-is-better approach). Yet we show that certain design elements can induce unfavorable customer experiences for specific products or brands. An essential part of the process is thus to also determine which elements not to use. If the firm does not already own certain design elements, our design guide suggests where it should allocate its resource investments to produce valuable new elements. For example, investing in high-quality imagery can benefit any product or brand, but the most appropriate amounts of textual detail and linguistic style depend on the product type (search vs. experience focus).Our design guide can also inform contract negotiations between sellers and retailers. Many retailers offer premium content options that require additional financial investments from sellers. Amazon, for example, offers multiple tiered categories (e.g., Basic A+ Content, Premium A+ Content) that provide access to additional design elements or configurations. For some products, these investments grant access to necessary design elements; for other products, investing in premium content might not be necessary or could even be disadvantageous. For example, premium content modules might support larger pictures and more visually stimulating content (e.g., scrolling pictures), but they also restrict the number of characters available to describe product features and benefits. Such designs can induce social or sensory experiences, but they likely are less effective at creating informative experiences. Thus, a lower-cost alternative may be more attractive to a seller that wants to provide mainly informative experiences.Our design guide is also relevant for retailers. The more conversions sellers generate on a retailer's website, the greater are its earnings. Yet retailers also must provide an infrastructure to support the digital content and guarantee adequate page load and transaction speeds. Helping sellers build effective web pages as efficiently as possible is in the retailer's best interest. With our design guide, retailers can develop tutorials to help sellers improve the effectiveness of their product web pages, as well as recommend available design elements to those sellers, based on the products and brands they market. This approach could improve conversions but also lessen storage demands, by reducing ineffective content. With our design guide and a dedicated customer experience mindset, sellers and retailers can work together strategically to maximize the performance of their product web pages. Limitations and Future Research DirectionsAlthough our research setting and design allowed us to determine the effects of various design elements on dimensions of the online customer experience and purchases, this work is not without limitations. Our results show no effects of return policy information or expert endorsement on any experience dimension, after we account for the impact of the other elements. Additional research might explore these elements further to determine any circumstances in which they prove effective. In addition, no design element exerts a particularly strong effect on the entertainment dimension. Thus, research could analyze other design elements that might prove especially instrumental in shaping this dimension. Although purchase is our final outcome of interest, an extended version of our framework might address how product web page design elements influence consumer decision-making quality, long-term satisfaction, product returns, or social media behavior ([28]; [69]).Researchers could also investigate how the effects we find translate to mobile environments and whether the same design elements induce similar or different experiences. We focus on design elements most relevant to the product presentation, and thus website elements such as navigation warrant further investigation. Research could also examine the design of landing, overview, or checkout web pages, which we do not consider in our study. Our experimental design is based on a [73] orthogonal array design, which is rare in marketing research. We recommend its application in similar, seemingly intractable research settings to facilitate the simultaneous manipulation of multiple experimental factors, as might be required for advertising or product design studies. We focus on product web pages, but a design perspective could also improve understanding of other domains in which verbal and visual stimuli build customer experiences, such as user manuals or mobile apps. As online shopping environments continue to approach the richness of the offline retail world, research should further investigate the value of design for providing unique experiences, customized to the specific characteristics of the products and brands sold. "
21,"Cueing Morality: The Effect of High-Pitched Music on Healthy Choice Managers often use music as a marketing tool. For example, in advertising, they use music to intensify emotions; in service settings, they use slow music to boost relaxation and classical music to convey sophistication. In this article, the authors posit a novel effect—higher-pitched music can boost healthier choices. Recognizing that many perceptual characteristics of higher pitch (e.g., lighter, elevated) are conceptually associated with morality, they theorize that listening to higher- (vs. lower-) pitched music can cue morality. Furthermore, thoughts about morality can prompt moral self-perceptions and, in turn, thoughts about ""good"" behaviors, including healthy choices. Thus, listening to higher-pitched music may increase healthier choices. Employing field settings and online studies, the authors find that listening to higher-pitched music increases consumers' likelihood to choose healthy options (Studies 1, 3, and 5), choose lower-calorie foods (Study 2), and engage in health-boosting activities (Study 4). This effect arises because high pitch raises the salience of morality thoughts (Studies 4 and 5). The article concludes with a discussion of theoretical and managerial implications.KEYWORDS_SPLITManagers have long used music as a marketing tool. For example, managers often employ music in advertising to intensify emotion and increase memorability of ads. Retail outlets and restaurants play music to set ambience and improve customer satisfaction. For example, slow-tempo music can help relax consumers and increase the quantity of food they consume ([31]). Classical music can cue thoughts about national origins of the music and increase perceptions of products as selections from those regions ([34]). Managers seem to recognize that these two factors—music tempo and associations—can boost customer satisfaction and, thus, sales, so they use them to construct customer experiences ([31]). However, an additional important dimension of music is its pitch ([25]; [35]). Pitch ranges along a spectrum, is easily manipulated, is pervasive in all audio bites, and is likely among the first thing a person encounters in any audio bite; yet marketers have limited understanding of how it might affect choice (for examinations of the impact of auditory pitch in a marketing context, see [23]; [24]).Indeed, in ten in-depth interviews with restaurant managers (see Web Appendix W1, Tables W1.1., and W1.2. for details), 90% indicated that they believe background music positively affects sales. They also noted that their primary consideration in choosing music is the extent to which it relaxes consumers and the fit between the associations it cues and the restaurant theme, confirming findings in the literature on the importance of these two factors ([30], [31]). However, none of the managers we interviewed overtly considered pitch in choosing background music, nor were they able to articulate how it might affect consumer choice. Here, we propose that a higher music pitch can cue morality and make consumers more virtuous in their choices, increasing preferences for healthy options because consumers consider such choices virtuous. In doing so, we provide both practical insights into how music can strategically influence healthy choices and novel theorizing on how music pitch can be an antecedent of morality thoughts, which in turn lead to healthy choice.Three streams of research show support for our theorizing that high-pitched music can cue morality. First, the research stream on perceptual effects indicates that a high pitch can bring about perceptual responses such as looking up ([42]; [47]) and perceiving objects as brighter ([ 7]; [48]) and smaller ([23]; [36]; [50]). Second, the research stream on morality associates upward directionality (e.g., people think God is up), brightness (e.g., people tend to classify lighter and brighter things as good), and size (e.g., people report things in small quantities as purer) with godliness, goodness, and purity ([27]; [28]; [43]). Morality thoughts refer to thoughts about virtuous actors and actions and therefore include thoughts about godliness, goodness, and purity ([17], [18]). Combining the disparate insights from these two streams of research—that higher pitch cues perceptual responses that also arise when considering morality—allows us to correlate music pitch with morality thoughts. Third, the evolutionary perspective further supports a link between pitch and morality. Primates emit high-pitched shrieks to warn their clans of impending danger at the cost of attracting danger to themselves ([11]). Self-sacrifice is the ultimate moral act, and in primates, higher-pitched sounds signal this act for the greater good. Drawing from these three research streams suggesting that pitch is positively associated with morality, we posit it may also cue morality. Furthermore, morality is likely to be positively associated with healthier choices, the second link in our argument. Healthier choices are often considered virtuous or good, as they are more justifiable ([ 6]; [32]; [51]), and morality is associated with goodness of actions or doing what is virtuous and right ([40]; [54]). Thus, if high-pitched music cues morality, and if healthier choices are consistent with morality, high-pitched music could increase healthier choices.In linking pitch to healthy choice through morality cueing, this research offers three specific contributions. First, whereas prior research has tied high pitch to several lower-order perceptual responses, such as size, visual sharpness, and verticality, the current research links pitch to conceptual higher-order judgments, in particular, morality. This research thus makes an important contribution by showing the effects of sensory stimuli on higher-order thinking. Second, while healthy choice can be virtuous, this research links morality to heathy choice and, in turn, pitch to healthy choice. In doing so, we show that a novel ambient factor—music pitch—can influence judgment and choice, and the more consumers consider healthy choice moral, the more they will choose healthier options when exposed to high-pitched music. Third, consumers often listen to music when making choices; thus, the findings of this research bear implications for marketers and policy makers who want to boost consumers' healthy choices through their marketing actions. Theoretical Development Pitch, Perceptual Responses, and MoralityPitch is the perceptual dimension along which sharpness of musical notes can be ordered from low to high (hertz) ([22]). Pitch is a major component of music, constituting one of the primary aspects of any audio bite, along with its sound pressure (loud or quiet), duration, and timbre ([25]; [35]). Pitch is one of the first things people experience in any sound bite; yet scant research has investigated how pitch can affect human thought and action (for exceptions, see [23]; [24]). In the handful of investigations conducted in this area, some important findings have emerged, especially those related to the impact of pitch on perception (for a summary of literature, see Table 1). For example, research indicates that people look up when they hear a high-pitched sound, suggesting that higher pitch is perceptually associated with spatial height ([42]; [47]; [48]). People also rate higher-pitched sounds as brighter ([ 7]; [49]), and they perceive the source of higher-pitched sounds as smaller than sources emitting lower-pitched sounds ([36]; [50]) and also lighter ([49]). Conversely, lower-pitched sounds are perceived as arising from lower spaces and being emitted by larger sources ([36]; [50]) and are associated with a lowering of the gaze ([48]), visual darkness ([ 7]), and heaviness ([49]). Thus, pitch is associated with distinct perceptual responses of height, brightness, and size.GraphTable 1. Overview of the Pitch Literature.  While judgments of height, brightness, weight, and size arising in response to hearing high-pitched sounds are indeed distinct perceptual responses to pitch, they all seem to converge conceptually in activating a common, higher-order construct of morality. For example, impressions of height or the act of looking up is associated conceptually with thoughts about ""goodness"" of actions, which is a fundamental aspect of morality. Indeed, people evaluate positive words more rapidly when they appear at the top rather than the bottom of a computer screen, and the effect is opposite for negative words, implying that good is up and bad is down ([28]). People also recognize words with a moral meaning (e.g., ""caring,"" ""charity,"" ""trustworthy"") more quickly when displayed at the top of the screen, while they recognize words with an immoral meaning (e.g., ""adultery,"" ""corrupt,"" ""evil"") more quickly when displayed at the bottom of the screen ([29]). Similarly, concepts of God are related to metaphors for elevation, such that people tend to put the divine above themselves ([27]), and God-related words can shift attention upward ([ 4]). Researchers have explained this link between height and morality using the conceptual metaphor framework ([17], [18]; [19]; for a review, see [20]), which proposes that all cognition is grounded in lower-order perception ([52]) and that people represent high-level concepts in terms of their low-level physical experiences. Thus, people represent morality metaphorically in terms of actually looking up and as grounded in the concepts of upward, elevation, or height. If pitch is associated with the perceptual response of looking up, and if looking up is associated with higher accessibility of moral thoughts, pitch might also cue moral thoughts.Furthermore, vision is primary among all perceptions ([13]), and therefore the impact of pitch on height perceptions and of height on thoughts of morality could dominate other perceptual effects of listening to high-pitched sounds. However, other perceptual responses to pitch, such as brightness or size, are also linked to morality and therefore may converge in cueing it. For example, consumers perceive players in dark T-shirts as more malevolent ([10]), thus implying that darker is immoral. Furthermore, research has shown the reverse link—cueing people with morality increases how bright they perceive their surroundings to be ([ 2]). Similarly, people consider smaller objects scarcer ([55]; [56]) and purer, and purity is related to morality ([57]). Conversely, they consider dense, heavy objects less pure or moral, perhaps in part because they perceive heavier things as pulling downward; for example, people consider flatter objects heavier than narrower objects ([38]). It is possible that these perceptual responses individually or as a set also conceptually cue some other constructs in addition to morality, but our core argument is that higher pitch evokes perceptual responses that are indicative of conceptual accessibility of morality thoughts, and thus pitch may cue morality. Impact of Accessible Morality Thoughts on Healthy ChoicesThe accessibility of morality thoughts may increase the likelihood that people act more morally, for several reasons. Research indicates that cueing people with thoughts they may not have otherwise considered can increase the likelihood that they will act according to these thoughts by bringing new information to mind ([33]). Thus, making morality more accessible might provide an additional consideration to consumers when making choices. Cues also remind consumers of aspects of their identity. For example, research indicates that morality thoughts can make moral identity salient, and salient moral identity can increase moral self-perception among consumers, reminding them that they are moral and virtuous and that being moral is an important goal for them ([ 3]. These moral self-perceptions might then increase moral actions, such as acting for the greater good ([ 1]; [39]; [53]; [54]). Thus, ample research demonstrates that situationally activated thoughts affect choice. If pitch activates morality, salient morality thoughts are likely to increase moral self-perceptions and, in turn, the likelihood of making moral choices.In the context of food decisions, moral choices are likely to be healthier choices. Morality is associated with perceived rightness or goodness of actions and actions socially mandated and approved. In general, healthy choices are considered good and, therefore, virtuous (e.g., [15]), while indulgent foods are considered vices ([ 6]; [32]; [51]), as indulgent foods are sugary, fatty, and tasty and often lead to immediate gratification. Healthy foods instead are immediately less gratifying but more justifiable. In support, research indicates that mothers cued with moral responsibility increase intentions to consume healthier skim milk ([37]). Research has also found that people judge consumers who choose healthier food as more moral ([46]). Taking these arguments together, we expect that listening to high-pitched music will cue morality, and increased accessibility of morality thoughts will increase actions considered moral, which in the context of food may be healthier choices. Overview of StudiesWe propose that ( 1) listening to high- (vs. low- vs. no) pitched music is likely to cue morality, ( 2) accessibility of morality is likely to increase moral self-perception and healthy choice, and ( 3) consumers will make healthier choices insofar as they consider these choices moral. In line with our theorizing, we expect only high-pitched music to increase the choice of healthy options, but we do not necessarily expect low-pitched music to further increase indulgent choice. The reason is that consumers often struggle with making healthier choices and indulgence may typically be a preferred, default choice. Unless consumers are actively trying to be moral and therefore strive to make healthier choices, they may default to a more indulgent choice. Moreover, the literature on primates suggests that normal-pitched sounds are closer to lower-pitched sounds and that primates employ higher-pitched sounds selectively in service of the greater good ([11]). Thus, listening to high-pitched music may indeed increase healthy choice, but low-pitched music may not further increase indulgent choice from the baseline. We return to this issue in the discussion of Study 2.We test these propositions in five studies. In a field study, we first investigate whether participants exposed to high- (vs. low-) pitched music are more likely to purchase a healthy (vs. indulgent) food item (Study 1). We then test whether participants are more likely to choose foods containing fewer calories (Study 2) and more healthful options (Study 3). By including a normal-pitch condition, the results of Study 2 further establish the role of high-pitched music in increasing healthy choice but not low-pitched music in decreasing it. After verifying the link between music pitch and healthy choice using different genres of music stimuli (Studies 1–3), we investigate the underlying process. Specifically, in Study 4, we test whether accessible morality thoughts increase moral self-perceptions and if this relationship underlies the effect of pitch on healthy choice. To further test the role of accessibility of morality thoughts as underlying the link between pitch and healthy choice, in Study 5 we cue consumers exposed to low-pitched music externally to morality thoughts. We examine whether this cueing makes their choices similar to those made by participants exposed to high-pitched music who do not need such external cueing of morality. We also rule out alternative explanations, including effects of arousal, sense of power, and mood (Studies 2–5). Study 1: Field InvestigationThe purpose of Study 1 was to provide initial evidence of whether high- (vs. low-) pitched music increases preferences for healthy foods in a real-life setting. Specifically, we set up a pop-up cookie store on campus for students and staff members. Method Participants and designSix hundred fifty-eight passersby at the student center food court at a North American university participated in a one-factor, two-level (music pitch: high vs. low) between-subjects field study. Of these, 3 people did not hear any music due to a glitch, 10 people arrived as couples, and 6 people, including 2 who arrived as a couple, reported knowing the research assistant. These people's attention and reactions to the music being played (the independent variable), their reasons for arriving at our stall, and their product choices (the dependent variable) are open to the social influence of their partner or the research assistant. Thus, to ensure that any effects of our independent variable (pitch) on our dependent variable (choice) are free of this possible confound of social influence, we removed these 17 people from our analysis, which resulted in an independent sample of 641 people (338 women, 302 men, 1 unrecorded). The dependent variable was the purchase of one of two types of 2-oz. cookies: oatmeal raisin (healthy option) or double chocolate chip (indulgent option).To verify that students perceive oatmeal cookies as healthier than chocolate chip cookies, we conducted a pretest (N = 40; 13 men). For each cookie type, we asked students to indicate ( 1) perceived healthfulness (1 = ""not at all,"" and 7 = ""very healthy/nutritious"") and ( 2) perceived indulgence (1 = ""not at all,"" and 7 = ""very indulgent/delicious""). As we expected, participants indicated that the oatmeal (vs. chocolate) cookies were healthier (M = 4.68, SD = 1.62 vs. M = 2.25, SD = 1.69; t(39) = 8.98, p <.001) and less indulgent (M = 4.85, SD = 1.82 vs. M = 6.18, SD = 1.08; t(39) = -4.38, p <.001). ProcedureWe operated a pop-up cookie store for five days at the student center on campus. Each day the cookie store operated for two hours, except the first day when the store ran for one hour. Two types of cookies (oatmeal raisin and double chocolate chip) were available for purchase, and we replaced purchased cookies immediately with new cookies. A poster in front of the store alerted passersby of a cookie promotion for a local manufacturer ($.25 per cookie). Any person could purchase only one cookie.Instrumental music played in the background while the store was operating. Using Audacity software, we adjusted the pitch 50% upward or 50% downward (all stimuli are available on request). We ran 18 half-hour sessions, 9 with high-pitched and 9 with low-pitched music, counterbalanced. A research assistant blind to the study's purpose counted all passersby, those who approached the store, those who made a purchase, and which cookie they bought.One hundred nineteen passersby (18.56%) approached the store. A somewhat larger proportion of passersby approached the store when high-pitched (N = 69/322 = 21.43%) rather than low-pitched music (N = 50/319 = 15.67%) played (z = 1.874, p =.061). To our surprise, supplementary analysis showed that a significantly higher proportion of women approached the stall when high-pitched (N = 39/170 = 22.94%) rather than low-pitched (N = 23/168 = 13.69%) music played (z = 2.197, p =.028). Pitch of music did not differently attract men to the stall (N = 30/151 vs. 27/151; z =.441, p =.659).Eighty-six people who approached the stall made a purchase (72.27%). Of those who approached the store, music pitch did not differentially affect overall purchase rate of both cookies (Nhigh-pitch = 50/69 = 72.46% vs. Nlow-pitch = 36/50 = 72%; z =.056, p =.956; women: z =.409, p =.683; men: z = –.342, p =.733).[ 6] Thus, compared with low-pitched music, high-pitched music attracted a larger proportion of passing women to the store, but on arriving at the store, their purchase rate did not differ depending on music or gender. People also overwhelmingly preferred purchasing the chocolate chip cookie (N = 57/86 = 66.28%) to the oatmeal cookie, and this rate was similar for women (N = 28/45 = 62.22%) and men (N = 29/41 = 70.73%; z = –.834, p =.404).Importantly, as we predicted, the proportion of healthier cookies purchased was higher when high- (vs. low-) pitched background music played (25/50 = 50% vs. 4/36 = 11.11%; z = 3.763, p <.001, see Table 2). In addition, pitch increased the purchase of the healthy cookie more strongly among women (z = 3.24, p =.001) than among men (z = 1.96, p =.05).GraphTable 2. Summary of Effects of Music Pitch on Healthy Choice.  1 ^Out of 658 passersby in this field study, 86 made a purchase (excluding those who did not hear any music, arrived as couples, or reported knowing the research assistant).2 *There is a missing data point on the measure of pleasantness, so the number of participants for the analysis is 299. DiscussionIn this study, we examined actual purchases in a naturalistic setting. A higher proportion of passersby were attracted to our stall when pitch was high (vs. low), especially women. Among people who then bought cookies, higher-pitched music increased the proportion of healthy cookies selected, especially among women. Why pitch affected women more, and whether it attracted a type of woman more predisposed toward morality, warrants further research. One potential reason for this gender difference in response to high pitch may be that women often serve as primary caregivers to children, and children have higher-pitched voices. Furthermore, women themselves have higher-pitched voices than men, and people generally are more attentive to their in-group and their characteristics, as these are more self-relevant. For both these reasons, women may generally be more attentive than men to higher-pitch sounds. A third reason may be that women are more communal ([ 8]), and we ran the study in a location where there are often student-benefit activities. Student-benefit activities are prosocial in nature, and therefore, women may have been cued toward prosociality, and more generally morality, in this venue (student center food court) than men. Being more attuned to morality might have made women more sensitive to a high pitch, if a high pitch is associated with morality, as we claim. For example, in a religious venue, women may also be more sensitive to a high-pitch audio bite. To avoid these potential problems of self-selection caused by pitch that can be common in any field setting, we replicate this effect in a controlled experiment in Study 2 with a different type of choice (healthy items ordered) and a different music genre (rock); we also include an unaltered (control) normal-pitch condition. Study 2: High-Pitched Music and Healthy Items Ordered Method Participants and designThree hundred participants (133 men; Mage = 41.39 years, SD = 13.19) recruited from the Amazon Mechanical Turk (MTurk) online platform took part in the study for payment (US$.50). The study was a one-factor, three-level (pitch: high vs. normal vs. low) between-subjects design. As a prerequisite to take part in the study, participants were required to have an audio-capable device and available headphones. ProcedureThe cover story indicated that shoppers often carry an iPod to listen to music, including when they visit a café. To simulate such a situation, we instructed participants to put on their headphones and click on the music file. Similar to Study 1, we adjusted pitch up or down by 50%. We randomly allocated participants to listen to a high, a low, or an unaltered (normal) pitch version of the same rock music. Participants were asked to put on their headphones and confirm that they could hear the music playing. They were then asked to complete manipulation check and control questions about the music that was playing (1 = ""low-pitched, unfamiliar, discomforting, slow tempo, unpleasant""; 9 = ""high-pitched, familiar, comforting, fast tempo, pleasant""). They then imagined that they were ordering breakfast at a local café off a menu listing different food options and associated calories of each option (see Appendix A; see also Web Appendix W2 for pretest details of these stimuli for Studies 2 and 3). Participants indicated all items they would order. Among all 11 items on the menu, 5 (items 2, 3, 5, 8, and 11) were low-calorie and relatively healthy items. As our key dependent variable, we summed the total number of healthy items ordered (range: 0–5).After making their choices, as supplementary measures, participants reported their arousal (1 = ""relaxed, sluggish, depressed, drowsy, calm""; 9 = ""stimulated, frenzied, upbeat, energetic, aroused""; averaged into an arousal index, α =.75), feelings of power (1 = ""powerless""; 9 = ""powerful""), and mood (1 = ""sad""; 9 = ""happy""). Considering that the choices participants made likely influenced these responses, these data are not insightful to our investigation, but we report all associated means and standard deviations in Table 3 (see also Web Appendix W3 for the main analysis with these items included as covariates for Studies 2–5). Then, participants reported their age and gender, where they took the study (86.0% completed the study at home), the device used (41.7% used a desktop, 52.0% used a laptop), and whether they followed our instructions to wear a headset and listen to the music. For all the studies, we did not exclude any data.[ 7]GraphTable 3. Studies 2–5: Manipulation Checks and Controls: Mean (SD) Summary Table.  3 Notes: Cells with different superscripts in each column (within each study) differ at p <.05. *There is a missing data point in the measure of pleasantness; therefore, the number of participants for this analysis is 299. Results Manipulation checks and controlsAs expected, participants indicated that the high-pitched music was of a higher pitch than the normal- and low-pitched music (for means and standard deviations, see Table 3). Music across the high- and normal-pitch conditions did not differ in terms of familiarity, comfort, tempo, or pleasantness (ps >.21), giving us confidence that any predicted difference between high and normal/low pitch on healthy items ordered cannot be accounted for by these factors. Participants rated the low-pitched music as less familiar, less comfortable, slower in tempo, and less pleasant than the high- and normal-pitched music, but we expected the normal- and low-pitch conditions to exert similar influence on choice; thus, it is again unlikely that these factors account for our key effects. However, because comfort and pleasantness of music can influence mood and mood can increase healthy choice among consumers, in Studies 2–5 we control for these two factors in our analyses. Doing so gives us greater confidence that any effects on the dependent variable are not driven by mood. For Studies 2–5, Web Appendix W5 reports the analyses without these covariates. Our overall conclusions across studies are not affected by whether or not we use these covariates. Total healthy items orderedA missing data point on the measure of pleasantness resulted in a sample of 299. A one-way analysis of covariance (ANCOVA) on healthy items ordered, controlling for music comfort and pleasantness, yielded the expected main effect of pitch (F( 2, 294) = 4.31, p =.014; covariate ps >.52). Participants listening to high-pitched music ordered more healthy items (M =.78, SD =.62) than those listening to normal- (M =.55, SD =.69; F( 1, 294) = 7.21, p =.007) or low-pitched music (M =.51, SD =.59; F( 1, 294) = 5.29, p =.022; no difference between the latter two conditions, p >.92). Thus, high-pitched music increased healthy choice, though low-pitched music did not reduce it.To ensure that higher pitch did not increase more choices of foods in general and to confirm that participants swapped healthier foods for unhealthier ones, we also ran a one-way ANCOVA on overall number of choices participants made. We found no differences across conditions on total food items chosen (Mhigh-pitch = 1.31, SD =.90; Mnormal-pitch = 1.33, SD =.60; Mlow-pitch = 1.30, SD =.55; F( 2, 294) =.032, p =.968; covariate ps >.67). Thus, the overall number of food items chosen did not change depending on high pitch, but participants swapped healthier options for unhealthier ones. DiscussionUsing a different music genre from Study 1 and including normal-pitched music as a control, we found further evidence that compared with normal- and low-pitched music, listening to high-pitched music can increase healthy choice. The effect stems from high-pitched music increasing healthy choice, not low-pitched music decreasing it. We expected the normal- and low-pitch conditions to be similar because people have a default tendency to choose indulgent options. Although healthy options are what people think they should choose, indulgent options are what they want to choose, and they usually give in to their wants. Thus, playing low-pitched music may not increase indulgent choice further, as it is already high.We conducted a separate test to verify that indulgent (vs. healthy) options are what people prefer to choose (vs. should choose). We presented 51 MTurk workers (26 men, Mage = 38.37 years, SD = 11.36) with definitions of healthy choices (i.e., ""foods that provide long-term health benefits; e.g., fruit salad"") and indulgent choices (i.e., ""pleasurable foods; e.g., cheesecake""). Participants indicated on a nine-point scale the extent to which healthy and indulgent choices are what they want to choose ( 1) or what they should choose ( 9). As expected, we found that indulgent options (M = 3.57, SD = 2.29) are what they want to choose (vs. what they should choose), compared with healthy options (M = 7.00, SD = 2.15; F( 1, 50) = 46.59, p <.001). Therefore, the finding that the low-pitch condition was not significantly different from the control (normal-pitch) condition confirms that people might already choose indulgent options as a default and do not increase indulgent choices further.Thus far, we have provided evidence that high-pitched music increases healthy choices while low-pitched music does not reduce healthy choices. We observed these results after controlling for any possible mood effects pitch might evoke. Thus, our objective in Study 3 is to replicate the effects of pitch on healthy choice using two other genres of music and another dependent variable. Study 3: Generalization to Other Types of Music Method Participants and designSix hundred one MTurk workers (299 men, Mage = 40.62 years, SD = 11.79) participated for payment (US$.50) in a 2 (pitch: high vs. low) × 3 (music genre: metal vs. jazz vs. rock) between-subjects study. As a study prerequisite, participants were required to have an audio-capable device and available headphones for the study. ProcedureAs in Study 2, we first asked participants to put on their headphones and confirm that they could hear the music file playing. We used the same rock music stimuli as in Study 2 but also included metal and jazz music for this study (pitch adjusted 50% up or down). Participants, assigned randomly to listen to one genre of music in either high or low pitch, first rated the characteristics of the music (as in Study 2) and then made four choices, each between a healthy and an unhealthy option (see Appendix B). We coded healthy choice as 1 and unhealthy choice as 0, summing scores across all choices for each participant to create a healthy-choice index (range: 0–4). As in Study 2, participants then indicated their postchoice arousal (α =.76), sense of power, and mood; their age, gender, study location (85.0% at home), and device used (39.9% desktop, 52.2% laptop); and whether they wore a headset and could hear music. Results Manipulation checks and controlsAs expected, participants rated the high- (vs. low-) pitched music as higher in pitch. No other clear patterns emerged across the three genres of music from pitch familiarity, comfort, tempo, or pleasantness (see Table 3). Healthy choiceWith music comfort and pleasantness as covariates, a 2 (pitch) × 3 (music genre) ANCOVA predicting the healthy-choice index revealed only a main effect of pitch (Mhigh-pitch = 1.66, SD = 1.22 vs. Mlow-pitch = 1.24, SD = 1.07; F( 1, 593) = 16.95, p <.001; genre p >.65, covariates ps >.23). Thus, pitch effects on healthy choice occur across music genres. DiscussionUsing additional music genres, we replicated our finding that listening to high-pitched music increases healthy choice. This finding extends generalizability of our results to multiple genres of music and provides additional confidence in this effect. We posit that this effect arises because high-pitched music cues morality thoughts, which increase healthy choice. We test for this mediation process in Study 4. We also use an audio clip and a dependent variable that differ from those in the previous studies to further test robustness. Study 4: The Mediating Role of Moral Self-Perception Method Participants and designTwo hundred one participants (93 men; Mage = 39.57 years, SD = 11.10) recruited from the MTurk online platform took part in the study for payment (US$.50). The study was a one-factor (pitch: high vs. low) between-subjects design. ProcedureParticipants first put on headphones (after reading the same cover story and instructions as in Studies 2 and 3) and then were assigned to listen to either high- or low-pitched music. To ensure generalizability across the different audios, participants listened to another instrumental piece, with pitch adjusted upward or downward by 50%. Participants confirmed that they could hear the music and then evaluated it with the same set of music characteristics as in the previous studies. In line with the cover story, participants then proceeded to the main task designed to collect our process measures and dependent variables. They indicated their likelihood of engaging (1 = ""very unlikely,"" and 9 = ""very likely"") in each of four activities at this time: ""exercising in the gym,"" ""joining a fitness class (e.g., Yoga/Pilates),"" ""going running/cycling,"" and ""avoiding tasty, tempting, high-calorie foods containing bad cholesterol and fat."" As process measures, participants rated moral self-perception along three items (""At this moment, I feel I am moral,"" ""At this moment, I feel I am virtuous,"" and ""At this moment, I have high moral standards""; 1 = ""disagree very much,"" and 9 = ""agree very much,"" averaged into a morality index, α =.92). We counterbalanced the process and dependent measures and found no order effects. Last, as in Studies 2 and 3, participants rated their postchoice arousal (α =.72), sense of power, and mood; their age, gender, study location (91.5% at home), and device used (35.3% desktop, 55.2% laptop); and whether they wore a headset and could hear music. Results Manipulation check and controlsParticipants reported that the high-pitched music was of higher pitch (see Table 3) but was not more familiar or faster in tempo than the low-pitched music (ps >.42). Unlike in Studies 2 and 3, participants rated the high- (vs. low-) pitched music in this study as less comforting and less pleasant. Engaging in healthy activitiesWe averaged the four measures of engaging in healthy activities into an index (α =.79). A one-way ANCOVA using music pleasantness and comfort as covariates revealed a main effect of pitch on desire to engage in healthy activities (F( 1, 197) = 6.37, p =.012; pleasantness p =.023; comfort p >.75). Listening to high-pitched music increased participants' desire to engage in healthy activities (M = 3.88, SD = 1.90) more than listening to low-pitched music (M = 3.39, SD = 1.64). MediationWe conducted mediation analyses to test whether high-pitched music heightens moral self-perception, which increases healthy preference because people consider such choices more virtuous (see Figure 1). Music comfort and pleasantness served as covariates in the mediation analysis models to ensure that any mediation effects we observe are not accounted for by mood. Regression analysis showed that listening to high-pitched music (1 = high pitch; –1 = low pitch) heightens moral self-perception (b =.22, SE =.10, t(197) = 2.13, p =.034) and increases the likelihood to engage in healthy activities (b =.31, SE =.12, t(197) = 2.53, p =.012). Moral self-perception is also positively related to healthy choice (b =.20, SE =.09, t(197) = 2.36, p =.019). When we entered both pitch and moral self-perception as predictors of healthy activities in the analysis, both pitch (b =.28, SE =.13, t(196) = 2.21, p =.028) and moral self-perception (b =.17, SE =.09, t(196) = 2.03, p =.044) remained viable, indicating that moral self-perception at least partially mediates the effect. Bootstrapping ([12]; Model 4) confirmed the indirect effect of pitch on healthy choice through moral self-perception ( 5,000 draws; bias-corrected 95% confidence interval: [.0047,.1134]).Graph: Figure 1. Study 4: Mediation investigating the effect of music pitch on likelihood to engage in healthy activities. * p <.05; controlling for music comfort and pleasantness.Notably, while music pleasantness increased desire to engage in healthy activities (p =.002) and moral self-perception (p =.016), high pitch reduced perceptions of music pleasantness (p =.018). Thus, in this study, although pleasantness was positively associated with moral perception and healthier activities, because a high pitch reduced pleasantness, the effect of pitch we observed on healthier activities cannot be accounted for by mood. In situations in which high pitch also increases pleasantness, it is possible that increased preferences for healthier activities will result from the direct effect of pitch on moral perceptions and preferences that we observed in this study, as well as from a positive and additive effect of mood on preferences, which goes against our effect in this study. Pitch did not affect postrated arousal, power, and mood (ps >.10; Table 3). DiscussionListening to high-pitched music increased moral self-perception and, in turn, intention to engage in healthy actions. This study thus provides mediation support for our proposed process. We also found that music pleasantness increased intent to engage in healthy activities, but importantly, mood did not mediate the proposed effect of pitch on intent to engage in healthy actions. We observed the effect of pitch on intent to engage in healthy activities even after controlling for any possible mood effects evoked by comfort or pleasantness of pitch. Thus, when high-pitched music is pleasant, it can increase intent to engage in healthy actions, consistent with prior research showing that a positive mood can increase healthy actions ([ 9]). However, pitch also exerts an influence on healthy choice independent of any mood effects, because it cues thoughts about morality, prompting people to engage in healthy choice because doing so is virtuous. In Study 5, we provide further process evidence through moderation ([44]). If high-pitched music cues morality thoughts, priming people listening to lower-pitched music with morality should increase their healthier choices to levels similar to those of people listening to high-pitched music without the additional morality cue. Study 5: The Moderating Role of Morality Salience and Gift-Card Choice Method Participants and designFour hundred one participants (185 men; Mage = 39.43 years, SD = 12.22) recruited from the MTurk online platform took part in the study for payment (US$.50). The study was a 2 (pitch: high vs. low) × 2 (salience: morality vs. neutral thoughts) between-subjects design. ProcedureTo manipulate morality salience (vs. not), participants completed a sentence-unscrambling task, allegedly for use in future studies ([16]; [45]). We designed this cover story to create separation between the independent (priming) and dependent (choice) variables, to minimize socially desirable responding. Participants received six sets of five words and had to make a grammatically correct four-word sentence out of each set. The sentences either reminded participants of morality (e.g., ""being moral is important"" from the words ""being, moral, important, is, am"") or were neutral (e.g., ""this ball is blue"" from the words ""this, blue, ball, is, be""). In line with the cover story, participants rated how positive (1 = ""negative"", 9 = ""positive"") and how easy (1 = ""easy"", 9 = ""difficult""; reverse coded) the task was. Responses to these two items were averaged to form a task evaluation index (r =.31, p <.001).Next, we instructed participants that they would make a consumer choice while listening to music. As people often make choices while listening to music, we wanted to simulate such a situation. All participants put on earphones and indicated that they could hear music. We employed the same instrumental music as in Study 1, with its pitch adjusted upward or downward by 50%. We than asked participants to make a choice between two gift cards, one for a healthy restaurant and one for an indulgent restaurant. Both gift cards had the same value (US$20). Participants also received basic information about the restaurants (e.g., style, menu; see Appendix C). Then, as in Studies 2–4, participants rated their postchoice arousal (α =.71), sense of power, and mood; their age, gender, study location (84.5% at home), and device used (43.4% desktop, 52.4% laptop); and whether they wore a headset and could hear music. Results Manipulation checks and controlsParticipants rated the high- (vs. low-) pitched music as higher in pitch (see Table 3) but not in familiarity (p >.87). They rated the high-pitched clip as less comfortable, faster, and less pleasant. The task evaluation of the sentence-unscrambling task was higher in the control condition (M = 7.40, SD = 1.39) than in the morality salience condition (M = 6.92, SD = 1.57; F( 1, 399) = 10.62, p =.001). Gift-card choiceMusic comfort and pleasantness served as covariates in the following analysis. A logistic regression predicting gift-card choice (1 = healthy, 0 = indulgent) from pitch (1 = high pitch, –1 = low pitch), salience (1 = morality, –1 = neutral thoughts), and their interaction revealed only a significant interaction effect (b = –.26, SE =.10, z = –2.50, p =.012). The main effects of pitch, morality salience, and the covariates did not reach significance (ps >.14). Crucial to our theorizing, morality (vs. neutral) priming increased healthy choice among participants listening to the low-pitched music (Mmorality = 53.33% vs. Mneutral = 34.04%, b =.41, SE =.15, z = 2.79, p =.005), making their choices similar to those of participants in the high-pitched music condition who did not receive this additional morality cue (M = 42.59%). Thus, morality cues increased healthy choice among people listening to low-pitched music, implying that they did not already have access to such thoughts from the music alone. Equally importantly, as we predicted, morality priming did not boost the healthy choice among people listening to high-pitched music further (Mmorality = 38.30% vs. Mneutral = 42.59%, b = –.11, SE =.14, z = –.75, p =.456), which implies that listening to high-pitched music already cues morality thoughts, which in turn boost healthy choices among participants. Although there was a significant difference in task evaluation between the control and morality salience conditions, the interaction effect between pitch and salience remained significant (p =.010) even when task evaluation was included as an additional covariate in the model. DiscussionHigh pitch seems to spontaneously cue morality and increase moral self-perception. Priming morality externally increased healthy choice among participants listening to low-pitched music, making their choice similar to that of participants listening to high-pitched music. General DiscussionAcross five studies, we showed that high pitch cued morality and increased moral self-perception, which in turn increased healthy choices (see Web Appendix W6 for three calibration studies). We found that consumers were more likely to purchase healthier foods in a pop-up store when listening to high- (vs. low-) pitched music (Study 1). In addition, music pitch increased consumers' choice of lower-calorie foods (Study 2) and also increased the likelihood of choosing healthier foods (Study 3) and engaging in health-promoting activities (Study 4). We found that listening to higher-pitched music cued morality and increased moral self-perception (Studies 4 and 5), which in turn increased healthy choices, but only insofar as morality thoughts were salient (Study 5). Importantly, cueing morality independently of music increased salience of such thoughts and moral self-perception among participants listening to low-pitched music, thereby increasing their healthy choices to levels observed among participants listening to high-pitched music who did not need the external reminder of morality. We found these effects using five different pieces of music and four different dependent variables, including healthy versus unhealthy choices (Studies 1, 3, and 5), likelihood to engage in healthy activities (Study 4), and healthy items with lower calories (Study 2). We used low-pitched music rather than normal-pitched music as the comparison group in all studies except Study 2 to ensure that the music stimuli employed in both conditions were comparable (i.e., both were altered). Not doing so could have resulted in a confound that normal-pitched music is more familiar than altered music. Across all five studies, single-paper meta-analyses ([26]) confirmed a significant difference in preference for healthy options between the high-pitch and low-pitch conditions (choice measures: estimate =.2357, SE =.1236, z = 1.91, p =.057; Likert measures: estimate =.3494, SE =.0716, z = 4.88, p <.001; for more details, see Web Appendix W7).A possible alternative explanation for our results is that high pitch cues lower perceptions ([23]); smaller product size is also linked to higher quality ([56]). If consumers also perceive healthy choices as high-quality choices, high pitch might promote healthy choice because they deem such choices of higher quality. To explore this possibility, we tested the quality inference of the options used in our studies (see Web Appendix W2); healthy choices were not always considered also of higher quality across our studies. Furthermore, quality perceptions do not explain why priming morality increased healthy choice among participants listening to low-pitched music in Study 5. Theoretical ContributionsThis research offers several important theoretical advances. First, it theorizes a link between perceptual responses to pitch and conceptual associations of morality. According to prior research, high-pitch sounds affect perception in several ways ([23]; [48]). Research has also linked perception to morality. Merging these two research streams, we show that pitch can cue morality.Second, this research reveals that morality thoughts can increase healthy choice. We theorize that morality is associated with goodness in actions and that people often consider healthy choices virtuous, right, or good actions. Thus, when morality thoughts are more accessible, consumers act according to these accessible thoughts, making healthier choices. This presumption of healthy choice as virtuous has not been empirically tested before. Thus, this research advances the literature theoretically by proposing a novel link between pitch and morality and between morality and healthy choice.Third, this research adds to the body of work on environmental influences and choice ([ 5]; [14]), especially the influence of music on choice. Prior research has found that fast-tempo music can cause consumers to eat more quickly in a restaurant or complete their shopping trip sooner than slow-tempo music ([30], [31]). Adding to these findings, we find that music pitch can cue morality and increase healthy choice by increasing moral self-perception, but only insofar as healthy choices are perceived as moral and thus are diagnostic to choice. We also ruled out the effects of arousal, power, and mood on choice as alternative explanations. Given that consumers often listen to music when making choices, the effects of pitch on healthy choices may be widespread. Marketing ImplicationsThis study holds relevance for marketers and policy makers. We found the effects in a field setting with a pop-up store, implying that similar effects are likely to arise when managers play music in their stores. If exposure to high-pitched music can cue morality thoughts and increase people's choices in line with an enhanced moral identity, grocery stores, malls, and restaurants could play high-pitched music to encourage healthier choices. This insight can also help marketers design ad appeals to more effectively promote healthy products. For example, marketers might consider choosing high-pitched songs as background music. Further research could investigate the generalizability of these findings to domains beyond food and health. For example, might charities that play high-pitched music when raising funds increase donations?That we found these effects in the health domain and for food is also important. Given the high obesity rate among adults (39.6%) and children (18.5%) in the United States ([21]), our findings suggest an additional easy way for policy makers to promote healthy choices through marketing actions—that is, playing high-pitched background music in campaigns to curb obesity. Notably, the Healthy Menu Choices Act ([41]) states, ""food service premises with at least 20 locations in Ontario must now comply with the Act's food labelling requirements, focused on displaying calorie information for most food and drink items offered for sale."" This information allows consumers to count calories, and our study shows that high-pitched music can curb ordered calories. Thus, these findings suggest a simple way to boost healthier choices. Moreover, we speculate that the effect of music pitch—as a situational factor—is stronger for general settings (e.g., cafeterias) than settings in which consumers may already have salient morality thoughts (e.g., healthy restaurants, gyms). Limitations and Future Research DirectionsThis work opens avenues for further research. In this study, we investigated an effect of instrumental music on choice and controlled for meaningfulness of lyrics to avoid confounding semantic activation through lyrics with semantic activation through pitch. Intuition suggests that when lyrics are sufficiently ambiguous, pitch might alter the interpretation of the lyrics as more or less moral. When the meaning of the lyrics is unambiguously moral or immoral, the lyrics may override any pitch effects, though this possibility remains to be investigated. Indeed, Study 5 showed that priming morality increased healthy choice even among consumers who listened to low-pitched music, suggesting that lyrics can alter meanings of pitch. Further research could also test whether these effects generalize to voice pitch.Finally, our focus was on examining consumers' preferences for healthy options as the downstream consequence of listening to high-pitched music. Additional research could more systematically investigate virtuous choices other than health, such as charitable giving. If the effect can generalize to this realm, charities could play high-pitched music when raising funds to persuade passersby to donate. Perhaps the high-pitched bell-ringing of Salvation Army workers serves more than just attracting attention—it may also cue moral identity and increase donations. These possibilities await further investigation. "
22,"Customer Experience Journeys: Loyalty Loops Versus Involvement Spirals Customer experience management research is increasingly concerned with the long-term evolution of customer experience journeys across multiple service cycles. A dominant smooth journey model makes customers' lives easier, with a cyclical pattern of predictable experiences that builds customer loyalty over time, also known as a loyalty loop. An alternate sticky journey model makes customers' lives exciting, with a cyclical pattern of unpredictable experiences that increases customer involvement over time, conceptualized here as an involvement spiral. Whereas the smooth journey model is ideal for instrumental services that facilitate jobs to be done, the sticky journey model is ideal for recreational services that facilitate never-ending adventures. To match the flow of each journey type, firms are advised to encourage purchases during the initial service cycles of smooth journeys, or subsequent service cycles of sticky journeys. In multiservice systems, firms can sustain customer journeys by interlinking loyalty loops and involvement spirals. The article concludes with new journey-centered questions for customer experience management research, as well as branding research, consumer culture theory, consumer psychology, and transformative service research.KEYWORDS_SPLITCustomer experience management (CXM) research is increasingly concerned with the long-term evolution of customer experience journeys across multiple service cycles ([15]; [46]; [61]). Much of this research suggests that firms should make customer journeys as ""consistent and predictable"" as possible ([37]; [48], p. 55; [57]). Firms are advised to invest in ""streamlining"" techniques ([28], p. 90), such as simplification, personalization, and contextualization. These streamlining techniques are intended to enroll customers into an ""ongoing cycle"" of retrigger, repurchase, and reconsumption experiences ([21], p. 101), known as a ""loyalty loop"" (p. 102). In time, this loop can feel seamless, like ""sliding down a greased chute"" ([34], p. 227). Given the emphasis on consistency, effortlessness, and predictability, we call this approach to customer journey design the ""smooth"" journey model. This approach is mostly derived from research on instrumental services, such as banking (e.g., Citibank), pharmacies (e.g., MedPlus), and transportation (e.g., Amtrak).However, many firms today offer a dramatically different kind of customer journey, one that intentionally features inconsistency, effortfulness, and unpredictability to keep customers excited ([ 2]; [32]; [62]). For example, CrossFit, a group fitness service, offers customers ""constantly varied"" workouts ([41]) in which ""the excitement never seems to wear off"" ([74], p. 4). Pokémon Go, an augmented reality game, keeps players wandering through real-world locations to catch randomly spawning virtual creatures ([ 5]). Tinder, a geosocial dating app, facilitates a dating journey ""filled with adventure, unknowns, and endless possibilities"" called the #swipelife ([90], p. 3). The press refers to such customer journeys as ""sticky"" to emphasize that customers cannot seem to pull away, and even when they do pull away, they are eager to return for more ([63], p. 7; [65]; [78]). Simply put, sticky journeys are exciting journeys that customers yearn to continue. Despite the rising popularity of sticky journeys, CXM researchers have yet to question the assumptions of the smooth journey model or to develop an alternate conceptual model. Redressing these oversights is important because CXM research is too quickly converging on the smooth journey model, without recognizing legitimate alternatives.In this article, we make three contributions to CXM research on customer journey design. Our first contribution is to challenge the dominance of the smooth journey model. This model advises firms to enroll customers into a loyalty loop of predictable experiences, such as Citibank transactions, MedPlus refills, and Amtrak trips, regardless of the service category. Such predictable experiences offer customers convenience, ease, and satisfaction, but also risk losing customer attention in competitive markets.Our second contribution is to empirically develop an alternate sticky journey model, premised on the excitement of unpredictable experiences. Beyond CrossFit workouts, Pokémon Go walkabouts, and Tinder dating adventures, other examples of such experiences include those of Blue Apron meal kits, dramatic HBO serials, Instagram image feeds, Spotify music streams, and trendy Zara fashions. At the heart of the emergent sticky journey model is the notion of an ""involvement spiral""—a roller coaster ride of thrilling and challenging experiences that motivates increasing experiential involvement over time.Our third contribution is to address practical CXM concerns at the nexus of the two journey models, including which model to select, when to encourage purchases, and how to sustain journeys. We advise firms to employ the smooth journey model in instrumental service categories, wherein customers have jobs to be done, and the sticky journey model in recreational service categories, wherein customers seek never-ending adventures. We also advise firms to encourage purchases at different times within each journey type: during the initial service cycles of smooth journeys, when customers are motivated to make complex decisions, and during the subsequent service cycles of sticky journeys, when customers are already caught up in involvement spirals. Finally, we trace six possible ways of interlinking loyalty loops and involvement spirals to sustain customer journeys in multiservice systems. For example, firms could spark involvement spirals from existing loyalty loops. Overall, this article challenges the dominance of the smooth journey model, offers an alternate sticky journey model, and encourages new ways of thinking about customer experience journeys. The Customer Experience JourneyThe concept of customer experience is generally defined as a customer's multidimensional—cognitive, emotional, sensorial, behavioral, and relational—responses to a firm's service ([79]). Building on the notion of customer experience, the concept of customer experience journey (or customer journey) is typically defined as the ongoing customer experience across the phases of a service cycle ([35]). These phases are variously demarcated in the CXM literature as ""pre-purchase, purchase, and post-purchase situations"" ([46], p. 384); ""pre-core, core, and post-core service encounters"" ([92], p. 270); and ""search, purchase, experience, and reflect [phases]"" ([25], p. 243). However, exclusively focusing on phases within a service cycle is too myopic for CXM practitioners if they hope to have customers returning for several service cycles ([15]; [67]; [99]).To overcome this myopia, recent CXM literature has expanded the scope of the customer journey concept—from the relatively short-term customer experience of a single service cycle to the relatively long-term customer experience across multiple service cycles ([56]). This literature emphasizes that the customer experience during the first service cycle is different from the customer experience during repeat service cycles ([21]), necessitating distinct conceptualizations of journey patterns during initial and subsequent service cycles. Moreover, the customer experience during each subsequent service cycle tends to build on the experiences of prior service cycles ([24]). In other words, the customer journey across multiple service cycles is not repetitive but iterative ([61]). Finally, when journeys near the end, the journey pattern across the final few service cycles may also be different from those in prior service cycles ([20]), necessitating distinct conceptualizations of termination trajectories.In summary, recent CXM literature advises customer journey researchers to look beyond the short-term customer experience of a single service cycle to the long-term journey patterns across initial, subsequent, and terminating service cycles. In this way, recent CXM literature is renewing the originally intended scope of the customer journey concept ([35]). Thus far, this literature has developed around an interconnected set of conceptual axioms that we frame as the smooth journey model. The Smooth Journey Model The Initial Service Cycle in the Smooth Journey ModelThe initial service cycle of customer experience journeys is widely understood as a highly deliberate, multiphase, customer decision-making process, motivated by internal and external triggers ([21]; [44]; [84]). Firms compete for customer attention during every phase of this process: ( 1) the initial consideration of multiple brands, ( 2) the active evaluation of those brands, ( 3) the moment of purchase, and ( 4) the consumption experience. To win market share during these four key phases, firms are advised to provide customers with ""decision support,"" including ( 1) brand advertising and content marketing during the initial consideration phase, ( 2) interactive website tools for the active evaluation phase, ( 3) in-store advertising and special offers at the moment of purchase, and ( 4) informative packaging and service updates to enhance the consumption experience. Winning customers over during these four phases increases the likelihood that customers will return to the firm for future purchases when retriggered. Subsequent Service Cycles in the Smooth Journey ModelFollowing the initial service cycle, firms are advised to streamline the customer journey ([28]) by ( 1) eliminating unnecessary steps (or simplification), ( 2) anticipating customer preferences (or personalization), and ( 3) providing just-in-time support (or contextualization). Such streamlining techniques facilitate predictable as well as convenient, easy, and satisfying customer experiences ([34]; [49]; [57]). Even more importantly, these techniques enroll customers into a routinized or automated cycle of retrigger, repurchase, and reconsumption experiences known as a loyalty loop. The loyalty loop is named as such to emphasize that customer loyalty builds every time the service meets customer expectations ([21]). In the best-case scenario, the brand becomes a trusted provider, and the customer in turn becomes a brand advocate ([60]). Termination Trajectories in the Smooth Journey ModelLoyalty loops are generally visualized as infinite cycles ([21]). However, loyalty loops can come to an end following loyalty-weakening incidents, such as when the brand delivers poor service or when a competing brand offers a better service ([34]). Following such incidents, customers tend to follow one of two patterns. Whereas ""switchers"" reenter the deliberate decision-making process and choose an alternate brand, ""vulnerable repurchasers"" tentatively consider competing brands but end up repurchasing the incumbent brand for the time being ([20], p. 66). Toward an Alternate Sticky Journey ModelUnderlying the smooth journey model is a taken-for-granted assumption that firms should try to make customers' lives easier by creating consistent and predictable experiences ([21]; [28]; [49]). This assumption has a long history in marketing thought. For example, service research has long argued that predictability across service encounters is ""integral to consumer satisfaction"" because it ""increases cognitive control, minimizes risk, and reduces cognitive effort"" ([85], pp. 88–89). More recently, CXM research argues that touchpoint cohesion, consistency, and context sensitivity ""reduce the amount of time and effort customers must invest in living through a customer journey"" ([57], p. 556). Given this history, one can better appreciate why the smooth journey model assumes that customers always value predictable experiences.However, customers sometimes value unpredictable experiences. For example, entertainment research shows that dramatic serials with unpredictable plotlines (e.g., Game of Thrones) motivate binge-watching, whereas dramatic procedurals with predictable structures (e.g., Law & Order) are less captivating ([66]). Likewise, gambling research shows that unpredictable reward schedules are much more exciting than predictable ones ([81]). The ""intermittent wins"" of unpredictable reward schedules can produce ""states of arousal"" like a ""drug-induced high"" ([13], p. 491), motivating gamblers to keep on gambling and some gamblers to become addicted ([81]). Similarly, gaming research shows that unpredictable gameplay outcomes can be simultaneously ""enjoyable,"" ""frustrating,"" and thought-provoking ([50], p. 221), within and beyond playtime, ""keep[ing] players returning to the game"" ([17], p. 40). Today's video games (e.g., World of Warcraft) are even stickier than prior generations because of their greater unpredictability ([ 2]). The combination of expansive virtual worlds, massively multiplayer capacities, and evolving game objectives escalates the unpredictability as well as the excitement. Finally, consumer research on desire ([10]), extraordinary experiences ([ 4]), and repetitive decisions ([83]) also show that customers are much more likely to persist on a journey when they are not entirely sure what comes next. One reason is that the suspense is itself exhilarating ([32]). Another reason is that the need for resolution is strong ([83]).In summary, multiple fields of research indicate that predictable experiences satisfy customer expectations but also risk losing their attention. Meanwhile, unpredictable experiences keep customers excited and yearning for more but also risk fostering addictions. To put these insights in CXM terms: high (low) customer experience predictability facilitates smooth (sticky) customer experience journeys. Methods Research ContextsThe aim of this study is to develop a conceptual model of sticky journeys, including service design principles on the firm side and customer journey patterns on the customer side. To achieve our aim, we examine three brand contexts: CrossFit, Pokémon Go, and Tinder. Each of these brands features customer experience unpredictability as a core service attribute. Furthermore, each of these brands is well-known for being especially sticky in its respective service category ([63]; [65]; [78]). Together, the brands offer a mix of journey formats that help develop a generalizable model of sticky journeys. CrossFit journeys are largely offline, Tinder journeys are largely online, and Pokémon Go journeys are both.CrossFit is a group fitness regimen founded by Greg Glassman in 2000. The signature ""constantly varied"" workouts include gymnastics, weightlifting, and bodyweight exercises in well-equipped indoor-outdoor servicescapes called ""boxes."" Athletes are encouraged to strive toward increasingly higher levels of fitness, measured in terms such as reps, weight, and time ([23]). CrossFit is a multi-billion-dollar brand ([72]), growing from 13 affiliates in 2005 to more than 15,000 affiliates worldwide in 2019 ([23]).Pokémon Go is an augmented reality mobile video game released by Niantic in 2016. Drawing on Google Maps data and the global positioning system, the app reveals a dynamic virtual reality world in players' own local surroundings. Players hunt for virtual fictional creatures (Pokémon) that appear unpredictably and marshal those creatures in subsequent gameplay activities such as battles and raids ([69]). Pokémon Go was the fastest mobile app to reach $1 billion in revenue ([68]), and ""more cumulative time is spent playing Pokémon Go than any other [mobile] game"" ([ 5], p. 3).Tinder is an online dating app launched by Hatch Labs in 2012. Based on user locations and preferences, Tinder presents users with a seemingly infinite supply of other users' profiles. Tinder users can swipe right on profiles to express interest, swipe left to express disinterest, swipe up to express high interest, and chat with ""matches"" (i.e., users who have expressed mutual interest; [91]). Tinder is among the highest-grossing nongaming apps worldwide ([88]) and ""the most-used dating app in the UK and the US"" ([45], p. 1). Data CollectionThe first author collected the data using an ethnographic combination of experiencing via participant observation, enquiring via in-depth interviews, and examining via archival research ([97]). The majority of this data collection occurred in the United Kingdom between 2016 and 2019. Some data were also collected in North America and continental Europe. ExperiencingTo experience the stickiness of the services directly, the first author exercised at three different CrossFit boxes, played Pokémon Go to a moderate level of proficiency, and swiped through dozens of Tinder profiles. On his Tinder profile, the first author displayed his real name, university affiliation, and research intent. Communications were focused on the research project. Tinder users who expressed other interests were unmatched to avoid confusion ([53]). Field notes about these immersive activities amounted to 185 single-spaced pages. All descriptions of the three services in this article are based on these observations, except where otherwise noted. EnquiringUsing social networking and snowball sampling, the first author recruited 40 informants who have customer experience with one or more of the three services. Five informants also have provider-side experience at CrossFit as owners or coaches, and four informants also have gaming or technology expertise. These nine informants are more likely than other informants to use industry jargon in their stories, but their journeys in a customer role are similar to those of other informants. Of a total of 43 distinct customer journeys culled from the interviews, 13 journeys pertain to CrossFit, 19 to Pokémon Go, and 11 to Tinder. At the time of the interview, some informants had just begun using the services a few weeks prior, while others had been customers for several years. Eleven of the 43 journeys included discernible termination trajectories. The informants are mostly white and middle class but vary in terms of age (16–59 years) and gender (18 female, 22 male). Interviews were conducted in person or by telephone, ranging from 30 to 172 minutes (83 minutes on average). Interviews were loosely structured around five areas of inquiry: ( 1) the informant's everyday experiences with the focal service (e.g., how the service enters and exits their day); ( 2) their long-term journey with the service (e.g., how they got started, what keeps them interested, when they lose interest); ( 3) their experiences with competing services, if any; ( 4) their recollections of significant moments or time periods; and ( 5) the life contexts surrounding these service experiences. The audio-recorded interviews yielded 1,464 single-spaced pages of transcribed text. Informants that are quoted in this article are renamed for confidentiality and their quotes are edited for clarity. Quotes from foreign language speakers are translated into English. ExaminingUsing keyword searches and a custom Google feed, the first author collected publicly available materials about the three services, including websites, press releases, industry reports, and news articles, from mainstream media (e.g., The Guardian) as well as niche media (e.g., Wired). These data include announcements of service updates and upcoming events, newsworthy customer experiences, and industry leader perspectives. In total, the archival data set amounts to over 200 documents, about 20 of which are cited in this article. Data InterpretationOur interpretive process consisted of three iterative activities: making constant comparisons across our informants' lived experiences to discern common patterns; creating memos of our preliminary insights to debate within the research team; and tacking back and forth between the existing literature and our emerging understanding to crystallize our theoretical insights ([ 3]). We drew on different types of data to discern firm-side and customer-side insights. Specifically, we drew on firm-side fieldnotes and archival materials to discern the service design principles, and customer-side fieldnotes and interview transcripts to discern the corresponding customer journey patterns. To trace the evolution of sticky journeys, we compared journey patterns in the initial, subsequent, and terminating service cycles of customer journeys across the three research contexts (see the Appendix). As is often the case in interpretive research, no single informant provides a complete view of the phenomenon. Rather, that complete view emerges from a critical mass of empirical snapshots. We terminated our interpretive process at theoretical saturation, when new rounds of data interpretation did not meaningfully alter the emergent model. For an overview of the extant and emergent journey models, see Table 1 and Figure 1.GraphTable 1. A Comparison of the Smooth and Sticky Journey Models.  Graph: Figure 1. A visualization of the smooth and sticky journey models. The Sticky Journey Model The Initial Service Cycle in the Sticky Journey Model Rapid entry: the service design principle in the initial service cycleFirms nurture smooth and sticky journeys differently. At the beginning of smooth journeys, firms support the customer's deliberate decision-making process with considerable decision support. By contrast, at the beginning of sticky journeys, firms attempt to eliminate customer decision making altogether by giving customers immediate access to the service. As our informants reveal subsequently, their CrossFit, Pokémon Go, and Tinder journeys tend to begin on a whim, motivated by the promise of fun. Accordingly, the most appropriate firm action at this juncture is to give potential customers a taste of the excitement to come, as soon as their curiosity is sparked.Many CrossFit boxes, for example, offer newcomers a free beginner class, followed by an affordable beginner plan (e.g., a low-cost one-month membership). Unlike traditional gyms, CrossFit gyms do not greet newcomers with gym tours, salesperson interactions, or a complex menu of service plans, which necessitate deliberate decision making. Pokémon Go's virtual moderator, Professor Willow, orients new players via a rapid sequence of fun and easy steps. Players learn the game's mission via short-text snaps, customize their avatar with a few clicks, and catch a trial Pokémon with a couple of swipes. Unlike dating services that begin with extensive questionnaires (e.g., eHarmony), Tinder only asks new users for their gender, distance, and age preferences ([91]). Users can import photos into their Tinder profiles from Facebook and begin swiping through potential matches immediately. As commentators have noted, ""Tinder's most revolutionary aspects were to nix the web[sites] and questionnaires"" ([78], p. 2).We conceptualize these speedy onboarding techniques as the service design principle of ""rapid entry."" This conceptualization highlights the expediency with which firms facilitate the beginnings of sticky journeys. As soon as potential customers visit a service entry point, firms rapidly offer exciting service experiences. Conspicuously absent are the tedious entry practices of most service industries (e.g., complex menus of purchase options, extensive questionnaires, servicescape tours). If customers cannot experience the excitement of a service quickly, easily, and for free, they may turn their attention to something else that is more immediately accessible. (For additional examples of the rapid entry principle, see the Appendix.) Quick spin: the customer journey pattern in the initial service cycleThe initial customer experiences in smooth and sticky journeys are remarkably different. Smooth journeys begin with a highly deliberate, multiphase decision-making process. Prior to our research, we expected that sticky journeys would also begin with some sort of decision making. However, contrary to our expectations, we find almost no deliberate decision-making process among our informants. As Dora, a Tinder user puts it, ""I didn't do proper research."" Instead, most of our informants begin their journeys on a whim, after receiving enthusiastic reviews, or observing customers enjoying themselves.[My Bootcamp instructor] said to me: ""CrossFit, that's something you'll like.""...And then a neighbor told me she had started at [a local box] and invited me to come by and give it a try....I went with her and did a couple of regular workouts. Then I attended a beginner's introduction...which was great, answered a couple of questions, and then we were thrown into it!"" (Karen, CrossFit athlete)My brother tells me, ""You walk around the city. And you pick up Pokémon."" I'm like, ""That is amazing. I definitely want to do that.""...I walked around London for the whole afternoon and I was, like, ""I've never seen that statue before! I live five minutes away!...Thank you Pokémon Go for that interaction with my environment."" (Aron, Pokémon Go player)When Tinder first came out, I was still in a relationship, so I never really played it, but I saw my mates play it, and I thought the idea of it was amazing in the sense that you literally just swipe, ""Yeah, I think she's hot!"" or ""No, not for me!"" And then if you did get a match out of it, I think that's hilarious, but I wasn't able to [try Tinder at that time]....When I became single...I was like, ""All right, let's see what the hype's about.... This is definitely a game changer!"" (Charles, Tinder user)As these vignettes indicate, CrossFit, Pokémon Go, and Tinder journeys begin with sparks of curiosity about the focal service, rather than an active evaluation of multiple brands. These sparks of curiosity are often ignited by highly enthusiastic word of mouth from family (Aron), friends (Charles), and acquaintances (Karen). Such word of mouth excites our informants only if the service complements their already existing life projects. For example, Karen is already a fitness enthusiast when she hears about CrossFit, and Aron is already a passionate gamer when he hears about Pokémon Go. Charles hears about Tinder when he is in a relationship, so he does not download the app immediately, but soon after he becomes single again. Some informants are also exposed to these services through advertising, news, and social media, but regardless of their sources, informants answer these calls to adventure because the promise of fun is compelling and the hurdles to entry are minimal. Of course, services must deliver on the promise of fun for customers to want to continue the adventure. Karen relishes her first CrossFit class, Aron rediscovers his neighborhood through Pokémon Go, and Charles finds Tinder to be ""a game changer!""We conceptualize the initial service cycle of sticky journeys as a ""quick spin"" to emphasize not only the lack of deliberate decision making but also the rapid transitions from observed excitement to anticipated excitement to realized excitement. Although customers intend to try the service briefly, once they experience the exciting service firsthand, they have so much fun that they are often swept up into subsequent service cycles, again without much deliberation. In other words, what starts out as a ""test drive"" turns into a ""joy ride"" that turns into a ""road trip."" (For additional examples of quick spins, see the Appendix.) Subsequent Service Cycles in the Sticky Journey Model Endless variation: the service design principle during subsequent service cyclesService design principles diverge even further in the subsequent service cycles of smooth and sticky journeys. The smooth journey model advises firms to streamline the customer journey such that subsequent service cycles are as consistent, easy, and predictable as possible. In stark contrast, CrossFit, Pokémon Go, and Tinder focus on providing customers with infinitely variable configurations of a core service experience. Delivering such ""endless variation"" along the customer journey depends on at least three concrete service design features: ( 1) the expansiveness of the service system, ( 2) the open-endedness of the service system, and ( 3) the uniqueness of each service encounter.One essential design feature is a highly expansive set of service system elements. For example, CrossFit workouts combine innumerable exercises from global athletic traditions (e.g., handstands, muscle-ups, power squats) in a blended indoor-outdoor gym equipped with considerable workout gear (e.g., jump ropes, kettlebells, pull-up bars). Similarly, the Pokémon Go game includes hundreds of Pokémon; elaborate reward structures, including coins, medals, and points; and countless real-world locations, where players can collect game-relevant items (""PokéStops"") and battle rival teams (""Gyms""). Thanks to Tinder's rapid growth to millions of active daily users ([59]), the app presents users with a virtually infinite supply of potential matches, and once matched, users can exchange unlimited private messages.A second essential design feature is openness to the addition, subtraction, and transformation of firm-owned, customer-owned, and external service elements. For example, CrossFit boxes design novel workouts daily, coaches add their own flair, and athletes exercise with various partners at different skill levels. Meanwhile, Pokémon Go keeps adding new creatures, features, and events, some of which are time-limited (e.g., Halloween Pokémon events), environment-based (e.g., the dynamic weather gameplay system), and community-dependent (e.g., group raids). Tinder too regularly introduces exciting new features (e.g., Top Picks, Swipe Night, Tinder Gold). Moreover, Tinder's pool of active daily users is constantly changing as new users join the app and existing users take a break.A third essential design feature is the service system's capacity to perpetuate unpredictable service experiences, even for seasoned customers, by foregrounding a unique configuration of service elements for the customer at every service encounter. For example, every CrossFit workout is a unique mix of aerobic/anaerobic, individual/partner, and indoor/outdoor exercises in varied temporal configurations. Every Pokémon Go walkabout is a unique mix of gameplay activities such as catching varied Pokémon, battling opposing teams, and conducting group raids. Every Tinder session is a unique mix of swiping through new profiles, advancing conversations with matches, and planning off-platform dates. In this manner, no two CrossFit workouts, Pokémon Go walkabouts, or Tinder sessions are ever the same ([16]; [38]; [63]). (For additional examples of the endless variation principle, see the Appendix.) Involvement spiral: the customer journey pattern during subsequent service cyclesIn the smooth journey model, the customer journey pattern during subsequent service cycles is a cyclical pattern of predictable experiences that increases customer loyalty over time, thus the name loyalty loop. By contrast, the customer journey pattern during subsequent service cycles of CrossFit, Pokémon Go, and Tinder is a cyclical pattern of unpredictable experiences that increases customer involvement over time. We conceptualize this pattern as an involvement spiral (see Figure 1). From a conceptual standpoint, the involvement spiral has two noteworthy patterns, one in the moment-to-moment timescale of the customer journey, the other in the long-term timescale of multiple service cycles.In the moment-to-moment timescale of the customer journey, the involvement spiral entails a variegated pattern of thrilling and challenging experiences that we describe as an ""experiential roller coaster."" Such an unpredictable pattern of positive and negative experiences, including emotions of anticipation, dread, amazement, disappointment, and enjoyment, keeps customers in a state of high psychological arousal; in their highly aroused state, customers become highly attuned to the multidimensional intricacies of service experiences ([ 4]; [13]; [17]).In the long-term timescale of multiple service cycles, the involvement spiral entails an upward trend in customer involvement that we describe as increasing ""experiential involvement."" Here, our composite notion of experiential involvement refers to customer involvement (i.e., interest, excitement, and investment) in the customer experience (i.e., the cognitive, emotional, sensorial, behavioral, and relational responses to a service) ([79]; [96]; [98]). Increasing experiential involvement does not imply that customers spend more time on the service each day. Rather, it implies that customers become more deeply invested in the multidimensional intricacies of their service experiences. With each successive cycle of the customer journey, customers also acquire new service-relevant competencies, including new insights, mindsets, and skills ([ 2]; [18]; [32]). Given the centrality of the involvement spiral to the sticky journey model, we next empirically illustrate this journey pattern in each of our three service contexts. The involvement spiral at CrossFitCrossFit's core service is a one-hour group-training class. The prototypical class includes a warm-up, a weightlifting segment, and a workout of the day (WOD). The warm-up is customized daily for the segments that follow. Warm-ups include static stretches (e.g., the hip-flexor stretch), dynamic stretches (e.g., the side shuffle), and other creative activities (e.g., push-ups to the beat of a pop song). Next, the weight-lifting segment might combine multiple exercises or focus on one compound exercise (e.g., the clean-and-jerk). The target number of rounds and repetitions are posted on a large screen, but athletes scale the weights to their current abilities. Coaches often encourage athletes to beat their own personal record. Finally, the WOD is the fastest-paced segment of the class. A WOD can include not only weight-lifting movements but also gymnastics and bodyweight exercises (e.g., pull-ups, rope climbs, lunges) and metabolic conditioning (e.g., running, biking, rowing). Overall, CrossFit classes can feel easier or harder depending on a host of factors such as the athlete's current abilities, the competitiveness among attendees, or even the weather conditions. Some CrossFit boxes post the workouts online the night before, and some athletes take a peek at those workouts in advance to jump-start their excitement. Other athletes, like Alan, take pleasure in the suspense of not knowing what comes next.Interviewer:What makes you want to go to CrossFit again?Alan, CrossFit athlete:It's the un-knowing of what you're going to do that night, because you're not really supposed to know.... You go to the gym the night before, you do a horrible workout, but you love it.... It makes no sense, because why would you love something that's horrible?...But you've worked up a sweat because it's horrible. And then you're like, ""Well, I'm going to book [a class], because if I know what it's going to be tonight, I won't turn up,"" and that's why, that's the beauty of it, because you don't know, so you've got to go to find out. It's like a present. If you get a present, if they just tell you, you're not going to be excited...[but] if it's a surprise, then when you open it, you're excited. You're amazed by what you've got. And that is literally the beauty of just going to a CrossFit class, because every day, you're like, ""I'm going to go tonight"" because you are so excited to see what the workout is. It could be amazing, it could be bad, but you still get excited....It's like swings and roundabouts really.Alan's words nicely illustrate why the endless variety of CrossFit classes can feel like an experiential roller coaster. There are moments of anticipation (""it's the un-knowing""), surprise (""it's like a present""), and reflection (""but you love it""). Classes can be ""amazing"" or ""horrible,"" but regardless, they always get one ""excited."" Simply stated, the journey is a mix of positive and negative moments (""it's like swings and roundabouts""). We use the conceptual metaphor of the experiential roller coaster to describe the moment-to-moment experience of the sticky journey because it encompasses the full spectrum of experiential dynamics: the ""peaks"" of pleasurable experiences, the ""valleys"" of painful experiences, the ""climbs"" toward peaks, the ""dives"" into valleys, and the ever-present suspense about what's around the next turn.At the same time, a sticky journey is no mindless roller coaster; rather, it is one that continually shifts customer attention to the many possible connections between the service experience and one's own life goals. In this manner, a sticky journey invites greater experiential involvement over time. For example, many CrossFit informants speak of developing greater physical and psychological mastery through CrossFit's workouts.If you are not good at something, it takes a lot for you to dedicate your time to want to be better at it. And I think CrossFit is the only [fitness regime] that has made me do that. I hate squatting, I hate doing anything like that. And I am forced to do it at CrossFit...[and] that's really good for my hips and my back, and as I get older, that movement is really important....When you are like, ""I don't know what I'm doing, I don't know what this activity is?,"" watching [other CrossFit athletes] do it sort of helped me remember the technique, so I was like, ""Okay, so when I need to squat, for example, I should be getting that low.""...The more you watch...the better you'll be. (Jenny, CrossFit athlete)In this vignette, Jenny describes one meaningful trajectory of her CrossFit journey as overcoming her psychological barriers to the compound exercise of squatting. Jenny is an intermediate athlete who still has much to learn, but unlike a beginner, she has become aware of the general importance of good form (""I should be getting that low""), the specific functions of different exercises (""good for my hips and my back""), and the potential linkages between her CrossFit activities and long-term goals (e.g., staying fit as she ages). We interpret this tendency of customers to become more deeply invested in the intricacies of service experiences as increasing experiential involvement. The involvement spiral at Pokémon GoPokémon Go has an elaborate game structure, including 40 game levels; rewards such as bronze, silver, and gold medals; and different point allocations for different in-game actions. Pokémon tend to appear unpredictably and for brief time spans, thus motivating the gamer to catch them immediately. The game's tagline, ""Gotta catch 'em all,"" refers to the goal of catching every type of Pokémon by throwing PokéBalls at them. Commentators have noted that ""each capture session...each walk a player goes on...is unique"" ([63], p. 4). Although players can perform select game actions without walking around (e.g., reviving fainted Pokémon), most game actions require walking or other modes of travel. Collectively, these various triggers, actions, and rewards during each Pokémon Go service cycle (or walkabout) generates considerable excitement for players.When I went out with my daughter, and we go, ""Oh, there's an egg about to hatch."" And we gather round and look at it and go, ""Oh no, it's a [common Pokémon]!"" [laughs]. And then, we get excited about another one! It's the medals. I have walked 1,502 kilometers....[There's] a lot of unique goals and different routes you can go through. [Niantic] keeps releasing new features.... They have Pokémon only released in certain countries, so when I'm in America, I'm catching American Pokémon. It's quite exciting....Some are incredibly difficult to find, and you get very excited when you find one. And some are legendary. The legendary ones you couldn't find anywhere.... It's really exciting cause it's time-limited, so if you want to complete your Pokédex...you've got to get [the released Legendary Pokémon].... You've got to find a Gym that's got one....You've got to take part in a raid. The raids themselves are time-limited. And you can't win a raid unless you've got about ten people there. (Ruth, Pokémon Go player)Ruth derives pleasure from Pokémon Go's varied gaming activities (catching Pokémon, hatching eggs, group raids) in varied social constellations (alone, with her daughter, in groups) at varied real-world locations (in the United Kingdom and the United States). Like other informants, she experiences an unpredictable sequence of thrills (""Oh, there's an egg"") as well as challenges (hunting for ""incredibly difficult to find"" Pokémon), making the moment-to-moment Pokémon Go journey feel like a roller coaster ride.Further analysis of the Pokémon Go data set reveals that informants' journeys also evince increasing experiential involvement across multiple walkabouts.I walked past a PokéStop...[and] I was like ""Oh, let me try and catch [a Pokémon], see what happens,"" and before I knew it I was catching them and then trying to figure out which ones were better to catch and which numbers were good...and learning that stuff. I went back to work after the summer and there were lots of PokéStops and [other players] wanted to get walking so that they could hatch the eggs. I thought, ""I walk a lot while I'm at work, I go from one building to the other and back again."" So when I'm out...I can have it on.... Every night when I get home, [my son] would check how much I'd walked and which Pokémons I'd got. I found myself using it more and more.... Because there are still challenges in Pokémon Go, because new Pokémon appear, because there's rare ones, or trying to get one to the maximum level, that stuff, it gets me interested.... I'm not done with this, there are Pokémon to get, there are achievements to achieve, medals to get. (Daniel, Pokémon Go player)Daniel's vignette illustrates how informants can get swept up into the involvement spiral of sticky journeys without any explicit intentions to do so. He initially downloads the game as a family pastime, then continues playing the game on his own. Like many other players, Ruth included, Daniel soon incorporates playtime into his daily walking routines, connects with fellow players, and finds himself playing Pokémon Go ""more and more."" Although his time spent on the app does not increase indefinitely, his experiential involvement during his playtime keeps increasing. He hunts for different, new, and rare Pokémon; powers them up to their maximum levels; and continually learns new ways to earn rewards. His end game is a ""moving target"" ([63], p. 5). Over weeks, months, and sometimes even years of playing the game, informants such as Ruth and Daniel become increasingly well-versed in the game's numerous intricacies, which in turn increase their enjoyment of the game. The involvement spiral at TinderDeparting from traditional matchmaking services that connect customers based on compatibility questionnaires ([33]), Tinder thrusts users into an ""open"" stream of fellow users' profiles ([91]). Anna, a Tinder user, describes the resulting experience: ""Tall men, small men, fat men, thin men, poor [men], rich [men], doctors, gardeners, and everything! You really see a big cross-section of society. And that was super exciting!"" Tinder also includes a messaging stream for matched users to get to know one another, schedule off-platform dates, and keep in touch for as long as there is mutual interest. These two main streams of user interaction generate Tinder's experiential roller coaster.I was going back home, and instead of sleeping, I was spending an hour, and I was saying, ""Okay, it will be the next one that I might like, it will be the next one,"" but no, it wasn't....In the morning, if someone liked my profile, if I was finding it interesting, I would say ""Hello, good morning,"" stuff like that, and then I would try to initiate a discussion.... It was really addictive. In the morning, I might lose, like, 10–15 minutes to see what's happening, who liked me....Sometimes the application shows you profiles first, and then, if the other person likes you, it will appear in your profile as a match. But there were times that I would like someone, and he had liked me first, so I will talk with them straight away. That was when I would text someone more often. (Sophia, Tinder user)For Anna, Sophia, and other Tinder informants, swiping through profiles is a psychologically arousing process with moments of suspense, delight, and frustration. Users only see one profile at a time in the default swiping channel (""Discover""). They must swipe right to ""Like,"" swipe left to ""Nope,"" or swipe up to ""Super Like"" before the next profile is revealed. In Sophia's journey pattern of ""obsessively swiping through Tinder"" ([26], p. 1), she follows each ""Nope"" with a wish that ""it will be the next one"" that she might ""Like,"" followed by a near-immediate revelation of whether her wish has come true or not. Matching with a few users and chatting with them injects new variety into her experiential roller coaster, rendering the overall experience ""really addictive."" Tinder informs a user about a match as soon as two users have liked one another. Sophia's urge to check the app as soon as she awakes indicates that the suspense she experiences while swiping also endures through the matching and messaging process. Intense feelings of desire and disappointment can occur for informants even before they have scheduled any off-platform dates ([ 7]).As informants keep swiping through profiles, communicating with matches, and going on dates, their experiential involvement increases, albeit without any explicit reward structure. Unlike Pokémon Go, Tinder does not award points for successful plays, and unlike CrossFit, Tinder does not chart performance metrics on scoreboards. After all, ""'success' in online dating can mean many things to many people"" ([78], p. 3). Even so, the Tinder journey does have an implicit reward structure: the quantity and quality of one's matches, chats, and dates, which users interpret subjectively. Many informants also express personally meaningful developments, such as a growing self-awareness about their own relational desires and an increasing ability to understand and respond to matches.[The] fruits from Tinder come out only with constant use.... At the beginning, I would invest more time chatting with some specific people, while now, I'm much more direct. Also, because it's a matter of numbers, in the sense that after a while, you get more matches. You basically spend less time on average with every person.... My philosophy is chat a little bit, and if you see that there is some kind of common ground and chemistry that you can feel at the very beginning, just by texting someone, then my next proposal is ""Okay, let's meet!""...How people reply, how people write you, you can really get an idea, more or less, of the kind of person it is. There are people who are very funny and start making jokes, or tell you something different, or something more clever, while other conversations [are] more standard, boring ones. (Roberto, Tinder user)Over the course of his Tinder journey, Roberto refines his approach in several ways. For example, he learns to start swiping during the week to arrange a date for the weekend. He abbreviates unnecessary conversations with a ""more direct"" style. He becomes quicker at recognizing the ""kind of person"" he is chatting with based on their texting style. From week to week, Roberto also gets more matches, juggles more conversations, and enjoys more dates. Such increasing experiential involvement in the intricacies of the Tinder journey allows him to become more efficient, effective, and even philosophical about dating. (For additional examples of involvement spirals, see the Appendix.) Termination Trajectories in the Sticky Journey ModelSmooth journeys are generally visualized as infinite loyalty loops. However, in reality, smooth journeys can and do come to an end. Loyalty-weakening incidents, such as poor service experiences and attractive competitor offerings, can trigger customers to reenter the deliberate decision-making process and switch to a new brand. Sticky journeys, by contrast, tend to terminate with service usage fluctuations fueled by well-being concerns. Sometimes, sticky journeys also terminate for brand-specific reasons. Service usage fluctuations fueled by well-being concernsWe observe that some of our informants begin to question whether to continue their sticky journeys when those journeys start to feel addictive in the pathological sense of the term (i.e., the service discernibly conflicts with the customer's own sense of well-being; [86]). In these instances, informants tend to withdraw from the service, either gradually or suddenly. Often, they repatronize the service, then withdraw again. Christine's dissonance about continuing her CrossFit journey stems from its overly enthusiastic culture.I did it quite intensively until Christmas....And then I did it a bit less. Somehow, I could not motivate myself to go as often....But for four months, really intense, and then three months...not quite so intense. Then, when I went home, I actually stopped it....What rather scared me is the fanaticism that many have.... I thought, ""Okay, that's not my world, as far as I'm concerned.""...It's very important to me to become fit and stay fit, but only to a certain level. (Christine, former CrossFit athlete)As a former competitive athlete, Christine is well aware of how fitness and health concerns can become all-consuming over time. For her, the CrossFit journey is fun ""to a certain level,"" but she reaches that upper limit after several months of enthusiastic participation. By contrast, that upper limit comes very early in Aron's journey with Pokémon Go.Downloaded it, walked around, saw the historical sites that are within it, the PokéStops, it tells you little things about what might be on the street. Loved it, did it for four or five hours and deleted it, because...I will do this way, way too much.... I definitely need to consume fewer video games. (Aron, former Pokémon Go player)Aron's concerns about the addictive potential of Pokémon Go arise within a few hours of playing the game. To put this episode in perspective, Aron is an avid gamer who has preexisting concerns about keeping his playtime in check. Accordingly, he deletes the app the very same day he starts playing. However, following this episode, Aron downloads the app again and plays the game for a few more weeks, before giving it up for a second time. Whether users take mere hours or several years to reach their upper limit of the involvement spiral, they nonetheless express the same general concern about the addictive potential of sticky journeys.It's very addictive....I would spend a lot of time.... It was like...an addictive game, so in order to stop using it, at some point, I just deleted it, and it worked fine....If I don't want to do something, I'm trying to not have Sirens around me. (Sophia, former Tinder user)Sophia tries to use the Tinder app less at first but eventually decides that deleting the app is the only way to cope with its addictive potential. In telling her story, Sophia draws on the myth of the Sirens—beautiful-voiced but dangerous creatures who lure gullible sailors to shipwreck themselves on the Sirens' island. In some versions of the myth, sailors plug their ears so as not to hear the Sirens' call. In a similar vein, Sophia blocks out the call of Tinder by deleting the app. Of course, not all informants terminate their journey when well-being concerns arise.I'd always want to keep training and training, but I think with experience, I've learned to say...""Just take a week, let your body recover a little bit."" And our coach is quite good at saying, ""If you're tired...then take the week off. It's not going to do any harm and, if anything, you'll benefit from it."" (John, current CrossFit athlete)Unlike Christine, Aron, and Sophia, John simply takes time off when his well-being concerns arise, suggesting that some informants are better at self-regulation than others. (For additional examples of service usage fluctuations fueled by well-being concerns, see the Appendix.) Brand-specific termination trajectoriesSticky journeys also fluctuate or terminate for brand-specific reasons (e.g., physical injuries at CrossFit, boredom with Pokémon Go, relationship status changes in Tinder). In the context of CrossFit, athletes can get injured while participating in high-intensity workouts. For example, Olivia recalls being ""surrounded by individuals who were a hell of a lot fitter than me...looking at them as my role models and icons, going, 'I can do that if I want to.'"" However, her journey came to a sudden stop: ""I did too much too soon....And then, as a result, I got injured....I fell off the rig and broke my elbow."" Two years after this ""breaking point,"" she resumed CrossFit. In the media, controversy over the ""cultish"" nature of CrossFit focuses on such ""overuse injuries [that] are not uncommon among CrossFitters"" ([38], p. 2). Many in the industry are ""wary"" of the fitness regime because of its ""risk of injury and drop out"" (Denoris, in [38], p. 2).In the context of Pokémon Go, boredom is a common theme. For example, Aron says, ""I've put enough hours into this, every egg that hatches is the same, every Pokémon I find is the same, I'm bored."" Timothy too stops playing for several months because the journey eventually loses its appeal: ""I walked a 100 kilometers to get a [specific Pokémon]. And it was not even a good Pokémon....That was a chore, and that did feel boring....I was like, 'No, I don't have to do this,' and so I stopped."" Informants' waning interest in the first year of the game's launch corresponds with Niantic's delay in effectively deploying endless variation across the customer journey, ironically due to the overwhelming success of the game. As chief executive officer John Hanke noted, ""We had to redirect a substantial portion of the engineering team to [work on] infrastructure versus features....I'd say we're about six months behind where we thought we would be"" ([94], p. 2). When Niantic launched Generation 2, some of our informants enthusiastically returned to the game. As Jill says, ""[Niantic] introduced Generation 2 at just the right moment for me, because it piqued my interest again!""In the context of Tinder, journeys terminate when users wish to settle down with one partner, and then do not find one despite significant effort, or do find one. Former Tinder user Enrico withdrew from Tinder for each of these two reasons. After many ""dead [end] conversations"" with matches, ""[I] felt disengaged with the application, as I was not achieving anything in particular,"" and ""at some point I decided to uninstall the application."" However, Enrico rejoins Tinder about 18 months later, when his friends encourage him to ""go on Tinder and try to have fun."" This time, being ""more mature in the use of the application,"" and having ""fate"" on his side, he matches with someone that he falls in love with, prompting another uninstallation of the app: ""since things were almost done, I also decided to uninstall Tinder."" Theoretical Implications Challenging the Dominance of the Smooth Journey ModelPrior CXM research on customer journey design is too quickly converging around the smooth journey model, without adequately interrogating its underlying assumptions. The smooth journey model is certainly useful but only in terms of maximizing hyperrational factors such as consistency, effortlessness, and predictability. As our findings highlight, customers also sometimes yearn for the excitement of unpredictable journeys, if only to temporarily escape their otherwise hyperrational lives. Accordingly, in this article, we have developed an alternate journey model that is premised on the excitement of unpredictability. This model explains how firms can design sticky journeys that customers yearn to continue. Each of the two models advocates for a unique set of service design principles and customer journey patterns (see Table 1). In essence, the smooth journey model helps customers to make an informed decision, then fall into a comforting, trust-building routine (i.e., a loyalty loop). By contrast, the sticky journey model yanks customers onto an experiential roller coaster ride that increases customers' experiential involvement over time (i.e., an involvement spiral).A caveat for CXM researchers is that both journey models are ideal types (i.e., tidy abstractions of messy realities; [93]). Real-world customer journeys are never wholly predictable nor wholly unpredictable. Most services facilitate a mix of predictable and unpredictable experiences. What distinguishes the two journey models is the relative emphasis on high versus low customer experience predictability. Furthermore, all journeys are interrupted and interwoven in customers' everyday lives. No journey unfolds in isolation from all others. These caveats aside, journey models are valuable as ""cultural mindsets"" for coordinating CXM activities across organizational stakeholders ([46], p. 385). Figure 1 can help customer experience officers (CXOs) coordinate all customer-facing departments in a firm toward a shared vision of the customer journey. If that vision is a sticky journey, then the notion of an involvement spiral can help CXOs to emphasize the importance of ( 1) keeping customer experiences unpredictable in the moment-to-moment timescale, and ( 2) increasing customer opportunities for experiential involvement across successive service cycles. Connecting Sticky Journeys to Other Marketing ConceptsThe emergent concept of sticky journeys is related to several existing marketing concepts (see Table 2). Among these concepts, customer involvement ([98]) is the most central to understanding sticky journeys. As sticky journeys evolve, customers become increasingly involved in the service experience. Given that involvement is a decades-old construct with several variants (e.g., product, brand, and purchase involvement; [ 8]), we emphasize that experiential involvement is the most appropriate concept for our model as well as CXM research at large. As journeys evolve, customers may also become more engaged in the sense that they begin to contribute direct and indirect value to the firm. However, such customer engagement ([73]) is not necessary for journeys to be sticky. Journey stickiness can be distinguished from customer loyalty in both behavioral and affective terms. When customers regularly consume one brand in a service category, out of a sense of commitment, that repatronage is best conceptualized as loyalty ([71]). However, when customers frequently return to a service, out of a sense of excitement, that repatronage may be better conceptualized as stickiness, which does not imply brand exclusivity.GraphTable 2. Sticky Journeys and Related Marketing Concepts.  Consumer desire is a type of consumer motivation that is much more energetic, passionate, and urgent than need or want ([10]). Our study indicates that customers do not need or want their sticky journeys to continue but urgently desire such continuity. However, when sticky journeys become compulsive or pathological, they may be better conceptualized as consumer addiction ([86]). Finally, extraordinary experiences are highly positive and infrequent experiences ([ 4]). Sticky journeys, by contrast, entail a variegated pattern of positive and negative experiences in quick succession. All of these interrelated marketing concepts point to customer interests in something more than efficient service experiences, but that 'something more' varies across these seven concepts. Only the concept of sticky journeys denotes a cyclical pattern of unpredictable customer experiences, with increasing experiential involvement, that customers yearn to continue. Practical ImplicationsThe CXM literature generally advises firms to design smooth journeys. With the rising popularity of sticky journeys, three new practical questions arise: ( 1) How should CXM practitioners choose between the smooth and sticky journey models? ( 2) Within each journey type, when should firms encourage purchases—during the initial or subsequent service cycles? ( 3) How can firms interlink loyalty loops and involvement spirals to sustain customer journeys in multiservice systems? How to Choose Between the Smooth and Sticky Journey ModelsThe strategic choice between the two journey models boils down to whether the service is more instrumental or recreational in nature. In instrumental service categories, customers are like ""jobbers,"" trying to get their tasks done as efficiently as possible; thus, the smooth journey model is a perfect fit. In recreational service categories, customers are more like ""adventurers,"" looking for thrills, challenges, and fun times; thus, the sticky journey model is a better fit. Smooth journeys are ideal for instrumental service categoriesExamples of instrumental service categories include business hotels (e.g., Courtyard by Marriott), insurance (e.g., Progressive), and transportation (e.g., Amtrak). Customer journeys in these service categories are like ""jobs to be done"" ([19], p. 54). There are tiresome evaluation tasks (e.g., Are buses, subways, or trains the best transportation option for my commute?), difficult purchase decisions (e.g., Should I buy a cheaper nonrefundable ticket or a pricier refundable one?), and potentially significant consequences (e.g., delays, exhaustion, fees). Jobbers are generally willing to deliberate through the initial service cycle, but they expect subsequent service cycles to be easier. To win these jobbers, firms must provide superior decision support during the initial service cycle, then streamline subsequent service cycles into easy loyalty loops. Sticky journeys are ideal for recreational service categoriesExamples of recreational service categories include driving clubs (e.g., Jeep Jamboree USA), lifestyle media (e.g., Thrillist), and content-sharing networks (e.g., Instagram). Customer journeys in these service categories are more like adventures than jobs. A vaguely defined hunger for excitement leads to a series of unexpected twists and turns, and a sense of purpose keeps the customer moving forward, overcoming challenges in the process ([82]). Our research suggests that customers often consider such adventures on a whim, so firms must invest in rapid entry mechanisms, especially when the entry hurdles are significant. For example, instead of limiting Jamborees to Jeep owners, Jeep Jamboree USA could rent out Jeeps to potential Jeep owners who wish to join the driving adventures. Our research also suggests that customers will only continue their adventure if it remains exciting, so firms must also invest in endless variation mechanisms. For example, Jeep Jamboree USA keeps changing its adventure sites, from the Catskill Mountains of New York to the Death Valley of California. Thrillist has a global team of freelancers to cover the ever-changing nightlife of super cities (e.g., London, New York City, Paris). Instagram intentionally exposes users to new, personally relevant influencers (e.g., Jivamukti yoginis, Latinx actors, Turkish wrestlers) to keep customers scrolling. When to Encourage Purchases in Smooth and Sticky JourneysFirms today offer customers a variety of free, affordable, and expensive service access options, as well as one-off purchase opportunities. Free service at the outset of customer journeys can take the form of free sample sessions (CrossFit), free basic services (Tinder), or even free full services (Pokémon Go). Thereafter, some firms offer customers relatively affordable time-limited options, such as one-time passes (e.g., CrossFit's drop-in passes), package deals (ten-class passes), and short-term service plans (e.g., three-month plans). Most firms also offer monthly subscription plans, some of which are tiered (e.g., Tinder's Plus and Gold plans). Finally, some firms also offer customers one-off purchase opportunities (e.g., Pokémon Go raid passes). Firms that provide unlimited full service access for free (e.g., Niantic) rely on these one-off sales to generate revenue. All of these options can work with smooth or sticky journeys. However, to match the distinctive flow of each journey type, firms are advised to encourage purchases at different times within each journey type (see Figure 1). Encourage purchases during the initial cycle of smooth journeysFirms aiming to facilitate smooth journeys tend to showcase their complex menu of purchase options during the initial service cycle. For example, Verizon, a telecom service provider, promotes several possible phone plans on its website. One reason is that customers approach instrumental service categories with the mindset of a job to be done ([19]), and they are highly motivated to conduct a deliberate decision-making process. Another reason is that once customers complete that process, they do not want to be bothered by difficult choices again ([34]). From a customer's point of view, the value of a loyalty loop is to minimize the cognitively demanding labor of deliberate decision making. Accordingly, firms should avoid the common practice of promoting upgrades during a loyalty loop (e.g., advertising a new phone plan to existing Verizon customers). When firms do so, they run the risk of triggering customers to reconsider their prior decisions and switch providers altogether ([20]). Encourage purchases during the subsequent cycles of sticky journeysFirms aiming to facilitate sticky journeys should avoid presenting customers with complex menus of purchase options at the outset. One reason is that such menus are antithetical to the promise of fun, and they immediately dampen customers' excitement to try the service. Another reason to wait until well after the quick spin is that customers are most likely to make substantial purchases when they are already caught up in the involvement spiral. That said, firms must be patient. Each sticky journey is a unique adventure, so each customer will advance at their own pace. Firms such as CrossFit and Tinder recognize that customers feel ready to commit to premium plans at different times. Accordingly, these firms tend to enroll all newcomers into a free or affordable beginner plan, with little pressure to upgrade that plan until customers themselves seek premium plans. These firms also recognize the indirect value of nonpaying, low-paying, and short-term customers. Unlike instrumental services, recreational services thrive on having a sizable number of active customers within the servicescape at all times. For example, CrossFit thrives on a fleeting sense of hypercommunity, which requires a mix of core and peripheral community members to show up for workouts. Likewise, playing Pokémon Go is much more exciting alongside and against other players ([ 5]). Tinder, too, can only offer its users hundreds of potential matches if there are indeed hundreds of other users. As these examples indicate, recreational services often need a critical mass and steady turnover of users, regardless of whether those users are paying customers. For these reasons, recreational service firms (e.g., Grindr, Spotify, TikTok) often need angel investors, crowdfunding, and venture capital to survive the early years, when their revenue streams are limited. How to Sustain Customer Journeys in Multiservice SystemsMany large firms operate multiservice systems that include instrumental and recreational services. These firms must not only design the first loyalty loop or involvement spiral, but also sustain the customer journey beyond that existing loyalty loop or involvement spiral (see Figure 2). Firms that have customers simultaneously enrolled in multiple loyalty loops and involvement spirals are at less risk of losing their customers.Graph: Figure 2. Sustaining customer journeys in multiservice systems. Sustaining the customer journey beyond an existing loyalty loopWhen a firm already has customers enrolled in one loyalty loop, CXM practitioners can expand on that loyalty loop using three possible journey expansion pathways. To illustrate these pathways, we discuss a prototypical customer at quick service chains (e.g., Dunkin', Pret a Manger, Starbucks). This customer purchases the same type of coffee every morning using the firm's app, thus getting her ""energize me"" job done efficiently. In CXM terms, the customer is locked into a loyalty loop.One way to expand on the existing loyalty loop is to trigger an adjacent loyalty loop (see Figure 2, Panel A). For example, on a special occasion such as the customer's birthday, the chain could reward the customer a free breakfast sandwich of her own choosing for the next three service encounters. In this manner, the customer is invited to enter a new deliberate decision-making process about which sandwich might best suit her breakfast needs. When the free offer ends, this tactic could result in the customer regularly purchasing a breakfast sandwich with her coffee, to get the ""energize me"" job done even better.Another way to expand on an existing loyalty loop is to spark an involvement spiral (see Figure 2, Panel B). For example, instead of rewarding the customer with a self-selected breakfast sandwich on her birthday, the chain could surprise her with a varied food offering at each of the next three service encounters (e.g., a cranberry scone, a cheese sampler, a fruit salad). When this birthday treat ends, the customer's involvement with the chain's food offerings may be sufficiently elevated to motivate her own exploratory purchases. Alternatively, the chain could reward the customer a free short-term subscription to a partner's recreational service (e.g., Hulu, Netflix, Spotify). Such interfirm alliances can create value for both firms ([46]). For the quick service chain, providing such rewards can strengthen the customer's loyalty. For the streaming service, these short-term subscriptions, framed as rewards, can spark involvement spirals, unlike direct mail offers, which are often ignored.Yet another way to expand on an existing loyalty loop is to escalate that loop with spiraling logic for a brief period of time (see Figure 2, Panel C). For example, the chain could reward its loyal customer any beverage on the house for the next three service encounters. In this scenario, the customer may upgrade her orders to more premium beverages each morning (e.g., a caramel macchiato, a nitro cold brew, a pumpkin spice latte). Alternatively, the chain could provide the customer with surprise beverages, with the order label placed on the underside of the cup, to foster the excitement of ""blind tasting"" ([39]). Exposure to the chain's premium beverages could motivate the customer to permanently upgrade her loyalty loop, to get the ""energize me"" job done with a dash of self-indulgence. Sustaining the customer journey beyond an existing involvement spiralWhen a firm already has customers caught up in one involvement spiral, CXM practitioners can expand on that involvement spiral using three journey expansion pathways. To illustrate these pathways, we discuss a common marketing problem at group fitness services (e.g., CrossFit, Orange Theory, SoulCycle): once-enthusiastic athletes are coming in less often.The first way to expand on an involvement spiral that is losing momentum is to spark a new one (see Figure 2, Panel D). At CrossFit, for example, the most enthusiastic athletes eventually reach a level of fitness at which the regular classes are no longer much of a challenge. At this juncture, CrossFit coaches invite those members to special classes for advanced athletes, such as Barbell Club and Strongman. As these new classes have significantly different structures, memberships, and challenges, athletes can be understood as entering a new involvement spiral. Eventually, some of these athletes may go on to compete at the CrossFit Games and related competitions, sparking new involvement spirals once again.The second way to expand on an involvement spiral is to trigger an adjacent loyalty loop (see Figure 2, Panel E). For example, some CrossFit boxes include smoothie bars. While the athletes primarily come to CrossFit for the involvement spiral of varied workouts, some members may also become locked into loyalty loops of smoothie purchases on their way out. In this manner, customers accomplish the job of ""workout recovery"" efficiently. If these add-on services offer unique value (e.g., organic fruits, paleo sweeteners, vegan proteins), some members might also swing by the CrossFit box just for the smoothie. In CXM terms, a parallel involvement spiral and loyalty loop in the same multiservice system can keep customers returning for one or the other journey pattern.The third way to sustain a customer journey when a customer's interest is waning is to stabilize the involvement spiral into a loyalty loop (see Figure 2, Panel F). This pathway is especially relevant when the customer is switching from an adventurer mindset to a jobber mindset. For example, some CrossFit athletes eventually tire of the ethos of relentlessly challenging themselves. However, rather than quitting, these athletes convert their upwardly spiraling journey into a stable cyclical one, ""just [to] keep a certain level of fitness"" (Emily, a CrossFit athlete). A CXM lesson to be derived from these mindset-switching athletes is that involvement spirals can sometimes be stabilized into loyalty loops, if that is what the customer wants. ConclusionThis article has made three contributions to CXM research. First, it has challenged the dominance of the smooth journey model. Second, it has offered an alternate sticky journey model. Third, it has addressed practical concerns at the nexus of the two journey models. In closing, this article also opens up several new avenues for future research on customer journeys (see Table 3). Chief among these avenues is examining new and different types of customer journeys. No one customer journey design is optimal under all circumstances. Accordingly, we hope that this article inspires CXM researchers to keep exploring the fascinating variety of customer journeys in the contemporary marketplace.GraphTable 3. Sample Avenues for Future Research.   Appendix. Additional Evidence for the Sticky Journey ModelGraph    "
23,"Customer Satisfaction and Its Impact on the Future Costs of Selling Although scholars have established that customer satisfaction affects different dimensions of firm financial performance, a managerially important but overlooked aspect is its effect on a firm's future cost of selling (COS), that is, expenditures associated with persuading customers and providing convenience to them. Accordingly, this study presents the first empirical and theoretical examination of the impact of customer satisfaction on future COS. The authors propose that while higher customer satisfaction can lower future COS, the degree to which a firm realizes this benefit depends on its strategy and operating environment. Analyzing almost two decades of data from 128 firms, the authors find that customer satisfaction has a statistically and economically significant negative effect on future COS. While the negative effect of customer satisfaction on future COS is weaker for firms with higher capital intensity and financial leverage, this effect is stronger for more diversified firms and for firms operating in industries with higher growth and labor intensity. The authors also find that these effects may vary across two components of COS, cost of persuasion and convenience.KEYWORDS_SPLITPopular wisdom suggests that customer satisfaction relates to the top and bottom line (i.e., gross sales and net profits of firms; e.g., [23]). Accordingly, an impressive body of empirical research has examined the effect of customer satisfaction on measures of financial performance as reflected in stock market–based measures such as stock returns and risk (e.g., [29]; [75]) and operational measures such as market share, profitability, and cash flows (e.g., [ 4]; [36]). Although a firm derives its net profits from the generation of sales after accounting for the associated costs, the relationship between customer satisfaction and costs remains largely unexamined. Investigating the impact of customer satisfaction on cost-based measures is important because aggregate profit-based metrics might not capture the true effect of customer satisfaction on costs (see [11]).Of particular salience to marketing managers is the cost of selling (COS); that is, the expenditures incurred by a firm to persuade customers to purchase its offerings and make it convenient for customers to do so. Firms often incur substantial costs related to persuading customers to purchase, such as sales commissions and marketing and advertising expenses. Firms also incur significant costs in trying to make purchases convenient for customers by absorbing freight-out costs and providing convenient or generous payment options that can result in bad debt expenses if customers default on payments (e.g., [71]; [82]). Indeed, marketing managers are constantly under pressure to look for marketing initiatives that produce the desired product-market outcomes at lower costs (e.g., [53]). The COS is also closely tracked by investors and analysts, as a reduction in COS is generally viewed as a positive signal in financial markets (see [37]). For example, Qualcomm enjoyed a surge in its stock price after it informed investors that it had been successful at reducing its selling-related expenditures ([28]). Similarly, when Best Buy pledged to continue to further reduce its selling-related expenses in 2014, analysts provided a positive forecast for its stock price (Forbes Trefis [27]).Prior research has suggested that customer satisfaction is a market-based asset that can potentially reduce a firm's future COS by increasing customer retention and loyalty and by generating positive word of mouth and higher willingness to pay (e.g., [65]). However, how this effect of customer satisfaction on future COS varies across firm and industry factors remains conceptually and empirically unclear. As such, we focus on the impact of customer satisfaction on future COS and the contingencies that influence this relationship to make two key contributions.First, drawing on almost two decades of data comprising 1,207 observations from 128 firms, we present the first empirical examination of the impact of customer satisfaction on future COS. As such, we respond to recent calls by [53] to study the effect of marketing actions on the costs incurred by a firm. Combining a text-analysis tool with data from COMPUSTAT and 10-K reports, we present a novel approach to measure COS by isolating the ""selling"" component of the firm's selling, general, and administrative expenses (SG&A). Consistent with our expectations, we find that customer satisfaction has a statistically and economically significant negative impact on future COS. Specifically, for an average firm in our sample, a 1-point increase in customer satisfaction, as measured on a 100-point scale of the American Customer Satisfaction Index (ACSI), corresponds to a decrease of almost US$130 million in future COS.[ 6] This amount is equivalent to nearly 3% of the average COS in our sample. This finding, therefore, is of direct importance for chief executive officers (CEOs) because cost reduction is their topmost concern (PwC 2018).Second, we develop a contingency framework that outlines the conditions that moderate the impact of customer satisfaction on future COS. Drawing on prior research (e.g., [78]), we propose that a firm's strategic focus and flexibility and its operating environment are likely to influence the extent to which customer satisfaction lowers future COS. Underscoring the importance of strategic focus, results show that the negative effect of customer satisfaction on future COS strengthens as diversification of firms increases. Consistent with our expectations about the moderating effects of strategic flexibility, we find that the negative effect of customer satisfaction on future COS weakens as capital intensity and financial leverage increase. Results also bring to fore the impact of operating environment, as we find that the negative effect of customer satisfaction on future COS strengthens as industry growth and labor intensity increase.We complement our focal analysis with a post hoc exploration of the effect of customer satisfaction on two components of COS: cost of persuasion and cost of convenience. We find that cost of persuasion seems to drive the effect of customer satisfaction on future COS, as the effect of customer satisfaction on the future cost of convenience is substantially smaller in magnitude than its effect on the future cost of persuasion. In addition, the moderating effects of firm strategy and operating environment predominantly manifest in the case of customer satisfaction's effect on the future cost of persuasion as opposed to the future cost of convenience. Taken together, results concerning the contingency framework and post hoc exploration provide a nuanced view of the impact of customer satisfaction on future COS. Cost of SellingCost of selling refers to the costs incurred by a firm in persuading customers to purchase its offerings and in making it convenient for them to do so.[ 7] Typically, to persuade a customer, a firm implements activities aimed at providing information and developing product/service perceptions (see [31]). Such activities include engaging sales personnel to serve as product/service experts to customers, implementing a marketing plan to enhance brand image, or purchasing banner advertisements to promote a new product/service. Selling also involves activities that make it easier for customers to purchase a firm's offerings (e.g., [ 9]; [13]). For example, firms offer delivery services to customers to reduce the time and effort required for them to travel to a store or offer ancillary services such as providing flexible payment terms.We draw on prior research (e.g., [49]; [60]) and derive a firm's COS from its SG&A. Specifically, SG&A comprises 16 expense items (COMPUSTAT Online Help Manual SG&A 2017). According to the Generally Accepted Accounting Principles, SG&A can be separated into selling expenses and general and administrative expenses ([49]). Of the 16 expenses that constitute SG&A, we consider the following 5 to make up COS: commissions, marketing expense, advertising expense, freight-out expense, and bad debt expense ([60]).Firms typically pay commissions to sales personnel and channel partners for successfully converting a prospective customer. Firms also incur marketing and advertising expenses when building brand awareness, promoting a product, or introducing a new one. These activities are aimed at influencing customers' understanding and perceptions of the firm's offerings (e.g., [99]). As such, we view commissions, marketing expenses, and advertising expenses as the costs of persuading customers and, therefore, consider them part of COS.Costs related to freight-out and bad debt expenses reflect costs incurred to make it convenient for customers to purchase a firm's offerings. Freight-out expense is the transportation cost that the firm incurs when it delivers products/services to customers ([104]). Thus, a firm incurs freight-out expense if it provides delivery services to customers to reduce the time and effort required to purchase its offerings. Bad debt expense reflects the loss that the firm incurs when customers renege on payments for products/services they bought ([71]). Firms, therefore, run the risk of incurring bad debt expense if they offer customers the flexibility to pay later or in installments. As such, we consider both freight-out and bad debt expenses as part of COS.It is important to note that COS is distinct from cost-related concepts such as SG&A, cost of goods sold (COGS), and operating expenses (OPEX). Rather, COS is a subset of SG&A that is an aggregated metric comprising 16 distinct expense items. While COS reflects the costs related to selling a firm's offerings, COGS comprises costs incurred by a firm in the production of its offerings (COMPUSTAT Online Help Manual COGS 2019).[ 8] Finally, COS is also a subset of OPEX, which is the sum of COGS and SG&A (COMPUSTAT Online Help Manual Operating Expense OPEX 2018). We illustrate the relationships between COS and these cost-related concepts in Figure 1 and Table A1 of Web Appendix A.Graph: Figure 1. Comparing cost of selling with other cost-related concepts.aThe expense items listed under General and Administrative Expense comprise only the expense items that are relevant to our research context (for a more detailed discussion, see Table 1). Conceptual Framework and HypothesesDrawing on the extant literature ([65]; [92]), we propose that customer satisfaction should lower future COS by reducing the potential expenses in persuading customers and offering convenience. Importantly, the extent of this effect should be a function of the firm's strategy and its operating environment (see Figure 2).Graph: Figure 2. Impact of customer satisfaction on future COS.A firm's strategy reflects the fundamental organizational sources of competitive advantage that address two key issues ([40]): ( 1) what firms want and ( 2) what firms have. The question of ""what firms want"" brings the issue of strategic focus (or intent) to the forefront ([40]). The question of ""what firms have"" brings organizational resources to the forefront ([103]), where we examine organizational strategic flexibility, a polymorphous construct that assesses malleable firm resources that can easily take on different forms based on organizational contexts and needs ([35]). A firm's operating environment is an indicator of the customer's perspective because it reflects the nature of the preferences and expectations of its current and potential customers ([78]). In this way, the moderating impact of a firm's operating environment demonstrates the effectiveness of the firm in benefiting from customer satisfaction to lower future COS. Impact of Customer Satisfaction on Future COSCustomer satisfaction refers to customers' postconsumption comparison of their expectations and perceptions of performance of a product or service ([43]). Increase in customer satisfaction should lower a firm's future COS because satisfied customers respond in several beneficial ways. First, customer satisfaction enhances customer loyalty intentions and repurchase behaviors ([38]). Satisfied customers also have lower price sensitivity ([107]) and greater willingness to pay ([47]). As such, salespeople are likely to find greater success rates in persuading satisfied customers to purchase than unsatisfied customers ([80]). Therefore, commissions required to incentivize salespeople should reduce as customer satisfaction increase. Second, increase in customer satisfaction signals improvements in the quality of a firm's offerings relative to competitors ([86]); consequently, satisfied customers promote a firm's offerings through positive word of mouth ([65]). The quality signal, coupled with the positive word of mouth, should reduce future advertising and marketing expenses needed to persuade customers to purchase ([101]).An increase in customer satisfaction should also lower spending related to providing convenience to customers. Satisfied customers should be more tolerant of inconvenience associated with the consumption of a firm's offerings than unsatisfied customers ([ 9]). In fact, satisfied customers often inconvenience themselves to purchase from a firm they are satisfied with even if competitors' offerings are conveniently available ([92]). For example, consider the offers for free shipping often used in promotions to keep prices competitive and generate additional sales ([62]). As price sensitivity reduces with increase in customer satisfaction ([107]), the attractiveness of competitors' free shipping offers should reduce as satisfaction increases.Similarly, firms typically offer customers the flexibility to pay later or in installments in hopes that putting off payments to incentivize customers to spend more ([74]). However, as repurchase behaviors increase with customer satisfaction ([38]), the importance of payments terms should reduce with an increase in satisfaction. As such, higher customer satisfaction should reduce a firm's bad debt expenses, as it need not depend on the provisions of flexible payment terms to generate sales. Taken together, higher customer satisfaction should lower a firm's future COS through reductions in the potential costs that relate to persuading customers and providing convenience to them. Formally, H1:  As customer satisfaction of a firm increases, its future COS decreases. Moderating Effect of Firm StrategyThe strategic focus and flexibility of a firm should influence its willingness and ability to benefit from customer satisfaction. First, strategic focus represents the extent to which a firm concentrates its business portfolio on few versus many business sectors ([56]). As firms expand their operations over many businesses (i.e., increase firm diversification), their strategic focus reduces ([26]). Importantly, a firm's strategic focus can serve as either a facilitator or an inhibitor when it examines organizational and operating changes that are required to pursue opportunities related to customers ([110]). For example, diversified firms may enjoy reputation spillovers across markets that can potentially reduce their need for advertising. Second, strategic flexibility can affect organizational ability to deploy resources to utilize customer satisfaction to lower future COS ([61]). Indeed, firms with resource constraints can find it difficult to, for example, exploit the positive word of mouth from satisfied customers to lower their advertising expenses because they lack the flexibility to alter promotional programs. Extant research in marketing and operations suggests that a firm's reliance on capital equipment and its debt burden are crucial indicators of strategic flexibility ([44]; [77]). Therefore, to assess strategic flexibility, we consider two key resource positions: the degree to which a firm employs capital equipment (i.e., capital intensity) and its debt burden (i.e., financial leverage). Strategic focusA firm's extent of diversification refers to the breadth of its offerings ([58]). Existing research in operations and marketing suggests that firms that provide a wider range of products and services can benefit from operational and financial synergies (e.g., [58]; [100]). In particular, diversification allows firms to enjoy operational synergies through the transfer of skills and tacit knowledge across multiple businesses ([83]). Indeed, a core competency of diversified firms is to employ specific knowledge acquired in one business to formulate solutions for problems and capitalize on the favorable circumstances in other businesses ([112]). This ability to transfer a variety of knowledge across multiple businesses serves as a catalyst in transferring both simple and complex customer knowledge ([32]) across diverse products and services. The gains from sharing customer knowledge should increase as firm diversification increases; this knowledge should enable firms to utilize the benefits of customer satisfaction to reduce their future COS. Put differently, the benefits due to higher customer satisfaction in each business sector—for example, the positive word of mouth and/or the higher quality signals—are likely to spill over to other business sectors, thus allowing diversified firms to lower their future costs required to persuade and offer convenience to customers. Therefore, we expect the following: H2:  The negative effect of customer satisfaction on future COS is stronger (weaker) for more (less) diversified firms.In contrast, prior research also suggests that a diverse range of offerings can lead to an increase in operational complexity and therefore lower resource allocation efficiency ([66]). Greater diversification can also result in significant constraints on the attention of the top management team (TMT; [100]), which, in turn, should reduce emphasis on activities that reduce costs. It is also possible that there is little overlap in the customer segments across multiple businesses, which makes it difficult for diversified firms to benefit from customer knowledge transfers. The lack of TMT attention, as well as the inability to benefit from customer knowledge transfers, should lower the likelihood of diversified firms utilizing these benefits. Therefore, we propose an alternative hypothesis: H2alt:  The negative effect of customer satisfaction on future COS is weaker (stronger) for more (less) diversified firms. Strategic flexibilityCapital intensity reflects the degree to which a firm relies on capital investments such as property, plant, and equipment for its operations ([20]). Higher capital intensity, therefore, implies lower strategic flexibility. Capital-intensive firms typically rely on automation but deemphasize reliance on employee specific skills ([44]). Such firms tend to gain competitive advantage from reducing costs and therefore find opportunities for cost reduction to be attractive ([72]). However, the benefits from satisfied customers should be less valuable for firms with higher capital intensity. Higher capital intensity means that a firm relies more on tangible signals of the quality of its offerings such as manufacturing plants and physical stores. Attractiveness of intangible signals, such as the positive word of mouth, reduces in the presence of such tangible signals. Thus, although the potential cost savings may provide initial incentives for capital-intensive firms to leverage customer satisfaction to lower future COS, the reduced salience of these benefits is likely to negate these motivations.As employees enable benefits from satisfied customers (e.g., [44]; [110]), even capital-intensive firms with higher customer satisfaction should lack the capability to use customer satisfaction to lower future COS. In fact, due to reliance on automation in capital-intensive firms, their desire to benefit from satisfied customers would require substantial transition of existing routines, such as retraining their workforce (e.g., [44]; [77]). Because capital-intensive firms have high fixed costs and low variable costs (e.g., [63]), their motivation to undertake revision to routines that might raise variable costs should be low ([20]). Thus, we expect the impact of customer satisfaction on future COS to weaken as capital intensity increases because of scarce strategic flexibility to utilize customer satisfaction. H3:  The negative effect of customer satisfaction on future COS is weaker (stronger) for firms with higher (lower) capital intensity.Financial leverage reflects the degree of debt that a firm has relative to its total assets ([77]). As debt increases with an increase in financial leverage, financial flexibility declines because the accumulation of debt restricts the availability of uncommitted cash flows ([88]). The constraints on financial flexibility arise because leveraged firms devote a substantial amount of cash flows toward fulfillment of interest payments ([68]). Lower financial flexibility creates incentives to reduce costs and therefore should motivate leveraged firms to realize the benefits of customer satisfaction to lower future COS ([ 3]). However, lack of financial flexibility also limits the capacity to pursue customer-related opportunities ([68]). Furthermore, because customers are typically concerned about the quality of offerings ([67]), it is unlikely for leveraged firms to derive benefits from customer satisfaction to lower future COS. Indeed, prior research has suggested that an increase in financial leverage lowers the likelihood of customers making specific investments with suppliers ([51]).In summary, we expect an increase in financial leverage to weaken the negative impact of customer satisfaction on future COS despite the predisposition of leveraged firms to engage in cost-reduction activities. This weakening arises because financial leverage should not only reduce a firm's financial flexibility to utilize the benefits of customer satisfaction but should also lower customers' perceptions of the quality of the firm's offerings. More formally, H4:  The negative effect of customer satisfaction on future COS is weaker (stronger) for firms with higher (lower) financial leverage. Moderating Effect of Operating EnvironmentA firm's operating environment provides information about the nature of the preferences and expectations of its current and potential customers and, therefore, should affect the impact of customer satisfaction on future COS (e.g., [46]; [78]). We draw on prior work to identify three industry conditions in a firm's operating environment that reflect competitive pressures, growth prospects, and labor intensity in managing customers. First, reflecting the centrality of competitive conditions for COS, we examine the moderating effect of industry concentration ([65]; [68]). Second, consistent with the logic that COS should be a critical consideration in growth markets ([ 1]), we examine the moderating effect of industry growth. Third, consistent with prior work that emphasizes the pivotal role of labor intensity for customer satisfaction, we consider the moderating effect of industry labor intensity ([22]). Industry concentrationIndustry concentration captures the degree of competition in an industry such that as industry concentration decreases, competitive intensity increases, and as a result, customers have more options to choose from ([25]). With the increase in options, customers are more likely to consider competing firms as substitutes and thus have lower loyalty and higher price sensitivity, which should make it difficult to retain even highly satisfied customers ([65]; [68]). As such, the efficacy of the benefits of higher customer satisfaction in lowering future COS should weaken as industry concentration decreases because the options available to customers increase. Conversely, the decrease in the number of options as industry concentration increases makes it difficult even for dissatisfied customers to discontinue their relationships with the firm ([65]). Formally, H5:  The negative effect of customer satisfaction on future COS is weaker (stronger) for firms operating in industries with lower (higher) concentration. Industry growthThe rate of growth in demand in an industry (i.e., industry growth) plays an integral role in strategic marketing models ([46]). Findings from existing research suggest that salience of the benefits of customer satisfaction (i.e., positive word of mouth and lower price sensitivity) should increase with industry growth because customers' reliance on word-of-mouth communication increases due to reduced availability of alternative options ([111]). Indeed, criticality of word-of-mouth communication should increase with industry growth due to the presence of a new and diverse base of customers in such industries (see [50])—that is, customers unfamiliar with firms' offerings.Prior research also suggests that price elasticity increases with industry growth ([10]) as competing firms are willing to make costly investments—such as pricing below their costs—to capture greater market share ([ 1]). In this way, customers' lower price sensitivity resulting from higher customer satisfaction is also more critical in growth industries because customers who are satisfied with the firm's offerings are unlikely to be swayed by its competitors' tactics because they tend to have a higher willingness to pay ([47]). Thus, the negative impact of customer satisfaction on future COS should be stronger in growth industries because the cost-reducing benefits of higher customer satisfaction (free word of mouth, less price-induced switching) are more relevant in such industries. Formally, H6:  The negative effect of customer satisfaction on future COS is stronger (weaker) for firms operating in industries with higher (lower) growth. Industry labor intensityIndustry labor intensity reflects the extent to which firms in an industry are reliant on employees to produce and deliver products and services ([22]). In labor-intensive industries, customers should find it difficult to compare competing offerings due to heterogeneity in offerings that reliance on labor creates ([22]). This difficulty in comparing competing offerings should make customers loyal ([84]). Furthermore, customers' uncertainty in evaluating a firm's offerings increases with an increase in labor intensity as the intangible nature of offerings should increase their perceived consumption risks ([26]). Therefore, to reduce such risks, customers increase their reliance on word-of-mouth communication to obtain information concerning the offerings ([ 8]). Because the impact of customer satisfaction on future COS can be driven by its effect on the presence of loyal customers and positive word of mouth, it is likely that this effect strengthens as labor intensity increases because customers are more loyal and rely more on word-of-mouth communication in such industries. That is, the negative influence of customer satisfaction on future COS should strengthen as labor intensity increases. Thus, H7:  The negative effect of customer satisfaction on future COS is stronger (weaker) for firms operating in industries with higher (lower) labor intensity. Method Data CollectionWe obtain the customer satisfaction score and the accounting data on each firm from the ACSI and the Standard & Poor's COMPUSTAT databases, respectively. We define a firm's industry using its primary four-digit North American Industry Classification System (NAICS) code. In addition, we follow precedent in the finance and accounting literature to exclude firms from the utilities and the financial services industries (e.g., [21]).[ 9]We collect the ACSI scores from the first quarter of 1994 to the fourth quarter of 2013. Given that the ACSI releases customer satisfaction scores on an annual basis but does so in different quarters for firms in different industries, we use the quarterly accounting data from COMPUSTAT and align it with the four quarters between the releases of the ACSI scores. We only include firms for which at least two consecutive years of customer satisfaction data are available, as our model requires the future values of COS. Our sampling criteria yield 1,207 pooled time series and cross-sectional observations from 128 firms. Measures and Data Customer satisfaction and moderatorsThe ACSI collects customer satisfaction data from over 50,000 customers every year through telephone interviews. The customer satisfaction scores for each firm are scaled from 0 to 100. For firms that own multiple brands covered by ACSI, we use the average customer satisfaction scores across all brands as a measure (see [68]).We calculate the extent of firm diversification as one minus the Herfindahl index of the firm's sales across all its business segments (e.g., [58]) and the firm's capital intensity as the ratio of its net plant, property, and equipment to its total assets (e.g., [72]). Consistent with prior research (e.g., [106]), we measure the firm's financial leverage as the ratio of its total long-term debt to its total assets. Industry concentration is the four-digit NAICS Herfindahl index of firm sales (e.g., [36]), and industry growth is the difference in the natural logarithm of the total sales of all firms within the same four-digit NAICS code at the end of the current year from the end of the preceding year (e.g., [105]). We measure industry labor intensity as the average ratio of the number of employees to the total sales of the firms within the same four-digit NAICS code (e.g., [64]). Dependent variableWe measure a firm's COS using the selling expenses obtained from its SG&A. Specifically, out of the 16 distinct expense items in SG&A (see COMPUSTAT Online Help Manual SG&A 2017), we consider the following five expense items—commissions, marketing expense, advertising expense, freight-out expense, and bad debt expense—as selling expenses and thus part of a firm's COS ([60]). To account for the differences in size across firms, we scale selling expenses by the firm's total sales.Following prior research that decomposes SG&A (e.g., [49]; [60]), to measure a firm's COS, we subtract the expense items that are not relevant to a firm's COS from its SG&A. Table 1 outlines the expense items in SG&A and the methodology and data sources utilized to subtract items that are not relevant to a firm's COS.GraphTable 1. Deriving COS from SG&A.  1 Notes: SEC = U.S. Securities and Exchange Commission; WRDS = Wharton Research Data Services; EXECUCOMP = Executive Compensation; DT = Data Item. We present the 16 SG&A items based on the definition provided by COMPUSTAT (see COMPUSTAT Online Help Manual SG&A 2017). The decomposition of SG&A expenses into selling expenses and general and administrative expenses is adapted from [60]. For more information on the text analyses conducted using the WRDS SEC Analytics Suite, refer to Web Appendix B.Although there are 11 expense items that are not relevant to a firm's COS (see Table 1), not all of them require subtraction from SG&A. Given that we are only focusing on firm-year observations for which SG&A is reported (i.e., for operating expense) and we do not have subsidiary firms in our sample (i.e., for parent company charges), operating expense and parent company charges are not applicable to our research context. As such, subtraction of these expense items is not required.However, we need to subtract the following five items—engineering expense, foreign currency adjustments, indirect costs, strike expense, and extractive industries' expenses—from SG&A. Because they are not available in COMPUSTAT as separate items, we draw on the text-analysis tool WRDS SEC Analytics Suite to search through the 10-K filings of the firms within our sample for these items and subtract them from SG&A if they are disclosed separately (for detailed information on WRDS SEC Analytics Suite and the text analyses procedure, see Web Appendix B). By doing so, we follow ""SEC Regulation S-X (17 CFR Part 210) §210.402 Items not material"" and the materiality principle in accounting (see, e.g., [24]). That is, if these items are not disclosed as separate items in the 10-K filings, then they are not material (i.e., the amount is not significant enough to be disclosed as a separate item). To subtract the remaining four items—directors' compensation, pension-related expenses, research-and-development expenses (R&D), and research revenue—from SG&A, we draw on data at either the annual or quarterly level depending on the data availability in COMPUSTAT (for more details on the data sources, see Table 1). Taken together, we need to subtract nine expense items from SG&A to obtain a firm's COS.Before we subtract any items, it is important that we verify that these items are indeed included in SG&A to avoid erroneously removing an item when it is not actually included. This data step is vital because a firm's definition of SG&A may not always be consistent with COMPUSTAT's. For example, Atlantic Richfield does not consider items such as R&D, pension, and exploration expenses as part of its definition of SG&A, even though these items are listed as a part of the itemized expenses of SG&A according to COMPUSTAT's definition. Thus, we adopt the following process to subtract items from SG&A.First, we obtain a firm's SG&A from COMPUSTAT. Second, we collect information on the nine expense items (see Table 1), which are likely to be included in SG&A but are irrelevant to COS. Third, for each firm, we first determine which of the nine items are included in its SG&A and then subtract only those items that are included. When subtracting items from SG&A, we use a ratio-based approach in which we express the item as a fraction of the total SG&A for that fiscal year before multiplying this fraction with the SG&A for a specific quarter within that same fiscal year.We present the comparisons of other commonly used SG&A-based measures of selling-related expenses in Table 2—that is, a firm's SG&A scaled by its total sales (SG&A/TS) and the difference in its SG&A and R&D expenditures scaled by its total sales [(SG&A − R&D)/TS]. As Table 2 shows, the difference in means for these two alternative measures and our focal measure is positive and statistically significant. For some firms, this difference can mean an overestimation of almost US$3 billion (e.g., Apple) if we use the SG&A/TS measure and more than US$750 million (e.g., HP Inc) if we use the [(SG&A − R&D)/TS] measure. Importantly, our findings are consistent with recent work showing that the utilization of these commonly used SG&A-based measures of selling expenses is likely to result in an inflation of firms' COS and possibly an erroneous estimation of its effects (see [89]).GraphTable 2. Comparing COS with Other SG&A-Based Measures of Selling-Related Expenses.  2 ***p <.01 (two-sided).3 a To obtain meaningful correlations of COS with the other SG&A-based measures of selling-related expenses, we subtract COS from SG&A and SG&A − R&D because COS is a subset of SG&A (see Table 1 and COMPUSTAT Online Help Manual SG&A [2017] for the definitions of COS and SG&A, respectively). The difference in means, their corresponding standard errors, and the correlations appear in their original values (i.e., before applying any variable transformations. We compute the dollar values of the difference in means using values from the unscaled versions of the measures (i.e., COS, SG&A and SG&A − R&D, respectively). Correlations that are significant at p <.10 (two-sided) appear in bold. There are 1,207 observations from 128 firms.4 Notes: SG&A − R&D = SG&A minus R&D; TS = total sales; COS/TS = ratio of COS to TS; SG&A/TS = ratio of SG&A to TS; (SG&A − R&D)/TS = ratio of SG&A minus R&D to TS; SG&A − COS = SG&A minus COS; SG&A − R&D − COS = SG&A minus R&D minus COS. Control variablesIn addition to the six moderators, we also control for several financial characteristics that can influence a firm's spending behavior and thus its future COS. Specifically, prior research has suggested that the financial performance of a firm can drive its spending behavior such that firms that are less profitable are likely to lower their future COS ([76]). As such, we include Tobin's q (i.e., a firm's market-to-book ratio) to account for firm performance ([81]).Furthermore, the amount of slack organizational resources that a firm possesses may also influence its ability to respond to changing demands of customers ([45]) and thus affect its spending behavior. To account for factors that can potentially drive a firm's future COS, we include three variables. First, given that changes in inventory levels can influence a firm's capacity to react to supply and demand variations ([58]), we include inventory slack to capture the spare physical inventory of a firm ([ 7]). Second, we include retained earnings to take into account the resources that a firm allocates in preparation for unanticipated circumstances and implementation strategies ([12]). Finally, to consider the firm's effectiveness in its usage of liquid assets (e.g., cash) to generate sales, we include working capital ([61]).Prior research has suggested that when firms have limited budgets, they can potentially make trade-offs between their spending behaviors in R&D and marketing to allocate funds effectively ([16]). Thus, we include R&D intensity as a control variable. In addition, we also account for the difference in firms' spending behaviors across industries. Existing research suggests that when the unpredictability in the nature and quantity of customers' requirements increases, it becomes harder for firms to rely on customers' prior knowledge ([46]). The constant changes in customers' preferences can potentially influence a firm's willingness to adjust its future COS. Thus, we control for industry turbulence to account for the extent to which industry demand changes rapidly and unpredictably ([25]). We present all control variables, their measures and data sources, and references to prior literature supporting the use of these measures in Web Appendix C, Table C1.[10] Empirical StrategyWe use a linear model specification to test our hypotheses, in which we treat COS as a function of customer satisfaction, moderators, and control variables. Because a firm's future COS is likely to depend on its current customer satisfaction and other explanatory variables, we incorporate this temporal separation into the model with a panel data structure. In addition, our model also takes into account the following identification challenges emanating from endogeneity concerns due to four types of omitted variable biases.First, exogenous shocks concerning boom and bust business cycles can influence a firm's COS and customer satisfaction. For example, it is well documented that marketing budgets are curtailed during bust periods ([96]). Thus, to control for exogenous shifters that might influence a firm's COS, we include year dummies (i.e., time-specific fixed effects). Second, variables such as organizational culture that are largely stable over time ([42]) can also influence both how much a firm spends on marketing (i.e., COS) and the level of customer satisfaction it achieves. For example, firms that place more emphasis on customer engagement can spend more on selling-related activities and also have higher levels of customer satisfaction. Therefore, to capture firm-specific variables that do not vary over time (e.g., organizational culture), we also include firm-specific fixed effects. Because industry idiosyncrasies can be teased out using both the time- and firm-specific fixed effects (see [52]), inclusion of these fixed effects also accounts for industry variables that do not change over time.Third, firm-specific variables that change over time can also potentially influence a firm's COS and its customer satisfaction levels. To illustrate, the mindset of the CEO and/or chief marketing officer (CMO) can determine the emphasis on customer satisfaction or the firm's expenses on marketing-related activities (i.e., its COS) over two- to five-year periods (i.e., based on average tenures of CEOs and CMOs; e.g., [33]). Thus, we also include a rich set of covariates to proxy for any time-varying omitted variables.Fourth, given that prior research has used SG&A as a predictor of customer satisfaction (e.g., [75]), reverse causality can also be a cause for concern. For example, one can argue that a firm's current COS can instead have an impact on its current customer satisfaction levels. We aim to mitigate such concerns in our model by lagging the explanatory variables by one year (e.g., [97]). However, given that customer satisfaction is persistent, the use of lagged variables may not completely correct for the potential reverse causality. Literature in economics suggests that reverse causality can be framed as a form of omitted variable bias, where the omitted variable varies over time (e.g., see econometric texts such as [109]]). In this way, the inclusion of a rich set of covariates also accounts for concerns relating to reverse causality if there is autocorrelation in the explanatory variables despite the temporal separation. Thus, we can specify our model as follows: COSij(t+1)=β0i+β1CSijt+CSijt×(β2DIVijt+β3CIijt+β4LEVijt+β5HERFjt+β6IGRWTHjt+β7LIjt)+β8CNTRLSijt+∑k=926βkDt+ϵij(t+1), Graph1where  β0i  is the fixed effect that captures the firm-specific heterogeneity in  COSij(t+1)  ,  COSij(t+1)  is the future COS of firm  i  in industry  j  at time  t+1  ,  CSijt  is the customer satisfaction of firm  i  in industry  j  at time  t  ,  DIVijt  is firm diversification,  CIijt  is capital intensity,  LEVijt  is financial leverage,  HERFjt  is industry concentration,  IGRWTHjt  is industry growth,  LIjt  is industry labor intensity,  CNTRLSijt  is the vector of firm- and industry-specific control variables,  Dt  are the year dummy variables, and  ϵij(t+1)  is the random error term.The identifying assumption in Equation 1 is that beyond the firm-specific fixed effects, the omitted variables do not vary over time ([33]). Although we include a rich set of covariates in Equation 1 to account for the time-varying omitted variables, it is difficult to argue that we can observe all important variables that may influence both firms' customer satisfaction levels and their COS. Therefore, to account for the time-varying omitted variables, we adopt a two-step control function approach with an appropriate instrumental variable (e.g., [41]).We perform the control function estimation by first estimating an auxiliary equation with customer satisfaction as the dependent variable using the following specification: CSijt=α0i+α1WPCSijt+α2DIVijt+α3CIijt+α4LEVijt+α5HERFjt+α6IGRWTHjt+α7LIjt+α8CNTRLSijt+∑k=926αkDt+γijt, Graph2where  α0i  captures firm fixed effects that account for the firm-specific heterogeneity in customer satisfaction and  γijt  is the random error term (other variables, including time fixed effects, are defined as previously).The auxiliary equation specified in Equation 2 includes the six moderators (  DIVijt  ,  CIijt  ,  LEVijt  ,  HERFjt  ,  IGRWTHjt  , and  LIjt  ), the rich set of covariates (  CNTRLSijt  ) and the year dummy variables (  Dt  ). For identification purposes, we also include weighted peers' customer satisfaction (  WPCSijt  ) as an instrument. We identify peer firms as firms with brands that are classified in at least one common ACSI-defined sector with that of the brand(s) owned by the focal firm (for sector definitions, see [ 2]]).[11] An ACSI-defined sector consists of several ACSI-defined industries. For example, the ACSI-defined industries Airlines and Consumer Shipping belong to the Transportation sector in ACSI (see Web Appendix D, Table D1). In this way, a firm's (e.g., Delta's) peers can either be from the same ACSI-defined industry as that of the firm (e.g., United) or from another industry that is in the same ACSI-defined sector (e.g., FedEx belongs to the Transportation sector).[12] Before delving into the operationalization of the instrument, we note that the instrument meets the standard instrument relevance and exclusion restriction criteria, as we discuss subsequently. In addition, we also aim to use an instrument with a property we refer to as ""granularity,"" such that a granular instrument implies that the instrument varies at the level of the endogenous explanatory variable; we delve into this and other instrument validity criteria after defining the instrument.[13] Specifically, we define the instrument as follows: WPCSijt=∑i′sN[wii′st×CSi′st]∑i′sNwii′st, Graph3where  wii′st  denotes the weight of the relationship between focal firm  i  and peer firm  i′  in ACSI-defined sector  s  at time  t  , and  CSi′st  is the customer satisfaction score of the peer firm  i′  in ACSI-defined sector  s  at time  t  . Consistent with our measurement of a firm's overall customer satisfaction, for firms that possess multiple brands across ACSI-defined sectors, we take the average weighted peers' customer satisfaction scores across these sectors.[14] We discuss the operationalization of the weights for the relationship between the focal firm and its peers, and present examples of the measurement of the instrument in Web Appendix D.[15]To evaluate the conceptual quality of the proposed instrument, we follow recent recommendations and delve into discussing instrument relevance, exclusion restriction, and granularity (see [33]; [94]). Instrument relevance implies that the proposed instrument conceptually correlates with the endogenous variable. According to the long-standing conceptualization of customer satisfaction, customers determine their satisfaction levels with a firm on the basis of the comparisons that they make between their expectations and their perceptions of the performance of the firm's products/services ([43]). Because customers often distribute their purchases amongst several competing firms, their perceptions of the performance of the products/services of the firm's peers are likely to influence their expectations of the firm's offerings ([54]). Thus, customers are likely to evaluate a firm's customer satisfaction relative to its peers ([55]), and peers with similar firm characteristics are likely to have greater influence (see, e.g., [108]). Taken together, the weighted customer satisfaction scores of a firm's peers are likely to influence its customer satisfaction level, thus making it a relevant instrument for customer satisfaction.Exclusion restriction implies that the proposed instrument does not correlate with the omitted variables that are a part of the error term but is likely to be correlated with the endogenous variable ([109]). To provide a theoretically grounded explanation for the proposed exclusion restriction, consider the example of the degree of TMT integration, a plausible omitted variable that is likely to be correlated with customer satisfaction and COS. TMT integration reflects the extent of collaboration, shared information, joint decision making, and shared vision within the TMT ([39]). These attributes, in turn, are critical for coordinating actions among TMT members and for improving the quality of strategic decisions ([15]). As such, the extent of TMT integration within a firm should be correlated with its level of customer satisfaction ([95]). However, there is little reason to believe that our proposed instrument (i.e., the weighted customer satisfaction levels of a firm's peers) would correlate with a firm's degree of TMT integration.First, prior research has suggested that a firm's extent of TMT integration is more likely to be influenced by attributes within the firm, such as concurrent changes in the diversity of team members or CEO mindset ([95]), as opposed to outside factors that are not within its control. Second, the customer satisfaction of a firm's peers is a representation of the joint decisions made by both the customers of the firm and its peers. Because customers rarely possess knowledge of the identities of one another, it is highly implausible for them to collectively adjust their satisfaction ratings of all the firm's peers due to changes in a firm's degree of TMT integration. Coordination among peer firms to imitate the firm's level of TMT integration is also unlikely, as it is not common for peers to jointly monitor the implementation of specific strategies from a particular firm ([41]).Importantly, the identification of a firm's peers using the ACSI-defined sector classification also mitigates the possibility of cooperative monitoring of an individual firm from all its peers. Specifically, because our definition of a firm's peers also includes peer firms that are from an adjacent (but not the same) ACSI-defined industry as that of the focal firm, it is implausible for the customer satisfaction levels of a firm's peers to correlate with its degree of TMT integration.[16] For example, consider the Transportation sector as presented in Web Appendix D, Table D1. While UPS and FedEx have little incentive to collaborate with the other airlines to monitor and react to changes in Delta's level of TMT integration, it is also improbable for Delta to change the extent of its TMT integration due to the changes in the customer satisfaction levels of UPS and FedEx. Taken together, both prior theory and the construction of our instrumental variables indicates that the weighted peers' customer satisfaction meets the exclusion restriction and is a valid instrument.A common criticism against the use of peer-based instruments is granularity, when there is insufficient systematic variation in the group composition ([ 6]) such that the likelihood for a firm to behave in a certain way varies with the behavior of the group but does not sufficiently vary with the exogenous characteristics of the group ([69]). Consider the common operationalization of peer-based instrument—the industry average value excluding the focal firm. In this case, the only source of variance in the instrument across firms comes from the exclusion of the focal firm while calculating the value of the instrument, resulting in little change in its value across firms and over time. In addition, the simple exclusion of the focal firm's value also makes it impossible to determine whether the focal firm's behavior is being influenced by that of its peers or that the behavior of its peers is actually an aggregation of the behavior of individual firms ([70]).The measurement of the weighted peers' customer satisfaction scores relies on the construction of groups that are partially overlapping in terms of the ACSI-defined sector classifications and the weighing of firms by their characteristics within each ACSI-defined sector.[17] In this way, the use of this measure is likely to increase the variation in group composition and thus can preclude concerns associated with granularity as ""it breaks down the linear dependence between endogenous and exogenous peer variables"" that varies at the individual firm level (see [94], p. 13). Reassuringly, we find that there is substantial variation in the weighted peers' customer satisfaction scores across ACSI-defined sectors (see Web Appendix E, Figure E1).In the second step, we then estimate Equation 1 with the predicted residuals obtained from Equation 2. As such, our final equation is as follows: COSij(t+1)=β0i+β1CSijt+CSijt×(β2DIVijt+β3CIijt+β4LEVijt+β5HERFjt+β6IGRWTHjt+β7LIjt)+β8CNTRLSijt+∑k=926βkDt+β27μijt+ϵij(t+1), Graph4where  μijt  is the predicted residuals from estimating the auxiliary regression (Equation 2). Following the recommendation of [87], we also implement the bootstrap to obtain standard errors that consider the additional source of variation due to the use of an estimate (i.e.,  μijt  from Equation 2) in Equation 4. Using 500 bootstrap samples, we first obtain 500 sets of predicted residuals from the estimation of Equation 2 and then use each set of predicted residuals in the estimation of Equation 4 ([85]). ResultsWe outline the descriptive statistics and bivariate correlations between the variables and present the results from the auxiliary equation (Equation 2) in Table 3 and Table E1 of Web Appendix E, respectively. Consistent with our expectations, we find that weighted peers' customer satisfaction significantly and positively predicts firms' customer satisfaction  (α1=.567,p<.01)  . Results from the F-test for instrument strength also suggest that our focal instrument is not a weak instrument, as we find that the F-statistic is 98.570 (  p<.01  ), well above the cutoff of 10 ([98]). We report the estimated results of both a model that includes only the main effect of customer satisfaction and the covariates (M1), and a model that also includes the moderating effects (M2) in Table 4. The condition indices of M1 and M2 (i.e., 12.170 and 14.000, respectively) are both well below the more rigorous cutoff criterion of 20 ([34]). Thus, multicollinearity does not seem to be an issue for either model. As shown in Table 4, M1 already supports H1, with a significant negative effect for customer satisfaction  (β1=−.004, p<.05)  . Importantly, the addition of the hypothesized interaction effects resulted in a significant increase in model fit (  χ2(6)=40.73,p<.01  ). Thus, we focus our subsequent discussion on the results from M2.GraphTable 3. Descriptive Statistics and Correlation Matrix.  5 a Future cost of selling refers to the cost of selling of a firm in the following year, where we refer to year as the aggregation of data over the four quarters corresponding to the period between the release of ACSI scores.6 Notes: Correlations that are significant at p <.10 (two-sided) are in bold. The mean, standard deviation, minimum, maximum, and correlation values of the variables appear in their original values (i.e., before applying any variable transformations). There are 1,207 observations from 128 firms.GraphTable 4. Impact of Customer Satisfaction on Future COS.  7 *p <.10.8 **p <.05.9 ***p <.01.10 a We obtain the residuals from the estimation of the auxiliary regression in Web Appendix E, Table E1.11 Notes: Two-sided tests of significance. SE = bootstrap standard errors derived from performing 500 bootstrap replications using the approach illustrated in [85]; N (n) = Total number of observations (unique firms). All continuous variables are mean-centered and Winsorized at the 1st and 99th percentile levels.The predicted residuals term is statistically significant (  β27=.004,p<.10  ), thus indicating the importance of accounting for the endogeneity of our focal independent variable, customer satisfaction (see [87]). Consistent with H1, we find that customer satisfaction has a significantly negative effect on future COS  (β1=−.005,p<.05)  . Parameter estimates also provide support for H2 (as opposed to H2alt), as we find that the negative effect of customer satisfaction on future COS becomes stronger as diversification increases  (β2=−.003, p<.01)  . We also find support for H3, as the negative effect of customer satisfaction on future COS becomes weaker as capital intensity increases  (β3=.005, p<.01)  . Results also indicate that the negative effect of customer satisfaction on future COS becomes weaker as financial leverage increases  (β4=.002, p<.10)  , thus providing marginal support for H4. We also find support for H6 and H7, as the negative effect of customer satisfaction on future COS becomes stronger as industry growth and industry labor intensity increase  (β6=−.005, p<.01; β7=−.035,p<.05)  . Contrary to our expectations, however, there is insufficient evidence to support H5, as we find that changes in industry concentration do not significantly influence the negative effect of customer satisfaction on future COS  (β5=.003,p>.10)  . Sensitivity AnalysesWe conduct several sensitivity analyses to examine the robustness of our conclusions across the use of alternative instruments, an instrument free approach, alternative model specifications, different measures of industry classification and of the focal dependent variable, and alternative sample composition. As elaborated in Web Appendix F, we continue to find broad support for the proposed hypotheses. Post Hoc Analyses: Splitting COS into Cost of Persuasion and Cost of ConvenienceGiven that COS reflects both cost of persuasion and convenience, a natural question concerns how the effects of customer satisfaction differ between these two components of COS. To explore this question, we first break down COS into cost of persuasion (comprising commissions and marketing and advertising expenses) and cost of convenience (comprising freight-out and bad debt expenses).[18] Next, we estimate separate models for the two components using the same specification as in our focal analysis.Table 5 outlines the results of our model for the future cost of persuasion and convenience. Across the two dependent variables, we continue to find that customer satisfaction has a significant negative main effect. In addition, we find that the negative effect of customer satisfaction on both the future cost of persuasion and convenience is stronger in industries with higher growth. Whereas the results for the impact of customer satisfaction on the future cost of persuasion are largely in line with the proposed hypotheses, those for its impact on the future cost of convenience differ in three keys ways.GraphTable 5. Splitting COS: Impact of Customer Satisfaction on Future Cost of Persuasion and Cost of Convenience.  12 *p <.10.13 **p <.05.14 ***p <.01.15 a We obtain the residuals from the estimation of the auxiliary regression in Web Appendix E, Table E1.16 Notes: Two-sided tests of significance. SE = bootstrap standard errors derived from performing 500 bootstrap replications using the approach illustrated in [85]; N (n) = Total number of observations (unique firms). All continuous variables are mean-centered and Winsorized at the 1st and 99th percentile levels. There are 1,207 observations from 128 firms.First, the magnitude of the impact of customer satisfaction on the future cost of persuasion is significantly higher than its negative effect on the future cost of convenience. Put differently, the benefits of higher customer satisfaction are more salient for persuading customers via initiatives of advertising, marketing spending, and commissions to sales personnel, as compared to spending efforts on providing convenience to customers. One plausible explanation for this observed difference concerns the higher stickiness of convenience from the perspective of customers. Indeed, prior research has suggested that convenience serves as a significant switching cost for customers ([92]), where they are likely to formulate their expectations of a firm's provision of convenience based on industry norms. As such, although satisfied customers tend to be loyal ([38]), a firm's ability to encourage actual repurchases may still be limited if it does not offer sufficient convenience ([102]). In fact, it is plausible that this higher salience of convenience for customers might explain the robustness of the negative effect of customer satisfaction on the future cost of convenience across levels of capital intensity, financial leverage, and labor intensity.Second, consistent with H2, the negative effect of customer satisfaction on the future cost of persuasion is stronger for diversified firms (i.e., the benefits of customer satisfaction are more salient for such firms because they can leverage these benefits across multiple business segments; e.g., [112]). However, the weaker negative effect of customer satisfaction on the future cost of convenience for more diversified firms is consistent with H2alt (i.e., benefits of satisfied customers are less salient for diversified firms because the operational complexity of such firms makes it difficult for them to leverage these benefits across diverse business segments; e.g., [66]). Indeed, one can argue that provision of convenience for customers requires significant and complex investments in logistics and financial structures (e.g., [93]).Third, the negative effect of customer satisfaction on the future cost of convenience is stronger for concentrated industries. This effect is consistent with the expectations of [92], who find that convenience is important for customers in concentrated industries. As industry concentration decreases, competitive offerings increase and customers' access to options increases as well. As such, even loyal customers may be tempted to shift their purchases to a competitor (e.g., [65]). Taken together, the post hoc analyses underscore the need for future research to theoretically and empirically examine the two components of COS (i.e., cost of persuasion and convenience). Discussion Theoretical ImplicationsSynthesizing the literature in economics ([13]), marketing ([68]), and operations ([58]), we present the first empirical study of the effect of customer satisfaction on the future COS of a firm. Indeed, extant research that examines the financial effects of marketing assets does not investigate their cost implications. Thus, our key theoretical contribution is in outlining the arguments for the negative effect of customer satisfaction on future COS and bringing to fore the firm and industry level contingencies. In addition, we also qualify the received view that customer satisfaction drives profits predominantly through its effects on revenue expansion by identifying it as an asset that enables a firm to achieve a ""dual emphasis"" of not only revenue expansion but also cost reduction ([65]; [75]).Our findings also complement existing research that identifies circumstances under which firms are more or less likely to utilize customer satisfaction information (e.g., [78]). First, we contribute to a nascent body of literature by showing that the strategic focus of a firm not only moderates the effectiveness of its spending behavior (see [72]) but also influences the impact of marketing assets, such as customer satisfaction on future COS. In addition, our post hoc analyses identify a boundary condition as we find that the negative effect of customer satisfaction on the cost of persuasion (convenience) is stronger (weaker) as firm diversification increases (decreases).Second, we contribute to prior work that underscores the importance of strategic flexibility in moderating the financial outcomes of marketing assets (e.g., [61]). Consistent with this stream of research, we find that the negative impact of customer satisfaction on future COS and cost of persuasion is weaker for firms with higher capital intensity and financial leverage. The negative impact of customer satisfaction on future cost of convenience, however, does not vary across levels of strategic flexibility.Third, the results also highlight the influence of firms' operating environment on the effect of customer satisfaction on future COS. In particular, we find that the negative impact of customer satisfaction on future COS strengthens as industry growth and industry labor intensity increase. Taken together, these findings contribute to theory development by complementing existing studies that examine the heterogeneity of customer satisfaction across different operating environments (e.g., [59]).The current study also has implications from a methodological perspective as we outline an approach to measure COS by isolating the ""selling"" component of a firm's SG&A. Given that firms do not publicly disclose COS as a separate item, existing research views advertising and sales force spending as indicators of the selling-related expenditures of the firm ([57]). To adopt a more comprehensive measure of selling-related expenses, studies also use COMPUSTAT's reported SG&A is also frequently utilized. Although these studies typically exclude R&D expenses in their measure (e.g., [76]), a firm's SG&A still consists of several other expense items that might not be relevant to its COS ([49]; [60]). In fact, we find that using aggregated SG&A-based measures of selling-related expenses can result in an overestimation of more than 20% when compared with our measure of COS (see Table 2). Importantly, our approach also has sufficient face validity as we find that in firms for which more detailed costs are available, our measure of COS can discriminate between firms with higher versus lower costs (see Web Appendix A, Table A2).Furthermore, by presenting an approach to isolate the ""selling"" component of a firm's SG&A, we also augment recent research that examines the suitability of using SG&A to capture different marketing-related concepts ([89]). Specifically, our findings indicate the need for a more nuanced approach to the use of SG&A in future research. Whereas the utilization of our approach in decomposing a firm's SG&A is beneficial for studies that aim to investigate the antecedents or consequences of COS or its specific components (i.e., cost of persuasion and convenience), it may not be necessary for instances where COS is to be included as a covariate. Managerial ImplicationsGiven that cost reduction is a top priority for CEOs ([90]), marketing managers are often pressured to produce the same product-market outcomes at lower costs ([53]). Thus, our findings have direct managerial implications. In particular, the economic significance of customer satisfaction's effect on COS has direct communication implications for senior marketing managers as we find that a one-point increase in ACSI customer satisfaction score on a 100-point scale lowers future COS by almost US$130 million for an average firm in our sample. Building on the results of the post hoc analyses, we also derive the dollar values of the impact of customer satisfaction for the specific components of COS. As illustrated in Table 6, higher customer satisfaction can lead to an approximate decrease of US$100 million in the future cost of persuasion and US$30 million in the future cost of convenience. These findings are of direct importance to CMOs as they can now articulate the economic value of customer satisfaction for reducing future COS to internal and external constituents. This is especially crucial from an internal perspective as prior research suggests that customer-satisfying executives are often underappreciated ([48]). In this way, CMOs can now incorporate our findings in their communications to the CEO and the chief financial officer to underscore the economic value of customer satisfaction.GraphTable 6. Marginal Effects of Customer Satisfaction on Components of Future COS Across Changes in the Moderating Variables.  17 *p <.10.18 **p <.05.19 ***p <.01.20 Notes: Two-sided tests of significance. High (low) = the marginal effects of customer satisfaction on future cost of persuasion (convenience) when the corresponding moderating variable is high (low); low − high = difference in the marginal effects of customer satisfaction on future cost of persuasion (convenience) when the corresponding moderating variable is low versus when it is high; high − low = difference in the marginal effects of customer satisfaction on future cost of persuasion (convenience) when the corresponding moderating variable is high versus when it is low. We identify the moderating variable to be high (low) if its value is equivalent to 80th (20th) percentile value of the distribution of the corresponding variable within our sample of 1,207 observations from 128 firms. We multiply the marginal effects by 100 for ease of interpretation and report their dollar values in millions of dollars. We derive the dollar values of the marginal effects by multiplying the marginal effects with the average firm sales (i.e., before Winsorizing and mean-centering) within our sample of 1,207 observations from 128 firms. We compute the marginal effects using the results from Table 5 and dollar values for marginal effects that are significant (p <.10, two-sided) are in boldface.From an external perspective, CMOs can use our findings to articulate customer satisfaction as a leading indicator of lower future COS values and can disclose it to investors and financial analysts who closely watch COS and its components. As such, our results complement recent work that encourages the incorporation of a customer lifetime value approach to firm valuation ([73]). Indeed, a direct implication for consulting firms and analysts that use the customer lifetime value approach for firm valuation is that their models should account for the effects of customer satisfaction on future COS.Results of our contingency framework also identify specific conditions under which senior managers can expect higher customer satisfaction to result in a higher or lower reduction in the future COS of the firm. In addition, these findings also have direct implications for firm valuation models, as they highlight critical contingencies.We find that the negative impact of customer satisfaction on the future cost of persuasion is stronger for more diversified firms, as they can enjoy almost US$60 million more in cost savings (see Table 6). In contrast, the cost savings in terms of the future cost of convenience is approximately US$9 million lower for more diversified firms (see Table 6). This suggests that despite the cost savings in terms of the future cost of persuasion, more diversified firms struggle to leverage the benefits of higher customer satisfaction to lower their future cost of convenience. A direct implication for managers in these firms is to evaluate their formal and informal mechanisms for sharing customer insights relating to the provision of convenience. Such an exercise could identify specific steps that they can take to enhance the effectiveness of customer satisfaction in lowering the future cost of convenience.We also find that the negative impact of customer satisfaction on the future cost of persuasion is weaker for firms with higher capital intensity. In fact, the difference in cost savings can be more than US$50 million (see Table 6). To overcome this relative disadvantage, managers in such firms can conduct a cost-benefit analysis to explore potential payoffs from increasing the salience of their positive word of mouth for prospective customers and from increasing customer-facing opportunities for their employees. Similarly, whereas managers in firms with higher financial leverage are less likely to utilize benefits of customer satisfaction due to urgent financial pressures, our results indicate that they should carefully assess whether their current systems and procedures are creating impediments to utilizing the benefits of customer satisfaction. It is plausible that due to managerial attention being diverted to servicing high levels of debt, resource allocation to selling activities are following suboptimal routines that do not consider higher levels of customer satisfaction.Our findings for firms' operating environments also bring to fore the nuances of the effect of customer satisfaction on COS and its components. First, the variance in the effect of customer satisfaction on COS and its components across the three industry conditions are of direct importance for the valuation models of financial analysts and investors that seek to understand the future COS for firms in such industries. Second, given that the received view is that the effect of customer satisfaction on financial performance is expected to be weaker in less concentrated industries ([ 5]), our findings alert managers to the financial benefits of customer satisfaction especially in terms of the lower future cost of persuasion in less concentrated industries as it can amount to almost US$40 million more in cost savings (see Table 6). At the same time, the weaker negative effect of customer satisfaction on the future cost of convenience in such industries suggests that managers will also need to reevaluate their efforts in terms of how they use their understanding of customers to allocate their spending in their provision of convenience.Finally, the variation in the effects of customer satisfaction on COS and its components across industry growth and labor intensity alert managers to reevaluate their mechanisms for allocating resources in selling efforts. For example, managers of firms with higher customer satisfaction in industries with lower growth and labor intensity should carefully consider whether they are overinvesting in selling efforts and not utilizing the benefits afforded by higher customer satisfaction. Limitations and Further ResearchThe current study not only presents the first step in understanding the effect of customer satisfaction on future COS but also offers several avenues for further research. First, more research is required to establish the generalizability of our results beyond the ACSI database as it comprises mainly large business-to-consumer firms that are publicly listed in the United States. There might exist differences in customer satisfaction across countries and cultures resulting in differing cost advantages ([79]). Second, given that firms are aware that ACSI tracks the satisfaction of their customers, it is possible for firms in the ACSI data set to be more conscious about their customer satisfaction levels compared with those that are not. As such, future research should explore the impact of customer satisfaction across a larger sample of firms. Third, prior research has suggested that some aspects of selling-related expenses can, in turn, also have an impact on higher customer satisfaction ([75]). For example, an advertising strategy that conveys higher quality could influence customer expectations and, therefore, customer satisfaction. As such, research is needed to examine specific sales efforts that are likely to have a cyclical relationship with customer satisfaction.Fourth, our post hoc analyses suggest that while the impact of customer satisfaction on the future cost of persuasion is largely consistent with the proposed hypotheses, those for its impact on the future cost of convenience are significantly different. As such, by decomposing COS into more components, it is possible to also identify other potential differences in the impact of customer satisfaction on the different components of COS. Fifth, given that firms often engage in initiatives to improve customer satisfaction ([91]), and that reducing costs is a critical concern for CEOs ([90]), more research is required to explore the effects of customer satisfaction on other cost related concepts. Finally, future research can also build on the current study to examine the cost implications of other marketing assets such as brand licensing. "
24,"Design Crowdsourcing: The Impact on New Product Performance of Sourcing Design Solutions from the ""Crowd"" The authors examine an increasingly popular open innovation practice, ""design crowdsourcing,"" wherein firms seek external inputs in the formof functional design solutions for new product development from the ""crowd."" They investigate conditions under which managers crowdsource design and determine whether such decisions subsequently boost product sales. The empirical analysis is guided by qualitative insights gathered from executive interviews. The authors use a novel data set from a pioneering crowdsourcing firm and find that three concept design characteristics—perceived usability, reliability, and technical complexity—are associated with the decision to crowdsource design. They use an instrumental variable method accounting for the endogenous nature of crowdsourcing decisions to understand when such a decision affects downstream sales. The authors find that design crowdsourcing is positively related to unit sales and that this effect is moderated by the idea quality of the initial product concept. Using a change-score analysis of consumer ratings, they find that design crowdsourcing enhances perceived reliability and usability. They discuss the strategic implications of involving the crowd, beyond ideation, in helping transform ideas into effective products.Increasingly, firms are tapping into a wide range of external sources of knowledge to source innovations (Chesbrough 2003; Laursen and Salter 2006). One popular facet of this new trend is the leveraging of online infrastructure to tap an underexplored and richly heterogeneous pool of knowledge resident in the general population of consumers for innovative ideas (Bayus 2013), a practice termed ""crowdsourcing."" Extant research on the efficacy of using external sources of knowledge for innovation has centered on the opportunity identification stage (Foss, Lyngsie, and Zahra 2013). For example, research in marketing has examined the practice of involving the crowd during ideation (e.g., Bayus 2013; Poetz and Schreier 2012) and has suggested that such early involvement in new product development (NPD) empowers potential consumers while enabling firms to attract more participants overall and more diverse participants to the idea generation process (Fuchs, Prandelli, and Schreier 2010; Schreier, Fuchs, and Dahl 2012).Recent evidence has suggested that the role of external knowledge sources may go beyond opportunity identification and extend to opportunity exploitation stages (Foss, Lyngsie, and Zahra 2013).Congruent with this idea, firms are increasingly using crowdsourcing in phases following ideation, specifically, in the solicitation of actionable design solutions (see Table 1), a practice we call ""design crowdsourcing."" We define design crowdsourcing as the practice of soliciting functional design[ 1] solutions from the crowd. For example, crowdsourcing platforms, such as Redclay.com, allow firms to submit new product design briefs and seek crowd input for the development and/or refinement of the design. In such situations, the client firm already has a new product idea, but may lack the resources or know-how to bring this idea into fruition, and, therefore, seeks external input from the broader user community to help create a manufacturable design.TABLE: TABLE 1 Examples of Companies Utilizing Design Crowdsourcing and Design Crowdsourcing Platforms TABLE 1 Examples of Companies Utilizing Design Crowdsourcing and Design Crowdsourcing Platforms  aSources of the data for the empirical analysis in the study.B.J. Allen is Assistant Professor of Marketing, Sam M. Walton College of Business, University of Arkansas (email: BAllen@walton.uark.edu). Deepa Chandrasekaran (corresponding author) is Assistant Professor of Marketing, University of Texas at San Antonio (email: deepa.chandrasekaran@utsa.edu). Suman Basuroy is Department Chair and Graham Weston Endowed Professor of Marketing, University of Texas at San Antonio (email: Suman.Basuroy@utsa.edu). The authors thank the Carolan Research Institute and Dr. Joel Saegert for helping fund this research; participants at the Marketing Science Conference and PDMA Research Forum for their helpful comments; and Dr. Ram Ranganathan, Dr. Raji Srinivasan, and Dr. Richard Gretz for detailed comments on earlier versions of the article. The authors are deeply grateful to all interviewees for their invaluable insights. Michael Haenlein served as area editor for this article.These compelling anecdotal examples raise interesting research questions: Does design crowdsourcing lead to a better new product development process? Does design crowdsourcing lead to improved products and performance? Whether and how crowdsourcing affects critical downstream activities, such as executing ideas in the NPD process, has received little research attention. In fact, studies have highlighted the significant challenges of internalizing external input into the NPD process, including the rejection of outside input by insiders (Katz and Allen 1982), the costs of distant searches (Afuah and Tucci 2012), and the difficulty of communicating tacit information needed for problem solving (Von Hippel 1994). Furthermore, scant research has linked crowdsourcing to product performance (for an exception, see Nishikawa, Schreier, and Ogawa 2013), and current literature has focused primarily on crowdsourcing during ideation.The goals of this research are to examine ( 1) whether and how design crowdsourcing affects the NPD process; ( 2) what design antecedents lie behind the decision to crowdsource; ( 3) whether design crowdsourcing has a positive impact on product performance—and, if so, to identify some boundary conditions for this effect; and ( 4) whether design crowdsourcing helps improve the functional design attributes of product ideas. We use the knowledge-based theory (Alavi and Leidner 2001; Chang and Taylor 2016), as well as exploratory insights from interviews (suggested by Kumar et al. [2016] to uncover new phenomena), to propose that ( 1) the crowd is a repository of design knowledge and design crowdsourcing is a mechanism that enables firms both to tap into the broader community for workable design solutions and to assimilate/exploit these solutions to aid the transformation of new product ideas into products, ( 2) such identification and the exploitation of external design solutions will improve new product performance, and ( 3) the efficacy of design crowdsourcing on performance will depend on the quality of the original product ideas.We test our hypotheses using a novel data set of 86 new products collected from Quirky, a pioneering, community-driven NPDwebsite. The empirical analysis on this data set indicates that the probability of design crowdsourcing was influenced by a need to increase perceived usability and reliability and to decrease technical complexity. Using an instrumental variable procedure (Wooldridge 2010) for dealingwith endogenous binary variables, we find that design crowdsourcing has a positive effect on sales, as proposed, with an important boundary condition. The positive impact of design crowdsourcing on sales is contingent on the idea quality of the original product concept—design crowdsourcing is associated with increased sales when the idea quality of the product concept is low. Furthermore, design crowdsourcing enhances perceived reliability and usability from idea to final product.Our results suggest that design crowdsourcing can help managers move a greater number of ideas through development by using the community's assistance in making (initially) lesspromising ideas marketable, thus improving the effectiveness of the NPD process. Rather than discarding such ideas, firms may use external sources of knowledge to develop them and interact with these sources extensively to ensure that the outcome is of high quality. In addition, we highlight specific design functionalities that managers can improve using design crowdsourcing, allowing for a more targeted approach when leveraging crowdsourcing. Finally, our findings suggest opportunities for crowdsourcing platforms to market themselves as solution spaces that provide tangible downstream benefits through enhanced functional attributes. The next sections present the conceptual development as well as managerial insights into design crowdsourcing leading to the hypotheses, data, modeling methodology, results, and discussion. Conceptual Development Design Crowdsourcing and Knowledge ManagementGrant (1996, p. 112) states that ""fundamental to a knowledge-based theory of the firm is the assumption that the critical input in production and the primary source of value is knowledge."" A firm's ability both to create new knowledge and to apply knowledge forms the basis of developing a competitive advantage (Alavi and Leidner 2001). One online mechanism that grants firms access to a wide, diverse knowledge pool is crowdsourcing (Schreier, Fuchs, and Dahl 2012). Extant marketing literature has treated the crowd as a resource base for new ideas and treated crowdsourcing as amechanism that enables the identification of new ideas from this resource base. However, firms may also need to engage external resources, such as the crowd, for opportunity exploitation (Foss, Lyngsie, and Zahra 2013).We propose that the crowd is also a knowledge source for design solutions, which are utilized to solve firm-specific problems in the context of new product development.Knowledgemanagement theory suggests that the identification/ acquisition and assimilation/exploitation of externally generated knowledge improves innovation performance (Cohen and Levinthal 1990). However, in the context of design crowdsourcing, little is understood about how this process manifests itself in practice. Given the lack of research into design crowdsourcing, our research begins with qualitative interviews to investigate this question and then integrates the findings from the interviews with a literature review. Qualitative InterviewsWe conducted exploratory interviews with practitioners from the United States, China, Italy, Israel, and the United Kingdom who had extensive experience with crowdsourcing (see Table 2). Because the purpose of these interviews was to assist in theory development, we ensured that the interviewees were familiar either with the experiences of established firms that engaged in crowdsourcing or with start-ups whose business models involved crowdsourcing. We followed a standard format and approach for each interview.[ 2] The authors carefully read the interview transcripts and notes and documented the main concepts and themes that emerged.TABLE: TABLE 2 Description of Managerial Interviews TABLE 2 Description of Managerial Interviews   Insights on Design Crowdsourcing and New Product PerformanceIn this subsection, we explore the link between design crowdsourcing and new product performance by utilizing common themes from the interviews and the literature review.Design crowdsourcing helps firms move product ideas into development. ""How can I execute my innovative ideas?"" is a question that represents an increasing concern for chief executive officers and business executives (eYeka 2016). For example, an executive of a design crowdsourcing firm noted this about her clients:These people come with new ideas in innovation; they have a great innovation, but they're not really sure how to make that innovation happen. (Cofounder and chief operating officer, design crowdsourcing firm)Design crowdsourcing helps make development a reality in situations where firms know what product or solution they want but are looking for an executable design. The difference between using the crowd to obtain design solutions and using the crowd for ideation itself seems to be twofold: First, the emphasis of ideation crowdsourcing may be on an unconstrained flow of ideas,whereas design crowdsourcing involves the crowd tackling a focused need and, thus, all submissions and iterations are working toward solving the same problem. Second, the solution space in design crowdsourcing may also be smaller (i.e., more manageable). From our investigation of design crowdsourcing websites, the number of design submissions (being in the tens or hundreds and not thousands, as, for instance, in ideation challenges) were more tractable for clients (especially small businesses). Thus, design crowdsourcing moves product ideas closer to development and, thus, to delivering value:Different crowds [are viewed] as layers of technology that can powerfully work together.In online sourcing from a crowd … you're connecting multiple people to get to an end solution. The ideas become more powerful when you bring these different skill sets together … to get to a solution a little more efficiently. (Cofounder and chief operating officer, third-party design crowdsourcing platform)Design crowdsourcing helps identify new sources for and types of design solutions. A consistent theme from our interviews was that although in-house specialists may be constrained by their past experience while trying to create new design solutions, crowdsourcing brings in novel and fresh solutions to design problems. For example, when asked why managers would crowdsource product design, one expert noted:[Managers are influenced by] a desire to bring a fresh insight to the design process. Crowdsourcing can help with the design process by bringing new ways of thinking and unique ideas. An in-house team can be impacted by things like legacy ideas, office politics, and being too close to the product. By bringing in outside help it brings a fresh approach, which aids the creative process. (Editor and author on crowdsourcing)When asked whether there were differences in the kinds of solutions companies were looking for, an executive commented on the criticality of diversity of perspectives:I think it really depends on the company because some of our smaller and medium sized companies don't have any design talent in-house, so they're really looking for that design. Then the companies that do have design talent in-house, they're looking to get more of a new perspective and understanding that when you pull more than two designers into a project, you're going to get a very diverse amount of perspective, which starts to really begin the true design thinking of why we design and go through the full process, which is pulling those different ideas together, iterating on them this idea that there's a community to build on them versus one person'sway of thinking. (Cofounder and chief operating officer, design crowdsourcing firm)This point is consistent with the literature that finds that user involvement in design generates greater numbers of diverse, need-specific, and unconstrained designs (Schreier, Fuchs, and Dahl 2012) compared with in-house design. Furthermore, managers believed that the utilization of the crowd led to a greater congruency between design and user needs. As noted by a leading design expert:Just having ideas doesn't work. The question is, really, who are you solving it for? Insights and the human side of design is the most important aspect you can add…. I think using design as a differentiator is what we're seeing in the market. (Former president of a leading crowdsourcing firm/design consultancy group)Managers are continually looking for ways to respond to consumers' wants and needs in a way that optimizes firm resources (Fennell and Saegert 2004). Firms are thus able to use design crowdsourcing to integrate knowledge to develop a product more congruent with consumer needs, which is more likely to succeed when it enters the marketplace.Design crowdsourcing increases available resources for NPD. Nearly every manager interviewed mentioned that design crowdsourcing serves as a resource-supplement strategy that simplifies and accelerates the flow of the NPD process:A lot of those (client) companies are small. They need to move quick and they need to keep their prices down, so budget becomes a big concern. Innovation becomes a concern. (Cofounder and chief operating officer, design crowdsourcing firm)I'm doing all the things that I'm doing as a typical product development cycle, but I'm actually accelerating that by getting the crowd involved…. You know your product, you know your design. You know what you're good at, but you're intentionally leveraging the crowd to get into the market fast. (Vice president, crowdsourcing consulting firm)Popular press publications also use words like ""efficiency,"" ""simplify,"" and ""streamline"" when describing why firms crowdsource during NPD. Traditional NPD processes are constrained by resource availability, such that only a small number of the ""best"" ideas can be implemented. Accessing the crowd increases the knowledge resources available to a firm by both leveraging the skills and expertise of hundreds of people outside the organization and freeing up firm resources, allowing for the development of a greater number of ideas.Design crowdsourcing may be iterative and collaborative. Design crowdsourcing is not just about obtaining new ideas but also about refining and fine-tuning ideas, and it provides the capability to engage in a high degree of collaboration with the broader community. This represents one of the key differences from traditional ideation crowdsourcing, wherein the firm may select novel ideas, but there is not much collaboration going forward (Bayus 2013). The selected designers, suppliers, and clients often (depending on the crowdsourcing platform)work together collectively, using insights they gain from design submissions to iterate toward a manufactureready product. Because members of the ""crowd"" are neither familiar specialists nor a part of the internal team, there is a need for closermonitoring and internal involvement to move toward a solution. Furthermore, the process of iteration often results in a better translation of tacit suggestions to workable solutions:My crowd is going to be an extended team within my company. (Vice president, crowdsourcing consulting firm) Today we have an on demand industrial design community…. They start to look at a lot of these different crowds as layers of technology that can powerfully be able to start the work together.In online sourcing from a crowd [you are not just] able to connect [with one person], but you're connecting multiple people to get to an end solution. (Cofounder and chief operating officer, design crowdsourcing platform)In summary, a firm has the choice of whether to involve the crowd in the design phase or to simply refine the product design in-house. Our exploratory insights suggest that design crowdsourcing enables firms to ( 1) translate ideas into executable solutions, ( 2) provide access to new sources that can provide novel and meaningful design solutions, ( 3) help increase the resources available for NPD, and ( 4) help create a more iterative and collaborative process in integrating external solutions with inhouse guidance. Because a firm's ""ability to identify, assimilate, and exploit knowledge from the environment"" is related to a firm's innovative performance (Cohen and Levinthal 1990, p. 128), it follows that identifying, assimilating, and exploiting knowledge using design crowdsourcing should increase a new product's performance.In the specific case of NPD, we expect all four of these factors to contribute to new product success, as prior literature has suggested that ( 1) the development of an increased number of new products that more accurately reflect customer preferences during the NPD process improves NPD performance (Joshi and Sharma 2004); ( 2) design newness and creative solutions that are novel and meaningful to consumer needs are key determinants of new product success (Talke et al. 2009); ( 3) slack creates resources that help better exploit existing competencies, explore new competencies, and develop innovations (Atuahene-Gima 2005); and ( 4) conscious and meaningful customer interactions in the form of engaging with the design of new products (along with or in addition to ideas) will provide a differentiating advantage to the firmin themarketplace (Ramani and Kumar 2008).Thus, drawing on the insights derived from our interviews and past theory, we propose:H1: Design crowdsourcing has a positive effect on new product performance. Moderating Effect of Initial Idea QualityA key premise of the prior hypothesizing on crowdsourcing's positive effect on new product performance is that crowdsourcing the design will help make the product more marketable. What if the initial raw concept (idea) was already marketable? Kornish and Ulrich (2014) establish that better ideas, as assessed by commercial value (purchase intent of the raw concepts), lead to increased sales. Their finding raises two important questions: ( 1) Is there incremental value added by involving the crowd in suggesting design solutions if the initial product idea itself is good? and ( 2) Can firms extract value from lower-quality ideas rather than from discarding them?The managerial insights showed that design crowdsourcing likely leads to an evolutionary process of the new product idea. As one manager said of a product that she managed, ""The product just kept developing and iterating."" Because design crowdsourcing draws on the knowledge of the crowd to improve functional attributes and involves a process of iteration, it is likely that product ideas with a significant need for improvement will benefit most from the process. We propose that when the idea quality is low, the incremental value of design crowdsourcing will be high. When the initial quality of the raw idea is high, the firm might be better off with in-house design.This line of thinking is consistent with the broader nature of organizational conflict in the exploration of new and exploitation of current knowledge. Andriopoulos and Lewis (2009) conduct a comparative case study approach of five leading ambidextrous firms in the product design industry. They note that whereas exploitation demands efficiency and convergent thinking to improve product offerings, exploration involves search and experimentation efforts to generate novel recombinations of knowledge, creating tension. Furthermore, there is tension between, on the one hand, the use of standardized best practices for NPD that may breed rigidity, and, on the other hand, engagement in new routines that may bring in fresh thinking and free up resources but may also be less efficient. Organizations often have best practices and routines in place to progress their most promising ideas with their in-house research and development/design teams. Thus, for the best ideas, design crowdsourcing may be less beneficial, because the challenges associated with processing and assimilating new and diverse design solutions may outweigh potential benefits. However, for less marketable ideas, design crowdsourcing may facilitate the process with better interaction to help evolve and develop ideas, leading to better performance. Furthermore, lower-quality concepts can be used as an opportunity to learn which attributes are important to customers, which in turn helps firms develop higher-quality products (e.g., Ries 2011, p. 107). Design crowdsourcing can help uncover such attributes to improve the NPD process. Thus, we propose,H2: The positive impact of design crowdsourcing on new product performance is greatest for products with low initial idea quality. Functional Design Attributes as Antecedents to Design CrowdsourcingDesign crowdsourcing is not a one-size-fits-all strategy to be leveraged ubiquitously. Rather, as one executive noted, ""[it needs to be] a very cautious and well-designed, well-thoughtout approach."" The decision to design crowdsource is strategic and based on product, people, and cost considerations. The next question we consider is how specific design attributes of the product conceptmay guide the choice of design crowdsourcing. We searched extensively within various literature streams for product design attributes that influence product success and user acceptance (e.g., Poetz and Schreier 2012). We retained five influential design attributes judged by current literature to be relevant and useful in enhancing user response and experience, as well as two core objectives for the utilization of external inputs from users.[ 3] We then assessed whether the crowd's design knowledge and inputs may help better these attributes. Next, we describe briefly how these functional attributes influence decisions to crowdsource (see Web Appendix Table WA1 for references to these attributes from extant literature and managers).Technical complexity. We define technical complexity as the perceived degree of complexity due to the technical nature of the design. New products in their initial phases are often complex and need to be simplified. The more complex the design, the costlier it is to build, sell, and service a product (Radjou and Prabhu 2015) and the greater the need for access to a wider range of capabilities, user involvement, and design choices (Gann and Salter 2000; Hobday 2000). Insights from theory and practice (Web Appendix TableWA1) suggest that managers may use design crowdsourcing to simplify the technical complexity of the product. Thus, we expect that the probability of design crowdsourcing will increase with increased levels of perceived technical complexity of the product idea.Usefulness. Usefulness is defined as the product's ability to meet customer needs (Moldovan, Goldenberg, and Chattopadhyay 2011). User-designed products are perceived as better able to meet the needs of customers than professionally designed products (Poetz and Schreier 2012). Drawing on extant literature and current practice (Web Appendix Table WA1), we expect that the probability of design crowdsourcing will increase with lower levels of perceived usefulness of the product idea.Reliability. Perceived reliability relates to how well a product is likely to perform, encompasses aspects such as durability and dependability (e.g., Grewal et al. 1998), and influences perceptions of value and purchase intentions. Managers may look to the crowd for ideas on enhancing reliability. For instance, many design proposals on Crowdspring (a design crowdsourcing website; see Table 1) use the words ""durable,"" and ""reliable"" in describing what they want in a product design sketch. Even when someone is given a simple product brief, evaluations of durability can be assessed. For example, one industrial designer working on a simple paper sketch, said, ""whenever I am sketching, I want to make sure … it looks durable, that's going to have to come across in the overall design"" (Troy 2015). Thus, we expect that the probability of design crowdsourcing will increase with lower levels of perceived reliability of the product idea.Usability. We define perceived usability as the expected extent of effort (physical or mental) required to use the new product. March (1994, p. 144) notes that ""user-centered design … encompass[es] the cognitive aspects of using and interacting with a product, or how logical and natural a product is to use."" Thus, user inputsmay be valuable in enhancing usability of the concept, which can be assessed in early stages (see additional insights in Web Appendix Table WA1). For example, in the product briefs submitted to Crowdspring, managers requested a product that ""is easy to setup and use,"" ""will be easy to install,"" and ""is unobtrusive and easy to use."" Thus, we expect that the probability of design crowdsourcing will increase with lower levels of perceived usability of the product idea.Novelty. Novelty refers to the degree of newness or originality of the product (e.g., Moldovan, Goldenberg, and Chattopadhyay 2011; Talke et al. 2009). Poetz and Schreier (2012) demonstrate that user-designed products scored higher on novelty than professionally designed products (see practice insights in Web Appendix Table WA1). Thus, we expect that product ideas with lower perceived novelty will have a higher likelihood of being crowdsourced. In summary,H3: The probability of design crowdsourcing increases with perceptions of (a) higher levels of technical complexity, (b) lower levels of usefulness, (c) lower levels of reliability, (d) lower levels of usability, and (e) lower levels of novelty of the original product concept.Nonlinearity of antecedents. While the associations proposed in H3 relate to the initial directional nature of the relationships, these relationships need not be strictly monotonic. For example, when creating new products, Rust, Thompson, and Hamilton (2006) recommend offering enough functionality for the product to not be too simplistic, but not so much that consumers perceive the product as being too difficult to use, suggesting a nonlinear effect of usability on product success. Similarly, while managers noted that they are more likely to use crowdsourcing as technical complexity increases, as one of the interviewed managers noted, in some instances crowdsourcing is not possible, ""because you cannot expect the general crowd to be intelligent in terms of your mechanical [engineering]."" In the absence of a specific theory, we do not propose precise directions but leave it to the empirics to model these nonlinearities. We synthesize these collective insights and theories into Figures 1 (broad conceptual framework) and 2 (specific design crowdsourcing—performance link). Data Empirical ContextWe collected data on new product concepts from publicly available information from Quirky, a pioneering, community-driven NPD website where members submit new product ideas and participate in development efforts. Staff sorted through idea submissions and selected idea(s) to move forward. Once an idea was selected, Quirky's management decided what help they wanted from the community. The Wall Street Journal described the process as such:Each week, Quirky's staff whittles down the stream of new ideas into a dozen or so top picks that are scrutinized and voted on…. At that point, engineers and designers, working out [of] a vast red brick warehouse in New York and three other locations, turn sketches into marketable products, tapping the online community for suggestions about design, product names and price points. (Simon 2014)Quirky's community members were promised a portion of the product sales in exchange for their participation. Quirky's staff chose to ask for help in designing the product and selecting the name, logo, or pricing, or any combination thereof. We captured whether Quirky asked its customers to aid in the design phase. As stated on the website, in the design phase, Quirky asked its community members to ""submit sketches, images, videos, and prototypes that illustrate industrial design directions for [the product]. We'll use the top concepts as a starting point for our final design."" The staff made the final decision on design selection. Quirky had no obligation to utilize any ideas from the community, and the staff selected the phases in which to involve the community. Note that this is a similar situation faced by firms that have a product idea and must decide whether to further develop the design in-house or crowdsource the design. Description of Empirical Setting and DataOur data set includes the 86 different products sold on the Quirky website during our data collection period in October 2014. Quirky was very transparent with details about the NPD process, design contributions, and sales, which makes this original data set unique and valuable for addressing our research questions. We gathered three key pieces of information from the website: First, we retrieved the raw new product idea as submitted by the original community member, including the text describing the product and, if available, pictures or sketches submitted by the original inventor. (Web Appendix Figure WA1 provides an example of the Quirky design process.) Second, we collected information on whether Quirky subsequently asked the community to help with the product design (yes/no). The third key variable we collected from the website was monthly unit sales for each product, which Quirky published on its website. Measures: Key Dependent, Independent, and Control VariablesNew product performance. To assess new product performance, we used data on total sales of new products (including sales from both its website and retailing partners [e.g., Amazon, Costco]). Because not all products are released at the same time, we used total units sold in the first year, starting with the first complete month that Quirky reported. Of the products in our data set, we observed sales for a full 12-month period for 66 products (77%). For the remaining 20 products, we used a simple three-month moving average approach to estimate the sales for the missing months.[ 4]Decision to design crowdsource. We collected information on whether Quirky asked the community to help in the functional design of each of its selected ideas (yes/no). This event was clearly defined by Quirky as the ""design phase."" In response to such requests for help, community members submitted product drawings or sketches from which the Quirky staff selected the best one. Community members had a high degree of autonomy when it came to submitting designs. The submission could be similar or different from the original idea; all that was required was that it kept to the general essence of the product's purpose. Of the products in our data set, 22 (26%) did not go through a design phase with community help.Functional attributes of design. We utilized consumer ratings of the functional design attributes because managerial crowdsourcing decisions will be based on consumer perceptions. As one manager noted, ""This leads to your … question [about when one incorporates the end user]. I believe it is key to empathize with the end user(s) throughout the entire design process."" We recruited 119 undergraduate business students at a large U.S. university to assess raw product ideas. We took a similar block design approach as in prior research and divided the 86 products into 14 different blocks, with 6–7 products in each block (Kornish and Ulrich 2014), to simplify the survey and minimize respondent fatigue. Each raw design for each final product sold on Quirky's website was randomly assigned to one of the 14 blocks.[ 5]Each respondent viewed the product concept (picture and description) and was asked to evaluate each design on items relating to technical complexity, usefulness, reliability, usability, and novelty. Each product was evaluated seven times, on average, by independent raters. Responses to each question were averaged across the respondents who evaluated that product. The scale questions and their reliabilities appear in Table 3. To check the validity of the model, we successfully tested the construct scales using a confirmatory factor analysis. The root mean square error of approximation is .085, comparative fit index is .964, Tucker—Lewis index is .949, and average variance extracted is .800, all of which show good measurement validity. While larger samples are usually desirable when performing confirmatory factor analyses, these fit statistics provide confidence that our constructs meet validity assumptions. All questions were on a 1–7 Likert scale, anchored by ""strongly disagree"" and ""strongly agree."" We had one filtering question to filter out those respondents who were not paying attention (discussed in Table 3). All construct scales were averaged across their scale questions to create composite construct scores.TABLE: TABLE 3 Product Concept Constructs and Reliabilities TABLE 3 Product Concept Constructs and Reliabilities  aOn a Likert scale measured by 1 = ""strongly disagree,"" and 7 = ""strongly agree."" bCronbach's alpha or correlations if two-item scale.Notes: To filter out respondents who were not paying attention, we included a test question worded, ""If you are paying attention, click on 'somewhat agree' in the evaluation of each product."" Any respondent who missed multiple test questions across the products they evaluated were removed from the analysis. This left us with 97 of the 119 respondents.Idea quality. Along with the design constructs, after viewing the product concept, respondents answered three questions (scale items in Table 3) capturing the purchase intent construct (adapted from Schreier, Fuchs, and Dahl 2012). Similar to prior research, we use purchase intent as the measure of idea quality (e.g., Kornish and Ulrich 2014).Control variables. Wecollected data for product category, price, and characteristics of the idea's inventor. Our products fall into five categories, as defined by Quirky: electronics (37 products), home (17 products), kitchen (23 products), and travel and health (9 products combined). Given the small number of products in the travel and health categories, we reclassified these products into one of the other three categories. Modeling Methodology and ResultsWe tested our hypotheses using two different models that are integrated in a two-step process. First, we modeled the factors that influence whether firms will crowd source product designs using a binary response (probit) model. In this case, we modeled the dichotomous outcome (crowd source design: yes/no) against the various ratings of design attributes of the submitted product concepts (e.g., usability, novelty) and other variables. Second, we examined the impact of crowd sourcing product designs on market outcomes (unit sales) using an instrumental variable regression controlling for the endogenous nature of the crowd sourcing decision (Wooldridge 2010), since the firm self selects which products will be crowdsourced (i.e., the event is not purely exogenous). The two-step process allows us to observe the variables that influence the decision to crowdsource design and enables us to utilize the probit model to address the endogenous nature of the crowdsourcing variable in the model predicting sales.Wooldridge (2010) outlines the following procedure for dealing with endogenous binary variables that utilizes the following process, where Y represents the dependent variable of interest (in our case, unit sales), G represents the binary endogenous variable (design crowdsourcing), Z represents the instruments, and X represents the vector of control variables: ( 1) estimate a binary choice model of dichotomous variable G on Z and a set of controls X, ( 2) obtain the fitted probabilities of Ĝ estimated in step 1, and ( 3) estimate a two stage least squares (2SLS) instrumental regression model, regressing Y on G and X, using Ĝ as an instrument for G. This procedure has a few notable advantages. First, it takes the binary property of the endogenous variable into account. Other procedures, such as the standard 2SLS, may produce biased estimates in finite samples. Second, note that this is different than directly inserting the fitted probabilities of the probit in place of the endogenous variable. Using the estimated probabilities in place of the dichotomous variable in a standard ordinary least squares requires very strict assumptions on the error terms and the functional form to be a valid option (Adams, Almeida, and Ferreira 2009). Third, the 2SLS procedure is robust to misspecification in the probit model and provides consistent estimates with asymptotically valid errors when using standard corrections for heteroskedasticity in the instrumental variable estimation (Adams, Almeida, and Ferreira 2009; Wooldridge 2010, p. 939, procedure 21.1). We present these two models sequentially next. Model SpecificationModel for predicting product performance. The model used to test the relationship between crowdsourcing the design and performance, as measured by unit sales, can be represented by the following functional form:Due to image rights restrictions, multiple line equation(s) cannot be graphically displayed. ( 1)where Ln (UnitSales)i is the natural log of all units sold for product i in the first year. CrowdsourceDesigni is a dummy variable that takes on a value of 1 if the product design was crowdsourced and 0 otherwise. Idea Qualityi is the initial idea quality of the product and is measured via purchase intent for the raw concept as discussed previously (Table 3). We created an interaction term between CrowdsourceDesigni and Idea Qualityi to test for the moderating effect of idea quality on design crowdsourcing. We hypothesized this interaction to be negative, indicating that design crowdsourcing is less impactful for high-quality product ideas. HolidayLaunchi is a dummy variable that controls for whether the product was first introduced (its first few months on themarket) during the holiday season (November or December) to control for the positive proliferation effect that may come from launching the product during a high-volume period. Ln(Price)i is the natural log of the selling price of the product at the time of data collection. PCharacteristicsi represents the five design-related constructs that we predict will influence design crowdsourcing, along with their squared terms. We inserted these as control variables because it is possible that these constructs will also affect the unit sales of the product. In addition, because we predicted that they will influence the decision of whether to crowdsource the design, we included them as controls to assure that the crowdsourcing variable is capturing variance unique to crowdsourcing's effect. PCategoryi represents a vector of product category dummies.Instrumenting for price. In addition, price is often considered an endogenous variable, given the simultaneous relationship between price and demand. Cost is used as an instrument for price because it is a determinant of price but remains orthogonal to the error term (e.g., Rossi 2014). Rossi (2014, p. 666) states that ""the idea here is that costs do not affect demand and therefore serve to push around price (via some sort of mark-up equation) but are uncorrelated with the unobserved demand shock."" The cost of raw materials does not influence demand because consumers are not aware of the cost of the items. We used the cost of raw materials as an instrument for price. Following Kornish and Ulrich (2014), for the Quirky data, we used the pictures and descriptions of the final products being sold on the website to estimate the cost of the materials used to manufacture the product. We recruited three mechanical engineering doctoral students from the same university, with an average age of 28.5 years and allwith industrywork experience. These students estimated the cost of the raw materials used to produce the final product in a separate task. The three doctoral students first researched the current market costs of raw materials (e.g., metal, plastic, cotton) and then used the product pictures and descriptions of the final product to estimate the cost of the raw materials (in dollars) used tomanufacture the product. We averaged their cost estimates to develop an instrument for price and utilized the instrument in the 2SLS procedure. The correlation between the natural log of price and the natural log of the raw material cost is .792.Model for predicting design crowdsourcing. Following the methodology outlined previously, we specify the model predicting whether an item will be crowdsourced using a discrete choice specification. We derive a probit model for the design crowdsourcing decision of the new product concept i:Due to image rights restrictions, multiple line equation(s) cannot be graphically displayed. ( 2)where Φ is the standard normal cdf, and( 3) where Technicali, Usefuli, Reliabilityi, Usabilityi, and Noveltyi correspond to their respective construct ratings. These constructs were measured by the construct scores developed from the surveys described previously. In addition to each linear term, we also included a quadratic term for each of these constructs. Controlsi represents a vector of control variables. As noted by Wooldridge (2010), the probit model should contain all exogenous control variables that are inserted in Equation 1. Instrumentsi represents a vector of variables used as instruments (explained in detail subsequently). Instruments and Exclusion RestrictionsSo that the probit (Equation 2) results can be utilized in the 2SLS procedure for Equation 1, the probit model must contain additional instruments (as noted by Instrumentsi in Equation 2) that are not simultaneously listed in Equation 1. These variables should influence design crowdsourcing decisions but should remain unrelated to unit sales. Next,we describe the instruments (inventor characteristics and cost variables) and justify their use as instruments (Rossi 2014).Social network: number of community members the inventor is following. The managers in our interviews highlighted that a primary motivation behind crowdsourcing is to secure the engagement of many people. When managers crowdsource, they must forecast the likelihood that there will be enough potential problem-solvers (Afuah and Tucci 2012) to ensure diverse and better solutions. Thus, we seek a variable that will signal to the firm that a large number of people are likely to participate in providing solutions.We propose that the inventor's social network size is a good proxy for the likelihood that a large number of people will be aware of the product and, thus, will participate in the crowdsourcing process. Indeed, social connections of the inventor are something firms take into account (Lohr 2015). Social networks can lead to a better crowdsourcing process, due to improved reciprocity, collaboration, feedback, and integration of ideas (Piller,Vossen, and Ihl 2012). Therefore, we have a strong rationale that the social network of the inventor will matter in the decision to crowdsource design.We look for a variable that approximates the inventor's social network and meets exclusion restrictions. The Quirky community profile allowed us to capture the number of people the inventor is ""following."" The act of following forms a tie in the networks literature (e.g., McGee, Caverlee, and Cheng 2013), where the strength of the tie is indicated by markers such as reciprocity in following. The literature predicts that there is greater mobility of information and social cohesion through weak ties than through strong ties (Granovetter 1973), where weak ties represent links with distant acquaintances, such as, in this context, following someone on an online network. Thus, irrespective of whether the follower is followed back, the act of following is a tie and all such ties form the social network. Furthermore, businesses monitor brand-related conversations on social media platforms to gain access to valuable information, influential people, and relevant conversations (Kumar and Mirchandani 2012). Similarly, by choosing to follow other people/inventors, the inventor keeps abreast of any key developments (e.g., inventions, opinions, trends). The act of following is an act of engagement/listening (e.g., Crawford 2009) and not an entirely costless act, as the inventor may choose to follow people on a crowdsourcing platform like Quirky depending on his or her available time and cognitive resources. Thus, an inventor who is following a large number of people belongs to a larger network with the attendant reciprocal advantages, and his or her ideas are more likely to benefit from a better crowdsourcing process. We show subsequently that this measure is also statistically informative. Thus, the number of people the inventor follows is an informative and relevant instrument that meets exclusion restriction requirements because it is unlikely to directly affect sales, as this measure is not salient to the average buyer on Quirky's website.[ 6] In addition, the measure contains exogenous information driving design crowdsourcing. The number of people the inventor follows is the decision of the inventor and not a result of external forces.Product cost. We used the cost (the same instrument used for price) of the item as a second instrument. Nearly every manager we interviewed highlighted crowdsourcing as a way to reduce manufacturing and production costs. Thus, because the cost of goods sold includes the cost of raw materials and the production cost of the individual item, if the cost of raw materials is high, managers are more likely to look for ways to reduce production costs; this will enable the firm to keep the total cost as low as possible. As noted previously, we used the doctoral students to estimate the cost of the materials used to manufacture the product. This measure of cost is a valid instrument, as it is unlikely that cost directly affects consumer demand (Rossi 2014), but it does affect the crowdsourcing decision. We further included a quadratic term for cost, because not all increases in costswill be associated with the same increases in crowdsourcing. For example, the change in probability of crowdsourcing between items that cost $10 versus $20 might be quite different than between items that cost $200 versus $210.Finally, we also included interaction terms between the instruments—namely, cost and the number of community members that the inventor is following. The desire to lower costs will trump other external cues (such as the inventor's network) to determine whether the firm should crowdsource. Thus, the hypothesized direction for the number of people the inventor is following will hold at low levels of cost, but the effect should be nonlinear per our managerial insights, which we capture with the interaction with cost. The probit model indicates that all of the instruments are significant (see Table 4), and the pseudoR2 increases from .286 to .476 with the inclusion of these instruments. We next discuss results for Equation 2 and then Equation 1.TABLE: TABLE 4 Probit Model: Predicting Design Crowdsource TABLE 4 Probit Model: Predicting Design Crowdsource  Notes: For brevity, product category dummies along with the constant are estimated but not displayed. Robust standard errors are presented. The z-valuesforthe instruments are as follows: Following (z = 2.91), Ln(ItemCost) (z = 1.89), Ln(ItemCost)2 (z = -2.22), Following χ Ln(ItemCost) (z = -3.39), Following χ Ln(ItemCost)2 (z = 3.62). Characteristics That Influence Design CrowdsourcingThe summary statistics and correlation matrix are included in Table WA2 of the Web Appendix.[ 7] Table 4 displays the results for Equation 2. Heteroskedasticity-robust standard errors are used in computing the Wald tests. The results show that while the estimated coefficients for Useful and Novelty are not significantly different from zero, the linear and quadratic terms for all the other constructs are statistically significant. Specifically, Usabilityi has a negative linear term (b = -62.245, p = .001) and a positive quadratic term (b = 5.418, p = .001); this suggests that, initially, the probability of crowdsourcing design decreases with an increase in usability. After the perceived usability reaches a certain level, the probability of crowdsourcing design increases. This suggests that firms are more likely to crowdsource designs that appear overly difficult or too easy to use. The lowest probability occurs at about its mean,where the squared term dominates the linear term, around 5.74 on the 7-point Likert scale (≈ 62.245/[2 × 5.418]). The effect of perceived reliability of the raw concept on the choice to crowdsource design follows a somewhat similar pattern. The negative linear term (β = -32.391, p = .004) suggests that as the perceived reliability of the concept increases, the probability of crowdsourcing design decreases. Thus, at low levels of perceived reliability, the firm is more likely to seek the help of the community in developing the design. The positive quadratic term (β = 3.158, p = .005) suggests that after a certain level of reliability—which occurs at roughly 5.13 (≈ 32.391/[2 × 3.158]), increases in perceived reliability are not associated with a decrease in design crowdsourcing.Technical complexity (Technicali) of the product follows a pattern opposite those of usability and reliability. The positive linear coefficient (β = 4.322, p = .006) demonstrates that the more technical a product idea is, the more likely the firm is to crowdsource design. However, with higher levels of technical complexity, marginal increases in technical complexity are associated with a decreasing probability of crowdsourcing, as indicated by its negative quadratic term (β = -.685, p = .001). The inverted U-shape of technicality shows that the highest probability of crowdsourcing is 3.155 (≈ 4.322/[2 × .685]) on the 7-point scale, with the lowest probability occurring at the ends. This supports the notion presented by some of the managerial insights that firms are less likely to crowdsource designs that are too technical, because the community will lack the needed expertise, and supports extant research that suggests that crowdsourcing is less likelywhen a firm doubts the crowd's ability/expertise to evaluate solutions (Afuah and Tucci 2012).Overall, these results suggest a strong relationship between the probability of crowdsourcing a design and the design attributes of the raw product concept. The constructs we hypothesized, with the exception of usefulness and novelty, were related to the decision to design crowdsource, in support of H3a, H3c, andH3d.We discuss further validation of the importance of these design characteristics on design crowdsourcing decisions next. Validating the Importance of Design Characteristics on Design Crowdsourcing DecisionsWe next demonstrate that these design constructs influence similar decisions in a different data context. The following analysis is not meant to replicate the exact same decision but to provide convergent evidence that design constructs drive design crowdsourcing decisions.Our second data set comes from Crowdspring, an online crowdsourcing platform where firms post design challenges. Crowdspring allows clients in need of design help to post their requirements and get responses from the crowd (see Web Appendix Figure WA2). On average, a brief receives over 90 entries, and the client typically picks a winning design from these entries. We obtained data on 27 completed design projects from Crowdspring. We retained consumeroriented products and deleted fashion-related products (e.g., clothes, jewelry) because these projects deal with aesthetics more than functional design, leaving a total of 20 projects. We collected the winning design(s) and selected ten other ""nonwinning"" designs at random for each of the design briefs. The submitted product designs (the winning designs plus the ten chosen at random) were evaluated using the same construct scales (Table 3) by respondents from Amazon's Mechanical Turk (MTurk). Each respondent (there was an average of eight respondents per product) saw a picture of the design submission and a write-up describing the product. Their answers were averaged to form the constructs' scores; all construct reliabilities (Cronbach's alpha) or correlations (for two-item measures) were greater than .80.Using this new data set, we tested whether the same design characteristics that affected the earlier design crowdsourcing decision also influence which design managers select as the winning design from all submissions (and presumably choose to implement). We utilized a binary regression method similar to the previous analysis because the outcome variable is dichotomous (whether the design was chosen or not), with one modification: we controlled for the fact that each design is not independent but is clustered within a specific project and that the winner is determined from the specific cluster. We did this using a conditional logistic regression model, which is fitted to such situations where the data are based on matched cases (groups), via the ""clogit"" command in STATA 13. This controls for the conditional nature of the outcome variable where the likelihood estimation is calculated relative to the group. We included all five design characteristics from Equation 2 (for the results, see Table WA3 of the Web Appendix).Two of the significant constructs from our prior analysis on Quirky data are also significant in this data set (p < .10)— usability and reliability—with both linear and quadratic terms significant. The probability of a design being chosen (as a winner) increases as Usability increases (β = 10.612, p = .079); its quadratic term, Usability2 (β = -1.004, p = .069), indicates a tapering effect, suggesting that it increases at a decreasing rate. Reliability positively increases the probability of a design being chosen, (β = 18.877, p = .091), with Reliability2 indicating a tapering effect (β = -1.882, p = .081). Technical complexity was not statistically different from zero for either the linear or quadratic terms (p > .10). It could be that there may be a higher degree of variation in this construct when measuring across different products (such as with Quirky), but not across different designs for the same product, as in Crowdspring, where managers may have been more explicit about technicality. The significant results for two design constructs, usability and reliability, and their quadratic terms, across data contexts provide convergent evidence for their influence on crowdsourcing decisions. Design Crowdsourcing and New Product PerformanceWenext present the results on the impact of design crowdsourcing on postlaunch new product performance using data from Quirky.Table WA2 of the Web Appendix presents the summary statistics and correlation matrix. Equation 1 is estimated with 2SLS using the instrumental procedure, as previously noted, where the predicted result from the probit model (Equation 2) is used as an instrument. Table 5 shows the results of Equation 1 utilizing two nested models. First, we present the results of the main-effects-only model (excluding the multiplicative term b3) and then present the results of the full model with interaction effects. Both models use heteroskedasticity-robust standard errors. Table 5 also shows the first-stage F-statistics and the partial R-squares of the instruments as estimated in the first-stage regressions of the 2SLS procedure. The diagnostics for the crowdsourcing dummy, the crowdsourcing • idea quality interaction, and price instruments show that, collectively, our instruments are not weak (Stock and Watson 2003).TABLE: TABLE 5 The Effect of Crowdsourcing the Design on Unit Sales TABLE 5 The Effect of Crowdsourcing the Design on Unit Sales  aR-squared is shown for directional purposes only. R-squaredfor2SLS models is not interpreted the same as ordinary least squares (percentage of variance explained). See Wooldridge (1999).Notes: Robust standard errors presented. A constant and category dummies are estimated but not displayed for brevity.The results in Table 5 demonstrate an interesting relationship between design crowdsourcing and unit sales. Model 1 (main effects only) shows that the design crowdsourcing dummy does not significantly affect sales (β = .566, p = .527). This indicates that the effect of design crowdsourcing, on average, is not statistically different from zero. However, Model 2, the full model that includes interaction, presents a more nuanced picture. The estimate for CrowdsourceDesign (β = 12.824, p = .002) and the interaction between CrowdsourceDesign and IdeaQuality (β=-3.188, p = .001) are both statistically significant.We remind the reader that the beta coefficients in Model 1 (the main-effects-only model) represent the estimation of the main effect, or the average effect across the dependent variable based on the conditional mean function E(y|x) (Baum 2013). The coefficient for CrowdsourceDesign in Model 2 (the full model with interaction) represents the estimation of the simple effect or the estimated impact of CrowdsourceDesign when IdeaQuality is at zero (for a discussion of simple effects in interactive models, see Echambadi and Hess 2007). The significance of CrowdsourceDesign in the model with interactions and lack of significance in the model without interactions suggest that crowdsourcing the design helps sales, but only when the concept has low IdeaQuality. The sign of the interaction term helps make sense of this distinction. The negative interaction term suggests that the positive effect of design crowdsourcing on sales dissipates as the idea quality of the product concept increases.To increase the managerial relevance of our findings, we aim to show that crowdsourcing the design helps, on average, all products with low levels of IdeaQuality (not only those at zero) We measure the marginal effect at different low levels of IdeaQuality. The marginal effect of CrowdsourceDesign is positive and significant at various levels of IdeaQuality. In fact, the positive effects do not become insignificant (p > .10) until around IdeaQuality's mean. Thus, we find support for H1 when the product idea shows room for improvement, and we find support for H2. Table WA4 in the Web Appendix shows robustness of this analysis to alternative measures of sales. We also show that the results are robust, accounting for the fact that some products originated from the same raw design by using clustered standard errors (TablesWA5 andWA6 in the Web Appendix). What Product Functionalities Does Design Crowdsourcing Influence?A logical follow-up question to the preceding analysis is whether design crowdsourcing improves the functional design attributes from idea to final product. To test this notion, we collected additional data to assess the design of the final product as presented on the Quirky website. We replicate the maineffects-only model (Model 1 from Table 5), replacing unit sales (the previous dependent variable) with the change scores of the three design characteristics found to be significant in the probit model (reliability, technical complexity, and usability) as the new dependent variables. We assess the change scores, using consumer ratings, by measuring the improvement from the initial product idea to the final product for each of the measured design characteristics. We utilized the same design, same filtering questions, and the same construct questions used for the initial product ideas to assess the design characteristics of the final product, using students from the same population, but excluding any respondents who participated in evaluating the raw concepts. After aggregating the questions into the final constructs, we calculated a change score for each of the design constructs by subtracting the initial rating from the final rating. For example, DReliabilityi, the dependent variable, would be calculated as Reliablityi, final — Reliability i, initial, where Reliability i,final relates to the score for perceived reliability of the final product and Reliability i,initial relates to the score for perceived reliability of the initial idea. This model used the same instrumental variables procedure as before and controls for the initial level of the product characteristic ratings.Three different models were run, using ∆Reliabilityi, ∆Technicali, and ∆Usabilityi as the dependent variables, respectively, and design crowdsourcing dummy as the key independent variable. The results (Web Appendix Table WA7) show that the design crowdsourcing dummy positively influences ∆Reliabilityi (β = .548, p = .079) and ∆Usabilityi (β = .772, p = .000), but not ∆Technicali (β = -.211, p = .542). These results suggest that design crowdsourcing enhances perceived reliability and usability from product idea to final product.[ 8] Data Robustness ChecksUse of student sample. Another potential concern with our data could be that we used undergraduate business students to assess design ratings and idea quality. The primary objective of crowdsourcing is to design products that are aligned with what people want. Thus, it is clear that consumer preferences around product design attributes drive the crowdsourcing decision. Our use of student surveys is representative of these considerations of obtaining consumer preferences to aid managerial decision making. Students are often used as proxies for general consumers (e.g., Aaker and Keller 1990; Larson and Billeter 2013). Furthermore, students are a specific target segment for Quirky, as evident from some of the product descriptions, such as ""dorm occupants needn't schlep their shower shoes; just hook a cord around them and they're along for the ride."" Therefore, students are representative of a strong consumer base for Quirky. However, to show that the student ratings are similar to the ratings from other general segments, we collected product ratings for a subsample of the same products from two different groups of respondents: one group recruited from MTurk and another group of business professionals (master of business administration [MBA] graduates, using a panel provided by Qualtrics).[ 9] We randomly selected 42 products (3 products from each of the 14 blocks) and had each group rate the products on the same construct questions shown in Table 3. We compared these ratings with the earlier ratings for the same 42 products taken from our original (undergraduate business) sample. We first performed a comparison of between-sample similarities using correlations between the samples, and then a comparison of within-sample similarities using the Jennrich test (Jennrich 1970).The high and significant correlation between the MTurk ratings and the undergraduate ratings across the 42 products for technical complexity (.714, p < .001), usefulness (.528, p = .003), reliability (.305, p = .049), usability (.609, p < .001), and novelty (.449, p = .003) demonstrates that the construct scores are consistent across these different respondent groups. Next, we used the Jennrich test of equality of correlation matrices, which formally tests whether the correlational structure for the five constructs differs between the groups (Jennrich 1970). In other words, it tests whether the correlation matrix for the student sample is similar to the correlation matrix in the MTurk sample (e.g., Compas et al. 1989; Gande and Parsley 2005), without requiring the assumption of equal means or standard deviations (Gande and Parsley 2005). The test shows no significant difference (p = .87). In other words, we fail to reject the null hypothesis that the correlational structures are equal, indicating that the relationships among constructs are similar across samples. Similarly, if we include the design crowdsourcing dummy and the idea quality rating in the correlation matrices, the test again shows no significant difference (p = .61).Next, we recruited 135 business professionals using a Qualtrics panel to rate the same 42 products. All constructs between the two samples (students and MBA professionals) were significantly correlated (p < .01), except for Reliability, demonstrating that the ratings on the construct scores are generally consistent across these two respondent groups. The Jennrich test showed no significant difference (p = .59) between matrix structures. Similarly, if we include the design crowdsourcing dummy and rating on idea quality, the test again shows no significant difference (p = .72). We elaborate on the Jennrich test and other robustness checks regarding our sample in the section on additional robustness checks in the Web Appendix.Nonlinearities. One possible question is whether the use of all of the quadratic terms is warranted in the model predicting the probability of design crowdsourcing. We have noted that extant theory suggests that at least two of the constructs should be nonlinear (technical complexity and usability). Therefore, we replicate Equation 2 but include only two quadratic terms for technical complexity and usability (see Web Appendix Table WA8). The same constructs that are significant in the previous analysis (Table 4) are significant (Technical, Technical2, Reliability, Usability, and Usability2) and in the same direction. In addition, the same constructs that were previously not significant (Useful and Novelty) are still not significant. DiscussionThe use of design crowdsourcing to seek external inputs during design is emerging as a significant practice. Our article is one of the first to build and test a theoretically grounded model of factors that influence the design crowdsourcing decision and the effect of design crowdsourcing on performance, providing implications for both academics and managers. Contributions to TheoryKnowledge management theory. Our exploratory insights contribute to the knowledge management literature by revealing how design crowdsourcing, as a mechanism, can help improve the NPD process and performance. Design crowdsourcing aids in knowledge identification by aggregating diverse sources of user-based design knowledge and extracting novel, workable, and meaningful design solutions. The literature on ideation crowdsourcing reveals similar insights on the use of the crowd as a resource base for ideas during the opportunity identification stage. We complement this research by illustrating how crowdsourcing can strategically tap into the crowd during the critically important design stage. Design crowdsourcing bolsters opportunity exploitation by supplementing NPD resources and creating a more collaborative process of integrating external solutions with in-house guidance, thereby contributing to product development and performance.Crowdsourcing theory. We add to the extant crowdsourcing literature by illuminating the antecedents of design crowdsourcing and by examining design crowdsourcing's effect on new product performance. Exploratory interviews and results reveal that the inherently iterative, userdriven, and evolutionary process of design crowdsourcing can lead to a more focused search for innovative solutions while simultaneously enhancing product effectiveness. Our analysis reveals that product ideas with significant need for improvement may likely benefit the most from this iterative process that allows for crowd-driven refinement and enhancement of such ideas.Product design theory. We contribute to the literature on product design by finding that design elements—usability, reliability, and technical complexity—matter in influencing crowdsourcing decisions. Surprisingly, usefulness and novelty do not emerge as significant drivers from our data. One post hoc explanation is that novelty and usefulness matter during idea selection, where the emphasis may be on differentiation, while the other dimensions are of consequence during design selection, where the emphasis may be on objective functionality and user experience. Based on the design literature (Noble and Kumar 2010), it seems that managers may be seeking utility from design crowdsourcing to enhance function (rational value), and user experience (kinesthetic value), rather than differentiation (emotional value). Furthermore, this research establishes that the crowd can serve as a knowledge resource for design solutions, and design crowdsourcing can improve user perceptions of design from ideas to products. Managerial ImplicationsOur results provide three important managerial implications. First, whereas managers may fear losing control of the design process by opening it up, our interviews indicate that they can maintain better control over the design process, while creating slack for their research and development/ design team, through the process of engagement/iteration with users/designers by selecting appropriate design crowdsourcing platforms. Furthermore, managers are faced with pressure to generate greater numbers of innovative products while being constrained by internal resource limitations. Therefore, they often prioritize only their best product ideas and concepts, discarding many others. Our results suggest that design crowdsourcing can helpmanagers move a greater number of ideas through development by using the community's help in making (initially) less-promising ideas marketable. Thus, we address the question we posed previously: there is incremental value to be extracted from even initially lesspromising ideas. Rather than discard such ideas, firms may use external sources of knowledge to develop them, and interact with these external sources extensively to ensure that the outcome is of high quality. Newer crowdsourcing firms, such as Crowdspring, are now setting up systems for the idea generator (client firm) to provide feedback to the community as the community aids in the design process. This feedback process may be rated and monitored by the crowdsourcing platform. It is this interactive and iterative process of design and development that eventually moves ideas into production, and herein lies the true value of design crowdsourcing.Second, our analysis suggests design crowdsourcing increases the perceived reliability and usability from ideation to final product. Managers of client firms aiming to improve specific functional attributes of design may turn to crowdsourcing as a supplementary design resource.Third, we provide insights to the crowdsourcing platforms, as well as the client firms, on better managing the process of crowdsourcing. First, design crowdsourcing firms are increasingly facing pressure from members of design communities, who perceive a threat posed by the availability of thousands of low-cost designs provided by the crowd. (Grefe 2016). Our research suggests that design crowdsourcing can help improve specific design functionalities through a process of iteration and feedback. Design crowdsourcing firms can (re) position themselves as intermediaries that help solve genuine product needs. Second, this research emphasizes the iterative process of design. However, given the start-up nature of many of the crowdsourcing platforms, there may be difficulties in empathizing with the end user(s) throughout the entire design process. As an executive remarked when we presented our summary results, ""[Empathizing] sometimes falls by the wayside due to outside constraints such as budget, timing, exhaustion, or purely wanting to keep things simple."" Our results may provide insights to such platforms on the optimal timing of user engagement at different phases of NPD. Third, our qualitative interviews suggest that managing the design crowdsourcing process may not be trivial, similar to insights from ideation research that suggested that firms may be overwhelmed with ideas from the crowd, and, thus, this research suggests that the decision to crowdsource should not be taken lightly. This is one reason for the growing popularity of third-party platforms, as these platforms assist in managing much of the process and provide important guidance to managers. Limitations and Promising Avenues for Future ResearchWe note a few limitations of this research and discuss research opportunities. First, our key results are based on the product performance (sales) of a single firm. Although our results have significant implications, they do not directly speak to the viability (or profitability) of the overall business models.[10] Future research will benefit from a large-scale study involving nonplatform firms. Second, our sample for the product concepts originated from students. While we validate our constructs using another crowdsourcing platform and ratings from other sources, future studies can make substantial contributions by using broader consumer and managerial surveys. Third, we do not consider firm capabilities, whichmay influence the decision to seek an external solution (Afuah and Tucci 2012). While this limitation is mitigated because all our products come from the same firm, caution should be used when extrapolating these results to other companies. Fourth, all of the product ideas in this data set originated from the community, and crowdsourcing design may have different results depending on whether the product idea had originated internally or from the community. Fifth, there are likely differences in efficacy or the degree of collaboration depending on whether design solutions were consumer generated or designer generated. Although the current context cannot address these issues, these are promising questions for future research. Future research could examine effective mechanisms to incentivize collaboration in crowdsourcing platforms and determine best practices for managing design crowdsourcing. It is our hope that our findings motivate further research on crowdsourcing decisions in various phases within the NPD process.  Endnotes   1  Many of the design crowdsourcing platforms focus on the manufacturing makeup of design; thus, our study focuses on functional design rather than aesthetics. This use is also consistent with extant research studying the early phases of design with a ""focus on functional performance in product design, as opposed to the product's aesthetic qualities or appearance"" (Dahl, Chattopadhyay, and Gorn 1999, p. 19). 2  After a brief description of the research project, each interviewee was asked about issues related to crowdsourcing and how those outside the organization help with providing design solutions. In three of the cases, the interviewee chose to reply to these questions by email, in which case further emails were sent to follow up on responses, if needed. We supplemented these insights with a search for popular press articles to gain a broader understanding of how crowdsourcing aids product development, using the LexisNexis database, as well as by examining firms' internal websites, design crowdsourcing websites, and blog posts. 3  To the best of our knowledge, this is the first article to test all five design variables simultaneously in the same study. Subsets of these variables are linked together theoretically in extant literature. For example, Bloch (1995) groups durability, technical sophistication, and ease of use as product-related beliefs created or influenced by product form and classifies novelty as affecting how consumers perceive the product relative to other products. Noble and Kumar (2010) divide design elements into three categories: rational value, kinesthetic value, and emotional (differentiating) value. Our five identified dimensions thus emphasize function (reliability and technical complexity providing rational value), user experience (ease-of-use and usefulness providing kinesthetic value), and differentiation (novelty providing emotional value). 4  We used unit sales within the first year for a few reasons: First, most products have observed sales for one year, so it limits extrapolating beyond what is known. Second, it focuses our analysis on performance soon after initial launch; this timing seems reasonable because Talke et al. (2009) show that product design affects sales most at the beginning of a product's life cycle. Our results are robust to other possible methods of completing yearly sales, such as proportional annualization (Chandrasekaran et al. 2013) or measuring sales at three or six months (see Table WA4 in the Web Appendix). 5  We note that some of the final products originated from the same raw design and design phase, but because we wanted to have a unique estimate for each final product, we allowed each final product's raw design to be rated separately. We ensured that each of these designs were inserted into different blocks. We ran robustness checks to demonstrate that this does not influence the results, which we describe subsequently. 6  To find information about who an inventor follows, a consumer would have to consciously click through the website to find the inventor's profile. Furthermore, much of Quirky's sales occurs in other retailer settings, for example, in Walmart, where such retail customers will know nothing about individual inventors. 7  There may be a possible concern about the quadratic terms having a multicollinearity problem as a result of their linear terms. As Allison (2012) notes, high correlation between variables and their product is expected and ""is not something to be concerned about, because the p-value for [the product] is not affected by the multicollinearity,… so the multicollinearity has no adverse consequences."" Furthermore, the variance inflation factor analysis of the base models for Equation 2 (without interactions and nonlinear terms) confirms that multicollinearity is not an issue due to high correlations between constructs, because all of the variance inflation factor values were substantially less than ten. 8  We rationalize post hoc that technical complexity may be more significant in the antecedents model than in the change-score analysis because technical complexity may be more internal and, thus, significant when firm capabilities matter in the decision of whether to crowdsource. The change-score results demonstrate that crowdsourcing improves those attributes of design that are perhaps more user-centric. 9  For the MTurk sample, we recruited 165 respondents to rate 42 products (average age = 39.8 years old, 39% male, median household income: $50,000–$99,999). Respondents in the Qualtrics sample had all graduated from an MBA program (average age = 48 years old, 64% male, medium household income: $50,000–$99,999, average work experience in business: 18 years). The same scales were used for the five design constructs and the average rating across the respondents (at least five respondents per product) was obtained. All Cronbach's alpha/correlations are above .80 for the MTurk sample and above .70 for the Qualtrics sample. Quirky has since filed for bankruptcy and changed its website structure. We thank our anonymous reviewers for highlighting this point.DIAGRAM: FIGURE 1 Conceptual Framework for TestingDIAGRAM: FIGURE 2 How Design Crowdsourcing Affects New Product Performance: A Closer Look at the Link Proposed in Figure 1  REFERENCES   Aaker, David, and Kevin Lane Keller (1990), ""Consumer Evaluations of Brand Extensions,"" Journal of Marketing, 54 (1), 27–41. Adams, Ren´ee, Heitor Almeida, and Daniel Ferreira (2009), ""Understanding the Relationship Between Founder—CEOs and Firm Performance,"" Journal of Empirical Finance, 16 (1), 136–50. Afuah, Allan, and Christopher L. Tucci (2012), ""Crowdsourcing as a Solution to Distant Search,"" Academy of Management Review, 37 (3), 355–75. Alavi, Maryam, and Dorothy E. Leidner (2001), ""Review: Knowledge Management and Management Systems: Conceptual Foundations and Research Issues,"" Management Information Systems Quarterly, 25 (1), 107–36. Allison, Paul (2012), ""When CanYou Safely Ignore Multicollinearity?"" (accessed September 8, 2017), https://statisticalhorizons.com/ multicollinearity. Andriopoulos, Constantine, and Marianne W. Lewis (2009), ""Exploitation-Exploration Tensions and Organizational Ambidexterity: Managing Paradoxes of Innovation,"" Organization Science, 20 (4), 696–717. Atuahene-Gima, Kwaku (2005), ""Resolving the Capability: Rigidity Paradox in New Product Innovation,"" Journal of Marketing, 69 (4), 61–83. Baum, Christopher F. (2013), ""Quantile Regression,"" (accessed January 18, 2016), http://fmwww.bc.edu/EC-C/S2013/823/EC823. S2013.nn04.slides.pdf. Bayus, Barry L. (2013), ""Crowdsourcing New Product Ideas over Time: An Analysis of the Dell IdeaStorm Community,"" Management Science, 59 (1), 226–44. Bloch, Peter H. (1995), ""Seeking the Ideal Form: Product Design and Consumer Response,"" Journal of Marketing, 59 (3), 16–29. Chandrasekaran, Deepa, Joep W.C. Arts, Gerard J. Tellis, and Ruud T. Frambach (2013), ""Pricing in the International Takeoff of New Products,"" International Journal of Research in Marketing, 30 (3), 249–64. Chang, Woojung, and Steven A. Taylor (2016), ""The Effectiveness of Customer Participation in New Product Development: A Meta-Analysis,"" Journal of Marketing, 80 (1), 47–64. Chesbrough, Henry W. (2003), Open Innovation: The New Imperative for Creating and Profiting from Technology. Boston: Harvard Business School Press. Cohen, Wesley M., and Daniel A. Levinthal (1990), ""Absorptive Capacity: A New Perspective on Learning and Innovation,"" Administrative Science Quarterly, 35 (1), 128–52. Compas, Bruce E., David C. Howell, Vicky Phares, Rebecca A. Williams, and Normand Ledoux (1989), ""Parent and Child Stress and Symptoms: An Integrative Analysis,"" Developmental Psychology, 25 (4), 550–59. Crawford, Kate (2009), ""Following You: Disciplines of Listening in Social Media,"" Continuum, 23 (4), 525–35. Dahl, Darren W., Amitava Chattopadhyay, and Gerald J. Gorn (1999), ""The Use of Visual Mental Imagery in New Product Design,"" Journal of Marketing Research, 36 (1), 18–28. Davis, fired D. (1989), ""Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology,"" Management Information Systems Quarterly, 13 (3), 319–40. Echambadi, Raj, and James D. Hess (2007), ""Mean-Centering Does Not Alleviate Collinearity Problems in Moderated Multiple Regression Models,"" Marketing Science, 26 (3), 438–45. eYeka (2016), ""Online Co-Creation to Accelerate Marketing and Innovation,"" (accessed March 10, 2016), available at https://en. eyeka.com/resources/whitepapers#online-cocreation. Fennell, Geraldine, and Joel Saegert (2004), ""Identifying Prospects and Reaching Targets: Neglected DistinctionWithin Marketing,"" in Proceedings of the 2003 SCP Summer Conference. San Francisco: Society for Consumer Psychology, 193–202. Foss, Nicolai J., Jacob Lyngsie, and Shaker A. Zahra (2013), ""The Role of External Knowledge Sources andOrganizationalDesign in the Process of Opportunity Exploitation,"" Strategic Management Journal, 34 (12), 1453–71. Fuchs, Christoph, Emanuela Prandelli, and Martin Schreier (2010), ""The Psychological Effects of Empowerment Strategies on Consumers' Product Demand,"" Journal of Marketing, 74 (1), 65–79. Gande, Amar, and David C. Parsley (2005), ""News Spillovers in the Sovereign Debt Market,"" Journal of Financial Economics, 75 (3), 691–734. Gann, DavidM., andAmmon J. Salter (2000), ""Innovation in Project- Based, Service-Enhanced Firms: The Construction of Complex Products and Systems,"" Research Policy, 29 (7), 955–72. Granovetter, Mark S. (1973), ""The Strength of Weak Ties,"" American Journal of Sociology, 78 (6), 1360–80. Grant, Robert T. (1996), ""Toward a Knowledge-Based Theory of the Firm,"" Strategic Management Journal, 17 (Winter), 109–22. Grefe, Richard (2016), ""What's the Harm in Crowdsourcing?"" AIGA (June 24), http://www.aiga.org/whats-the-harm-in-crowdsourcing. Grewal, Dhruv, R. Krishnan, Julie Baker, and Norm Borin (1998), ""The Effect of Store Name, Brand Name and Price Discounts on Consumers' Evaluations and Purchase Intentions,"" Journal of Retailing, 74 (3), 331–52. Hobday, Mike (2000), ""The Project-Based Organization: An Ideal Form for Managing Complex Products and Systems?"" Research Policy, 29 (7), 871–93. Jennrich, Robert I. (1970), ""An Asymptotic c2 Test for the Equality of Two Correlation Matrices,"" Journal of the American Statistical Association, 65 (330), 904–12. Joshi, Ashwin W., and Sanjay Sharma (2004), ""Customer Knowledge Development: Antecedents and Impact on New Product Performance,"" Journal of Marketing, 68 (4), 47–59. Katz, Ralph, and Thomas J. Allen (1982), ""Investigating the Not Invented Here (NIH) Syndrome: A Look at the Performance, Tenure, and Communication Patterns of 50 R&D Project Groups,"" R&D Management, 12 (1), 7–20. Kornish, Laura J., and Karl T. Ulrich (2014), ""The Importance of the Raw Idea in Innovation: Testing the Sow's Ear Hypothesis,"" Journal of Marketing Research, 51 (1), 14–26. Kumar, V., Ashutosh Dixit, Rajshekar (Raj) G. Javalgi, and Mayukh Dass (2016), ""Research Framework, Strategies, and Applications of Intelligent Agent Technologies (IATs) in Marketing,"" Journal of the Academy of Marketing Science, 44 (1), 24–45. Kumar, V., and RohanMirchandani (2012), ""Increasing the ROI of Social Media Marketing,"" MIT Sloan Management Review, 54 (1), 55–61. Larson, Jeffrey S., and Darron M. Billeter (2013), ""Consumer Behavior in 'Equilibrium': How Experiencing Physical Balance Increases Compromise Choice,"" Journal of Marketing Research, 50 (4), 535–47. Laursen, Keld, and Ammon Salter (2006), ""Open for Innovation: The Role of Openness in Explaining Innovation Performance Among U.K. Manufacturing Firms,"" Strategic Management Journal, 27 (2), 131–50. Lohr, Steve (2015), ""The InventionMob, Brought toYoubyQuirky,"" The New York Times (February 14), http://www.nytimes.com/2015/02/ 15/technology/quirky-tests-the-crowd-based-creative-process.html. March, Artemis (1994), ""Usability: The New Dimension of Product Design,"" Harvard Business Review, 72 (5), 144–49. Markowitz, Eric (2011), ""The Case for Letting Your Customers Design Your Products,"" Inc. (September 20), http://www.inc.com/guides/ 201109/how-to-crowdsource-your-resarch-and-development.html. McGee, Jeffrey, James Caverlee, and Zhiyuan Cheng (2013), ""Location Prediction in Social Media Based on Tie Strength,"" in Proceedings of the 22nd ACM International Conference on Information & Knowledge Management. New York: Association for Computing Machinery, 459–68. Moldovan, Sarit, Jacob Goldenberg, and Amitava Chattopadhyay (2011), ""The Different Roles of Product Originality and Usefulness in Generating Word-of-Mouth,"" International Journal of Research in Marketing, 28 (20), 109–19. Nishikawa, Hidehiko, Martin Schreier, and Susumu Ogawa (2013), ""User-Generated Versus Designer-Generated Products: A Performance Assessment at Muji,"" International Journal of Research in Marketing, 30 (2), 160–67. Noble, Charles H., and Minu Kumar (2010), ""Exploring the Appeal of Product Design: A Grounded, Value-Based Model of Key Design Elements and Relationships,"" Journal of Product Innovation Management, 27 (5), 640–57. Piller, Frank, Alexander Vossen, and Christoph Ihl (2012), ""From Social Media to Social Product Development: The Impact of SocialMedia on Co-Creation of Innovation,"" Die Unternehmung, 66 (1), 7–27. Poetz, Marion K., and Martin Schreier (2012), ""The Value of Crowdsourcing: Can Users Really Compete with Professionals in Generating New Products Ideas?"" Journal of Product Innovation Management, 29 (2), 245–56. Radjou, Navi, and Jaideep Prabhu (2015). Frugal Innovation: How to Do More with Less. New York: The Economist. Ramani, Girish and V. Kumar (2008), ""Interaction Orientation and Firm Performance,"" Journal of Marketing, 72 (1), 27–45. Ries, Eric (2011), The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. New York: Crown Books. Rossi, Peter E. (2014), ""Invited Paper—Even the Rich Can Make Themselves Poor: A Critical Examination of IV Methods in Marketing Applications,"" Marketing Science, 33 (5), 655–72. Rust, Roland T., Debora Viana Thompson, and Rebecca W. Hamilton (2006), ""Defeating Feature Fatigue,"" Harvard Business Review, 84 (2), 98–107. Schreier, Martin, Christoph Fuchs, and Darren W. Dahl (2012), ""The Innovation Effect of User Design: Exploring Consumers' Innovation Perceptions of Firms Selling Products Designed by Users,"" Journal of Marketing, 76 (5), 18–32. Simon, Ruth (2014), ""OneWeek, 3,000 Product Ideas,"" TheWall Street Journal (July 3), http://www.wsj.com/articles/one-week-3-000- product-ideas-1404332942?mod=djem_jiewr_swwps_071014. Stock, James H., and Mark W. Watson (2003), Introduction to Econometrics. New York: Addison-Wesley. Talke, Katrin, Soren Salomo, Jaap E. Wieringa, and Antje Lutz (2009), ""What About Design Newness? Investigating the Relevance of a Neglected Dimension of Product Innovativeness,"" Journal of Product Innovation Management, 26 (6), 601–15. Troy, Julia (2015), ""Howto SketchLike an Industrial Designer,"" Skillshare (accessed February 17, 2016), https://www.skillshare.com/classes/ design/How-to-Sketch-Like-an-Industrial-Designer/625650159]. Unilever (2016), ""Open Innovation,"" (accessed February 10, 2016), https://www.unilever.com/about/innovation/open-innovation/ challenges-and-wants/. Venkatesh, Viswanath, Michael G. Morris, Gordon B. Davis, and fired D. Davis (2003), ""User Acceptance of Information Technology: Toward a Unified View,"" Management Information Systems Quarterly, 27 (3), 425–78. Von Hippel, Eric (1994), ""'Sticky Information' and the Locus of Problem Solving: Implications for Innovation,"" Management Science, 40 (4), 429–39. Wooldridge, Jeffrey M. (1999), Introductory Econometrics: A Modern Approach. Mason, OH: South-Western. Wooldridge, Jeffrey M. (2010), Econometric Analysis of Cross Section and Panel Data. Cambridge, MA: MIT Press.~~~~~~~~B.J. Allen is Assistant Professor of Marketing, Sam M. Walton College of Business, University of ArkansasDeepa Chandrasekaran (corresponding author) is Assistant Professor of Marketing, University of Texas at San AntonioSuman Basuroy is Department Chair and Graham Weston Endowed Professor of Marketing, University of Texas at San AntonioCopyright of Journal of Marketing is the property of American Marketing Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.Record: 36Detecting, Preventing, and Mitigating Online Firestorms in Brand Communities. By: Herhausen, Dennis; Ludwig, Stephan; Grewal, Dhruv; Wulf, Jochen; Schoegel, Marcus. Journal of Marketing. May2019, Vol. 83 Issue 3, p1-21. 21p. 2 Diagrams, 5 Charts, 1 Graph. DOI: 10.1177/0022242918822300. Persistent link to this record (Permalink): http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=135774461&site=ehost-liveCut and Paste: <A href=""http://proxy.library.nyu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=135774461&site=ehost-live"">Detecting, Preventing, and Mitigating Online Firestorms in Brand Communities.</A>"
25,"Detecting, Preventing, and Mitigating Online Firestorms in Brand Communities Online firestorms pose severe threats to online brand communities. Any negative electronic word of mouth (eWOM) has the potential to become an online firestorm, yet not every post does, so finding ways to detect and respond to negative eWOM constitutes a critical managerial priority. The authors develop a comprehensive framework that integrates different drivers of negative eWOM and the response approaches that firms use to engage in and disengage from online conversations with complaining customers. A text-mining study of negative eWOM demonstrates distinct impacts of high- and low-arousal emotions, structural tie strength, and linguistic style match (between sender and brand community) on firestorm potential. The firm's response must be tailored to the intensity of arousal in the negative eWOM to limit the virality of potential online firestorms. The impact of initiated firestorms can be mitigated by distinct firm responses over time, and the effectiveness of different disengagement approaches also varies with their timing. For managers, these insights provide guidance on how to detect and reduce the virality of online firestorms.KEYWORDS_SPLITMore than 65 million firms leverage online brand communities to connect with customers and achieve known performance benefits, such as increased online reputation, brand patronage, and customer spending ([ 4]; [43], [51]). However, online communities also engender significant risks of online firestorms—that is, negative electronic word of mouth (eWOM) that receives substantial support from other customers in a short period of time ([63]). Similar to prominent online firestorm examples, such as #deleteUber and United Airlines' passenger removal incidents, less publicized negative eWOM messages by dissatisfied customers also can go viral; a single 466-word Facebook post by a disgruntled customer in Odeon Cinemas' Facebook brand community prompted more than 94,000 likes, damaging the firm's reputation and causing it to lose thousands of customers ([23]).Detecting, preventing, and mitigating this virality of negative eWOM in online brand communities therefore constitutes a critical managerial priority ([40]), yet 72% of firms rate their preparedness for online firestorms as ""below average"" ([24]). Managers seem to have a limited understanding of how to respond to negative eWOM describing dissatisfactory consumption experiences ([80]), nor do they know how to predict the evolution of negative eWOM messages or address angered mass audiences exposed to such negative eWOM. Lacking clear guidelines, firms continue to suffer damages from negative eWOM. We aim to address this gap by identifying sources of firestorms and detailing appropriate sequences for firm responses to negative viral content.Extant marketing research has described the spreading of word of mouth (WOM) as a contagious process, whereby receivers ""catch"" others' emotions through social transmission ([ 8]). The relatively rare research that specifically investigates negative WOM suggests that its contagiousness primarily depends on the sender's emotions ([10]; [38]) and the relationship between senders and receivers ([14]; [58]). Yet few studies have applied these valuable insights to an online brand community context, so we identify sender and relational aspects pertinent to firestorms of negative eWOM in online brand communities.In addition to identifying sources of the spread of negative eWOM, firms need to pinpoint how to respond ([16]). Services recovery research has proposed several viable approaches to negative customer experiences, including empathic and explanatory responses (e.g., [12]). In contrast with traditional complaint channels, however, the online brand community makes customers' complaints and the firm's recovery efforts visible. Therefore, beyond mitigating the complaining customer's unsatisfactory consumption experience, the firm needs to craft a response that can minimize any negative effects on the wider audience of online brand community members. By investigating regulation strategies that can reduce receivers' susceptibility to negative emotions ([33]), we investigate how firms should tailor their responses to limit the virality of negative eWOM. In so doing, we do not limit our assessment to a single response, because customers in online brand communities often evaluate cross-message developments ([79]). Therefore, in an extension of Batra and Keller's (2016) work, we consider how sequences of firm responses might mitigate the virality of online firestorms as they evolve.With these empirical assessments of ways to detect, prevent, and mitigate the virality of negative eWOM in online brand communities, we offer three main contributions. First, we draw on negative WOM research to investigate how sender and relational aspects aid in the detection of potential firestorms, then specify how different levels of emotional arousal and the strength of the senders' structural ties and their similarity to the online brand community relate to the virality of negative eWOM. At an operational level, we establish a reliable, computerized technique to determine the similarity of language use within online brand communities. Second, our findings provide insights into firms' ability to prevent online firestorms by issuing responses designed to engage with or disengage from customers online. More explanatory responses are best for negative eWOM messages containing above-average negative high-arousal emotions; the effectiveness of disengaging approaches varies. Third, we identify structured sequences of different engaging responses across multiple firm messages as a novel, actionable approach to mitigate the impact of evolved online firestorms.To achieve these aims, we first systematically delineate sender and relational aspects that trigger greater virality of negative eWOM. We then systematize extant knowledge on common firm responses and contrast their effectiveness with the arousal of negative eWOM and viable cross-message composition. We test our hypotheses on large-scale data, reflecting negative customer posts from the online brand communities of 89 S&P 500 firms, which constitute potential online firestorms. In the final section, we summarize the findings, discuss the implications, and list some limitations. Conceptual Foundations and HypothesesExtant marketing research primarily has focused on identifying the presence and efficacy of positive eWOM ([81]), but open customer communication also bears the risk of unprecedented, rapidly discharged, large quantities of negative eWOM ([63]). To cope with negative experiences and warn others, customers share negative consumption experiences in online brand communities. Often highly emotional, such posts may emerge, diffuse, and dissolve quickly ([37]). Similar to positive eWOM, the extent to which other customers approve of and share negative eWOM determines its virality and firestorm potential. Prestudy interviews with 16 social media managers responsible for online brand communities suggest that firms regard negative customer posts as evolved online firestorms if the firm's initial response does not suffice to prevent the negative eWOM from ""catching fire"" among other customers. Every like or comment that follows means that another customer may be lost. At the outset, every negative post has the potential to cause an online firestorm; not every post does so. Compared with positive eWOM, negative WOM is transmitted more often and is more influential ([40]), so firms must detect and adequately respond to negative posts in online brand communities to avoid potential public debacles, customer defections, and profit reductions ([63]).Both product- and service-related WOM evaluations are shared through a social transmission process, like emotional contagion ([ 8]). The Web Appendix contains an overview of studies that detail drivers of and firm responses to eWOM. Various studies have indicated that eWOM contagiousness depends on the emotions conveyed, the structural ties of the sender, or the perceived similarity between the sender and receivers; however, the joint impact of these determinants on the virality of negative eWOM is unknown. Moreover, although some studies have examined the effectiveness of the presence of firm responses, they do not differentiate the circumstances in which a certain type of response is more effective. Finally, firms might need to respond to the same negative eWOM several times to resolve customers' negative experiences, and insight is lacking on how such responses should be sequenced over time. To fill these critical research gaps, we investigate ways to detect, prevent, and mitigate online firestorms arising from negative eWOM messages. Detecting Potential Online FirestormsConventional wisdom suggests that customers in online brand communities first read about the cause of negative eWOM messages and then decide whether to approve and share them. However, faced with the information overload that tends to characterize communication exchanges on social media platforms, customers might not elaborate in detail on the arguments and instead could resort to heuristic processing ([35]). Accordingly, research has suggested that the relative transmission of WOM is a result of the contagiousness of heuristics related to the sender's message and the relationship aspects between the sender and receivers ([14]; [38]; [58]).For example, particularly expressive people seem to transmit emotions effectively ([ 5]). Although emotions are not verbal properties, the verbal use of emotional words makes them relatively accessible and contagious. With increased use of affective words in a post, it efficiently reveals and makes accessible the intent or simplest raw feelings underlying the posting customer ([19]). At a granular, word-use level, increasing the number of negative emotion words in eWOM translates directly into stronger behavioral responses by message recipients ([54]). Even if the content is unrealistic, more negative emotional messages are shared more frequently ([13]). However, general negativity is a broad concept, and the influence of negative emotional expressions might depend further on people's relative arousal levels ([70]). For example, in their study of urban legends, [38] investigate high-arousal disgust emotions rather than just general negative emotions. Similarly, online firestorms may be more likely to arise from high-arousal (e.g., ""This is so frustrating"") rather than low-arousal (e.g., ""This is disappointing"") negative eWOM. [10] show that New York Times newspaper stories that include more intensive high-arousal emotions (e.g., fear/anxiety, anger) prompt emailed shares to others more frequently than stories with more intensive low-arousal emotions (e.g., sadness). Thus, rather than simply being one-dimensional, the contagiousness of emotionally charged negative eWOM in online brand communities may depend on the level of arousal. Therefore, we posit: H1:  The intensity of high-arousal emotion words in negative eWOM messages relates to greater virality in online brand communities compared with the intensity of low-arousal emotion words.The decision to approve or share eWOM also depends on the relationship and relational cues between the sender and receiver ([ 9]). Emotional contagion theorists cite the importance of interpersonal relations that enable message recipients to evaluate others and devise appropriate responses ([ 5]). Marketing research on WOM suggests that tie strength and perceptions of similarity are two primary relational cues that cause receivers to regard senders as more proximate ([14]).Tie strength is relevant in various information-sharing contexts; it refers to both the frequency of communication and the importance attached to the relations ([ 4]; [14]; [67]). Despite considerable debate about the relative advantages of weak and strong ties, researchers commonly agree that strong ties increase the likelihood that social actors will share sensitive information ([64]) and engage in collective action ([60]). Measures of tie strength rely on a range of variables ([58]; [64]), including frequency of contact ([67]). Weak ties reflect members of a community who interact less frequently with each other, whereas strong ties describe relationships of members who interact frequently ([15]). Frequent, positive encounters typically (if not always) lead to stronger structural ties ([58]) and increase opportunities to transmit opinions ([28]). This assumption is also in line with [15] suggestion that the more frequent and empathic the communication is between two users, the more likely that one user's opinion will influence the other user's opinion. Strong structural ties, as characterized by more frequent interactions, in turn increase imitative behavior within networks ([57]). Members with an exceptionally great number of ties within a community often act as opinion leaders, who influence purchase decisions and product adoptions ([49]). Certainly, the potential of well-connected customers to influence others in a brand community is likely to be stronger than the potential of less connected members in the same community ([29]). If the member who posts negative eWOM has stronger structural ties in the online brand community, the firestorm potential of the post thus should be greater: H2:  Stronger structural ties between the sender of negative eWOM and the receiving online brand community relate to greater virality.In addition, perceived similarity (or homophily perceptions; [14]) between the sender and customers in online brand communities may relate to the virality of negative eWOM messages. Although perceptions of similarity are not required for contagion to occur, they can act as qualifiers of information relevance ([35]). For example, [ 3] find that perceptions of similarity between customers explain more than half of the effect of behavioral contagion on new product adoption. Although interactions in online brand communities tend to be relatively anonymous, studies drawing on psycholinguistic research suggest that perceptions of similarity in computer-mediated settings are an automatic outcome of a linguistic style match (LSM). The similar use of function words—or LSM between two or more conversation partners—represents a form of psychological synchrony that elicits perceptions of similarity, approval, and trust in receivers ([47]). Just like conversation dyads, communities may develop a distinctive collective communication style ([25]). An individual customer's alignment with a common, community-level communicative style may elicit similarity perceptions and in turn influence the approval likelihood by the collective ([34]). [54] confirm that the congruence of a customer review with the typical linguistic style demonstrated by a product interest group on Amazon influences other customers' purchase behavior. Accordingly, negative eWOM that matches the typical linguistic style of an online brand community (i.e., evokes perceptions of similarity) should induce greater online firestorm potential: H3:  Closer LSM between the sender of negative eWOM and the receiving online brand community relates to greater virality. Preventing Potential Online FirestormsThe growing influence of online evaluations on customer behavior has increased managerial and research interest in firm recovery strategies that can reduce the contagiousness of negative eWOM ([56]). Recovery in the context of online brand communities is unique though, in that the customer's complaint and a firm's recovery efforts are visible to thousands or even millions of other customers. Effective recovery thus must ( 1) adequately restore relationship equity to the complaining customer and ( 2) prevent the negative eWOM message from spreading to other customers in the online brand community. The viability of common recovery approaches—offering an apology, compensation, responding empathically, or providing explanations—has been investigated mainly in bilateral firm–customer communication contexts (e.g., [41]). To gain further insight into the suitability of these approaches for reducing the contagiousness of emotions in negative eWOM, we turn to theory about emotion regulation strategies ([33]) and propose that firm responses to negative eWOM might be classified as disengaging or engaging.A disengaging approach to emotion regulation implies reacting in ways to avoid or block elaboration, rather than preparing an adaptive response ([73]). Observations and anecdotes suggest that avoidance and nonresponse is the poorest approach to regulating the virality of negative eWOM. As an alternative, firms might try to halt an ongoing public online conversation by suggesting a communication channel change (e.g., ""Please contact our service center""). Such a channel change suggestion might be effective for pushing customers to the right channels ([ 2]), but it is unclear how other online brand community react to being excluded from the continued conversation. The effectiveness of offering compensation to the complaining customer also is uncertain. Some service research has suggested that halting further elaborations is an effective recovery strategy (e.g., [12]), yet [31] find that compensation is not always effective and instead may depend on other response features.Active engagement with negative eWOM messages instead might be more appropriate ([80]). Service recovery literature has outlined two primary response approaches that represent active firm–customer conversational elaboration ([41]): empathic or explanatory. To express empathy, a spontaneous affective response ([42]), a firm might sympathize (e.g., ""We understand that you are unhappy"") or shift to a positive outlook (e.g., ""We hope you have a better experience next time""). Highly empathic responses may enhance the complainant's and online brand community's perceptions of interactional justice and signal politeness and courtesy, which may reduce the virality of negative eWOM. An engaged firm response also might include substantiated explanations, and the number of reasons offered has more influence than the actual content of those reasons on decision outcomes ([72]). When firms provide more substantiated arguments, it may enhance perceptions of response quality and effort among brand community audiences (e.g., ""We could not assist you quickly because the store was extremely busy""). By providing more explanation, firms might enhance evaluations of their recovery efforts ([12]).However, in line with cognitive appraisal theory and an affect infusion model, [45] posit that an affective approach, such as empathy, is more effective in affect-intensive environments characterized by social interactions and spontaneous decisions, such as online brand communities. In general, then, more empathic responses might be better suited to regulating the contagiousness of negative eWOM. According to the affect infusion model, the relative impact of cognitive responses, such as explanations, increases with stronger affect and higher involvement ([27]). Research on emotion regulation strategies further indicates that some stimuli may be too emotionally intense for an empathic response to suffice, and instead, receivers may seek explanations to reappraise the situation ([32]). The more contagious the emotions in a negative eWOM message, the more attention customers will pay to the message and the stronger their expectations about what needs to be done to remedy the situation ([39]). In such situations, customers are more likely to engage in deliberate processing of negative eWOM by cognitively reappraising the situation; that is, they consider more information and perform more intricate evaluations of the explanations ([52]). Empathic responses may help shift the attention of consumers who experience low-arousal emotions, but firms might better mitigate the virality of high-arousal emotions by offering more explanations. Thus, the relative effectiveness of firm responses for preventing online firestorms may be contingent on the intensities of the high- and low-arousal levels in the negative eWOM message: H4:  More explanation, rather than more empathy, in firm responses is better suited to contain negative eWOM with more intensive high-arousal emotions. H5:  More empathy, rather than more explanation, in firm responses is better suited to contain negative eWOM with more intensive low-arousal emotions. Mitigating Evolved Online FirestormsThrough observational learning processes, as an online firestorm evolves, and other members support the negative eWOM, its perceived reliability should increase ([21]). Thus, customers pay even more attention to the negative eWOM and form revised expectations about what needs to be done to remedy the situation. Therefore, beyond the compositional elements of individual firm responses, when negative eWOM evolves into an online firestorm, multiple firm responses become necessary to mitigate its detrimental impacts.As [ 6] suggest, online messages build on one another, and thus their sequence can determine their success in terms of persuading customers, building brand equity, or driving sales. [79] find that posting the same (vs. mixed) consecutive brand message decreases (vs. increases) customers' engagement. Considerations of cross-message dynamics in firm responses may advance understanding of how to mitigate evolved online firestorms too. For example, by empathically sympathizing with the customer in the first reply, then issuing a second, complementary response that provides explanations, the firm might reduce the overall virality of the negative eWOM, compared with a situation in which it repeats its offer of sympathy in the second response, which might cause frustration ([50]). We predict that such cross-message sequencing should mitigate the virality of negative eWOM to a broad customer audience ([ 6]): H6:  Consecutive firm responses with varying rather than repeated (a) empathic intensity and (b) explanatory intensity are better suited to mitigating evolved online firestorms. Methods Sampling Frame and Text Analysis ProcedureWe used Facebook's Application Programming Interface (API) and processed detailed information on potential online firestorms from the official Facebook brand communities of all U.S. firms listed on the S&P 500 between October 1, 2011 (the introduction of the timeline feature), and January 31, 2016 (the introduction of emojis). We chose this setting for three reasons. First, it is common for customers to use Facebook to interact with firms in their brand communities and to complain through this channel. Second, unlike other rating and review sites that encourage only customers to share their views, firms actively participate in Facebook conversations and respond to customer posts ([71]). Third, in line with previous research on online brand communities (e.g., [53]; [65]), the count of likes and comments indicates the degree to which others approve and share a message and provides an objective measure of virality.We selected all firms that target private customers, have an official Facebook brand community, and allow user posts in their community (see the Web Appendix). Because of our focus on negative eWOM, we analyzed text-based features to determine which posts were negative in two steps. We first applied the R Quanteda package ([ 7]) using Linguistic Inquiry and Word Count (LIWC) text-mining dictionaries to derive the intensity of positive- and negative-emotion words in each post (for more details, see [46]]). We then applied the Stanford Sentence and Grammatical Dependency Parser ([75]) to subdivide each post into sentences and identify dependencies between emotion words and negations (i.e., bigrams). The parser first identifies the presence of an emotion word and then, in cases of negation, automatically assesses whether there is a grammatical relationship (e.g., in the sentence ""The service was not nice,"" the negation ""not"" is grammatically related to the adjective ""nice"" and therefore reverses it). Negated positive emotion words were counted toward negativity and negated negative emotions words toward positivity. If a customer post is more negative than positive overall, we count it as a potential online firestorm.We excluded 48,480 posts that contained a video, picture, event, or external link, because we cannot control for this external content. Furthermore, we excluded 140 posts with fewer than three words, because every full thought requires at least a subject, verb, and object to be understandable for receivers. On Facebook, zero counts of likes and comments from other customers might occur for two reasons: ( 1) The post may have been viewed by other customers but prompted no reaction, or ( 2) the post may not have been displayed to other customers. Thus, we excluded 128,681 posts with no customer reactions, because we are interested in the inflation of virality. We also replicated our analysis with the sample including posts with no customer reactions, and we report the results in the Web Appendix.With these restrictions, our final sample counted 472,995 negative customer posts in English across 89 online brand communities. The posts averaged 99 words (SD = 121.44, ranging from 3 to 8,121 words) and received 2.95 likes and comments on average (SD = 59.22, ranging from 1 to 37,760 likes and comments). Included in this final sample are both well-publicized incidents (e.g., customer post complaining that a police officer was prohibited from using a coffeehouse chain bathroom in September 2015) and less drastic cases in which complaints were supported by a only small number of other customers. Notably, for the well-publicized incidents, newspapers and news portals cited the original Facebook post (e.g., ""A Facebook user reported that a police officer was prohibited from using a bathroom"" [www.snopes.com]). Figure 1 displays illustrative examples of online firestorms and firm responses. In terms of timing, 78% of the online firestorms emerged and dissolved within a day, such that the last comment was posted within 24 hours of the initial negative post, and 93% of firm responses arrived within one hour. This short time frame is a unique feature of online firestorms that differentiates our study from previous research on product-harm crises (e.g., [18]).Graph: Figure 1. Illustrative examples of online firestorms and firm responses.Notes: Posts are edited to exclude company names and customer names. MeasurementTable 1 contains the operationalizations and sources of all variables, along with our rationale for including the control variables, and Figure 2 displays hypotheses and the measurement approach. The key variable of interest is the virality of negative eWOM, measured as the total number of likes and comments a post receives from other customers.[ 5] The total number of likes and comments correlate closely (r =.81), justifying their use as a composite variable. Because of community-level differences in virality (some online brand communities feature thousands of posts every day; others just a few), we use the deviation from the community average. Then, noting the data range and extreme values of virality (see the Web Appendix), we add a constant to have only positive values and apply a logarithmic transformation. To investigate how firms prevent and mitigate online firestorms, we include only likes and comments posted after the respective firm's response ([80]), which ensures that virality has been influenced by the firm response. Importantly, the API does not allow us to capture time stamps for likes. However, comments are time-stamped and known to evolve simultaneously with likes over time ([66]). Therefore, we use the amount of comments following a firm response to approximate the number of likes.Graph: Figure 2. Hypotheses and measurement approach.We measure the intensity of high and low arousal for each negative post with computerized text analysis. In a top-down manner ([46]), we compared each word in a message with predefined emotion word categories. We then calculated an intensity score per emotion word category: the proportion of total words that match each dictionary. In line with the main four negative emotion types in the circumplex model ([70]), we classified the proportion of word use related to fear/anxiety, anger, and disgust as the intensity of high-arousal negative emotions and the proportion of sadness as the intensity of low-arousal negative emotion. We used existing LIWC dictionaries for fear/anxiety, anger, and sadness ([62]), but we needed to develop a new dictionary to derive disgust. We provide this dictionary and details on its development in the Web Appendix. To validate the new dictionary, we compared statistical differences in arousal levels, according to an extremity measure from the Evaluative Lexicon 2.0 (EL 2.0; [68]), between our disgust words and words classified as representing low negative arousal (i.e., sadness). We find a significant difference (F = 7.57, p <.01), with extremity mean scores of 3.24 and 2.89 for disgust and low arousal, respectively. This result confirms that our disgust dictionary matches the EL 2.0 measure for expression extremity.Strength of structural ties has been measured using different variables, including subjective and objective measurements. Because we cannot collect perceptions of tie strength across the millions of brand community users, we followed [67] and operationalized strength of structural ties (SST) as ""the frequency of communication"" ([14], p. 356). Formally, SST for customer i posting negative eWOM at time t in community c is: SST ic =∑τ=0t−1Received Likesicτ+Received Commentsicτ+Received Sharesicτ+∑τ=0t−1Likes Givenicτ+Comments Givenicτ, Graph1where t − 1 is the entire period prior to the post at time t, and SSTic is the sum of likesc, commentsc, and sharesc that customer i received from others in the brand community c before the post at t, as well as the sum of likesi and commentsi the customer gave to others in the brand community c prior to the post at t (all calculated based on the comment timestamp). The API does not allow us to identify customers who share a certain post, due to privacy restrictions.We derived the degree of LSM between customer i posting negative eWOM at time t with the receiving brand community c in three steps. First, we mined the use intensity of each of the nine function word categories j separately in focal customer i's message and across all customer messages (negative and positive) in the brand community c posted in the previous three months in response to the focal negative eWOM post (moving community average). Second, the degree of similar use intensity LSM of each function word category (FWj) by customer i posting the negative eWOM into community c comes from the formula: LSMjic=1−(|FWji−FWj¯ic|FWji+FWj¯ic+.0001). Graph2Third, by aggregating all nine LSM scores with equal weights, we obtain an LSM score bound between 0 and 1, and scores closer to 1 reflect greater degree of communication style matching between customer i and the online brand community c.We measured the intensity of empathy, or the degree of spontaneous affective response ([42]) a firm provided, as the proportion of affect words in the response text, according to the LIWC dictionary for affect. Using the LIWC dictionary for causal expressions, we also measured the intensity of explanation in a firm's responses. We then measured variations in the response sequences as the standard deviation in empathic and explanatory intensities across all firm responses.Following prior research, we account for multiple control variables that might influence the virality of negative eWOM (see Table 1). Firm-related aspects that influence eWOM include industry membership, brand familiarity, and brand reputation. At the online brand community level, we account for community size, member attentiveness and expressiveness, firm engagement frequency, average structural tie strength among members, and variance in linguistic style. Post-related aspects include the number of competing inputs at the time of the post, the sentiment of the previous post, post length, post complexity, and the frequency with which the customer had complained on Facebook in the past. Furthermore, firm response–related aspects include whether a firm responds or not and the firm response time. We used dummy variables to account for whether a firm offered an apology, offered compensation, or suggested a communication channel change; a firm can use more than one response approach in the same message for this measure (e.g., combining apology and compensation). Finally, as fixed effects, we account for the year and month to control for seasonality in user activity and policy changes (see the Web Appendix). We also include fixed effects for weekends and time of day ([48]). The Web Appendix reports correlations and descriptive statistics. Firms responded at least once to 331,370 out of 472,995 negative posts, yielding an average response rate of 70%. Across all firm responses, suggesting a channel change (61%) and apologizing (53%) were most commonly used, while compensation was used less often (3%). The degree to which explanations were offered (8% of the time) was slightly more than the use of empathy (6%). We found that 15,762 negative posts achieved above-average virality and got multiple firm responses, suggesting that 3% of potential online firestorms evolved during the period of observation. The Web Appendix reveals the evolution of the number of potential firestorms, average virality, and average firm response rates over time. While the number of potential online firestorms increased during the study period (and we control for this increase with time-related fixed-effect), both the average virality and the average percentage of firm responses are rather stable over time.GraphTable 1. Operationalization and Sources of All Variables.  1 Notes: We use time dummies to control for year, month, weekend, and time of day. Modeling ApproachThe incidences of negative eWOM are nested within the online brand communities, and thus the negative posts and firm responses might be interdependent. To determine whether a multilevel approach is warranted, we first conducted a one-way analysis of variance with random effects to reveal any systematic between-group variance in the virality of negative eWOM. We find significant between-group variance (χ2(88) = 818,729, p <.01). In addition, the design effect of 36.74 suggests that a multilevel structure is possible ([59]). The maximum variance inflation factor score across all models is 3, indicating no potential threat of multicollinearity.We specified a series of separate hierarchical models, with parameters at the post and the firm/brand community level, using full information maximum likelihood estimation and grand mean-centering. Virality, our focal outcome measure, is operationalized differently across these models along three time periods. Viralityt_1 is the total number of likes and comments from other customers any time after posting at t. Viralityt_2 is the number of likes and comments from other customers any time after the first firm response, and Viralityt_3 is the number of likes and comments from other customers any time after the last firm response. We provide more detailed explanations for each variable in Table 1. Thus, we assess the predictors of virality for all 472,995 negative posts across the 89 brand communities as follows: Viralityict_1=γ00+γ01−04Firm Controlsc+γ05−10Brand Community Controlsc+γ11−17Post Controlsic+γ18Dum No Firm Responseic+γ19−23Post Predictorsic+γ24−43Dum Timingic+u0c+ric, Graph3where t_1 is the time period after the time t of customer i's post in brand community c; Viralityict_1  = the combined sum of likes and comments post i receives from other customers in community c any time after it was posted (brand community-centered and log-transformed); Firm Controlsc  = community-specific controls using the Global Industry Classification Standard: GICS Consumer Discretionaryc, GICS Consumer Staplesc, Brand Familiarityc, and Brand Reputationc; Brand Community Controlsc  = Brand Community Sizec, Brand Community Attentivenessc, Brand Community Expressivenessc, Firm Engagementc, Average Tie Strengthsc, and Variance in Linguistic Stylec; Post Controlsic  = Competing Inputsic,  Sentiment Previous Postic  , Post Lengthic, Post Complexityic, Negation in Postic, and  Previous Complainsic  ; Dum No Firm Responseic  = 1 if there is no firm response at any time, and 0 otherwise; Post Predictorsic  = Intensity of High Arousalic, Intensity of Low Arousalic,  SSTic  , and  LSMic  ; Dum Timingic  = dummy variables for years (baseline is 2015), month (baseline is December), weekend day (baseline is week day), and time of the day (baseline is night time, EST); u0c  = brand community–specific error term; and ric  = post-specific error term.Next, to determine how firms can prevent viral online firestorms, we examine 331,370 negative posts that received at least one firm response according to the following equation: Viralityict_2=γ00+γ01−04Firm Controlsc+γ05−10Brand Community Controlsc+γ11−17Post Controlsic+γ18Firm Response Timeic+γ19−23Post Predictorsic+γ24−43Dum Timingic+γ44−48First Firm Responseic+γ49−52Interactionsic+u0c+ric, Graph4where t_2 is the time period after the first firm response; Viralityict_2  = the combined sum of likes and comments post i received from other customers in community c any time after the first firm response (brand community-centered and log-transformed); Firm Response Time  = time until the first firm response; First Firm Response  =  Compensationic  ,  Apologyic  ,  Channel Changeic  ,  Intensity of Empathyic  , and  Intensity of Explanationic  ; and Interactionsic  = Intensity of High Arousalic ×   Intensity of Empathyic  , Intensity of High Arousalic ×  Intensity of Explanationic  , Intensity of Low Arousalic ×   Intensity of Empathyic  , and Intensity of Low Arousalic ×   Intensity of Empathyic  .Finally, we investigate how firms can mitigate the evolved online firestorms represented by 15,762 negative posts that achieved above-average virality and to which firms responded multiple times: Viralityict_3=γ00+γ01−04Firm Controlsc+γ05−10Brand Community Controlsc+γ11−17Post Controlsic+γ18Firm Response Timeic+γ19−23Post Predictorsic+γ24−43Dum Timingic+γ44−48First Firm Responseic+γ49−51Subsequent Firm Responsesci+γ52−53Variance in Firm Responsesci+u0c+ric, Graph5where t_3,..., T is the time period after the last firm response; Viralityict_3  = the combined sum of likes and comments post i received from other customers in community c any time after the last firm response (brand community-centered and log-transformed); Subsequent Firm Responsesic  = include all firm responses after the first firm response,  Compensationic  ,  Apologyic  ,  Channel Changeic  ,  Intensity of Empathyic  , and  Intensity of  Explanationic  ; and Variance in Firm Responsesci  =  Variance in Empathyic  and  Variance in Explanationic  (across all Firm Responsesic). Results Detecting Potential Online FirestormsThe results provide support for our hypotheses that the intensity of high-arousal emotions (vs. low-arousal emotions), SST, and LSM relate to the virality of negative eWOM (Table 2, Model 3).[ 6] Both intensities of high arousal (γ =.186, p <.01) and low arousal (γ =.026, p <.01) relate positively to virality. However, a t-test reveals that the intensity of high arousal is more strongly related to virality than the intensity of low arousal (t = 35.15, p <.01), in support of H1. In addition, SST (γ = 1.432, p <.01) and LSM (γ =.025, p <.01) relate positively to virality, in support of H2 and H3. Considering the relative influence of the drivers, we find that SST exerts the strongest impact on virality (all t ≥ 77.97, p <.01).GraphTable 2. Predictors of Potential Online Firestorms.  2 †p <.10.3 *p <.05.4 **p <.01.5 Notes: Significance is based on two-tailed tests. We report t-values for Level 2 and effect size r for Level 1. Change in fit in comparison with the model with control variables only. We test GICS consumer discretionary and GICS consumer staples against GICS other. Fixed effects for year, month, weekend, and time of day are included in the model and reported in the Web Appendix. Slopes of high-arousal and low-arousal emotions are different at p <.01 and t = 35.15. An additional model that compared the effects of different high-arousal emotions (fear/anxiety, anger, disgust) with the low-arousal emotion (sadness) revealed that fear/anxiety (t = 10.75, p <.01), anger (t = 37.52, p <.01), and disgust (t = 1.74, p <.10) are all more strongly related to virality than sadness. The small difference between disgust and sadness is in line with Berger and Milkman's (2012) finding and can be attributed to the relative scarcity of disgust in the negative eWOM messages. We also find that anger relates more strongly to virality then other high-arousal emotions (all ts ≥ 21.34, p <.01).Regarding brand community controls, we find that average tie strength relates negatively to virality (γ = −.247, p <.05), and greater variance in linguistic style relates positively to virality (γ = 5.550, p <.10). Online firestorms thus appear to occur less in brand communities in which members have stronger connections with one another and are more similar. Regarding the post controls, we find that competing inputs (γ = −.001, p <.01) and previous complaints (γ = −.213, p <.01) relate negatively to virality. Conversely, post length (γ =.005, p <.01) and post complexity (γ =.017, p <.01) relate positively to virality. Finally, a lack of firm response is significantly related to increased virality (γ =.029, p <.01), clearly indicating the importance of actively managing negative eWOM in online brand communities. Preventing Potential Online FirestormsWhen considering the main effects of the intensities of empathy and explanation in firm responses in Model 5 (Table 3), we find that the increased use of empathy (γ = −.069, p <.01) leads to significantly lower virality than the increased use of explanation (γ = −.011, p <.01; t = 14.42, p <.01). Regarding other firm responses that reflect disengaging from the conversation, we find that responses that contain an apology (γ = −.004, p <.01) or a suggestion for a channel change (γ = −.005, p <.01) relate negatively to virality. However, immediately offering compensation fosters the virality of negative eWOM (γ =.003, p <.01).GraphTable 3. Preventing Potential Online Firestorms.  6 †p <.10.7 *p <.05.8 **p <.01.9 Notes: Significance is based on two-tailed tests. We report t-values for Level 2 and effect size r for Level 1. Fixed effects for year, month, weekend, and time of day are included in the model and reported in the Web Appendix.When considering the interaction effects in Model 6 (Table 3), we find a positive interaction between the intensity of high arousal and the increased use of empathy in the firm response (γ = 2.678, p <.01). The increased use of empathy in a response to a post with a high intensity of high arousal thus increases, rather than decreases, virality. Conversely, we find a significant negative interaction between the increased intensity of high arousal and the increased use of explanation in the firm response (γ = −1.437, p <.01). Therefore, the increased use of explanation in a response to a post with a high intensity of high arousal significantly reduces its virality, as depicted in Figure 3. A t-test further reveals significant differences between providing more empathy versus more explanation in buffering the effect of high arousal (more empathy: γ =.386, p <.01; more explanation: γ =.180, p <.01; t = 23.43, p <.01). Overall, these results support H4 and indicate that intensive high arousal is better contained with more explanation.Graph: Figure 3. Firm response strategies moderate the effect of high-arousal emotions on virality.Notes: Viralityt_2 is measured after the first firm response.Contrary to our expectations, we find no negative interaction between the intensity of low arousal and the increased use of empathy in the firm response at conventional significance levels (γ = −.042, p =.30) and a negative interaction between the intensity of low-arousal emotions and the increased use of explanation in the firm response (γ = −.206, p <.01). A t-test reveals no significant differences between providing more empathy versus more explanation in buffering intensive low-arousal emotions (more empathy: γ =.025, p <.01; more explanation: γ =.016, p <.01; t = 1.63, p =.10). Thus, H5 is not supported. Mitigating Evolved Online FirestormsVariations in the intensity of empathy (γ = −.185, p <.05) and intensity of explanation (γ = −.205, p <.01) relate negatively to virality in Model 9 (Table 4), in support of H6a and H6b. These findings suggest that firms should vary their response formulation to decrease the virality of evolved online firestorms. We further find that an increased use of explanation in subsequent responses increases virality (γ =.083, p <.01). To mitigate the virality of negative eWOM, firms should vary their response, focusing on more empathy in later responses. Notably, at an evolved stage of an online firestorm, firm responses that contain an apology (γ =.031, p <.01) or a channel change (γ =.038, p <.01) positively relate to virality. These findings reveal that such response strategies not only are ineffective in mitigating evolved online firestorms but even fuel the fire. In contrast, offering compensation in a subsequent response negatively relates to virality (γ = −.045, p <.01).GraphTable 4. Mitigating Evolved Online Firestorms.  10 †p <.10.11 *p <.05.12 **p <.01.13 Notes: Significance is based on two-tailed tests. We report t-values for Level 2 and effect size r for Level 1. Fixed effects for year, month, weekend, and time of day are included in the model and reported in the Web Appendix. Additional Analyses Addressing potential endogeneityIn our research design, endogeneity might arise out of reverse causality, omitted variables, or a learning effect. First, the timestamps provided allow us to avoid reverse causality by considering only customer comments that occur after the respective firm response. Second, we address omitted variables stemming from the 89 firms with firm-fixed-effects regressions to account for the unobservable heterogeneity of each brand community ([ 1]). All results are fully in line with the main analyses, as displayed in the Web Appendix. Third, we account for a learning effect in an analysis in which we include a continuous time variable that captures the 52 months of our study period (1 = October 2011 to 52 = January 2016). If endogeneity from a learning effect biases our results, this time variable would reduce the virality of negative posts and/or increase the effectiveness of the response strategies. Instead, when we include the time variable in our prevention model, we find a nonsignificant main effect on virality (p =.98). The positive interaction effects of the time variable with empathy (γ =.010, p <.01) and explanation (γ =.002, p <.08) indicate that both response strategies become less effective over time. Taken together, these additional analyses suggest it is unlikely that endogeneity biases our results. Additional interaction effectsRecent research has suggested that different drivers of virality may reinforce one another ([54]). Thus, we tested for potential interaction effects among the four drivers of online firestorms in Model 3. We find that both SST (γ = 28.840, p <.01) and LSM (γ =.743, p <.01) increase the effect of high-arousal emotions. In addition, LSM increases the effect of low-arousal emotions (γ =.107, p <.01), and SST and LSM reinforce each other (γ = 2.079, p <.01). When we tested whether SST influences the effectiveness of empathy and explanation, we find that both empathy (γ = −4.00, p <.01) and explanation (γ = −5.83, p <.01) are more effective in reducing the virality of negative eWOM from customers with strong structural ties in the brand community. Alternative measures of viralityIn line with previous research (e.g., [65]), we regard the combined number of likes and comments as the most appropriate measure for virality. For robustness, we tested our results with three separate, alternative measures of virality: likes, comments, and shares. Previous research has indicated mixed results, such that [53] find no major difference in using likes or comments as measures, but [20] find different results using likes versus comments to measure virality. We replicate all the models with these different outcome measures and report the results in the Web Appendix; nearly all estimates for the hypothesized effects are directionally similar and significant at conventional levels. Discussion Theoretical Implications and ExtensionsExtensive literature has addressed the benefits of eWOM (e.g., [ 4]), but theoretical and empirical work devoted to negative eWOM in brand communities is scarce. Drawing on research on negative WOM (e.g., [14]; [38]), we combine multiple sender and relational aspects that likely increase the virality of negative eWOM in online brand communities. We then empirically assess their role in driving virality across 472,995 potential online firestorms in 89 online brand communities of S&P 500 firms. Integrating common recovery approaches from service literature ([41]) with emotion regulation strategies ([33]), we also highlight the relative effectiveness of different firm response approaches and cross-response variations to prevent and mitigate online firestorms. Thereby, our study makes three primary contributions to extant marketing research (for a summary of the results, see Table 5).GraphTable 5. Overview of Results.  First, we advance research on how to detect the online firestorm potential of negative eWOM with an empirical investigation of prototypical conceptions of different drivers of virality and their interrelations. In line with prior marketing research on negative WOM ([14]; [38], [58]), we show that the virality of negative eWOM in online brand communities varies depending on sender and relational aspects. As an extension of [54] findings about sharing newspaper articles, we find that in online brand communities, the use of more high-arousal-emotion words in negative eWOM increases its virality and makes it relatively more contagious than the use of low-arousal-emotion words. We also find that stronger structural ties between the complaining customer and the receiving online brand community relate to greater virality of negative eWOM. Thus, Brown and Reingen's (1987) conclusions, gathered from a small offline community, hold in large digital communities. Notably, owing to data limitations, we were only able to use the frequency of communication as tie strength indicator, not the importance attached to the relationship. Moreover, in this otherwise anonymous context, we use the degree of LSM as an indicator of interpersonal similarity between senders and receivers. In line with [54], we find that closer LSM between the complaining customer and the receiving online brand community relates to greater virality of negative eWOM. In contrast with previous studies, we consider a broader set of drivers of virality and test their relative importance and interrelationships. Structural ties are the strongest driver of virality. Furthermore, strong structural ties and a close LSM amplify the virality effect of high-arousal emotions in negative eWOM. To put our estimated effects into perspective with related research, we calculate effect sizes r using the formula from [69]. The identified drivers show effect sizes ranging from.04 to.13. Taken together, our study thus advances negative eWOM research with a theoretically grounded framework of sender and relational aspects, useful for detecting potential online firestorms in brand communities.Second, as suggested in prior research (e.g., [56]), we consider the effectiveness of firm response approaches to reduce the contagiousness of negative eWOM in online brand communities. Contributing to this emergent stream of research, we reconcile research on service recovery ([41]) and emotion regulation ([33]) to delineate common, theoretically grounded firm response alternatives. We empirically confirm the common knowledge that not responding to a negative customer post is a firm's worst choice and should be avoided. In line with [44], we further find that responding fast is important. When actively engaging in elaboration with the complaining customer within the online brand community, the increased use of empathy is more effective overall. However, if a negative eWOM message contains exceptionally intense high-arousal emotions, increasing the amount of explanation is more effective for preventing and mitigating its virality. Contrary to our expectations, we find that explanation rather than empathy best contains negative eWOM containing severe low-arousal emotions (e.g., sadness). Indeed, in line with [33], customers who are experiencing severe emotions (either anger or sadness) are not able to shift attention but are always looking for explanations beyond empathy.Initially aiming to block and disengage, rather than engage in elaborate online discussions, firms are best advised to offer an apology or suggest a channel change. At a later stage, once the negative eWOM has gathered support within the online brand community, these disengagement approaches are not only ineffective but may further increase a post's virality. Offering to take a customer's complaint offline seems to remove at least some negative conversations before they go viral. However, at a later stage, when the support of others for the negative eWOM has increased its perceived reliability ([21]), both the complaining customer and the community likely feel further encouraged by a firm's admittance of guilt (i.e., apology) and disgruntled if forcefully removed from the conversation (i.e., channel change). Moreover, the switching effect of channel change is in line with recent research from [30], who find that for customers using indirect revenge behaviors with public exposure (e.g., complaining online in a brand community), desire for revenge increases over time. Offering compensation only mitigates the virality of the negative eWOM message when used as a later response by the firm. Some controversy exists regarding the effectiveness of compensation as a means to recover from a service failure. That is, compensation may dissipate customers' frustrations ([12]), but offering it without explanation increases attributions of control and indicates an admission of guilt, evoking more negative evaluations ([11]). In line with [31], we therefore suggest that compensation is most effective if it follows an offer of an explanation or empathy. With these findings on how to prevent online firestorms, we extend debates about service recovery strategies to negative eWOM in online brand communities.Third, online firestorms that have evolved often require multiple responses. We contribute to research on social media sharing by analyzing the implications of variations in firm response sequences ([ 6]). Several firm responses are likely to be interpreted jointly rather than in isolation ([79]). We find that varying response approaches, rather than consistently responding in the same way, can reduce the virality of evolved online firestorms in brand communities. Firms that use the same intensity of empathy or explanation to respond to negative eWOM increase, rather than mitigate, its contagiousness to other community members. Together with our findings that the effectiveness of compensation, apology, and channel change depends on when they are used, we add new insights into how best to sequence multiple firm responses in social media. Finally, as a useful resource for research, we have developed text-mining dictionaries to derive firms' response approaches automatically using a top-down text-mining approach (see the Web Appendix). We carefully followed traditional dictionary development standards ([46]), so researchers who want to examine written or transcribed firm responses in firm–customer exchanges may use these dictionaries as a starting point for their own investigations of firm response strategies. Managerial ImplicationsBy investigating how to detect, prevent, and mitigate the virality of negative eWOM in online brand communities, we offer several actionable implications for managers. We discuss them in the following subsections. Detecting potential online firestormsBrand community managers, who struggle to identify potentially threatening negative eWOM messages, should consider complainers' message formulations, beyond what is literally said, as well as their relationship with other members of the community. First, by using our dictionary-based, straightforward, automatic text-mining approach, managers can assess the high- and low-arousal levels of negative messages to predict their potential virality. The higher-arousal-emotion words a message contains, the more likely it is to go viral. Second, managers should assess the tie strength of the customer posting the negative message. Negative messages by customers who frequently interact with other community members are more likely to go viral than messages by customers who are relative strangers in the community. Third, text-mining tools can track the brand community's dominant communication style continuously and contrast it with the style of each negative customer post. Posts that closely match the dominant communication style are more likely to go viral. Taken together, the identified drivers explain 25% of virality across all examined brand communities. Finally, the different drivers amplify one another, so managers should be particularly cautious of complaining customers with strong structural ties who closely match the community's dominant communication style. Preventing potential online firestormsUnlike in a traditional service recovery setting, the success of managers' responses in preventing online firestorms critically depends on their ability to satisfy both the complainant and the brand community. Not responding is the worst choice, but the firm response also needs to be fast and tailored to the customer's message. In an initial response, empathy is generally most effective for containing negative eWOM. However, very negative messages that use an exceptional amount of high-arousal emotion words (e.g., ""angry,"" ""hate"") demand more explanation. To disengage from the conversation and reduce the virality of negative eWOM upfront, managers should apologize or ask the unsatisfied customer to use another channel to raise the issue. Offering compensation immediately is not advised; it is effective only as a later response. We find that an appropriate response strategy can reduce virality of an intensive high-arousal post by up to 10%, which may equal hundreds of angry customers supporting and sharing negative eWOM.[ 7] Mitigating evolved online firestormsSome negative eWOM cannot be prevented from going viral or ""catching fire"" among other customers, and managers will need to respond multiple times. These responses are likely to be viewed collectively, rather than in isolation, so managers should consider each response as part of an overall response sequence. Rather than consistently posting the same message, managers should vary the use of empathy and explanation to mitigate the further virality of negative eWOM messages. An explanatory approach is viable as a first response to exceptionally intense high-arousal negative eWOM, and later firm responses should use increased empathy. If used at a later response stage, apologizing or suggesting a different communication channel will ""feed the fire"" and increase the virality of the negative eWOM. Instead, offering compensation should be the last resort for managers to prevent further elaborations and reduce the virality of negative eWOM. Using an appropriate response strategy over time can reduce subsequent virality by up to 11%.[ 8] Limitations and Directions for Further ResearchOur results are consistent with the proposition that firms need to manage negative eWOM in their online brand communities actively to prevent or mitigate their detrimental effects (e.g., [40]; [63]). Although we believe our findings have broad applicability, managing online firestorms is a vast and largely neglected field of research that is of critical importance to managers. Thus, it is important to recognize some limitations of our study and suggest further research. Although our large-scale study offers theoretical and empirical insights into textual aspects related to the virality of negative eWOM, using what [46] call a top-down approach, we also acknowledge that alternative bottom-up approaches might usefully derive specific service or product failures and their severity to test the suitability of the response approaches we outline. Similarly, in other communication contexts where heuristic processing is less prevalent, the implications of systematic content in firm responses should be assessed (e.g., size of the compensation, legitimacy of the argument).Moreover, the scope of our study is limited to all posts visible in the communities. On Facebook, firms have the option to remove comments. Deletion criteria may include especially offensive (e.g., racist, derogatory) messages. This option may bias our results for high-arousal emotions, because we do not observe deleted posts. Managerial reports strongly discourage deleting negative customer posts on social media (e.g., [22]), but extreme posts missing from our data set could further increase the virality effect, or the effect may taper off or even reverse with an extreme use of high-arousal emotions. The consequences of deleting customer posts remain to be investigated. In addition, we could not assess perceptions of source credibility or how source credibility may interact with the use of empathy or other firm responses to make them more or less effective. Therefore, further research might seek novel ways to determine the importance of source credibility for message acceptance. Similarly, lacking an ability to account for the attitudinal importance that customers attribute to their ties in brand communities, continued research could extend our study by assessing the implications of weak and strong tie perceptions for sharing negative eWOM in brand communities. Potential measures that could be adapted for this purpose could be obtained from [77]. Furthermore, we were not able to obtain data on the number of friends due to privacy restrictions in Facebook's API terms and conditions. However, in line with [61], we believe that strength of structural ties (i.e., the number of encounters with other users in the brand community) influences virality regardless of the number of friends.Interestingly, we found that customers expressing intense negative emotions, irrespective of whether they are high (e.g., anger) or low (e.g., sadness) on arousal, are looking for explanations rather than empathy. Future research should consider how the relative lack of emotionality might relate to the suitability of firm's response options. Finally, it was surprising that immediate compensation leads to more virality. Potentially, if firms offering an initial compensation that is perceived as not high enough could lead to an offense. If firms would then offer a higher compensation at a later stage this might lead to less virality. Thus, future research should investigate whether offering greater levels of compensation at later stages lead to the observed effects. "
26,"Do Consumers Always Spend More When Coupon Face Value is Larger? The Inverted U-Shaped Effect of Coupon Face Value on Consumer Spending Level Commonly, a coupon can be applied to one of several vertically differentiated products sold at different prices within the same product line of a brand. With such a product-line coupon, consumers need to decide on the specific product to buy, resulting in different levels of consumer spending. One field data set and four lab experiments demonstrate that the relationship between coupon face value and consumer spending level may not always be intuitively positive; under certain circumstances, it could take an inverted U-shape. The authors develop a threshold-based model to explain the inverted U-shaped effect of coupon face value on consumer spending level and show that this effect occurs when the price level of products is high, when consumers have a strong saving orientation, when they experience low information load from processing a small number of products, when they are inclined to engage in thorough product comparison, or when they have a weak preexisting preference for a specific level of product benefit.Supplement: http://dx.doi.org/10.1509/jm.14.0510Price-based promotions have been widely used by retailers to stimulate product sales (DelVecchio, Krishnan, and Smith 2007). In marketing practice, it is quite common for sellers to offer product-line coupons to consumers. A product-line coupon is not restricted to a specific product. Instead, it can be used to buy one of several vertically differentiated products sold at different prices within the same product line of a brand.1 For instance, consumers may get a $50off coupon with which they can enjoy a price discount on any model of a Dell laptop computer series. In another scenario, consumers may receive an unrestricted $10-off coupon from a restaurant, which can be applied to any combination of dishes and drinks.Extensive marketing research has examined the promotional effects of product-specific coupons, wherein a discount is restricted to a specific product (e.g., Alba et al. 1999; Chandran and Morwitz 2006; Chen, Monroe, and Lou 1998; Chen and Rao 2007; Lee and Tsai 2014; Mishra and Mishra 2011; Nunes and Park 2003; Raghubir 1998; Shiv, Carmon, and Ariely 2005). Yet much less attention has been paid to product-line coupons, wherein an unrestricted discount can be applied to any option within the same product line of a brand. In this scenario, the consumer must decide not only whether to redeem the coupon (e.g., Cheema and Patrick 2008) but also what specific product to buy with the coupon, which may lead to different spending amounts.In this research, we focus on product-line coupons in an amount-off format and examine how the face value of a product-line coupon could influence consumers’ specific product choice and, consequently, their spending level. One would intuitively expect that a larger coupon face value would incentivize consumers to spend more. In contrast, by developing a threshold-based model, the present research identifies conditions in which the relationship between coupon face value and consumer spending level does not take a positive form but, instead, an inverted U-shape.With one field data set and four lab experiments, we demonstrate that the inverted U-shaped effect of coupon face value on consumer spending level occurs when products are expensive, when consumers have a strong saving orientation, when they experience low information load from processing a small number of products, when they have a high tendency to compare options thoroughly in their decision process, or when they have a weak preexisting preference for a specific level of product benefit. These findings contribute to the price-based promotion literature and offer important implications for managing coupon face value. Conceptual FrameworkA consumer’s spending decision during a transaction is highly susceptible to the influences of sales promotions (e.g., Park, Iyer, and Smith 1989; Ramanathan and Dhar 2010). As Table 1 summarizes, prior research has mainly examined how the monetary value of a price-based promotion (i.e., promotion depth) influences consumers’ purchase incidence and quantity in the focal promotion period as well as their responses in the postpromotion period, in the case of restricted product-specific promotions. The present research fills a gap in the literature by focusing on the circumstance of unrestricted product-line coupons and by investigating how coupon face value shapes consumers’ spending level under this circumstance.To simplify the theoretical analysis, we begin with a scenario in which consumers make a binary choice between a high-priced, high-benefit option (i.e., a high spending level) and a low-priced, low-benefit option (i.e., a low spending level) and then discuss the boundaries of our analysis. The aim of this research is to examine how consumers’ choice of a high spending level (SH) over a low spending level (SL) varies as the face value of a product-line coupon in an amount-off format (C) increases from zero to the lower bound of potential spending (0 < C < SL < SH). We first discuss two forces that may drive consumers’ spending decision opposingly and then analyze how the relative strengths of these two forces may shift as coupon face value increases. Budget IncreaseConsumers’ total amount of spending is constrained by their mental budget (Karlsson et al. 2004, 2005; Larson and Hamilton 2012; Stilley, Inman, and Wakefield 2010; Van Ittersum, Pennings, and Wansink 2010). When a coupon is present, consumers may experience an increase in their mental budget for product expenditure and thus are encouraged to spend more, which was documented by Heilman, Nakamoto, and Rao (2002) as a “psychological income effect.” Similarly, Dre`ze, Nisol, and Vilcassim (2004) found that promotions increase in-store expenditures. These findings suggest that an increase in the face value of a product-line coupon will propel consumers to spend more by choosing a more expensive product that provides greater benefits due to an increase in their mental budget (Allenby and Rossi 1991).According to this budget-increase perspective, the relationship between the face value of a product-line coupon (C) that consumers receive and their likelihood of choosing a high level of spending (LH) over a low level of spending can be expressed as: ( 1) LH = a • C + b, where b is the baseline attractiveness of the high spending level, and a is positive and represents that consumers’ likelihood of choosing a high level of spending increases as coupon face value increases. Nevertheless, another theoretical perspective suggests that coupon face value may have an opposite effect on consumer spending level. Savings ComparisonResearch in marketing and economics has shown that when people evaluate price discounts, they often base their spending decision on the savings percentage associated with a specific discount, which is the ratio of the monetary value of the discount to the original product price (Nunes and Park 2003; Saini, Rao, and Monga 2010; Saini and Thota 2010; Tversky and Kahneman 1981). From this perspective, it is likely that consumers may focus on savings percentages associated with redeeming a product-line coupon for different products in their spending decision. Given that consumers could enjoy a greater savings percentage by applying the same coupon to a less expensive product than to a more expensive one, the less expensive product would become more attractive when consumers base their spending decision on savings percentages.To further illustrate this perspective in a simplified binary choice scenario, consumers may compare the savings percentage associated with choosing a high spending level (C/SH) with that associated with choosing a low spending level (C/SL). Consequently, their likelihood of choosing a high spending level (LH) over a low spending level is determined by the relative savings percentage, which is denoted by C/SH – C/SL, such that: ( 2) LH = C=SH - C=SL + b = ðSL - SHÞ=ðSL • SHÞ • C + b;where b again represents the baseline attractiveness of the high spending level, and (SL – SH)/(SL • SH) is negative because SL is smaller than SH. As Equation 2 indicates, as coupon face value increases, the relative savings percentage related to choosing the high (vs. the low) spending level decreases. Consequently, consumers’ likelihood of choosing the high spending level should also decrease. In summary, the savings-comparison perspective suggests a negative relationship between coupon face value and consumer spending, such that the relative attractiveness of choosing the high spending level is reduced when coupon face value increases.TABLE: TABLE 1 Review of Literature on Influences of Promotion Depth on Consumer ResponsesTABLE: TABLE 1 Review of Literature on Influences of Promotion Depth on Consumer Responses   A Threshold-Based AnalysisOstensibly, the savings-comparison perspective leads to a prediction opposite to that based on the budget-increase perspective. We reconcile these two mechanisms by analyzing when each would be more prevalent. We argue that, consistent with one’s intuition, the budget-increase mechanism is the default mechanism driving the effect of coupon face value because it serves as a straightforward heuristic for consumers. In contrast, the savings-comparison mechanism is more cognitively complex and effortful, and thus its activation should depend on specific conditions.Relevant to the focal issue, previous research has shown that consumers are not responsive to savings percentages of pricebased promotions unless the savings percentages exceed a certain threshold (Chen, Monroe, and Lou 1998; Gupta and Cooper 1992). From this threshold account, we predict that the relative strengths of the budget-increase mechanism and the savings-comparison mechanism may vary as coupon face value increases. This is because the magnitude of the relative savings percentage associated with choosing a high spending level over a low spending level (C/SH – C/SL) is directly determined by coupon face value (C).Consider the following example for a numerical illustration. Suppose that the same coupon can be used for buying either Product A (price = $40) or Product B (price = $60). When coupon face value is as small as $5, the relative savings percentage associated with choosing Product A over Product B is only 4.1% (i.e., $5/$40 – $5/$60). Such a difference may be too small to exceed the threshold above which consumers start to use the relative savings percentage as an important basis for decision making (Chen, Monroe, and Lou 1998; Gupta and Cooper 1992). Thus, we expect that when the face value of a product-line coupon is relatively small, the savings-comparison mechanism will not be activated. At this stage, we expect that coupon face value will influence consumer spending level mainly through the budget-increase mechanism and thus will have a positive impact on consumer spending level.In the same numerical example described in the last paragraph, if coupon face value increases to a large amount, such as $25, the relative savings percentage becomes 21% (i.e., $25/ $40 – $25/$60). At this stage, the relative savings percentage (i.e., 21%) may become large enough to exceed a certain threshold (Chen, Monroe, and Lou 1998; Gupta and Cooper 1992) so that it is more likely to be used as an important decision input for consumers. As a result, coupon face value may influence consumers’ spending level mainly through the savingscomparison mechanism that suppresses the budget-increase mechanism. Consequently, consumers could be more attracted by a lower spending level that is associated with a higher savings percentage, and consumer spending will decrease as coupon face value further increases. Building on these threshold-based analyses, we propose that the face value of a product-line coupon in an amount-off format might have an inverted U-shaped effect on consumers’ total amount of spending. Conditions for the Inverted U-Shaped RelationshipThe previous threshold-based analyses suggest that the savingscomparison mechanism remains inactive when coupon face value is small because the relative savings percentage is too small to be used as an important decision input. In contrast, at large coupon face values, the relative savings percentage becomes large enough to exceed a certain threshold so that it could potentially serve as a meaningful decision input for consumers. However, consumers’ potential adoption of the relative savings percentage as a decision input could still be inhibited in the following circumstances: ( 1) when consumers have a weak saving orientation and thus do not eventually base their spending decision on the relative savings percentage (H1), ( 2) when consumers cannot figure out the relative savings percentage in the first place because of either a low cognitive ability constrained by information overload (H2) or a low tendency to compare savings percentages associated with different options (H3), or ( 3) when consumers have a preexisting preference for a certain product that provides a specific level of benefit and thus do not base their product choice on external factors at all, including savings percentages (H4). In these cases, the savings-comparison mechanism and, consequently, the inverted U-shaped effect of coupon face value on consumer spending will still not emerge. Figure 1 shows the conceptual framework summarizing these moderators. We develop our hypotheses in the following paragraphs.First, we discuss a circumstance under which consumers differ in the motivation to enjoy a greater savings percentage by choosing a low-priced option. Prior research has shown that consumers can vary substantially in frugality, in terms of being “restrained in acquiring and in resourcefully using economic goods and services to achieve longer-term goals” (Lastovicka et al. 1999, p. 88). In the focal research context, we focus on the saving orientation component of being frugal in product acquisition. When consumers care about saving money and minimizing their product acquisition cost, they should be more attracted by the greater savings percentage associated with choosing a low-priced option. For these consumers, the savings-comparison mechanism should be more likely to emerge at large coupon face values. Therefore, the inverted U-shaped effect should be more likely to occur.In contrast, consumers who have a relatively weaker saving orientation should be less motivated to base their spending decision on a sizable relative savings percentage at large coupon face values. Instead, these consumers may be either more likely to take advantage of an increase in their mental budget by simply spending more when coupon face value increases or constantly inclined to choose the high-priced option regardless of coupon face value. Thus, we propose: H1: Consumers’ saving orientation moderates the effect of coupon face value on consumer spending level, such that coupon face value has (a) an inverted U-shaped effect for consumers who have a strong saving orientation but (b) a simply positive effect or no effect for consumers who have a weak saving orientation.Second, while upwardly adjusting one’s spending level according to an increase in his or her available budget is effortless, roughly calculating and comparing savings percentages necessarily requires a greater extent of cognitive processing (Kahneman 2011). This implies that if consumers have already spent a large amount of cognitive resources browsing and processing excessive product information, further calculating and comparing savings percentages would become a highly formidable task because of the depletion of cognitive resources (Shiv and Fedorikhin 1999). In this case, consumers would not be able to figure out the relative savings percentage, which cannot be further used as a decision input as well. Consequently, the savings-comparison mechanism would be unlikely to operate, and the budget-increase mechanism would remain the driver for a positive effect of coupon face value on consumer spending as coupon face value further increases. Thus, we expect that information overload in product presentation will diminish the inverted U-shaped effect and instead foster a positive effect of coupon face value on consumer spending. In contrast, the inverted U-shaped effect should be more likely to occur when product information is not excessive for consumers to process. Thus, we formally propose: H2: Information load moderates the effect of coupon face value on consumer spending level, such that coupon face value has (a) an inverted U-shaped effect when information load is low but (b) a simply positive effect when information load is high.Third, given that comparing the savings percentages associated with different spending levels to calculate the relative savings percentage tends to be cognitively demanding (Kahneman 2011), a facilitating condition for the savings-comparison mechanism to dominate the budget-increase mechanism at large coupon face values is that consumers indeed engage in a deep level of comparative information processing. Following this logic, the inverted U-shaped effect should be more evident for consumers who have a high tendency to compare options thoroughly in their decision process. In contrast, if consumers have a low tendency for thorough comparison, they would be less likely to figure out the relative savings percentage. Consequently, they would not further rely on the relative savings percentage as a decision input. For these consumers, the savings-comparison mechanism may not be triggered. Instead, the budget-increase mechanism would be still prevalent and lead to an overall positive effect of coupon face value on consumer spending even when coupon face value increases to a large amount. We formally propose:H3: Consumers’ tendency to compare options in their decision process moderates the effect of coupon face value on consumer spending level, such that coupon face value has (a) an inverted U-shaped effect for consumers who have a high tendency for comparison but (b) a simply positive effect for consumers who have a low tendency for comparison.TABLE: TABLE 2 Summary of Effects of Coupon Face Value on Consumer Spending Level  Finally, another underlying assumption for the inverted U-shaped effect is that consumers have a weak preexisting preference for a specific level of product benefit. These consumers need to make a trade-off between price and benefit for different options within the same product line of a brand. Thus, their product choice is easily susceptible to the influences of situational factors, such as coupons. However, if consumers have a strong preexisting preference for a product that provides a specific level of benefit (e.g., a preference for a certain capacity of a mobile hard drive), their product choice should be less influenced by coupon face value. As a result, for these consumers the inverted U-shaped effect will disappear. Thus, we propose:H4: Consumers’ preexisting benefit-level preference moderates the effect of coupon face value on consumer spending level, such that coupon face value has (a) an inverted U-shaped effect for consumers who have a weak preexisting benefit-level preference but (b) no effect for consumers who have a strong preexisting benefit-level preference. Overview of StudiesFive studies explore conditions for the proposed inverted Ushaped effect and investigate a set of moderators theoretically relevant to the savings-comparison mechanism. Study 1 uses field data to examine consumer spending at restaurants and provides correlational evidence for the inverted U-shaped relationship between coupon face value and consumer spending when the price level of a restaurant is relatively high, which may induce a strong saving orientation (H1) or a high tendency to compare (H3). Studies 2–5 provide more evidence for causality by experimentally manipulating coupon face value in a controlled lab setting and show that the inverted U-shaped effect would be more likely to occur when consumers have a strong saving orientation (H1), experience low information load (H2), have a high tendency for comparison (H3), or have a weak preexisting benefit-level preference (H4). Table 2 summarizes the results of the aforementioned studies as well as three additional studies. Study 1: Evidence from Field DataIn Study 1, we provide preliminary evidence for the existence of an inverted U-shaped effect of coupon face value in an amount-off format on consumer spending level by using actual consumer spending data from restaurants. When eating in restaurants, consumers can choose different combinations of dishes and drinks with different prices and quantities. Therefore, the same coupon can be linked to different levels of total spending on consumption. Because the dishes and drinks are consumed together while consumers dine in restaurants, they can be regarded as parts of an integrated product, making consumption in restaurants an ideal empirical context for our hypothesis testing. DataWe obtained the field data from a third-party restaurant review site in China (similar to Yelp.com). Consumers who registered for the review site were provided with a membership card and could download coupons available on the review site onto their accounts. When consumers used their membership cards at participating restaurants, the actual transaction amounts were recorded. The data set contained 48,787 observations from a major city in China on a weekly basis from May 2005 to March 2008, with 26,660 registered consumers who spent money in 106 participating restaurants that posted coupons involving amount-off discounts on the review site.2 We regressed total consumer spending per transaction on a set of variables specified as follows: ( 3) Spending = b0 + b1FaceValue + b2FaceValue2 + b3PriceLevel + b4FaceValue • PriceLevel + b5FaceValue2 • PriceLevel + e:The dependent variable was the total amount of money (in Chinese yuan [CNY]) involved in an individual consumer’s single transaction with a specific restaurant (Spending; i.e., total amount paid plus coupon face value). To test the inverted U-shaped effect of coupon face value (FaceValue; in CNY), we included both the linear and squared terms of this variable. We also examined the moderating role of the price level (PriceLevel; in CNY) of a restaurant in the inverted U-shaped effect. Price level was operationalized as the average spending amount per person consumers reported on the review site for a specific restaurant. We expect that a higher price level will activate a stronger saving orientation (H1) or a higher tendency for thorough comparison (H3) and thus facilitate the inverted U-shaped effect. In contrast, we expect that a lower price level will not trigger a strong saving orientation or a high tendency to compare among consumers. Thus, it would be more likely to foster a simply positive effect of coupon face value on consumer spending. To test this prediction, we entered the firstand second-order interactions between coupon face value and price level in the model.3 Results and DiscussionCentral to our prediction, the model specified in Equation 3 generated a negative second-order interaction between coupon face value and price level (B = -.0002, t = -7.48, p < .001), suggesting that the price level of a restaurant moderated the nonlinear effect of coupon face value on consumers’ spending amount per transaction in the focal restaurant. We further decomposed this second-order interaction using a spotlight analysis (Aiken and West 1991; Fitzsimons 2008). When price level was high (1 SD above the mean), we observed a positive linear term (B = 2.27, t = 10.78, p < .001) and a negative squared term (B = -.01, t = 6.67, p < .001) of coupon face value. Initially, consumer spending increased with coupon face value. Yet after coupon face value reached 162 CNY (approximately US$24), a further increase in coupon face value started to decrease consumer spending. In contrast, when price level was low (1 SD below the mean), the model only revealed a simply positive effect (B = 1.39, t = 11.24, p < .001) of coupon face value on consumer spending (for a graphical illustration, see Figure 2; for the detailed statistics for this and other studies, see the Web Appendix).Study 1 demonstrates an inverted U-shaped effect of coupon face value on consumer spending when price level is high but a positive effect of coupon face value on consumer spending when price level is low. This study has two limitations. First, the findings are preliminary given that the nature of the data is correlational. We could not exclude the possibility that consumers who actively look for high-value coupons are less likely to buy high-priced items in the first place. Second, we assume that a higher price level may trigger a stronger saving orientation (H1) or a higher tendency for comparison (H3). These two proposed constructs await further testing. We address these two limitations in the follow-up lab experiments. Specifically, we examine saving orientation in Study 2 and tendency for comparison in Study 4. Study 2: The Moderating Role of Saving OrientationIn Study 2, we manipulated coupon face value to provide direct evidence for the causal relationship between coupon face value and consumer spending level. Whereas Study 1 provides preliminary evidence for the moderating role of saving orientation by assuming that it is associated with price level, in Study 2 we directly measured participants’ saving orientation. If the inverted U-shaped effect is indeed driven by the fact that the savings-comparison mechanism dominates the budget-increase mechanism at larger coupon face values, this effect should be more evident when consumers have a strong saving orientation. In contrast, if consumers have a weak saving orientation, the savings-comparison mechanism will not be triggered, and thus the inverted U-shaped effect should not be observed (H1).In this study, we simplified consumer spending level to a binary choice between a low-priced, low-benefit product and a high-priced, high-benefit product from the same product line of a brand. We also provided participants with a third option to not redeem their coupons for the two presented products (i.e., no-purchase option). In this setup, we were able to examine how the face value of a product-line coupon influences both product category purchase incidence (i.e., whether participants would choose to redeem their coupons in the first place) and spending level (i.e., which product participants would choose when they have decided to redeem their coupons). Spending level (rather than product category purchase incidence, which has been extensively examined in prior research) is the primary focus of the present research. Design and ProcedureStudy 2 adopted a 5 (coupon face value: $5, $15, $25, $35, or $45) • 2 (saving orientation: weak vs. strong) between-subjects design, with saving orientation measured as an individual difference variable. Three hundred U.S. residents (145 women; Mage = 35.69 years, SD = 11.76) from Amazon Mechanical Turk (MTurk) participated for monetary compensation. In this study, participants were asked to imagine that they had received a coupon that could be used to buy a mobile hard drive at an online store. We chose this product category because the performances of different mobile hard drives can be unambiguously differentiated. Thus, participants had to make a clear trade-off between product benefit and product price, making their product choice highly susceptible to the influence of coupon face value. Participants imagined that the coupon they received could be used to buy one of two mobile hard drives of the same brand. The two mobile hard drives differed in capacity ( 1,000 GB vs. 2,000 GB), revolutions per minute (RPMs; 5,400 vs. 7,200), and price ($55.45 vs. $99.95; for a description of product stimuli and a coupon example, see Figure WA1 in the Web Appendix). Participants indicated which of the two mobile hard drives they would like to buy using the coupon they received. They could also choose to not redeem the received coupon, such that they had three options in total (i.e., low-priced hard drive, high-priced hard drive, or no purchase).At the end of the survey, we measured participants’ saving orientation by borrowing items related to saving money in product acquisition from the frugality scale (Lastovicka et al. 1999), including “I am willing to wait on a purchase I want so that I can save money,” “There are things I resist buying today so I can save for tomorrow,” “I believe in being careful in how I spend my money,” and “I discipline myself to get the most from my money” (1 = “strongly disagree,” and 7 = “strongly agree”). We also more directly asked participants the extent to which they put more emphasis on “lower price” ( 1) or “better performance” ( 7) on a seven-point bipolar scale (reverse coded). These items were averaged to form a saving orientation index (a = .75). Results and DiscussionPurchase incidence. First, we examined the effect of coupon face value on product category purchase incidence in a logistic regression (0 = “not purchasing any product,” and 1 = “choosing either the low-priced product or the high-priced product”). Although purchase incidence increased from 86.9% to 93.4% as coupon face value increased from $5 to $45, such an increase was not significant (B = .02, Wald( 1) = 1.17, p = .28).4 Furthermore, neither the quadratic effect of coupon face value nor the first- or second-order interactions between coupon face value and saving orientation were significant (ps > .78) when they were added to the logistic regression. Given that, in this study, coupon face value did not significantly affect product category purchase incidence, we dropped the “no-purchase” option from the dependent variable and further investigated the effect of coupon face value on participants’ spending level.Spending level. To examine the moderating role of saving orientation, we entered saving orientation, coupon face value, coupon face value’s squared term, and the firstand second-order interactions between coupon face value and saving orientation into a logistic regression with product choice as the dependent variable (0 = “low-priced option,” and 1 = “high-priced option”). Central to our theorization, the second-order interaction between coupon face value and saving orientation was negative and marginally significant (B = -.002, Wald( 1) = 2.69, p = .10). This result indicates that participants’ saving orientation moderated the nonlinear effect of coupon face value on spending level.In a further spotlight analysis (Aiken and West 1991; Fitzsimons 2008), when saving orientation was relatively stronger (1 SD above the mean), the linear term of coupon face value was positive (B = .11, Wald( 1) = 3.86, p = .05), and its squared term was negative (B = -.002, Wald( 1) = 4.25, p = .04), suggesting that the relationship between coupon face value and consumer spending was inverted U-shaped. Participants’ spending level was the highest when coupon face value was between $25 and $35. When saving orientation was relatively weaker (1 SD below the mean), coupon face value had only a directionally positive yet nonsignificant effect on spending level (B = .004, Wald( 1) = .09, p = .77; for a graphical illustration, see Figure 3). As Figure 3 shows, a weaker saving orientation encouraged a stronger preference for the high-priced option (i.e., around 70%) in the first place. Thus, room for an increase in the choice share of the highpriced option could be very limited. Taken together, these results support the moderating role of saving orientation (H1).Study 2 establishes the causal effect of coupon face value on consumer spending level by using an experimental approach and provides support for H1’s assertion that coupon face value has an inverted U-shaped effect on spending level only when consumers’ saving orientation is relatively stronger. In the following experiments, we seek further evidence for the savingscomparison mechanism that underlies the inverted U-shaped effect by examining other theoretically relevant moderators. Given that, in Study 2, we found that coupon face value did not significantly influence product category purchase incidence, in the following studies we do not include a “no-purchase” option in the choice set, to simplify the experimental design. Study 3: The Moderating Role of Information LoadAccording to our theorization, information overload should inhibit the inverted U-shaped effect because such overload depletes consumers’ cognitive resources that are necessary for carrying out a thorough comparison among savings percentages. Instead, under information overload, consumers should follow a less effortful path by simply adjusting their spending level in line with an increase in their mental budget as coupon face value increases (H2). In Study 3, we tested this hypothesis and, specifically, focused on the description (Townsend and Kahn 2014) and number (Scheibehenne, Greifeneder, and Todd 2010) of products as two sources of information overload. Another purpose of this study is to generalize the findings of Study 2 to another product category. Whereas the mobile hard drive stimuli used in Study 2 were primarily utilitarian, we chose a pair of product stimuli that were primarily hedonic for Study 3. Design and ProcedureIn Study 3, we examined participants’ choice of food menus and investigated the effects of two sources of information overload: ( 1) the description of products and ( 2) the number of products in a choice set. For this purpose, this study adopted a 4 (coupon face value: $2, $4, $6, or $8) • 2 (perceived information load of product descriptions: low vs. high) • 2 (number of presented products: small vs. large) between-subjects design, with the second factor measured at the end of the study. In total, 1,162 U.S. residents (515 women; Mage = 33.63 years, SD = 11.46) were recruited from MTurk and took a survey for monetary compensation.Participants imagined that they received a coupon that could be used in a Korean barbecue buffet restaurant. Then, they were presented with Korean barbecue buffet menus, which differed in price and variety of dishes. In the small-number condition, participants could choose from only two Korean barbecue buffet menus: a five-item menu at $12.99 and a ten-item menu at $19.99. Those in the large-number condition could choose from six Korean barbecue buffet menus, which contained five to ten items each (these included the two menus presented in the small-number condition; for a description of product stimuli and a coupon example, see Figure WA2 in the Web Appendix). After viewing the coupon and menus, participants indicated their menu choice.At the end of the survey, we measured the perceived information load of product descriptions by asking participants the extent to which they thought that “there was too much information in the menu descriptions” and “it was difficult to process all the menu information” on a seven-point scale (1 = “not at all,” and 7 = “very much”), which formed an information load index (a = .82). Compared with the product stimuli used in Study 2, which listed only three key attributes of mobile hard drives, the stimuli employed in this study listed all the dishes contained in the buffet menus and, thus, presented moderately excessive information even when the number of presented products was small. Such amounts of information would create a reasonable variation in perceived information load among participants and thus facilitate our hypothesis testing regarding the moderating role of information load. We expect that when the number of presented products is small, the effect of coupon face value on consumer spending level will be inverted U-shaped when participants’ perceived information load from processing product descriptions is low. Conversely, this effect will become positive when consumers’ perceived information load is high. In contrast, when the number of presented products is large, the effect of coupon face value on consumer spending level will be positive because the presence of multiple products plus the moderate amount of information per product already increases the perceived information load of all participants to a relatively high level (H2). Results and DiscussionWhen the number of presented products was small. We conducted a logistic regression in which level of spending (0 = “low-priced option,” and 1 = “high-priced option”) was regressed on the linear and squared terms of coupon face value, perceived information load, and their first- and second-order interactions. There was a positive second-order interaction (B = .04, Wald( 1) = 5.41, p = .02) between coupon face value and perceived information load, suggesting that the nonlinear effect of coupon face value was moderated by perceived information load.In a further spotlight analysis (Aiken and West 1991; Fitzsimons 2008), for participants who perceived low information load in product descriptions (1 SD below the mean), there was a positive, marginally significant linear effect (B = .54, Wald( 1) = 3.16, p = .08) and a negative, marginally significant quadratic effect (B = -.05, Wald( 1) = 3.24, p = .07) of coupon face value, suggesting that the effect of coupon face value was inverted U-shaped. Participants’ choice share of the high-priced menu was the highest when coupon face value was between $4 and $6. In contrast, for participants who perceived high information load (1 SD above the mean), coupon face value had a directionally positive yet nonsignificant effect on spending level (B = .02, Wald( 1) = .15, p = .70; for a graphical illustration, see Figure 4, Panel A).When the number of presented products was large. In line with our theorization, a large number of presented products also created higher information load (Msmall = 2.41, SD = 1.40 vs. Mlarge = 3.31, SD = 1.77; F( 1, 1,160) = 91.47, p < .001). Because the dependent variable in this condition was no longer a binary choice and instead had six levels, we treated it as continuous and ran an ordinary least squares regression. We found that coupon face value increased participants’ spending level (B = .12, t = 2.53, p = .01; for a graphical illustration, see Figure 4, Panel B) when they experienced higher information load as a result of a larger number of presented products.Taken together, the results support H2 and demonstrate that information load plays an important role in determining the effect of coupon face value on consumer spending, such that the effect takes an inverted U-shape only when consumers experience low information load from processing a small number of presented products. Yet when a larger number of presented products, with a moderate amount of information per product, imposes information overload on consumers, the effect becomes simply positive. Study 4: The Moderating Role of Tendency for ComparisonWe propose that the occurrence of the savings-comparison mechanism results in the inverted U-shaped effect. In Study 4, we further demonstrate the savings-comparison mechanism by examining the moderating role of consumers’ tendency to compare options in their decision process. Consumers who are more inclined to compare options thoroughly should be more likely to engage in savings percentage calculation and comparison, such that the savings-comparison mechanism will emerge at large coupon face values and result in an inverted Ushaped effect. In contrast, consumers who are less inclined to compare options would not bother to engage in savings calculation and comparison. Thus, for these consumers, the budgetincrease mechanism should always be dominant, resulting in an overall positive effect of coupon face value on consumer spending level (H3). We tested this hypothesis in Study 4.Study 4 also aims to exclude two alternative explanations. The face value of a product-line coupon could affect consumers’ inferences about product quality (Biswas et al. 2013; Chandran and Morwitz 2006; Darke and Chung 2005) and marketers’ persuasion attempts (Hardesty, Bearden, and Carlson 2007). To rule out these two alternative explanations, we directly measured these variables to control for their possible influences in Study 4. Design and ProcedureTwo hundred eighty-two undergraduate students (174 women; Mage = 20.03 years, SD = 1.42) from a large U.S. West Coast university participated in Study 4. This study adopted a 4 (coupon face value: $5, $10, $15, or $20) • 2 (tendency for thorough comparison: low vs. high) between-subjects design, with the second factor measured as an individual difference variable.We used Godiva chocolate boxes as our focal stimuli. Participants imagined that they received a coupon that could be used to buy either a Godiva spring chocolate gift box ($28.98, 16 pieces) or a Godiva signature chocolate truffle gift box ($38.98, 18 pieces) for their own consumption. Then, they indicated which gift box they would like to buy (for a description of product stimuli and a coupon example, see Figure WA3 in the Web Appendix). These two boxes differed in price, quantity, and flavor and thus represented two different Godiva products.After participants made their product choice, they further answered a set of ancillary questions regarding the chocolate boxes. First, we measured quality inference by asking participants to rate the two Godiva chocolate gift boxes on two bipolar scales anchored at “very poor quality” ( 1) versus “very good quality” ( 7) and “very poor condition” ( 1) versus “very good condition” ( 7), which formed a quality inference index (a = .85). Second, we assessed participants’ inference about the persuasion attempt associated with the coupon by asking them the extent to which the promotional offer seemed “to be a sales gimmick used to get consumers to buy,” “to have strings attached,” and to be “too good to be true” (1 = “not at all,” and 7 = “very much”), adapted from Hardesty, Bearden, and Carlson (2007). Given that the reliability of the persuasion knowledge scale was too low (a = .46) in our study, we conducted analyses on the three individual items separately.At the end of the survey, to measure participants’ tendency to compare options thoroughly in their decision process, we followed Parker and Schrift (2011) by combining the items from two subdimensions of the shortened maximization scale (Nenkov et al. 2008), including “When I am in the car listening to the radio, I often check other stations to see if something better is playing, even if I am relatively satisfied with what I’m listening to”; “When I watch TV, I channel surf, often scanning through the available options even while attempting to watch one program”; “No matter how satisfied I am with my job, it’s only right for me to be on the lookout for better opportunities”; “I often find it difficult to shop for a gift for a friend”; “When shopping, I have a hard time finding clothing that I really love”; and “Renting videos is really difficult. I’m always struggling to pick the best one” (1 = “strongly disagree,” and 7 = “strongly agree”). We averaged these six items to form a tendency for comparison index (a = .67), which represents both the breadth and the depth of option comparison in people’s decision process.Prior research has shown that a maximization mindset predicts the effort that people exert in their decision-making process across various domains (Iyengar, Wells, and Schwartz 2006; Levav, Reinholtz, and Lin 2012). In particular, Nenkov et al. (2008) show that the items used in our tendency forcomparison index strongly predicted the amount of information participants processed, the time participants took to make decisions, and the number of options participants considered in a decision task. Parker and Schrift (2011) further demonstrate that the tendency for comparison index represents a comparative thinking style. Taken together, the tendency for comparison index captures people’s chronic inclination to perform a thorough comparison among different options in their decision process.As a supplementary measure for tendency for comparison, participants’ decision time was also recorded in this study (i.e., how many seconds they spent before their last clicks on the web page on which they made their product choices). Because a higher tendency for comparison should lead to longer decision time (Nenkov et al. 2008), we expect that decision time will moderate the effect of coupon face value in a similar way tendency for comparison does. Results and DiscussionThe role of tendency for comparison. We entered tendency for comparison, coupon face value, its squared term, and its first- and second-order interactions with tendency for comparison into a logistic regression with product choice as the dependent variable (0 = “low-priced option,” and 1 = “highpriced option”). The second-order interaction between coupon face value and tendency for comparison was negative and significant (B = -.01, Wald( 1) = 4.14, p = .04), indicating that the nonlinear effect of coupon face value was moderated by tendency for comparison.We further conducted a spotlight analysis to understand this moderation (Aiken and West 1991; Fitzsimons 2008). When tendency for comparison was relatively high (1 SD above the mean), the linear term of coupon face value was positive (B = .49, Wald( 1) = 7.46, p = .006), and the squared term of coupon face value was negative (B = -.02, Wald( 1) = 7.80, p = .005). The highest choice probability of the high-priced option occurred when coupon face value was between $10 and $15, demonstrating an inverted U-shaped effect of coupon face value on consumer spending level. In contrast, when tendency for comparison was relatively low (1 SD below the mean), there was a simply positive effect of coupon face value on consumer spending level (B = .08, Wald( 1) = 5.77, p = .02), confirming H3 (for a graphical illustration, see Figure 5). Moreover, adding product quality or items of persuasion knowledge as a covariate in the logistic regression did not change the results reported previously regarding the effects of coupon face value and tendency for comparison, which suggests that inferences about product quality and persuasion attempt from different coupon face values could not explain our results.The role of decision time. As predicted, participants’ decision time as a supplementary measure for tendency for comparison was indeed correlated with their tendency for comparison (r = .19, p < .001). Further moderated logistic regression analyses show that although the second-order interaction between coupon face value and decision time did not reach significance (B = -.001, Wald( 1) = 1.77, p = .18), the effects of coupon face value at the high and low levels of decision time were similar to those at the high and low levels of tendency for comparison. For participants who spent more time making their product choices (1 SD above the mean), the effect of coupon face value was inverted U-shaped (linear term, B = .44, Wald( 1) = 5.74, p = .02; quadratic term, B = -.02, Wald ( 1) = 5.23, p = .02). In contrast, for those who spent less time making their product choices (1 SD below the mean), the effect of coupon face value was directionally positive (linear term, B = .04, Wald( 1) = 1.54, p = .21). Overall, the effects of decision time were consistent with those of tendency for comparison. A comparison of the results of the two moderated regressions also suggests that the tendency for comparison scale is a more sensitive measure than the decision time measure in revealing when the inverted U-shaped effect would emerge, given that for some participants, longer decision time might result from the difficulty in calculating the savings percentage rather than from a thorough product comparison.By demonstrating that consumers’ tendency to thoroughly compare options determines whether the effect of coupon face value on consumer spending level is inverted U-shaped or simply positive, Study 4 further supports the role of the savingscomparison mechanism in shaping the inverted U-shaped effect. Study 5: The Moderating Role of Preexisting Benefit-Level PreferenceStudy 5 examines consumers’ preexisting preference for a specific level of product benefit as a boundary condition for the inverted U-shaped effect (H4). To provide further evidence for the savings-comparison mechanism, we also measured participants’ consideration of savings percentages in the decisionmaking process. In our theorization, only at large coupon face values do consumers start to base their spending decision on savings percentages. At this stage, the savings-comparison mechanism occurs, resulting in the inverted U-shaped effect. According to such theorization, we expect that participants’ consideration of savings percentages mediates the negative effect (i.e., the “downward” side) of coupon face value on spending level at large coupon face values, but it does not mediate the positive effect (i.e., the “upward” side) of coupon face value on spending level at small coupon face values.Design and ProcedureStudy 5 used a 5 (coupon face value: $5, $15, $25, $35, or $45) • 2 (preexisting benefit-level preference: weak vs. strong) between-subjects design, with preexisting preference measured as an individual difference variable. Two hundred two U.S. residents (92 women; Mage = 34.94 years, SD = 10.77) from MTurk participated in exchange for monetary compensation. The procedure and stimuli of this study were the same as those of Study 2, except for the following three differences.First, in Study 5 participants made a binary choice between two mobile hard drives, with the more expensive mobile hard drive providing greater benefits (for a description of product stimuli and a coupon example, see Figure WA1 in the Web Appendix). Second, after participants made their product choice, they indicated the extent to which they thought about the percentage of the list price that they could save by using the coupon when making their choice (1 = “not at all,” and 7 = “very much”). Participants’ rating on this scale serves as a measure for their consideration of savings percentages. Third, at the end of the survey, participants rated the extent to which they had a clear preference for the capacity of a mobile hard drive (1 = “no preference,” and 7 = “a strong preference for 2,000GB over 1,000GB”). A higher score represents a stronger preexisting preference for a higher level of product benefit. Results and DiscussionThe role of preexisting preference. We entered preexisting benefit-level preference, coupon face value, the squared term of coupon face value, and the first- and second-order interactions between coupon face value and preexisting preference into a logistic regression with product choice as the dependent variable (0 = “low-priced option,” and 1 = “highpriced option”). We obtained a positive second-order interaction between coupon face value and preexisting preference (B = .01, Wald( 1) = 11.47, p < .001), indicating that the quadratic effect of coupon face value was moderated by preexisting preference.We further conducted a spotlight analysis (Aiken and West 1991; Fitzsimons 2008). When preexisting preference for the 2,000 GB capacity was relatively weaker (1 SD below the mean), the positive linear term (B = 1.30, Wald( 1) = 13.71, p < .001) and the negative squared term (B = -.02, Wald( 1) = 13.47, p < .001) of coupon face value confirmed the inverted U-shaped relationship between coupon face value and consumer spending. Participants’ spending level was the highest when coupon face value was between $25 and $35. When preexisting preference for the 2,000 GB capacity was relatively stronger (1 SD above the mean), the effect of coupon face value became nonsignificant because the choice share of the 2,000 GB hard drive was above 88% regardless of coupon face value (B = -.01, Wald( 1) = .20, p = .65; for a graphical illustration, see Figure 6, Panel A). These results establish preexisting preference for a specific level of product benefit as a boundary condition for the inverted U-shaped effect (H4).The role of savings percentage consideration. In this study, we also found that coupon face value had an inverted Ushaped effect on consumer spending level in the overall sample even when the moderating role of preexisting benefit-level preference was not modeled. Thus, we further tested the mediating role of savings percentage consideration in the overall sample.Consistent with our theorizing, a regression analysis shows that participants’ consideration of savings percentages increased as coupon face value increased, indicating that savings percentages indeed became a more important decision input for participants as the percentages increased (B = .03, t = 3.38, p = .001). Given that the effect of coupon face value on spending level was initially positive but started to turn negative when coupon face value was between $25 and $35, we further examined the differential effects of savings percentage consideration in two data ranges, from $5 to $25 and from $35 to $45, respectively.We conducted mediation analyses using a bootstrapping approach (Hayes 2013). When coupon face value was from $5 to $25, neither the direct effect of savings percentage consideration on consumer spending (B = -.08, Wald( 1) = .80, p = .37) nor the mediating effect of savings percentage consideration between coupon face value and consumer spending (95% confidence interval = [-.011, .002]) was significant. In contrast, at larger coupon face values from $35 to $45, both the direct effect of savings percentage consideration on consumer spending (B = -.31, Wald( 1) = 4.27, p = .04) and the mediating effect of savings percentage consideration between coupon face value and consumer spending (95% confidence interval = [-.029, -.001]) were significant. These results further support the activation of the savings-comparison mechanism at large coupon face values that underlies the inverted U-shaped effect.In summary, Study 5 identifies preexisting preference for a specific level of product benefit as a boundary condition for the inverted U-shaped effect. This study also provides convergent evidence for the proposed savings-comparison mechanism by demonstrating the mediating role of savings percentage consideration at large coupon face values but not at small coupon face values. Extension Studies for Study 5We further conducted two extension studies that assessed the moderating role of consumers’ preexisting benefit-level preference. We elaborate on these next.Three-option choice set. In the first extension study (U.S. online sample, N = 403), we generalized the findings of Study 5 from a two-option choice set (i.e., low price vs. high price) to a three-option choice set (i.e., low price, midprice, or high price; for a description of product stimuli, see Figure WA4 in the Web Appendix). Replicating Study 5, a multinominal logistic regression shows that both ( 1) the relative choice share of the high-priced hard drive over the midpriced hard drive and ( 2) the relative choice share of the high-priced hard drive over the lowpriced hard drive had an inverted U-shaped relationship with coupon face value only when consumers’ preexisting preference for a larger hard drive capacity was relatively weaker (for detailed results, see the Web Appendix).Brand liking. In the second extension study (U.S. undergraduate sample, N = 161), we used brand liking to operationalize preexisting preference for a specific level of product benefit. Consumers with a strong brand liking may also have a strong preexisting preference for the product that provides greater benefits and thus is more expensive, regardless of the face value of the product-line coupon they receive, given that a strong brand liking reduces consumers’ price sensitivity (Ailawadi, Lehmann, and Neslin 2003). As a result, for these consumers, the inverted U-shaped effect of coupon face value on consumer spending on this brand should disappear as well. The second extension study using the Godiva stimuli from Study 4 (for a description of product stimuli, see Figure WA3 in the Web Appendix) supports the moderating role of brand liking. Specifically, the inverted U-shaped effect was revealed only when consumers had a relatively weaker liking for Godiva (for a graphical illustration, see Figure 6, Panel B; for detailed results, see the Web Appendix). General DiscussionThe present research explores conditions in which the face value of a product-line coupon has an inverted U-shaped effect on consumer spending level. Our findings are robust for both hedonic products (e.g., food and snack) and utilitarian products (e.g., mobile hard drive) and across well-known brands (e.g., Godiva chocolate), less famous brands (e.g., Touro mobile hard drive), and fictional brands (e.g., Korean barbecue buffet). We provide evidence for the savings-comparison mechanism by showing that the inverted U-shaped effect is contingent on the price level of products (Study 1), consumers’ saving orientation (Study 2), information load (Study 3), and consumers’ tendency for thorough comparison (Study 4), and this effect is driven by consumers’ savings percentage consideration (Study 5). We also identify a preexisting preference for a specific level of product benefit as a boundary condition for the inverted U-shaped effect (Study 5). Theoretical ContributionsOur research contributes to the literature on price-based sales promotions. Whereas most existing research in marketing has focused on a price discount or coupon that is restricted to a specific product with a fixed price (e.g., Alba et al. 1999; Chandran and Morwitz 2006; Chen, Monroe, and Lou 1998; Chen et al. 2012; Chen and Rao 2007; Lee and Tsai 2014; Leone and Srinivasan 1996; Mishra and Mishra 2011; Nunes and Park 2003; Raghubir 1998; Shiv, Carmon, and Ariely 2005; Thomas and Morwitz 2009), our research examines a popular marketing practice in which a coupon can be used for different products within the same product line of a brand. When a product-line coupon is redeemed, the spending level associated with a specific product choice becomes a very crucial issue—one that, to the best of our knowledge, has not been studied in the literature. Our research makes an important first attempt to examine how coupon face value influences consumer spending level in this context.More importantly, contrary to what the conventional wisdom would predict, we show that an increase in coupon face value does not always lead to an increase in consumer spending level. Instead, for product-line coupons, the effect of face value on consumer spending level could be inverted U-shaped under some circumstances. There are two streams of research on sales promotions that lead to opposite predictions on the effects of coupon face value on consumer spending level. Although the budget-increase perspective suggests a positive effect, the savings-comparison perspective predicts the opposite. Our research reconciles these two opposing predictions by proposing a threshold-based account and by showing that the magnitude of coupon face value could determine the effect of coupon face value on consumer spending level. Managerial ImplicationsIt is quite common for retailers to offer deep discounts, especially during major holidays. Prior research has suggested that deep discounts can be detrimental when they dilute brand images (Dodson, Tybout, and Sternthal 1978) or lower consumers’ future price expectations (DelVecchio, Krishnan, and Smith 2007), which may further negatively affect future product sales in the long run. Our research suggests that the negative effects of deep discounts can take place more immediately. When a deep discount is offered in the format of an amount-off coupon that can be used to buy different products within the same product line of a brand, it may hurt sales because a larger coupon face value may motivate consumers to choose less expensive options under certain circumstances.Fortunately, our findings offer a contingency approach for effectively managing the face value of a product-line coupon to avoid this negative consequence. Drawing on our findings, marketers can determine when they should offer product-line coupons with either a large or a moderate face value, depending on whether the relationship between coupon face value and consumer spending is simply positive or inverted U-shaped. Specifically, marketers can use a large coupon face value to encourage consumer spending when the large coupon face value eventually leads to a high spending level. We identify the positive influence of coupon face value on consumer spending when products are less expensive, when product-line coupons can be applied to a large number of products, or when a firm is able to deliver customized product-line coupons to a target group of consumers who are less inclined to engage in thorough product comparison (e.g., when the firm is able to identify these consumers by tracking their product browsing history). In these cases, marketers could be confident in the power of a large coupon face value to increase consumers’ spending level.In contrast, marketers might consider utilizing a moderate coupon face value when the effect of coupon face value on consumer spending takes an inverted U-shape because a large coupon face value may backfire in terms of inducing consumers to spend less. Our findings suggest that a firm should offer product-line coupons with a moderate face value when the firm’s products are expensive or when product-line coupons can be applied to only a small set of products, because in these cases the relationship between coupon face value and consumer spending is inverted U-shaped. Moreover, we advise marketers to choose a moderate coupon face value if they are able to identify consumers who care more about saving money, who are motivated to conduct thorough product comparison, who do not have a strong preexisting preference for a specific level of product benefit, or who do not have a strong liking for the focal brand. These consumers’ spending amount can be maximized when the face value of a product-line coupon is at a moderate level.Although the present set of studies focuses on the research context in which a coupon can be applied to different products sold at different prices within the same product line of a brand, we expect that the findings derived from this context would still hold when a retail store provides consumers with a coupon that can be used for vertically differentiated products offered by different brands within the same product category. This is because, in the latter context, consumers need to make a similar trade-off between price and benefit for different products, just as participants in our experiments did. Our findings are also applicable to scenarios in which the same coupon can be linked to different combinations of products that are consumed together, such as food and drink items during a restaurant visit. Given that these interrelated items can be regarded as components of an integrated product package, our theoretical framework can also predict the relationship between consumers’ total spending on these items and coupon face value. Limitations and Future Research DirectionsThe underresearched context of unrestricted product-line coupons provides several interesting avenues for further exploration. First, our studies examine a scenario in which face values of a product-line coupon are below the lower bound of consumers’ potential spending level. What would happen when coupon face values fall between the lower and upper bounds of the retail price range of products to which a coupon can be applied is a subject for future research. Second, future studies could build more sophisticated models to more accurately specify the range of the turning point of the inverted U-shaped effect. Finally, in the present research the operationalizations of some moderators may have limitations. For example, we operationalized information overload by increasing the number of presented products in Study 3. Yet a large number of products could activate other constructs, such as consumers’ expected choice regret (Iyengar and Lepper 2000). Future studies could address such limitations to provide more evidence for the savings-comparison mechanism that underlies the inverted Ushaped effect.a“ES” indicates two extension studies for Study 5; “NI” indicates one study that is not included. Details are available on request. bMeans and choice shares are estimated from regression coefficients. “Low” and “high” represent the lower and upper bounds of coupon values; “medium” represents the coupon value at which the inverted U-shaped curve reaches its peak point. cHigh information load due to a relatively larger number of products presented.  "
27,"Driving Brand Engagement Through Online Social Influencers: An Empirical Investigation of Sponsored Blogging Campaigns Influencer marketing is prevalent in firm strategies, yet little is known about the factors that drive success of online brand engagement at different stages of the consumer purchase funnel. The findings suggest that sponsored blogging affects online engagement (e.g., posting comments, liking a brand) differently depending on blogger characteristics and blog post content, which are further moderated by social media platform type and campaign advertising intent. When a sponsored post occurs on a blog, high blogger expertise is more effective when the advertising intent is to raise awareness versus increase trial. However, source expertise fails to drive engagement when the sponsored post occurs on Facebook. When a sponsored post occurs on Facebook, posts high in hedonic content are more effective when the advertising intent is to increase trial versus raise awareness. The effectiveness of campaign incentives depends on the platform type, such that they can increase (decrease) engagement on blogs (Facebook). The empirical evidence for these findings comes from real in-market customer response data and is supplemented with data from an experiment. Taken together, the findings highlight the critical interplay of platform type, campaign intent, source, campaign incentives, and content factors in driving engagement.KEYWORDS_SPLITConsumers are increasingly relying on peer-to-peer communications; for this reason, influencer marketing has continued to grow in importance as a key component of firms' digital marketing strategies ([ 3]). Nearly 75% of marketers today are using influencers to spread word of mouth (WOM) about their products and brands on social media. Influencer marketing is often considered critical to strengthening online brand engagement ([67]). Consequently, 65% of multinational brands have indicated plans to increase spending on influencer marketing, with spending expected to reach $10 billion by 2020 ([ 7]; [65]). However, despite the explosion of these social influencers, their effectiveness is still low; for an influencer on Facebook, the average engagement rate per post is.37%; on Twitter, it is even lower at.05% ([77]).A large and important category of influencer marketing is sponsored blogging, in which companies solicit bloggers to post about specific products and brands (i.e., ""sponsored posts"") ([57]). Bloggers can help generate WOM about a brand, product, or service directly through the content of their sponsored posts. Firms have deployed sponsored blogging both successfully (i.e., Nokia's camera phone campaign in Finland) and unsuccessfully (i.e., Dr Pepper's ""Raging Cow"" campaign) ([17]). However, the field needs to develop a better understanding of what drives the success of influencer marketing as a whole and sponsored blogging in particular. Given the significant marketing expenditures dedicated to this strategy and the paucity of knowledge on success drivers, this is an important research gap worth addressing.Sponsored blogging is a hybrid approach combining aspects of paid and earned media (e.g., [15]; [59]). We distinguish this phenomenon from a purely paid media strategy because influencers engage in WOM and have control over the ultimate message of the advertisement. As companies reimburse bloggers (with either cash or free goods) to generate posts on social media, influencer marketing is distinct from organically generated WOM. Because influencer marketing blends elements of paid and earned media, we can distinguish this from prior research focusing on paid and owned media (e.g., [20]; [59]) or earned media, including online WOM (e.g., [38]). We also extend the traditional advertising literature on the impact of source credibility and message content ([32]).We provide a comprehensive framework that examines the drivers of sponsored blogging strategies, including blogger characteristics, content characteristics, and campaign incentives and, in doing so, contribute to the literature in three ways. First, this study advances prior research by examining how social influencers (or sponsored bloggers) can influence consumers at different stages of the consumer purchase funnel by examining different campaign intents (e.g., awareness vs. trial). Second, this research sheds light on the important role of campaign intent as a moderator of the impact of blogger (i.e., expertise) and content (i.e., hedonic value) characteristics on social media engagement. Third, we suggest that the type of social media platform (blogs vs. Facebook) can moderate the impact of these factors on engagement.Our theoretical basis for predictions derives from the literature on the elaboration likelihood model (ELM) ([75]). We focus on two moderators that indirectly affect consumers' ability and motivation to engage in effortful processing. The first involves social media platforms (blogs vs. Facebook), which vary in their level of distraction and involvement, implying differences in consumers' ability and opportunity to engage in effortful processing. The second is the stage of the consumer decision journey (CDJ) (awareness vs. trial), which may imply increasing levels of motivation closer to trial. Early in the CDJ, consumers process through the peripheral route, whereas later they process through the central route ([15]). Taken together, we argue that both the platform and the stage of CDJ act as key moderators.Our findings show that in a blog context, blogger expertise, campaign intent, hedonic value of post, and campaign giveaways are key drivers of engagement. In addition, blogger expertise exerts a greater impact in awareness (vs. trial) campaigns. On Facebook, hedonic value exerts a positive impact, and trial campaigns benefit more from the use of hedonic content. Campaign giveaways exert a negative impact, highlighting the potential cannibalizing role of one platform on another (blog vs. Facebook). Taken together, the findings shed light on various factors that govern how influencer campaigns elicit consumer engagement across multiple platforms. Panel A of Figure 1 presents our conceptual framework on the blogging platform, and Panel B presents the same for the Facebook platform.Graph: Figure 1. Conceptual framework of factors influencing sponsored blogging campaign effectiveness.This study advances prior research by examining how social influencers can affect consumers at different stages of the consumer purchase funnel. This research suggests that the type of social media platform moderates the impact of social influencer and post characteristics. We develop a framework of strategies based on the social media platform in use and the firm's campaign intent to inform practitioners about the type of content and influencer to use under each condition. The findings have implications for practitioners who want to employ influencers and show that the choice of bloggers should be guided by campaign intent.This research uses real in-market customer response data, assembles a large data set of sponsored blogging campaigns, measures various characteristics, and links these to concrete brand engagement outcomes. Thus, our field data provide a unique vantage point and draw a richer picture of not only what constitutes an effective influencer marketing campaign but also how this varies across social media platforms. We supplement the findings by collecting data in a lab study. Theoretical Background and HypothesesResearch on influencer marketing examines elements of sponsored advertising, product type, source characteristics, and sponsorship disclosure. The findings include differentiating impacts of expertise, product involvement ([95]), customer involvement ([28]), sponsorship disclosure ([86]), and two-sided messages ([84]). Recent research on influencers indicates that information seekers' objectives and issue involvement drive a blog's influence ([ 5]). [58] demonstrate the importance of message content, source credibility, and homophily in influencer marketing. Table 1 provides a review of research on the key variables of influencer marketing.GraphTable 1. Previous Research Related to Sponsored Blogging Key Variables.  Related research also examines native advertising, or the phenomenon of sponsored social media posts or news articles disguised to resemble nonsponsored content. [89] examine native ads and unveil how their effectiveness changes across serial positions by analyzing a large-scale data set. [92] examine how source credibility plays a critical role in perceptions of native advertising. Other research has also examined the phenomenon of social dollars, or the effects of online social connections on users' product purchases in an online community. [74] demonstrate that social dollars vary depending on type of product (hedonic vs. functional), user experience, and network density. Together, these findings shed light on the important role of influencers, particularly those embedded in social networks, on consumer choices and purchase behavior. EngagementOur key dependent variable for the primary field study is social media engagement. We follow [40], p. 555) and define engagement as a ""customer's cognitive, emotional, and behavioral activities."" More specifically, our focus is on indirect customer engagement, which includes incentivized referrals, social media conversations about products/brands, and customer feedback to companies ([73]). These types of actions contribute to a firm's revenue, as referred customers are typically more profitable than those not referred ([72]; [85]). This impact of engagement on profitability has also received empirical verification across business-to-business ([55]) and business-to-consumer ([56]) contexts, and its benefits can derive from both cost reduction and revenue enhancement ([33]).Consumer engagement literature highlights several potential factors that may influence consumer engagement, including emotionality, direct firm actions, and product involvement ([33]; [73]). We derive our key factors from this literature and add new factors, such as overall campaign intent, influencer characteristics (i.e., source expertise and post content), and level of involvement elicited by the social media platform. The customer engagement activity we focus on is social media interactions with sponsored influencer content, and we operationalize this as likes and comments on sponsored posts. Influencer Marketing on the Blogging PlatformFirms often launch influencer marketing campaigns on multiple platforms simultaneously. The blog platform constitutes the primary environment for sponsored bloggers to exert their influence. People who choose to interact with bloggers and their postings are typically followers of the blogger. Followers have opted to obtain information posted by bloggers and therefore are likely highly involved in the environment. This high involvement translates into several facets of blog campaigns that help strengthen engagement. Platform differencesWhile there are various differences across social media platforms, a key difference is the rationale or motivation for consumers to engage with platforms. Some consumers seek out platforms (e.g., blogs) for their content, which implies a higher level of motivation to engage in effortful processing of content. Others may primarily use platforms (e.g., Facebook) to connect with others, implying a focus on relationship maintenance ([47]). Another key difference is the level of distraction prevalent on a platform. Platforms such as Facebook are relatively less involving and more distracting for each individual post because of the large amount of information and content provided ([93]). In our research, Facebook is a relevant platform (in addition to blogs) because many bloggers post links to their blog posts on Facebook.To provide a priori evidence of the differences in social media platforms, we pretested blogs and Facebook (the two relevant platforms in this study) using a survey of participants (N = 264, Mage = 35.2 years, 50.0% male) on Amazon Mechanical Turk. Participants were randomly assigned to one of two platform conditions: blog or Facebook. Recalling their last time on the platform, they reported how distracted they felt and the degree to which they were seeking specific content on the platform on scales from 0 to 100. Controlling for age and gender in both regression models, we found that distraction was higher on Facebook than blogs (F( 3, 260) = 7.22, p <.01; Mblog = 32.65, MFacebook = 42.83; bplatform = –10.67, p <.01), and specific content seeking was higher on blogs than Facebook (F( 3, 260) = 3.61, p <.01; Mblog = 59.19, MFacebook = 48.14; bplatform = 11.25, p <.01). These results lend support to our argument that platform distraction and content search differ between blogs and Facebook, with distraction being lower and content seeking being more common on blogs. Therefore, Facebook should result in low-involvement processing of information.Given the low-involvement nature of the Facebook platform, consistent with the ELM ([75]), there should be a greater emphasis on peripheral cues (e.g., number of followers, hedonic content, timing and number of posts). Conversely, in line with ELM predictions, in high-involvement platforms such as blogs, argument quality should exert a greater impact on persuasion; this implies that source expertise and post content should play important roles in eliciting engagement on blog platforms. We articulate these differences in our separate predictions we develop for blogs and Facebook platforms. Campaign intent on the blogging platformBroadly, influencer marketing campaigns have two goals: ( 1) to increase awareness and ( 2) to encourage trial. From a marketer's perspective, awareness campaigns are an easier-to-achieve goal and do not require any overt action on the part of consumers. Trial campaigns, which encourage consumers to make a purchase, are typically linked to consumer actions (e.g., purchase, app download) and therefore have a more overt persuasion intent and also a higher hurdle to generate customer engagement. These advertising goals (awareness vs. trial) can also affect the activation of persuasion knowledge of consumers, depending on whether there is a more direct advertising motive, as in the case of a trial campaign, or a less direct advertising motive, as in the case of an awareness campaign.These campaign intents align with the beginning and end of the consumer's decision journey, which typically involves multiple stages in a hierarchy of effects, such as awareness, knowledge, liking, preference, conviction, and purchase. Prior research has examined this dichotomy of awareness versus trial intent in a traditional advertising context (e.g., [66]). As noted previously, the processing route to persuasion differs depending on the stage of the CDJ, with early stages being processed through the peripheral route and later stages being processed through the central route ([15]). We propose that campaign intent is a potential moderator that can influence engagement differently depending on the stage of the CDJ. We predict that trial intent versus awareness will have a greater impact on the blogging platform. Main effect of blogger expertiseSource expertise refers to the level of credibility a source possesses. Expertise reflects the extent to which a consumer is qualified to discuss a subject ([ 2]), such as source qualifications ([10]), competence, knowledge, education, expertise, and ability to share knowledge ([39]). This can derive from informational power, in which the expert has knowledge that others do not have ([22]). Expert power can be knowledge within a specific domain (e.g., law) ([27]). Endorsers are more likely to be considered experts if they are competent and have relevant knowledge ([42]).Source expertise affects attitude change ([43]; [64]; [70]), level of confidence and positivity ([82]), and behavioral changes ([18]) and leads to higher levels of persuasion ([76]). Higher levels of persuasion are a result of high source expertise leading to a deeper processing of the advertising message ([42]). In an influencer marketing context, expertise increases behavioral intention toward products ([84]). In a sponsored blogging context, consumers will prefer products endorsed or referred by a blogger with expertise because they perceive the message as more persuasive and credible ([48]; [95]). Thus, H1:  Blogger expertise has a positive impact on blog engagement. The higher the blogger's expertise, the higher is the number of blog post comments. Interaction effect of campaign intent on blogger expertiseBoth high- and low-expertise bloggers may be considered influential, under varying circumstances. Despite the expected positive impact of blogger expertise on engagement in a sponsored blogging context, source expertise can also have a neutral (or even negative) effect in some situations. Prior research suggests that in the presence of an extreme advertising claim, the positive impact of source expertise diminishes ([31]). Depending on the context, type of claim, and stage in the decision-making process, source expertise may even have a nonsignificant (or negative impact) on engagement. The nonsignificant impact of source expertise also stems from the countervailing positive impact of low expertise bloggers (novice endorsers). Novice endorsements can be as effective as those from experts ([88]). Therefore, we expect blogger expertise to vary in its impact depending on campaign intent.Involvement affects blogger success, such that for low-involvement products, a blogger with low expertise can have greater success ([95]). Under different stages of the CDJ, we similarly anticipate blogger expertise to affect engagement differently depending on the level of involvement. The peripheral processing that occurs early in the CDJ gives more attention to peripheral cues, such as expertise. In the early stages of the CDJ, expertise becomes a more important influencer for persuasion than homophily ([90]). Regarding an awareness intent, early in the CDJ we expect high expertise to be beneficial. For trial campaigns that correspond to later in the CDJ process and lead to higher motivation for central processing, we predict the opposite effect. Regarding a behavioral versus attitudinal change, low-expertise (vs. high-expertise) sources can be more effective ([23]). In line with this reasoning, audiences may perceive a source with higher expertise as less similar to them (i.e., less homophilous). For a campaign with a trial intent, we expect low-expertise (vs. high-expertise) bloggers to be more effective. In turn, this pattern of effects will result in a differential impact of higher blogger expertise depending on campaign intent. Thus, H2:  There is an interaction effect between campaign intent and blogger expertise. Specifically, (a) when blogger expertise is high, awareness campaigns are more effective in generating brand engagement; (b) when blogger expertise is low, trial campaigns are more effective at generating brand engagement. Main effect of hedonic value of postThe hedonic value of a post refers to the enjoyment, emotions, and entertainment a consumer experiences from reading the post. Evidence suggests that hedonic content can have an impact on attitudes and WOM ([ 9]; [49]). In a traditional advertising context, researchers have shown that hedonic value captures attention ([81]) and influences attitude toward an ad ([49]). [ 8] suggest that specific emotions (e.g., awe, anxiety) trigger arousal, which in turn results in greater virality of online content. [71] extend these findings and argue that consumers share expressive or assertive brand messages more frequently than directive brand messages. Relatedly, [37] indicate that hedonic content can be a key factor in the virality potential of online firestorms. Building on these findings, we expect a post featuring high hedonic value content to increase arousal, deepening customer engagement. Therefore, we posit a general positive impact of hedonic content on the blogging platform. H3:  Post content, in terms of hedonic value, is positively related to engagement in blog post comments. Campaign incentivesCampaign incentives are marketing actions designed to elicit specific responses and engagement from consumers. The purpose of a campaign incentive is ""to give followers a free item (or a chance to win a free item) in exchange for them sharing, liking, following, and/or reposting a picture"" ([68]). For example, Rafflecopter is a giveaway platform widely used by sponsored bloggers, and the requirements to enter each giveaway are at bloggers' discretion. For some campaign giveaways, bloggers require consumers to comment on the blog post itself, while others require a different action (e.g., become a Twitter follower, share the giveaway) to enter the giveaway.Campaign incentives are a direct firm action to increase customer engagement ([87]). Other benefits of giveaways include an increased desire to buy more, higher-quality perceptions of the product, and increased WOM about the product ([91]). In [ 9] study, consumers who received a free product talked about it 20% more than those who did not receive the product for free. Therefore, we expect the presence of incentives to increase blog engagement because they elicit consumer comments for a chance to win the giveaway. H4:  Campaign incentives are positively related to engagement, such that inclusion of a campaign incentive leads to more blog post comments. Influencer Marketing on the Facebook PlatformAs mentioned, influencer marketing campaigns co-occur across platforms. The Facebook platform is a secondary environment for sponsored bloggers to link to the posts on their respective blog pages. In contrast with the blogging platform, people who encounter the Facebook post may or may not follow the blogger's blogging page. In other words, followers could come across the Facebook posts because they follow the person or because a friend has shared or interacted with the post. The Facebook platform is one example of a low-involvement, high-distraction social media platform. This lower involvement primes a different set of important features while diminishing the importance of others. Campaign intent on the Facebook platformAs noted previously, we expect awareness and trial campaign intent to align with the CDJ. Engagement generated by sponsored posts may vary depending on campaign intent. In trial campaigns, we expect Facebook participants' willingness to engage in campaigns with overtly commercial intent to be low. [47] propose that the primary motivations for Facebook usage are to gain knowledge, acquire new connections, and strengthen existing relationships. An overtly commercial intent, as in the case of trial, can interfere with the intended usage of the platform and therefore be met with resentment by users. Because gaining knowledge and encountering ideas and information are reasons to use Facebook, these are more in line with the awareness intent. The awareness intent is more of a helping motive associated with WOM communications in the network. Facebook users may spread WOM about awareness campaigns because doing so generates positive feelings and strengthens social connections ([36]). Thus, H5:  Campaign intent has a positive impact on engagement on the Facebook platform. Specifically, awareness (vs. trial) campaigns generate more Facebook engagement (i.e., likes). Hedonic value on the Facebook platformEvidence shows that engagement on Facebook is positively related to hedonic content ([13]). The primary rationale for this is that the hedonic value generates an emotional response ([24]), which leads to higher arousal and a greater propensity to like and share in online settings ([ 8]; [26]). Research based on the ELM ([75]) indicates that when consumers are less involved with products, they use peripheral routes to process information ([28]). On a low-involvement platform, we expect hedonic value to be more salient to the reader. Under low involvement, [14] finds that attention-getting online advertising appeals were more effective. Consequently, we predict that hedonic content associated with blog posts is highly relevant to low-involvement platforms such as Facebook, as it helps overcome low involvement by raising the interestingness of a post. In support of this idea, the in-store shopping literature (e.g., [ 4]) shows that the hedonic value of a shopping experience plays a key role in elevating involvement and inducing purchase behavior. For a low-involvement, high-distraction platform such as Facebook, peripheral cues, such as hedonic value, should be important. Thus, H6:  The hedonic value of blog posts has a positive impact on Facebook engagement (i.e., likes). Interaction effect of campaign intent and hedonic value on Facebook platformIn addition to the preceding main effect predictions, we anticipate an interaction effect of campaign intent and hedonic value on Facebook engagement. Hedonic content leads to a greater likelihood of message forwarding ([13]) and increased private sharing of news articles ([ 8]), but these links are dependent on the context of sharing and type of outcome being studied ([83]). Ads viewed as too ""outrageous"" may result in lower purchase intent and persuasion, but this is not the case when the ad also leads to consumer responses such as comments ([83]).As the Facebook platform is a low-involvement, high-distraction environment, in which people are predisposed toward information overload ([52]), opportunity and motivation to process become key components in information processing ([61]). A sufficiency threshold dictates that the amount of processing a person is willing to undertake is dependent on the perceived risk involved ([12]; [63]). Under a trial intent, in which the perceived risk would be higher than in an awareness context, we anticipate a high sufficiency threshold and, thus, central processing.In addition, a consumer's motivation to process an ad will be higher when there are more hedonic cues ([61]). [11] posit that when people identify a Facebook post as an advertisement, they develop feelings of distrust, which may imply a threat to the relationship. In this case, a highly hedonic post can alleviate this threat. Heuristic processing is likely to occur when the person views the information as agreeing with his or her beliefs ([30]). Taken together, this would imply that for a trial intent, a post high in hedonic value could overcome the disposition to process more systematically. This suggests that when campaigns involve purchasing (e.g., trial), the hedonic value of the post will be valuable. Thus, H7:  High (vs. low) hedonic value has a more positive impact on trial campaigns than awareness campaigns. Study 1 DataThe data come from The Motherhood, a leading agency for sponsored blogging campaigns that focuses on ""mommy"" bloggers. The data consist of 1,830 sponsored posts written by 595 bloggers,[ 7] collected from September 2012 to December 2016.[ 8] These blog posts came from 57 different campaigns, including Awesome Avocados, Banner Alzheimer, Chef Boyardee Little Chef, Latte Love, and Barnes & Noble.For each campaign, companies work with the blogging agency to coordinate campaign details, such as the intended message, target audience, and goals. Bloggers are recommended for the campaign on the basis of their demographics, age of children, and expertise, and they can choose to work on projects in line with their interests, availability, and willingness to work within the set budget. Bloggers are required to disclose that they are sponsored bloggers either at the beginning or at the end of the blog posts. Depending on the budget and requirements of a given campaign, each blogger receives compensation (in the form of either money or free products). Bloggers then write and post content on their own blog websites about the campaign; bloggers get paid more if they post something on multiple social media channels. Dependent Variables Number of blog post commentsEvery blog post had an option for blog readers to leave comments. Comments of other readers are visible to any subsequent reader of the blog, but other readers do not receive notification when a new comment has been posted. Our primary measure of engagement is the number of comments each blog post received (see Table 2 for constructs, Table 3 for descriptive statistics of the variables, and Table 4 for the correlations).GraphTable 2. Constructs and Measures.  GraphTable 3. Descriptive Statistics.  1 aPercentage of occurrences.GraphTable 4. Variable Correlations.  2 *p <.05.3 Notes: The unit of analysis is the blog post. Number of Facebook post likesBloggers frequently used their social media outlets to post about different blog campaigns. Facebook followers of a blogger can see the new post in their Facebook news feed, while others need to seek out the post directly from the blogger's Facebook profile. To measure Facebook engagement, we counted the number of Facebook post likes. Independent Variables Campaign intentCompanies typically divide campaigns into two categories: those designed to raise awareness and those designed to increase trial. In our data set, of the 57 total campaigns, 29 had an awareness intent and 28 had a trial intent. The awareness campaigns focused on increasing brand recognition. For example, an AT&T Mobile School Safety campaign encouraged people to talk to their children about using mobile phones safely. These campaigns were not directly trying to motivate people to make a purchase but instead were focused on building brand awareness. By contrast, the trial campaigns focused on increasing consumer trial for products or services. Examples of this type of campaign included Church Hill Classics' diploma frames and Veritas Genetics' at-home BRCA test. Campaign incentivesWe identified campaigns (29%) in terms of whether they included a campaign incentive (i.e., a giveaway). Giveaways typically request that readers like or share a blog post to be entered for a chance to win a prize. For example, Johnson & Johnson's Donate-a-Photo campaign had a giveaway prize of $100 worth of products. Blogger average number of followersThe average number of Twitter and Facebook followers represents a blogger's social media presence and is also an indication of blogger strength. We used bloggers' Twitter and Facebook followers for two reasons: ( 1) Facebook and Twitter are two of the largest social media platforms, and ( 2) the number of followers on the blog web pages themselves are unavailable. We use the natural log of the average number of Twitter and Facebook followers in the models to account for the large spread, and we mean-centered them improve interpretation of coefficients. We also use alternative operationalizations and reestimate the main model using these measures (for the results, see the Web Appendix). Blogger psychographic profile factorsFirst, we pulled the public profiles of each blogger in our data set, as described on their blog pages. Second, three coders (blind to the hypotheses) examined the bloggers' public profiles and listed key themes that captured their psychographic profiles (i.e., interests, activities, and opinions; see the Web Appendix). This procedure revealed 14 main psychographic profile dimensions, dummy coded as 1 if present in the profile and 0 if not. Third, we used factor analysis with varimax rotation[ 9] to identify five overarching characteristics of bloggers.[10] Blogger expertiseWe measure blogger expertise by the presence of the person's educational affiliation and blogger credentials in his or her profile. Prior research has also used blogger profiles to manipulate source ([84]). An educational affiliation includes reference to a specific higher education degree (e.g., ""Bachelor of Arts""), while blogger credentials refer specifically to status as a credible blogger (e.g., ""social media consultant,"" ""Nielsen 50 Power Mom""). Blogger expertise, which ranges from 0 to 2, is the sum of the two measures. Post Variables Weekend postWeekend post is an indicator variable for whether the post occurred on a weekend, coded as 1, or a weekday, coded as 0. We used this to capture weekend versus weekday seasonality. We incorporate this as a control variable for the temporal element. Number of Facebook postsThe number of Facebook posts serves as a control variable. For sponsored blogging campaigns, bloggers will post on blogs and then post on Facebook linking to the blog post. To control for the number of Facebook posts, we include this as a variable in the model. Hedonic and informational value associated with the blog postThree coders classified the hedonic and informational value associated with a given blog post; we prequalified the coders to match the demographics of the bloggers' audiences. We based our measures on [94], who develop a 20-item emotion scale for advertisements. We used coders from Amazon Mechanical Turk to measure various aspects of sentiment on a seven-point scale (1 = ""not at all,"" and 7 = ""extremely""), including how much the blog posting was attention getting as well as how boring, creative, emotional, energetic, genuine/sincere, honest, humorous, informative, irritating, memorable, pleasant, strong, unique, warmhearted, relatable, understandable, believable, and relevant the post was. We selected coders who were as similar to the blog audience as possible (i.e., they were also mothers with a child under 18 years in the household). We solicited three coders for each blog post and asked them to code only a subset of blog posts (typically three posts each), suggesting that there are variations introduced across different blog posts from the varying identities of coders.First, to measure the agreement between coders and calculate a more accurate alpha score, we used the methodology [79] describe and computed the Shrout–Fleiss single intraclass correlation score agreement of.998, which is considered quite high ([51]). As a second approach to assess reliability, we estimated a standardized alpha within three coders for each sentiment value for each blog post, to account for the different coders on each post. We then averaged these standardized alphas and obtained an average reliability of.51 and a median reliability of.56.Each blog post was coded for a variety of sentiment variables, some of which may be correlated. To reduce the dimensionality of the data and increase parsimony, we conducted a factor analysis. Factor analysis with varimax rotation revealed two factors with eigenvalues greater than 1 (for factor loadings, see Table 5), which we labeled as ""functional"" and ""hedonic."" The variables that loaded most highly on perceived functional value were genuine/sincere, honest, informative, pleasant, relatable, understandable, believable, relevant, and benefits believable. The variables that loaded most highly on perceived hedonic value were attention getting, creative, emotional, energetic, humorous, memorable, strong, unique, and warmhearted.GraphTable 5. Varimax Factor Pattern Rotation for Blog Post Sentiment Variables.   Selection ModelBecause bloggers are chosen to participate in campaigns, selection bias may occur. To address this potential problem, we implemented a [35] selection model. The first-stage model used a probit regression to predict a blogger's selection for a campaign. To achieve identification, the set of covariates in the Stage 1 probit model must contain at least one variable that can supply the exclusion restriction—that is, it must affect blogger selection for a campaign but not directly affect the engagement generated by the blogger ([29]; [35]). Industry practice is to match bloggers with common interests in and similarity to the focal campaign. In line with this method, we use the bloggers' profile descriptions and employed varimax factor rotation to create a psychographic index score using psychographic categories not directly related to the outcome of engagement: travel/foodie, persona, lifestyle, and values.The variable providing the exclusion restriction used in the Stage 1 probit model is blogger selection of most similar other bloggers. To determine the blogger most similar to the target blogger, we created a blogger-by-campaign matrix. We multiplied this matrix by its transpose to create a blogger-by-blogger matrix, which showed which bloggers coappeared (i.e., participated in the same campaign) most frequently. We selected the blogger who appeared most often with the target blogger and used his or her selection for a campaign as an independent variable in the Stage 1 probit (for details of this procedure and a specific example, see ""Details on Stage 1 Probit Model"" in the Web Appendix).Table 6 provides the results of the Stage 1 probit model. We find that the intercept (b = −2.2744, p <.01), similar blogger selection (b = 1.5550, p <.01), and the travel/foodie blogger psychographic (b =.0374, p <.01) are significant for selection in the Stage 1 probit model. The exclusion criterion, similar to the blogger selection, indicates that when a similar blogger to the target blogger is selected for a campaign, the target blogger is more likely to be selected for the campaign. We then included the inverse Mills ratio from the first stage as an independent variable in all second-stage models. The inverse Mills ratio was not significant in the blog post comments (z = 1.22, p =.223) or Facebook post likes (z = −.060, p =.950) models.GraphTable 6. Stage 1 Probit Selection Model.  4 *p <.05.5 **p <.01.6 Notes: Standard errors are in parentheses. Model ChoiceThe dependent variables of interest, blog post comments and Facebook post likes, are count variables. Therefore, we considered using either a Poisson distribution or a negative binomial distribution for count data. A likelihood ratio test indicated that there was overdispersion in the data for the blog post comments (χ2 = 6,231, p <.001) and Facebook post likes (χ2 = 31,000, p <.001) models. Thus, we used a negative binomial model instead of a Poisson model. In addition, we find no correlation between the error terms of the two models. For each post i, we estimated the following second-stage model equations: Blog post comments=β0+β1(Number of Followers)+β2(Weekend Post)+β3(Awareness Campaign)+β4(Blogger Expertise)+β5(Functional Value of Post)+β6(Hedonic Value of Post)+β7(Giveaway)+β8(Awareness Campaign*Number of Followers)+β9(Awareness Campaign*Functional Value of Post)+β10(Awareness Campaign*Hedonic Value of Post)+β11(Awareness Campaign*Blogger Expertise)+β12(Awareness Campaign*Giveaway)+β13(Inverse Mills Ratio)+ϵ. Graph Facebook post likes=β0+β1(Number of Followers)+β2(Weekend Post)+β3(Number of Facebook Posts)+β4(Awareness Campaign)+β5(Blogger Expertise)+β6(Functional Value of Post)+β7(Hedonic Value of Post)+β8(Giveaway)+β9(Awareness Campaign*Number of Followers)+β10(Awareness Campaign*Functional Value of Post)+β11(Awareness Campaign*Hedonic Value of Post)+β12(Awareness Campaign*Blogger Expertise)+β13(Awareness Campaign*Giveaway)+β14(Inverse Mills Ratio)+ϵ. Graph Model ResultsBefore we describe our full model results (see Table 7), several main effects results are worth noting. In the main-effects-only model (see the Web Appendix), we find that campaign intent exerts a differential main effect on each platform, with awareness intent being more effective for Facebook and trial intent being more effective for blogs. We conjecture that because of Facebook's lower commercial intent, an awareness campaign is potentially more readily shared among peers in an organic fashion. The purpose of campaign incentives (i.e., giveaways) is to encourage participation with specific tasks. The negative impact of incentives on Facebook and the positive impact on the blog platform highlight the potential cannibalizing effect of one social media platform on another.GraphTable 7. Model Results Table.  7 †p <.10.8 *p <.05.9 **p <.01.10 Notes: Standard errors are in parentheses. AIC = Akaike information criterion; BIC = Bayesian information criterion. Blog post comments modelTable 7 reports the results of the second-stage model with blog post comments as the dependent variable (N = 1,237). The Akaike information criterion for this model was 6,663, the Bayesian information criterion was 6,740, and the likelihood ratio test was significant (χ2(13) = 100.64, p <.01). Variance inflation factors (VIFs) were all below 1.09, with an average VIF of 1.04, indicating no issues with collinearity. We found a significant main effect of our control variable, average number of followers (b =.3514, p <.01), indicating that this factor significantly drives the number of blog post comments.The main effect of campaign intent (awareness vs. trial) was marginally significant in the blog post comments model (b = −.2351, p <.10), while the main effect of blogger expertise was not significant in the final model incorporating interaction effects (b = −.1228, n.s.), which does not support H1. The interaction between type of campaign (awareness vs. trial) and blogger expertise was positive in the blog post comments model (b =.7283, p =.01), in support of H2. Further investigation of the interaction effect reveals that the simple slope for high blogger expertise was significant (p <.05), indicating that the impact of high blogger expertise varies by campaign intent, in support of H2a. The simple slope for low blogger expertise was marginally significant (p =.10), suggesting no differences in effectiveness of low-expertise bloggers across awareness and trial campaigns; thus, H2b is not supported (or receives weak support at p =.10). Figure 2, Panel A, summarizes the pattern of effects for the interaction between campaign type and blogger expertise. Perceived functional value of the post was not statistically significant, but the hedonic value of the post had a significant impact on the number of blog post comments (b =.2616, p <.01), in support of H3. Campaigns that included campaign incentives also significantly increased the number of blog post comments (b =.4526, p =.01), in support of H4.Graph: Figure 2. Blog post comments and Facebook post likes (empirical study).The results indicate that high blogger expertise is beneficial when paired with awareness campaigns but has a lesser effect in the case of trial campaigns. While we initially hypothesized that for low-expertise bloggers, more engagement would occur under trial than awareness intent, we find no evidence of this relationship. We find that hedonic content is positively associated with an increase in blog post comments. We return to this point in the ""Discussion"" section. Facebook post likes modelTable 7 also reports the results of the second-stage model with Facebook post likes as the dependent variable. The Akaike information criterion for this model was 5,508, the Bayesian information criterion was 5,582, and the likelihood ratio test was significant (χ2(14) =221.91, p <.01). The VIFs were all below 1.06, with an average VIF of 1.04, indicating no issues with collinearity. The number of Facebook posts was significant (b =.5728, p <.01). We found a significant main effect of the average number of followers (b =.2055, p <.05), indicating that this drives Facebook post likes. Campaigns that included giveaways also significantly decreased the number of Facebook post likes (b = −.7840, p <.01).Blogger expertise was not significantly related to the number of Facebook post likes (b =.0253, n.s.). The main effect of campaign intent (awareness vs. trial) was significant in the Facebook post likes model (b =.7416, p <.01), in support of H5. We found a significant main effect of hedonic value (b =.2215, p =.03), in support of H6. There was a significant, negative interaction effect of campaign intent and hedonic value (b = −.4824, p <.01). In light of the positive main effects of hedonic value and awareness campaigns, the negative interaction term implies that hedonic value is positively related to Facebook post likes for trial campaigns and negatively related to Facebook post likes for awareness campaigns, providing support for H7. Panel of B of Figure 2, which plots the pattern of results, shows that posts low in hedonic value can weaken engagement, particularly for trial campaigns. Taken together, the results indicate that multiple factors can increase engagement in sponsored Facebook posts. Regarding the control variables, having more Facebook posts, posts on weekends, and a higher number of followers are all related to an increased number of Facebook post likes. Posts lower in hedonic content are particularly harmful when paired with trial campaigns on Facebook.We find that the blog platform and Facebook platform exhibit differences in drivers of engagement. Campaign incentives negatively affect the Facebook platform but not the blog platform; we conjecture that this is due to the cannibalizing effect of the blogging platform. Timing of the posts (weekends vs. weekdays) also positively affects Facebook, but this effect is not consistent for the blog platform model. DiscussionIn this study, we evaluated the effectiveness of influencer marketing campaigns using an empirical database of sponsored bloggers. The results provide support for most of our hypothesized effects (with the exception of H1 and H2b). Across both models, we find a positive impact of the number of followers. Controlling for the number of followers, we find that blogger expertise, campaign intent, hedonic value, and interactions among these variables influence engagement on blog and Facebook platforms. We also find differences in the success drivers of sponsored blogging campaigns across the platforms. High blogger expertise interacts with campaign intent on the blog platform but not the Facebook platform.We find a significant interaction between campaign intent and hedonic value on Facebook platforms. Specifically, our findings indicate that hedonic value exerts a greater impact in trial campaigns, which supports the explanation that hedonic content may provide a reason for Facebook users to share information or like a blog post with an overtly commercial intent, confirming the compensatory role of hedonic value in mitigating the negative effect of a less desired post. In addition, we find a negative effect of campaign incentives on Facebook post likes for both awareness and trial campaigns, potentially due to cross-platform cannibalization of the Facebook platform by the blogging platform. Campaign incentives may cause participants to interact with a blog post more directly in the blogging environment, even though they may have first encountered the information on Facebook.In addition, we estimated a series of alternative models for robustness checks, including examining when posts are cross-posted on blogs and Facebook, alternative measures of content sentiment, alternative specifications of number of followers, varying measures of post engagement, and alternative coding for the varimax factor rotations. The robustness checks also included another version of the Stage 1 probit model specification, models using a Gaussian copula, and fixed-effects negative binomial models. The results of these alternative specifications are consistent with our reported findings (for details, see the Web Appendix, as well as an overall summary of the robustness check results in the ""Details on Stage 1 Probit Model"" section).Our results thus far have been based on data collected from a real-world context (actual campaigns featuring sponsored bloggers), providing high generalization and meaningful insights into the complex interplay of multiple factors that influence how these campaigns actually function in real life. However, field data limit our ability to manipulate key independent variables, creating the possibility that extraneous variables could account for the effects. To account for this possibility and improve our ability to draw meaningful conclusions from this research, we aimed to replicate our findings in a tightly controlled setting, by experimentally manipulating our key variables. We focused on finding additional support for a key interaction effect observed in the blog platform setting—namely, the interaction between campaign intent and blogger expertise in a blog setting. Study 2The purpose of Study 2 is to replicate one of the more counterintuitive results (i.e., the blogger expertise × campaign intent interaction on the blog platform) to provide further support for H2. We posit that campaign intent will have a differential impact on purchase likelihood in the case of high-expertise bloggers but will not affect purchase likelihood in the case of low-expertise bloggers. Pretests Expertise pretestThis pretest served to ( 1) link blogger profile characteristics with perceived expertise and ( 2) check the strength of our manipulation of blogger expertise. We kept the sample population as similar to the target audience as possible. Those sampled were women with children under 18 years of age (N = 97). The pretest was a between-subjects design with two expertise levels (high and low) manipulated using blogger profile descriptions (for details, see the Web Appendix). To create a robust measure of expertise, we used the items from [69] scale to measure celebrity endorser expertise. On a scale from 0 (""strongly disagree"") to 100 (""strongly agree""), participants rated whether they believed the blogger was expert, experienced, knowledgeable, qualified, and skilled. We averaged these five items together (α =.96) to create an overall perceived expertise score.We controlled for homophily to rule this out as an alternative explanation of the blogger expertise effects. Therefore, participants rated three items regarding blogger homophily (0 = ""strongly disagree,"" and 100 = ""strongly agree""): ""I feel that the blogger is similar to me,"" ""I feel that the blogger is a peer,"" and ""I feel that the blogger thinks like me."" We averaged these three items together to create an overall homophily score (α =.95). Controlling for age and homophily, we found that perceived expertise is higher under the high blogger expertise manipulation than the low blogger expertise manipulation (Mhigh = 62.20, Mlow = 59.37; F( 3, 93) = 20.13, p <.01). In addition, we found a difference in perceived homophily for high- versus low-expertise bloggers when controlling for age, such that homophily is higher in the case of low-expertise bloggers (Mhigh = 46.89, Mlow = 60.26; F( 1, 94) = 7.50, p <.01). Campaign pretestThe goal of this pretest was to test the manipulation of campaign intent. This pretest was a three-condition (campaign intent: awareness, trial, control) between-subjects design (N = 164).[11] Participants read a sponsored blog post; the awareness and trial campaigns were both about an educational mobile game targeted at middle schoolers. The posts were identical, except that the trial campaign had an additional message at the bottom that read ""BUY NOW!!!"" In the control condition, participants read an unrelated post about finding the right job.We measured persuasion knowledge using scale items from [ 1]. Participants rated the author of the blog post on a nine-point bipolar scale for three items: ""good/bad,"" ""not pushy/pushy,"" and ""not aggressive/aggressive."" We averaged these items to create a persuasion knowledge measure for the source (α =.85). Controlling for age, we found a significant difference in perception of the source based on the campaign intent manipulation (Mawareness = 3.38, Mtrial = 4.27, Mcontrol = 3.35; F( 3, 160) = 2.81, p <.05). Using planned pairwise contrasts, we found a significant difference in persuasion knowledge between the trial and awareness campaigns (p <.05) and between the trial and control campaigns (p <.05). This indicates that persuasion knowledge is higher in trial campaigns than in either the awareness or control campaign conditions. Method and ResultsThis experiment was a 2 (expertise: high, low) × 2 (campaign: awareness, trial) between-subjects design. The sample came from a Qualtrics panel of mothers (N = 395). Our context for this study is an educational paid app (Water Bears) targeted at middle schoolers (and their parents), designed to improve spatial reasoning. Participants read identical sponsored blog postings about Water Bears (similar to what was used in the pretest). In the trial condition, an additional phrase at the bottom stated: ""BUY NOW!!!"" The expertise conditions were identical to those in the pretest. Participants rated how likely they would be to purchase the Water Bears app on a scale from 0% (""not likely at all"") to 100% (""very likely"").We analyzed the data using analysis of variance containing all the main effects (i.e., blogger expertise, campaign intent, and their interaction). The overall model was significant (F( 7, 387) = 22.29, p <.01). The main effect of expertise was not significant (F( 1, 387) =.95, p =.33), but the main effect of campaign intent was significant (F( 1, 387) = 8.56, p <.01). In support of our hypothesized effect, the interaction between expertise and campaign intent was also significant (F( 1, 387) = 8.88, p <.01). We controlled for age, homophily, whether participants had children in middle school, and whether they follow sponsored bloggers online. After controlling for these variables, the test of simple slopes indicated that, consistent with H2a, the impact of high blogger expertise on purchase likelihood is stronger for awareness campaigns than for trial campaigns (Mawareness = 34.10, Mtrial = 20.66; F( 1, 387) = 13.32, p <.01). That is, when blogger expertise was high, participants expressed a higher purchase intent for the awareness campaign than the trial campaign. Next, examining purchase likelihood under low blogger expertise, we found no significant difference between awareness and trial campaigns (Mawareness = 30.24, Mtrial = 30.29; F( 1, 387) =.14, p =.7106), which, consistent with our empirical results, fails to support H2b. The impact of a low-expertise endorser on purchase likelihood does not depend on campaign intent. This pattern of findings confirms those from our empirical data set (see Figure 3).Graph: Figure 3. Blogger expertise and campaign intent effects on purchase likelihood (Study 2). DiscussionStudy 2 provides additional, supplemental evidence for the interaction between source expertise and campaign intent on a blog platform. We show that in the case of high-expertise bloggers, awareness intent yields a higher purchase likelihood than trial intent. By demonstrating this effect using purchase likelihood in an experimental setting, we provide further support for the validity of this finding. However, the results in Study 1 are driven by high expertise, while the results in Study 2 are driven by differences due to low expertise. This may be because the outcome variable (purchase intent) is closer to the trial condition (which is driven by low expertise) while Study 1's outcome variable (engagement in the form of blog comments) is much closer to awareness building. General DiscussionThis research sheds light on the key drivers of success of influencer marketing campaigns and offers a novel contribution by examining the interplay of social media platforms and success factors. We find that while network, blogger characteristics, and content characteristics affect multiple types of sponsored blogger engagement, the level of platform involvement and the campaign intent matter for the degree of success. We use both field data based on a large data set of influencer marketing campaigns and a controlled experiment to show convergent evidence of the majority of the hypothesized effects. By understanding this framework to increase engagement, companies can choose bloggers more effectively, matching their characteristics to campaign goals.We expect the sponsored blogging results to differ from those for other social media and paid media for two reasons. First, the nature of influencer marketing is distinct from both WOM and traditional advertising because influencers blend elements of paid and earned media. From a motivation standpoint, while traditional advertising may have multiple goals to meet brand equity–based objectives, influencers have additional loyalty to their followers. Second, the influencer designs and implements the message, not the company. This is also distinct not only from traditional advertising and spokesperson marketing tactics, due to bloggers' creative freedom, but also from pure organic WOM, because bloggers are sponsored by the company. With these influencer nuances in mind, we expect that consumers will interpret the message and source differently depending on where and how it is presented. Theoretical ContributionsOur key contributions involve understanding the interplay of post content characteristics (i.e., hedonic value of a blog post), source expertise, and campaign characteristics (i.e., campaign intent and incentives in an awareness-building or trial campaign) on campaign intent and social media platform. While campaign intent has received attention in advertising literature ([66]), our study is the first to examine the impact of influencer marketing campaign intent on engagement. We find that campaign intent is an especially pertinent moderator to many of the relationships in our study. For example, campaign intent moderates the relationship between source expertise and blog post engagement. Campaign intent also moderates the relationship between hedonic content and Facebook post engagement. These findings suggest that the relationship among source, content, and engagement should not be assessed in isolation from campaign intent.In addition, we contribute to the literature on blogger expertise by demonstrating conditions in which expertise has ( 1) a positive impact, ( 2) a negative impact, and ( 3) no impact. Specifically, we demonstrate that in some conditions, source expertise is positive, and in others, it is nonsignificant. Expert endorsement is beneficial under an awareness intent, while a novice endorsement is beneficial under a trial intent. This effect holds under high-involvement, low-distraction social media platforms. On low-involvement, high-distraction social media platforms, however, source expertise does not affect engagement. We provide a more nuanced explanation of expertise and its role in online brand engagement. Taken together, these findings provide a richer understanding of source expertise in the case of influencer marketing.We extend prior research on influencer marketing by highlighting the importance of consumer skepticism differences, which may cause campaign intent (awareness or trial) aimed at different stages of the CDJ to function differently. At early stages in the CDJ, consumers are open to guidance from those with high perceived expertise. However, closer to trial, consumers are open to endorsements that originate from either less expert (presumably more homophilous) or more expert sources. This difference is only true in high-involvement platform settings such as blogs. Understanding the contextual effects guiding the impact of source expertise in online influencer marketing settings is a key contribution of this research. It extends prior works on influencer marketing settings that focus on either expertise ([84]; [95]) or stage of the CDJ ([44]) but do not examine their interplay.We also argue that the motivations driving people to use social media platforms influence how they view different types of influencer marketing campaigns. In a blog environment, in which users are motivated to process information deeply and to engage with bloggers' information and content, trial campaigns are better received. In a Facebook environment, in which users' motivations are more focused on sharing information with peers, awareness campaigns have a more positive impact. Given this general preference for awareness (vs. trial) campaigns on Facebook, hedonic value of a blog post takes on more significance in the context of trial campaigns.Furthermore, our findings show that post content (i.e., hedonic value) is important in generating post engagement. We extend the findings of [ 8], who argue that hedonic content increases social transmission and virality of online messages. We find that hedonic value has a significant effect on both blog and Facebook platforms. We also show that on low-involvement, high-distraction social media platforms, hedonic content can be beneficial when paired with trial campaigns, perhaps because of their overtly commercial intent. In awareness-building campaigns, in which user motivations involve sharing information, hedonic value may be distracting to the primary goal. Thus, hedonic content of online communications is not always beneficial to marketing campaigns.Our findings are revealing on the impact of campaign incentives, which research has previously shown to increase WOM ([ 9]) and enhance quality perceptions of a product ([91]). We demonstrate that incentives (a chance to win a giveaway) generate WOM benefits in the form of increased engagement (i.e., blog post comments). This finding advances the literature by showing that increased WOM and engagement can be generated without giving a free product to every person; simply offering a chance to be the recipient is enough to induce the benefits of free products. This greatly reduces the costs associated with running a campaign with free product incentives while still generating a similar response.One potential rationale for why giveaways have a positive effect in a blog environment but a negative effect in Facebook posts is that giveaways are typically executed in a blog platform setting, and blogs cannibalize Facebook engagement. An alternative explanation, which could be the focus of further research, is that high-involvement platforms in general are more conducive to driving engagement through free goods and incentives. Prior research suggests that when consumers are more involved, they want to minimize risk through information search in their decision-making processes ([21]), and they might view free products or incentives as a way to lower the risk of a new purchase. This is worth examining under a broader understanding of customer engagement.This article offers a unique contribution by examining the differences between social media platforms. While we empirically focused on blogs and Facebook, the findings can be extended to other social media platforms. As platforms continue to develop, the extent of involvement generated by a platform can help inform decisions on influencer marketing strategies. Moreover, the focus of this research was on online engagement, which sheds more light on customer profitability than a mere focus on customer attitudes or preferences. Furthermore, our examination of cross-platform impacts (i.e., blogs and Facebook) dovetails with other research examining how different advertising media may synergistically improve customer engagement and profitability. [50] investigate the dynamic interaction between paid search and display ads. We extend their findings by focusing on one form of social media marketing that straddles the earned and paid social media types. Therefore, our findings are of particular relevance in light of the increased blurring between these two types of social media marketing. The variations observed across social media platforms indicate that the type of platform can affect the profitability of digital marketing expenditures. Managerial ImplicationsWe offer novel insights to managers implementing influencer marketing campaigns. First, this article delineates best practices for sponsored bloggers based on marketing campaign intent and platform. When trying to bolster awareness campaigns on a blogging platform, managers should feature the expertise and credibility of the blogger. However, in the case of trial campaign intent, campaigns by both expert and novice sources will be equally successful.Second, when implementing campaigns on Facebook or any other high-distraction platform, managers should vary content strategy depending on campaign intent. Trial campaigns can benefit from featuring posts with high hedonic value, particularly in high-distraction environments such as Facebook. Furthermore, when choosing bloggers to implement a strategy involving multiple high-distraction platforms, managers should focus on selecting bloggers with a large follower base to ensure the highest penetration and engagement.Third, with regard to the impact of campaign intent on outcomes, we recommend that managers use the appropriate drivers of success for blog engagement (i.e., blogger expertise, campaign incentives) in awareness campaigns and rely on hedonic-valued content on blog platforms. We further recommend that managers avoid using campaign incentives on Facebook or other low-involvement, high-distraction platforms, such as Twitter or Instagram, and instead focus on the hedonic value of post content. Return on EngagementResearchers have begun examining the impact of social media expenditures on firm performance and shareholder value (e.g., [19]). By examining sponsored blogging across social media platforms, we contribute to extant literature examining the differential profitability impacts of these platforms, thereby extending research that examines across-media profitability impacts of advertising expenditures ([80]). In addition, many companies have tried to quantify the value of social media engagement. Estimates range from $.33 to $8 per Facebook like, while social media shares are estimated at $8 per retweet and $14 per Facebook post share ([34]). We conservatively estimated the dollar value of each type of engagement at $1 for a Facebook post like, $1 for a Facebook post comment, $2 for a Facebook post share, and $2 for a blog post comment. We multiplied the number of blog post comments and Facebook post likes, comments, and shares by an estimated dollar value for each type of engagement. We then summed these values and used them as the revenue per campaign, per blogger, generated by engagement. Next, we calculated return on engagement (ROE) by dividing the total revenue generated by the total cost for each campaign by blogger. We modeled ROE using campaign intent, expertise, campaign incentives, and hedonic content. We found a marginally significant, positive relationship of expertise and a significant, positive relationship of campaign incentives on ROE (for details, see the Web Appendix). This implies that both blogger characteristics and campaign intent can affect firms' bottom lines. By optimizing social influencer marketing strategies with these results, firms can increase ROE for influencer marketing campaigns. Limitations and Future Research DirectionsThis research is subject to certain limitations, which may present new directions for further research. We examined only a limited set of outcome metrics associated with a particular blog post and did not directly test for the impact on return on investment (ROI). However, [53] show that both social media and customer WOM increase ROI, and [54] demonstrate the relationship between engagement and ROI. Further research could increase the set of outcome measures of a given campaign by considering the direct impact of a blog post on consequential outcomes, such as sales and ROI. Further exploration of why customer engagement could affect these performance outcomes is worth examining and would extend [33] framework. Our measurement of key constructs, such as sentiment, relied on post hoc measures based on judges evaluating each blog post for factors such as creativity/uniqueness and personal relevancy. A more direct measure would involve having the audiences of a given blogger rate his or her posts for various aspects of sentiment. In addition, research could include a field experiment of blogger choice informed by this research versus the current methods for selecting bloggers for campaigns. In addition, this research uses the bloggers' network size at the time of the posts but does not formally take into account the entire past performance or longevity of the bloggers' careers. This could be an important variable to consider in future research. Finally, we acknowledge the highly evolutionary nature of social media platforms. Therefore, while we explore the effects of campaign, source, and hedonic value in terms of two social media platforms, we recommend considering these findings in the light of platform characteristics versus specific platforms.In general, sponsored blogging and influencer marketing have been the target of ethical debates in recent years. Some critics argue that social influencers fail to reveal their sponsorship by companies, thereby creating a perception that their sponsored posts are organic WOM. This type of deceptive marketing practice has been at the heart of various Federal Trade Commission investigations of Instagram posts in recent years ([46]). The [25] has reached out to influencers directly and reiterated its requirements to disclose any endorser and advertiser connection. As noted, all sponsored posts in the current research included a declaration of sponsorship at the beginning of the blog post. Still, there is room for research on how sponsored blogging as an advertising medium is distinct from other forms of advertising that consumers view unambiguously as paid advertising. "
28,"Dynamic Governance Matching in Solution Development Facing competitive and commoditization threats, many companies shift to solution offerings, albeit with mixed results. With a qualitative analysis of dyadic data (suppliers and customers), this article investigates an important, often overlooked reason for such mixed outcomes: the complex, dynamic role of governance matching. This study identifies a series of tensions arising from solution-specific exchange conditions and the matched governance mechanisms actors use to address them: temporary asset colocation, network closure, knowledge-based boundary objects, rights allocation agreements, and liaison champions. It also reveals the dynamic nature of governance matching. Solutions evolve in three phases—experimentation, integration, and evolution—in which single mechanisms have different functions (safeguarding and/or coordination), provide contingent and transient benefits, and can be used in combination to address complex tensions. This study also identifies two decision points, mutual commitment and balanced power, that separate the three phases; their outcomes help explain why certain solution efforts do not take off, others stall, and still others revert to mere spot exchanges. Beyond contributing to solutions literature, these findings provide actionable insights to marketing managers.KEYWORDS_SPLITFacing competitive and commoditization threats, many business-to-business companies increase the service part of their offers ([12]), such that they integrate product and service content to satisfy customer needs—an offering commonly referred to as a ""solution"" ([61]). Anecdotal evidence notes famous successes (e.g., IBM, Rolls Royce) along with comparably unsuccessful examples (e.g., Xerox, Siemens) of solutions ([51]). Empirical analyses also suggest that solution benefits are difficult to achieve ([17]; [52]), leading some authors to speculate that companies might struggle to exploit their solutions' potential because the development process remains poorly understood ([ 3]; [38]).With its inherent service shift, solution development exhibits some differential features compared with other types of business-to-business exchanges. First, value in solution relationships is intensively coproduced in interactions between the supplier and the customer, who cannot simply exchange products or services but rather need to combine their key resources to cocreate an integrated offer ([68]). Coproduction makes solution exchanges more complex and equivocal and requires actors to negotiate and ""normalize"" their divergences through a set of specific governance choices ([57]). Second, the service shift induces radical changes in relational habits, putting the customer at the center of the supplier's value proposition and altering tasks and duties attributed to suppliers and customers in the exchange ([73]). In such circumstances, the involved actors need time and several interaction episodes to adapt and make sense of one another's perspectives ([60]). A recent survey in the health care industry highlights such an initial lack of mutual understanding as a major hurdle to solutions development ([63]). As a direct consequence of these sensemaking requirements, the service shift in solutions and its associated governance issues unfold dynamically ([27]). These features create a context of emerging exchange conditions that expose solution actors to evolving governance tensions (e.g., the potential for opportunistic behavior by one of the actors) that, if not addressed properly, reduce their willingness to engage in value-creation initiatives ([70]).Extant solutions literature has not investigated the resulting governance issues, though some studies have indirectly suggested a key role of governance in determining the fate of solutions. For instance, to achieve the full potential of a solution, it is known that customers must share key resources with the supplier ([44]), and the supplier must take responsibility for tasks that the customer previously performed ([67]). But under such circumstances, how can actors avoid opportunism threats that emerge from responsibility sharing? Moreover, how can they prevent information leakage during the exchange of critical resources? Considering that the dyadic interaction between actors extends over time ([18]; [19]) and that actors exhibit divergent perspectives that need to be aligned ([65]), other tensions are likely to emerge and potentially affect the solution development process. Taken together, these notions reflect what analysts and practitioners identified as a key challenge to many industrial firms' servitization efforts—that ""both parties need a mechanism to resolve disputes"" ([22]).Against this backdrop, our research sheds light on an overlooked issue: How can solution actors succeed in governance matching, such that they implement appropriate mechanisms to avoid or address the tensions that emerge from solution-specific features? Governance matching is a critical factor for developing business relationships, in that ""managers seek to maximize performance by matching exchanges, which differ in attributes, to governance structures, which differ in their capacities to respond effectively to disturbances"" ([71], p. 79).With this study, we aim to ( 1) identify systematically the various governance tensions associated with solution development and how actors might match specific mechanisms to those tensions and ( 2) detail how the tensions vary over the course of solution development and how actors thus adjust to deal with their evolution. To achieve these objectives, we conduct a multimethod (interviews and observations), in-depth, qualitative investigation of solution development efforts by a leading supplier of measurement and automation control systems and its clients.Figure 1 depicts the dynamic framework of solution development that we derive on the basis of this investigation; it also serves as a road map for the article. As the framework reveals, solutions, from a governance standpoint, evolve in three main phases (experimentation, integration, and evolution), separated by two decision points (mutual commitment and balanced power). For each phase, we delineate the central components of the governance matching sequence (conditions arising, tensions, and mechanisms) that explain how solutions evolve from a governance standpoint. Table 1 provides a glossary of the key terms used in this framework.Graph: Figure 1. Notes: Two-way arrows represent the governance matching process.GraphTable 1. Governance in Solutions: Glossary of Key Terms.  In the first phase, actors engage in small-scale experimentation, so governance matching aims to safeguard them from opportunistic behaviors by their counterpart, as well as from any competitive threats that may result from shifting roles and responsibilities. Asset colocation and network closure are the two key mechanisms in this phase. If the match works, actors develop mutual commitment and engage in tacit knowledge integration practices. Governance matching in the second phase involves the coordination of goals and intangible assets, along with safeguarding from potential knowledge appropriation by the counterpart. Boundary objects and rights allocation agreements ensure governance matching in this phase. If this integration evolves in a context of balanced power, actors proceed to the third phase of solution development, in which governance matching aims to coordinate the interplay of adaptation and proactivity to deal with unforeseen contingencies. Liaison champions are the core mechanism in this phase. If any matching attempts fail, the process may stall, revert to spot market exchanges, or evolve as vertical integration. As our further analysis reveals, governance matching in solutions entails a careful blending of contractual and relational mechanisms to address the complex, evolving set of tensions that actors face over the course of solution development. Specifically, the aforementioned mechanisms delimit risk and promote a collective climate in the experimentation phase, facilitate knowledge sharing processes in the integration phase, and stimulate a forward adaptation attitude among actors in the evolution phase.From a theoretical standpoint, this study contributes to existing solutions literature by specifying the relevant, overlooked role of governance matching, as well as how solution actors execute these challenging, dynamic matching activities. Moreover, we illustrate what happens when governance matching fails and why solution development may take unplanned directions. From a managerial standpoint, this study recommends actors to engage in governance engineering activities to succeed in solution development. They should take a learning-by-doing approach, monitoring and anticipating the various evolutionary paths that solution development may take, and engage in periodic decisions about whether and how to continue in the process. In short, our study reveals that successful solution development requires a negotiated, socially constructed process that features the complex, joint activity of governance matching. Governance in Business Relationships Literature on GovernanceThe discriminating alignment principle in transaction cost economics asserts that certain conditions in an exchange (e.g., asset specificity) generate specific tensions between parties (e.g., opportunism), which must be addressed with proper governance mechanisms (e.g., contracts). Identifying proper governance mechanisms (i.e., governance matching) is thus crucial, offering ""the means by which order is accomplished in a relation in which potential conflict threatens to undo or upset opportunities to realize mutual gains"" ([72], p. 37). When successful, the actors can realize ""economizing advantages,"" such as reduced transaction costs and increased willingness to engage in value-creation initiatives ([30]).Different manifestations of the stylized conditions–tensions–mechanisms sequence implied by governance matching appear in prior research ([54]). For example, transaction cost theory originally prioritizes asset specificity and uncertainty as key exchange attributes that raise potential opportunism and lock-in risks, which in turn require increasing vertical integration (or hybrid forms of governance, as later acknowledged). Agency theory instead highlights information asymmetry, which can hamper monitoring and control and may require formal contracts to align actors' behaviors. In contrast, relational marketing focuses on trust and commitment as key features of a successful relationship, suggesting the use of informal social rules as proper mechanisms to reduce risk and facilitate coordination. Stewardship theory further proposes the benefits of an ex ante determination of mutual goals. Although not exhaustive, this list reveals the multiplicity of conditions that might arise in a relationship and, accordingly, the heterogeneous mechanisms available to actors to face tensions and ensure governance matching.We define governance mechanisms as the tools, devices, and tactics available to managers to regulate the conduct of the exchange relation ([48]). They entail concrete activities and processes that managers can deploy and execute, which may reflect shared beliefs or formal rules of conduct ([33]). Governance mechanisms have two main functions: safeguarding actors from exchange hazards and coordinating their behaviors ([24]). Typical hazards include various instances of opportunism and other threats related to information sharing; coordination refers to an optimal combination of activities and outcomes ([33]). Traditionally, safeguarding and coordination have been accomplished using contractual or relational governance forms ([ 9]). Contracts regulate an exchange through formal promises and obligations to perform future actions ([45]) and can vary in their complexity, with respect to roles and responsibilities, monitoring issues, and outcomes ([56]). Relational forms instead use social processes embedded in repeated exchanges, which promote different social norms, such as flexibility, solidarity, mutuality, harmonization, and restrained uses of power ([ 8]). According to the discriminating alignment principle, because governance forms vary in their capacity to ensure safeguarding and/or coordination goals, actors must identify the proper mechanisms for dealing with specific tensions as they emerge in exchanges ([71]).In the context of solutions, which involves high levels of coproduction and the potential for radical shifts in relational habits, we expect actors to face a series of emerging exchange conditions that can, if left unaddressed, generate a series of heterogeneous governance tensions, which also evolve dynamically over the various phases of solution development. Achieving governance matching in solutions thus seems particularly challenging and may require the use of multiple contractual and relational mechanisms to ensure both safeguarding and coordination goals as they unfold over time. Therefore, we aim to identify specific manifestations of governance tensions that arise over the course of a solution development effort, as well as to detail how actors dynamically address them with proper mechanisms. This study accordingly constitutes a middle-range theoretical extension of existing governance models (e.g., [28]; [70]), with which we aim to identify operational meanings and actionable insights for the specific context of solutions ([29]). Governance in Solutions LiteratureFrom a thorough review of solutions development literature (see Web Appendix W1, Table W1, for search and selection criteria), we generated two interesting insights regarding the role of governance in solutions. First, as Table 2 shows, a set of core papers provide interesting outcomes but do not take a governance view; they mostly focus on firm-level resources and capabilities. However, they reveal that actors often express divergent expectations of solution development ([65]) and face various capability integration issues in the process ([44]; [67]). Such considerations may generate actor misalignment and responsibility-sharing challenges, which need to be matched with proper governance mechanisms. Other studies also indicate that solution development is embedded in long-term relationships ([18]) that evolve along different paths ([19]). The governance issues associated with solution development thus are likely to change over time, requiring a dynamic approach to governance matching. The outcomes of these studies implicitly suggest the presence of dynamic governance issues, which warrant more attention.GraphTable 2. Governance in Solutions Literature: Core Studies on Solution Development.  Second, as Table 3 reveals, a few recent works have explored single governance elements in a fragmented fashion, issuing preliminary evidence that responsibility sharing ([73]), misalignment ([59]), and uncertainty ([40]; [66]) generate governance challenges for actors engaged in solution development. Yet a holistic view of governance matching in solution development is still lacking in these studies, though their joint consideration would better reflect the breadth and relevance of governance issues in solutions.GraphTable 3. Governance in Solutions Literature: Studies on Solutions with Specific Governance Issues.  Taken together, evidence from the aforementioned literature (see Web Appendix W1 for more details on each study) highlights the usefulness and need for a systematic, thorough analysis of heterogeneous, dynamic governance issues that emerge in solutions development, as we attempt here. Research Method and Data Collection MethodologyIn line with our exploratory research questions, we adopt a grounded theory approach to perform a qualitative inquiry of a relevant case ([16]). Consistent with the iterative logic of qualitative research ([37]), we rely on both theory and data to integrate existing theoretical accounts with field evidence and shed new light on the solution development process. Following [64], we use theories as informed priors to frame and reconcile the results of the grounded analysis ([47]).Because our research objectives do not include causal reasoning, we employ a narrative (vs. propositional) style of theorizing to obtain a process model of generative processes (i.e., governance tensions and mechanisms) that unfold as a general sequence of events (phases, decision points, scenarios), leading to the outcome under investigation (i.e., solution development) ([11]). Core outputs of a narrative style are a storyline populated with rich explanatory details, organized in abstract patterns, as well as a plot that depicts core processes and sequences, which serves as a guideline for the story (Figure 1).Following clinical case technique principles ([ 5]; [50]), we seek to reconcile the key empirical findings (governance mechanisms) with theoretical knowledge on governance, to gain deeper understanding of the role of specific mechanisms and reveal how governance matching works. First, we classify the mechanisms according to two conceptual categories: governance form (contractual vs. relational) and governance function (safeguard vs. coordination). Second, drawing on existing governance theories, our reconciliation effort identifies how the specific mechanisms solve the governance tensions they are supposed to address. Overall, we aim to improve understanding of the rationale for governance matching efforts in solution development. Sampling LogicWe use a single, relevant case ([74]), involving a leading, Italy-based, global supplier of measurement and automatic control systems that serves multinational, market-leading customer companies in diverse industries. This approach offers several advantages. First, we can perform a purposeful sampling of matched supplier–customer dyads; for each customer company, we knew the counterpart in the supplier's business unit responsible for it. Accordingly, the analysis features matched supplier–customer quotes from the same dyad for each identified governance challenge and mechanism (Web Appendix W2 includes additional representative quotes). Second, focusing on one single, committed supplier granted us continuous access to the board and members at various hierarchical levels, full support for our interactions with key customers, and several possibilities to discuss and validate the findings. Third, an individual case supports a micro-level, in-depth investigation, focused at the product-market level of analysis, which represents the dimension at which governance matching actually operates ([20]). Fourth, using the information available at the time of sampling—which we obtained from the supplier company and validated in interviews with the respective customers—we included dyads engaged in solution development at different levels of maturity (early vs. advanced). Thus, we encountered solutions in different phases of development and could generate a fine-grained, dynamic model of solution development (Figure 1).[ 5] Fifth, the sampling produces heterogeneity in relationship tenures and industries, which helped us investigate potential patterns across cases.The focal supplier is a well-established industry leader that delivers high-tech, customized products that support customers' production processes and components. Customers represent a wide array of industries (automotive, electronics, transport, health care, food), and the supported measurements range from end-of-line quality tests in assembly processes to prototype testing and benchmarking activities in production or research and development. The company's strategic intent is to shift from being mostly an equipment or test bench supplier to becoming a solution provider or even a pure service player (e.g., running test campaigns in its own research center and delivering the results to customers). In this sense, its offering represents what [67] call process delegation service—that is, the most advanced form of a product–service hybrid offering. In the chief executive officer's words, ""We have been used to selling tons of intelligent iron to our customers, but now we should also try to sell them our intelligence applied to their data."" In support of the appropriateness of our sampling approach, we note that the aggregate service share of this company's turnover increased, from less than 5% to more than 20% in the previous ten years, with some variation across different products and markets. Thus, we have a unique opportunity to examine a significant service transition process, spanning both preliminary attempts and more established episodes of solution development. Data Collection and AnalysisWe applied different data collection techniques at the different stages of our study. Specifically, we conducted 29 semistructured interviews with 12 managers of the supplier and 17 managers of customers, in line with the dyadic nature of our investigation ([23]). The interviews took place in two rounds, between November 2015 and September 2016. They lasted 69 minutes on average and were audiotaped and transcribed verbatim into 388 pages, supplemented by 150 pages of notes. To obtain further information about implementation efforts, we carried out five in situ observations, of one day each, between September 2014 and October 2018. With this approach, we visited the supplier on a regular basis to capture internal changes or advances in its solution development processes over time, while also directly observing some relationships with customers (e.g., conference at the supplier's headquarters, inauguration of a new facility). In addition, we carried out opportunistic conversations with other organization members to further explore unexpected issues that emerged during the interviews or observations.To ensure rigor, process verification, and content validity, we employed member checks and validated insights with participants throughout the research process. [49] note the relevance of this step, not only at the end of but also during the study, when additional information can still direct the ongoing research. So, for example, we contacted the participants from the first round of interviews, asking them to confirm our classification of solution development processes with specific customers as rather early or more advanced. We also presented the intermediate research results in three dedicated workshops, with 20, 20, and 10 participants from the supplier company (the second workshop also was video recorded). With this feedback, we reviewed and reinforced the preliminary findings, included in subsequent research steps. Finally, we presented the results in a half-day workshop with seven managers from the supplier company. This step helped us gather feedback about the entire framework of our analysis, which we integrated in the conclusive version of the study. Specifically, Table 4 provides a summary of the data collection process, outlining the objectives and results of each step, and Table 5 lists the different data sources and dyadic information. Further details regarding data sources and analysis procedures are provided in Web Appendices W3–W5.GraphTable 4. Data Collection Process.  GraphTable 5. Data Sources.  1 Notes: OEM = original equipment manufacturer. ResultsThe presentation of the results follows the sequence of phases foreshadowed in Figure 1. When analyzing the data (interviews and observations), we accounted for the relative maturity of the solution development processes and therefore were attentive to any governance issue specific to earlier or more advanced processes. In the subsequent cross-case comparison, we could relate statements describing recurrent governance issues from the interviews to the maturity of the respective process. Thus, the three phases discussed hereinafter are a direct reflection of the dynamic emergence of governance issues over the course of the solution development process. This is portrayed in the process model in Figure 1, which constitutes the primary outcome of our narrative style of theorizing and depicts the process and the dynamics of solution development. The Experimentation Phase Cooperating while avoiding opportunism: asset colocationAt the onset of solution development, actors face a first arising exchange condition: role shifting. The supplier takes over some responsibility for tasks that the customer previously performed, which may require the customer to provide information and resources, as the following quotes reveal:We always sold the equipment to these large clients, but sometimes they actually need some small tests or some small solutions. So, we go to them and say, ""We perform these tests for you, we take your place."" (Sup_4, Dyad 6)To test our products, we have to share information with them. We do it sort of on a need-to-know basis, where we share the amount that is needed to be shared,...so that they can do this new activity for us. (Cust_11, Dyad 11)In these new, challenging roles, the actors face an initial governance tension: They realize that solution development requires a tighter, more intense form of coproduction, but they fear sharing critical resources, because it might leave them vulnerable to opportunistic threats, so they question the extent to which they should cooperate at all ([69]). As the following quotes indicate, the key tension sparks the question: How can actors foster deeper cooperation while safeguarding themselves from opportunism threats?I have to decide case by case: do I want that [information sharing] or not? How secret are the data and how should we deal with them?...Also, I don't need to be able to do everything by myself; however, I have to pay attention that I am always able to have a say in these collaborative models. Otherwise the counterpart can fool you in any possible way. (Cust_7, Dyad 7)Some of our customers...are very reluctant when it comes to sharing their know-how, because it implies passing information to us, and in their eyes, this means ""to the outside."" (Sup_1, Dyads 1 and 2)Faced with this governance tension, several dyads in our sample offer a common answer: They physically colocate proprietary assets (e.g., machinery, employees) using temporary agreements. With simple contractual forms, these customers transfer specific physical equipment (e.g., components) to suppliers' premises, while suppliers send some personnel (e.g., resident engineers) to customers' premises, in both cases for specific periods of time.We have developed a test installation and had a person from our supplier [resident engineer] who was staying with us and was responsible for the whole software development. This was welcomed very positively, from the plant's responsible employees. (Cust_14, Dyad 14)They [the supplier's managers] provided us the opportunity to send them a number of fuel injectors that they would test and then give back to us with a report. (Cust_2, Dyad 2)It has now been a couple of weeks that these two people from [customer company] have been here with us and working in our lab. We are trying to set up a new test installation to do some trials together and see what comes out. (Sup_1, Dyads 1 and 2)Interestingly, recent reports on successful solution cases in the business press have also mentioned the role of engineers as ""swap inventory"" at customers' premises ([ 7]). In a similar vein, tire manufacturer Michelin, when introducing fleet management solutions, started hosting customers to make them aware of requirements and advantages of solution development (such that an adequate driving style prevents damages to tires, which is in Michelin's interest, and reduces fuel consumption, which is in the customer's interest; [39]).Through these colocation activities, actors engage in preliminary episodes of deep collaboration, with two important advantages from a governance standpoint. First, each actor commits to a condition of temporal asset specificity ([46]), so at least in the short run, both the supplier and the customer signal their commitment. While it is possible that the actor that transfers assets or personnel does so because it faces less risk compared with the counterpart, neither is exclusively exposed to a risk of exploitation during colocation ([70]). Second, beyond being temporary, asset colocation does not require massive investments, so actors gain additional protection from the established upper bound on the possible risk of failure—a sort of ""affordable loss"" in case the collaboration does not work ([58]).[Collaborations can start with us giving the supplier] access to a field of ours without a direct order from our side, and the supplier does some initial work at its own expense. Then we share the results, thanks to the fact that we are both [willing] to bear the consequences of small initiatives. (Cust_6, Dyad 6)Implemented through simple contractual forms, asset colocation emerges as a first mechanism that solution actors adopt during early development phases to face the governance dilemma of initiating a deeper cooperation in a novel relational context marked by potential skepticism. Through colocation, actors can engage in small-scale experiments that serve as solution trials, in a context marked by delimited, controlled risk. Their simple contractual nature is effective for managing bilateral governance issues that can be identified in advance and involve proprietary assets ([33]):So, there is more what we would call risk sharing. Essentially, we enter into contracts to do this, and when we enter into these contracts, we are saying that, you know, this is what we are going to accomplish and this is what our company is responsible for. (Cust_15, Dyad 15)The contracts we use at the onset of the relationship are pretty standardized. They mostly deal with privacy issues, not with money or economic aspects, without strong enforcements. Such contracts mainly set up the timing of the collaboration while delimiting its scope (e.g., dos and don'ts of a resident engineer and for how long). (Sup_6, Dyad 8)Notably, this approach conflicts with a traditional view, which holds that formal mechanisms hinder commitment at the start of a business relationship ([36]). Instead, in a solutions context, simple contracts may help avoid unspoken assumptions and unintended behavioral consequences early in the process ([ 8]). Metaphorically, colocation as a contractual mechanism acts as a sort of ""trust Trojan horse,"" used to gain confidence and safeguard from the counterpart by leveraging physical closeness. Seizing opportunities while avoiding information leakage: network closureA second exchange condition arising in the first phase of solution development is the presence of indirect links with rivals, reflecting the systematic links of the solution provider with various actors. These linkages likely involve concurrent relationships with any particular customer's competitors. When engaging in solution development, the customer recognizes that the common supplier might both receive and provide its rivals with some sensitive knowledge:They have to walk the line between multiple customers that do similar things. And they can't break [these customers'] trust; however, they can give you anecdotal type of feedback with no names of anybody else. (Cust_8, Dyad 8)Developing solutions requires a cooperative environment between the supplier and customer to achieve joint value ([44]). However, the presence of indirect links between the supplier and rivals may make customers reluctant to seize the opportunity arising from joint solution development due to the risk of indirect information leakage to those competitors ([ 2]). Even if customers recognize the potential gained from developing solutions, they worry that the supplier might share crucial information with their competitors, even if inadvertently, as the following quotes reveal:It often happens that the client tells me he wants a system doing this and that, but doesn't tell me why. He doesn't tell me why, because he is afraid that the news about him having a certain problem will spread and become public. (Sup_3, Dyads 4 and 5)I do not go a priori to my supplier, tell them everything, and then ask them, ""How can you help me?"" They are a partner of ours, that's true, but they are not only our supplier, they work with others as well...so we necessarily limit the information flow. (Cust_4, Dyad 4)To handle this tension, the dyads in our sample set up relational mechanisms aimed at realizing network closure. The supplier aims to embed a set of selected customers (also rivals) in a collective, cooperative network ([31]). Collective initiatives in the form of industry-specific activities or communities represent networking events designed to build a harmonized, noncompetitive, cultural climate among selected customers that are also competitors in the value chain. These workshop-like initiatives provide forward-looking opportunities to solution actors that otherwise might not be seized, but they also represent ""neutral"" environments, where actors feel free to discuss and share new features for existing solutions, as well as broader interests:So, we created this concept of a ""market club,"" where almost everyone is happy to discuss with others, and this idea of exchanging with others is a completely different and pleasant experience. (Sup_4, Dyad 6)The system that they are operating with the customer is very experience-centered: They visit each customer-hospital regularly, and then there is this annual meeting in which they discuss with their customers the innovations to implement and what specific needs are emerging among them. (Cust_15, Dyad 15)So, we created an international community. This community takes place every two years and is only open to some sites [clients], not to every site. It is an international event which we open to the sites that helped us create the markets. (Sup_11, Dyad 15)Consistent with the concept of clans, whose shared values and beliefs help monitor cooperation efforts implicitly ([ 4]), closed-network initiatives increase the level of social monitoring among participants and circumscribe the flow of information shared in networks ([31]). This mechanism also became evident in our in situ observations. Some respondents went even further, claiming positive spillovers from working with a supplier that is at the center of a network that includes their competitors:[Fourth in situ observation] Two managers from competitor companies met at the event held at the supplier's facility during a coffee break. It was clear that they knew each other but probably had not seen each other for a long time. They exchanged a few comments about the workshop and the nice location, before discussing details of the presentation that had just finished (about future technological trends characterizing the industry). Each one prudently exposed his view by stating which standard he considered the most relevant for the upcoming years. Before returning to the workshop, they agreed to continue the conversation later that day.Interviewer: How do you feel about the fact that your supplier is also working with your main competitors?Interviewee: I actually see that as a benefit! Because I think in terms of understanding where the industry itself is going, they have a better idea of what many people in the industry are doing, that I may not have. So if they have something about my competitors or a supplier, then they are able to develop those capabilities and either have those when I need them or know how to get them. I don't [think] any proprietary information that I am concerned about will be passed to our competitors, because I think that if that would happen, in their business that would be very detrimental. (Cust_2, Dyad 2)Closed-network communities thus emerge as a second mechanism that actors employ in the first phase of solution development to address the tension involved in the balance between the opportunities spurred by solutions and the threats from competitors that share the same supplier. Network closure constitutes a relational mechanism to address this tension, because by sharing harmonization norms, it defines a ""social matrix"" in which all actors agree to participate, and it aligns the scope and expected behavior of all these participants ([45]). Harmonization norms are particularly effective in the early experimentation phase because they act as socialization devices ([70]) that expose prospective partners to common goals and values that they can internalize and align in future collaborations:We host this event [in the automotive industry] every four years. We try to invite as many [original equipment manufacturers] and tier ones as possible to each event, because this is a meeting point, a relational and networking chance with and among the various customers that come. And by doing so, they also see our context. (Sup_5, Dyad 7) First decision point: the role of mutual commitmentThe first phase is essentially a trial period in which actors experiment with various interactions to develop solutions. Simple contractual mechanisms, in the form of asset colocation initiatives, and relational mechanisms, featuring closed-network harmonization efforts, both have important roles: they facilitate collaboration efforts to seize early opportunities while also safeguarding actors from initial exchange threats, such as opportunism and information leakage. Whether the experimental period is short or long, our data reveal two possible outcomes: the trial efforts are successful, and actors decide to engage even more seriously, or the experimentation is not productive, and relationships dedicated to solution development become more sporadic (see left side, Figure 1).The first scenario results from the emergence of mutual commitment among actors, including their bilateral willingness to make short-term sacrifices to realize long-term benefits ([ 1]). As we describe subsequently, the sequential exchange of commitment generates interdependence across the actors' organizations ([34]), which deepens solution development efforts. Several quotes describe situations in which both actors, after some cooperation, decide to devote more resources to the relationship and accept more risk to move forward:Your management has to put resources and accept some level of risk to compete. If you are interested in that, then I am glad to give you an opportunity. (Cust_8, Dyad 8)With this customer, we invest a lot in innovative solutions now, especially in robotics, and all this started with an initial collaboration that allowed us to kickstart bigger projects....At this point, it is not a mere give-and-take relationship anymore, but we are getting really ""in tune"" with this customer. (Sup_10, Dyad 14)Solution means really taking care of the customer,...which means making sure that the investment the customer did when buying from you is perpetuated over time....So, the customer sees this [attitude to continue] as added value. (Sup_6, Dyads 8 and 9)The second scenario instead involves an exit option ([32]). The actors, dissatisfied by their experimental collaborative attempts, decide to remove any relational component from their solution exchanges and conceive of them as mere spot transactions, mostly governed by price mechanisms. In this case, solution development remains embryonic and follows a market-based trajectory, represented in Figure 1 by the first dotted downward line. In the following quote, after an initial effort, one customer felt ""abandoned"" by the supplier in the experimentation phase, preventing the relationship from developing:Nobody from the supplier company comes proactively to us. Conversely, one time the supplier told us they wanted to understand how to develop a specific measurement solution, and we agreed they could install one in our facility in the U.S....but they also made it end there. They never came back saying, ""Look, the results are interesting."" They basically left us alone with it, and I think this was really a pity. (Cust_1, Dyad 1)Such outcomes might be a result of the difficulties that the supplier experiences in its effort to develop solutions that truly match both customers' and the supplier's needs and expectations. This evidence about the exit scenario is also in line with empirical findings detecting a curvilinear impact of servitization on performance ([52]): before they can reap the lucrative payoffs of offering solutions, some actors become frustrated and forgo service investments prematurely. The Integration Phase Sharing knowledge while avoiding appropriation hazards: knowledge-based boundary objectsIf the first decision scenario applies, actors enter the second phase of solution development, which we call integration. Due to the accumulation of mutual commitment pledges from the experimentation phase, actors are now willing to engage in deep coproduction practices implied in solution development. However, such practices require actors to interact in the presence of a new exchange condition, namely, interdependence. Each actor is dependent on the other to achieve solution goals, and mutual (vs. individual) interests should guide their joint value–creating relationships ([41]). The quotes citing the emergence of interdependence in this phase of solution development acknowledge that single actors' activities are instrumental to the achievements of the wider system, and their strategic assets (e.g., tacit knowledge) must be exchanged and integrated for this purpose:I don't think we would completely avoid [knowledge sharing], even if we could, because you don't go anywhere without the supplier. (Cust_4, Dyad 4)It is crucial that we share with our customers as much as we know about the problem, and as much as we know about what we consider important about it, and our customers should do exactly the same with us. This way communication works, this way innovation can be achieved. (Sup_4, Dyad 6)There are several suppliers to whom we grant deep access to our core knowledge...to the point that, in some cases, we even bring our knowledge to their premises for joint initiatives. (Cust_8, Dyad 8)Yet the condition of interdependence also raises a governance tension between the opportunity for knowledge integration and the incentive to pursue knowledge appropriation ([53]). Integrating the knowledge bases would be in the interest of the wider system, but the involved actors may refrain from doing so. First, knowledge assets constitute sensitive information in tacit form that is difficult to exchange, and their transfer may generate opportunism and exploitation risks ([69]). Second, because tacit knowledge is a core asset, each actor also might try to retain it, to limit its dependence on its counterpart ([41]). The following quotes highlight this tension:It is inevitable that [suppliers] get in touch with some sensitive information of ours. You can limit it, but you cannot eliminate it; otherwise, the whole thing doesn't work. (Cust_4, Dyad 4)Yes, they could learn a lot about our product, by testing our product and developing something to test our product. I would say, that they definitely learn....So, it is a concern for us, and because of that we share only the confidential information that we feel is needed, that they need to know, and we try not to share anything more than that. (Cust_15, Dyad 15)To address this governance tension, the actors in our sample generate boundary objects, or artifacts of various types that reflect embedded knowledge, such as codeveloped machines, shared layouts, trained employees, or metric dashboards ([10]). Boundary objects are intended to facilitate information exchanges related to knowledge assets by establishing shared meaning for them, which is crucial for solution development, because providers and customers usually interpret solution content differently ([65]). The examples cited in the following quotes reflect these efforts:We controlled all the aspects in the workflow and we decided to work with this supplier in our process, and since that time, it has ended in a kind of shared road map, implementing and developing new systems. It's been kind of a shared vision...that has been, I guess, three-and-a-half years now. (Cust_15, Dyad 15)We started a tight collaboration [with the customer] aimed at developing an injection measurement system for a car's engine together. While we invented the solution, we decided to develop and implement the application together with the customer within our Kite lab and work on it jointly for further improvement. (Sup_2, Dyad 2)We jointly developed a chemotherapy mixing machine with the main local hospital, and when it was still a prototype, it was hosted in their facilities and further jointly developed from there. (Sup_11, Dyad 15)These uses of boundary objects constitute a relational governance mechanism: the exchange of knowledge assets is difficult to codify in advance and cannot be safeguarded by contractual mechanisms, so joint responsibility is fundamental to solution coproduction ([33]). In this phase, boundary objects are effective because they leverage so-called mutuality norms, or a sense that each party's success is a function of everyone's success and that one party cannot prosper at the expense of another ([ 8]):We almost are required to interchange with the supplier. You cannot grow without your supplier, and the growth is joint. Thus, it is obvious that I have to share information, so then in the end it leads to joint profits. (Cust_9, Dyad 9)Furthermore, boundary objects also are important because they help align incentives and facilitate perspective taking, such that each actor imagines the other's viewpoint ([43]). By ""walking a mile in others' shoes,"" the parties can identify different views on goals and behaviors and align them better. If realized, perspective taking provides a basis for adopting a stewardship logic, devoted to maximizing the goals of the counterpart ([13]):What I think is also very important is to have someone who looks at the tasks from a different perspective and asks important questions, which also help [in] achieving a bigger picture of the topic. Taking this other perspective is extremely helpful for me, and we need to foster this. (Cust_6, Dyad 6)Some people of the business units now think more like clients than like us....Now they are into their system. (Sup_9, Dyad 13)It's a mission for us to wear the customer's shoes and understand how he consumes [energy], how it works, what its processes look like....If he spends 100,000 euros for electricity, I should ask myself, what could I save if I were him? (Sup_2, Dyad 3)Perspective taking and the stewardship logic both support a more balanced power relationship, avoiding a situation in which one actor might feel too dependent on the other. Sharing knowledge while avoiding appropriation hazards: rights allocation agreementsKnowledge integration is truly challenging in solution development, because it entails a long-term commitment to exchange core assets, which promises high benefits (gains from innovative solutions) as well as risks (exploitation of knowledge assets) and potentially unpredicted outcomes (reallocation of power among actors). To handle the governance tension associated with integrating versus appropriating knowledge, several dyads in our sample thus complement their uses of mutuality norms and boundary objects with contractual mechanisms aimed to safeguard against potential exploitation of core competences and maintain balanced power relationships:When we and our client share core knowledge, we both negotiate in a detailed contract the conditions under which the know-how of one actor can be used by the counterpart, inside and outside the solution partnership (Sup_4, Dyad 6)When we engage in the codevelopment of particularly innovative solutions with clients, we involve lawyers and managers to write down in a contract the ownership rules regarding the joint outcomes of the project. (Supplier's lawyer)We have confidentiality agreements with few suppliers, and so we feel like we can trust them and so invest in them...not only monetarily, but technology-wise. (Cust_11, Dyad 11)The use of multiple governance mechanisms (in this case, combining the use of contracts with boundary objects based on relational norms) to address the same tension (integrating vs. retaining knowledge) is consistent with the principle of plural governance forms ([ 6]), according to which single mechanisms are not always traded off but rather can be combined to handle challenging governance tensions like those that arise in the integration phase of solution development. These contracts also are more complex than those associated with colocation initiatives and used at the onset of solution development. Although complex contracts remain incomplete, their use can ""deter behaviors that could compromise the performance of a buyer–supplier exchange"" ([56], p. 709). In our case setting, such contracts provide guidance for determining rights allocations among the solution actors. First, they establish the ownership of the outcomes jointly developed in the solutions context, with a goal of maintaining the balanced relationship. Second, the contracts define the know-how that remains under the control of each individual actor, the use of which requires permission, so they provide additional safeguards against the exploitation of core assets. It is worth noting that, because they are specific to each dyad, the details of the rights allocation agreements are highly customized and negotiated between solution actors. Second decision point: the role of power balanceThe integration phase of solution development is thus characterized by the emergence of interdependence. Our data reveal that this interdependence can evolve in two ways, leading to different solutions trajectories, as depicted in the center of Figure 1. In one case, actors' interdependence grows in a balanced fashion, so the relationship evolves symmetrically ([62]). In this case, the actors continue to coinvest in solutions, and the development trajectory enters a third phase called evolution (see the next section):After five years, ours is an equal relationship; there is even a friendship. This relationship of reciprocal safety and transparency made new business opportunities and solution evolutions possible. (Sup_12, Dyad 16)In another case, interdependence evolves in an unbalanced way, and one actor becomes significantly less powerful and more dependent on the other. Such dependence asymmetry negatively influences business relations, by fostering coercive uses of power and reducing the actors' willingness to compromise ([25]; [41]). The emergence of this power unbalance evokes two possible outcomes. First, the more powerful actor may decide to manage the relationship using hierarchical or bureaucratic mechanisms, even to the extreme level of a quasi- or full vertical integration:So, we started working as an armed branch of [company X], in the sense that they got the order and we managed it. Then in 2005 we detached ourselves from them and started moving our own steps....So, what we did was to go to [company X's] clients and asked them to come with us. (Sup_8, Dyad 12)Second, the less powerful actor may feel dissatisfied and worried about the unbalanced evolution of the relationship, to the point that it considers reducing the effort it devotes to the solution process, reverting to an exchange that resembles a market-like relation (both possibilities are represented by the second downward dotted line in Figure 1):This was a question I asked to myself silently: ""Is the supplier's aim to sell me the solution now and to take the business away from me, becoming a competitor of mine in two years? Basically, I would be nourishing a competitor of mine, then....I see a big quo vadis in this, but to some extent also dynamite. (Cust_16, Dyad 16) The Evolution Phase Balancing proactivity and adaptation: liaison championsIf the first decision scenario from the previous section applies, actors that have grown interdependent in a balanced way can enter the third phase of solution development, which we call evolution. The codevelopment efforts of the supplier and customer have evolved through a symmetrical relationship that has enabled both of them to benefit. At this point, they mutually expect to perpetuate their joint efforts over time to exploit the full potential of solutions. However, their long-term, forward-looking perspective exposes these actors to the new arising condition of unpredictability. Future exchanges among these solution actors may be exposed to unforeseen changes in upstream and/or downstream market conditions, which ""reflect the tug and pull of new contingencies and participants"" ([60], p. 69). Such changes could be beneficial or could leave the joint efforts for solution development obsolete:Technology can change our customer's priorities and then we have to adapt, too. (Sup_3, Dyads 4 and 5)We learn something new daily, and if my supplier is very strict...it will be more difficult for me to satisfy my own customers, and the quality of the project in the end will be not so good. (Cust_10, Dyad 10)These unpredictable conditions generate a new governance tension in this phase, in that actors must balance adaptation and proactivity attitudes to handle future contingencies. On the one side, uncertainty requires adaptive routines ([26]) to guarantee flexibility in the evolution of the solution development process. In the first of the following quotes, an informant from the supplier side explicitly refers to the importance of being responsive to customer requests, particularly when the process is already advanced, as also confirmed by the customer in the second quote:We go to our customers and say ""So, you need test equipment. Do you have a budget? Should you not have it, we can still do it for you, don't worry: we make our spaces, people, and technologies available for you to do testing activities."" (Sup_1, Dyad 2)We like to do this [being flexible] and to have that flexibility also from the supplier: we like the idea of having something that you can adapt. (Cust_11, Dyad 11)On the other side, unpredictability requires a proactive attitude, which enables actors to anticipate future trends and act even in the presence of partial information and weak signals ([14]). Such a forward-thinking, out-of-the-box approach also gives solution actors a signal that their counterpart cares about the long-term relationship and is willing to resolve problems:We have to be good at understanding that we cannot live only for the present and the current demand, but we have to anticipate needs and ask ourselves questions about what can be useful for the customer in the future....This also requires changing communication schemes. (Sup_4, Dyad 6)It is always nice when your [supplier's] colleagues add some ideas that go beyond the actual commissioning. Our arms are always wide open here. Very often this leads to subsequent commissions. Ideas for discussion and suggestions are always welcome. (Cust_6, Dyad 6)When we hired some Brazilian engineers, did [the customer] ask us to do so? No! We did more; we asked him to come with us to a Brazilian university and select the person that we were going to hire. So we had a person coming from the same culture and speaking the same language [of the customer]. (Sup_4, Dyad 6).Yet because proactivity focuses on anticipating future conditions while adaptation relies on responding to present conditions, considering them simultaneously is difficult and generates another governance tension. From one side, proactivity provides early mover advantages but generates rigidities, because actors commit to premature market conditions, which can make later adaptation difficult ([42]). From another side, adaptation fosters flexibility and avoids rigidities but reduces strategic foresight, thus constraining actors to think and act within the existing market conditions ([14]). The following quotes reveal this tension:We should try to be a bit more proactive sometimes, but at the same time we cannot go in a direction where the customer feels we are not responsive to its needs and will take the projects away from us. (Sup_3, Dyads 4 and 5)It is a pity because sometimes we are very proactive in proposing some [new] solutions to the customer, but he perceives them as necessary and useful only when it is too late. (Sup_6, Dyad 8)I think as a supplier sometimes you might get the perception that the market is not interested in that [solution innovation]—and maybe the market is really not interested in that....This might be because you have a very risk-adverse customer base that likes a lot of the [existing] competences. (Cust_15, Dyad 15)The solution actors in our sample try to address this governance tension by establishing an executive liaison champion, an organizational role that ensures sufficient agency to take proactive decisions and enough vision to adapt to the actual relationship context. This empowered champion is different from a key account manager, which mostly handles relationship uncertainty from the seller side ([66]). Liaison managers might be employed by the supplier, the customer, or both and have enough agency to anticipate changes and make flexible decisions, thus acting as a bilateral governance mechanism:We are trying to privilege more the relational rather than economic aspects, so there is the need to have people who are also smart in understanding the client's [needs] instead of [simply] having a technical key account. (Sup_1, Dyad 2)What has been crucial was the creation of a team where there were two project leaders—one for the supplier, and this was me, and one for the client. These involved regularly some industry experts, but also gave continuity to the project from a relational standpoint, for example, meetings for updates, tests, and further developments, as well as continuous engineering. (Cust_12, Dyad 16)A liaison champion thus is an additional relational mechanism that solution actors use in the evolution phase to leverage flexibility and solidarity norms. It helps actors consider agreements as starting points that can be modified as the market, exchange relationship, and fortunes of the parties evolve. Moreover, it facilitates actors' alignment even in the face of adversity and the ups and downs of marketplace competition ([ 8]):So often I need you to be very flexible and fast-responding to my requests, because we are working on the product and on the process at the same time. (Cust_10, Dyad 10)Over time, the interplay of adaptation and proactivity produces an ongoing process of institutional alignment, involving both innovative activities and mundane adjustments ([27]). An informant from the supplier company describes this long-term process using a marriage metaphor:When you are engaged with somebody you focus on being liked by the other half, on reciprocal fit, and on all those things you need to do to please the other person. There is way more diplomacy, because the focus is on starting a long-term relationship. So you know you have to avoid certain behaviors; otherwise you are out. When you get married, it is a different story, because priorities change: when the relationship is established, your focus moves from liking each other to solving common problems, that is, from aesthetics to effectiveness. (Sup_3, Dyads 4 and 5) Governance Mechanisms: Theoretical ReconciliationThe key takeaway of our analysis is that solution development evolves through different phases (experimentation, integration, and evolution), characterized by specific exchange conditions that may expose actors to various governance tensions. To address such evolving tensions, actors use a series of different mechanisms that change over the course of solution development.Consistent with clinical case technique principles, we now reconcile the key empirical findings (i.e., mechanisms) with theoretical knowledge on governance, to gain a deeper understanding of the role of specific mechanisms and reveal how governance matching exemplifies the discriminating alignment principle. Table 6 summarizes the outcome of this effort. In accordance with governance literature ([24]), we classify each mechanism first according to its intended function (safeguard vs. coordination) and the form it takes (contractual vs. relational). Then, integrating various streams of literature, we discuss precisely how each mechanism addresses the specific tensions emerging in solutions development.GraphTable 6. Governance Mechanisms for Solutions: A Theoretical Reconciliation.  In the first phase (experimentation), actors use both relational and contractual governance mechanisms to safeguard from two different exchange threats, with the final effect of delimiting risk in the new exchange environment. Temporary asset colocation helps address opportunism (vertical) threats by restricting the time and investments required in the first solution trials, thus ensuring temporal asset specificity ([46]) and affordable loss conditions ([58]). Network closure instead aims to address competition (horizontal) threats, by promoting a collective, noncompetitive climate that ensures participation and social monitoring conditions in early solution exchanges ([31]). Temporary asset colocation can be implemented through simple contracts; network closure represents a relational governance form that leverages harmonization norms.In the second phase (integration), actors still use contractual and relational governance mechanisms but, in this case, to coordinate the goals and exchanges of intangible assets, in addition to safeguarding actors from further exchange threats ([69]). The effect is to facilitate the core, challenging process of knowledge sharing, characterized by deeper cooperation efforts but with potentially misaligned incentives. The development of boundary objects helps align actors and facilitate the integration of tacit knowledge ([10]), while the use of rights allocation agreements ensures that knowledge integration occurs in a context of shared value appropriation rules ([56]). Boundary objects represent relational forms leveraging on mutuality norms. Rights allocation agreements are implemented through complex negotiated contracts.In the third phase (evolution), actors use one specific relational governance mechanism, which helps them coordinate their behaviors in an exchange context marked by future uncertainties, with the final effect of stimulating the coexistence of adaptation and proactivity inclinations ([14]; [26]). Establishing a liaison champion role helps actors understand when to remain responsive to their counterpart's requests and when to act to anticipate the market. This relational governance form leverages flexibility and solidarity norms.Table 6 suggests that efficient governance matching requires a contingent, dynamic approach, possibly involving several forms of governance mechanisms to achieve safeguarding and/or coordination functions whose salience changes across various solution development phases. The complexity of this contingent and dynamic approach can be appreciated in various ways. First, ensuring safeguard in the first phase requires the use of both relational and contractual mechanisms because the threats are different. Second, the integration phase requires paying attention to safeguarding and coordination, and actors need to use both contractual and relational mechanisms to achieve coordination while the contractual form also must ensure safeguard. Third, the evolution phase requires a focus on certain coordination issues that can be addressed only by using a specific relational form of governance. DiscussionTo the best of our knowledge, this study is the first to investigate solution development systematically as a governance problem. In this light, our analysis provides two main contributions with various implications for marketing literature and practice.First, we identify key components of the governance matching sequence (conditions arising, tensions, and mechanisms) that characterize solution development. Being characterized by intense coproduction efforts and changes in the relational habits, solutions introduce actors to new exchange conditions, such as role shifting, indirect links to rivals, interdependence, and unpredictability. Such conditions generate a series of tensions among actors: collaborating while avoiding opportunism, seizing opportunities while eluding information leakage, integrating versus retaining core knowledge assets, and balancing proactivity and adaptation. Our analysis reveals that success in solution development requires the actors to address the tensions with a heterogeneous mix of contractual and relational governance mechanisms—namely, temporary colocation contracts, closed-network relational initiatives, boundary objects for knowledge sharing, rights allocation agreements, and liaison champions. A smooth and successful solution development process thus requires ongoing and careful governance matching efforts.Second, we reveal the dynamic nature of governance matching efforts in solutions, whose development evolves through different phases separated by key decision points. In the experimentation phase, governance matching aims to safeguard actors placed in a new exchange context, by delimiting their risks and promoting a collaborative climate. In the integration phase, it instead focuses on facilitating knowledge sharing practices by coordinating actors' goals and actions while safeguarding them from opportunism. In the evolution phase, the role of governance matching shifts to promote a forward adaptation attitude among actors, coordinating their activities in the presence of unforeseen contingencies. Moving from one phase to the other is not automatic. If actors do not feel mutual commitment at the end of the first phase, solutions remain embryonic, and market-like exchanges prevail. If actors sense a power imbalance at the end of the second phase, solution development may stall or revert, because either the weaker actor prefers to engage in market-like exchanges or the stronger one leans toward vertical integration. Theoretical ImplicationsThe aforementioned contributions generate several implications for marketing literature. In particular, we complement existing literature on solution development that mainly focuses on actors' organizational resources; we show that solution development implies a careful negotiated process at the dyadic level, in which actors suffer limited freedom and cannot unilaterally execute a strategy with only their own resources and competences. Achieving progress in solution development thus requires setting governance mechanisms that align properly with new exchange conditions in the solutions context. Our findings introduce the central role of governance matching in ensuring a successful solution development process.As a further extension of extant literature on solutions, we reveal that their development requires a complex contingent approach to governance matching. The efficacy of single mechanisms is specific to solution phases, and in each phase, mechanisms of different forms combine to address complex emerging tensions. In this sense, our study reveals how to implement proper governance matching to ensure a successful solution development process. Such findings provide a middle-range extension of literature on the plurality of governance forms ([ 9]; [70]) in the solutions context.Due to the complexity of governance matching, solution development also cannot be taken for granted. In this domain, our findings contribute to existing solutions literature by revealing why some development efforts evolve smoothly, while others halt (lack of mutual commitment) or, after a promising start, stall or revert to deservitization (unbalanced power). A dynamic perspective on business exchanges has long been recommended ([15]) but is rarely applied to solution development, despite recent calls ([38]). Our multipath framework in Figure 1 offers a dynamic perspective and also responds to broader calls for dynamic models that outline how business relationships evolve in nonlinear fashions in specific contexts ([35]; [75]), such as those in which solutions are expected to evolve.In summary, this study contributes to existing solutions literature by detailing The relevant but overlooked role of governance matching in solution development. What it means for actors to execute the challenging, evolving activity of governance matching. What happens when governance matching is not achieved.Some of these findings also inform broader literature streams pertaining to the plurality of governance forms and dynamic models of business relationships. Managerial ImplicationsDespite its qualitative nature, our study features heterogeneous markets, includes a broad sampling, and is characterized by a managerial focus, so that it can produce some practical implications for solution developers. At the managerial level, the core insight of our study is that the main goal for solution actors is to systematically resolve, with proper governance mechanisms, the evolving tensions that originate from solution exchanges' specific features. Developing firm-level capabilities is not enough to succeed, because solutions do not ""naturally"" evolve, and without ensuring a complex activity of governance matching, actors do not have the proper incentives to engage in the bilateral value creation activities that are at the heart of solution development.As a first implication from this major insight, our investigation provides practical guidelines to properly execute the governance matching activity, which consists of a series of ""governance engineering"" tasks ([21]) that actors should include in their managerial agenda. The first of these tasks is monitoring the exchange conditions, which means systematically checking the status of the relation with the solution counterpart. Some (successful) dyads in our sample stress the importance of being always vigilant on tracking the changing conditions that may characterize the relationship with the solution counterpart and create various occasions of interaction to this purpose:We need to continuously monitor how it is going with the client: the challenge is to getting used to examine and reexamine the state of the relationship, which may vary frequently and suddenly. (Sup_4, Dyad 6)To understand the evolution of an important relationship we organize periodic customer-specific initiatives called ""open days,"" in which we invite our partner, and sometimes its clients, and talk about everything may concern current and future exchanges. (Sup_4, Dyads 4 and 5)Being vigilant is important for solution developers, but our results indicate that when tensions inevitably emerge, actors need to succeed in a second governance engineering task: designing a coherent set of mechanisms. As described previously, this goes beyond engaging in the simple choice among single mechanisms by trading off their individual characteristics, but reflects the capability to design proper combinations of contractual and relational mechanisms to handle the complex and evolving tensions that characterize solutions development. Executing design tasks requires actors to mobilize, and sometimes combine, the various types of knowledge (technical, managerial, and legal) necessary to ensure governance matching across the various phases of solution development. The following quote reflects this circumstance.Not being able to manage conflicts and mediate positions with your solution counterpart is often a problem. Ideally, who manages the relation should possess some technical knowledge, a business/economics attitude, and legal expertise: sometimes you need a team. (Sup_1, Dyads 1 and 2)Our results also indicate that the tasks of being vigilant and designing mechanisms should be put in a dynamic perspective: solution developers need to periodically assess the development process, evaluate its progress, and take actions accordingly. Our framework/road map depicted in Figure 1 points out two criteria, mutual commitment and balanced power, which can concretely help solution actors to decide whether to engage in the next steps of the development process because without achieving governance matching, continuing the solution development may not be in the best interest of the actors. The importance of developing a managerial tool to ensure dynamic assessment and informed decisions concerning solution development has also been recalled by our informants:We and our clients would love to have a framework, a dashboard to systematically evaluate our behaviors in these innovative solution projects, to learn where each of us excels and especially what is going wrong...in order to learn from the past and be able to replicate. (Sup_3, Dyads 4 and 5).In addition to the development of governance engineering tasks, a second practical implication of our study regards the role of existing ties in implementing governance matching. Solution actors face radical changes in their relational habits, because the start of the development process forces them to reframe the relational approaches they have used with business partners. Thus, contrary to the majority of cases in business relations, to succeed in solutions development, actors cannot rely on existing ties or capitalize on prior relationships; they even could act as obstacles to change. Rather, they must reestablish mechanisms to manage their relations from scratch. A quote from our third workshop with the supplier nicely depicts this need:We have noticed that it is easier for us to sell solutions to our new customers, because there is no preexisting standard and thus we can set up the relationship from the beginning, based on that business model. With our existing customers, instead, we face more difficulties since they prefer to stick to the schemes they already know and want to continue with the ""traditional"" supply. (Manager of the supplier company).Accordingly, to succeed in solution development, managers should adopt a learning-by-doing approach, characterized by trials and real-time adjustments to deal with the transient nature of the benefits associated with governance mechanisms. As described previously, learning by doing implies that actors engage in periodic structural decisions about whether to continue the solutions development, depending on the extent to which some mutual, intermediate outcomes have been reached. Thus, evolving towards solution offers is by no means a ""no brainer,"" as some observers still speculate ([55]), and actors' agency mainly determines why some solutions never take off, others stall after a promising start, and still others revert to deservitization.In summary, from a managerial standpoint, solution development requires actors to ( 1) include governance engineering activities (monitor exchange conditions, design sets of mechanisms, and assess the development process) in their managerial toolkit to achieve the challenging governance matching process and ( 2) modify their approach to business relations by overcoming existing ties and establishing new rules for relationships, in a learning-by-doing fashion. Limitations and Further ResearchAs with any qualitative inquiry, this study is limited in its capability to support empirical generalizations. Our purposive sample represents a wide range of markets, yet it was dominated by representatives of the automotive industry, so our results can be generalized only theoretically ([74]). Large-scale, quantitative studies should try to test the validity of our findings regarding the role of aligned governance mechanisms. For example, studies might investigate if boundary objects actually facilitate knowledge sharing or if closed-network participation really contributes to opportunity seizing. With our purposive sampling logic, the results also provide only preliminary insights into change and transition processes. The vast time required for such processes means that investigating these aspects would demand a detailed, longitudinal case analysis, which could add new, micro-level evidence of the shifts from one phase to another.Another limitation stems from the contingent value of our findings, which may be highly idiosyncratic to each solution case and its specific exchange features. Other relationship contexts, less characterized by high coproduction or changes in relational habits, may require different (simpler) governance matching activities. Some of the mechanisms we identify also might be redundant or even inefficient. For example, not all vertical relations expose customers to potentially harmful indirect links to rivals; network closure mechanisms likely would not be necessary if the supply network were sparse. Similarly, boundary objects are critical when knowledge assets are substantially involved, but exchanges mainly based on products or components may not need this governance mechanism and instead could be ruled by simple contracts that detail the economic terms and nondisclosure agreements. In contexts in which exchanges are relatively repetitive, a key account manager might be sufficient to manage the main issues as they evolve. To this point, it would be interesting to extend our analysis to other empirical settings, such as those characterized by different supply chain structures or distinct governance problems. "
29,"Effect of Alliance Network Asymmetry on Firm Performance and Risk Persistent high failure rates of new product alliances call for identification of factors that might improve alliance outcomes. In this research, the authors identify two attributes of alliance network asymmetry that affect alliance performance and performance uncertainty: differences in the number of prealliance direct ties, which can create asymmetry in the volume of resources of the two firms, and differences in the interconnectivity among prealliance indirect ties, which leads the firms to possess different types of resources. The authors theorize that absolute levels of such asymmetries have curvilinear effects on alliance performance and performance uncertainty, which materialize as a focal firm's abnormal returns and risk, respectively. They demonstrate that direct tie asymmetry has an inverted U-shaped effect on the focal firm's abnormal returns and a U-shaped effect on its risk. Indirect tie asymmetry also has a U-shaped effect on the focal firm's risk. However, the focal firm's innovation quality and preexisting ties with its partner flatten these curvilinear effects. The findings have implications for partner selection in new product alliances.KEYWORDS_SPLITMore than 50% of new product alliances fail to achieve their desired objectives (e.g., [20]; [38]; [46]). In consideration of such high numbers, scholars have examined factors that improve chances of alliance success (e.g., [23]; [65]). Yet the persistent high failure rates affirm the continued relevance of research that identifies specific factors that might improve the likelihood of alliance success. In this study of new product alliances, we use the theoretical lens of interdependence asymmetry ([49]) to identify prealliance network ties between a firm (hereinafter ""focal firm"") and its alliance partner (hereinafter ""partner firm"") as important to alliance performance and uncertainty, which are established metrics of alliance success ([42]; [98]).We focus on two attributes of prealliance network ties: direct tie asymmetry, which we define as the absolute difference in the number of direct ties between two firms forming an alliance relationship, and indirect tie asymmetry, which we define as the absolute difference in the interconnectivity of indirect ties between two firms forming an alliance relationship. Our use of asymmetry as an absolute difference is much in line with the findings in prior empirical studies on interfirm interdependence asymmetry. Most studies that consider directional levels of asymmetry conclusively show that asymmetry in interfirm relationships affects the fundamental relationship and therefore triggers similar attitudes and behaviors from both firms, regardless of which firm the asymmetry favors (e.g., [36]; [48], [49]; [87]). Other studies have examined absolute levels of interdependence asymmetry and concluded that asymmetry affects the relationship, and therefore both firms react similarly to the asymmetry (e.g., [30]). Thus, in our context of alliance relationships, our definitions of direct and indirect tie asymmetry in absolute rather than directional terms are appropriate. We also provide multiple robustness tests in subsequent sections to establish face validity of absolute levels versus directional levels of asymmetry.First, we discuss direct tie asymmetry. The number of a firm's direct ties correlates strongly with the volume of resources the firm can access from its network. These resources mainly comprise institutional knowledge resources dominant in the network ([40]) and relational resources, such as social legitimacy and attractiveness as a potential alliance partner ([ 1]). For example, in the network of alliances presented in Figure 1, Boehringer Ingelheim has more direct ties than its partner Retractable Technologies. Thus, relative to its partner, Boehringer Ingelheim not only has access to more institutional knowledge from the network, due to direct access to a greater number of alliance partners, but also has more social visibility, which makes it a more attractive potential partner to many other firms in the network.Graph: Figure 1. Network asymmetries between Retractable Technologies and Boehringer Ingelheim.Second, in addition to direct ties, we acknowledge the strategic importance of firms' indirect ties (i.e., partners' partners) (e.g., [ 9]; [23]). The strategic nature of a firm's indirect ties stems from their interconnectivity ([89]). If indirect ties are closely interconnected, fewer disconnected clusters are in the firm's network. Such a position in the network has benefits similar to that of a highly interconnected network position (i.e., access to institutional knowledge resources in the network); it also has costs, such as limited access to breakthrough knowledge resources ([ 9]; [28]), which are critical for breakthrough innovations. Given the strategic importance of interconnectivity among indirect ties, we examine how indirect tie asymmetry (i.e., the absolute difference in the extent of interconnectivity among indirect ties of two firms) at the time of alliance formation affects the alliance relationship. We provide an illustration of indirect tie asymmetry in a newly formed alliance between Retractable Technologies and Boehringer Ingelheim in Figure 1. While some indirect ties of Retractable Technologies are positioned in relatively disconnected clusters (e.g., Biovail Corporation, Thoratec Corporation), many indirect ties of Boehringer Ingelheim are located in highly interconnected clusters (e.g., Novo Nordisk, Schering-Plough). Thus, given their relative positions in the network, Retractable Technologies has access to breakthrough knowledge resources, while Boehringer Ingelheim has access to institutional or dominant knowledge resources.In this study, we are theoretically interested in how direct and indirect tie asymmetry in the alliance create interdependence asymmetry in the relationship. Direct tie asymmetry creates interdependence asymmetry in the alliance because one firm possesses a higher volume of resources than the other. However, indirect tie asymmetry creates interdependence asymmetry in the alliance for different types of resources (i.e., a firm with institutional knowledge resources depends on the partner for breakthrough knowledge resources, and vice versa). Interdependence asymmetry in the relationship offers both benefits and costs to the alliance. On the one hand, interdependence asymmetry creates resource reliance between a focal firm and its partner, which in turn facilitates coordination and mutual commitment that not only increase alliance performance (e.g., [70]) but also reduce uncertainties of alliance performance (e.g., [16]). On the other hand, too much interdependence asymmetry may create power imbalance, fostering mistrust and a lack of commitment between the firms ([30]) and thereby jeopardizing alliance performance and making it unstable (e.g., [32]). As alliance performance will naturally affect the extent to which the alliance generates financial returns for a focal firm, we use the focal firm's financial performance as our first dependent variable. In addition, uncertainty in alliance performance will manifest itself in the volatility of the focal firm's financial returns from the alliance. Thus, we use the focal firm's financial performance uncertainty as our second dependent variable.In summary, we argue that both direct and indirect tie asymmetry are sources of benefits and costs in an alliance, the former due to differences in the volume of resources and the latter due to differences in the type or nature of resources to which each firm in the alliance has access. Corroborating the presence of both benefits and costs of direct and indirect tie asymmetry, our results indicate that direct tie asymmetry has an inverted U-shaped effect on the focal firm's financial performance, and both direct and indirect tie asymmetry have U-shaped effects on the focal firm's financial performance uncertainty. We also identify important contingencies that can have an impact on the effects of direct and indirect tie asymmetry. We propose that the focal firm's motivation to maintain the alliance relationship with the partner serves as a contingency. As long as the focal firm is motivated to maintain the alliance relationship with its partner, it will have incentives to mitigate the costs of the relationship from direct and indirect tie asymmetry, and consequently the effects of tie asymmetry on alliance outcomes will improve ([86]). The focal firm's motivation to maintain the alliance relationship might stem from the importance it gives to interfirm relationships as a source of learning (as measured by its innovation quality; [12]), as well as from the extent to which the firm and its partner have interdependencies outside the focal alliance due to preexisting ties (as measured by total interdependence of the focal firm and the partner; [45]). Our results indicate that innovation quality and total interdependence significantly moderate (i.e., flatten) the curvilinear effects of direct and indirect tie asymmetry.This research contributes to the marketing literature in three ways. First, we add to the strategic alliance literature that examines the effect of alliance attributes on the focal firm's performance (for a literature review, see Table 1). Whereas prior research has suggested that individual network ties influence a focal firm's performance (e.g., [65]; [98]), we contend that the network ties of both firms in the alliance should be considered simultaneously. At the level of the dyad, we can incorporate the notion of asymmetry in direct and indirect ties, the levels of which can significantly contribute to the success of the alliance. For example, when alliance success is measured in terms of market capitalization ($) of a focal firm, we find that the focal firm improves its market capitalization by $86.9 million by choosing an alliance partner with moderate (50th percentile) rather than low (25th percentile) asymmetry in direct ties. This improvement in market capitalization is significantly less (only $67.9 million) if the firm chooses an alliance partner with high (90th percentile) rather than low (25th percentile) direct tie asymmetry.GraphTable 1. Contributions to the Literature on Strategic Alliances and Interfirm Relationships.  Second, studies suggest that alliances are characterized by differences in firm size ([33]), financial resources ([104]), and innovation resources ([83]), all of which represent typical sources of interdependence asymmetries in alliances. Our research contributes to the strategic alliance literature by bringing a network perspective to the notion of interdependence asymmetry. Such a perspective has substantive implications for a focal firm in terms of its choice of an alliance partner. For example, in context of new product alliances in the biopharmaceutical industry, our results indicate that a focal firm with high innovation quality can gain $87.3 million and a focal firm with preexisting ties with the partner can gain $99.03 million in market capitalization, when considering a potential partner with which it has a moderate difference in direct ties (i.e., 50th percentile) versus another potential partner with which it has a low difference in direct ties (i.e., 25th percentile). Furthermore, a firm can reduce the financial risk of an alliance by 68.7% by choosing an alliance partner with which it has moderate (50th percentile) rather than low (25th percentile) indirect tie asymmetry. Thus, in terms of both financial returns and risk reduction, a focal firm may be able to improve its chances of alliance success by selecting partners with specific prealliance network asymmetry.Third, unlike many prior interfirm relationship studies that treat interdependence asymmetry as a directional construct (e.g., [87]), we show that absolute levels of interdependence asymmetry are meaningful because, regardless of which firm the direct and indirect tie asymmetries favor, it is the alliance that is affected. Although alliance performance is unobservable in our data, the focal firm's financial outcomes reflect the alliance's performance. Conceptual Framework and Hypotheses Effects of Direct Tie AsymmetryThe number of a firm's direct ties in a network directly correlates with the volume of resources it can access from its network, such as institutional knowledge ([26]; [40]) and social legitimacy ([ 1]; [79]). When the focal firm and the partner firm possess similar number of direct ties, they are in a symmetric position of interdependence for resource volume, as both firms are equally dependent on each other. A movement in either direction (toward the focal or partner firm) from this position of equivalence will create direct tie asymmetry. As we theorize subsequently, in either direction, direct tie asymmetry has benefits and costs for the alliance and consequently affects the focal firm's performance outcomes.As direct tie asymmetry increases moderately in either direction, interdependence asymmetry also increases moderately (i.e., one firm increasingly relies on the other for supplementing its lower supply of resources). With this resource reliance, the less resourceful firm may readily accept the other firm's expectations of resource sharing and alliance routines ([104]), which should facilitate its collaboration in the alliance to the benefit of the other firm. Such reliance should also strengthen cooperation in the alliance, because the less resourceful firm gains access to larger alliance networks with proportional resource advantages. As both firms stand to gain, increases in interdependence asymmetry due to direct tie asymmetry will benefit the alliance, thereby incentivizing both firms to commit to the alliance. Several studies have shown that if both firms are committed to an alliance, less miscommunication and more mutual cooperation occur ([61]; [72]), which should not only make the alliance stable (i.e., reduce the uncertainty in alliance performance) ([16]) but also increase alliance performance ([70]). Reduced uncertainty in alliance performance implies less volatility in returns from the alliance, which should naturally engender less financial performance uncertainty of the focal firm. Increases in alliance performance should increase the returns from the alliance for the focal firm, which should naturally yield greater financial performance for the focal firm.However, as interdependence asymmetry becomes increasingly skewed toward either firm, the resulting power imbalance may be high enough to give rise to costs at an increasing rate. As a result, the more powerful firm (regardless of whether it is the focal or the partner firm) may unfairly dictate ongoing alliance terms, knowing that the less powerful firm is more likely to acquiesce to its demands. It may coerce the less powerful firm to accept norms extended from its own network, even though they may not complement norms embedded in the less powerful firm's network. Furthermore, high power imbalance sets the stage for moral hazard on the part of the more powerful firm (i.e., appropriating benefits without proportional efforts) ([44]). Such behavior might provoke the less powerful firm's distrust and consequent lack of commitment to the alliance, invariably jeopardizing the performance of the alliance ([32]). Power imbalance also makes the alliance unstable, due to disruptive communication between the two firms and conflicting approaches toward alliance goals ([32]). Thus, regardless of whether the power imbalance favors the focal firm or the partner, the alliance is negatively affected. In other words, at high levels of direct tie asymmetry, the costs of interdependence asymmetry can outweigh the benefits for the alliance, thereby decreasing the focal firm's financial performance and increasing its financial performance uncertainty.A case in point is the new product alliance between Roche (a multinational pharmaceutical firm) and Trimeris (a domestic biotechnology firm) in 1999. Roche had a declining HIV (human immunodeficiency virus) pipeline, which it wanted to replenish using Trimeris's HIV-related expertise in Fuzeon technology. Although both firms were large, Roche clearly had more external resources, with approximately 150 research-and-development (R&D) collaborations. Trimeris, though a biotechnology firm with high internal R&D expertise, had fewer R&D collaborations at the time. Roche actively supported Trimeris in developing drugs in the Fuzeon portfolio until problems arose from declining profits of the portfolio in 2004. Roche unilaterally put a sudden hold on clinical developments in the Fuzeon portfolio after assessing Fuzeon's role in its own long-term performance. This decision put the alliance in jeopardy, and analysts began predicting the losses each firm might incur from a potentially failed alliance ([100]). Although collaborations were renewed almost completely on Roche's terms ([56]), this example reveals how both firms gained from interdependencies, but at the cost of power plays that adversely affected alliance performance.In summary, as the direct tie asymmetry increases, at first the focal firm's financial performance increases and related uncertainty decreases. However, after a certain threshold level, the effects reverse; the focal firm's financial performance decreases and related uncertainty increases. Thus, H1:  An increase in direct tie asymmetry leads to (a) an inverted U-shaped effect on the focal firm's financial performance and (b) a U-shaped effect on its financial performance uncertainty. Effects of Indirect Tie AsymmetryHigh interconnectedness of indirect ties of a firm indicates that the firm has access to abundant institutional knowledge resources embedded in a network (e.g., [ 9]; [28]). Institutional knowledge resources refer to overlapping technological know-how across most firms in the network ([77]) and to established norms of commercialization routines and innovation techniques in the network ([43]). By contrast, low interconnectedness of indirect ties indicates the presence of relatively disconnected firms in the network that are hubs of breakthrough knowledge ([ 9]; [28]). Moving to a dyadic perspective, when both the focal firm and the partner firm have similar interconnectedness among their indirect ties, access to institutional and breakthrough knowledge resources may also be similar. From this equivalent position, indirect tie asymmetry increases in either the direction of the focal firm or that of the partner firm.A moderate increase in indirect tie asymmetry should give one firm access to more institutional knowledge resources and the other more breakthrough knowledge resources. In this sense, a moderate increase in indirect tie asymmetry should strengthen the interdependence of the alliance, as each firm depends on the other for unique knowledge resources. Consequently, both the focal firm and the partner firm will expend effort to commit to the alliance. Mutual commitment leads to less miscommunication and uncertainty about each other's approach toward the alliance, thereby facilitating alliance stability and increasing alliance performance ([61]; [70]). In turn, greater alliance stability should lower volatility in alliance returns and consequently lessen the focal firm's financial performance uncertainty. Increased alliance performance should increase returns from the alliance, thereby increasing the focal firm's financial performance.The alliance between Sanofi (a multinational pharmaceutical firm) and Alnylam (a large biotechnology firm) initiated in 2014 is a case in point. Alnylam was developing breakthrough genomics innovations, and most pharmaceutical firms, including Sanofi, wanted a slice of the genomics market in 2014. We assume that the breakthrough nature of Alnylam's innovations partly resulted from its position in a network with few interfirm connections. This position is rather likely, as the alliance with Sanofi was one of Alnylam's initial R&D collaborations with a highly interconnected firm ([11]). Later in 2017, although the Food and Drug Administration suspended late-stage clinical trials, and prospects for a breakthrough hemophilia drug developed through the alliance seemed shaky, Sanofi kept its resource commitment and continued to actively back the alliance. This example shows how more interconnected, resourceful firms might be dependent on alliance partners that provide access to breakthrough knowledge resources.However, when the knowledge resources of both alliance partners are too dissimilar, the firms are unable to leverage each other's expertise because of a lack of common ground ([50]; [76]). For example, when the types of knowledge resources diverge substantially between the firms, the R&D department of the firm with more institutional knowledge resources is unable to understand or communicate effectively with the R&D department of the firm with more breakthrough knowledge resources ([14]; [101]). The resulting lack of knowledge assimilation leads to mutual dissatisfaction, uncertainty in alliance goals, and lack of cooperation ([75]). A classic case study is the alliance between Alza Corporation, a start-up with cutting-edge advanced drug delivery systems, and Ciba-Geigy, a multinational pharmaceutical firm. The divergent knowledge resources became the core problem between these two firms. In the first two years of the alliance, Ciba-Geigy could not understand Alza's path of technology development, and as a result, it objected to expenses devoted to the development of the path-breaking technology initiated by Alza. In turn, Alza complained about Ciba-Geigy's lack of understanding of the technology. The alliance formally terminated after five years ([39]).At high levels of indirect tie asymmetry between the focal firm and its partner, although the firm closest to highly interconnected clusters has substantial access to institutional knowledge resources, it may struggle to assimilate the breakthrough nature of knowledge resources of the partner firm. In turn, this struggle might lower its perceived value of the partner firm. Although the inability to assimilate breakthrough knowledge resources represents growth-related opportunity costs ([96]), immediate firm survival is rarely at stake because of buffers from institutional resources, such as dominant knowledge patterns and innovation protocols that facilitate product commercialization ([60]; [67]). By contrast, the firm with access to more breakthrough knowledge resources will have higher immediate opportunity costs if it loses access to institutional knowledge resources, which represent real-time lifelines for the survival of firms typically positioned in disconnected parts of the network ([47]). This scenario likely tilts interdependence asymmetry in favor of the firm with access to more institutional knowledge resources, which creates a power imbalance in the alliance. This imbalance has costs for the alliance, due to the more powerful firm losing interest and the resulting mistrust of the less powerful firm. Thus, costs from the increased likelihood of one-sided power plays and the consequent lack of mutual commitment rise at an increasing rate, leading to increased alliance instability and decreased alliance performance. Correspondingly, at high levels of indirect tie asymmetry, the alliance suffers because costs outweigh the benefits, thereby increasing the focal firm's financial performance uncertainty and decreasing its financial performance.In summary, as indirect tie asymmetry increases, at first the focal firm's financial performance uncertainty decreases and financial performance increases. However, after a threshold level, the effects reverse. Therefore, we propose a nonlinear relationship: H2:  An increase in indirect tie asymmetry leads to (a) an inverted U-shaped effect on the focal firm's financial performance and (b) a U-shaped effect on its financial performance uncertainty.We summarize the theoretical mechanisms for the main effects of direct tie asymmetry (H1) and indirect tie asymmetry (H2) in Table 2. We present the conceptual framework in Figure 2.Graph: Figure 2. Conceptual framework.GraphTable 2. Theoretical Mechanism for Main Effects (H1 and H2).   Moderating EffectsWe apply a basic principle of the interfirm relationship literature that, regardless of differences, a focal firm will actively cooperate in an interfirm exchange with a partner as long as it is motivated to maintain the relationship (e.g., [87]). This perspective suggests that the focal firm will likely cooperate (i.e., try to minimize costs of asymmetries), insofar as it is motivated to maintain the alliance relationship with the partner firm. In the case of high direct tie asymmetry, costs arise because the power imbalance created by large differences in resource volume ultimately results in mistrust in the alliance. In this scenario, if a focal firm (regardless of its resource volume) is motivated to maintain the alliance relationship, it will proactively seek ways to resolve the mistrust. Similarly, in the case of high indirect tie asymmetry, costs arise from the firms' inability to assimilate each other's unique knowledge resources. In this scenario, if the focal firm (regardless of its resource type—institutional or breakthrough knowledge) is motivated to maintain the alliance relationship, it will proactively resolve barriers to assimilating its partner's knowledge resources.We propose that two factors influence the focal firm's motivation to maintain the interfirm relationship in an alliance: innovation quality and total interdependence. First, firms that focus on innovation quality often depend on the exchange of complex and tacit know-how that is only possible in informal interfirm relationships ([13]). Firms with high (vs. low) innovation quality view interfirm relationships in alliances as critical to their innovation outcomes ([12]). Consequently, the motivation to maintain any alliance relationship should be greater in firms with high innovation quality. Second, total interdependence is the extent to which the firms contribute to each other's performance through preexisting ties beyond the specific alliance ([45]). Thus, the focal firm should be more motivated to maintain the alliance with the partner firm if it has other preexisting relationships outside this specific alliance. Innovation qualityInnovation quality refers to the extent to which knowledge generated by the focal firm is useful to future knowledge generation within and across domains ([51]). Firms that focus on innovation quality value external knowledge sources, especially informal interfirm relationships, as they are vital for the transfer of tacit external knowledge ([13]). The quality of a firm's innovation depends on the extent to which the firm recognizes the potential impact of external knowledge by interacting informally with alliance partners ([ 2]). Social relationships are the most critical ingredient in high-quality ideas ([ 7]). Firms with high innovation quality value informal learning from external knowledge sources more than firms with low innovation quality ([99]). By definition, informal learning from external sources requires taking time to develop informal relationships with other firms (e.g., alliance partners). As a result, a focal firm with high (vs. low) innovation quality is more likely to be motivated to maintain an alliance relationship and expend efforts to mitigate costs arising from direct and indirect tie asymmetries. Given higher cost mitigation, focal firms with high innovation quality are less likely to see the benefits from tie asymmetries taper off beyond a threshold. As a result, the nonlinear effects of direct and indirect tie asymmetries (H1 and H2) on the focal firm's performance outcomes are flattened when the firm has high (vs. low) innovation quality. H3:  As a focal firm's innovation quality increases, (a) the nonlinear effect of direct tie asymmetry on financial performance flattens, (b) the nonlinear effect of direct tie asymmetry on financial performance uncertainty flattens, (c) the nonlinear effect of indirect tie asymmetry on financial performance flattens, and (d) the nonlinear effect of indirect tie asymmetry on financial performance uncertainty flattens. Total interdependenceTotal interdependence refers to the extent to which the focal firm and the partner firm rely on each other, beyond the specific alliance, to achieve performance outcomes. Total interdependence reflects the overall extent to which two firms are enmeshed in each other ([45]) and can occur from multiple active alliance relationships and a high degree of mutual learning beyond the specific alliance. Total interdependence between firms serves as a relational platform that supports new exchanges between them ([64]; [87]). A breakdown of any single exchange can potentially cascade to all other exchanges between the firms. Thus, if the focal firm and the partner firm are interdependent outside the alliance, the focal firm should remain motivated to maintain this specific alliance relationship (i.e., the focal firm is more likely to mitigate costs arising from direct and indirect tie asymmetries to facilitate the alliance relationship). As total interdependence increases, due to higher cost mitigation, the focal firm is less likely to see the benefits from tie asymmetries taper off beyond a threshold. Thus, as the degree of total interdependence between the focal firm and the partner increases, the nonlinear effects of direct and indirect tie asymmetries (H1 and H2) on the focal firm's performance outcomes are flattened. H4:  As total interdependence between the focal firm and partner increases, (a) the nonlinear effect of direct tie asymmetry on financial performance flattens, (b) the nonlinear effect of direct tie asymmetry on financial performance uncertainty flattens, (c) the nonlinear effect of indirect tie asymmetry on financial performance flattens, and (d) the nonlinear effect of indirect tie asymmetry on financial performance uncertainty flattens. Methodology DataWe collected alliance data from Knowledge Express, which provides alliance information in the biopharmaceutical industry. This industry has high rates of alliance failure (58%) ([46]), but the value of interfirm networks is well acknowledged ([65]; [93]). The raw data in Knowledge Express include new product alliance announcements. Although the database reports termination dates, many alliances may go inactive without issuing formal notices of termination. To account for this possibility, we rely on alliance initiation dates to arrive at our final sample. For a network matrix at year t, we consider only alliances that initiated as far back as five years, thus assuming an average alliance duration of five years (e.g., [88]). The sample consists of 2,483 alliance observations, including 1,507 firms over 17 years (1997–2013).[ 6] Of the two firms in an alliance, we define a firm as focal if either it is the only public firm in the relationship (about 21% of observations) or, when both firms are public, the firm reportedly initiated the new product alliance in the announcement (about 7% of observations).We gather financial performance measures, financial performance uncertainty measures, and other financial information from COMPUSTAT and CRSP. We collect patent data from the U.S. Patent and Trademark Office. The final sample consists of 452 firms and 708 alliances. We provide descriptions and measures of all constructs in Table 3.GraphTable 3. Variable Description and Operationalization.   Measures Dependent variablesWe are interested in examining how direct and indirect asymmetry influence the financial performance and financial performance uncertainty of the focal firm after alliance formation. We measure the focal firm's financial performance and financial performance uncertainty after one year of alliance formation. We propose that direct and indirect tie asymmetries create interdependence asymmetry at the time of alliance initiation, with benefits and costs gradually materializing over the course of the alliance. Thus, a realistic period in which benefits and costs may play out to generate outcomes for the focal firm is the one-year mark after alliance initiation.Given this one-year time frame, we use one-year buy-and-hold abnormal returns (BHAR), which is a metric of long-term equity returns from an event. We compute this metric by subtracting the performance of a benchmark portfolio of stocks with similar risk profiles to those of the focal firm from the performance of a focal firm's stock. It is commonly used to quantify long-term performance metrics of different types of marketing relevant events, such as mergers and acquisitions ([91]) and sales takeoff ([66]). The advantage of BHAR over other long-term metrics of equity returns, such as calendar time portfolio, is that with the former, we can measure firm-specific abnormal returns, which is critical to test our theoretical framework, in a cross-sectional analysis ([92]). A typical measure of a firm's financial performance uncertainty is idiosyncratic risk ([62]; [78]), which can reflect at least 80% of a firm's total equity risk ([29]). We use the Fama–French three-factor model to construct the firm's idiosyncratic risk (for estimation details, see Web Appendix A).We test for several short-term and long-term metrics. We use cumulative abnormal returns (CARs) at 30, 60, and 180 days (after controlling for all other firm-specific announcements reported in the media in the period) and BHAR for durations longer than a year. We use firm risk over each of these durations as well. (We discuss details in the ""Postestimation and Robustness Tests"" subsection.) Independent variablesWe operationalize direct tie asymmetry as the absolute difference in number of alliance partners of the focal firm and the partner firm in the year preceding the alliance. Similarly, we operationalize indirect tie asymmetry as the absolute difference in network constraint in the year preceding the alliance. We use network constraint as the base for indirect tie asymmetry because it can capture the nature of knowledge resources potentially flowing to a firm. The network constraint measure (e.g., [ 5]; [28]) is a summary measure that captures the extent to which a firm's direct partners are connected with other firms that, in turn, are interconnected ([ 9]). Network constraint reflects knowledge redundancy of the firm due to a paucity of disconnected firms in its network.For example, in Figure 3, Panel A, the focal firm has three direct partners—A, B, and C—that are highly interconnected. However, the indirect partners—D, E, F, and G—are nominally connected. Thus, although the direct network is interconnected, the larger network to which the focal firm belongs has several relatively disconnected firms with corresponding breakthrough knowledge flows to which the focal firm may be privy. By contrast, consider the focal firm in Panel B of Figure 3. Here, Á, B´, and C´ are direct partners of the focal firm but are not connected with each other. As such, the focal firm may be perceived as having access to fewer institutional knowledge resources and more breakthrough knowledge resources; however, the lack of connections among direct partners in Panel C loses significance because of the high interconnectedness among indirect partners D´, E´, F´, and G´. Thus, although the direct partners are not interconnected, the high interconnectedness among indirect partners hinders the flow of breakthrough knowledge resources to the focal firm. Overall, then, in Panel C the focal firm has less access to breakthrough knowledge resources than the focal firm in Panel B and thus is more constrained in the network.Graph: Figure 3. Network constraint. ModeratorsAlliance formation and implementation begin in year t. Direct and indirect tie asymmetries represent prealliance or ex ante conditions (measured at t − 1) at the time of alliance formation between two firms. Ex ante attributes typically have enduring effects on the alliance and, as such, influence alliance outcomes ([73]). They also include our moderators, such that direct and indirect tie asymmetries at t − 1 juxtapose with the focal firm's innovation quality at t − 1 and total interdependence with the specific partner firm at t − 1 (for details, see Table 3). More specifically, we measure innovation quality of the focal firm using its citation-weighted patents ([51]; [53]) at t − 1. For total interdependence, we first calculate the ratio of the number of alliances between the two firms to the total number of alliances of each firm, and then add the ratio of each firm (e.g., [30]). Control variablesWe include variables that may ( 1) be empirically reflected by our independent variables but are conceptually different and/or ( 2) influence a focal firm's financial performance and related uncertainty. Using this logic for including control variables, we account first for the nature of alliances in the biopharmaceutical industry. In this industry, alliances are typically formed between ( 1) large and small firms ([42]), ( 2) firms with high and low financial resources ([71]), and ( 3) firms with high and low innovation output ([37]). Given that these prealliance differences (regardless of which firm they favor) may create interdependence asymmetry in an alliance, we need to account for them to ensure that our network-based measures of interdependence asymmetry do not reflect these typical sources of interdependence. Thus, we calculate the absolute levels of three asymmetry constructs using firm size, firm profits, and innovation output between the focal firm and the partner at t − 1. We also calculate the absolute difference in strategic emphasis at t − 1 because the firm that gives more relative importance to value creation may be more competent at fulfilling new product alliance goals. In addition, we include the absolute level of closeness centrality asymmetry as another dyadic measure that might correlate with one of our independent variables. Closeness centrality of a firm captures the average shortest distance between the focal firm and any other firm in the network. Closeness centrality and direct ties can serve as different dimensions of a focal firm's resource accessibility in the network ([81]).After accounting for dyadic differences, we turn to the focal firm's attributes that may directly affect both its current and future financial performance and financial performance uncertainty. We account for marketing absorptive capacity of the firm at t − 1 because the efficiency of translating external knowledge into marketable products is a critical cash-flow-generating capability of any firm ([17]). We include the focal firm's cash ratio at t − 1 to account for fundamental uncertainty in the firm's business operations ([ 4]). We also include firm leverage because the extent of debt a firm incurs influences its choice of strategies as well as financial outcomes (e.g., [62]). We include the focal firm's past alliance experience, as greater alliance experience proxies for better ability to manage any specific alliance ([61]). We also explicitly account for alliance management capability along the lines of [94]. Finally, we include dummy variables as required if major events, such as acquisitions, new alliance announcements, and top management turnover, occurred in the one-year window after the alliance initiation. Model and Estimation Basic model specificationWe denote the focal firm as i, the partner firm as j, their alliance as k, and the year of their alliance as t. We use the following general linear specification: DVkit+1=β0+β1DTAkt−1+β2DTAkt−12+β3ITAkt−1+β4ITAkt−12+β5IQit−1+β6TIkt−1+ β7DTAkt−1×IQit−1+β8DTAkt−12×IQit−1+β9ITAkt−1×IQit−1+β10ITAkt−12×IQit−1+β11DTAkt−1×TIkt−1+β12DTAkt−12×TIkt−1+β13ITAkt−1×TIkt−1+β14ITAkt−12×TIkt−1+ ∑c=15αc, CONTROLCONTROLkt−1, c+ ∑c=15βc, CONTROLCONTROLit−1, c+∑E=13ηit+1 + εit+1, dependent variable , Graph1where the dependent variable (DV) captures financial performance in one equation and financial performance uncertainty in a separate equation, DTA is the absolute level of direct tie asymmetry, ITA is the absolute level of indirect tie asymmetry, IQ is innovation quality of the focal firm, and TI is total interdependence between the focal firm and the partner firm. The first vector  CONTROLkt−1, c  includes the control variables at the alliance level, the second vector  CONTROLit−1, c  includes the control variables at the focal firm level, and η represents the indicator variables for major events of the focal firm. Finally,  εkit+1, dependent variabe  is the generic error term. Unobserved heterogeneityThe aggregate analyses (by pooling all firms) may mask variances in estimated effects because of the unique characteristics of each focal firm i. Many such characteristics are unobserved. Unobserved heterogeneity may lead to biased estimates. Thus, to capture the unobserved heterogeneity, we include fixed effects of focal firm  i τi.  We also account for time-specific fixed effects (  τt)  , given that exogenous events at a specific time may affect firms' strategic choices about an alliance initiation as well as performance. EndogeneityIt is possible that the asymmetry in network ties of firm i with firm j is a choice on the part of firm i as it builds its alliance portfolio and tries to adjust its financial performance and related uncertainty. The potential bias can be addressed by accounting for unobservable factors using the control function approach ([74]). More specifically, in Equation 1, we introduce a control function using ( 1) instruments that are likely to be correlated with the endogenous variables but not with the error terms and ( 2) exogenous variables from Equation 1. To obtain the control function, we first regress the direct and indirect tie asymmetries on the exogenous variables and the two instrumental variables and then regress the quadratic terms of asymmetries on the exogenous variables and the two instrumental variables and their quadratic terms, respectively. Next, we include the estimated errors from the four regressions (  e^DTA  ,  e^ITA  ,  e^DTA2  , and  e^ITA2  ) in Equation 1.We acknowledge that statistical tests cannot uncover suitable instrumental variables ([82]), though we should find variables that meet the relevance criterion and satisfy the exclusion restriction ([ 3]). For our two instrumental variables, first we identify a direct competitor of the focal firm by the closest similarities in two dimensions: ( 1) patent domains at t − 1 ([ 6]) and ( 2) R&D productivity at t − 1 (i.e., number of issued patents and R&D expense). Similarity in patent domains indicates that a direct competitor operates in the same marketplace as the focal firm. As new product alliances are based on R&D activities, a direct competitor should also be a firm that has similar learning outcomes, as captured by R&D productivity. Second, we measure the direct competitor's direct tie asymmetry and indirect tie asymmetry, and similar to the measures for the focal firm, both are absolute levels and averaged across the competitor's new product alliances at t − 1. Firms in the same industry are known to exhibit isomorphic tendencies in their alliance strategies ([35]). As a result, direct competitors' similar strategies might affect a focal firm's network strategies. In terms of the exclusion criterion, it is not practical to assume that a competitor's tie asymmetries will directly affect unobserved factors (e.g., the unobserved value of a potential partner firm) that influence the focal firm's performance dimensions. Rather, the competitor, through its tie asymmetries, may build its own R&D absorptive capacity, which in turn may affect the focal firm's competitive advantage and, therefore, financial performance and financial performance uncertainty. To account for this possibility, we include the direct competitor's R&D absorptive capacity (Competitor's RDCap) as an additional control variable in the second stage. Accounting for possible selection biasFirms that seek alliances may be systematically different from those that do not. We use Heckman's two-stage approach to account for the selection bias. In the first stage, we use the sample of all public firms in the industry, and then for each t, we create a dependent variable, which is 1 if a focal firm announced a strategic alliance at t and 0 otherwise. We then run a probit model using all exogenous variables from Equation 1. We incorporate a dummy variable that reflects whether a direct competitor (as defined previously) initiated an alliance at t − 1 to serve as an instrument. This variable affects the focal firm's decision to initiate an alliance at t but does not directly affect any unobservables that influence the focal firm's financial performance or financial performance uncertainty. Then, we calculate the inverse Mills ratio and include it in the second stage (Equation 2). The representative equation to be estimated is DVkit+1=β0+β1DTAkt−1+β2DTAkt−12+β3ITAkt−1+β4ITAkt−12+β5IQit−1+β6TIkt−1+β7DTAkt−1×IQit−1+β8DTAkt−12×IQit−1+β9ITAkt−1×IQit−1+β10ITAkt−12×IQit−1+β11DTAkt−1×TIkt−1+β12DTAkt−12×TIkt−1+β13ITAkt−1×TIkt−1+β14ITAkt−12×TIkt−1+ ∑c=15αc, CONTROLCONTROLkt−1, c+ ∑c=15βc, CONTROLCONTROLit−1, c+∑E=13ηit+1+ β15Competitor'sRDCapit−1+τt+τi+∑x=DTAx=ITA2e^x+ IMRit+εit+1, dependent variable, Graph2where  e^x  represents the vector of control functions (i.e.,  e^DTA  ,  e^ITA  ,  e^DTA2  , and  e^ITA2  ). Finally, we bootstrapped the errors (500 iterations) in Equation 2 (e.g., [74]). ResultsWe provide the descriptive statistics and correlations among the variables of interest in Web Appendix B, Table WB1. The F-values of regressions with each of our four endogenous variables (direct tie asymmetry, squared direct tie asymmetry, indirect tie asymmetry, and squared indirect tie asymmetry) as dependent variables and only instruments as independent variables are all above 10, and all instruments are statistically significant at the 95% confidence interval. In addition, in the full first-stage regressions with instruments and exogenous variables from Equation 2, we find that most of our instruments are statistically significant at the 95% confidence interval. Thus, we do not have a weak instrument problem. We report the results of the first-stage regression and selection model in Tables WC1 and WC2 in Web Appendix C, respectively. We also provide the results of Equation 2 (second stage) by gradually introducing direct and indirect tie asymmetries to the model in Web Appendix C (Table WC3). We find that when our four focal constructs enter the model, R-square increases significantly, indicating the meaningfulness of absolute levels of direct and indirect tie asymmetries in explaining the focal firm's BHAR and risk.We report parameter estimates of Equation 2 (second stage) for one-year BHAR (financial performance) and one-year firm risk (financial performance uncertainty) in Table 4. We find support for our main effect hypotheses (H1a and H1b). Specifically, we find that increasing direct tie asymmetry initially has a positive effect on BHAR (linear effect: b =.59, p <.05), and then at higher levels, it has a negative effect on BHAR (quadratic effect: b = –.09, p <.01), indicating a significant inverted U-shaped effect as hypothesized in H1a. Furthermore, we find that increasing direct tie asymmetry initially has a negative effect on firm risk (linear effect: b = –.42, p <.05), and then at higher levels, it has a positive effect on firm risk (quadratic effect: b =.07, p <.05), indicating a significant U-shaped effect as hypothesized in H1b. Next, we find support for H2b but not H2a. Specifically, as hypothesized in H2b, we find that increasing indirect tie asymmetry initially has a negative effect on firm risk (linear effect: b = −.12, p <.05), and then at higher levels, it has a positive effect on firm risk (quadratic effect: b =.07, p <.05), indicating a significant U-shaped effect.GraphTable 4. Main Estimation Results.  1 *p <.1.2 **p <.05.3 ***p <.01.4 Notes: BHAR reflects financial performance, and risk is idiosyncratic risk or financial performance uncertainty. All dependent variables belong to the focal firm. Errors are bootstrapped. Estimates are rescaled. Results for financial performance (CAR and BHAR) and risk with alternative time specifications are reported in Tables WE5 and WE6, respectively, in Web Appendix E.We propose in H3a and H3b that as the focal firm's innovation quality increases, the nonlinear effect of direct tie asymmetry on BHAR and firm risk flattens, respectively. We find support for both hypotheses. For BHAR, we find a significant interaction of innovation quality with the quadratic term of direct tie asymmetry (b =.17, p <.01; see Figure 4, Panel A). For firm risk, we find a significant interaction of innovation quality with the quadratic term of direct tie asymmetry (b = −.19, p <.01; see Figure 4, Panel B). In H3c and H3d, we propose that as innovation quality increases, the nonlinear effect of indirect tie asymmetry on BHAR and firm risk flattens, respectively. We find support only for H3d (firm risk). Specifically, we find a significant interaction of innovation quality with the quadratic term of indirect tie asymmetry (b = −.49, p <.05; see Figure 4, Panel C). We note that the main effect of innovation quality is nonsignificant in all our models, which is surprising, as this variable is a critical ingredient of biopharmaceutical firms' R&D productivity. However, we find that all our models include interactions that perhaps mask the main effects. In a separate analysis without interactions, we find that the main effect of innovation quality on BHAR is positive and significant (b =.07, p <.05) and the corresponding effect on risk is marginally significant (b = −.04, p <.10). This analysis indicates that the innovation quality of a focal firm on its own is not enough to influence alliance outcomes; instead, it needs to be aligned with other alliance- and firm-level factors.Graph: Figure 4. Moderating effects of innovation quality.In H4a and H4b, we propose that as total interdependence between the two firms increases, the nonlinear effect of direct tie asymmetry on BHAR and firm risk flattens, respectively. We find support for both hypotheses. For BHAR, we find a significant interaction of total interdependence with the quadratic term of direct tie asymmetry (b = 1.03, p <.05; see Figure 5, Panel A), in support of H4a. For firm risk, we find a significant interaction of total interdependence with the quadratic term of direct tie asymmetry (b = –.08, p <.01; see Figure 5, Panel B), in support of H4b. In H4c and H4d, we propose that an increase in total interdependence flattens the nonlinear effect of indirect tie asymmetry on BHAR and firm risk, respectively. We find support only for H4d (firm risk). Specifically, we find a significant interaction of total interdependence with the quadratic term of indirect tie asymmetry (b = −1.08, p <.01; see Figure 5, Panel C). We provide illustrative examples on how to interpret interaction effects and figures to support our hypotheses in Web Appendix D.Graph: Figure 5. Moderating effects of total interdependence. Postestimation and Robustness Tests Absolute versus directional measures of direct and indirect tie asymmetryIn Web Appendix E, we provide detailed discussions on the theoretical meaningfulness of absolute levels of direct and indirect tie asymmetries, based on ( 1) a literature review of interdependence asymmetry in interfirm relationships (see Table WE1) and ( 2) the results of estimating Equation 2 with directional values of direct and indirect tie asymmetries as additional control variables (see Table WE2). To empirically demonstrate our theory that absolute levels of asymmetry are more meaningful in our context than directional asymmetry, we reestimate Equation 2, replacing absolute direct and indirect tie asymmetries with directional tie asymmetries. The results show that absolute levels of direct and indirect tie asymmetries have greater model fit (one-year BHAR: Akaike information criterion [AIC] = 1613.11, one-year risk: AIC = 953.27) than directional values of direct and indirect tie asymmetries (one-year BHAR: AIC = 1,614.26, one-year risk: AIC = 994.05), validating the theoretical meaningfulness of absolute levels of tie asymmetries in our context (see Table WE3). Alternative endogeneity correctionAs an alternative endogeneity correction, we employ the two-stage least squares estimation method in which we use the instruments to predict the endogenous variables. Most of the estimated coefficients remain robust to the alternative endogeneity correction method. Table WE4 in Web Appendix E summarizes the control function and two-stage least squares results for BHAR (one-year) and risk (one-year). Alternative measures of dependent variablesWe use equity-based measures to capture both financial performance and financial performance uncertainty. To test the robustness of our results with accounting-based measures of the focal firm's financial performance and financial performance uncertainty, first we substitute BHAR with the focal firm's sales, profits, and operating cash flow, because strategic actions likely affect shareholder value through top- and bottom-line performance as well as cash flow levels ([41]). Second, we substitute firm risk with cash flow volatility, as investor perceptions of financial performance uncertainty depend on cash flow volatility ([84]). We find that most of our relevant results for financial performance hold for operating cash flows (not sales and profits). Most of our results for financial performance uncertainty hold for cash flow volatility. We present these results also in Table WE4 in Web Appendix E. Alternative time specification of dependent variablesWe use one-year BHAR and firm risk, because partners slowly develop perceptions of relative contributions and rewards, and as such, the effects of interdependence asymmetry between the partners appear over time ([69]). In line with the logic of gradual realization of outcomes, it is possible that the effects of direct and indirect tie asymmetries appear sooner than one year after the alliance initiation. Moreover, the effects may persist beyond one year after alliance initiation. To assess such effects, we estimate our theoretical model using abnormal returns and firm risk over multiple periods, including 30 days, 60 days, 180 days, two years, three years, four years, and five years. We stop at five years because we assumed an alliance duration of five years. We use CARs based on the Fama–French three-factor model for 30, 60, and 180 days. For event windows less than a year, BHAR is not recommended ([92]). For all event windows, we control for all firm-specific announcements that suggest major events or developments, such as new alliance announcements, top executive turnover announcements, and new product launch announcements. The results for these alternative time specifications are available in Table WE5 in Web Appendix E for financial performance and Table WE6 for performance uncertainty.We find that the effects of direct and indirect tie asymmetries on abnormal returns and risk are mostly consistent for all periods up to two years. For three years and more, all effects on abnormal returns and risk disappear. It likely takes about two years for alliance partners to resolve issues related to prealliance tie asymmetries (in the context of high-tech alliances, see [57]]). DiscussionPrealliance network ties of a focal firm yield benefits, as these ties generate multiple resources. However, when these ties are assessed relative to those of the partner firm in an alliance, the resulting asymmetry, regardless of its direction, creates interdependence asymmetry in the alliance that has both benefits and costs, which in turn affect financial performance and performance uncertainty of the focal firm.Our sample, which represents the biopharmaceutical industry, is unique in terms of the critical importance of R&D and new product alliances to firm performance. For example, the R&D spend in 2017 was 21.4% of total revenue on average (see [18]; [97]). The only other industry with comparable R&D spending is the semiconductor industry, with average R&D spending ranging from 10% to 30% of revenue ([90]). According to industry reports, R&D spending in the computer and software industry has increased steadily, with some of the largest firms spending up to 20% of revenues on R&D.[ 7] Given the comparable R&D spending and the importance of new product alliances in the semiconductor and software industries, it is reasonable to expect similar effects in these high-tech contexts. Theoretical ContributionsOur research offers several contributions. First, with regard to networks, direct and indirect ties are sources of resource volume and variety (e.g., [80]). Existing research thus far has adopted a firm-level perspective of the role of such network ties in alliance outcomes, but it has overlooked the notion that alliance performance is an outcome of dyadic relationships. This is an important distinction, as the interdependence of the two firms in an alliance is a function of each firm's network ties relative to those of its partner.Second, the notion of interdependence asymmetry in extant literature is typically directional (i.e., what happens to the focal firm if asymmetry favors the partner firm) (see [87]). In contrast with most prior research, we focus on absolute levels of asymmetry. We do so because regardless of which firm's direction asymmetry shifts, the performance of the alliance, which is a dyadic relationship, will be affected. It is important to note that our theorization does not negate the findings of prior research. Although we assess the performance metrics of a focal firm, we conjecture that absolute levels of direct and indirect tie asymmetries affect the partner firm's performance possibly in the same direction as the focal firm ([30]), though the scale of the impact might differ.Third, to our knowledge, this empirical study is the first to assess the impact of interdependence asymmetry in an alliance on both the focal firm's abnormal returns and risk. Given that other firm-level differences may create interdependence asymmetry, we also create an index to control for all such relevant factors and estimate its effects on the focal firm's financial performance and financial performance uncertainty using Equation 2. We find that the index has significant nonlinear effects on our dependent variables. We present details on creating this index in Web Appendix F and present our empirical results in Table WF1. Managerial ImplicationsOur results on CAR show how changes in direct tie asymmetry affect the focal firm's market capitalization. We assess the average market capitalization of the top 100 firms in the biopharmaceutical industry as $27.07 billion as of 2017.[ 8] Our results with a CAR of 180 days suggest that if direct tie asymmetry increases from the 25th percentile to the 50th percentile, the corresponding increase in market capitalization is $86.8 million. The corresponding increases at high and low levels of innovation quality are $87.3 million and $85.1 million, whereas the corresponding increases at high and low levels of total interdependence are $99.03 million and $88.19 million, respectively. However, if direct tie asymmetry increases from the 25th percentile to the 90th percentile, the corresponding increase in the focal firm's market capitalization is $67.9 million. The corresponding increases at high and low levels of innovation quality are $88.9 million and $29.6 million, whereas the corresponding changes at high and low levels of total interdependence are $98.5 million and −$151.2 million, respectively. As these results indicate, to improve the chance of forming a successful alliance, managers should be cautious when or perhaps completely avoid selecting an alliance partner in the last condition, with both high direct tie asymmetry (i.e., large difference in number of direct ties) and low total interdependence (i.e., no preexisting alliances with the partner).We are unable to show economic significance of indirect tie asymmetry because of the nonsignificant effects on CAR. However, managers should also note the importance of a potential alliance partner's indirect ties, specifically how well these ties are interconnected relative to the interconnectivity of their firm's own indirect ties. If we calculate changes in the focal firm's predicted risk from an increase in indirect tie asymmetry from the 25th percentile to the 50th percentile of our data (keeping everything else in Equation 2 constant), we find a 68.7% decrease in the focal firm's risk. The corresponding decrease in risk when indirect tie asymmetry increases from the 25th to the 90th percentile is 24%. When we calculate high and low levels of the moderators, we find that while all combinations reduce predicted risk to different degrees, one specific combination highlights a worst-case scenario. Specifically, a focal firm's predicted risk increases by 56.8% with a combination of high indirect tie asymmetry (25th–90th percentiles) and low total interdependence. Thus, to improve the odds of forming a successful alliance, a focal firm should avoid seeking an alliance partner with which it has a large difference in interconnectivity of indirect ties and no preexisting ties. Limitations and Further ResearchOur research has a couple of limitations. We restricted our study to alliances between public firms only. However, in the biopharmaceutical industry, alliances are sometimes between a public pharmaceutical firm and a private biotechnology start-up. Examining the extent to which a partner firm is affected and whether the type of partner firm (e.g., public vs. private) matters is beyond the scope of this study but represents a direction for further research. In addition, although interdependence asymmetry is of relevance to alliances in general, the role of prealliance network ties may not be as clear when we consider global alliances comprising multiple cultures and political environments, both of which present environmental contingencies worth investigating. "
30,"Effects of Contract Ambiguity in Interorganizational Governance This work introduces the concept of contract ambiguity from the law literature into the interorganizational governance literature. Within the context of franchising, the authors present a three-study multimethod design empirically establishing the construct of contract ambiguity of franchisor obligations, providing new insights into the strategic design of contracts and their outcomes. In Study 1, the authors establish construct validity by demonstrating that contract ambiguity of franchisor obligations is distinct from contract specificity and contract completeness of franchisor obligations, with differential outcomes. In Studies 2 and 3, the authors demonstrate that contract ambiguity of franchisor obligations increases an interest-based (vs. a rights-based) conflict solving approach, implying greater cooperation and joint problem solving, and reduces franchisee-initiated litigation. The findings also indicate that while contract ambiguity of franchisor obligations decreases franchisee-initiated litigation, this effect is amplified by higher levels of franchisor training programs but mitigated by the presence of a franchisee association. The article closes with a discussion of implications for academics and practitioners.KEYWORDS_SPLITA contract, or provision of a contract, is defined as ambiguous if it is reasonably susceptible to more than one interpretation ([20]; [51]). A lack of contractual clarity pertaining to the responsibilities of each partner to a relationship can hamper interorganizational effectiveness ([35]; [72]). As such, both literature and best practice argue that contract wording should be clear and unambiguous ([51]). However, ambiguous statements, in reference to a franchisor's or franchisee's obligations, are common. For example, the Arby's Restaurant Group franchise agreement states the franchisor may ""charge proposed supplier the reasonable costs"" and that the franchisee will ""take reasonable measures to protect the confidentiality [of the franchisor's proprietary information]"" (emphasis ours). Similarly, the Buffalo Wild Wings franchise agreement states that with respect to the advertising fund, the franchisor ""will make a good faith effort to expend such fees in a manner that we determine is in the general best interests of the System"" (emphasis ours). But, what do ""reasonable costs,"" ""reasonable steps,"" or ""good faith effort"" mean?The importance of interorganizational contracting has stimulated significant research, with two particularly noteworthy streams: the content of written contracts and the joint effects of contractual and noncontractual elements. First, researchers have examined the content of written contracts as formal governance mechanisms, primarily focusing on the concepts of contract specificity and contract completeness.[ 6] ""Contract specificity"" is defined as the extent to which the contract states elements, such as implementation procedures, technical specifications, and resolution of problems ([16]; [54]; [56]), whereas ""contract completeness"" is defined as the extent to which relevant clauses are codified in a contract ex ante and in subsequent ex post governance efforts (e.g., [38]). Research on these contracting aspects have increased our understanding of interorganizational effectiveness, finding, for example, that the chosen level of contract specificity is a trade-off that balances ex ante contract costs with ex post transaction problems ([54]), and that greater contract completeness minimizes opportunism ([56]).Although interorganizational research has examined contract specificity and contract completeness, scholars have not explored the concept of contract ambiguity. This is surprising, as the law literature notes that contract ambiguity is a unique, prevalent, and substantive contracting issue (e.g., [20]; [24]; [51]). Its use in contracts, simultaneously with aspects such as contract specificity, is common. Take for instance, the standard form of the 7-Eleven franchise agreement. The franchisor's obligations in the franchise agreement are ambiguous in relation to Section 6, part (b), ""Conditions to Occurrence of Effective Date,"" stating in part, ""We agree to use our best efforts to make the store available to you within a reasonable time"" (emphasis ours). However, the franchisor's obligations are specific in relation to Section 26, part (e)( 3), ""Termination: Transfer and Refund Rights,"" stating in part, ""If you elect the Refund, the Refund will be calculated by deducting twenty thousand dollars ($20,000) from the Franchise Fee you paid when you signed this Agreement; dividing the remainder thereof by one hundred eighty (180)."" The use of contract ambiguity in practice, even when general convention recommends its elimination, suggests that its existence is not accidental, but likely strategic. The strategic nature of contract ambiguity is most notable when one considers that the contract in a franchise system is unilaterally designed by the franchisor, which works to protect its own prerogatives ([41]; [57]). Given this rationale, we focus our attention on contract ambiguity of franchisor obligations in franchise agreements.[ 7]The issue of contract ambiguity dovetails with a second stream of contract investigation: the joint effects of contractual and noncontractual elements (e.g., [16]; [59]). Although written contracts serve as the foundation of a franchise system, they are but one aspect of governance (i.e., noncontractual elements also affect the administration of a relationship and therefore should be investigated jointly with written contracts; e.g., [44]; [50]; [59]). For example, [16] find that greater contract specificity enhances a retailer's relational behaviors in supporting the manufacturer's product throughout the duration of the contract. [29], p. 81) argues that socialization (i.e., the process by which new parties learn skills and internalize another party's values, goals, and rules; [30]), complements written contracts. Although research has examined some joint effects, there is much still to learn. For example, do franchisor training programs or the presence of a franchisee association help or hinder a franchisor when it drafts a franchise agreement with greater contract ambiguity of franchisor obligations?We address these issues using a three-study multimethod design, contributing to the literature in three ways. First, we extend the interorganizational contracting literature by introducing the concept of contract ambiguity from the law literature ([12], [13]), discussing the strategic role contract ambiguity plays in governing interorganizational relationships. Our findings, derived from a survey of franchisees (Study 1), establish its nomological validity, demonstrating that contract ambiguity of franchisor obligations is empirically distinct from both contract specificity and contract completeness of franchisor obligations, because it has differential effects on franchisee outcomes (e.g., cooperative performance, perceived costs of litigation, and franchisee intention to litigate). In addition, our post hoc analysis suggests that contract ambiguity creates significant financial implications on firms, and thus its use may be strategic in nature. For example, a post hoc analysis indicates that a one-unit increase in franchisee-initiated litigation leads to a 7% (i.e., $45,285.38) decrease in franchisor net income, with more substantive changes when considering the marginal effects of noncontractual elements effects.Second, we advance the contracting literature by demonstrating franchisee responses (i.e., interest- over rights-based conflict solving approach and franchisee-initiated litigation) to contract ambiguity of franchisor obligations. Study 2 employs a scenario-based experimental design. We find that contract ambiguity of franchisor obligations stimulates an interest-based (vs. a rights-based) conflict solving approach, indicative of cooperation and joint problem solving. In addition, using a data set drawn from multiple archival sources encompassing a ten-year window (2004–2013) across 106 franchise systems, we demonstrate in Study 3 that contract ambiguity of a franchisor's obligations decreases franchisee-initiated litigation. Echoing prior research on contract design (e.g., [ 3]), our study demonstrates that a carefully designed contract may help firms solicit greater communication and cooperation with exchange partners and avoid undesirable relational outcomes (e.g., litigation).Third, we contribute to the literature on the joint effects of contractual and noncontractual governance in interorganizational relationships (e.g., [16]; [50]; [59]). Our findings, based on our longitudinal data set used in Study 3, demonstrate that while contract ambiguity of franchisor obligations decreases franchisee-initiated litigation, this effect is amplified by higher levels of franchisor training programs but is mitigated by the presence of a franchisee association. The results pertaining to franchisor training programs extends the theoretical work of [29], demonstrating that franchisor training programs can allow for franchisor–franchisee socialization that aids in governance when employed jointly with contract ambiguity of franchisor obligations. In addition, our findings add to an emerging literature on franchisee association effects (e.g., [18]), indicating that the presence of a franchisee association, where socialization occurs primarily among franchisees, mitigates the dampening effect of contract ambiguity of franchisor obligations on franchisee-initiated litigation, countering the franchisor's strategic use of contract ambiguity of franchisor obligations. Contracting in the Franchise SystemWritten contracts facilitate interorganizational governance by mitigating coordination problems ([38]; [50]). Wording in written contracts should be clear and unambiguous ([51]). However, contrary to this guidance, both the law literature (e.g., [12], [13]) and practice suggest that contract ambiguity is prevalent (see Table 1). In our research, we contend that contract ambiguity of franchisor obligations is a strategic decision used by the franchisor for governance purposes.GraphTable 1. A Summary of the Law Literature Addressing Ambiguity in Contracts.  A franchise agreement is a single, standard written contract used in the franchise system. A single contract is used because a franchisor seeks standardization and coordination of franchisees to maintain brand value and enhance franchise system sales ([ 2]; [33]). These contracts are unilaterally designed by the franchisor, with a potential franchisee deciding whether to accept contract terms ([41]). [57] argue that the power advantage of the franchisor results in the franchisor developing the contract to protect its own prerogatives. This allows the franchisor to use contract ambiguity in relation to its obligations for strategic purposes (e.g., to facilitate greater cooperation and joint problem solving as well as to minimize franchisee-initiated litigation).Although the franchise agreement is established by the franchisor, franchisees are not docile partners. Rather, franchisees strive for autonomy to maximize profit ([33]; [39]). Given a franchisor's focus on standardization and coordination and a franchisee's drive for autonomy, franchisor–franchisee relationships can experience conflicts on contractual matters ([ 2]; [ 9]; [25]), which can escalate to costly litigation ([ 2]; [34]). For example, in April 2018, a franchisee filed suit against Steak 'n Shake in an effort to be allowed to set its own menu pricing ([55]). Similarly, in June 2017, a Tim Hortons franchisee sued its franchisor for $500 million in relation to the franchisor's use of advertising fund revenue ([52]).To begin exploring franchisees' responses to contract ambiguity of franchisor obligations, we examine two types of conflict solving approaches introduced in the contracting literature: interest-based and rights-based ([48]). The interest-based approach is cooperative, fostering joint problem solving and mutually acceptable solutions (e.g., [ 8]). Contracts using ambiguous contract terms motivate parties to engage one another when facing conflicts, so as to articulate their expectations and interests ([24]). Alternatively, the rights-based conflict solving approach is argumentative, focusing on legitimate claims (e.g., [ 8]; [73]). Contracts using unambiguous terms (i.e., those having one definite meaning) foster a rights-based approach, as the contract gives each party ""facts"" to which they believe they are entitled (e.g., [27]; [21]).Furthermore, creating a contract with greater contract ambiguity of the franchisor's obligations provides the franchisor protection from claims that it has breached the contract. For example, how is one to prove that a franchisor did not negotiate in ""good faith""? [12] and [61] note that litigation costs associated with vague terms are a barrier to litigation due to the difficult nature of verifying that a contractual breach occurred. [13], p. 853) contend that contract ambiguity may be used strategically as a ""screen on the promisee's incentive to sue."" Thus, a franchisor can minimize franchisee-initiated litigation by employing contract ambiguity of its obligations in the franchise agreement.While contract ambiguity pertaining to the franchisor's obligations aid the franchisor in minimizing franchisee-initiated litigation, the use of ambiguous terms (e.g., ""reasonable costs,"" ""good faith effort"") can cause misunderstandings. As such, franchisors need to also consider the joint effects of contractual and noncontractual elements (e.g., [16]; [59]). First, franchisors may employ training programs, including both initial and ongoing training opportunities, to facilitate socialization between the franchisor and the franchisees ([29]). For example, Potbelly Sandwich Shop offers not only a 10–12-week initial training program but also encourages franchisees to participate in regular training both at company-owned shops and at the franchisor's site (where the franchisor provides additional mentoring and guidance). These efforts work to facilitate the development of shared values and the alignment of goals between parties, providing a context for increased discussions.Second, franchisors need to consider the effects of franchisee associations ([42]). We contend that when franchisees participate in a franchisee association (whether franchisor sponsored, franchise system specific, or independent), socialization occurs among franchisees, which may create resistance to franchisor efforts ([18]). This is particularly concerning because some franchisors are actively encouraging the development of franchisee associations (often as an alternative to franchisees joining an independent association, such as the American Association of Franchisees and Dealers). For example, in 2017 Kahala Franchising LLC encouraged its franchisees to form cooperative advertising associations to maximize the efficient use of local advertising media. Research Hypotheses The Effects of Contract Ambiguity of Franchisor ObligationsBuilding on our conceptualization, we argue that contract ambiguity of franchisor obligations has multiple outcome effects. First, we theorize that in response to a greater level of contract ambiguity of franchisor obligations, franchisees respond with an interest-based (vs. rights-based) conflict solving approach, implying greater cooperation and joint problem solving. Contract ambiguity creates room for interpretation, necessitating communication between franchisee and franchisor to reconcile interests ([24]). An interest-based approach to conflict solving works within a cooperative framework, whereas a rights-based approach works to determine who is right or wrong ([ 8]). With greater contract ambiguity of a franchisor's obligations, franchisees seek interpretation from the franchisor and solving problems jointly.Second, contract ambiguity is a cost inhibitor ([12], [13]), thereby decreasing franchisee-initiated litigation. The more ambiguous the contract terms, in relation to the franchisor's obligations, the more difficult (and costly) it is to prove that a franchisor failed to meet its obligations. The cost of litigation is particularly notable given the different financial situations of each party. [47], p. 59) notes that ""a franchisor has greater economic resources with which to fund litigation, giving it a distinct advantage in the process regardless of the merits of the franchisee's claims or defenses. For franchisees, however, it may be difficult to finance a protracted lawsuit."" Unless he is willing to pay high litigation costs to establish his cases, the franchisee, facing a contract with a greater level of contract ambiguity, would be less likely to opt for litigation against the franchisor. Formally, H1  : Increased contract ambiguity of franchisor obligations increases a franchisee's interest-based (vs. rights-based) conflict solving approach. H2  : Increased contract ambiguity of franchisor obligations decreases franchisee-initiated litigation. Moderating Effects of Franchisor Training Programs and a Franchisee AssociationFranchisee-initiated litigation disrupts franchise system operations and poses significant financial consequences to the franchisor. Scholars suggest that elements other than those written in the contract can play an important governance role when exchange partners face contractual hazards ([16]; [44]). As such, here we focus on how franchisor training programs and franchisee associations moderate the negative influence of contract ambiguity of franchisor obligations on franchisee-initiated litigation.Most franchisors, working to ensure standardization and coordination across the franchise system, enact initial and ongoing training programs. The presence of higher levels of franchisor training programs provides an opportunity and a context for the franchisor and franchisees to communicate on a regular basis, articulating their expectations and interests. In addition, it helps reduce misunderstandings through the creation of a common frame of reference and development of shared values ([ 6]; [30]). Working jointly with contract ambiguity of franchisor obligations, franchisor training programs can prevent latent conflicts from escalating to serious litigation through increased communication for clarification. In other words, with higher levels of franchisor training programs, it is more accessible, and thus more likely, for franchisees to opt for discussions to advance their interests as opposed to seeking relief from the court. Thus, we contend that increased franchisor training programs amplifies the negative relationship between contract ambiguity of franchisor obligations and franchisee-initiated litigation, which works in favor of the franchisor. Formally, H3  : Increased use of franchisor training programs amplifies the negative relationship between contract ambiguity of franchisor obligations and franchisee-initiated litigation.Franchisee associations provide an opportunity for the development of common frames of reference and cohesion among franchisees ([18]). Because all franchisees are subject to the same contract ambiguity of franchisor obligations, franchisees in an association will tend to be supportive of another franchisee's position. Perceptions of injustice, such as those stimulated by contract ambiguity of franchisor obligations, are reinforced through cohesion ([46]), making franchisees more confident in resisting the powerful franchisor. This argumentation is consistent with the concept of countervailing power ([22]) in marketing channels (e.g., [19]), which argues that less powerful members to an exchange band together to offset the power of a more powerful partner. By banding together, franchisees are also able to share costs, countering the financial resource advantage of the franchisor. For example, in May of 2019, 20 franchisees, representing 32 of the franchisor's 53 restaurants of Deli Delicious, formed an association to protest the franchisor's lack of leadership and transparency of business practices, accusing the franchisor of price fixing, retaliation, and so on ([76]). By banding together, these franchisees countered the power asymmetry between the franchisor and the individual franchisee. Thus, we argue that the negative relationship between contract ambiguity of franchisor obligations and franchisee-initiated litigation will be mitigated by the presence of a franchisee association, disfavoring the franchisor. Formally, H4  : The presence of a franchisee association mitigates the negative relationship between contract ambiguity of franchisor obligations and franchisee-initiated litigation. MethodWe employ a three-study multimethod research design to examine our underlying logic (see Figure 1). Study 1 establishes nomological validity of contract ambiguity of franchisor obligations. We use a survey of 146 franchise owners to establish discriminate validity of contract ambiguity of franchisor obligations from contract completeness and contract specificity of franchisor obligations, as well as differential effects on outcomes. Study 2 examines franchisee response to contract ambiguity of franchisor obligations, gaining insight into the process underlying our arguments. A scenario-based experiment with 92 online respondents formally tests H1 and examines process elements. Study 3 establishes external validity for our model, formally testing H2, H3, and H4 within a data set drawn from multiple archival sources encompassing a ten-year window (2004–2013) across 106 franchise systems.Graph: Figure 1. Overview of three studies. Study 1In Study 1 we worked to determine, through a survey of franchise owners, if contract ambiguity of franchisor obligation can be empirically distinguished from other contract constructs (i.e., contract completeness and contract specificity of franchisor obligations). To distinguish a new construct, it is important to ( 1) test the discriminant validity of the construct from similar constructs through confirmatory factor analysis and ( 2) establish a nomological network in which the construct and similar constructs may have differential effects on the same outcomes. Research Context and Data CollectionUsing a market research company's (Dynata) national business panel, we invited business owners (qualifying if self-reporting as a franchisee) to participate in a study. One hundred fifty-two franchisee owners completed the survey. After deleting six surveys due to straightlining, we were left with 146 observations. On average, the respondents were 41.45 years old and had 21.52 years of work experience. Each of them owned their franchise for an average of 8.38 years and averaged 48.36 hours of required franchisor training in the prior year. We compared early versus late respondents in relation to key study variables (i.e., CAFR, CCFR, CSFR, COST, INTENT, and COOPER) to assess nonresponse bias. The results of a multivariate analysis of covariance (MANCOVA) indicated no differences, suggesting that nonresponse bias is not a threat to the results. MeasuresFocal study constructs were captured through multi-item scales (see Table 2). We developed the scales for contract ambiguity (CAFR), contract completeness (CCFR), and contract specificity (CSFR) of franchisor obligations to adhere to the construct conceptualizations and to the items used in the literature. We used a seven-point, three-item, bipolar semantic differential scale for each contracting construct. To ensure valence consistency, items related to the contract being ambiguous, not specific, and not complete were on the left side, and items related to the contract being unambiguous, specific, and complete were on the right (we reverse-coded contract ambiguity items so that higher numbers refer to greater contract ambiguity of franchisor obligations). Consistent with [54], we provided construct conceptualizations to respondents prior to scale items.GraphTable 2. Definition, Measurement, Data Sources, and Literature Support for Focal Variables.  1 a Natural log-transformed.2 Notes: (r) = reverse-coded.To establish the nomological network, we measured perceived costs of litigation of franchisee (COST), franchisee intention to litigate (INTENT), and cooperative performance (COOPER). Seven-point Likert scales (1 = ""strongly disagree,"" and 7 = ""strongly agree"") were used to capture these constructs. Perceived costs of litigation of franchisee and franchisee intention to litigate were measured by three-item scales, adapted from [53]. Cooperative performance was measured by a three-item scale adapted from [45]. Control variablesWe controlled for the respondents' years of work experience (YFR); the number of franchised outlets owned (NUMOFR); the number of years of owning franchised outlets (YOWN); and their experiences in reviewing franchise contracts (REV), which is measured by a categorical variable (1 = ""I am responsible for reviewing the contract,"" 2 = ""My attorney is responsible for reviewing the contract,"" and 3 = ""Someone else other than my attorney or myself is responsible""). We also controlled the focal firm's sector (SECTOR) k (k = 1,..., 11) based on the North American Industry Classification System. Descriptive statistics and correlations are provided in Table 3.GraphTable 3. Descriptive Statistics and Correlation Matrix (Study 1).  3 *p <.05.4 a The construct is measured by a bipolar semantic differential scale.5 Notes: N = 146. Construct Reliability and ValidityThe fit statistics of the measurement models are good (χ2(32) =56.28, p <.005; comparative fit index =.938; incremental fit index =.938; root mean square error of approximation =.066). The composite reliabilities of the latent constructs are all greater than the.70 benchmark, indicating convergent validity. Contract ambiguity of franchisor obligations presents discriminant and convergent validity from contract specificity and contract completeness of franchisor obligations, on the basis of the following. First, an oblique (quartimax) rotation yields a clean three-factor structure, in which all loadings of the items, on their corresponding factors, range in magnitude from.87 to.98, whereas the magnitude of all cross-loadings is less than.15. Second, the covariances between contract ambiguity and the other two constructs in structural equation modeling are less than.30 (rhoCA, CS = −.23; rhoCA, CC = −.22). Third, the chi-square difference tests are significantly different from 1 for each pair of constructs, with the average variances extracted being larger than the corresponding squared interconstruct correlations. Analysis and ResultsWe used regression analysis to examine the effects of contract constructs of franchisor obligations on three outcome variables. The standardized coefficients provided in Table 4 indicate that greater contract ambiguity of franchisor obligations increases the perceived costs of litigation of the franchisee (b =.17, p <.05), decreases franchisee intention to litigate (b = −.55, p <.001), and increases cooperative performance (b =.43, p <.001), respectively. In contrast, greater contract completeness of franchisor obligations was found to lessen franchisee intention to litigate (b = −.13, p <.05), but did not have an effect on the perceived costs of litigation of the franchisee (b =.13, n.s.) or cooperative performance (b =.04, n.s.). Contract specificity of franchisor obligations significantly increased cooperative performance (b =.11, p <.05) but did not affect perceived costs of litigation of the franchisee (b =.10, n.s.) or franchisee intention to litigate (b = −.06, n.s.). The results demonstrate that contract ambiguity of franchisor obligations is a distinct construct from contract completeness or contract specificity of franchisor obligations.GraphTable 4. The Impact of Contract-related Constructs on Outcome Variables (Study 1).  6 *p <.05.7 **p <.01.8 ***p <.001.9 a The construct is measured by a bipolar semantic differential scale.10 Notes: N = 146; t-statistics are in parentheses; standardized coefficients are provided.To examine whether the proposed mediation effect of perceived costs of litigation of franchisee holds, we employed the bootstrapping procedure developed by [28], Model 4) to test for mediation (95% confidence interval (CI); 10,000 bootstrap resamples). We tested the extent to which the effect of contract ambiguity of franchisor obligations on franchisee intention to litigate was mediated by perceived costs of litigation of franchisee. The results indicated that greater contract ambiguity increased perceived costs of litigation of the franchisee (b =.17, p <.01), which in turn decreased franchisee intention to litigate (b = −.68, p <.001). In addition, the 95% CI for the indirect path through the mediator (i.e., perceived litigation cost) did not include zero (−.12; 95% CI = [−.23, −.03]). The results support our theorization that contract ambiguity of franchisor obligations influences franchisee intention to litigate through perceived costs of litigation of franchisee. We conducted similar mediation tests for contract completeness and contract specificity of franchisor obligations but did not find any significant mediating effect of perceived costs of litigation of franchisee in relation to them.Moreover, to compare the relative importance of contract constructs, we conducted a dominance analysis by comparing the change of adjusted R-square attributable to inclusion of each construct ([ 4]). The results showed that inclusion of contract ambiguity of franchisor obligations results in the largest change in adjusted R-square for all three outcome variables (Δadjusted R2COST, CA =.018; Δadjusted R2INTENT, CA =.271; Δadjusted R2COOPER, CA =.163), indicating its importance to contractual governance. This further supports our introduction of contract ambiguity of franchisor obligations into the literature. Study 2In Study 2, we worked to gain insight into franchisee responses to contract ambiguity of franchisor obligations. Specifically, we explored whether a greater level of contract ambiguity affects franchisee's response approach to conflicts (interest-based vs. rights-based) as hypothesized in H1, and whether perceived costs of litigation are the underlying process through which contract ambiguity of franchisor obligations influences a franchisee's intention to litigate. We developed a scenario-based experiment to allow for a formal test of H1. Scenario Development and TreatmentsWe employed a one-factor, two-condition (contract ambiguity of franchisor obligations: high, low), between-subjects design. Franchisor and franchisee managers, three scholars, and two legal experts assessed the scenario wording and response formats prior to administration. Appendix A presents the manipulation of contract ambiguity of franchisor obligations. Sampling Frame and ProcedureTo identify working professionals, we used Amazon's Mechanical Turk respondent pool. We invited individuals, based in the United States and qualifying if self-reporting at least five years of work experience, to participate in a study about franchising. We obtained 92 completed responses (HighCAFR n = 52; LowCAFR n = 40). Respondents averaged 29.18 years of age and 11.18 years of work experience; 75% of respondents were male and 25% were female. Respondents represented a range of economic sectors (e.g., transportation and warehousing [14.1%]; health care and social assistance [12.0%];, arts, entertainment, and recreation [12.0%]; accommodation and food services [12.0%]).We randomly assigned respondents to one of the two treatment conditions. Respondents in each treatment condition were asked to imagine that they owned a franchise of a Hawaiian-themed restaurant chain. A summary of the franchise was then presented. The scenario indicated that all franchisees would operate under a standard franchise agreement. Respondents were asked to carefully read a section of the franchise agreement. A series of questions followed.The questions captured the constructs in the conceptual path that we believed respondents would engage in, with regard to the task. We began by capturing interest- and rights-based conflict solving approaches, followed by perceived costs of litigation and franchisee intention to litigate. We next captured contracting constructs in relation to both franchisor and franchisee obligations as a manipulation check and then collected demographic information. MeasuresTo assess the extent to which a respondent employed an interest-based (vs. a rights-based) conflict solving approach (as hypothesized in H1), we captured the difference between two five-item Likert-type scales ([48]) assessing interest- and rights-based approaches and labeled this construct franchisee relative response (RELRES). Interest-based response items included ( 1) find consensus, ( 2) identify opportunities for joint problem solving, ( 3) find outcomes that serve our common interests, ( 4) identify solutions that are mutually beneficial, and ( 5) reach an agreement. Rights-based response items included ( 1) establish the legitimacy or illegitimacy of behaviors, ( 2) determine who was right or wrong, ( 3) determine who violated the terms of the contract, ( 4) identify the valid and invalid actions of each party, and ( 5) identify violations of the norms of the agreement. Acknowledging that understanding a firm's conflict solving approach is a matter of the extent to which firms rely on each approach, we calculated the difference by deducting the rights-based from interest-based approach score. We used measures identical to those used in Study 1 for perceived costs of litigation, franchisee intention to litigation, and contract constructs (CAFR, CCFR, and CSFR, respectively; see Table 2). To minimize spuriousness of the results, we controlled for respondents' years of work experience, age, and gender. Manipulation ChecksTo assess our manipulation, we examined means across treatments. The results indicated a difference in contract ambiguity of franchisor obligations across conditions (Mhigh = 5.32, Mlow = 4.59; t = 2.50, p <.01). No differences were observed for other contract characteristics across the two conditions (contract ambiguity of franchisee obligations: Mhigh = 4.76, Mlow = 4.83; t = −.21, p >.05; franchisor contract specificity [CSFR]: Mhigh = 4.17, Mlow = 4.11; t =.28, p >.05; franchisee contract specificity [CSFE] Mhigh = 4.13, Mlow = 3.98; t =.63, p >.05; franchisor contract completeness [CCFR]: Mhigh = 3.88, Mlow = 3.79; t =.33, p >.05; franchisee contract completeness [CCFE]: Mhigh = 3.94, Mlow = 3.79; t =.63, p >.05).[ 8] The results indicated a successful manipulation of contract ambiguity of franchisor obligations. Analysis and ResultsWe ran a MANCOVA with the dependent variables of franchisee relative response; franchisee intention to litigate; perceived costs of litigation; and the covariates of work experience, age, and gender. Differences across treatments were observed (Wilks' Λ =.71, F = 11.77, p ≤.001). Post hoc pairwise comparisons using the Bonferroni procedure identified mean differences, consistent with our arguments, for franchisee relative response (greater interest- than rights-based response; Mhigh = 1.02, Mlow = −.52; p <.001), which supports H1. In addition, we found significant mean differences for perceived costs of litigation of the franchisee (Mhigh = 4.63, Mlow = 3.60; p <.001) as well as for franchisee intention to litigate (Mhigh = 3.35, Mlow = 3.93; p <.05). The covariates were not significant.Next, to examine process elements of the model, we followed the bootstrapping procedure developed by [28], Model 4) to test for mediation (95% CI; 10,000 bootstrap resamples). We tested the extent to which the effect of contract ambiguity of franchisor obligations on franchisee intention to litigate was mediated by perceived costs of litigation. The results indicated that greater contract ambiguity of franchisor obligations increased perceived costs of litigation (b =.26, p <.01), which decreased franchisee intention to litigate (b = −.82, p <.001). In addition, the 95% CI for the indirect path through the mediator (i.e., perceived litigation costs) did not include zero (−.21, 95% CI: [−.38, −.03]). In addition, the direct effect of contract ambiguity of franchisor obligations on franchisee intention to litigate became insignificant when the mediator was added (b =.10, n.s.), suggesting full mediation. The results support our theorization that contract ambiguity of franchisor obligations influences franchisee intention to litigate through perceived costs of litigation.To explore whether franchisee relative response may be a possible mediator for the relationship between contract ambiguity of franchisor obligation and franchisee intention to litigate, we added franchisee relative response as another mediator in the model (Model 4), and the results showed that franchisee relative response had no effect on franchisee intention to litigate (b =.05, n.s.), and the indirect effect though franchisee relative response was not significant (95% CI: [−.02,.07]), allowing us to reject the mediating effect of franchisee relative response. Additional AnalysesGiven the use of a difference measure (i.e., relative response), we conducted additional analysis (MANCOVA) with separate dependent variables of franchisee interest-based response; franchisee rights-based response; franchisee intention to litigate; perceived cost of litigation of franchisee; and the covariates of work experience, age, and gender. Differences across treatments were observed (Wilks' Λ =.71, F( 4, 84) = 8.87, p <.001). The covariates were not significant. Post hoc pairwise comparisons using the Bonferroni procedure identified mean differences in relation to interest-based response (Mhigh = 4.70, Mlow = 3.91; p <.001) and rights-based response (Mhigh = 3.68, Mlow = 4.44; p <.001), perceived costs of litigation of franchisee (Mhigh = 4.63, Mlow = 3.60; p <.001) as well as franchisee intention to litigate (Mhigh = 3.35, Mlow = 3.93; p <.05). We defined a planned contrast between interest-based and rights-based response and found a significant difference (MIB = 4.36, MRB = 4.00; t = 2.19, p <.05). To further understand the data, we ran a planned contrast between interest-based and rights-based response using a split file analysis. The results indicated that in the treatment with low contract ambiguity of franchisor obligations, franchisors employ a rights-based over interest-based response (MIB = 3.91, MRB = 4.44; t = −1.95, p <.05), whereas in the high contract ambiguity of franchisor obligations treatment condition franchisors employ an interest-based over a rights-based response (MIB = 4.70, MRB = 3.68; t = 7.51, p <.01). Study 3In Study 3, we establish external validity for our model using archival data. We first tested whether greater contract ambiguity of franchisor obligations decreased franchisee-initiated litigation (H2). Next, we examined the moderating effects of franchisor training programs (H3) and the presence/absence of a franchisee association (H4). Furthermore, we explored the effect of franchisee-initiated litigation on franchise system performance. Empirical Context and Data CollectionWe constructed a database by obtaining the franchise disclosure documents (FDDs) filed in 2013 by a random sample of 106 franchisors, drawn from the electronic filings of franchisors in one state of the United States that required registration of franchisors. Then, we obtained the FDDs filed with the state's regulatory authorities for the years between 2004 and 2013 and bought the ones not registered in that state during this time window for the same set of companies from FRANdata. The FDDs contained the franchise contracts and the details of franchisor-related litigation history.We coded contract ambiguity, completeness, and specificity of both franchisor and franchisee obligations, franchisee-initiated litigation, the number of franchised outlets, financial performance of the franchise system, and total assets of the franchise system from the text of the FDDs and the franchise contracts appended in the FDDs. We then coded franchise age, franchisor training and franchisee association, and industry classification of each franchisor in our sample from Bond's Franchise Guide and Entrepreneur Magazine's Franchise 500, with the latter used as a convergent validity check and as a supplemental source when data were missing from Bond's Franchise Guide. The data set yielded an unbalanced panel data of 703 observations across 106 franchisors over a ten-year window (2004–2013). Unit of Analysis and MeasuresOur unit of analysis was the individual franchise system i (i = 1,..., 106) in year t (t = 2004,..., 2013), with outcome variables being time-varying. We measured our dependent variable franchisee-initiated litigation (FELIT) as the number of litigation events initiated by franchisees for franchisor i in year t, excluding accumulative litigation from previous years.We measured contract ambiguity of franchisor obligations (CAFRO) by the number of vague words used in the franchise contracts pertaining to franchisor obligations. Although there were many words indicating vagueness in contract drafting, we relied on an in-depth literature review of legal studies and included the words suggested by prior literature to indicate vague standards in contracts, which yielded a list of ten major words (see Appendix B). Second, following prior research (i.e., [ 2]), we had four research assistants read the franchise contracts appended in the FDDs. We had the coders count the number of vague words used in each section of the franchise contract, indicating whether the words were in relation to franchisor or franchisee obligations. The four research assistants worked in pairs. After each research assistant finished coding, the other research assistant checked the work independently and resolved any disagreements by discussion. The level of agreement between the coders was.96. In addition, one of the authors checked a random subsample of the coding to ensure convergent validity. We then aggregated the total number of vague words used in different contractual sections for franchisor obligations, including franchisors' assistance to franchisees with regard to operations, advertisement, and other miscellaneous items, for franchisor i in year t.We measured franchisor training programs (FRTP) by the annual number of training hours provided by the franchisor, including both initial training and training throughout the year as specified in the franchise agreement, as a larger number of training hours increases communication between the franchisor and franchisees allowing for the development of shared understanding and relational norm development ([36]; [71]). Compared with other methods, training programs are considered easier and more effective for communication ([58]). Franchisee association (FEASSOC) is a dichotomous variable based on whether a franchisee association, which hosts regular meetings and other occasions for franchisees to share information, is established within the franchise system ([18]). Control variablesIn addition to the hypothesized predictors, we also controlled for other factors. First, following [38], we controlled for contract completeness of franchisor obligations (CCFRO) by capturing the number of contractual clauses in franchise agreements in relation to franchisor obligations. We first generated an exhaustive list of contractual clauses regarding franchisor obligations compiled from all franchise contracts in our data set. We then had the coders count the exact number of clauses covered in each contract for franchisor i in year t. For contract specificity of franchisor obligations (CSFRO), as there was no readily available measure in the literature, we generated its measure based on the conceptualization of this construct in the law literature. Following [66], we measured contract specificity of franchisor obligations by the addition of the frequency of ( 1) a ""whereas clause,"" ( 2) a ""definition clause,"" ( 3) appendices, and ( 4) specific numbers (excluding irrelevant ones such as those indicating sequences) in each contract in relation to franchisor obligations. To reduce the skewness of CCFRO and CSFRO, we generated log-transformed values for both variables. In addition, we controlled for firm-specific characteristics including royalty rate (ROYAL), franchise age (AGE), number of franchised outlets (NUMFR), and total assets of franchisors (TOTASS) as reported in the FDDs, Bond's Franchise Guide, and Entrepreneur Magazine's Franchise 500. Finally, because different sectors may have different practices in contract drafting, we controlled for the focal firm's sector (SECTOR) k (k = 1,..., 11) based on the North American Industry Classification System. Table 2 provides the summary of the measurement and supporting literature. Table 5 provides the descriptive statistics and correlation matrix for the variables included in this study.GraphTable 5. Descriptive Statistics and Correlation Matrix (Study 3).  11 *p <.05.12 a Natural log-transformed.To better understand the franchise agreement, we compared the degree to which the contract obligations are ambiguous for franchisors and franchisees. To control for the influence of the length of contracts, we divided the degree of contract ambiguity, measured by the number of vague words, by the contract completeness, measured by the number of contractual clauses, for franchisor and franchisees' obligations, respectively. The average ratio (i.e., ambiguity/completeness) for franchisors' obligations was 3.02 (min = 0, max = 19, SD = 1.08). The average ratio (i.e., ambiguity/completeness) for franchisees' obligations was 1.59 (min = 0, max = 14, SD =.88). Although contract ambiguity appears larger in relation to franchisor obligations, no significant difference was observed (t = 1.03, p >.05). Model SpecificationTo examine the role of contract ambiguity of franchisor obligations and its joint effects with franchisor training programs and the presence/absence of a franchisee association on franchisee-initiated litigation, the following model features were considered. First, we had multiple equations, with correlated error terms. As such, we needed an estimation method that would allow the simultaneous estimation of multiple equations. Second, the dependent variables are continuous in Equations 1, 2, 4, 5, 6, and 7 but are dichotomous in Equation 3. Therefore, we needed an estimation method that would allow us to simultaneously estimate equations with mixed-type outcome variables. Finally, because the data were longitudinal and had repeated observations for each franchisor i, we needed to account for such clustering effects.Drawing on these considerations, we employed a conditional mixed process regression model (CMP). The CMP enables a simultaneous estimation of a recursive system of equations with mixed types of outcome variables that allows for clustering analysis by using a simulated maximum likelihood estimation algorithm ([26]; [62]). This approach yields consistent and more efficient estimates of interest compared with the traditional two-stage least squares method ([ 1]). Given this advantage, CMP has been used in prior literature (e.g., [ 2]; [38]). Endogeneity correctionTo account for the endogeneity of contract ambiguity of franchisor obligations, franchisor training programs, and the presence of a franchisee association, we needed valid instrumental variables that both were relevant (i.e., strongly correlated with the focal variables) and met the exclusion criterion (i.e., uncorrelated with the error term of the explanatory equation). We specified first-stage equations for each of the endogenous regressors, including the three theoretical constructs (CAFRO, FRTP, FEASSOC) and three control variables (CCFRO, CSFRO, ROYAL). We employed these variables' corresponding mean values across all franchise systems in the focal franchisor's sector, excluding the focal franchisor i, lagged one year, as their instruments ([ 1]; [23]). Instrument validity checksWe tested whether the instrumental variables met the requirements of the relevance and exclusive restrictions. First, we examined the Cragg–Donald Wald F-statistics on our instrumental variables for each of the first-stage equations, which were all above the rule-of-thumb threshold of 10 ([69]), with the lowest being 293.85. This suggests that our instruments satisfy the requirement for relevance (i.e., strongly correlated with the endogenous variables). Second, we used the Durbin–Wu–Hausman test to examine exogeneity of the instruments ([60]). We tested the exogeneity of each instrumental variable, and could not reject the null hypothesis that the focal instrument was uncorrelated with the error term in the second-stage equation. These tests suggest the validity of the instrumental variables.The empirical model is specified as follows. The first-stage equations for the endogenous variables CAFRO, FRTP, FEASSOC, CCFRO, CSFRO, ROYAL are presented in Equations 1–6. Each is regressed on its corresponding sector mean level (one year lagged). We included the one-year-lag control variables. We then specified the individual effect of contract ambiguity of franchisor obligations and its joint effects with franchisor training programs and franchisee association on franchisee-initiated litigation in Equation 7. Following prior research (e.g., [ 1]; [23]), we examined the impact of independent variables with one-year-lag term on the dependent variables and used cluster-robust error to account for the clustering effect of repeated observations for each franchisor i. CAFROi, t= β1,0+ β1,1MeanCAFROi, t − 1+ β1,2MeanFRTPi, t − 1+ β1,3MeanFEASSOCi, t − 1+ β1,4MeanCCFROi, t − 1+ β1,5MeanCSFROi, t − 1+ β1,6MeanROYALi, t − 1+ β1,7AGEi, t − 1+ β1,8NUMFRi, t − 1+ β1,9TOTASSi, t − 1 + ∑1,101,19β SECTOR + ω1, Graph1 FRTPi, t=β2,0+ β2,1MeanCAFROi, t − 1+ β2,2MeanFRTPi, t − 1+ β2,3MeanFEASSOCi, t − 1+ β2,4MeanCCFROi, t − 1+ β2,5MeanCSFROi, t − 1+ β2,6MeanROYALi, t − 1+ β2,7AGEi, t − 1+ β2,8NUMFRi, t − 1 +β2,9TOTASSi, t − 1 + ∑2,102,19β SECTOR + ω2, Graph2 FEASSOCi, t=β3,0+ β3,1MeanCAFROi, t − 1+ β3,2MeanFRTPi, t − 1+ β3,3MeanFEASSOCi, t − 1+ β3,4MeanCCFROi, t − 1+ β3,5MeanCSFROi, t − 1+ β3,6MeanROYALi, t − 1+ β3,7AGEi, t − 1+ β3,8NUMFRi, t − 1 + β3,9TOTASSi, t − 1 + ∑3,103,19β SECTOR + ω3, Graph3 CCFROi, t=β4,0+ β4,1MeanCAFROi, t − 1+ β4,2MeanFRTPi, t − 1+ β4,3MeanFEASSOCi, t − 1+ β4,4MeanCCFROi, t − 1+β4,5MeanCSFROi, t − 1+ β4,6MeanROYALi, t − 1+ β4,7AGEi, t − 1+ β4,8NUMFRi, t − 1+ β4,9TOTASSi, t − 1+∑4,104,19β SECTOR+ω4, Graph4 CSFROi, t=β5,0+ β5,1MeanCAFROi, t − 1+ β5,2MeanFRTPi, t − 1+ β5,3MeanFEASSOCi, t − 1+ β5,4MeanCCFROi, t − 1+ β5,5MeanCSFROi, t − 1+ β5,6MeanROYALi, t − 1+ β5,7AGEi, t − 1+ β5,8NUMFRi, t − 1+ β5,9TOTASSi, t − 1 + ∑5,105,19β SECTOR+ω5, Graph5 ROYALi, t=β6,0+ β6,1MeanCAFROi, t − 1+ β6,2MeanFRTPi, t − 1+ β6,3MeanFEASSOCi, t − 1+ β6,4MeanCCFROi, t − 1+ β6,5MeanCSFROi, t − 1+ β6,6MeanROYALi, t − 1+ β6,7AGEi, t − 1+ β6,8NUMFRi, t − 1+ β6,9TOTASSi, t − 1 + ∑6,106,19β SECTOR + ω6, Graph6 FELITi, t=β7,0+ β7,1CAFROi, t − 1+ β7,2FRTPi, t − 1+ β7,3FEASSOCi, t − 1+ β7,4CAFROi, t − 1× FRTPi, t − 1+ β7,5CAFROi, t − 1× FEASSOCi, t − 1+ β7,6COMPi, t − 1+ β7,7SPECi, t − 1+ β7,8ROYALi, t − 1+ β7,9AGEi, t − 1+ β7,10NUMFRi, t − 1+ β7,11TOTASSi, t − 1+∑7,107,19β SECTOR + ω7, Graph7where the variance–covariance matrix Ω ∼ MVN(µ, σ2). ResultsTable 6 presents the standardized coefficients of CMP regression estimates. The chi-square statistic of 3,627.80 (p <.001) suggests that the predictors in our model have significant effects on the outcome variables. Our results show that contract ambiguity of franchisor obligations negatively affects franchisee-initiated litigation (b = −.08, p <.01), supporting H2.[ 9] This effect is strengthened by franchisor training programs (b = −.05, p <.01), supporting H3, and weakened by the presence of a franchisee association (b =.12, p <.05), supporting H4.GraphTable 6. Conditional Mixed Process Regression Estimates (Study 3).  13 *p <.05.14 **p <.01.15 ***p <.001.16 a Natural log-transformed.17 Notes: n = 703; z-statistics are in parentheses.Unlike contract ambiguity of franchisor obligations, contract completeness and contract specificity of franchisor obligations did not have significant effects on franchisee-initiated litigation (b =.52, n.s.; b = −.10, n.s., respectively), indicating that these are distinct contracting concepts with differential effects. Number of franchised outlets had a positive effect on franchise-initiated litigation (b =.03, p <.01), indicating that a larger franchise system incurs more franchisee-initiated litigation. Post Hoc AnalysisTo better understand our results, we conducted post hoc analyses. First, we explored the moderating effects by tests of the simple slopes at low (−1 SD) and high (+1 SD) values of FRTP ([15]). Figure 2, Panel A, indicates that the inverse association between CAFRO and FELIT is stronger for franchise systems with high levels of FRTP (simple slope = −.35, p <.001) compared with low levels of FRTP (simple slope = −.24, p <.001), which indicates that a high level of FRTP strengthens the negative impact of CAFRO on FELIT. Second, with regard to the effect of FEASSOC, because it is a dichotomous variable, we calculated the slopes at FEASSOC = 0 and FEASSOC = 1. Figure 2, Panel B, indicates that the negative relationship between CAFRO and FELIT is reversed for franchise systems when FEASSOC = 1 (simple slope =.04, p <.05) compared with that when FEASSOC = 0 (simple slope = −.08, p <.01). The post hoc analysis supports our hypotheses.Graph: Figure 2. Moderating effects (Study 3).To evaluate the substantive importance of our model, we examined whether franchisee-initiated litigation (FELIT) has a significant impact on franchise system financial performance. We specified a random-effect generalized least squares estimation to examine the effect of franchisee-initiated litigation on revenues and costs of the franchise system, as well as on franchisor net income. The results, available in Web Appendix A, show that franchisee-initiated litigation has a significant negative effect on franchise-related revenues of the firm (b = −.11, p <.001), a significant positive effect on total costs (b =.19, p <.001), and a significant negative effect on franchisor net income (b = −.07, p <.01). These findings suggest that FELIT bears significant financial implications for franchisors by affecting both the revenue and cost aspects of the franchise operations, thus also influencing franchisor net income.To further evaluate the marginal effects of FELIT on franchisor net income, we calculated the margins of franchisor financial performance. The results indicated that the average level of financial performance we examined in our data is $646,934. In addition, results in Web Appendix A indicated that a one-unit increase in franchisee-initiated litigation leads to a 7% decrease of franchisor net income, which translates into $45,285.38 (i.e., $646,934 ×.07) in terms of franchisor net income. Incorporating the results of the simple slope analysis, at a high/low level of franchisor training program, a one-unit change in contract ambiguity of franchisor obligations incurs a −35%/−24% change in franchisee-initiated litigation, which is equivalent to a 2.45%/1.68% increase in franchisor net income. Multiplying it by $646,934, the marginal effect of contract ambiguity of franchisor obligations on franchisor net income is $15,849.88/$10,868.49, with a high/low level of franchisor training program, respectively. In a similar fashion, the marginal effect of contract ambiguity of franchisor obligations on franchisor net income is −$1,811.41/$3,622.83, with/without the presence of a franchisee association. These analyses indicate significant financial implications of using contract ambiguity of franchisor obligations strategically in contract design.To ensure that contract ambiguity of franchisor obligations, individually and jointly with the two noncontractual elements investigated, has distinctive effects on franchisee-initiated litigation in comparison with other contract-related constructs, we ran a random-effect generalized least squares estimation of the interaction terms of franchisor training programs and franchisee association with contract completeness and contract specificity of franchisor obligations, respectively. The results, reported in Web Appendix B, indicate that except for the interactive effect between contract specificity of franchisor obligations and franchisee associations (b = −1.99, p <.05), the other interaction terms do not have significant effects on franchisee-initiated litigation, nor is the main effect of contract completeness or contract specificity of franchisor obligations significant. DiscussionThis study worked to gain a greater understanding of contract ambiguity in interorganizational relationships. Specifically, we examined contract ambiguity of a franchisor's obligations, and its joint effects with franchisor training programs and presence/absence of a franchisee association, in the management of a franchise system. Our findings offer initial insights into franchise system management, providing implications for marketing academics and marketing practitioners. Theoretical ImplicationsFirst, we extend the interorganizational contracting literature (e.g., [10]; [16]; [38]; [37]; [50]) by introducing the concept of contract ambiguity from the law literature, bringing forth the likely strategic role contract ambiguity plays in governing business transactions. This advances the literature in that although legal scholars recognize contract ambiguity's prevalence and substantive impact on relationships (e.g., [12], [13]; [17]; [67]), marketing scholars have yet to investigate this topic. This work demonstrates that contract ambiguity of franchisor obligations is distinct from other contracting constructs (e.g., contract completeness and contract specificity of franchisor obligations evidenced in Studies 1–3). Furthermore, our findings provide empirical support to the arguments of [57], who contend that the power advantage of the franchisor results in the franchisor developing the contract to protect its own prerogatives.Second, this work extends the contracting (e.g., [16]; [38]) and franchise (e.g., [ 2]; [ 9]; [37]) literature by demonstrating franchisee responses to contract ambiguity of franchisor obligations. The results indicate that contract ambiguity of a franchisor's obligations stimulates a franchisee's use of an interest-based over a rights-based conflict solving approach (Study 2) and minimizes both intended and actual franchisee-initiated litigation via increased perceived litigation costs (Studies 2 and 3). The ability of contract ambiguity of franchisor obligations to limit franchisee-initiated litigation demonstrates the strategic importance of contract ambiguity, providing empirical support to the conceptual arguments put forth by [12] and insights as to the underlying process by which this occurs. Corroborating prior research on the importance of contract design (e.g., [ 3]), our research demonstrates that a carefully designed contract can be used to foster cooperation and avoid undesirable relational outcomes in business transactions.Third, this study extends the interorganizational literature on the joint effects of contractual and noncontractual elements (e.g., [16]; [59]). Our findings demonstrate that franchisor training programs strengthen the negative effect of contract ambiguity of franchisor obligations on franchisee-initiated litigation. Franchisor training programs provide an opportunity and context for discussion, allowing for the development of shared values and cohesion, which helps suppress conflicts in the relationship. Thus, these findings advance the literature on the development and outcomes of shared values (e.g., [ 6]; [30]) within a franchise system.Furthermore, we find that the presence of a franchisee association can be harmful to the franchisor by mitigating the negative relationship between contract ambiguity of franchisor obligations and franchisee-initiated litigation. These findings, consistent with the arguments of countervailing power ([22]), suggest that a franchisee association builds bonds between franchisees, who work together to offset the power differential in the franchise system (evidenced by the case of Deli Delicious noted previously). This finding extends the literature on franchisee associations (e.g., [18]), demonstrating the consequences of cohesion when connecting peer members of a franchise system. In a larger context, our work contributes to the literature on agent cooperation or agent collusion in the multi-agent context (e.g., [46]; [77]). Managerial ImplicationsOur results reveal several actionable insights for interorganizational governance. First, for franchisors, contract ambiguity of franchisor obligations can be a strategic tool deployed to enhance joint problem solving and collaboration and to deter franchisee-initiated litigation, enhancing financial performance of the franchise system. As indicated by our post hoc analysis, a one-unit increase in franchisee-initiated litigation leads to a 7% (i.e., $45,285.38) decrease in franchisor net income, with more substantive changes when considering the joint effects of noncontractual elements. These results highlight the strategic importance of contract ambiguity of franchisor obligations in hedging against franchisee litigation. As contracts predominate in interorganizational governance, our findings also bear implications to certain interorganizational contexts. For example, the franchisor–franchisee context mirrors the governance context of powerful manufacturers, which act as contract drafters, with less powerful suppliers, which are contract takers.Second, our work highlights the importance of noncontractual elements in interorganizational governance. Our findings suggest that franchisor training programs, when combined with contract ambiguity of franchisor obligations, serve as a buffer against franchisee litigation. These results highlight the importance of franchisors not only viewing training programs as vehicles for increased franchisee efficiency but also perceiving such efforts as an important mechanism that can aid in socializing franchisees, thereby facilitating the management of the franchise system. Thus, we recommend that franchisors invest in training programs that can build shared values with franchisees to enhance cohesion. Extending this to the broader interorganizational governance context, we would highlight the simultaneous practice of socialization efforts with employment of contract ambiguity. For example, a powerful retailer may use supplier training programs as an opportunity to clarify misunderstandings that may arise from the contract, facilitating joint problem solving and collaboration.Third, our results suggest franchisors should be aware of the potential negative consequences of a franchisee association, carefully managing relations with an association for the betterment of the franchise system. Consider the case of Denny's. In 1988 Denny's formed the Denny's Franchisee Council. This council gave franchisees a voice with which to communicate with corporate. However, the Denny's Franchisee Council was abolished in 1997, becoming independent from corporate sponsorship, reforming as the Denny's Franchisee Association. This may be an example wherein the franchisor-sponsored association stimulated bonding between franchisees (instead of bonding with the franchisor), creating countervailing power to the detriment of the franchisor. These findings also extend to other network governance situations. For example, several manufacturers have crated virtual supplier portals, working to not only build relationships with suppliers but also encourage suppliers to connect with one another. Our findings, applied to this context, would caution that the increased connectivity and bonding among suppliers could work counter to the manufacturer's governance efforts. Limitations and Future ResearchAlthough this work makes several contributions to the literature, it is not without its limitations. First, while we find that franchisors use contract ambiguity of franchisor obligations as a cost inhibitor to minimize litigation, there may be other explanations as to why contract ambiguity of franchisor obligations exists. For example, scholars may want to investigate whether franchisors employ contract ambiguity of franchisor obligations with the intention of engaging in active or passive opportunism ([68]). This seems plausible, as scholars have argued that opportunism occurs within franchise systems (e.g., [18]; [74]).Second, franchisors draft agreements with contract ambiguity of both franchisor and franchisee obligations. Scholars could investigate what stimulates heightened levels of contract ambiguity of franchisee obligations. One potential avenue of investigation could be the concept of fairness (e.g., [40]; [43]). For example, scholars could examine whether potential franchisees are more likely to enter into, or remain a part of, a franchise system in which contract ambiguity of members' obligations is roughly equivalent, resulting from perceptions of fairness.Third, while this work brought forth new insights into the effects of a franchisee association, more insights may be gained by examining the effects of different types of associations. For instance, franchisees could participate in a franchisor-sponsored association, franchise-system-specific associations controlled by franchisees, or an independent franchisee association (e.g., American Association of Franchisees and Dealers). Although all of these associations provide a context for socialization among franchisees, they may provide different opportunities for franchisor–franchisee socialization, thereby influencing franchise system governance. Investigation of the possible discrete effects across association types is warranted.Fourth, this work was specifically placed within the context of a franchise system. However, as we have noted throughout, there are multiple situations in which power differentials in interorganizational relationships exist, wherein one party may want to exert strategic influence through the inclusion of contract ambiguity in relation to its obligations (e.g., powerful buyers working with less powerful suppliers, large retailers working with small brand manufacturers in a store-within-store context, firms possessing highly valued assets engaging in alliances). Future research should test the boundary parameters of the phenomenon. "
31,"Effects of Liberalization on Incumbent Firms' Marketing-Mix Responses and Performance: Evidence from a Quasi-Experiment Many markets are liberalizing by opening up their economies to foreign competition, with the expectation that this will increase economic growth. While foreign competitors with superior technology and management practices pose serious threats to incumbent firms, they also provide them an opportunity to gain new marketing knowledge. How do incumbent firms respond to liberalization? Can incumbent firms' marketing-mix responses affect their performance following liberalization? Addressing these questions, the authors examine incumbent firms' marketing-mix responses to liberalization and the impact of these responses on performance, using the quasi-experiment of liberalization reforms in India. Estimation results from a panel of 3,927 firms in the period 1989–2000 suggest that while all incumbent firms intensified their product and promotions in response to liberalization, only incumbent firms with greater domestic market knowledge intensified their advertising and distribution responses. Furthermore, incumbent firms' marketing-mix responses significantly affect their performance outcomes. The research's findings extend theory and provide practical guidelines on how incumbent firms can design marketing-mix responses to liberalization to improve performance.KEYWORDS_SPLITIn recent years, several governments have liberalized, opening their domestic markets to foreign investment. Some have argued that liberalization of a market stimulates economic growth ([34]). Yet, because of the superior technologies, products, and management practices of foreign firms, managers of incumbent firms worry that liberalization will hurt their firms' performance ([58]). How do incumbent firms respond to the opening up of their markets through liberalization? In this research, we examine incumbent firms' marketing-mix responses to liberalization.From a theoretical perspective, a significant body of work has examined firms' responses to competition. Extant marketing literature on foreign competition studying the opening up of markets has examined the entry strategies of grocery retail firms into transition economies following liberalization ([30]) and the entry of multinational firms into fast-growing emerging markets ([41]). From an incumbent firm perspective, extant research in marketing has examined the effect of new product introductions, short-term marketing attacks, and new domestic firm entry on incumbent firms' marketing responses and performance ([ 4]; [32]; [53]; [59]; [63]).However, the marketing literature has overlooked how liberalization affects the marketing-mix responses of incumbent firms. We consider this a surprising omission, as liberalization represents a dramatic transition from a protected market to a more open, consumer-oriented market ([46]), suggesting an important role for incumbent firms' marketing-mix responses. In addition, existing studies on incumbent firms' responses have focused on a single industry, such as retail or banking, studying marketing-mix responses specific to the industry in question, which raises the question: what are the common factors that drive marketing-mix responses of firms in different industries to liberalization? In Figure 1, we present an organizing framework for the extant research on competition, which we further elaborate on in Table A1 of Web Appendix W1.Graph: Figure 1. Organizing framework for research on competition.Scholars in economics and international business have studied liberalization, focusing on the effects of liberalization on the performance of incumbent firms. Some studies report that liberalization improves incumbent firms' performance (e.g., [37]) through access to new knowledge, whereas others report that liberalization can hurt incumbent firms' performance (e.g., [ 5]; [43]). One reason for these conflicting findings may be that incumbent firms differ in their marketing-mix responses to liberalization, which in turn may affect their performance. Yet, as we noted previously, incumbent firms' marketing-mix responses to liberalization have been overlooked in the literature.Crucially, although prior research has examined firms' marketing-mix responses to domestic competition ([ 4]; [63]), from the perspective of incumbent firms, liberalization creates distinct competitive characteristics with no comparable analog in domestic competition. Following liberalization, there are many more new competitors originating in different countries with superior marketing and management practices ([36]; [51]) than in domestic competition. Thus, for incumbent firms, liberalization is a source of knowledge about superior marketing and management practices. At the same time, following liberalization, incumbent firms, relative to foreign entrants, have an advantage in their knowledge of domestic institutions and markets. Consequently, it is unclear whether insights on incumbent firms' marketing-mix responses to domestic competition, the primary focus of previous research, apply to liberalization.Thus, we examine incumbent firms' marketing-mix responses to liberalization and how their marketing-mix responses, in turn, affect their performance. We consider the ""4Ps"" of incumbent firms' marketing-mix responses, which we have adapted as advertising, product, promotions, and distribution ([ 9]). We note that although the traditional conceptualization of the 4Ps of the marketing mix includes price, we are unable to use it because the unit of analysis for price is a product, rather than the firm, which is the unit of analysis for this research. Instead, we consider the firm's promotions—that is, its spending on discounts and rebates, which is available at the firm level and may be considered a proxy for its pricing strategies. Specifically, we examine whether incumbent firms intensify (i.e., increase) their marketing-mix responses or mute (i.e., decrease) them in response to liberalization.We note that there are different types of liberalization, including trade liberalization, which lowers import tariffs to bring in cheaper products; stock market liberalization, which allows foreigners to purchase shares in the country's stock market; and foreign direct investment (FDI) liberalization, which removes restrictions related to foreign firm ownership, encouraging foreign firm entry in a market. Because FDI liberalization not only increases competition for incumbent firms but also creates learning opportunities for them through observation of foreign entrants, we focus on the effects of FDI liberalization on incumbent firms' marketing-mix responses.The research's findings also have high managerial relevance. Countries liberalize in a quest for economic growth, creating challenges for managers of incumbent firms who are, naturally, concerned about the negative effects of liberalization. By studying the factors that influence incumbent firms' marketing-mix responses and the effects of these responses on performance, this research provides managers of incumbent firms insights on developing appropriate marketing-mix responses to liberalization.Knowledge is a key asset representing a source of sustainable competitive advantage for firms ([33]; [52]). Applying this concept, we propose that incumbent firms' knowledge plays a unique role in the context of liberalization. Although incumbent firms have an advantage in their knowledge of domestic institutions and market forces, compared with foreign entrants, they are disadvantaged in their knowledge of superior management practices and how to effectively operate in liberalized open markets. Anchoring our theoretical development in the knowledge-based view of the firm ([33]), we propose that the domestic market knowledge and foreign market knowledge of incumbent firms will influence their marketing-mix responses (i.e., advertising, product, promotions, and distribution) to liberalization.To establish empirical identification of the effects of liberalization on incumbent firms' marketing-mix responses, we seek a context where the liberalization of the market is exogenous. One such context is India in 1991, where, following a severe balance of payments crisis, the Indian government enacted FDI liberalization reforms. We use the exogenous variation in the FDI liberalization of Indian industries (some industries were liberalized while others were not) to estimate the causal effect of liberalization on incumbent firms' marketing-mix responses using a differences-in-differences approach. We measure incumbent firms' domestic market knowledge by their business group affiliation and their foreign market knowledge by their foreign exchange earnings and spending. We then examine the effects of incumbent firms' marketing-mix responses on their performance, measured by their profitability.The results indicate that, on average, incumbent firms intensified their product and promotion responses to liberalization. Furthermore, there is heterogeneity in incumbent firms' marketing-mix responses to liberalization: whereas incumbent firms with greater domestic market knowledge intensified their marketing-mix responses to liberalization, incumbent firms with greater foreign market knowledge muted their marketing-mix responses. Additional analysis relating incumbent firms' marketing-mix responses to liberalization to their performance indicates contingent effects based on their domestic and foreign market knowledge.The study's findings make four contributions to the extant literature. First, we extend the marketing-mix response literature, which has primarily focused on incumbent firms' responses to domestic competition, by generating insights on incumbent firms' marketing-mix responses to liberalization, an important and substantive context which has hitherto not been examined in the literature. Second, the extant marketing literature on incumbent firm responses has primarily focused on how the size and financial capacity of incumbent firms affect their marketing-mix responses and performance. By demonstrating the key role of domestic and foreign market knowledge of incumbent firms on their marketing-mix responses, we identify a novel driver of their responses. Third, we also contribute to the marketing metrics literature by considering all four marketing-mix variables (advertising, product, promotions, and distribution) driven in part by our focus on a key emerging market (India) and the use of the Prowess database of the Centre for Monitoring Indian Economy (CMIE). In this regard, we note that much of the extant research in marketing metrics has focused on advertising and research-and-development (R&D) spending. Finally, this research's findings also contribute to the economics and international business literature on liberalization, which has overlooked the marketing attributes of incumbent firms.For managerial practice, the integration of the various findings across incumbent firms' marketing-mix responses and performance models indicates that a more intense marketing-mix response following liberalization does not necessarily ensure superior performance. The specific pattern of findings generates actionable guidelines for managers of incumbent firms facing liberalization, based on their domestic and foreign market knowledge. Our findings are also useful to ( 1) managers of foreign firms entering liberalized markets, to identify which incumbent firms will be strong competitors; ( 2) policy makers, to help incumbent firms perform better following liberalization, thus achieving economic growth without hurting domestic firms; and 3) investors, to identify incumbent firms for investment in newly liberalized markets.We organize the rest of the article as follows. We first present our conceptual framework and theory related to the effects of liberalization on incumbent firms' marketing-mix responses. We then discuss the data, method, and results. We conclude with a discussion of the paper's theoretical contributions, implications for marketing practice, and limitations and opportunities for further research. Theoretical BackgroundWe first provide an overview of liberalization and discuss how liberalization differs from domestic competition, the focus of most previous research on competition in the marketing literature. Next, we discuss the related literature that informs incumbent firms' marketing-mix responses to liberalization. Following that, using the knowledge-based view of the firm ([33]) as the theoretical anchor, we propose that incumbent firms' domestic market knowledge and foreign market knowledge will influence their marketing-mix responses to liberalization. Liberalization: A Brief OverviewLiberalization intensifies competition in an industry by increasing the number of firms and introducing new ways to compete in the marketplace ([16]). Typically, during liberalization, the permitted level of foreign ownership of firms is increased to encourage the entry of foreign firms. If a foreign firm is already in the market, liberalization provides the firm's foreign owner an opportunity to increase ownership and control over its operations in the market ([21]). Liberalization is undertaken by host countries both in the expectation of generating foreign exchange and jobs and, more importantly, to realize benefits to the economy through spillovers from foreign firms ([28]). Spillovers from liberalization include, for example, improvements in incumbent firms' practices through diffusion of the superior management practices of foreign firms entering the market ([73]). Following liberalization, the host country devotes considerable attention and resources to attracting foreign firm investments (Kosová [43]).Foreign firms that enter a market following liberalization are typically multinational enterprises that take advantage of differences in knowledge and expertise around the world ([71]). Thus, through exposure to sophisticated foreign competitors, liberalization enables incumbent firms to acquire knowledge of efficient production techniques, innovative marketing strategies, and novel technologies and product designs ([12]). For all of these reasons, we suggest that liberalization offers incumbent firms opportunities for learning, which occurs when their experiences generate systematic changes in their behaviors ([49]). Key differences between liberalization and domestic competitionLiberalization differs on multiple dimensions from domestic competition, the primary focus of most prior marketing scholarship (e.g., [25]; [53]). First, when markets are liberalized, incumbent firms that have been accustomed to operating in a protected market experience a shock, as the market transitions from one that is closed to one that is more consumer oriented ([10]). Following liberalization, incumbent firms with experience only in protected markets may be disadvantaged relative to foreign entrants with experience in liberalized, open markets ([38]). Thus, incumbent firms may be unfamiliar with the strategies and practices of foreign firms entering the market following liberalization. Compared with domestic competition, liberalization is likely to create high uncertainty for incumbent firms. Second, foreign entrants following liberalization have superior marketing and management practices and strengths in creating intangible assets such as brands and/or innovative technologies ([51]; [35]) to which incumbent firms may have had little prior exposure. Thus, liberalization may be a source of substantial knowledge for incumbent firms, which can learn more about new technologies, brand management, and business processes from their foreign competitors ([11]) than may be possible from domestic competitors. Third, the foreign firms encountered by incumbent firms following liberalization originate in different countries ([72]). Thus, following liberalization, there is greater heterogeneity in the competitors faced by incumbent firms. This is in contrast to domestic competition, in which, by definition, competitors are from the local market. Finally, a competitive advantage for incumbent firms facing liberalization is their knowledge of domestic consumers, market forces, and institutions, which is less likely to be an advantage during domestic competition, in which all firms have this common local knowledge. Liberalization and Incumbent Firms' Marketing-Mix ResponsesScholars in economics and international business have devoted attention to identifying the effects of liberalization on incumbent firms' performance ([ 5]; [37]). Liberalization can improve incumbent firms' performance by enabling them to acquire new knowledge from foreign firms ([61]). However, foreign firms can crowd incumbent firms out of markets for product, labor, and capital, causing them to lose market share and/or exit the industry ([ 2]). In summary, there is mixed evidence on how liberalization affects incumbent firms' performance, which appears to vary on the basis of their size, financial strength, and technological capabilities ([14]). We conjecture that one reason for the mixed evidence is that prior research has ignored the marketing responses of incumbent firms to liberalization, a crucial market disruption. Thus, we contend that a more complete picture of the effects of liberalization on incumbent firms' performance can be obtained through a study of the marketing-mix responses of incumbent firms to liberalization.Marketing can improve firm performance by increasing brand equity, customer equity, and customer satisfaction ([ 3]; [29]), all of which can help incumbent firms compete against incoming foreign firms following liberalization. In addition, foreign firms that enter a market following liberalization have strong intangible assets and marketing practices ([36]). Thus, incumbent firms may need to intensify their marketing-mix responses to counteract foreign competitors. Furthermore, observing and imitating foreign entrants may cause incumbent firms to intensify their marketing-mix responses. Finally, liberalization alters demand patterns, highlighting the need for firms to offer and deliver new products and services in new ways valued by consumers. Thus, it may be necessary for incumbent firms to intensify their marketing-mix responses to maintain strong performance. Next, we provide a brief description of how intensifying the four marketing-mix responses may improve incumbent firms' performance following liberalization, making them better positioned to compete in the new environment. AdvertisingAdvertising strengthens brand image, leads to greater awareness ([56]), differentiates products, and builds brand equity ([42]). Advertising also signals product quality, creating a positive and enduring effect on sales ([ 9]) and profitability through differentiation. ProductLike advertising, product activity (e.g., innovations) enhances perceived quality, increases purchase likelihood, and builds brand equity ([13]). Product line length, specifically, is positively related to sales and performance in the long run ([ 8]). PromotionsPromotions allow a firm to increase the sales of its products and extract cash flows from its intangible assets of brand equity and technology ([15]). DistributionDistribution ensures that a firm's products are readily available, increases customer demand, and is a strong driver of both its sales and profits ([ 8]; [60]). Effect of Liberalization on Incumbent Firms' Marketing-Mix ResponsesThere are many reasons why the entry of foreign firms after liberalization may affect incumbent firms' marketing-mix responses. First, following liberalization, the market transitions from being protected to consumer oriented. To counter the uncertainty of the changing environment, incumbent firms may respond by intensifying their marketing-mix responses—that is, advertising, product, promotions, and distribution ([ 9]).Second, foreign firms that enter a market after liberalization, with their superior intangible assets and marketing practices ([36]; [51]), present learning opportunities for incumbent firms ([49]). Thus, any knowledge transfers from foreign firms to incumbent firms—through imitation, forward and backward linkages with suppliers and distributors, and employee transfers ([61])—may increase the intensity of incumbent firms' marketing-mix responses.Third, incumbent firms' existing ties to trade partners, brand recognition, and knowledge of consumers may be a competitive advantage that foreign entrants lack. To utilize this competitive advantage, following liberalization, incumbent firms may intensify their marketing-mix responses so that they can deliver products that consumers value and better cater to consumers' preferences; this, in turn, may help incumbent firms retain consumers as well as maintain and improve their performance.Finally, because foreign entry following liberalization typically entails local manufacturing by foreign firms, they may have made significant local financial investments, signaling their long-term commitment to the local market. Most foreign firms that enter a market after liberalization possess strong financial and managerial resources ([51]). Thus, incumbent firms may infer that these foreign entrants will continue to operate in the market in the long run even if they are not profitable in the short run ([44]). Given the serious nature of the competitive threat posed by foreign firms, incumbent firms may respond by intensifying their marketing-mix responses. Accordingly, we hypothesize: H1:  In response to liberalization, incumbent firms intensify their marketing mix. Incumbent Firms' Knowledge and Marketing-Mix Responses to LiberalizationFirms compete not only through the creation, replication, and transfer of their own knowledge but also through their ability to absorb the knowledge of competitors ([71]). Thus, following liberalization, incumbent firms' responses and performance will be determined by their existing knowledge as well as their ability to absorb new knowledge about superior management and marketing practices from foreign entrants.A firm's ability to recognize the value of new, external knowledge, assimilate it, and apply it to commercial ends is largely a function of its prior knowledge ([23]). Because liberalization creates knowledge diffusion of superior management and marketing practices, we propose that incumbent firms' existing knowledge will affect their ability to absorb new knowledge from foreign entrants, which, in turn, will affect their marketing-mix responses. Following this line of reasoning, we propose that incumbent firms' marketing-mix responses to liberalization will be influenced by their knowledge, which enhances their responsiveness and adaptability to institutional changes ([33]).Specifically, we propose that, when faced with liberalization, incumbent firms with greater domestic market knowledge are better situated to learn superior management and marketing practices from foreign entrants ([23]; [50]). Likewise, we propose that incumbent firms' prior knowledge of foreign markets, firms, and consumers may help them assess the threat posed by foreign firms and learn from these firms' marketing actions ([11]). Thus, we propose that incumbent firms' domestic and foreign market knowledge will influence their marketing-mix responses to liberalization. We present the conceptual framework in Figure 2.Graph: Figure 2. Conceptual framework relating liberalization to incumbent firms' marketing-mix responses.Notes: For ease of presentation, we do not show the main effects of the various moderators on incumbent firms' marketing-mix responses, though we do include these in our estimated model. Domestic Market Knowledge and Incumbent Firms' Marketing-Mix ResponsesIncumbent firms with greater domestic market knowledge have accumulated knowledge of the tastes and preferences of domestic consumers and the practices of domestic trade partners ([65]). This domestic knowledge will be a source of competitive advantage relative to foreign entrants, which incumbent firms can leverage to better address the market's needs by intensifying their marketing-mix responses. Incumbent firms' domestic market knowledge may create a synergistic effect, enabling them to learn effectively from foreign firms. Thus, we propose that incumbent firms with higher domestic market knowledge may imitate foreign entrants that are much stronger in marketing, thereby intensifying their marketing-mix responses.Moreover, because incumbent firms with greater domestic knowledge have superior knowledge of domestic institutions and market forces ([50]), they may have greater awareness of the competitive threats posed by foreign entrants following liberalization and respond to these threats by intensifying their marketing-mix responses. Firms perceive competitors with similar resources and capabilities as relevant threats ([20]). Consequently, following liberalization, incumbent firms with high domestic market knowledge may consider incoming foreign firms their natural competitors and, in response, intensify their marketing mix. Thus, we hypothesize: H2:  The effect of liberalization on incumbent firms' marketing-mix responses is stronger for incumbent firms with high (vs. low) domestic market knowledge. Foreign Market Knowledge and Incumbent Firms' Marketing-Mix ResponsesIncumbent firms with greater foreign market knowledge, by definition, may have prior insights into the marketing practices of foreign firms. Through their ties with various economic actors in foreign markets (e.g., importers, exporters, trade partners, buyers, competitors, and governments), firms with greater foreign market knowledge can learn about more efficient product designs and marketing strategies deployed by foreign firms ([27]). Thus, such prior foreign market knowledge may enable incumbent firms to absorb new knowledge from foreign entrants and imitate their practices. Furthermore, foreign market knowledge may increase incumbent firms' awareness of the superiority of foreign firms, causing them to perceive foreign entrants as strong threats. All of these arguments suggest that incumbent firms with foreign market knowledge may intensify their marketing-mix responses to liberalization.At the same time, as incumbent firms with greater foreign market knowledge may have competed with foreign firms in other markets, they may have already increased their marketing mix to attract foreign buyers and may not intensify their marketing-mix responses to liberalization. Furthermore, the knowledge gap on marketing practices between incumbent firms with foreign market knowledge and foreign entrants may not be substantial, such that liberalization may not provide strong learning opportunities for these incumbent firms ([47]). Thus, incumbent firms with greater foreign market knowledge may mute their marketing-mix responses to liberalization. Given that the extant literature suggests competing predictions of how incumbent firms' foreign market knowledge may affect their marketing-mix responses, we propose the following competing hypotheses: H3:  The effect of liberalization on incumbent firms' marketing-mix responses is (a) stronger or (b) weaker for incumbent firms with higher foreign market knowledge. SummaryLiberalization creates uncertainty and turbulence for incumbent firms as well as opportunities to learn superior management and marketing practices from incoming foreign firms, to which incumbent firms may respond by intensifying their marketing mix. Incumbent firms' marketing-mix responses may be influenced by their extant knowledge—specifically, their domestic market knowledge and foreign market knowledge. While incumbent firms' domestic market knowledge may cause them to intensify their marketing-mix responses, the extant literature provides competing predictions on how incumbent firms' foreign market knowledge may influence their marketing-mix responses, which we resolve empirically. Method Identification StrategyA key concern for estimating the effects of liberalization on incumbent firms' marketing-mix responses is endogeneity, which can arise from two primary sources: reverse causality, and omitted variables. Reverse causality may occur if incumbent firms' marketing-mix responses erect strategic barriers, discouraging foreign firms from performing effectively in the new market. Alternatively, omitted variables can be a source of endogeneity, as foreign firms may enter markets with high growth prospects or weak incumbent firms. Because the inclusion of firm-level controls and firm fixed effects may not capture all sources of endogeneity, we seek a context with an exogenous shock of liberalization, which provides a quasi-experimental setting for hypothesis testing. One such context, which provides an institutional setting to make robust inferences, is India's FDI liberalization reform in 1991, as we describe next. Empirical ContextBefore 1991, the Indian government had a protectionist, inward-focused economic policy. In early 1991, various macroeconomic developments including deficits, increase in oil prices, and political uncertainty led to a balance-of-payments crisis in the Indian economy. To manage this economic crisis, the Indian government sought financial assistance from the International Monetary Fund, which offered support conditional on the implementation of liberalization reforms that would integrate the Indian economy with the global economy ([66]). In response, the Indian government enacted FDI liberalization reforms in August 1991. This liberalization entailed the reduction of FDI barriers (i.e., the percentage of FDI equity allowed was increased from 40% to 51% in 46 of the 129 primary industry categories defined according to a three-digit industrial code; [55]). In the remaining industries, the limit on FDI equity remained at 40%, and foreign investors had to obtain approval from the Indian government to increase their investment above 40%. After liberalization, FDI inflows dramatically increased. The stock of FDI in India increased from less than US$155 million in 1991 to US$586 million by 1993 ([54]).To eliminate political opposition to the FDI liberalization reform, the Indian parliament enacted it without much debate, creating an exogenous shock for incumbent firms. As Dr. Chelliah, a member of the Planning Commission (the body responsible for the reform) noted, ""When we started economic reforms in 1991...we didn't have time to sit down and think exactly what kind of a development model we needed....There was no systematic attempt to see two things: one, how have the benefits of reforms distributed, and two, ultimately what kind of society we want to have, what model of development should we have?"" ([68]). In support of the view that India's 1991 liberalization reform was an exogenous shock, it has been used as a quasi-experimental setting in the economics ([ 1]; [66]) and finance ([ 6]) literature streams.Thus, we exploit the exogenous shock of FDI liberalization reforms in India in 1991 to estimate the causal effects of liberalization on incumbent firms. The exogenous FDI liberalization reform offers two advantages with respect to estimation. First, it enables us to alleviate concerns resulting from endogeneity. Because the FDI liberalization reform was sudden and unanticipated, there was no time for firms to lobby for or against it, precluding concerns of reverse causality. Furthermore, the presence of restrictive policies related to foreign competition before the reform prevents an unobserved variable (e.g., the firm's intention to compete with foreign firms) from influencing the key explanatory variables (domestic and foreign market knowledge) and dependent variables (marketing-mix responses). Second, the quasi-experimental setting of the FDI liberalization reform in India (i.e., by which some but not other industries were liberalized) enables us to account for other macroeconomic factors that may affect incumbent firms' marketing-mix responses. Thus, we are able to isolate the causal effect of liberalization on the marketing-mix responses of incumbent firms ([67]) and the contingent effects of incumbent firms' knowledge on their marketing-mix responses to liberalization. Exogeneity of the FDI liberalization shockThe Industrial Policy Resolution of 1991 ([55]) provides the list of industries that were liberalized to FDI. We empirically confirm the exogeneity of FDI liberalization in India using kernel density plots of firm characteristics (i.e., assets, profitability, and sales; see Figure A1 of the Appendix) in industries that were liberalized (vs. not). The kernel density plots indicate that the distribution of firm characteristics is largely similar across the two groups before FDI liberalization. In addition, in Table A1 of the Appendix, we report t-tests comparing average values of key variables across liberalized and unliberalized industries before FDI liberalization in 1991 and find no significant differences. DataWe use data on Indian and foreign firms from the CMIE Prowess database to examine the effects of liberalization on incumbent firms' marketing-mix responses. Firms in this database account for 75% of all corporate taxes and more than 95% of excise duties collected by the Indian government. We collect data on both incumbent and foreign firms' advertising, promotions, and distribution spending; number of products; group membership; foreign exchange earnings; foreign exchange spending; total assets; total sales; earnings before interest, taxes, depreciation, and amortization (EBITDA); and R&D spending between 1988 and 2000. We provide details of the marketing data classification by Prowess in the Web Appendix W2. The first year for which data is available in the Prowess database is 1988. We lose one year because of lagging explanatory variables, so the final sample spans a 12-year period (1989–2000).We exclude observations of incumbent firms ( 1) incorporated after 1991, ( 2) for which asset information is missing, ( 3) for which the reporting period is not 12 months, and ( 4) from industries with fewer than two firms, and lose one year of observations due to lagged variables, after which, we are left with a sample of 16,636 firm-year observations for estimation. We describe the constructs, measures, and data sources in Table 1.GraphTable 1. Constructs, Measures, and Data Sources.   Measures LiberalizationThe liberalization variable is coded as 1 for firms in industries that were liberalized to FDI and 0 for firms in industries that were not liberalized to FDI (M =.467, SD =.499). As the FDI liberalization policy was implemented at the three-digit National Industrial Classification (NIC) level, we compute the variable at this level. PostWe code the post variable as 1 for firm-year observations after FDI liberalization in 1991 and 0 for those before FDI liberalization (M =.894, SD =.308). Marketing-mix responsesAdvertising represents the firm's spending on media. To exclude any scale effects of firm size, we scale advertising spending (and other spending) by the firm's total assets (M =.010, SD =.025). Product represents the length of the firm's product portfolio, which we obtain from firms' annual reports (Indian firms are required by the Companies Act of 1956 to disclose product-level information in their annual reports). We measure the firm's product by the number of products reported by the firm in a given year (M = 4.573, SD = 4.430). Promotions represents a firm's spending on rebates, discounts, and sales promotions (M =.021, SD =.038). Distribution represents a firm's spending to deliver products to various channel intermediaries, such as retailers, wholesalers, and distributors, including on consignment and loss of goods in transit (M =.026, SD =.040). Domestic market knowledgeWe measure a firm's domestic market knowledge using its business group affiliation. Firms affiliated with business groups have cross-shareholding and interlocking directorates, which facilitate knowledge sharing between business group firms ([11]). Typically, within business groups, there is a core administrative department ([70]) responsible for sharing information. Prowess classifies firms as belonging to business groups or not, based on multiple sources including continuous monitoring of firm shareholding, news announcements, and understanding of firms' group-related behaviors ([22]). Following prior research using this classification ([17]), we code the domestic market knowledge variable as 1 if the firm belongs to a business group and 0 otherwise (M =.498, SD =.500). Foreign market knowledgeExtant research ([31]; [45]) suggests that firms can gain experience of foreign sellers by importing inputs and can gain knowledge about foreign buyers' practices and processes by exporting to them ([64]). Thus, we operationalize a firm's foreign market knowledge using a sum of two measures: ( 1) foreign exchange spending, which includes spending on import of raw materials, stores and spares, finished goods, capital goods, royalties and technical knowhow, and ( 2) foreign exchange earnings, which includes earnings from export of goods and services. To obtain the measure of foreign market knowledge, we sum these two measures and scale it by the firm's total assets (M =.146, SD =.313).[ 6] For missing values of foreign exchange spending and foreign exchange earnings, we impute values to.0001.[ 7] Control variablesWe control for firm size using the lag of total assets, firm profitability using the lag of return on assets (EBITDA/Total assets); firm R&D using R&D spending[ 8]; domestic competition using the cumulative market share of all domestic firms in an industry, thus accounting for the relative participation of domestic firms in an industry; and industry concentration using the Herfindahl–Hirschman index. We provide the descriptive statistics and correlation matrix of all the key variables used in the estimation, winsorized at.25%, in Table 2.GraphTable 2. Descriptive Statistics and Correlation Matrix of Key Variables.  1 *p <.10.2 **p <.05.3 ***p <.01. Estimation Approach: Differences-in-DifferencesA quasi-experiment is defined as a naturally occurring contrast between a treatment and a comparison condition in which the cause cannot be manipulated ([24]). We argue that a quasi-experiment occurred during the time of FDI liberalization in India because of an exogenous event: the FDI liberalization of some Indian industries but not others. As a result of FDI liberalization, there was a natural treatment group (firms in industries liberalized to FDI) and a control group (firms in unliberalized industries), which we use to measure the causal effect of liberalization.We examine the effects of liberalization on incumbent firms using the differences-in-differences method, which is well-suited to establishing causal claims in a quasi-experiment ([18]; [67]). The differences-in-differences method compares the effect of the event (in this case, FDI liberalization) on incumbent firms in industries that are liberalized (the treatment group) with those in industries that are not (the control group). To examine the effect of liberalization on incumbent firm's marketing-mix responses, we subtract the average of a firm's marketing-mix response before the event from the average value of its marketing-mix response after the event. However, other factors might have changed as well. Thus, we use incumbent firms in the control group to account for any other observable or unobservable factors.We provide model-free evidence by dividing incumbent firms into two groups: liberalized firms (the treatment group) and unliberalized firms (the control group). We examine two periods, pre- (before FDI liberalization in 1991) and post- (after FDI liberalization in 1991). We compute the difference for the firms in the liberalized group pre- and postliberalization for each marketing-mix response variable by collapsing to averages for each period. Similarly, we compute the difference for the firms in the unliberalized group and then compute the difference between these two differences to obtain the differences-in-differences estimate. The model-free evidence suggests that, in response to liberalization, incumbent firms intensified their product (b =.617, p < .01) and promotions (b =.003, p <.05), but not their advertising and distribution responses.To estimate the differences-in-differences model, we regress the incumbent firm's marketing-mix response on the main effects of liberalization and post variables, their interaction, and the control variables. We control for unobserved heterogeneity using firm fixed effects and for any effects of time using year fixed effects. Marketing-mix responseit=α0i+α1Liberalizationit+α2Postit+α3(Liberalizationit×Postit)+α4Controlsit+γt+∊it, Graph1where  α0i  represents the firm fixed effect,  α3  represents the causal effect of liberalization on incumbent firms' marketing-mix responses, and  γt  represents the year fixed effects. We estimate four such equations for each marketing-mix response of incumbent firms: advertising, product, promotions, and distribution.We exploit the cross-sectional variation in the treatment and control groups to estimate the heterogeneous treatment effects of liberalization on incumbent firms' marketing-mix responses contingent on their domestic market knowledge and foreign market knowledge ([67]). This specification enables us to examine the effects of knowledge on incumbent firms' marketing-mix responses in liberalized industries (vs. those in unliberalized industries) following liberalization. Thus, we estimate the following model: Marketing-mix responseit=β0i+β1(Liberalizationit×Postit×Domestic market knowledgeit)+β2(Liberalizationit×Postit×Foreign market knowledgeit)+β3(Postit×Domestic market knowledgeit)+β4(Postit×Foreign market knowledgeit)+β5(Liberalizationit×Domestic market knowledgeit)+β6(Liberalizationit×Foreign market knowledgeit)+β7(Postit×Liberalizationit)+β8Postit+β9Liberalizationit+β10Domestic market knowledgeit+β11Foreign market knowledgeit+β12Controlsit+γt+∊it, Graph2where i is the subscript for the firm and t is the subscript for the year,  β0i  refers to the firm fixed effect,  γt  represents the year fixed effects,  β1  refers to the heterogeneous treatment effect of liberalization on incumbent firms with domestic market knowledge, and  β2  refers to the heterogeneous treatment effect of liberalization on incumbent firms with foreign market knowledge. Because we include firm fixed effects and year fixed effects, main effects and interaction terms completely predicted by the fixed effects drop out of the estimation. ResultsWe first estimated the differences-in-differences model without the control variables, followed by estimation with the inclusion of the control variables. We present the results of the differences-in-differences model without the control variables in Columns 1–4 of Table 3, Panel A. The results suggest that H1 is partially supported. Although liberalization did not affect incumbent firms' advertising (b = −.001, n.s.), incumbent firms intensified their product (b =.267, p <.01), promotions (b =.002, p <.05), and distribution (b =.002, p <.01) in response to liberalization.[ 9]GraphTable 3. Differences-in-Differences Estimates.  4 *p <.10.5 **p <.05.6 ***p <.01.We present the results of the differences-in-differences model including the control variables in Columns 1–4 of Table 3, Panel B. Again, the results suggest that H1 is partially supported. Although liberalization did not affect incumbent firms' advertising (b = −.000, n.s.) and distribution (b =.001, n.s.), incumbent firms intensified their product (b =.256, p <.01) and promotions (b =.003, p <.01) in response to liberalization.In Columns 1–4 of Table 3, Panel C, we present the results of the heterogeneous treatment effects model, including the effects of incumbent firms' domestic market knowledge and foreign market knowledge. The results support H2, as incumbent firms with greater domestic market knowledge intensify their advertising (b =.003, p <.05), product (b =.330, p <.10), promotions (b =.005, p <.05), and distribution (b =.004, p <.10) to a greater extent in response to liberalization.The results partially support H3b, as incumbent firms with greater foreign market knowledge intensify their promotions (b = −.007, p <.05) and distribution (b = −.008, p <.05) to a lesser extent in response to liberalization. However, liberalization has no effect on the advertising (b =.003, n.s.) and product (b = −.094, n.s.) of incumbent firms with greater foreign market knowledge.Next, we discuss the effects of the control variables. Firm size has a positive effect on incumbent firms' product (b = 17.086, p <.01) and a negative effect on their promotions (b = −.041, p <.05). Firm profitability has a positive effect on incumbent firms' advertising (b =.006, p <.01), promotions (b =.010, p <.01), and distribution (b =.010, p <.01); firm R&D has a negative effect on incumbent firms' product (b = −.190, p <.01); domestic competition has a positive effect on incumbent firms' promotions (b =.011, p <.01); and industry concentration has a negative effect on incumbent firms' promotions (b = −.006, p <.10) and a positive effect on incumbent firms' distribution (b =.009, p <.05). Summary of ResultsThe results of the differences-in-differences model suggest that incumbent firms intensified their product and promotions in response to liberalization. The results of the heterogeneous treatment effects model suggest that although incumbent firms with greater domestic market knowledge intensified all four marketing-mix responses to liberalization, incumbent firms with greater foreign market knowledge muted two of their marketing-mix responses (promotions and distribution) to liberalization. Additional Analyses and Robustness ChecksWe conduct additional analyses to rule out alternative explanations and to verify the robustness of our results to different samples. Because multicollinearity prevents the inclusion of some control variables (i.e., industry fixed effects, tariff changes, number of foreign firms, and foreign firms' marketing) in one comprehensive model, we conducted separate analyses with these control variables, as we discuss next. Reduction in trade tariffsIn addition to FDI liberalization, in 1991, the Indian government also liberalized trade by decreasing trade tariffs in some industries to increase the attractiveness of the market for foreign firms to sell their products (manufactured outside India). Trade liberalization can increase dumping and predatory price-based competition ([39]). To establish that the results are robust to the concurrent reduction in trade tariffs, we control for the reduction in trade tariffs across industries in the estimated model, using an indicator variable coded as 1 if the incumbent firm's industry had reduced tariffs in 1991 (0 if not). We present these results in Table C1 of Web Appendix W3, which are consistent with those in Table 3. Controls for number of foreign firmsIncumbent firms in industries where there were greater number of foreign entrants may be more motivated to intensify their marketing-mix responses. To check if this is the case, we reestimate the model by controlling for the number of foreign firms in an industry. We present these results in Table C2 of Web Appendix W3, which are again consistent with those in Table 3. Controls for foreign firms' marketingIncumbent firms' marketing-mix responses to liberalization may be influenced by the marketing actions of foreign entrants. Thus, we reestimate the models by controlling for foreign firms' advertising in an industry by dividing the average advertising intensity of foreign firms by the average advertising intensity of all firms in the industry in Equation 2 for incumbent firms' advertising responses. Similarly, we reestimate the models by controlling for foreign firms' product, promotions, and distribution in each of the corresponding equations. We present these results in Table C3 of Web Appendix W3, which are consistent with those in Table 3. Industry fixed effectsTo rule out the possibility that industry effects are driving the estimation results, we reestimate the model with the inclusion of industry fixed effects interacted with the Post variable. We present these results, which are consistent with those in Table 3, in Table C4 of Web Appendix W3. Sampling variationTo ensure that our results are robust across different samples and to different choices for the last year in the panel data, we reestimate the model excluding observations from the year 2000. We present these results, which are consistent with those in Table 3, in Table C5 of Web Appendix W3. Incumbent Firms' Marketing-Mix Responses and PerformanceWe next examine whether incumbent firms' marketing-mix responses to liberalization helped or hurt their performance. To do this, we estimate a model where the dependent variable is incumbent firms' profitability and the key independent variables are their advertising, product, promotions, and distribution responses to liberalization. Because incumbent firms' marketing in the current period may be affected by liberalization, we lag all marketing-mix response variables by one year. To simplify the empirical estimation and avoid the estimation and interpretation of four-way interactions, for the interaction term between Post and Liberalization we use a continuous measure, the extent of foreign competition in a given industry in each year. Thus, for the performance model, we measure the extent of liberalization using the cumulative market share of all foreign firms in the industry.We estimate a model including all three-way interactions of incumbent firms' marketing-mix responses and liberalization with the two moderators from the first stage, domestic market knowledge and foreign market knowledge. We control for unobserved heterogeneity using firm fixed effects and for annual changes using year fixed effects. To control for any industry-level changes in the performance of incumbent firms after liberalization, we include the interaction term between the Post variable and industry dummies in the model. Similar to the estimation of incumbent firms' marketing-mix responses, we control for firm size, firm profitability, firm R&D, domestic competition, and industry concentration. However, because domestic competition is perfectly predicted by the extent of foreign competition in an industry, it drops out of the estimation. Next, we present the equation for the performance model, where i is the subscript for the firm, t for the year, and j for industry, Performanceit=δ0i+δ1×(Domestic market knowledgeit×Liberalizationit×Marketing-mix responseit)+δ2×(Foreign market knowledgeit×Liberalizationit×Marketing-mix responseit)+δ3×(Domestic market knowledgeit×Liberalizationit)+δ4×(Foreign market knowledgeit×Liberalizationit)+δ5×(Domestic market knowledgeit×Marketing-mix responseit)+δ6×(Foreign market knowledgeit×Marketing-mix responseit)+δ7×(Liberalizationit×Marketing-mix responseit)+δ8Liberalizationit+δ9Domestic market knowledgeit+δ10Foreign market knowledgeit+δ11Marketing-mix responseit+δ12Controlsit+δ13×(Postit×ϑj)+γt+∊it. GraphWe present the performance model in Table 4. We present a comparison of incumbent firms' marketing-mix responses with the effect of their marketing-mix response on performance (using the directionality of the three-way interactions) in Table 5. Next, we discuss the convergence and/or divergence between incumbent firms' marketing-mix responses and appropriate marketing-mix responses as indicated by the parameter estimates of the three-way interaction terms in the performance model.GraphTable 4. Incumbent Firms' Marketing-Mix Responses and Performance during Liberalization.  7 *p <.10.8 **p <.05.9 ***p <.01.GraphTable 5. Comparison of Findings on Incumbent Firms' Marketing-Mix Responses and Performance.   Domestic market knowledgeThe results of the marketing-mix response model in Table 3 indicate that incumbent firms with greater domestic market knowledge intensified their advertising, product, promotions, and distribution in response to liberalization. The results of the performance model (Tables 4 and 5) indicate that this was partially the appropriate strategy for them, as their distribution (b = 1.610, p <.10) responses improve their performance. However, the results of the performance model indicate that for incumbent firms with greater domestic market knowledge, their advertising (b = −.712, n.s.), product (b = −.007, n.s.), and promotions (b = −.573, n.s.) responses—which they intensified—did not improve their performance following liberalization, suggesting opportunities for them to improve their performance. Foreign market knowledgeWhile the results of the marketing-mix response model (Table 3) indicate that incumbent firms with greater foreign market knowledge muted their promotions and distribution responses to liberalization, the performance model (Tables 4 and 5) indicates that this was also partially the appropriate strategy for them to follow, as both decreasing their distribution responses (b = −2.257, p <.01) and increasing their promotions responses (b = 1.661, p <.10) can improve their performance. General DiscussionSeveral markets are liberalizing, transitioning from protected markets to open markets. These transitions dramatically change the competitive environments faced by incumbent firms in these markets, suggesting a key role for their marketing responses on their performance. Yet there are few insights on incumbent firms' marketing-mix responses to liberalization. Addressing this research gap, we theorize and find that the market knowledge of incumbent firms influences their marketing-mix responses to liberalization. We exploit a quasi-experiment, the liberalization of the Indian economy in 1991, to causally identify the effects of liberalization on incumbent firms. We conclude with a discussion of the findings' contributions to theory, implications for managerial practice, and the limitations and opportunities for further research. Theoretical ContributionsTo the best of our knowledge, this is the first study to examine the marketing-mix responses of incumbent firms to liberalization. In doing so, it introduces an important phenomenon, liberalization, to the marketing literature, highlighting differences between liberalization and domestic competition, and demonstrates the opportunities in studying foreign competition ([35]). Through this study, we contribute to the marketing literature on competitive response, which has thus far focused on domestic competition ([ 4]; Mukherji et al. [53]). Prior research on domestic competition has suggested that no marketing response is the most common response for incumbent firms ([ 4]; [63]). However, by studying a new type of competition, we find that incumbent firms do respond aggressively to liberalization through their marketing mix, which, in turn, affects their performance. Given that the study of competition is an integral part of marketing, our focus on how firms respond to liberalization enables us to contribute to a significant stream of research in marketing.Second, through our study, we extend the literature on marketing-mix responses and liberalization in a novel way. Extant research has focused on incumbent firms' size and financial capacity as drivers of their response and performance during domestic competition and liberalization ([ 4]; [53]; [72]) but has overlooked incumbent firms' knowledge, a key source of competitive advantage. We show that incumbent firms' knowledge can help explain their marketing-mix responses to liberalization and their subsequent performance. We draw attention to the importance of incumbent firms' domestic market knowledge and foreign market knowledge in the context of liberalization. In doing so, we add a new angle to the literature on liberalization and competitive response, highlighting the need to account for incumbent firms' knowledge. More generally, this research's findings also contribute to the broader marketing literature on market knowledge ([49]; [50]).Third, much of the marketing metrics literature has focused on the effect of advertising ([56]; [69]) and R&D on firm performance (e.g., [62]). By considering the effects of four different types of marketing mix—advertising, product, promotions, and distribution—on firm performance in one study, we extend this literature in a novel way. Moreover, extant studies on marketing-mix responses have primarily examined a single marketing variable, such as advertising or product ([53]), or study industry-specific marketing-mix responses, such as product assortment in the retailing industry ([ 4]). Thus, we contend that these studies may not provide a complete picture of the effects of the marketing mix and risk suffering from omitted variable bias. By considering all 4Ps of the marketing mix and including firms across multiple industries, we provide a comprehensive picture of incumbent firms' marketing-mix responses.Finally, through this research, we contribute to the international business and economics literature, in which there is mixed evidence on the effects of liberalization on incumbent firms' performance (e.g., [ 5]; [43]). We suggest that one reason for the mixed evidence is that extant research has not accounted for important differences between incumbent firms facing liberalization, especially regarding their marketing-mix responses. By demonstrating that incumbent firms respond to liberalization through their marketing mix, which in turn, affects their performance, we help clarify the mixed evidence on the effects of liberalization on incumbent firms. Thus, we extend the literature on liberalization, demonstrating that incumbent firms' marketing-mix responses are a key determinant that must be accounted for when studying the effects of liberalization on the performance of incumbent firms. Managerial ImplicationsThe findings have implications for managers of incumbent firms facing liberalization, managers of foreign entrants, policy makers, and investors. We discuss these in the following subsections. Managers of incumbent firmsManagers of incumbent firms are naturally concerned about the effects of liberalization on their firms' performance. For example, in response to the potential liberalization of the Indian retail sector, Kishore Biyani (chief executive of the largest incumbent retailer in India), stated in opposition to the reform, ""The retail sector...should not be given away to the foreign players while it is too young to compete on a level-playing field"" ([19], p. 83). More recently, founders of incumbent Indian technology startups, who have been fiercely battling U.S. entrants including Amazon and Uber, have argued that foreign competitors destroy domestic industry and entreated the Indian government to introduce protectionist measures ([57]). Our findings offer a mechanism by which managers of incumbent firms can effectively compete with foreign competitors following liberalization (i.e., by adjusting their marketing-mix responses).Our findings suggest that incumbent firms with greater domestic market knowledge should intensify their distribution in response to liberalization. Incumbent firms' knowledge of domestic distribution networks and trade partners is a strong advantage for them, which they can exploit by intensifying their distribution to achieve superior performance following liberalization. Although incumbent firms with greater domestic market knowledge intensify their advertising, product, and promotions responses to liberalization, the findings of the performance model indicate that they do not benefit from these responses. Thus, these incumbent firms could reallocate resources away from these marketing-mix elements to improve performance.Furthermore, our findings suggest that incumbent firms with higher foreign market knowledge cut back on their promotions and distribution in response to liberalization. A potential reason for this may be that these incumbent firms may already be aware of the strength of foreign firms with respect to building intangible assets through advertising and product introductions and may consider additional spending on promotions and distribution to be superfluous in combating these entrants. However, the findings from the performance model suggest that while cutting back on their distribution is an appropriate response to liberalization for incumbent firms with higher foreign market knowledge, these firms can further improve their performance by intensifying their promotions. Finally, these incumbent firms do not respond to liberalization through their advertising and product, which is the appropriate strategy for them, because intensifying these marketing-mix elements does not improve their performance. Managers of foreign entrantsManagers of foreign firms entering newly liberalized markets can use this research's findings to benchmark themselves against incumbent competitors and understand their potential marketing-mix responses. For example, for foreign firms entering a market following liberalization, incumbent firms with higher domestic market knowledge and foreign market knowledge with appropriate marketing-mix responses are likely to emerge as strong competitors. Policy makersPolicy makers may be tempted to heed the demands of business leaders to raise barriers to protect domestic firms from foreign competitors. For example, in response to the Indian government's move to open up the aviation sector to foreign players, Ajay Singh (Chairman of one India's largest aviation incumbent firms) stated, ""We believe the ultimate objective of policy should be to strengthen indigenous aviation....We believe work needs to be done by the government to ensure that we keep strengthening indigenous aviation...making sure the growth remains profitable growth in the country"" ([40]). Our study identifies incumbent firms' marketing-mix responses as a mechanism to prevent their crowding out following liberalization. Policy makers need not accede to the demands of incumbent business leaders to heighten protectionist barriers but can find ways to facilitate incumbent firms' learning from foreign entrants (e.g., by fostering alliances and trade associations). InvestorsOur findings suggest that incumbent firms' marketing-mix responses play an important role in their performance during liberalization. Thus, institutional investors should consider incumbent firms' marketing-mix responses when deciding their targets of investment. Our findings show that when markets liberalize, incumbent firms with higher domestic market knowledge and foreign market knowledge that adjust their marketing-mix responses achieve superior performance, suggesting opportunities for investors in newly liberalized sectors. Limitations and Opportunities for Further ResearchOur study has some limitations that offer opportunities for further research. First, to gain an understanding of the complex phenomenon of liberalization, we focus on the effects of FDI liberalization on incumbent firms. Further research on the marketing-mix responses and performance of incumbent firms during other forms of liberalization, including trade liberalization and stock market liberalization, would be useful.Second, we study two factors (domestic market knowledge and foreign market knowledge) that affect incumbent firms' marketing-mix responses and performance following liberalization. Further research on other factors, including the chief executive officer's and marketing leadership's foreign education or work experience, family ownership, the motivation of foreign entrants, employee salaries, mergers and acquisitions, market share, level of technology, product portfolio and price, laggards versus leaders, and tangible and intangible government incentives would be useful. In addition, although we examine incumbent firms' product line length, because of the lack of data availability, we are unable to study the effects of liberalization on incumbent firms' product innovativeness and quality, which emerges as an area for research. Similarly, we use business group affiliation as a measure of domestic market knowledge. Future research using other measures of domestic market knowledge, including primary data through surveys, would be useful.Third, we examine incumbent firms' marketing-mix responses to liberalization in a single market, India. While this allows for a clean test of the effects of liberalization on incumbent firms, future research could examine whether our findings generalize to other markets (e.g., the United States, Brazil, China).In conclusion, we believe that the findings of this first study on the role of incumbent firms' marketing during liberalization provide novel insights on incumbent firms' marketing-mix responses to liberalization and their subsequent performance implications. Given the increasing pace of liberalization in markets around the world, we hope that our study stimulates additional work in this area. "
32,"Evaluating the Effectiveness of Retailer-Themed Super Saver Events In response to pressure to defend their stand sales against discounters, grocery retailers started engaging in retailer-themed super saver events: promotional events ( 1) specific to the retailer, in which they ( 2) mass advertise ( 3) unusually deep, immediate deals ( 4) across a broad range of categories ( 5) under a common savings theme and deal format. Given these characteristics, such events are expected to generate higher awareness and interest than typical day-to-day promotions, thereby enhancing visits and purchases during the event but also reducing them before and after. The authors evaluate these effects by analyzing 44 retailer-themed super saver events operated by the largest Dutch grocery retailers over four years. They find a substantial increase in visits and total purchases during the event, especially among nonprimary customers and hard-discount shoppers. The larger part of this lift stems from the use of an overarching event theme. Consumers buy less in anticipation of the event and visit the store more often afterward, but for smaller baskets—typically leading to a null effect in terms of profit. Finally, our results suggest that rather than the deal depth or advertising budget, the number of items and media resonance of the theme are key drivers of event success.KEYWORDS_SPLITPressured to defend their sales against (hard) discounters such as Aldi, Lidl, or Walmart, traditional grocery retailers have increased their promotional activities considerably in recent years ([22]; [42]). Studies suggest, however, that simply offering more or deeper discounts hardly generates incremental increases in traffic or sales. Given the high promotional clutter, consumers are often unaware that a product is on sale (e.g., [54]) and, thus, do not buy more, even though they could benefit from the discount ([23]). Even if such deals are noted, they often fail to attract consumers who would otherwise not visit the store. Research has indeed found limited evidence of direct grocery-store switching due to promotions ([25]). This has led ""a growing number of industry stakeholders [to question] the long-term viability of retailers' promotional activities"" ([42]).To address these problems, several grocery retailers have ventured into retailer-themed super saver events (ReTSS). These are promotional events ( 1) specific to and designed by the retailers, in which they ( 2) mass advertise ( 3) unusually deep, immediate deals ( 4) across a broad range of categories ( 5) with a common savings theme and deal format. Examples are Kroger's ""Cart Buster Savings Event"" (also known as ""Mega Sales""), during which the U.S. retailer claims to offer over $100 in savings on a set of products; Woolworths's ""Two-Day Super-Sale"" event, featuring 50% discounts on hundreds of grocery products throughout its online store; Éxito's (the leading grocery chain in Colombia) ""Días de Precios Especiales,"" offering exceptional discounts across a range of items; and Dutch retailer Albert Heijn's ""Hamsterweeks,"" which entice consumers to buy large quantities of groceries through a broad set of buy-one-get-one-free (BOGO) offers over consecutive weeks.Through such events, retailers hope to improve, or at least consolidate, their market position[ 6] by revitalizing their customer base (i.e., attracting extra visitors to the store) and increasing current customers' spending at the store ([21]). By integrating the offers into a common deal format and savings theme, and combining feature ads with mass media to advertise them, these events may generate extra attention and signal unusual bargain opportunities. As such, they may attract extra visitors and/or expand current customers' purchase baskets, thereby generating incremental business.Anecdotal evidence suggests that traffic and basket sizes do increase during the ReTSS period (e.g., [12]; [21]) and that such events are ""the engine behind revenue growth"" ([21], p. 2). This indicates that ReTSS may help traditional retailers defend against price fighters. However, it does not imply that ReTSS are a panacea. First, regular store customers may anticipate upcoming ReTSS and postpone purchases until the event. Moreover, the deep, storewide, and uniformly tagged promotions may simply entice those customers to stock up on larger inventories of more promoted items and to buy less after the event. Second, newly attracted visitors may stay away in postevent weeks, when the retailer's promotional activity returns to business as usual. Thus, some industry analysts express doubts about the net outcomes of these ReTSS ([37]), and any claims that ReTSS are the road to promotion success remain unsubstantiated.A rigorous analysis of ReTSS is currently lacking; this sets the stage for our research. Our contribution is twofold. Substantively, we conceptualize how the combined characteristics of ReTSS could make their effect different from that of business-as-usual retailer promotions. We develop a conceptual framework that lays out the behavioral mechanisms, and, although we do not test these mechanisms per se, we use them to form expectations for how ReTSS affect store visits and purchases. Empirically, we document the impact of ReTSS on these metrics before, during, and after the event. In so doing, we address several questions: Do ReTSS attract extra visitors to the store during the event? Do they increase visitors' purchases at the store? Are these effects incremental—that is, do increases during the event period outweigh negative pre- and postevent dips? We address these questions by studying weekly store visits and purchases of a panel of households, across the top seven Dutch grocery chains (of which four engage in ReTSS), during a period covering over 200 weeks and 44 ReTSS (nine themes, with several occurrences). We study the impact of ReTSS as a whole and show that it is stronger than the mere discounting and advertising effect. In addition, we explore which consumers respond more favorably to these events, what makes some ReTSS more successful than others, and how they affect the retailer's bottom line.The article is organized as follows. After a brief review of background literature, we describe the ReTSS and outline their characteristics. Next, we develop a framework for their anticipated effects on store visits and purchases before, during, and after the event. We then present the models followed by the empirical estimates, which we use as inputs for simulations to test and further explore the proposed effects. We conclude with summary insights for academics and managers. The Effects of PromotionsThis section offers a brief literature review on the nature of, and evidence for, promotional effects. Previous work has identified the components of promotional responses and indicated those that contribute to a net gain (excellent overviews are given in [ 2]], [ 4]], [51]], and [52]]). From a retail perspective, the total lift during the promotion period can be split into an effect due to changes in visits or purchases per visit and further decomposed into ( 1) deceleration (consumers postponing visits/purchases at the promoting store in anticipation of the promotion); ( 2) cross-store switching, either direct (consumers visiting the promoting store instead of a competitor) or indirect (consumers shifting purchases between stores they would visit anyway); ( 3) expansion (due to consumers engaging in more shopping trips and/or consuming more in the promoted categories); ( 4) acceleration (consumers visiting/buying earlier than they usually would to benefit from the promotion); and ( 5) halo effects (promotions lifting the purchases of other [nonpromoted] categories in the store).Of these components, only expansion, halo effects, and cross-store switching contribute to the incremental promotion lift for the retailer. Deceleration and acceleration produce pre- and postpromotion dips that must be deducted from the lift during the promotion period.[ 7] Thus, to appreciate the truly incremental outcome of promotions, one needs to consider not only immediate effects (during the promotion period) but also effects before (leads) and after (lags) the promotion period.Empirical studies on the impact of grocery retailer promotions abound. These works typically study the impact of what we refer to as ""business-as-usual promotions"": frequent discounts, premiums, or coupons on individual brands or stockkeeping units (SKUs) that are not part of an overarching event and that may be announced through feature ads or in the store's weekly flyer. Most of these studies focus on the effect of brand- or SKU-level offers in isolation, with brand or category sales as the outcome of interest[ 8] (for an excellent discussion, see, e.g., [ 1]] and [52]]). The results show that at the category level, the larger share of the promotional sales lift comes from purchase acceleration, followed by store switching, with only a small portion stemming from increased consumption (e.g., [29]; [51]). The store switching appears to be mostly indirect—consumers shifting category purchases among stores they visit anyway ([13]; [29])—such that, to the extent that competing supermarkets run promotions for different categories, the question remains how these effects translate to sales for the retailer as a whole.A smaller subset of research examines the combined effect of different promotional offers at the store level. Some of these document the impact of promotional calendars—in other words, the sequence of promotional actions over time ([36]; [43]; [46]). Others analyze how the total number of products promoted by a retailer and their average discount depth affect store traffic and sales (e.g., [22]; [27]; [49]; [55]). The general finding is that more and deeper promotions can increase traffic and basket size, but that this effect is often small, with elasticities in the range of.05 to.2 ([22]; [27]). Moreover, to the extent that consumers accelerate purchases to benefit from a promotion ([ 3]), only a portion of this temporary lift in traffic or spending is incremental. Retailer-Themed Super Saver Events: CharacteristicsRetailer-themed super saver events (ReTSS) exhibit a unique combination of characteristics. They promote storewide benefits across a broad range of categories in the store. The offers are immediate (i.e., the consumer receives the price cut or extra quantity at the time of purchase) and unusually deep. More importantly, the event-related deals share a common format that is easy to recognize (e.g., ""One Euro only,"" ""BOGO""), and they are presented to consumers under a theme that is unique to the retailer. This theme is not just apparent in-store or featured in the store flyer; it is also supported with mass-media advertising that emphasizes the considerable potential savings. In terms of timing, retailers themselves choose when the event takes place and for how long. As an example, the Hamsterweeks event, organized several times per year by Dutch chain Albert Heijn, involves BOGO offers on items across 50 categories, rotating during three consecutive weeks. The event is advertised on national TV, in newspapers (and on the radio) using images (sounds) of a hamster carrying the chain's logo and dragging large amounts of groceries out of the store while crying out, ""Hamsterééééén!""[ 9]Table 1 compares ReTSS with other retailer promotions. As shown there, ReTSS share some characteristics with other promotional activities. However, what sets ReTSS apart is the joint occurrence of these characteristics. Unlike popular, category, and seasonal events, which typically focus on specific types of products, ReTSS span categories storewide. In contrast to popular and seasonal events, the timing of the ReTSS is retailer-specific.[10] Unlike business-as-usual promotions that use store flyers and in-store tags focused on activation, ReTSS combine these with mass-media ads designed to form attitudes. The discounts that are part of the ReTSS event are immediate (unlike in temporary loyalty programs, in which consumers save for rewards) and unusually deep (typically 30%–50%; other promotions are, on average, less than 20%).[11] Perhaps the most discriminating feature of ReTSS is the use of a common deal format under a common (retailer-specific) theme focused on monetary savings. Other promotion activities either lack an overarching theme (i.e., business-as-usual promotions) or, if they do have a theme, encompass a variety of offers, some of which are not even savings-related (e.g., popular events including premium gadgets linked to the event, category and seasonal events also pertaining to temporary additions to the assortment). In our empirical analysis, we control for the presence of such other events when assessing the impact of ReTSS on visits and purchases.GraphTable 1. ReTSS Events Versus Other Promotion Activities.  1 aBusiness-as-usual promotions are typically not communicated through mass media (though the retailer often uses mass-media image advertising during business-as-usual weeks). Conceptual FrameworkThis section outlines a conceptual framework that examines the effects of ReTSS on consumer and store outcomes. We proceed in three steps. First, we argue how the joint characteristics of ReTSS increase promotion awareness and perceived benefits. Next, we delineate how this influences the different components of promotion response identified in previous literature. Although we do not observe these mechanisms directly, we use them in a third and final step to form expectations about consumers' visits and purchases at the store before, during, and after the ReTSS period, which we subsequently test empirically. Consumer Mechanisms Driving ReTSS SuccessWhy do promotions often fail to generate incremental visits and purchases? We identify two main reasons for this. First, consumers—and especially those who are not regular store customers—are often not aware of the promotion. As indicated by [ 7], p. 122), shoppers are ""perhaps more than ever...seemingly in a perpetual state of partial attention."" They are bombarded with promotional messages, yet experience high search costs ([23]). This makes it difficult for specific deals to stand out from the clutter and to reach potential customers. Second, the deals may not seem interesting enough to trigger a promotional purchase at the store. They are often not unique to the retailer—promotions that run concurrently with other stores yield less bang for the buck ([29])—and, many times, the benefits are too small to warrant the cost of an extra visit or even to justify the effort to look for the item in-store. In the following subsections, we argue how their combination of characteristics may allow ReTSS to overcome these hurdles. Impact on consumer awarenessPromotions at the store are typically communicated through store flyers for which readership may be high among current customers, but less so among potential customers (e.g., [18]; [53]). By combining store flyers with mass-media advertising, ReTSS cover a broader target audience, including noncustomers of the store. In addition, the presentation as an event with a common theme may create higher resonance ([ 7]) and trigger word of mouth, thereby further enhancing promotional reach.Moreover, consumers who encounter ReTSS messages are more likely to encode them. The combined use of different modes (mass media and store flyers) may render the ReTSS more salient ([48]; [56]). Furthermore, the common event theme provides a hook that fosters message processing ([28]; [31]), especially so because—unlike season-sale or popular external events—the theme and timing of ReTSS is retailer-specific and does not automatically coincide with above-average competitive clutter.In summary, consumers are more likely to be aware of ReTSS events. We expect this to hold during and after the event period but also—to the extent that these events are announced up-front or recur around the same period—in the period leading up to it. Impact on consumer interestRetailer-themed super saver events should generate higher interest by providing larger perceived benefits. They offer deeper-than-normal monetary discounts that are immediately available across a broad set of items and, therefore, appeal to many (current and potential) customers. This is reinforced by the integrated nature of the campaign: unifying the ReTSS deals through a common format and theme that is unique to the store may produce signaling value ([58]) and enhance the perceived monetary benefits.In addition, ReTSS offer nonmonetary benefits, which further contribute to consumers' promotional response ([14]): convenience, smart-shopper, and exploration benefits. First, ReTSS hold the promise of important convenience benefits. The broad offer propagates the store as the place of choice, with a multitude of deals under one roof, making it worth a visit. The common format makes it clear to consumers what to look for and easy to spot the deals in-store, reducing the search cost of promotional shopping ([23]).Second, the use of complementary media may instill psychological triggers to participate. While store flyers and in-store deals stimulate action, mass (TV) advertising is effective at eliciting emotions that heighten the success of direct sales incentives ([ 7]; [41]). In this vein, mass advertising that presents the ensemble of ReTSS deals as one large, not-to-miss event may create value-expression or smart-shopper benefits: the feeling of being a responsible shopper when visiting the store ([ 6]; [14]).Finally, ReTSS can create exploration benefits. To the extent that ReTSS deals are announced in mass media to cover a broad set of categories but are not individually listed in those ads or grouped in one place inside the store, consumers may become curious to discover which specific items are covered. This may stimulate them to look for (promoted) items in-store and to enjoy traveling the aisles ([14]).Table 2, Panel A, summarizes the links between ReTSS characteristics and consumer awareness and interest (due to perceived monetary and nonmonetary benefits). We expect these links to apply to ReTSS in general but to be particularly strong for events with more items, higher advertising budgets, deeper discounts, and more resonant themes.GraphTable 2. ReTSS: Consumer Mechanisms and Store Outcomes.  2 Notes: The table summarizes how the impact of ReTSS differs from business-as-usual promotions. Panel A links event characteristics (left side) to resulting consumer mechanisms (right side). For instance, because of the unusually deep, immediate discounts, ReTSS entail larger monetary benefits than regular promotions (✓). Panels B and C indicate which of these consumer mechanisms (left side) influence which promotion component (middle), and how this component affects visits (right side of Panel B) and purchases per visit (right side of Panel C). ↑ = the promotion component in the row enhances the outcome variable in the column; ↓ = the promotion component reduces the outcome variable; empty cells indicate that it has no impact. For instance: trip acceleration enhances visits during (↑) but reduces visits after (↓) the event, postpromotion direct switching to other stores reduces visits after the event (↓), and so on. ReTSS-Induced Consumer BehaviorsHow do increases in awareness and perceived benefits translate to consumer reactions to ReTSS, over and above business-as-usual promotions? To see this, we discuss how they influence the aforementioned promotion-response components, split into visit responses (Table 2, Panel B) and purchase responses given a visit (Table 2, Panel C). Impact on store visitsThe high awareness of ReTSS may make consumers decelerate visits in anticipation of the event ([40], [45]), and more strongly so than for regular promotions. This is bolstered by the interest generated by these events: the large expected monetary benefits and emphasis on smart shopping reinforce consumers' desire to be economical ([14]; [58]).During the promotion period, we expect ReTSS to trigger more direct cross-store switching than other promotion activities. Their high reach and salience make even noncustomers aware of the event. Attracted by the promise of substantial monetary benefits (which may act as a commitment device; [33]) and smart-shopper benefits (which make them feel like a responsible shopper; [14]), these consumers may decide to visit the promoting store instead of a competitor. Moreover, because the perceived benefits are likely to exceed the cost of a visit ([ 9]), consumers are more prone to engage in extra trips during the event (visit-expansion effect).The ReTSS mechanisms that attract new consumers may also trigger current customers to more strongly accelerate their shopping trips. While this further increases store traffic during the event, it leaves these consumers with an unusually high inventory that reduces their visit propensity in postevent weeks (see, e.g., [24]).Finally, more so than other promotions, ReTSS may produce direct cross-store switches that persist for some time after the event. These switches can be in either direction. On the positive side, increased awareness may produce more sustained switches toward the promoting store. New customers may have found their way to the store and, having become more familiar with it during the event, or realizing its attractive features in-store, return even after the ReTSS ends ([ 4]; [53]). Moreover, increased awareness may produce a rewarded behavior effect of consumers feeling obliged to those who treat them well ([16]) and becoming more committed to the store at the expense of competing stores. On the negative side, the stronger promotion salience and emphasis on smart shopping may trigger a reference-price effect and reduce consumers' willingness to visit at regular prices ([30]). Even consumers who did not visit the retailer during the event may exhibit such an effect and switch to competing stores subsequently ([57]). Impact on store purchasesThe promotion events may also affect consumers' purchases on a given visit, in a way that differs from their regular promotion response. Even if they maintain their visits before the ReTSS, awareness of and interest in the upcoming event may make current customers more strongly decelerate certain purchases—depleting inventories of items in their pantry to replenish them during the event.During the event period, and once customers are in the store, the combination of benefits (rather than their reach or salience) especially sets ReTSS apart from other promotions. These benefits may provoke multiple, sometimes countervailing, purchase responses. The monetary and convenience benefits may enhance indirect cross-store switching, stimulating consumers to procure promoted items at the ReTSS store instead of other visited stores ([14]). These same benefits may also foster purchase expansion, as when people buy and consume more of the promoted products ([ 5]; [ 4]). Moreover, because they find the offer so interesting, consumers (in particular current customers; [ 4]) may more strongly accelerate their purchases, which increases purchase volume in the course of the ReTSS period but produces deeper postevent purchase dips. When it comes to halo effects, the impact of ReTSS relative to business-as-usual promotions is equivocal. On the one hand, the perceived monetary gains and smart-shopper benefit may produce a windfall or licensing effect and justify extra expenses ([32]; [50]). Especially when coupled with the exploration benefits (consumers traveling more aisles), this may result in more (impulse) buying of nonpromoted items ([ 8]; [44]). On the other hand, the monetary and smart-shopping benefits may foster cherry-picking ([23])—that is, more consumers visiting the store for promoted items alone. As such, ReTSS may also come with smaller baskets.[12]Likewise, indirect store switching after the event can go two ways. On the positive side, ReTSS may more strongly expand the future basket at the expense of competitors because newly reached consumers have discovered the strengths of the store ([ 4]) or because the highly salient deals have elicited reciprocity and a shift in commitment to the store ([16]). On the negative side, this same promotion salience, along with the increased emphasis on smart shopping, may make customers less willing to pay the full price and cause shifts to rival stores after an event ([52]). ReTSS Impact on Visits and Purchases over Time: ExpectationsAdding up the responses[13] in each period (across the rows in Table 2, Panels B and C), the bottom of Table 2 summarizes the anticipated visit and purchase outcomes before, during, and after the event. Compared with business-as-usual promotions, we expect ReTSS to lower (extant) customers' visits and purchases prior to the event. In the course of the ReTSS period, we anticipate more visits, including visits from new customers. Because the increase in basket size from temporal shifts, expansion, or increased halo effects, on the one hand, is likely to exceed any negative effects of cherry-picking (reduced halo effects), on the other, we also expect purchases per visit to more strongly increase. Following the event, although newly acquired customers may continue to visit and buy at the store for some time, we expect this effect to be outweighed by below-baseline levels for extant customers, so we anticipate a larger postevent drop in visits and purchases.How these effects net out over time is not clear a priori, and we leave it as an empirical issue. Next, we present the household-level visit and conditional-purchase models used to verify these effects. ModelAs indicated previously, a ReTSS may influence both the decision to visit a retailer and the basket size at that retailer. Similar to [19] and [58], we model this in two layers. Visit incidenceThe first layer captures a household's decision about whether to visit a retailer in a given week. Like [58], we focus on retailer-visit incidence rather than retailer choice for a given visit, because large-scale events may well affect households' trip organization and number of store visits (e.g., they may begin to split their grocery trips between their regular and the promoting store in a given week). We specify the probability that household h visits retailer r in week w as follows: Prwh=Prob(vrwh>0)=Prob(urwh>−(αrh+xrwhζh)) Graph1with vrwh=αrh+xrwhζh+urwh, Graph2where  vrwh  is a latent variable reflecting the utility of visiting the store;  {αrh; ζh}  are the parameters;  xrwh  is a vector of household-, retailer-, and/or week-specific utility drivers (further specified subsequently); and  urwh  is an extreme-value distributed random component. A household may visit multiple retailers in a given week (i.e.,  vrwh  may be positive for different retailers that the household visits in w), and these patronage decisions are likely to be interdependent. To capture these interdependencies and to accommodate the possibility that the random components have retailer-specific variances, we assume a multivariate heteroskedastic extreme-value distribution for  urwh  and use a Farlie–Gumbel–Morgenstein copula model to specify and estimate the corresponding retailer-visit probabilities (see, e.g., [11]; [15]; Web Appendix W2). Conditional purchase volumeThe second layer relates to households' purchases at a retailer in a given week, equal to 0 if the retailer is not visited, and some quantity  pvolumerwh   otherwise. We model purchases conditional on a retailer visit as follows: pvolumerwh|(visitrwh=1)=exp{ιrh+zrwhβh+τ[log(Prwh)+(1−Prwh)×log(1−Prwh)Prwh]+∊rwh}, Graph3where  visitrwh=1  if household h visited retailer r in week w;  ιrh,βh  are parameters to be estimated;  zrwh  is a vector of household-, retailer-, and/or week-specific regressors (specified in the next section); and  ∊rwh  is a random component. This conditional purchase model is estimated only on observations in which the household actually visited the store. To correct for this selection issue (i.e., the nonrandom occurrence of the store visits), we use the approach suggested by [17]: we include the term  {τ[log(Prwh)+(1−Prwh)×log(1−Prwh)Prwh]}  in Equation 3 to ensure unbiased parameter estimates.[14] Moreover, the variance of the random purchase-quantity component  ∊rwh  is likely to depend on the number of stores visited that week (i.e., on the split of the total basket), and purchases at one store are likely to be interrelated with purchases at (an)other store(s) visited in the same week. We capture this through a multivariate normal specification on the random components  ∊rwh   with heteroskedastic variances and nonzero covariances between visited retailers within a household and week (details are in Web Appendix W2).To accommodate unobserved household heterogeneity, the parameters in the visit and purchase models follow a normal mixing distribution (with means and standard deviations to be estimated). We estimate Equations 1–3 with simulated maximum likelihood. Data and Measures DataOur primary data source consists of GfK panel data from 2008, week 31, until the last week of 2012. The data set contains household purchase histories as well as weekly price levels and feature activities at all Dutch retailers. We consider household purchases at the top seven retailers in terms of market share. Table 3, Panel A, provides some descriptive statistics for the considered retailers, which, together, cover about 60% of the Dutch grocery market. To ensure stable estimates, we retain only households that remain in the panel for more than 26 weeks and that make at least ten visits to (any of) the top seven chains throughout our observation window. For tractability, we estimate our models on a random subsample of 1,000 households. On average, a household visits 1.19 retailers per week and spends €31.73 per visited retailer. Albert Heijn has by far the highest weekly visit rate (i.e., fraction of weeks with a chain visit, averaged across households), followed by Aldi, C1000, and Lidl. Weekly spending by store visitors is more comparable across chains, with slightly higher levels for Albert Heijn and lower levels for the hard discounters.GraphTable 3. Descriptives.  3 a Fraction of weeks with household visit, averaged across households.4 b Weekly spending per household, conditional on store visit (in euros), averaged across households.5 c A detailed timeline for the ReTSS is given in Web Appendix W3.6 d Number of promotions that fall under the event-theme heading, based on anecdotal information and industry/press reports. ""Products"" can refer to entire brands or SKUs.7 e Average of LexisNexis mentions in event-year.8 Notes: Hi–lo = adoption of a high–low pricing scheme; HD = hard discounter; EDLP = everyday low pricing. Level = the average value at the retailer during event weeks (including any concurrent business-as-usual promotions or advertising). Index = the value relative to weeks without a ReTSS event at the same retailer. During ReTSS weeks, the retailer also features other promotions that do not fall under the event-theme. Depending on the number and discount depth of such other promotions, the total number of feature promotions and discount depth during ReTSS weeks may be higher or lower than usual. Likewise, mass-media budgets can be lower during ReTSS weeks than non-ReTSS weeks because of mass-media image campaigns or communication of other events during non-ReTSS weeks. Promotion Events: Descriptive StatisticsTo identify the promotion events that qualify as ReTSS, we combine several additional sources. We begin from a data set compiled by GfK that contains descriptive information, by retailer and week, on promotional actions that are somewhat broader (i.e., covering more than one specific brand/SKU in a specific category). As such, it includes a very diverse set of promotional events. For each event, it contains the name as communicated by the retailer, the event timing, and, in many instances, information on the deal format and the promotional conditions. Wherever the latter event-specific information is missing, we supplement it with data from newspapers and industry sources available online. Our second source consists of Nielsen data on weekly advertising spending, by retailer and by medium. These data allow us to gauge the advertising support received by these events through mass media (TV, print, and radio). Drawing on the event information and the characteristics in Table 1, two independent judges classified each event as one of the following types: Seasonal Events (e.g., promotion events related to Easter, Christmas), Temporary Loyalty Programs (e.g., saving stamps for collectables), Category Themes (e.g., ""Best Deals with the C1000 Butcher""), Popular Events (e.g., buying merchandising products for World Cup Soccer), Business-as-Usual promotions (premiums, quantity discounts, or coupons on specific items that are not part of an overarching theme), and ReTSS. The classification by the experts was identical in 98.4% of the instances, and the few disagreements were resolved through discussion.Table 3, Panel B, provides an overview of the ReTSS. We identify nine different ReTSS, all of which occur at traditional retailers, as hard discounters have no such savings events. On average, a ReTSS event lasts three weeks, with a maximum of four weeks, and some events recur multiple times during our observation period (Web Appendix W3 documents the calendar times of the events). Each ReTSS has a distinctive theme (e.g., ""Hamsterweeks""), whose core message is that the consumer can save large amounts of money by shopping at the retailer during the event. All ReTSS deals follow a unified format, with a consistent (low) price point (e.g., all for €1; Event 2), deep percentage discount (e.g., 50% off in Events 6 and 7), or a multibuy offer (e.g., BOGO in Events 3, 4, and 5). The items promoted under the theme heading usually rotate weekly. While the number of items varies (ranging from <30 per week for Event 1 to 100 per week for Event 2), the deals span a wide range of categories. Moreover, as Table 3 shows, retailers use mass-media advertising during each ReTSS event.Retailers may run different types of promotions concurrently; for example, during ReTSS weeks, consumers may also receive (business-as-usual) deals on specific brands and SKUs that are not part of the ReTSS offer. To isolate the impact of ReTSS per se, it is therefore imperative to assess (and account for) the overall depth and breadth of weekly promotional activity at the store level. Next to the event list and the advertising data, the GfK scanner panel provides us detailed indications—for each SKU in each week—on actual prices and promotions/feature appearances. We use these data to calculate, for each retailer-week, the total number of SKUs advertised in the store flyer (including offers that do and do not fall under the event theme), and the discount depth on promoted items (details on the operationalization are provided in the variables section). Table 3, Panel B, provides summary statistics for those variables during ReTSS, in absolute terms, as well as relative to nonevent weeks at the same retailer. It shows that the focal retailer carries more SKUs on feature and offers deeper discounts (p <.01 for all events) in ReTSS weeks than in other weeks.[15]Even if all ReTSS events do enjoy mass-media support, advertising spending is not always higher on average in event weeks than in nonevent weeks (see Table 3). This is because retailers advertise other types of events as well (see Table 1 and Web Appendix W1) or engage in image advertising unrelated to promotions, and because ad investments are subject to seasonal and long-term changes.[16] To grasp the presence and timing of (extra) advertising support related to our savings events, we regress retailers' weekly advertising spending (stacked for the four retailers involved in these events) against retailer-specific constants, time-related variables (i.e., year and quarter dummies, a trend, and end-of-year and beginning-of-year dummies), and variables related to the occurrence of the ReTSS. Specifically, we include dummy variables for ( 1) the week before the start of an event, ( 2) the first event week, and ( 3) the remaining event weeks. The results show that there is no significant lead-week advertising effect, but that advertising is typically higher in the first week of the event and lower in remaining event weeks. Model-Free EvidenceTable 4 provides model-free evidence on the impact of the different ReTSS. For each retailer and event, it reports the mean (standard deviation) of the weekly unconditional purchase amount per household (in euros), the weekly visit propensity per household, and the purchase amount conditional on a visit in that week. It does so for event weeks as well as baseline weeks (in which no ReTSS takes place at the focal retailer) and calculates the change rate.GraphTable 4. Model-Free Evidence.  9 aBaseline weeks are weeks for the focal retailer without a ReTSS event for that retailer.10 bStandard deviation between brackets.A few tentative observations can be made. First, for most events (seven of nine), spending levels are higher during event weeks. Second, this overall spending shift conceals countervailing forces: whereas retailer patronage (i.e., the number of households visiting the chain at least once) typically increases during event weeks, spending per visitor often decreases. Third, there are differences between events: some ReTSS show sizable increases in sales (e.g., Events 2 and 6, with spending levels that are almost 15% higher), others seem less successful (e.g., Event 1, during which we observe a 17% sales decline). The ReTSS effects also vary within retailers, as illustrated by Events 2 and 7, which—although organized by the same chain—show different spending increases.However, the values in Table 4 should be treated with caution. First, they do not distinguish quantity from price effects (consumers paying less per unit during event weeks). Second, because they do not control for changes in other variables that co-occur with the events, they cannot be interpreted as causal effects. Third, they do not allow us to separate the dynamic (over-time) impact of the events. Fourth, they do not account for reaction differences among consumers. Our formal modeling approach addresses these issues. Variables and OperationalizationTable 5 describes the variables and their operationalization. For each household, we set aside a 26-week initialization period and use the remaining observations for calibration.GraphTable 5. Variable Descriptions.  11 a See Web Appendix W4 for details.12 bMarketing-mix effects unrelated to the ReTSS events, but including other types of promotion events.13 c Variables related to the ReTSS events. These include advertising, feature and discount depth, which may be higher than usual during event weeks due to extra event-specific investments or deals in those weeks.14 Notes: All purchase volumes are expressed in constant average prices.The dependent variable in the retailer-visit model is a dummy equal to 1 for each retailer patronized by the household in the considered week, and 0 otherwise. In the purchase-volume model, the dependent variable is the logarithm of the volume purchased (  pvolumerwh  ) at each retailer visited by the household in a given week. Note that, like [35], we express the purchase volume, which is an aggregate across categories with different volume units (e.g., liters, grams), in constant average prices: we first multiply each category purchase quantity by the category's average unit price (across retailers in an initialization period) and then sum up over categories. By using constant prices, we ensure that variations in the purchase variable reflect only changes in quantity. The log-transform accommodates skewness in the purchase distribution and facilitates the interpretation and comparability of the parameters across households with different purchase levels.As explanatory variables, next to retailer constants, we incorporate multiple drivers of store visits and purchases identified in the literature (see, e.g., [19]; [52]). A first set comprises seasonal and state-dependence variables. To account for seasonality, we include end-of-year, beginning-of-year, and Easter dummies (  EoYw,BoYw,Easterw)  . Households that buy more in a given week (at any retailer) may be less inclined to shop or purchase in subsequent weeks because of built-up inventory; this is captured by the variable  inventorywh  , which we standardize within households. The visit-incidence model further includes a state-dependence variable (  lag_visitrwh,   indicating whether the household visited the retailer in the previous week) and a retailer-share variable (  ret_sharerh,   measured as the share of household visits to the retailer in a 26-week initialization period). Likewise, the conditional purchase model includes the household's lagged (log) purchases   (lag_pvolumerwh)  to capture state-dependence effects, and the (log of) average weekly purchases at the retailer in the initialization period  (avg_pvolumerh)  .Second, we incorporate marketing variables unrelated to the ReTSS. These include the (log-transformed) distance to the nearest retailer outlet  (distrh)  and a household-, retailer-, and week-specific regular price  (regpricerwh)  and assortment variable (  assortrwh)  constructed using household-specific category weights (see Table 5 and Web Appendix W4) and then indexed relative to the average across retailers in that same week to accommodate competitive effects. Because households cannot observe regular prices and assortments prior to visiting the store, we use past-week values to reflect households' expectations for these variables. To account for the occurrence of other types of promotion events, we include dummy variables for popular events (  poprw  ), temporary loyalty programs (  loyrw)  , seasonal events (  seasrw  ), and category events (  catrw  ). The variable  lag_otherrw  accommodates any dips in the week following these other events.The third set of variables captures the impact of the ReTSS events during event weeks. These include the (log of) investments in mass media  (advertisingrw)  , the percentage of SKUs featured in the store flyer in the considered week (  percfeatrwh)  , and the average discount depth for promoted items (  discdepthrwh  )—the latter variables, again, aggregated at the store level with household-specific category weights and indexed relative to the market average. These variables capture the overall communication and promotional activity of the store in the considered week,[17] including any changes in advertising, featuring, or discounting due to the ReTSS. In addition, for each of the NE ReTSS events, we include a dummy variable  (event#Xrw,X=1→NE)  equal to 1 during event weeks at the retailer, and 0 otherwise. The event-dummy coefficients capture the extra impact of the ReTSS, over and above their effect through communication and discounts. Thus, a significant positive coefficient would imply that offering and advertising the deals under a common savings theme enhances their impact. To allow for a differential event effect on retailer-visit incidence and purchases of more- versus less-customary store shoppers, we include an interaction with the initial retailer-share variable (  ret%5fsharerh×eventrw)  in both equations. Assuming a positive main effect of the ReTSS events, a negative coefficient for the interaction would mean that the event produces a smaller visit or purchase lift among customary shoppers at the store. Events at competing retailers can affect the visit propensity through  comp_eventrw  , a distance-weighted variable of ReTSS dummies at rival retailers.The fourth set of variables relates to the ReTSS dynamics.  Lead_eventrw   captures any anticipation effects in the week prior to the ReTSS event, whereas  lag_eventrw  (  lag%5fevent%5fmediumrw)   captures carryover effects in the first (four) weeks following the event (see also Table 5). Because anticipation and carryover effects may differ between more and less customary shoppers, we again include interactions with the households' initial retailer-visit share. Finally, the tendency to revisit the store or the impact of previous on current purchases may be different for shoppers who patronized the store during the event: it may be higher because of increased store salience and a positive store experience, or lower because the previous visit or purchases were promotion-induced. To accommodate this, we add extra lagged variables to the visit (  lag_event_visitrwh  ,  lag_event_medium_visitrwh  ) and the purchase equation  (lag_event_pvolumerwh, lag_event_medium_pvolumerwh  ), activated only for households that visited the retailer during the preceding event. These variables operate over and above the regular  lag_visitrwh  and  lag_pvolumerwh  variables, and capture deviations from business-as-usual carryover effects due to the ReTSS event.[18] Drawing on the Belsley–Kuh–Welsch diagnostics and the correlation matrices, we find no multicollinearity.[19] Estimation Results Store VisitsTable 6 reports the results for the visit-incidence model. With an average probability for hits of.654, the model fits the data well, and far better than chance.[20] For simplicity of exposition, in this section we focus on the estimated population means. We briefly discuss the control variables (i.e., the seasonal, state dependence, and other marketing variables) first. Inventory reduces the propensity to visit a store. The coefficient of the household's initial retailer share is positive, pointing to explained heterogeneity, as is the lag-visit parameter, indicating that shoppers tend to revisit a retailer where they shopped before. Distance exerts a negative, and assortment a positive, impact. The regular-price coefficient is not significant, probably because most of the price variation is promotional and thus captured in the discount variable. Except for seasonal events, the coefficients of non-ReTSS events (i.e., loyalty, popular, or category events) are not significant, nor is their lag—indicating that they do not alter traffic relative to business-as-usual weeks (which serves as the reference).GraphTable 6. Parameter Estimates.  15 *p <.05.16 **p <.01.17 Notes: Two-tailed tests of significance. For brevity, means and standard deviations of the retailer dummy coefficients and parameters of the error correlation structure are estimated but not reported.Turning to the immediate ReTSS-related effects, we find that discount depth, percentage of SKUs featured, and ad spending have the expected positive impact. In addition, all events show positive and significant dummy coefficients—except Event 1, whose impact is not significant. For the remaining events, the coefficients range from β =.134 (Event 7, p <.01) to β =.472 (Event 2, p <.01). Overall, this indicates that ReTSS do enhance store patronage during event weeks beyond the pure discount or advertising effects. The negative parameter associated with the Retailer Share × Event interaction (β = −.461, p <.01) indicates that ReTSS draw disproportionately less from regular store customers. A competing retailer event lowers the likelihood of a store visit for the focal retailer (β = −.154, p <.01). This confirms that, because of their high awareness and unusual perceived benefits, the events attract new customers through extra visits and/or direct store switching.As for the dynamics, the coefficient of the lead-event dummy is not significant (p >.10), but its interaction with the household's prior retailer share is significantly negative (β = −.411, p <.01), indicating that regular customers postpone store visits in anticipation of the event. We obtain negative coefficients for the lagged-event variables in the week following the event (β = −.332, p <.01) and afterward (β = −1.065, p <.01), as well as for their interactions with the households' initial retailer-visit share (β = −.838, β = −2.542, p <.01). This is consistent with stronger acceleration effects among heavier (i.e., more frequent) customers. Interestingly, though, the coefficients of  lag_event_visitrwh   (β =.878, p <.01) and  lag_event_visit_mediumrwh  (β = 2.938, p <.01) are positive and larger. These effects operate over and above the regular revisit tendency captured by lag_visit. Thus, households that did not patronize the store during the event seem even less likely to do so in postevent weeks, possibly because the previous-deal salience and emphasis on smart shopping make them less willing to patronize the store at full prices. Conversely, visitors during the event, who built up store familiarity, have an even stronger tendency to return to the store. Conditional Purchase VolumesThe right-hand side of Table 6 reports the results for the purchase-volume model conditional on a visit. The pseudo-R2, compared with a null model with store intercepts only, equals.702, and the mean absolute percentage error of predicted versus actual purchase volumes is only 6.92%, pointing to high explanatory power. Turning to the parameter estimates, we first consider the control variables. Households buy more at their customary store, as shown by the positive coefficients of average initial retailer purchases and lagged purchases. However, they procure smaller baskets when their inventory is still high. Purchases are higher in nearby stores with larger assortments. The regular-price coefficient is not significant—possibly, again, due to lack of variation. Once in-store, households spend less during loyalty, popular, and category events than during business-as-usual promotion weeks.[21]Our focus is on the ReTSS-related effects. As for the immediate effects, we find that baskets increase with deeper discounts and more mass-media advertising. Whereas feature activity enhances households' propensity to visit the store, it has a negative impact in the conditional-purchase model, suggesting that feature ads attract smaller-basket shoppers.[22] The event-dummy coefficients again reflect the impact of the event over and above the discounts and advertising investments per se. They are significantly positive for eight of nine events (and insignificant for the other): the effect is largest for Event 2 (β =.182), Event 3 (β =.201), and Event 6 (β =.173) (all ps <.01). The coefficient of the Retailer Share × Event interaction is negative but smaller in absolute value (β = −.150, p <.01). Thus, even for regular customers, baskets tend to increase during event weeks because customers stock up on promoted items (acceleration), consume more (expansion), and/or explore the aisles or feel licensed to purchase extra nonpromoted items (halo effects). In contrast, although events at nearby rival stores reduce households' propensity to visit, they do not affect purchases beyond the competitive impact already included in the relative discount variable (p >.10).Turning to the dynamics, the main lead effect is insignificant (β = −.004, p >.10), but its interaction with retailer-visit share is negative (β = −.067, p <.05), indicating that current customers decelerate purchases if they suspect an upcoming event. Finally, we obtain an interesting pattern of postevent effects. Heavier customers show deeper quantity dips immediately following the event (β = −.085, p <.01) and in the few weeks afterward (β = −.196, p <.01), a pattern indicative of purchase acceleration. However, consumers who bought more at the store during the event show a positive effect afterward (β =.0073, p <.05; β =.013, p <.01), consistent with the notion of increased store familiarity or commitment. SimulationsAlthough the coefficients in Table 6 shed light on the significance of event effects, they do not give a clear picture of the effect sizes or the net outcome of the (countervailing) dynamics. We use simulations to provide such insights. Using the actual data as a backdrop, we dynamically predict the panelists' visit sequence and purchase volumes per visit for each retailer, using their posterior estimates of the visit and purchase models and based on the procedure described in [47]. Starting from the first week, we calculate the panelist's visit probability for each retailer in the subsequent week. We then simulate 100 shopping sequences, each time taking a random draw from this probability to predict whether the retailer is visited in the shopping sequence in that week. For each visited retailer, we calculate the panelist's purchase volume based on the conditional-purchase model coefficients. Using these values, we update all the dynamic variables (i.e., inventory and all lagged variables) for the next week. We then average the results across shopping sequences to obtain the panelist's visits, conditional purchases, and total purchases (not conditional on a visit; thus equal to 0 in weeks without a store visit) per week and retailer. Using the actual retailer prices during event and nonevent weeks, we also obtain the corresponding spending levels. Finally, we add a layer to the simulations in which we draw sets of values for the means and standard deviations of the mixing distributions and obtain the corresponding posterior estimates by household and the associated ReTSS effects. We use the distribution of these outcomes to assess the statistical significance of the effects.We simulate three scenarios. In the Baseline scenario, we use the actual levels of the non-event-related explanatory variables but, in weeks where a ReTSS occurred, we set the event-related dummies to 0, and the promotion (discount depth and percentage of SKUs on feature) and advertising variables equal to their expected level in nonevent weeks.[23] In the Event + Support scenario, we fix a (three-week) event period. For each retailer, in turn, we assume that an event at that retailer took place (i.e., we activate all event-related variables for that retailer, set the promotion and advertising variables to their values during the retailer's event period, and flag the presence of a competing event for other retailers). To separate the impact of increased advertising and discounting from that of the event theme as such, we also consider a Support Only scenario, in which we keep advertising, features, and discounts at their event levels but set the event-related dummies to 0. For each retailer and week, we then compare the households' visit propensity, purchase volumes, and spending in the Event + Support and Support Only scenarios with the Baseline. Impact During Event WeeksTable 7, Panel A, reports the change in weekly visit propensity, purchases (spending) conditional on a visit, and total weekly store purchases (spending) during the event period, based on comparison of the Event + Support and the Baseline scenarios. It does so for the average, worst, and best event (results for individual events are in Web Appendix W5). As we expected, for eight of nine events, ReTSS leads to significant increases in store visit propensity. Visit incidence during event weeks increases by 7.75% on average (a 1.58-percentage-point increase, p <.01), but with variation across events (lowest value: −.25% for Event 7, highest value: 20.22% for Event 6). As the ""Conditional Volume"" row indicates, average basket sizes during ReTSS weeks also increase, but the effect is minimal (+.34% on average) and statistically insignificant. Combining the two, we find that the ""Total Volume"" typically increases during event weeks, with an average lift of 8.47% (p <.01) and an increase of up to 20.74% for the most successful event (Event 6). Retailers thus enjoy a clear upswing in visits and total purchases[24] in the course of the event. On average, this lift in purchase volume translates to a 3.67% immediate spending increase, with significant positive numbers for six of nine events, ranging from a decrease of 8.94% for the worst-performing event and to an increase of 14.61% for the best-performing event.GraphTable 7. Overall Impact of ReTSS over Time.  18 *p <.05.19 **p <.01.20 a Impact over the eight-week period following the event.21 b Impact over the 12-week period (1 pre-event week + 3 event weeks + 8 postevent weeks).22 Notes: One-tailed tests of significance based on distribution across parameter draws. % = the percentage change relative to the no-event baseline. Results for all events are given in Web Appendix W5. ""Worst"" and ""Best"" correspond to the lowest and highest % figures across events for a given period and outcome variable. Conditional volume = the change in purchase volume over the considered period, per household, given a visit, and expressed in constant monetary value (Euros). Total volume = the change in total purchase volume (in Euros) over the considered period, per household, unconditional on a store visit (so: zero if the household did not visit the store in those weeks). Total spending = the revenue equivalent of total volume, based on actual prices during (or before/after) the event. The economic significance of these figures is clear from Table 9, where we report the equivalent revenue value at the market level. Dynamic EffectsThe question remains ( 1) to what extent extra visits or higher purchases during the event period are due to the ReTSS as such, rather than merely to the accompanying promotion or advertising effort, and ( 2) if they are offset by negative pre- and postevent effects. Using the simulation results (i.e., comparing the Event + Support and Baseline scenarios), we calculate the changes in visits and total purchase volumes before, during, and after the event. We also consider the impact of increased mass advertising, featuring, and discount support absent an event theme (i.e., the difference between the Support Only and Baseline scenarios; Figure WA1 in Web Appendix W5 plots these results for three exemplar events). Two findings emerge. First, the larger part of the uplift during event weeks (across all events: 89.24% for visits, 80.91% for conditional volume) stems from the event as such: simply stepping up advertising or promotion activities entails a much smaller increase in visits and purchases (for details by event, see Web Appendix W5).[25] Second, the ReTSS impact extends beyond the event period and subsides in about eight weeks.Building on these insights, Table 7 reports, for each event, the impact (Event + Support minus Baseline) in the preceding week (Panel B), the eight weeks following the event (Panel C), and the net impact (Panel D). The table confirms the presence of negative anticipation effects in visits (−2.70% on average), basket sizes (−2.02%), and total purchase volumes (−7.84%; all ps <.01). Interestingly, the pattern of postevent effects is mixed. Against expectations, visits are still higher on average in the eight-week period following the event (+1.45%, p <.05), but this effect is nullified by lower purchase volumes per visit (−.86%, p <.01). Combining the figures across periods, we find that while the events yield a net visit increase on average (+2.68%, p <.01), this is partly offset by smaller basket sizes (−.66%, p <.01), resulting in only a 1.08% (p <.05) net increase in total purchase volume. For total incremental spending, we observe a bleak overall picture: the average being close to 0 (−.10%, p >.05), albeit again with differences between events (−3.74% to +3.23%). Impact by Customer SegmentDrawing on our conceptual framework and estimation results, we expect the values in Table 7 to conceal reaction differences between more and less customary shoppers of the store. To further explore this, for each retailer and event, we consider the visit and purchase effects for bins of customers with lower versus higher prior visit shares at that retailer (each bin representing an incremental 10% prior visit rate; plots for exemplary events are given in Web Appendix W5). We find that visits increase especially for nonregular customers of the store; for example, for Event 4 (which is close to average), the visit propensity increases by 21% for consumers with a 5% prior-visit probability (first bin) against a status quo for those with a 45% prior-visit rate (fifth bin). These consumers also account for the largest lift in purchase volumes; for example, for Event 4, the total purchase lift amounts to 35% for consumers with a 5% prior-visit rate, but only 8.3% for those with a 45% prior-visit rate. Higher-bin customers do not generate net volume gains: these consumers do not increase their visit rates, and their extra purchases during event weeks are likely cannibalizing nonpromotion purchases during or following the event. Competitive EffectsIf ReTSS yield extra business during the event, who suffers? To address this, we check the changes in visits and purchases in rival stores produced by a ReTSS at the focal store and calculate the portion of these changes that is borne by traditional versus hard-discount chains. We find that the larger share the competitive shifts (i.e., about 67% [70%] of the competitive visit [purchase] losses) is at the expense of traditional supermarket rivals, but this may merely be because they represent a larger share of the market (72%) to begin with. To explore this further, we identify the consumers who contribute most strongly (top 10%) versus least strongly (bottom 10%) to the ReTSS's (immediate) visit and purchase lift and compare their store-type allegiance in the initialization period. Interestingly, we find that for each event and for both visits and purchases, the more responsive consumers have a significantly higher share of wallet at hard-discount chains (on average, more than twice the share: 31% vs. 14%; for more details, see Web Appendix W6). Thus, hard-discount shoppers in particular incur extra visits and increase their purchase volume in response to the event. In summary, this indicates that although both traditional competitors and discounters suffer, ReTSS events disproportionately draw business from hard-discount rivals. Success Drivers and Profitability Drivers of ReTSS SuccessThe results show substantial differences in impact between ReTSS events. What drives these differences? In line with our conceptualization, the success of a ReTSS (over and above the accompanying advertising and discounting) may depend on the deal format (i.e., uniform [low] price per product, BOGO, or percentage discount), scope (number of products eligible for the ReTSS each week), discount depth, (extra) amount spent on advertising, and resonance of the event theme. To explore this further, we conduct a moderator analysis[26]: we rerun the visit and conditional-purchase models after replacing the event dummies with a function of these characteristics. Because, unlike the other characteristics, theme resonance is not directly observable, we approximate it through media attention to the ReTSS, as reflected in the number of LexisNexis mentions (offline and online articles that refer to the ReTSS) for each year in which it runs. To reduce the concern that ReTSS's success drives the media attention (rather than the other way round), we use previous-year values for the LexisNexis mentions. We also add the number of times the event has run before as a potential driver, the impact of which may be positive (higher event recognition) but also negative (wear-out).Table 8, Panels A and B, summarize the key results. Recall that because advertising and promotion are separately accounted for in the model, these coefficients indicate what makes the event as such more successful, over and above the underlying ad budgets, features, and discounts. We find that while the ReTSS's design characteristics hardly shape the conditional-purchase effects (Table 8, Panel B), they do influence its impact on visits (Table 8, Panel A). Stronger synergetic effects are generated from ReTSS that cover more items. Deal format also matters: (deep) percentage discounts and BOGOs contribute equally strongly to ReTSS success, whereas uniform prices (the financial advantage of which is less clear) bring somewhat lower visit and purchase lifts. Discount depth is not significant, probably because it hardly varies within deal formats. And although retailers always use mass media to advertise the ReTSS, higher levels for those budgets do not differentiate more from less successful events. We do find a strong positive association with press coverage (LexisNexis mentions in the previous year), which enhances both visit propensity and basket size. Together, this suggests that it is the content of the message that matters (rather than the advertising weight) and underscores that having a unique, resounding theme is key. Finally, ReTSS events that ran more frequently in the past do worse in terms of visits and basket size, suggesting the presence of wear-out.GraphTable 8. Impact of ReTSS Characteristics.  23 *p <.05.24 **p <.01.25 Notes: Two-tailed tests of significance. For brevity, we report only the coefficients of the ReTSS characteristics (i.e., the moderator variables). The full set of estimation results for visits and conditional purchases can be requested from the first author. Impact on Retailer ProfitabilitySo far, we have documented how ReTSS events affect consumer visits, purchase volumes, and spending. However, even if spending increases, an event may still be unprofitable if the revenue increase for the retailer does not outweigh the margin losses on ReTSS-promoted items. To calculate the profit implications rigorously, we would need detailed information on ( 1) the specific items sold under the ReTSS heading for each event week, ( 2) regular retailer margins on these items compared with items that consumers may have shifted away from, and ( 3) retailer pass-through for all these items in event and nonevent weeks. Because we do not have such data, we resort to back-of-the-envelope approximations of event profitability. For each store and event, we do know the average fraction of revenue sold on deal in event and nonevent weeks and the average discount depth for items sold on deal in such weeks. Based on these figures, the total (gross) profit associated with revenue Rrw for a given retailer and week can be approximated by (see Web Appendix W7): GPrw={Rrw×[m(1−PromSharerw)+(m−DDrw×gw)×PromSharerw(1−DDrw)]}−Arw, Graph4where m is the average retailer unit margin absent promotions (expressed as a fraction of the selling price), gw is the fraction of the promotional discount borne by the retailer,  Arw  is the advertising budget, PromSharerw is the fraction of revenue sold on promotion, and DDrw is the average discount depth. The latter three variables are obtained from the data for event weeks versus nonevent weeks. Based on anecdotal evidence and prior literature, we set m at.25. We then use our simulation outcomes for the Event + Support and Baseline scenarios to calculate the total revenue and margin difference between these scenarios and rescale this difference (obtained for our household sample) to the market level,[27] so that we can deduct the change in advertising budget associated with the event. We do so for different values of gw, which, based on informal exchanges with retailers, we set between.1 and.3.[28]Table 9, Panel A, reports the revenue implications at the market level. The table also shows that while the share sold on promotion is higher during event weeks, nonpromotional revenue decreases for only three of nine events and actually increases for Event 6 (+6.46%), in support of a halo effect. Table 9, Panel B, displays the profit outcomes. It shows that the profit implications remain quite limited. If the retailer bears a larger part of the discount during event than during nonevent weeks (e.g., gw_event =.3 and gw_regular =.1; Case 3 in Table 9, Panel B), ReTSS entail a small loss on average (−.44%), with statistically significant losses for five of nine events. If the retailer can convince the manufacturer to contribute more strongly during the ReTSS than usual (something that, as our interviews with retailers reveal, volume-oriented manufacturers or those under threat from hard-discounters are willing to do), the picture becomes slightly different. For instance, if gw drops from.3 in nonevent weeks to.1 in event weeks (Case 4 in Table 9), the loss turns into a small profit gain (+1.47%).[29]GraphTable 9. Profitability Analysis.  26 *p <.05.27 **p <.01.28 a In thousand Euros, at market level.29 b Increase as fraction of base level during the total 12-week period (1 pre-event week, 3 event weeks, 8 postevent weeks).30 c Increase as fraction of nonpromotional base revenue during event weeks.31 d Incremental gross profit during the total 12-week period.32 Notes: One-tailed tests based on distribution across parameter draws. The table contains revenue and (gross) profit effects of one three-week event occurrence. Extra advertising due to the event is approximated as 26% extra spending in first event week over retailers' average budget, based on regressions of (log) ad spending against event weeks.Again, however, profitability varies across events. Linking profit figures to event characteristics, we find that larger scope (.774, p <.05) and high resonance (.638, p <.10) (which were associated with stronger lifts during the event) also positively correlate to event profitability. ""Uniform price"" deal formats (which did worse in terms of immediate visit and purchase response) bring higher profits (correlation:.584, p <.10), possibly because their deals are less deep and they encourage consumers less to stock up on the product. Implications and Directions for Future Research Research ImplicationsUnlike business-as-usual retailer promotions—for which previous research revealed only weak evidence of direct store-switching—we find that ReTSS can substantially increase the number of shoppers drawn to a store. Even though basket size (conditional on a visit) typically does not change, this implies that shoppers buy more in total at the retailer during event weeks: 8.5% more on average, and up to 21% more for the more successful events.In particular, households that rarely patronize the retailer absent the event and that spend a larger share of their grocery budget at hard discounters shop and buy more at the store during event weeks. This corroborates that, based on their characteristics, ReTSS enjoy higher awareness than business-as-usual discounts and bring higher perceived monetary and nonmonetary benefits that outweigh the hurdles of an (extra) store visit. It also underscores that ReTSS can indeed revitalize the retailer's customer base. An intriguing observation, however, is that in some instances ReTSS (like our Event 1) reduce purchase volumes during event weeks. Because we observed and tested only shoppers' behavioral responses, we can only speculate on the underlying reasons. If retailers mass advertise the event theme but the actual scope of the offer is too small, this may produce a reactance effect. Insights into consumers' mindset metrics could further verify mechanisms underlying these effects.Only a small part of the lift in visits and purchases stems from the (increase in) discounts/features and advertising budget during the events. Instead, the event as such leads to marked performance improvement. Consistent with findings on popular event advertising ([28]; [31]; [38]), this confirms that unifying and communicating the deals under a common savings theme creates extra synergies. These synergies appear particularly strong for events with higher media resonance and that involve more items. Thus, not only do the deals or ad budget as such matter; what matters in particular is the thematic framing as a super saver event, which allows retailers to break away from the clutter and convince households that the gains are worthwhile. Future studies could pursue how retailers can craft and market savings themes for maximum buzz, as data on the virality of advertising is becoming more readily available.From a broader perspective, our findings underscore the critical importance of promotion communication and framing. In reality, the actual number of promoted items during the ReTSS (and the potential for extra savings relative to nonevent weeks) remains limited, and much of the ReTSS success stems from the theme that makes consumers aware and generates the perception of large and frictionless savings. Yet previous studies have shown that consumers do learn from experience. If an event does not yield the hoped-for savings, consumers may not be attracted by it next time and may even develop a negative attitude toward the retailer. Thus, our finding that specific ReTSS events lose effect over time may result not only from theme wear-out but also from consumer disappointment with actual savings. Future studies could analyze how the interplay between anticipated and actual savings shapes consumers' ReTSS response.The impact of ReTSS extends beyond the event period. The higher awareness and perceived benefits make some consumers lie in wait and decelerate visits and purchases prior to the event. This holds even though retailers do not seem to mass advertise the event beforehand and likely stems from the fact that most ReTSS recur roughly around the same time(s) each year. The question remains how retailers can circumvent these negative lead effects without jeopardizing the success during the event period. Should retailers randomize the timing of their event to prevent current customers from postponing their visits? Or, conversely, should they mass advertise an upcoming event, such that rival-chain customers hold back on their purchases at competing stores and buy more with the retailer during the event? The answers will depend not only on the size and composition of the retailer's current customer base but also on consumers' (psychological) reaction to (not) being notified up front—an issue for further study.The weeks following an event show a higher-than-usual number of visits but smaller basket sizes. Newly attracted customers are more likely to return to the store after the event, consistent with a store-salience and familiarization effect. However, consumers who bought at the chain during event weeks buy smaller quantities subsequently, possibly because they built up inventory or because the emphasis on smart shopping has reduced their willingness to purchase at full price. For the average event, the net result across periods is still an increase (albeit small: about 1% on average) in purchase volume. As for profitability, our back-of-the-envelope calculations suggest that unless the retailer bears the brunt of the extra discount depth, the ReTSS neither helps nor hurts the bottom line. In all, our results thus clearly show the immediate and medium-term outcomes of the events in terms of traffic, sales, and profit.Retailers may have additional, longer-term, motives to establish these events, such as improving the store's price image or fostering current customers' loyalty. Moreover, in time, more (frequent) ReTSS actions may lead to a new type of promotion trap: retailers being caught up in a race for events that stand out. On the consumer side, more exposure to events may desensitize shoppers and dilute their interest in (and response to) specific events. As longer data series including more events become available, analysis of these long-term outcomes becomes a fruitful area for study. Marketing ImplicationsOur results reveal that ReTSS can be an effective way for traditional retailers to (temporarily) regain customers and increase in-store purchases. Consumers who spend a higher share of their grocery budget at hard discounters are especially likely to increase their visits and purchases at the traditional chain in response to the event. Moreover, even if such events do not increase profits, they do not really hurt the bottom line, either. Thus, although not a panacea, ReTSS events can be a valuable defense tool, strengthening the retailers' share of wallet among light customers and preventing them from permanently defecting to discount stores.However, not all events succeed. Generating uplift in visits and purchase volumes calls for a sufficiently large event scope. Retailers should find the right balance between raising awareness and expectations and honoring promises by offering (deep) enough deals. As for format, whereas percentage-off discounts and BOGOs—which clearly emphasize the monetary advantage—appeal most strongly to consumers, ReTSS with uniform prices seem more profitable for the retailer. Although advertising matters, the key to success is not in increasing the advertising budget per se. Instead, the media resonance of the savings theme appears to be key. This is not surprising, given that most of the incremental gains come from nonregular customers who may be more responsive to sources other than the chain's communication. Thus, apart from creating a unique and easy-to-recognize theme, retailers should strive for more earned rather than owned media impressions and focus on how to make the theme go viral. As a caveat, we also find evidence of wear-out, urging retailers to craft novel themes in time. Our retailer interviews suggest that turning the event theme into a brand of its own, and/or using market influencers to promote it, may prove fruitful here.Our findings caution retailers to be wary of consumers lowering their purchase volumes prior to and after the event. In addition, deep discounting may hamper revenue and profitability. To guard against these dangers, retailers could try to capitalize on the exploration benefits of ReTSS, by judiciously steering consumers through the aisles in search of the ReTSS offers, and on the licensing effect, by displaying impulse items in indulgence categories next to the ReTSS deals. Finally, given that ReTSS weeks attract extra consumers (in particular, hard-discount shoppers), the events may be a unique way for national-brand manufacturers to increase volume or present consumers with their (new) brand offerings. Retailers could use these arguments to increase the promotion contribution of manufacturers during events, an essential ingredient of ReTSS profitability.The effects of ReTSS may differ in countries with different retailer landscapes or business-as-usual promotion activity. Given our framework and findings, these events would be most instrumental for traditional chains severely threatened by hard discounters, in markets with substantial promotional clutter. While our focus was on grocery retailers, similar savings events emerge in nongrocery settings, such as Inno's ""Crazy Days,"" Asda's ""Green Is the New Black"" savings event, or Amazon's ""Prime Days"" (recently extended to Whole Foods). And although some of our ReTSS effects (e.g., visit or purchase expansion) may hold in those settings as well, others (e.g., purchase acceleration and stockpiling) may not, or may emerge in a different time frame—aspects that we leave for future study. "
33,"Featuring Mistakes: The Persuasive Impact of Purchase Mistakes in Online Reviews Companies often feature positive consumer reviews on their websites and in their promotional materials in an attempt to increase sales. However, little is known about which particular positive reviews companies should leverage to optimize sales. Across four lab studies involving both hypothetical and real choices as well as field data from a retailer's website (Sephora), the authors find that consumers are more likely to purchase a product if it is recommended by a reviewer who has (vs. has not) made a prior purchase mistake. The authors define a purchase mistake as a self-identified suboptimal decision whereby people purchase a product that subsequently fails to meet a threshold level of expected performance. This persuasive advantage emerges because consumers perceive reviewers who admit a purchase mistake as having more expertise than even reviewers whose purchase experience has not been marred by mistakes. As a result, in marketers' attempts to increase the persuasive influence of reviews featured in their promotional materials, they may inadvertently decrease it by omitting the very information that would lead consumers to be more likely to purchase recommended products.KEYWORDS_SPLITPotential purchasers place a great deal of stock in product reviews written by previous purchasers. More and more, these product reviews are written, read, and evaluated online ([34]). Online reviews gain as much trust as personal recommendations for the majority of consumers (85%; [40]), and a glowing review motivates behavior more than discounts and other offers (in the domains of durable goods and electronics; [28]). Consumers seem to want product reviews (www.iperceptions.com), and firms seem happy to offer such reviews on their websites, in their advertising, and elsewhere ([ 3]; [16]; [22]; [46]). Indeed, firms are increasingly engaging in efforts around ""review solicitation"" and ""online reputation management,"" incentivizing previous purchasers to write reviews in exchange for discounted or free products.As firms invest in, and consumers trust, online reviews, their management has come to occupy a more prominent role in marketing practice. Likewise, consideration of the review characteristics exerting the most impact has come to occupy a more prominent role in marketing theory. Positive reviews generally boost sales while negative reviews hurt sales ([15]; see also [23]), and firms certainly opt to feature positive over negative reviews. [41] presented evidence going one step further, examining not simply whether the review was positive but rather how the review conveyed its positivity: in explicitly stating that they endorse the product, online reviews are more likely to lead to purchases than those with more implicit positivity. This more nuanced examination of the content of online reviews—going beyond the mere valence of the review—opens the door to complementary approaches that ask not whether reviewers like the product but what the reviewers say when expressing their positivity or negativity, providing insight as to how firms can optimize the persuasive influence of featuring reviews. To illustrate how we propose to add to this growing and important literature, consider the following two real reviews from Amazon:When my first Canon battery expired, I purchased a knock-off. What a mistake. It lasted about 1/3 as long as the Canon. (Richard J. Martin, review for a Canon battery)The Canon battery is essential to have. Knock-off brands don't last a quarter as long as the Canon branded batteries. (""tac cat,"" review for a Canon battery)Both of these reviews are positive, but which would—and which should—Canon feature: Richard, who previously purchased a product that turned out to be a mistake, or ""tac cat,"" whose purchase experience was not marred by a mistake?The present investigation develops a theoretical model in proposing that admitting to having made a previous mistake acts as a powerful cue through which potential purchasers infer that the reviewer has gained significant expertise about a focal product domain, which in turn increases the potential purchasers' likelihood of purchasing the product that the reviewer recommends. In keeping with academic and applied norms, we refer to the communication under consideration as a ""review"" and the communicator as a ""reviewer."" However, defined formally, reviews need only describe the reviewer's experience, whereas ""recommendations"" advise on what to do. Because we want to better understand how the content of communication shapes its ability to persuade, the communications in our experiments will be presented as reviews yet worded as recommendations that prescribe a certain course of action to allow us to measure the degree to which our participants are persuaded (cf. [56]). Admitting Mistakes: Helpful or Harmful for Persuasion?Drawing from previous conceptual work, we define a purchase mistake as a self-identified suboptimal decision whereby people purchase a product that subsequently fails to meet a threshold level of expected performance ([25]; [32]; [39]). Accordingly, we are not concerned with mistakes of execution, whereby someone purchases a product unintentionally or by accident. Rather, as an example of a mistake in the form of a suboptimal decision, a consumer might purchase a speaker system only to find that the speakers do not perform as well as anticipated. Does this purchase mistake—specifically, as admitted by the mistaken purchaser—alter the willingness of other potential purchasers to heed the advice of this mistaken purchaser?One possibility is that admitting to having made a mistake undermines other people's inclination to follow any advice from the mistaken reviewer. After all, admitting to a mistake necessitates the making of a mistake, and mistakes may signal incompetence: The mistaken reviewer was not sufficiently thoughtful, intelligent, and/or knowledgeable to adequately assess the relevant product specifications and make a good purchase decision ([ 2]; [18]; [54]). People often judge others who make mistakes as incompetent and punish them (e.g., [14]; [20]; [30]; [42]), so awareness of a reviewer's previous purchase mistake might predict an outcome resulting from an inference of incompetence and akin to punishment: refusing to take the reviewer's advice.Another possibility, which forms the foundation of our theoretical model, is that people do not necessarily attribute others' admission of a purchase mistake to incompetence (i.e., a stable, permanent cause; [17]; [24]; [43]). Rather, we theorize that a reviewer's admitted purchase mistake signals a temporary lack of knowledge, which is an unstable cause that can change over time ([17]; [24]; [31]; [43]). Provided that a lack of knowledge is temporary and, thus, fixable, we further propose that reviewers who admit to having made a mistake will prove especially likely to be seen as having rectified this lack of knowledge (i.e., to have gained expertise since the purchase mistake; we conceptualize expertise as a subcomponent of the broader construct of consumer knowledge in keeping with [ 1]]).These predictions derive from two lines of reasoning. First, to echo and advance a point made previously, all mistake admissions require a mistake to have occurred, but not all mistakes that are made result in the admission of a mistake. In other words, only a subset of mistaken purchasers will ever admit to their mistakes, and even fewer mistaken purchasers will admit to their mistakes in a public forum (like online reviews). We propose that admitting to having made a mistake inherently conveys that the mistaken reviewer has gained new expertise. In the case of product purchases, the mistaken reviewer, in admitting the mistake, must now know not only that the original product has fallen short of initial expectations (forming the basis of a negative assessment of the product) but also that a different course of action (purchasing a different product) would have proven better (for other opportunities to learn from mistakes and signal that learning to others in online reviews, see the ""General Discussion"" section). On top of this (which applies even in admitting a mistake to oneself), we reason that the public admission of having made a mistake signals that mistaken reviewers are especially confident in their current assessment (e.g., product review), resulting from new expertise gained, insofar as they are willing to engage in the costly behavior of conceding something negative about themselves. Conversely, when reviewers acknowledge switching between brands without admitting a mistake, that reviewer's reason for switching remains more vague: admitting a mistake signals a gaining of expertise, whereas no such admission could result from any number of factors (e.g., promotions, stockouts, variety seeking; [50]; [51]; [52]).Second, we propose that the impact on others of a reviewer admitting to a mistake also results from the experience of those others (here, potential purchasers reading online reviews) in making mistakes of their own. Everyone makes mistakes, and a common response is to exert additional effort to learn from the mistake to avoid making a similar mistake again in the future ([ 4]; [13]; [35]; [44]). Indeed, making a mistake (vs. enjoying success) prompts greater subsequent effort in the mistaken domain through the use of self-regulation ([13]): the mistake signals a discrepancy between a desired state (e.g., owning a good product) and the current state (e.g., having made a bad purchase), which triggers attempts to reduce the discrepancy and attain the desired goal ([12]; [44]). People experience this effect consciously: they are often clearly aware that making a mistake causes them to exert more effort in similar situations in the future ([12]). Just as people themselves exert greater effort following a mistake (vs. a success), they may believe that others exert greater effort after their mistakes as well. Because people assume that greater effort produces better outcomes ([29]), we formally predict the following: H1:  Consumers infer that reviewers who admit to having made a previous purchase mistake (vs. reviewers who mention having made a successful purchase) have more expertise in the mistaken product domain.We also predict that these inferences shape the persuasive impact of reviews. This is because the perception of a reviewer's expertise is a primary determinant of whether people purchase the products that a reviewer recommends: people are more likely to follow the recommendations of those who appear to have more expertise about the domain in which they are making a recommendation ([41]; [45]; [53]; [55]). At this downstream level, we predict the following: H2:  Consumers are more likely to purchase a product recommended by reviewers who admit to having made a previous purchase mistake (vs. reviewers who mention having made a successful purchase). H3:  Consumer inferences of expertise for reviewers who admit to having made a previous purchase mistake (vs. reviewers who mention having made a successful purchase) mediate the relationship between admission of a mistake and likelihood of purchasing the recommended product.Our hypothesis development has centered on inferences of expertise, but we do not propose that any admission of having made a previous mistake always enhances perceived expertise in a way that should increase advice taking. Rather, consumers should be able to discern whether a review in which a mistake was admitted presents compelling evidence that the reviewer has learned from that mistake. As an implication of this predicted sensitivity on the part of consumers, we propose—as a boundary condition to our main effect—that consumers should be more likely to follow the purchase-related advice of reviewers who admit to having made a previous mistake only when the mistake conveys that the reviewer has gained expertise since making the mistake. Specifically, we theorize that the expertise only should be seen as strengthened in the domain in which the reviewer made the mistake. Accordingly, potential purchasers should heed the mistaken reviewer's advice for products in that domain but, provided the expertise does not transfer readily across domains, not for products in any other domain. Formally, we predict the following: H4:  Consumers are more likely to purchase a product recommended by reviewers who admit to having made a previous purchase mistake (vs. reviewers who mention having made a successful purchase) in the same domain, whereas consumers are no more likely to purchase a product recommended by reviewers who admit to having made a previous purchase mistake (vs. reviewers who mention having made a successful purchase) in a different domain. Research OverviewStudy 1 tests our predictions that people infer mistaken reviewers to have more expertise in the product domain in which the mistake was made (H1), that purchase advice is more likely to be accepted from a mistaken reviewer (H2), and that the former accounts for the latter (H3). Study 1 tests these predictions using an incentive-compatible design, and Study 2 tests these same predictions (H1–H3) but by assessing inferred expertise with a different measure in the interest of providing a robustness check. Study 3 manipulates not only admission of a mistake but also the domain alignment of the mistake, introducing a key moderator to provide evidence for the boundary condition articulated in H4. Study 4 introduces several modifications to the general design of Studies 1–3 to provide evidence for a robust effect relating mention of making a mistake to acceptance of purchase advice (H2) for a real purchase decision. Finally, Study 5 examines mistaken reviewers' persuasive impact in the field by examining real reviews from a popular website (Sephora). Thus, these data suggest that the persuasive power of mistakes is sufficiently robust to emerge in the noisy real world.The inclusion of orthogonal experimental factors in these designs indirectly speaks to several alternative explanations that we discuss in turn, and a posttest of Study 3 directly measures other inferences consumers might make of reviewers who admit mistakes, finding that they do not account for the effect observed throughout the present investigation. We identify several such possibilities for these findings. First, a positive review for a focal product that includes admission of a mistake acknowledges the existence of both well-performing and underperforming brands. This may result in a brand comparison effect, whereby any mention of poor performance for one brand makes the other, well-performing brand look better (consistent with the effectiveness of comparative advertising; [21]). A second, related explanation might hold not for perception of the two brands but for perception of the reviewer: in mentioning the two brand performances, observers might infer that the mistaken reviewer has more carefully considered both the positives and the negatives of the focal alternatives. Third, a review that includes the admission of a mistake necessarily includes both positive information (a favorable review for the focal product) and negative information (that the previous purchase was a mistake), in contrast to the unilateral positivity of the successful purchase conditions. As a result, perhaps exposure to any negative information in a review orients consumers to potential losses, looming larger than potential gains and prompting them to accept more readily the advice in a review (e.g., as the result of risk aversion: [26]). Should this be the case, then mention of any mistake might strengthen the tendency to accept the advice in a review. These alternative explanations would predict that consumers should still be persuaded by a reviewer who first makes a mistake in one product domain and then makes a successful product purchase in a different domain. Instead, we predict and test in Study 3 that admission of a mistake will change only inferred expertise in the same, focal product domain, rendering moot advice regarding other product domains (H4). Separately, reviewers who admit mistakes might be perceived differently on multiple different inferred characteristics by observers, including specific personality traits and as having global expertise that extends across different product domains. Accordingly, a posttest of Study 3 measures several such potential inferences, finding no connection between them and the admission of a mistake.While we designed Study 3 with the goal of speaking against several alternative psychological explanations for our effect, we designed Study 4 with the goal of speaking to practitioners interested in the breadth by which our effect might be applied. For this reason, Study 4 departs from Studies 1–3 in subtle but meaningful ways. First, it presents not a single review but a set of ten reviews in which we either did or did not embed a single review in which the reviewer mentions making a previous purchase mistake. Second, rather than reviews for fictitious brands or brands with which participants ostensibly have little familiarity, Study 4 presents reviews for a known brand. Using a known brand facilitated our third change: having participants make a consequential choice. To compliment the consequential setup in Study 1 (in which one participant received a chosen outcome), all participants in Study 4 received the outcome they choose. Thus, Study 4 attests to the robustness of our effect in tandem with the large-scale data-mining approach adopted in Study 5. Study 1In an incentive-compatible context, Study 1 tests H1–H3: consumers infer that a reviewer has more expertise about a product category if the reviewer admits to previously making a mistake in purchasing a product from that category than if the reviewer does not, and this inference of expertise accounts for the greater tendency of consumers to choose in line with this reviewer's advice. MethodOne hundred sixty participants (mean age = 35 years; 39% male) from a large East Coast U.S. university participated in a laboratory study in exchange for course credit. This study was run as part of a session containing unrelated surveys from different researchers. All participants read that, as additional compensation for their participation in the lab session, they would be entered into a lottery for a prize. Participants further read that if they won the lottery, they would receive a pair of headphones and that they would choose which one of two sets of headphones they preferred to receive as their prize. All participants then viewed information about the two sets of headphones, which were called Orbin and Raymour (see Web Appendix A). They also read a consumer review that was (ostensibly) the most recently submitted review for these headphones. Specifically, participants saw that the most recently submitted review was written by a consumer named Sam. Participants were randomly assigned to one of two conditions: a mistaken reviewer condition or a successful reviewer condition. In both conditions, Sam's review recommended the Orbin headphones (purchased most recently) and also described a previous purchase of headphones, providing his experience with both sets of headphones in terms of general evaluation as well as performance on the same particular attribute (a sensor). In the mistaken reviewer condition, Sam noted that this previous purchase was a mistake:A couple years ago when I was searching for the last pair of headphones that I bought, I ended up buying the Nidec VIA headphones, and that was a mistake—it turned out that the headphones had a bad type of sensor and therefore did not work well. I recently decided to upgrade my headphones to a newer model, and I considered both the Orbin and Raymour headphones. I chose the Orbin headphones. I've had them for a month, and they are good—they have great features, including a good type of sensor, and they work well. I would recommend them. (Sam K.)Conversely, in the successful reviewer condition, Sam noted that this previous decision was successful:A couple years ago when I was searching for the last pair of headphones that I bought, I ended up buying the Nidec VIA headphones, and that was a good choice—it turned out that the headphones had a good type of sensor and therefore worked well. I recently decided to upgrade my headphones to a newer model, and I considered both the Orbin and Raymour headphones. I chose the Orbin headphones. I've had them for a month, and they are good—they have great features, including a good type of sensor, and they work well. I would recommend them. (Sam K.)Next, participants were asked whether they preferred to receive the Orbin or the Raymour headphones if they won the lottery. The instructions emphasized that this decision was real. Participants entered their decision by selecting a radio button that was labeled either with ""Orbin"" or ""Raymour."" They also rated how much they perceived that the reviewer had learned about how to choose good headphones (1 = ""Not much at all,"" and 7 = ""A lot""). At the end of the study, participants were debriefed (i.e., they were informed that the lottery was real but was for a pair of Sony headphones rather than for the brands that they read about in the study). Results and DiscussionAs we predicted, participants more often chose the recommended headphones when they were recommended by a mistaken reviewer (93.1%) than when they were recommended by a successful reviewer (79.5%; χ2(d.f. = 1, N = 160) = 5.87, p =.015; Cohen's d =.390). Furthermore, participants perceived that the mistaken reviewer learned more about how to choose good headphones (M = 5.96, SD = 1.01) than the successful reviewer (M = 4.97, SD = 1.39; t(158) = 5.07, p <.001, Cohen's d =.811).We hypothesized that the mistaken (vs. successful) reviewer more strongly influenced participants' real choices because participants inferred that the mistaken reviewer gained more expertise about how to choose good headphones. Consistent with this prediction, a mediation analysis with 5,000 bootstraps revealed that perceived learning mediated the effect of condition on participants' real choices (95% confidence interval [CI] for the indirect effect = [.1614,.8766]; see Figure 1).Graph: Figure 1. Mediation model in Study 1.Notes: The path coefficients are unstandardized betas. Values in parentheses indicate the effect of condition on the dependent variable after controlling for the mediator. 95% CI for the indirect effect = [.1614,.8766]. *p <.05. **p <.01. ***p <.001.In summary, Study 1 documents the persuasive power of mistakes in an incentive-compatible context. Moreover, Study 1 suggests that this power arises because people infer that a purchaser who makes a mistake subsequently gains more expertise—assessed here in the form of learning—about how to identify a good product in that domain than a purchaser who originally chose successfully. Here, the mistaken reviewer identified the previous purchase as a mistake on the basis of its performance on one particular attribute (a sensor), creating potential confounds (e.g., reviewer evaluation of this highly technical attribute leading to observer inferences of preexisting expertise) that we address in our subsequent studies by having the mistaken reviewers concede only that they made previous purchase mistakes without detailing the reasons for this conclusion. Study 2Study 1 provides, in a consequential choice setting, evidence consistent with the prediction that consumers are more persuaded by reviewers who have (vs. have not) previously made a purchase mistake through an inference that those reviewers have learned more. As we have noted, we believe that an inference of learning implies the gaining of new consumer knowledge in the form of expertise, though Study 1 assessed only the former. Thus, in Study 2, we test the same hypotheses as Study 1 (H1–H3) using a modified empirical approach. Specifically, we utilize a hypothetical scenario design to complement the incentive-compatible design of Study 1, and we measure our proposed mediator by asking whether perceivers judge mistaken reviewers to have more knowledge in the relevant domain compared with successful reviewers. MethodEighty participants (mean age = 34 years; 63% male) in an online participant pool participated in a study in exchange for monetary payment. All participants imagined that they lived in Seattle and were looking for a local florist. They further imagined that they had narrowed their choices down to two local artisan floral shops that served only the local Seattle area—FlowersNow and FreshBlooms. Participants viewed information about the two floral shops (Web Appendix B) and imagined that they looked at the websites of the two florists to help them make their decision. On the FreshBlooms website, participants saw that there was a review written by a consumer named Sam. In both conditions, Sam's review recommended FreshBlooms and also described a previous choice he had made between two florists when he lived in a different state. In the mistaken reviewer condition, Sam noted that this previous decision was a mistake:I just moved to Seattle from Boston. When I was in Boston, I decided to get flowers once from a local florist in Boston (which sells flowers only in the Boston area). That was a mistake—the florist I chose was not a good florist. Now that I've moved to Seattle, I wanted to get flowers for my housewarming party. My final choice came down to flowers from FlowersNow or FreshBlooms. After looking into both options, it was clear to me that FreshBlooms is the better florist. I decided to get flowers from FreshBlooms, and that was a great choice. I recommend FreshBlooms! (Sam K.)Conversely, in the successful reviewer condition, Sam noted that this previous decision was successful:I just moved to Seattle from Boston. When I was in Boston, I decided to get flowers once from a local florist in Boston (which sells flowers only in the Boston area). That was a good choice—the florist I chose was a good florist. Now that I've moved to Seattle, I wanted to get flowers for my housewarming party. My final choice came down to flowers from FlowersNow or FreshBlooms. After looking into both options, it was clear to me that FreshBlooms is the better florist. I decided to get flowers from FreshBlooms, and that was a great choice. I recommend FreshBlooms! (Sam K.)Next, participants reported whether they would choose to buy flowers from FreshBlooms or FlowersNow. They also completed a two-item index of their perceptions of the reviewer's knowledge. Specifically, they indicated how much knowledge the reviewer now had about how to choose a good florist and how much knowledge the reviewer had about whether to buy flowers at FreshBlooms or FlowersNow. Participants responded to each question on separate seven-point scales (1 = ""Not a lot,"" and 7 = ""A lot""). We combined the items into an index of perceived knowledge (r =.53, p <.001). We predicted that the mistaken (vs. successful) reviewer would more strongly influence participants' choices because participants would perceive the mistaken reviewer as more knowledgeable. Results and DiscussionAs we predicted, participants more often chose the recommended florist when it was recommended by a mistaken reviewer (90.9%) than when it was recommended by a successful reviewer (61.7%; χ2(d.f. = 1, N = 80) = 8.54, p =.003, Cohen's d =.691). Furthermore, participants perceived that the mistaken reviewer had more knowledge (M = 4.68, SD = 1.27) than the successful reviewer (M = 3.84, SD = 1.19; t(78) = 3.03, p =.003, Cohen's d =.683). To test the mediating role of perceived knowledge in determining the effect of condition on choice, we conducted a mediation analysis with 5,000 bootstraps. As we hypothesized, perceived knowledge mediated the effect of condition on choice (95% CI for the indirect effect = [.0545,.2834]; see Figure 2).Graph: Figure 2. Mediation model in Study 2.Notes: The path coefficients are unstandardized betas. Values in parentheses indicate the effect of condition on the dependent variable after controlling for the mediator. 95% CI for the indirect effect = [.0545,.2834]. *p <.05. **p <.01. ***p <.001.Along with Study 1, Study 2 provides evidence for the persuasive power of mistakes. Moreover, in Study 2, we establish that this power arises because consumers infer that a reviewer who admits to a mistake has more knowledge than an equivalent reviewer who originally chose successfully. Having documented a robust effect in Study 2, in Study 3 we probe a potential moderator of the persuasive power of mistakes. Study 3Our theoretical model proposes that the persuasive power of reviews featuring mistakes comes from an inference that the reviewer has significant expertise (H3). As a result, any information that undermines the tendency for observers to infer that the reviewer has this expertise should, in turn, undermine the extent to which the review is persuasive. Thus, to provide convergent evidence, Study 3 utilizes a moderation-of-process design ([49]) that manipulates not only the admission of a mistake but also an additional experimental factor designed to compromise the inferred expertise of the reviewer, which also addresses potential alternative accounts. Specifically, if the persuasive power of mistakes is due to the belief that reviewers' purchase mistakes signal that the reviewers have gained more expertise about the product domain, then mistaken reviewers' recommendations will be more persuasive only when the reviewers' purchase mistakes occur in the same product category as their focal review (H4). We tested this prediction in Study 3 by presenting participants with a review from either a mistaken or successful reviewer, in keeping with the designs of Studies 1 and 2, but departing from those studies in that the previous purchase was in either the same or a different product category as the review. MethodTwo hundred ninety-nine participants (mean age = 32 years; 49% male) from Amazon's Mechanical Turk (MTurk) completed an online study in exchange for monetary payment. The participants' task was to decide which one of two in-ceiling speaker systems they would prefer to purchase: the Mikana XPI in-ceiling speaker system or the Rokana SX2 in-ceiling speaker system. Participants viewed the specifications of the two speaker systems (which were identical to the specifications of the headphones described in Study 1, as the specifications could reasonably apply to both product categories; see Web Appendix A) and read two reviews written by a consumer named Taylor. Both reviews were ostensibly on a website that featured consumer reviews for different electronics. The focal review, which was the same in all conditions, noted that Taylor purchased a Mikana XPI speaker system and recommended it. However, the content of the nonfocal review differed by condition. Specifically, participants were randomly assigned to read that Taylor had previously purchased a bookshelf speaker (i.e., a product in the same domain as the focal product) or a printer (i.e., a product in a different domain from the focal product) and that this decision was either a mistake or a success (see Web Appendix C). These manipulations produced a 2 (nonfocal product type: speakers vs. printer) × 2 (nonfocal review type: mistake vs. success) design.After participants viewed Taylor's reviews (both reviews were explicitly noted as written by Taylor), they decided whether they would purchase the Mikana XPI or the Rokana SX2 speaker system. Participants entered their decision by selecting a radio button that was labeled either with ""Mikana XPI"" or ""Rokana SX2."" ResultsWe conducted a binary logistic regression using nonfocal review type (mistake vs. success), nonfocal product type (speakers vs. printer), and their interaction to predict participants' choices. The regression revealed an interaction on choice (b = 2.68, z = 2.67, p =.008). In a conceptual replication of the previous studies, when the reviewer had made a previous speaker purchase, participants who read that the previous purchase was a mistake chose the recommended speaker more often (97.4%) than participants who read about a successful purchase (87.0%; b = 1.72, z = 2.17, p =.030, odds ratio = 5.597). Conversely, when the reviewer had made a previous printer purchase, we found no such effect (mistake = 86.5%, success = 94.4%; b = −.96, z = −1.56, p =.119). PosttestIf a mistake's persuasive power arises because admitting a mistake signals some broadly positive character trait (e.g., the integrity to admit one's mistakes publicly), then mistaken reviewers should be more persuasive regardless of the domain of their mistake. Thus, we conducted a posttest to establish whether mentioning a mistake—either in the same domain or a different domain as the focal product—alters how consumers perceive the mistaken reviewer. The design of the posttest mirrored that of the main study, save for a switch from measuring choice to measuring several such potential perceptions. Specifically, the posttest began with participants' assessment of the extent to which a mistaken reviewer is discerning, has integrity, is likeable, and is similar to the self. The posttest also measured whether the mistake was surprising, because surprise can orient attention to a focal piece of information and increase persuasion as a result ([ 8]). Finally, in the interest of presenting evidence not only against these alternative explanations but also in support of our proposed mechanism, participants rated the perceived expertise of the mistaken reviewer to conceptually replicate the mediation results from Studies 1 and 2. We examined these issues using a posttest rather than in the main study to avoid potential demand effects from asking participants about both their purchase intentions and their perceptions of the mistaken reviewer in the same study.We recruited a separate sample of MTurk participants (N = 350; mean age = 38 years; 50% male), who were randomly assigned to view the same information presented to participants in the main study. Posttest participants then completed measures, presented in a random order, assessing their perceptions of the extent to which Taylor was discerning and had high integrity, the extent to which they liked Taylor and were similar to Taylor, and the extent to which Taylor's reviews were surprising. In addition, participants completed a two-item index of their perceptions of Taylor's expertise (adapted from [41]). Specifically, they indicated how much of an expert they thought Taylor was about speakers and how knowledgeable they thought Taylor was about speakers. Similar to [41], we combined these items into an index of perceived expertise that correlated at a level (r =.66, p <.001) commensurate with their original work (r =.53). Participants indicated their responses on separate seven-point scales (1= ""Not at all,"" and 7 = ""Very much"").As we expected, the interaction between nonfocal product type and nonfocal review type was significant for perceived expertise (F( 1, 346) = 4.43, p =.036). When the reviewer had made a previous speaker purchase, the mistaken reviewer was perceived as more expert (M = 4.64, SD = 1.13) compared with the successful reviewer (M = 4.31, SD =.99; Fisher's least significant difference: p =.043; Cohen's d =.311). In contrast, when the reviewer had made a previous printer purchase, there were no differences in perceived expertise between the mistaken reviewer (M = 4.59, SD = 1.09) and the successful reviewer (M = 4.74, SD = 1.01; F < 1). Participants' perceptions of the extent to which Taylor was discerning, had integrity, was likeable, and was similar to themselves, as well as the extent to which their mistake was surprising did not differ as a function of product type and review type (Fs( 1, 346) < 2.52, ps >.114; see Table 1).GraphTable 1. Descriptive Statistics in the Posttest of Study 3.  1 Notes: Statistics in parentheses are standard deviations. DiscussionThese results are inconsistent with the possibility that a mistake's persuasive power arises because they signal some broad character trait that enhances discernment, integrity, likeability, or perceived similarity to the observer. Theoretically, such character traits should have been signaled just as well by a printer-related mistake as by a speaker-related mistake, in which case we would not have found the predicted interaction. Moreover, Study 3's posttest confirmed that Taylor was perceived equivalently on each of these dimensions regardless of whether he made a prior mistake purchasing a printer or speakers. Study 3, however, does not speak to one remaining potential inference: that the mistaken reviewer had high expertise from the outset instead of gaining expertise. Although at face value this possibility might seem inconsistent with making a mistake in the first place, we conducted a supplementary study (reported in Web Appendix D) to demonstrate that the persuasive power of mistakes arises not because people assume that a mistaken reviewer had a lot of knowledge to begin with to discern their mistake (and not because of any other variable that consumers may assume plays into reviewers' ability or willingness to acknowledge a mistake, or because of some other difference in the content of mistake- vs. success-based reviews), but rather because they assume that a mistaken reviewer has acquired more expertise as a result of their mistake. Study 4The first three studies identified the psychological foundation underlying why mentioning a mistake causes consumers to place more credence in the advice of those reviewers. In the pursuit of this objective, the first three studies prioritized internal validity over external validity; Study 4 shifts its balance to consider the impact of mistaken reviewers in more ecologically valid contexts. While considering a purchase, consumers regularly read not only a single review in isolation but, rather, multiple reviews to form an overall conclusion. To capture this reality, participants see not one but ten reviews for a focal product in Study 4; two experimental conditions vary whether one review, embedded within that set of ten, mentions a prior purchase mistake. Furthermore, would the effect of mentioning mistakes extend from the fictitious or generic brands used in our previous studies to established, known brands presumably higher in brand equity? Study 4 considers this robustness issue by using a real brand (Altoids mints) as the focal product under consideration. Finally, as this study aims to provide the clearest point of direct application, it requires all participants to make a real purchase choice. Whereas the choice in Study 1 was incentive-compatible insofar as one participant would ultimately receive the chosen outcome, incentive compatibility is strengthened in Study 4, which asks all participants to make a real purchase decision. We predicted that mention of a mistake would still prove powerful under this more naturalistic set of conditions. MethodTwo hundred forty-nine participants (mean age = 35 years; 43% male) from a large U.S. East Coast university participated in a session of laboratory studies in exchange for monetary payment. This study was run as part of a session containing unrelated surveys from different researchers. All participants were told that they would be asked to read several reviews of spearmint mints made by a brand of which we presumed participants to have at least some knowledge (Altoids) and then asked to make a choice. Participants were randomly assigned to one of two conditions: a mistake condition or a no-mistake condition. All participants viewed a total of ten reviews that were presented in a random order. Nine of these reviews were identical between the experimental conditions and were taken from actual Altoids spearmint mints reviews posted on Amazon (including star rating, title of review, and review text; for all reviews, see Web Appendix E). We varied the content of the remaining review to either include a reference to the reviewer having made a previous mistake (mistake condition) or not (no-mistake condition). After reading the reviews, participants were told that as additional compensation for the survey session, they could choose to receive either one pack of Altoids spearmint mints or one additional dollar added to their payment. Participants entered their decision by selecting a radio button that was labeled either with ""Altoids Spearmint Mints"" or ""One Additional Dollar,"" and the researcher then provided the participant with their chosen form of additional compensation. Results and DiscussionA chi-square analysis revealed that participants reading a set of reviews that contained one review in which the reviewer mentioned making a mistake were more likely to choose the Altoids spearmint mints over additional monetary compensation (34.9%) than were participants for whom the provided set of reviews did not contain a review mentioning a mistake (22.0%; χ2(d.f. = 1, N = 249) = 5.14, p =.023; Cohen's d =.290). Thus, as we predicted, even when the review mentioning a mistake has only a minority presence (i.e., comprises one review out of ten), it still exerts an effect powerful enough to change consumer choice. Notably, the behavior under consideration here closely reflects real consumer decision making, as our participants learned about a widely known brand and subsequently made a consequential choice (between receiving the branded product or receiving additional money). These results attest to the strength and robustness of our effect, which we extend in Study 5 to a different naturalistic context. Study 5In our final study, we test the external validity of our findings by examining whether the persuasive power of mistakes is sufficiently robust to emerge in the noisy real world. To that end, we examine consumer reviews on Sephora's online retail platform. Conveniently, the Sephora review platform has a feature whereby readers can rate whether reviews are helpful, which is indicative of whether they are persuasive (see [ 7]; [38]; see also the pilot test in the ""Results and Discussion"" subsection of this study). We predict that reviews referencing a purchase mistake will be linked to consumers finding the review more helpful (as measured by reader-provided ratings of helpfulness). MethodThe Sephora website includes six product categories (makeup, skincare, hair, tools and brushes, fragrance, and bath and body). The category to be scraped (hair) was randomly chosen and, after determining 40 products as the maximum number able to be scraped within a reasonable time frame, 40 haircare products were randomly chosen. Within that subset, we scraped all reviews starting in August 2017 until the time of scraping (December 24, 2018). For the entire resulting set of 5,727 reviews, we used a series of indicator variables to tag whether each review mentioned a previous purchase mistake. Specifically, the (case-insensitive) indicators were: mistak: the string ""mistak"" is in the review; mistook: the string ""mistook"" is in the review; my_bad: the phrase ""my bad"" or ""my error"" is in the review; I_wrong: the word ""I"" is within 35 characters of a word starting with ""wrong"" (without a period, question mark, or exclamation mark in between, which are proxies for sentence divisions); my_fault: the word ""my"" is within 35 characters of the word ""fault"" (again, without a period, question mark, or exclamation mark in between); and our_fault: the word ""our"" is within 35 characters of the word ""fault"" (again, without a period, question mark, or exclamation mark in between). This tagging led to the identification of 502 reviews referencing a prior mistake. Two independent judges (interjudge reliability: r =.92) reviewed these 502 reviews, tagging 86% of them to be about mistakes in choice.[ 5] To equate sample sizes, we then randomly selected 502 reviews that did not reference a mistake from the remaining scraped data set, resulting in a data set of 1,004 reviews. Results and Discussion Pilot testPrevious research has suggested that helpfulness votes are a proxy for persuasiveness ([ 7]; [38]). We conducted our own pilot test to further verify this conclusion. That is, we tested whether consumers rate a review as helpful when it guides their purchase decision. To that end, we recruited 72 participants from MTurk who reported that they had previously rated an Amazon review as helpful or unhelpful. We then asked them to describe (in an open-response box) the factors that influence their decisions about whether to rate a review as helpful or unhelpful. On the next survey page, we showed them the description that they had written and asked them whether they wrote that they were more likely to rate an Amazon review as helpful when the review made them want to follow the reviewer's advice (a measure of persuasion; [10]; [11]; [36]). A chi-square analysis revealed that most people (72.2%) reported that they rate a review as helpful when it is persuasive (χ2 (d.f. = 1) = 14.22, p <.001). Thus, consistent with prior literature ([ 7]; [38]), reviews' helpfulness ratings serve as a proxy for their persuasive power. Primary analysesWe computed a measure of each review's persuasive power by subtracting the number of unhelpful votes from helpful votes associated with each review and dividing that by the total number of votes (helpful − unhelpful)/(helpful + unhelpful). This measure served as our dependent variable, and it ranged from −1 to 1 (M =.078, SD =.329; Mhelpful = 3.672, SD = 16.621; Munhelpful =.774, SD = 3.285). As a first step, we regressed this helpfulness measure on whether the review referenced a mistake (−1 = no, 1 = yes). As we predicted, the regression revealed that reviews referencing a mistake (vs. those not referencing a mistake) were deemed more helpful (b =.076, SE =.010, p <.001). Next, we regressed the helpfulness measure on whether the review referenced a mistake (−1 = no, 1 = yes) and the following control measures (all continuous control variables were mean-centered): review length (number of words), valence of review (−1 = negative, 0 = both positive and negative, 1 = positive; coded by two independent judges with high interjudge reliability: r =.93), star rating (1–5), explicit recommendation (−1 = no, 1 = yes; feature included on the Sephora website), loyalty program membership tier (with higher numbers indicating more dollars spent at Sephora in a calendar year; 1 = ""Insider,"" 2 = ""VIB,"" and 3 = ""Rouge""), reviewer expertise (with higher numbers indicating more reviews posted to the Sephora site; 1 = ""Rookie,"" 2 = ""Rising Star,"" 3 = ""Go Getter,"" and 4 = ""Boss""), number of images uploaded with the review, explicit mention of another brand in the review (−1 = no, 1 = yes; coded by two independent judges with high interjudge reliability, r =.94), and date of review (calculated as number of days since December 30, 1899 on the Gregorian calendar). The regression revealed a significant effect of review length on helpfulness, such that longer reviews were found to be more helpful (b =.001, SE <.000, p <.001). None of the other control variables had a significant effect on helpfulness (ps >.304). Most relevant to our focal theorizing, the regression also revealed that reviews referencing a mistake (vs. those not referencing a mistake) were deemed more helpful in the full analysis controlling for review length, valence of review, star rating, explicit recommendation, loyalty program membership, reviewer expertise, number of images uploaded with the review, explicit mention of another brand in the review, and date of review (b =.056, SE =.014, p <.001; see Table 2).[ 6]GraphTable 2. Results of Regression Analysis on Helpfulness Index for Study 5.  2 Notes: The R-square of the simple model (without controls) is.053; the R-square of the full model with controls is.080.3 *p <.05.4 **p <.01.5 ***p <.001.In summary, Study 5 provides evidence that Sephora users rate reviews that mention a purchase mistake as more helpful. Even after including numerous controls, the relationship between mention of a mistake and review helpfulness still holds. We targeted helpfulness as a meaningful construct of consideration, as our pilot test and previous research indicate that a helpful review is a persuasive review, and marketing managers interested in increasing sales begin with the goal of persuading consumers to purchase their products. Thus, these results underscore the applied relevance of mentioning mistakes by providing evidence for a robust connection to review helpfulness using data taken from a real online retailer. General DiscussionPeople are often skeptical about whether the reviews they encounter are authored by well-informed consumers and thus first evaluate whether a reviewer is credible before deciding whether to rely on his or her review (e.g., [45]; [53]; [55]). We find that people are more likely to conclude that a reviewer has more expertise, and are thus more likely to purchase the product that a reviewer recommends, if the reviewer admits to having made a purchase mistake in that domain. The current research thus suggests that featuring purchase mistakes in online consumer reviews offers a promising opportunity as a persuasive tactic. Our research therefore provides important, practical insight into the inputs underlying consumers' decisions about whether to purchase reviewed products while also making several theoretical contributions. First, whereas substantial research has documented the negative inferences that observers make about mistake makers, our research uncovers the conditions in which learning about others' mistakes leads people to perceive those others more positively (cf. products made by mistake; [47])—in particular, the conditions in which mistaken reviewers are perceived to be more expert and better able to identify the optimal course of action than even reviewers whose previous experience has not been marred by mistakes. We integrate the persuasion and attribution theory literature streams to illuminate a powerful factor that shapes the persuasive impact of consumer reviews on purchase decisions. Alternative ExplanationsIn addition to illuminating the persuasive impact of mistake makers and the mechanism underlying this phenomenon, we also examined several alternative explanations. Merely comparing two brands proved insufficient to evoke our effect in Study 3, which instead provided evidence consistent with H4, which posited domain dependence. The posttest of Study 3 provided further evidence inconsistent with the possibility that inferences regarding mistaken reviewers being discerning, having integrity, being likeable, and being similar to the message recipient contribute to the phenomenon we document, nor did potential surprise from admitting a mistake. We note that these findings do not preclude the possibility that some types of purchase mistakes could alter perceptions of reviewers (for these or other traits) in a manner that would affect their persuasive power. Nevertheless, the fact that the proposed phenomenon emerged—for both purchase decisions in the main study and inference of expertise in the posttest of Study 3—when the reviewers were perceived equivalently on these dimensions suggests that these accounts are insufficient to explain our findings.Furthermore, the mere delivery of negative information does not appear to increase mistaken reviewers' persuasive impact (neither through loss aversion [[26]] nor through alternate routes including but not limited to a negativity bias, a blemishing effect, mistake-induced perceptions of warmth, or negativity-induced perceptions of competence [[19]; [48]]). If the mere delivery of negative information drove mistaken reviewers' persuasive impact, then the reference to a mistake in a different product domain could have increased persuasion. We did not observe this outcome. These results are thus inconsistent with the possibility that the mere delivery of negative information increases mistaken reviewers' persuasive impact.These findings also cannot be accounted for by a pratfall effect (clumsy actions that enhance the attractiveness of superior others; [ 5]). For one thing, this alternative possibility is theoretically implausible: pratfalls only affect assessments of superior, and thus potentially threatening, others ([ 5]), and it seems unlikely that participants in the current studies perceived their fellow consumers to have a superiority needed for the pratfall effect to emerge. Moreover, Study 3 finds that the mere presence of a mistake is insufficient to increase a reviewer's persuasive impact, as would be predicted by the pratfall effect; thus, the current results are inconsistent with this potential alternative explanation. Taken together, these results thus suggest that acknowledging mistakes can play an important role in promoting persuasion and influencing purchase decisions. Future DirectionsThe inferential process documented in these studies suggests that there are likely several boundaries to the persuasive power of mistakes. We documented one in the current research (H4): when a reviewer makes a mistake in a different domain from the focal purchase (Study 3), potential purchasers are no longer more persuaded by the reviewer. Our theoretical framework predicts several other boundaries deserving of further attention as well. First, we have considered what happens when a prior purchase mistake is corrected—presumably through the process of gaining expertise—in a subsequent purchase. However, if an initial mistake goes uncorrected (as in the case of a reviewer who mentions making not one but two successive mistakes), then a lack in ostensibly gained expertise should result in a lack of reviewer persuasiveness. This might suggest, for instance, a boundary around review valence, as a negative product review would reasonably be more likely to include mention of a (second) mistake than would a positive review. Second, mistaken reviewers may indeed be seen to have gained expertise, but they may no longer be more persuasive if people discount the utility of that expertise. In particular, not all mistakes lead to learning that is relevant for other consumers. For example, suppose a person buys a speaker system and subsequently realizes that the purchase was a mistake because the color of the speakers does not match the person's home decor. That person may well have learned something from that experience, but the expertise gained from this mistake bears no relevance to others whose homes are decorated with different color schemes. If so, admitting to making this kind of subjective, idiosyncratic mistake may not increase a reviewer's persuasive impact because others may recognize the irrelevance of the mistake (and thus the irrelevance of the mistaken reviewer's subsequent expertise) to their own decisions. This would suggest more broadly that our effect should hold more strongly for more global, general reviews of products (but see Study 1).Our theorizing further suggests that if consumers attribute a reviewer's purchase mistake to stable incompetence that cannot be fixed, they will be less likely to infer that expertise has been gained and may thus be less influenced by the mistaken reviewer's product reviews. Accordingly, future research could profit from investigating the factors that affect people's expertise attributions. For example, observers may judge that an admitted mistake is attributable to stable incompetence if the observers perceive that the correct product choice was patently obvious at the time of the mistake. In such cases, observers may infer that the cause of the mistake is most likely to be an absence of basic common sense rather than a fixable lack of knowledge about the product category. Extending the common sense thread, should the content of the review contain not only mention of a mistake but also other content indicative of a lack of expertise (e.g., stating that a newly purchased speaker serves as a fantastic paperweight, using poor grammar), our model would predict that the reviewer's advice would have less impact. The severity of a mistaken outcome may further moderate observers' attributions. In the current studies, the consequences of the admitted mistakes were relatively minor—for example, in Study 1, the reviewer's mistake merely resulted in the purchase of suboptimal headphones. People may perceive that mistakes that cause more severe consequences signal stable incompetence: because people assume that more severe outcomes emanate from larger antecedents ([33]), they may further assume that larger antecedents are more foreseeable and attribute foreseeable mistakes to incompetence. We encourage future research to examine these possibilities.If the incompetent occupy the low end of the expertise spectrum and reviewers who admit to their previous mistakes find themselves situated much higher along that same spectrum, what might determine where other reviewers fall on the basis of their reviews? Our research, in complementing existing work ([41]), attests to the continued importance of addressing this question, as inferred expertise increases the likelihood that consumers will heed the advice of reviews (as either featured by brands or encountered on review repositories). For future research consideration, we underscore one noteworthy domain in which consumers regularly have the opportunity to learn about products: that of purchase decisions made by friends and family. In all of our experiments, reviewers acknowledge their own mistakes on the basis of direct prior experience: they made the purchase, used the product firsthand, became aware of its shortcomings, and gained sufficient expertise to identify this purchase as a mistake and to rectify it in a subsequent purchase.Incorporating the vantage point of mistaken purchases made by others, we identify situations in which consumers might also gain new information. First, a consumer might proceed through all of the steps in the aforementioned sequence, save for making the purchase in the first place. That is, the consumer might borrow a product purchased by a friend and then proceed to use and hold a negative evaluation of it. Would this consumer learn anything? We propose that it depends on the preexisting opinion held by (or at least acknowledged by) the consumer. If the consumer knew all along that their friend's purchase was a dud and using it only verified this opinion, then summarizing this event in a review should not signal any learning on the part of the consumer authoring the review. Such a review might be appraised in a manner similar to the reviewers in our studies who made two successful purchases without any mention of a mistake—as stable, rather than trending upward in expertise—and would observe a degree of persuasiveness akin to theirs. However, the consumer instead might have held high or neutral expectations for the product borrowed from the friend, only to have them dashed after using the product firsthand. Should this type of consumer write a review recapping their experience, we expect that it would convey that learning had taken place (i.e., a knowledge-based update to the reviewing consumer's degree of expertise possible even in the absence of making the mistaken purchase per se), which our results suggest is key in translating the admission of a mistake (be it a mistaken purchase or simply a mistaken opinion) into powerful persuasion.Perhaps reviewers who use products purchased by others and describe how they learned from the experience would be highly persuasive but still not as persuasive as reviewers who made the purchase mistake themselves. This possibility, echoing the aforementioned possibility of a continuum or spectrum of reviewer persuasiveness, might arise as the result of our proposal that the admission of a personal purchase mistake is more costly (and more diagnostic of confident expertise) than the admission of a personal incorrect (favorable) opinion about a product. To be sure, should the friend in this situation (rather than the consumer doing the borrowing) write a review attesting to their own purchase mistake, our results suggest this review would be maximally persuasive. But what if, instead, the friend merely described their experience to the consumer and the consumer then authored a review summarizing the friend's experience—would the reviewing consumer (rather than the friend) have an impact on potential purchasers? We offer that the answer to this question might depend on the degree to which those potential purchasers see the reviewing consumer as socially close to the friend who made the mistake. If the relationship seems distant, then readers might discount the potential for the reviewer to learn from the friend's mistake. But, if the relationship seems close, then readers might believe that the reviewer gained just as much expertise as the mistaken friend ([27]), bolstering the degree to which potential purchasers place stock in the review. Contributions to PracticeThe primary takeaway of our research for practitioners advises the featuring of mistakes to drive more online traffic and, ultimately, more sales. As such, outlets at which online retailers have control to structure the decision environment provide the best point of entry for this recommendation. Though, to be sure, they cannot control the content of online reviews authored by independent consumers, online retailers do have the power to flag certain reviews as ""featured"" or ""highlighted,"" warranting placement ahead of an otherwise long and undifferentiated list of reviews. By identifying one or multiple reviews that mention a previous purchase mistake and bumping them up to the top, online retailers can make online shoppers more likely to see, read, and accept the advice of these reviews that our research suggests prove especially persuasive. However, it is not only online retailers that aspire to put helpful reviews in front of their audience. Review curation websites benefit from persuading customers not toward any one particular course of purchase-related action but, instead, toward feeling that the website itself offers a valuable source of information. If customers believe that sites such as Yelp, TripAdvisor, and Rotten Tomatoes offer a consistent, reliable set of reviews, they return to them more frequently ahead of various purchases, with such increased traffic in turn increasing advertising revenues for curation sites. Broadly speaking, then, any firm or brand in the business of offering helpful, positive advice should feature reviews that mention previous purchase mistakes.As previously noted, companies frequently feature consumer reviews to market their products. Thus, in addition to providing novel insight into the inferences that people make about others' purchase mistakes, this research also has significant practical import because it suggests that marketing managers may strategically omit information that actually increases persuasive influence. In other words, featuring reviews that include purchase mistakes might be a widely underused persuasive tactic: in their attempts to increase their persuasive influence, managers may inadvertently decrease persuasive influence through the missed opportunity of failing to feature mistakes. The present investigation thus offers the clear directive to incorporate more (perhaps any) mention of mistakes when featuring reviews to promote products.We propose that this directive may be especially well-suited to smaller firms with fewer marketing-related resources at their disposal. What the effect documented by the present investigation lacks in magnitude and everyday prevalence, it makes up for in subtlety and ease of implementation. As such, it may help level the playing field between large corporations that can pour significant resources into market research, carefully determining which reviews to feature, and smaller companies that lack such deep market research pockets. These relatively smaller companies may take comfort in (and win sales from) the insight that featuring a review referencing a prior mistake acts as a simple but beneficial tool in shaping consumer preference.Firms are not alone in their desire to persuade, and our findings not only might warrant consideration by the marketing departments of large corporations but also might be brought to bear on any attempt to convince others to take purchase-related advice. Dovetailing with the literature on word of mouth ([ 6]; [ 9]; [37]; [41]), we offer two additional domains well-suited to apply our work. First, friends often give purchase-related advice to each other, driven either by the relatively selfless joy of facilitating a positive purchase experience for their friend (e.g., a great gym) or by more self-interested motives (e.g., a gym that promises a referral bonus). We propose that both objectives should be facilitated by making mention of a previous mistake, extending our work to closer interpersonal relationships.Second, a growing number of individuals have taken to online forums to build personal brands around product reviews and tutorials, as evidenced by the thousands of personal blogs reviewing electronics and YouTube channels demonstrating how to apply makeup. Insofar as these influencers hope to build their personal brands in the form of likes, follows, and mentions, they need to be seen as credible experts. Our model offers the possibility that perhaps they would be more likely to attain their objectives should they incorporate mention of previous mistakes into their content. Aside from the content of the review, they might also persuade others to click on their written and recorded reviews in the first place by including mention of a mistake in the title itself (e.g., ""Learn from my mistake!""). Only after navigating to their review will others read and incorporate the reviewer's purchase-related advice. We note, though, that the decision to follow an influencer on social media in perpetuity results from a confluence of many factors that may or may not overlap with the momentary evaluation of helpfulness and one-time purchase decisions around which the present investigation centered. Still, this application of the phenomenon documented and detailed herein would suggest that inferred expertise, through admission of mistakes, can not only drive sales but also build brand equity writ large. "
34,"Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments Marketers often claim to be part of an exclusive tier (e.g., ""top 10"") within their competitive set. Although recent behavioral research has investigated how consumers respond to rank claims, prior work has focused exclusively on claims having a numerical format. But marketers often communicate rankings using percentages (e.g., ""top 20%""). The present research explores how using a numerical format claim (e.g., ""top 10"" out of 50 products) versus an equivalent percentage format claim (e.g., ""top 20%"" out of 50 products) influences consumer judgments. Across five experiments, the authors find robust evidence of a shift in evaluations whereby consumers respond more favorably to numerical rank claims when set sizes are smaller (i.e., <100) but more favorably to percentage rank claims when set sizes are larger (i.e., >100), even when the claims are mathematically equivalent. They further show that this change in evaluations occurs because consumers commit format neglect when making their evaluations by relying predominantly on the nominal value conveyed in a rank claim and insufficiently accounting for set size.KEYWORDS_SPLITWhen making purchase decisions, consumers often need to sift through a vast amount of information on the many products or services in the marketplace. To facilitate the comparison and evaluation of these different options, numerous third-party entities (e.g., Motor Trend, U.S. News & World Report, Bon Appetit) regularly prepare and disseminate ranked lists of competitive offerings within a particular category. Because they are able to summarize complex information in an easily digestible manner, ranked lists are valued by consumers in a variety of domains. Indeed, prior research has suggested that consumers consult ranked lists for comparative information when choosing a restaurant, hospital, college, hotel, car, book, and so on (e.g., [16]; [18]; [26]; [32]).The value of ranked lists to consumers is not lost on marketers. When a company learns that it has been included in a ranked list, it frequently communicates this information to its customers. As a result, rank claims regularly appear in advertising and other marketing communications. For example, of the new U.S. restaurants that GQ ranked as the ten best new restaurants of 2017, eight reported this honor on their websites or shared it on social media.[ 6] More generally, 78% of companies in the Fortune 100 referenced rank claims on their primary websites (often on the site's landing page or in the newsroom section [all Fortune 100 websites accessed in June 2017]).Rank claims are typically communicated using a numerical format (e.g., ""Product X is in the top 10"") or a percentage format (e.g., ""Product X is in the top 20%""). Both formats are commonly used in practice.[ 7] As an example, Insight Global adopted a numerical format when communicating its inclusion in Comparably's ""Best Places to Work"" list, noting in a recent press release that it was ""a top 20 best place to work in the nation.""[ 8] In contrast, LinkedIn used a percentage format in an email to members whose user profiles were viewed most in 2012, congratulating them for being in the top 1%, 5%, or 10% of the company's 200 million members ([36]). Importantly, communicators must often choose which rank claim format to use. For instance, Hampton Inn annually designates the top 5% (i.e., top 100) of its 2,000 hotel properties as Lighthouse Award winners, and honorees and tourist sites can decide whether to communicate this selection using percentages (e.g., ""top 5% of 2,000 hotels"") or numerical ranks (e.g., ""top 100 of 2,000 hotels"") in their marketing collateral.[ 9]Despite the regular occurrence of both rank claim formats, it remains an open question as to whether consumers will judge an item more favorably if its rank is communicated using one format versus the other. Prior research on rankings has shown that merely being included in a third-party list can enhance brand evaluations, with superior ranks judged more favorably ([16]; [26]) both for goods and services (e.g., [17]; [18]; [22]; [23]; [32]). Yet, in spite of the recent surge of research on rankings, little is known about how consumers judge, interpret, and evaluate different rank claim formats. Specifically, because prior research has focused on only numerical rank claims, it is unclear how percentage rank claims are processed and evaluated relative to numerical ranks. The present research attempts to address this gap in the literature.In addition to introducing the concept of percentage rank claims to the burgeoning literature on ranking effects, we demonstrate when and why numerical and percentage formats might produce different evaluations even when they make mathematically equivalent claims. Specifically, we find that the integration of two pieces of information that are routinely conveyed in rank claims—nominal value (e.g., the number 10 in a ""top 10"" claim or the number 20 in a ""top 20%"" claim) and set size (i.e., the number of items in the ranked set)—determines whether numerical or percentage formats will produce more favorable evaluations.Next, we describe our specific predictions and the theoretical bases for them, followed by five experiments that were designed to test our proposed effect and the underlying process. We conclude with a more detailed discussion of the contributions and implications of this work for both academics and practitioners. Theoretical FrameworkIn this research, we provide evidence for the differential impact of equivalent numerical (e.g., Product X is in the top 10 out of 50 products) versus percentage (e.g., Product X is in the top 20% out of 50 products) rank claim formats on consumer judgments and decisions. According to the principle of description or frame invariance (e.g., [ 2]; [43]), the same position on a ranked list should produce the same evaluation irrespective of which format is used. If this were the case, there should be no difference in how the aforementioned claims (i.e., top 10 vs. top 20% out of 50 products) are evaluated, despite their distinct formats. However, others have shown that the format of information presentation affects judgments ([14]; [15]). For example, [15] document that communicating mathematically equivalent predictions as odds ratios versus percentage probabilities affects risk perceptions.In the context of ranked lists, we posit that the size of the set in which an item is being judged (e.g., out of 50 products) will influence whether a numerical claim or an equivalent percentage claim engenders more favorable evaluations. Specifically, we expect equivalent rank claims that use a numerical (vs. percentage) format to elicit more positive evaluations when the set size is relatively small. However, when the set size is relatively large, we expect evaluations to shift such that percentage (vs. numerical) claims will lead to more positive evaluations. We attribute this shift in evaluations to format neglect, a novel bias in which consumers fail to fully account for claim format because they infer that nominal value is more important to their evaluations than set size (even though both components should be considered in conjunction). As we discuss subsequently, our theorizing enables us to delineate an exact set size that serves as the inflection point for the shift in evaluations between equivalent numerical claims and percentage claims. In addition to demonstrating how and why format neglect influences evaluations, we also demonstrate two ways in which this bias can be eliminated. Next, we summarize the extant literature on ranking effects and discuss why consumers might commit format neglect when making their judgments. Ranking EffectsPrior research on rankings has documented several heuristics that affect consumer evaluations. For example, [18] find that rank improvements resulting in a ranked item entering a new round-number tier, such as the ""top 10"" (e.g., moving from 11 to 10), are viewed more favorably than equivalent rank improvements that do not span tiers (e.g., moving from 12 to 11 or from 10 to 9). This ""top-ten effect"" stems from a subjective categorization process that does not properly account for equivalent changes in rank. In a similar vein, [22] document a ""ranking effect"" in which consumers rely primarily on favorable numerical rankings within a particular list when making product decisions, without fully considering the prominence or status of the ranked list itself. For example, consumers in one study were willing to pay more for the highest-ranked automobile model in a lower-status category (e.g., the Volkswagen Passat V6) than the lowest-ranked model in a higher-status category (e.g., the Audi 4). Taken together, these findings suggest that when processing rank-related claims, consumers' evaluations and/or decisions can be biased because they do not equally consider or accord the same importance to each piece of information that they receive.We extend this stream of research by documenting a novel bias—format neglect—that differentially influences how consumers evaluate rank list claims. If consumers were to fully take claim format into account when making their evaluations, they would need to integrate the nominal value expressed in the claim with set size. We propose that instead of doing this, consumers insufficiently factor in set size and overrely on nominal values when making their evaluation, thereby committing format neglect. In support of our position that consumers insufficiently account for set size, we next discuss a related bias—base rate neglect—that has produced evidence that decision makers often fail to fully utilize information that is both relevant and available to them at the time of their decision. Base Rate NeglectExtensive research suggests that when making judgments, consumers often give greater importance to focal and specific information about a case while neglecting other relevant general information (e.g., [ 6]; [19]; [24]; [28]; [44]). One such class of errors occurs when consumers neglect base rates. Base rate neglect occurs when participants rely more on a salient, individuating feature or characteristic and disregard or discount the more general piece of information about the population as a whole. In a seminal article, [28] showed that when participants were provided the description of a target person (i.e., specific case-related information) along with the proportion of people by profession in the total population (i.e., general base-rate information), judgments were not significantly influenced by the relative incidence of these professions in the population. For example, when a target person was described as ""short, slim, and likes to read poetry,"" participants were likely to predict that the target was a professor rather than a truck driver despite the greater incidence of truck drivers (vs. professors) in the overall population.Consumers' failure to sufficiently account for relevant information has been demonstrated in many contexts. For instance, when making probability judgments, consumers exhibit a variation of base rate neglect called ratio bias or denominator neglect ([35]), in which they pay greater attention to the number of times a target event has occurred and fail to fully consider the number of opportunities for the event to occur. As a result, when evaluating the likelihood of randomly drawing a red jelly bean from a tray, consumers believe their chances to be higher if the tray contains 10 red and 90 white jellybeans versus 1 red and 9 white jelly beans ([30]). This bias occurs because consumers rely too much on the number of red jelly beans (the numerator) and too little on the total number of jelly beans (the denominator) in the tray.We believe that a similar mechanism may be at play when consumers encounter rank claims. Specifically, we anticipate that consumers may insufficiently account for the set size when making evaluations because they believe this information is less important than nominal value, despite the fact that both pieces of information should be considered together to appropriately account for the rank claim's format. Set size may be perceived as less diagnostic to consumers' evaluations because it is less dynamic across items (i.e., set size is the same for each item in a set, whereas nominal value is not) and over time, which makes it appear less focal or specific to the item that is being judged.Although our proposed mechanism of format neglect resembles base rate neglect, it also differs from previous instantiations of base rate neglect in several ways. For example, whereas denominator neglect (e.g., [30]; [35]) constitutes a case of base rate neglect in which both the numerator and the denominator vary simultaneously (1/10 vs. 10/100), format neglect occurs even when the denominator (set size) remains constant and only the numerator (nominal value) and claim format are varied. Furthermore, although prior research has implied that base rate neglect often arises from attentional oversight (e.g., [21]), we propose that format neglect is an evaluation bias. In other words, it is not necessarily the case that consumers fail to notice set size or are unable to correctly recall the set size conveyed in a claim. Rather, our contention is that consumers do not fully consider the implications of set size when producing their evaluations because they think it is relatively unimportant, resulting in format neglect.It is crucial to note that although consumers' failure to insufficiently account for set size when evaluating rank claims might be considered an instantiation of base rate neglect, this by itself is inadequate to develop our hypotheses. In other words, although base rate neglect certainly contributes to format neglect, the two are not synonymous. Take the example of ""top 10 out of 50"" and ""top 20% out of 50,"" which are equivalent numerical and percentage claims. Whereas prior research on base rate neglect might predict that consumers will rely relatively less on set size, the base rate neglect literature is agnostic as to how evaluations of ""top 10"" versus ""top 20%"" might differ when set sizes are equivalent, as may be the case in our research paradigm. Unlike denominator neglect, the values of 10 and 20% are not comparable given their different formats. Because of the inadequacy of base rate neglect alone to inform our prediction that consumers will overrely on nominal values, we draw from prior research exploring how consumers interpret and use percentages. Biases in Evaluating PercentagesWhen evaluating rank claims of different formats, one might expect consumers to rely on processing ease. Because consumers find whole numbers more intuitive than percentages ([20]; [31]), numerical claims may be more cognitively fluent and easier to process than percentage claims. Because fluency generally increases liking ([33]), numerical claims might be expected to produce more positive evaluations than equivalent percentage claims. As such, ""top 10"" and perhaps even ""top 30"" would be preferred over ""top 20%.""We suspect, however, that numerical and percentage rank claims will not differ substantially from each other in terms of processing ease as consumers are likely to be experienced at processing both types of formats. Instead, we predict that consumers will overrely on the nominal value conveyed in a rank claim (e.g., 10 in ""top 10 out of 50"" or 20 in ""top 20% out of 50""). Our prediction is consistent with several known biases involving percentages, in which consumers rely heavily on nominal values while ignoring or underweighting other diagnostic information. For example, [ 5] showed that when evaluating the accuracy of probabilistic forecasts (e.g., 70% vs. 30% chance of occurrence), consumers erroneously judge the estimate with the higher nominal value (70 vs. 30) as being more accurate. Consumers seem to rely chiefly on the nominal value of the forecast and infer that the magnitude of this value corresponds with the forecaster's confidence in his or her prediction.Likewise, extensive research in pricing contexts has shown that consumers overrely on nominal values when encountering percentages. For example, [ 9] found that when offered multiple discounts (e.g., 10% off followed by 20% off), consumers simply add the nominal values, leading to an overestimation (30% vs. 28%). Relatedly, [ 8] demonstrated that when evaluating promotions, consumers prefer a bonus pack over an economically equivalent price discount when both are expressed as percentages. For example, they found that consumers favor a bonus pack quantity increment of 50% over an economically equivalent price discount of 33.3% because they directly compare the nominal values of 33.33 with 50. [20] showed that when subjectively judging the distance between two numbers whose difference is expressed in percentages, consumers are influenced by the nominal values conveyed in the percentages. For example, when comparing the numbers 1,500 and 1,000, consumers judged 1,500 to be much larger than 1,000 when 1,500 was presented as being 50% more than the latter, versus if 1,000 were described as being 33.3% less than the former. Although this result may be partially a function of the different frames (i.e., ""more than"" vs. ""less than""), the authors suggested that participants' overreliance on nominal values (i.e., 50 vs. 33.3) was also a contributing factor. Collectively, this research shows that consumers tend to overrely on nominal values when making percentage-based judgments.Whereas many previously identified biases in the processing of percentage information have been attributed to attentional oversight or calculation complexity (e.g., [ 5]; [ 8]; [10]; [20]), we posit that consumers believe that nominal value is more important to the evaluation task than other information contained in the rank claim. As a result, consumers will overrely on nominal values when making their evaluation. Because smaller nominal values typically indicate superior ranks, we expect consumers to judge an item represented by a smaller nominal value more favorably than an equivalent (or nearly equivalent) item with a larger nominal value. Thus, consumers will react more favorably to claims using smaller nominal values regardless of format. For example, ""top 10"" will be judged as better than an equivalent rank of ""top 20%"" because the number 10 is smaller than 20. Hypothesis DevelopmentBecause numerical ranks and percentage ranks have different mathematical properties, they often require different nominal values to denote an equivalent position in a set. Furthermore, while the nominal value of a numerical rank claim is an absolute measure of an item's position in a set (e.g., top 10 out of 50), it is a relative measure in a percentage claim (e.g., top 20% out of 50). As a result, the same relative position in a set can lead to numerical claims with very different nominal values as a function of set size. For example, when expressed as a percentage claim, a product ranked in the top 20% would have the same nominal value (i.e., 20) irrespective of whether there were 50 or 200 products in its set. However, this product would have a nominal value of 10 or 40 when expressed as a numerical claim, depending on if there were respectively 50 or 200 products in the set.Given that percentages are, by definition, proportions with respect to a set of 100, we propose that a set size of 100 will act as the inflection point for our proposed effect as this is the point where numerical and percentage values converge (e.g., top 20% out of 100 is equivalent to top 20 out of 100). In accordance with format neglect, consumers who rely relatively more on nominal value but relatively less on set size should evaluate numerical and percentage claims similarly when set size is equal to 100.However, when an item is part of a small set with fewer than 100 items, the same objective position in the ranked list—say, 10 out of 50—will have a smaller nominal value when it is expressed numerically (top 10 out of 50) than when a percentage format is used (top 20% out of 50). Given our prior theorizing, we expect a numerical format to elicit more positive evaluations relative to a percentage format when the set size is smaller than 100. Conversely, a different pattern of effects will emerge when the set size is larger than 100. In these cases, the same objective position in the ranked list—say, 10 out of 200—will have a larger nominal value when it is expressed numerically (top 10 out of 200) than when a percentage format is used (top 5% out of 200). Thus, we propose that the effect of claim format on consumer evaluations will depend on set size. Stated formally, we hypothesize the following: H1a:  Consumers evaluate a ranked item that is part of a set of more than 100 items more favorably when it is described with a percentage (vs. numerical) rank claim format. H1b:  Consumers evaluate a ranked item that is part of a set of less than 100 items more favorably when it is described with a numerical (vs. percentage) rank claim format. H1c:  Consumers evaluate a ranked item that is part of a set of exactly 100 items equally favorably when it is described with a numerical or percentage rank claim format.While our H1a–c delineate when consumer evaluations will diverge or converge, H2 explains why these effects occur. As previously discussed, we contend that the underlying mechanism for these shifts in consumer evaluations is format neglect, a novel bias that emerges when consumers encounter rank claims. In essence, format neglect is the net result of consumers relying too much on nominal value and too little on set size. Stated formally: H2:  Consumers rely more (less) on an item's nominal value (set size) when evaluating a rank claim because they consider it more (less) important to their evaluations. Overview of StudiesWe report findings from five experiments that document an interaction between rank claim format and set size on consumer evaluations and demonstrate that this interaction arises because of format neglect. Experiment 1 provides initial support for our main thesis (H1a and H1b) in a laboratory-controlled environment. This study also provides evidence for H2, the existence of format neglect, by showing that participants rely on nominal values more than set sizes when formulating their evaluations. Experiment 2 provides further support in favor of format neglect as the underlying mechanism and corroborates our claim that the inflection point for this effect occurs when set size is 100 (H1c). In Experiments 3 and 4, we identify interventions that can be used to debias consumers. These theoretically derived interventions also provide additional support for our proposed format neglect mechanism (H2) by demonstrating that when the importance of set size on evaluations is highlighted perceptually (Experiment 3) or cognitively (Experiment 4), the effects found in our earlier experiments are attenuated. Finally, Experiment 5 is a field experiment conducted at a cheese shop over a 12-week period. It demonstrates our basic effect in the context of actual purchasing behavior, thereby enhancing the external validity of this work.In Web Appendix A, we report findings from four additional studies that demonstrate further robustness and generalizability of our findings, while highlighting other approaches for debiasing. Experiment 6 shows that even marketing professionals who are likely to have considerable familiarity with rank claims are susceptible to format neglect. Experiments 7 and 8 show that format neglect applies to nonmarketing communications and that claim format can even influence judgments about a student's academic performance. Experiments 8 and 9 provide evidence that debiasing may be possible if participants are forced to consider both percentage and numerical format simultaneously, either by converting one format to the other (Experiment 8) or by jointly evaluating a numerical claim and a percentage claim (Experiment 9). Experiment 1In Experiment 1, we vary set size but equate the relative favorability of an item in the set, irrespective of whether participants encounter a numerical or percentage rank format. If participants do in fact rely more (less) on nominal value (set size) when making their evaluations, we should find that percentage rank claims are favored when set size is large (i.e., > 100), but also that numerical rank claims are favored when set size is small (i.e., < 100). Experiment 1 also aims to provide support for our format neglect mechanism by comparing the extent to which participants consider nominal values and set size when making judgments. MethodExperiment 1 was a lab study conducted with 233 students at a large public university in the U.S. (50.2% female; average age = 20.69 years, SD = 1.34) who participated in exchange for course credit. The study involved a 2 (claim format: numerical rank, percentage rank) × 2 (set size: small, large) between-participants design. Participants read a brief summary of the sales performance of a particular product, GLS, relative to the other products in its category on Amazon. Depending on set size condition, participants were informed that there were 50 products in GLS's product category (small set size) or 500 products (large set size). Participants in the numerical format conditions who were also assigned to the small set size condition learned that GLS was among the top 20 of the 50 products in its category, whereas those assigned to the large set size condition learned that GLS was among the top 200 of the 500 products in its category. Participants in the percentage format conditions learned that GLS was among the top 40% of products in its category irrespective of the set size. For the wording of stimuli in Experiment 1 and all subsequent studies, see Web Appendix B.After reviewing the description of GLS's sales performance, participants indicated how well GLS was performing on an unnumbered sliding scale (0 = ""not very well,"" and 100 = ""very well""). On a new screen, we asked participants to explain in an open-ended text box what information they used to determine how well GLS was performing. Participants advanced to another screen where they stated how much they had considered rank information (i.e., nominal value) and the number of products in GLS's category (i.e., set size) when evaluating GLS's performance (1 = ""not at all,"" and 9 = ""very much""). Participants' answers revealed the relative importance they placed on rank information versus set size when evaluating the product. Afterward, participants proceeded to another screen where they were asked to recall GLS's rank (i.e., its nominal value) and the number of products in its category (i.e., its set size) by entering these numbers in text boxes. Results and DiscussionA 2 (claim format) × 2 (set size) between-participants analysis of variance (ANOVA) on participants' evaluation of GLS's performance revealed a main effect of set size (F( 1, 229) = 4.09, p =.04, ηp2 =.02), such that participants in the small set conditions (M = 61.24, SD = 25.59, N = 118) evaluated GLS more favorably than participants in the large set conditions (M = 54.90, SD = 26.86, N = 115). There was no main effect of claim format (F( 1, 229) = 1.02, p =.31, ηp2 =.004). More germane to our theorizing, we also observed a significant interaction between set size and claim format (F( 1, 229) = 16.76, p <.001, ηp2 =.07). Planned contrasts revealed that among participants in the small set size conditions (i.e., 50 products), those who encountered the numerical (i.e., top 20) claim evaluated GLS more favorably (M = 66.54, SD = 19.52, N = 57) than participants who encountered the mathematically equivalent percentage (i.e., top 40%) claim (M = 56.28, SD = 29.49, N = 61; F( 1, 229) = 4.82, p <.03, ηp2 =.02). Conversely, among participants in the large set size conditions (i.e., 500 products), those who encountered the numerical (i.e., top 200) claim rated GLS's performance lower (M = 46.20, SD = 29.73, N = 56) than participants who encountered the identical percentage (i.e., top 40%) claim (M = 63.17, SD = 20.91, N = 59; F( 1, 229) = 12.85, p <.001, ηp2 =.05). These results appear in Figure 1.Graph: Figure 1. Effectiveness of numerical versus percentage rank claims depends on set size (Experiment 1). Notes: Higher numbers indicate more favorable evaluations. Participants evaluated a product's sales performance more (less) favorably when its performance was described using a numerical rank claim versus an identical percentage rank claim if the product was a member of a small (large) set of products.A mixed ANOVA with self-reported consideration of nominal value and set size measured within-participants revealed a main effect of measure (F( 1, 229) = 83.97, p <.001, ηp2 =.27), which indicates that participants were more likely to consider nominal value (M = 6.77, SD = 1.86) than set size (M = 5.13, SD = 2.46). None of the interactions were significant (all ps >.34). As a complement to this self-reported measure, we examined the rationales provided by participants for the evaluations they had provided. Two independent coders, blind to condition, decided whether each rationale indicated that the participant had considered the product's rank (i.e., the nominal value) during the evaluation process (1 = yes, 0 = no). They also determined whether each rationale indicated that the participant had considered the number of products in GLS's category (i.e., set size) (1 = yes, 0 = no). Coder agreement was an acceptable 84% ([29]; [40]) and differences were resolved by discussion between the coders. Consistent with our hypothesis that consumers rely more on the nominal value in a rank claim when making an evaluation (H2), 57.5% of participants' rationales indicated that they had considered the product's nominal value whereas only 22.3% of rationales indicated that they had considered the product's set size (χ2( 1) = 65.61, p <.001), and this disparity was observed across conditions (all ps <.01).Unlike these measures, recall rates of nominal value and set size were universally high (i.e., >75%) across conditions. Taken together, these results support our proposal that consumer evaluations may be driven by perceived importance and not by recall accuracy. Experiment 2Although Experiment 1 provided support for H1a, H1b, and H2, it did not test whether the inflection point for the observed shift in evaluations between numerical and percentage rank claims occurs when set sizes have 100 items (H1c). Experiment 2 aims to test H1c as well. MethodExperiment 2 was conducted with 784 participants (43.1% female; average age = 35.54 years, SD = 12.11) recruited through Amazon Mechanical Turk (MTurk). Participants were asked to evaluate Brand TFN, a brand whose real name had purportedly been withheld for the purpose of the study. Participants were informed that TFN had been evaluated and ranked by a consulting firm that evaluates and ranks brands across a variety of industries. Subsequently, participants received information about TFN's rank (i.e., its nominal value), using either numerical or percentage format, and the number of brands in its industry (i.e., its set size).There were ten between-participant conditions in Experiment 2. Four of the conditions emulated the design of Experiment 1, in that the actual rank of Brand TFN was equally favorable across these conditions but claim format (i.e., numerical vs. percentage) and set sizes varied (i.e., either greater or less than 100). Those in the numerical format condition learned that TFN had been ranked in the top 10 of 40 brands (small set size) or the top 100 of 400 brands (large set size), whereas those in the percentage format condition learned that TFN had been ranked in the top 25% of 40 (small set size) or 400 brands (large set size). To test our proposition that a set size of 100 serves as an inflection point where consumers are indifferent between equivalent numerical claims and percentage claims, we added two inflection point conditions that had either a numerical format (i.e., top 25 of 100 brands) or a percentage format (i.e., top 25% of 100 brands). Based on the numbers we used, Brand TFN was in the top 25% of brands in its industry across all six of these experimental conditions.So that we could test whether the inflection point of 100 was robust irrespective of claim favorability, we included four more inflection point conditions. The first two of these extra conditions had either a numerical format (i.e., top 10 of 100 brands) or a percentage format (i.e., top 10% of 100 brands) and were superior to the six other claims. The last two conditions had either a numerical format (i.e., top 40 of 100 brands) or a percentage format (i.e., top 40% of 100 brands) and were inferior to the six other claims. In addition to establishing the robustness of 100 as an inflection point, these paired conditions enable us to compare equivalent rank claims in which nominal value and set size remain constant and only format is manipulated. This is useful as it helps us rule out the possibility that rank format may influence consumer evaluations independent of either nominal value or set size.After reviewing rank information about TFN, participants in all conditions were asked to indicate, in their opinion, how well TFN was performing in its industry (1 = ""not very well,"" and 10 = ""very well""), the extent to which TFN was one of the best in its industry (1 = ""not one of the best,"" and 10 = ""one of the best"") and if they would consider buying the brand if they were shopping in that industry (1 = ""would not consider,"" and 10 = ""would consider""). These three questions were combined to form a single evaluation measure (α =.91). Results and DiscussionWe separated our analysis into two main parts. First, we focused on the six conditions that were equally favorable (i.e., top 25%) but in which claim format and set size were varied. Thus, we estimated a 2 (claim format: numerical rank, percentage rank) × 3 (set size: small, inflection, large) between-participants ANOVA on participants' evaluation of TFN performance. There was no main effect of claim format (F( 1, 462) =.027, p >.86, ηp2 <.001). However, results revealed a main effect of set size (F( 2, 462) = 3.81, p =.023, ηp2 =.02), such that participants in the large set conditions (M = 7.36, SD = 2.08, N = 156) evaluated TFN less favorably than both participants in the small set conditions (M = 7.85, SD = 1.78, N = 164; F( 1, 462) = 6.62, p =.01) and participants in the inflection set conditions (M = 7.79, SD = 1.49, N = 148; F( 1, 462) = 4.60, p =.03). Evaluations of participants in the small set conditions and inflection set conditions did not differ significantly (F( 1, 462) =.13, p >.71).More importantly, we also observed a significant interaction between set size and claim format (F( 2, 462) = 4.03, p =.02, ηp2 =.02). Planned contrasts revealed that among participants in the small set size conditions (i.e., 40), those who encountered the numerical (i.e., top 10) claim evaluated TFN more favorably (M = 8.15, SD = 1.76, N = 77) than participants who encountered the equivalent percentage (i.e., top 25%) claim (M = 7.58, SD = 1.76, N = 87); F( 1, 462) = 4.07, p <.05). Conversely, among participants in the large set size conditions (i.e., 400), those who encountered the numerical (i.e., top 100) claim evaluated TFN less favorably (M = 7.06, SD = 2.39, N = 74) than participants who encountered the identical percentage (i.e., top 25%) claim (M = 7.63, SD = 1.72, N = 82); F( 1, 462) = 3.94, p <.05). These results are consistent with the findings of Experiment 1.However, in the two equivalent inflection point conditions (when set size was equal to 100), evaluations provided by participants who encountered the numerical (i.e., top 25) claim (M = 7.75, SD = 1.52, N = 70) did not differ from evaluations provided by participants who saw the identical percentage (i.e., top 25%) claim (M = 7.83, SD = 1.47, N = 78); F( 1, 462) =.07, p >.79). These results appear in Figure 2.Graph: Figure 2. Effectiveness of numerical versus percentage rank claims depends on set size (Experiment 2). Notes: Higher numbers indicate more favorable evaluations. Participants evaluated a brand more (less) favorably if its rank was described using a numerical claim versus an identical percentage claim if the brand was a member of a small (large) set of brands. However, at the inflection set size of 100 brands, participants evaluated the brand the same irrespective of whether its rank was described using a numerical claim or an identical percentage claim.In the second part of our analysis, we again considered the same two inflection point conditions (i.e., top 25 of 100, and top 25% of 100), as well as the four other inflection point conditions. This resulted in three pairs of conditions with set sizes equal to 100 and the same nominal value for the numerical and percentage conditions. The nominal values in these pairs of conditions were 10, 25, and 40, respectively. We estimated a 2 (claim format: numerical rank, percentage rank) × 3 (nominal value: 10, 25, or 40) between-participants ANOVA on the evaluation of TFN. Results revealed a main effect of nominal value (F( 2, 458) = 56.41, p <.001, ηp2 =.20), such that participants in the superior (i.e., 10) nominal value conditions (M = 8.52, SD = 1.68, N = 145) evaluated TFN more favorably than those in the middle (i.e., 25) nominal value conditions (M = 7.79, SD = 1.49, N = 148; F( 2, 458) = 15.28, p <.001) and the inferior (i.e., 40) nominal value conditions (M = 6.34, SD = 2.25, N = 171; F( 2, 458) = 92.39, p <.001). Moreover, participants in the middle nominal value conditions evaluated TFN more positively than those in the inferior nominal value conditions (F( 2, 458) = 44.92, p <.001). There was neither a main effect of claim format (F( 2, 458) =.10, p >.75, ηp2 <.001) nor a significant interaction between nominal value and claim format (F( 2, 458) =.07, p >.93, ηp2 <.001). Together, this suggests that claim format does not affect evaluations when set size is 100.Experiment 2 demonstrates that the effect of rank claim format on consumer evaluations depends on set size but also shows that this effect is absent when set size is equal to 100. Thus, this experiment corroborates our hypothesis (H1c) that a set size of 100 acts as an inflection point for our effect. In the next two experiments, we discuss interventions that can be used to debias consumers even when set sizes are smaller or larger than 100. Experiment 3Whereas we have examined identical numerical rank and percentage rank claims in our experiments thus far, in Experiment 3 we test whether inferior numerical rank claims are preferred over superior percentage rank claims when set sizes are small. In addition, as we have demonstrated in multiple contexts how set sizes influence the assessments of rank claims as a function of format, hereinafter we focus on either a small or large set. This enables us to provide more focused and nuanced support for our theorizing. In Experiment 3, we use a small set, and in Experiment 4 we use a large set.Prior research has suggested that perceptual cues can change the perceived importance of information, which in turn affects consumers' reliance on this information when making judgments. For example, [27] found that in situations in which both numbers and units are presented together, highlighting numbers (units) perceptually leads to greater reliance on one versus the other in judgments. Likewise, in the context of risk perception, Stone and colleagues ([38]; [39]) showed that when graphical formats are used to convey ratio information, highlighting the numerator or the denominator increases consumers' reliance on this information. If our theorizing is correct, then making set size seem more important through its visual presentation is likely to increase consumers' reliance on set size, which should mitigate reliance on the nominal value information and thereby attenuate format neglect. Therefore, in Experiment 3, participants encounter inferior numerical rank claims and/or superior percentage claims, but we manipulate set size importance by varying whether set size information is underlined, bolded, and presented in a larger and different-colored font than rank information. MethodWe conducted Experiment 3 with 300 U.S. participants (54.3% female; average age = 35.29 years, SD = 9.35) recruited using MTurk. The study involved a 2 (claim format: numerical rank, percentage rank) × 2 (importance of set size information: high, low) between-participants design. Participants were asked to read a brief advertisement for a (fictional) library, Midtown Library. Participants were informed that Midtown Library was ranked in either the top 10 (numerical rank claim) or the top 30% (percentage rank claim) of the 20 libraries in its greater metropolitan area by Interlibrary Magazine. Unlike our previous studies, these claims were nonequivalent; being in the top 10 (out of 20) is an inferior claim (top 50%) relative to the percentage claim of being in the top 30% (in top 6).In conditions with low set size importance, rank information was underlined, bolded, and presented in a red font that was larger than the other text in the claim. However, in those conditions in which set size importance was high, set size information (instead of rank information) was underlined, bolded, and presented in a larger red font.After reviewing the advertising claim for Midtown Library, participants were asked two questions that served as our key dependent variables. Specifically, they were asked to indicate, in their opinion, how well Midtown had performed (1 = ""not very well,"" and 11 = ""very well"") and how well it is likely to perform in the future (1 = ""not very well,"" and 11 = ""very well""). These two questions were combined to form a single evaluation measure (r =.83).An assumption of this study is that underlining, bolding, and presenting a portion of text in a larger, different-colored font increases its perceived importance. To confirm this, we asked participants (on a new screen toward the end of the study) to rate the extent that performing each of these actions (underlining, bolding, using a different font color, and increasing the font size) affects the perceived importance of a portion of text (1 = ""decreases perceived importance,"" 5 = ""does not affect perceived importance,"" and 9 = ""increases perceived importance""). After combining the four items into a composite measure (α =.87), we found that the mean perceived importance rating (M = 7.32, SD = 1.35, N = 300) was significantly higher than the scale midpoint (t(299) = 29.85, p <.001). This confirms that our visual presentation manipulation increased the perceived importance of the target item.On a new screen, participants were asked to recall Midtown Library's rank (i.e., its nominal value) and the number of libraries in its area (i.e., its set size) by entering these numbers in text boxes. Finally, participants were asked to indicate the extent to which they considered themselves a library expert on a nine-point scale (1 = ""not at all,"" and 9 = ""very much""). Inclusion of self-reported library expertise as a covariate had no impact on our results; therefore, we will not discuss it further. Results and DiscussionA 2 (claim format) × 2 (set size importance) between-participants ANOVA on the composite measure revealed neither a main effect of claim format (F( 1, 296) = 1.85, p > 18, ηp2 <.01) nor set size importance (F( 1, 296) =.93, p >.33, ηp2 <.01). However, we observed a significant interaction between claim format and set size importance (F( 1, 296) = 4.21, p =.04, ηp2 =.01). Planned contrasts revealed that when set size importance was low, those who encountered the numerical rank (i.e., top 10) claim evaluated Midtown Library more favorably (M = 8.78, SD = 1.88, N = 67) than those who encountered the objectively superior percentage rank (i.e., top 30%) claim (M = 8.06, SD = 1.97, N = 79); (F( 1, 296) = 5.65, p <.02). However, when set size importance was high, those who encountered the numerical rank (i.e., top 10) claim evaluated Midtown Library no differently (M = 8.15, SD = 1.71, N = 75) than those who encountered the objectively superior percentage rank (i.e., top 30%) claim (M = 8.29, SD = 1.66, N = 79); F( 1, 296) =.24, p >.62. These results appear in Figure 3. As in our previous studies, recall rates of nominal value and set size were universally high (i.e., >75%) across conditions.Graph: Figure 3. Set size importance attenuates format neglect n small sets (Experiment 3). Notes: Higher numbers indicate more favorable evaluations. When set size importance was low, participants evaluated a library more favorably if it used an inferior numerical rank claim versus a superior percentage rank claim. However, this bias was eliminated when set size importance was high. In all cases, the library was part of a small group of libraries (set size = 20). Experiment 4In Experiment 4, we manipulate set size importance by asking participants to type in the set size that they had been shown before providing an evaluation. If our theorizing is correct, cognitively reinforcing set size in this way will communicate to participants the importance of set size and should attenuate any differences we observe in the evaluations of those who encounter a numerical versus percentage rank claim. Because Experiment 3 focused on a small set context, in Experiment 4 we test the effectiveness of this proposed debiasing intervention in the context of a large set size. We also directly measure the perceived importance of nominal value and set size. We predict that the perceived importance of nominal value will exceed that of set size only when set size importance is low. MethodWe conducted Experiment 4 with 362 U.S. participants (45.6% female; average age = 35.36 years, SD = 11.17) recruited using MTurk. The study involved a 2 (claim format: numerical rank, percentage rank) × 2 (relative importance of set size information: high, low) between-participants design. Participants were informed that they were deciding whether to invest in the Bantam mutual fund and had consulted the website FundTracker.com, which ranks mutual funds. Those randomly assigned to the numerical rank condition learned that the Bantam Fund had been ranked as one of the top 150 mutual funds in its class. Participants in the percentage rank condition learned that the Bantam Fund had been ranked as one of the top 30% of mutual funds in its class. In addition, all participants were given identical information about set size (""Number of mutual funds in the Bantam Fund's class: 500"").Participants in the high-set-size-importance condition were asked to review the information they had been provided and to enter (in a text box) the number of mutual funds in Bantam Fund's class. Those in the low-set-size-importance condition were not asked to input set size information into a text box. We expected that the act of entering set size would cognitively reinforce set size and increase importance accorded to it.Subsequently, all participants evaluated the Bantam Fund by responding to three ten-point items (1 = ""not likely to be a good investment/not likely to generate a positive return/not likely to consider purchasing,"" and 10 = ""likely to be a good investment/likely to generate a positive return/likely to consider purchasing""), which served as our key dependent variables. These three items were combined to form a single evaluation measure (α =.89).Finally, participants were asked to recall the Bantam Fund's rank (i.e., its nominal value) and the number of mutual funds in its class (i.e., its set size) by entering these numbers in text boxes. Participants advanced to another screen where they indicated how much they had relied on rank information (i.e., nominal value) and the number of other mutual funds in Bantam's class (i.e., set size) when evaluating the Bantam Fund (1 = ""not at all,"" and 7 = ""very much""). This question enabled us to directly assess whether the relative importance of nominal value versus set size differed across conditions. Results and DiscussionA 2 (claim format) × 2 (set size importance) between-participants ANOVA on the composite evaluation measure revealed a main effect of claim format (F( 1, 358) = 4.08, p =.044, ηp2 =.01) but no main effect of set size importance (F( 1, 358) =.04, p >.83, ηp2 <.01). More germane to our theorizing, we observed a significant interaction between claim format and set size importance (F( 1, 358) = 9.02, p <.01, ηp2 =.03).Planned contrasts revealed that when set size importance was relatively low, those who encountered the percentage rank (i.e., top 30%) claim evaluated the Bantam Fund more favorably (M = 8.09, SD = 1.43, N = 89) than those who encountered the equivalent numerical rank (i.e., top 150) claim (M = 7.11, SD = 2.22, N = 92); (F( 1, 358) = 12.65, p <.001, ηp2 =.03). However, when set size importance was high, those who encountered the percentage rank (i.e., top 30%) claim evaluated the Bantam Fund no differently (M = 7.47, SD = 1.90, N = 86) than those who encountered the numerical rank (i.e., top 150) claim (M = 7.66, SD = 1.76, N = 95; F( 1, 358) =.48, p >.48, ηp2 <.01). These results appear in Figure 4.Graph: Figure 4. Set size importance attenuates format neglect in large sets (Experiment 4). Notes: Higher numbers indicate more favorable evaluations. When set size importance was low, participants evaluated a mutual fund more favorably if it used a percentage rank claim versus an equivalent numerical rank claim. However, this bias was eliminated when set size importance was high. In all cases, the mutual funds were part of a large group of mutual funds (set size = 500).We then conducted a 2 (importance measure: rank, set size) × 2 (claim format: numerical rank, percentage rank) × 2 (set size importance: low, high) mixed ANOVA, with self-reported importance of nominal value versus set size measured within-participants. We detected a main effect of measure (F( 1, 358) = 15.39, p <.001, ηp2 =.04), which indicates that participants considered nominal value (M = 5.82, SD = 1.34) to be more important than set size (M = 5.46, SD = 1.48). There was also a significant interaction between importance measure and set size importance (F( 1, 358) = 31.61, p <.001, ηp2 =.08). Participants in the low-set-size importance condition considered nominal value (M = 6.06, SD = 1.07, N = 181) to be more important than set size (M = 5.17, SD = 1.60; F( 1, 358) = 45.61, p <.001, ηp2 =.11). However, participants in the high-set-size importance condition considered nominal value (M = 5.58, SD = 1.53, N = 181) and set size (M = 5.74, SD = 1.30) to be equally important (F( 1, 358) = 1.44, p >.23, ηp2 <.01. No other main effects or interactions were significant (all ps >.10). These results give us confidence that our cognitive reinforcement manipulation affected perceived importance in the hypothesized direction. Unlike ratings of perceived importance, recall rates of nominal value and set size were again universally high (i.e., >75%). Experiment 5Our first four experiments provided extensive support for our proposed effect and the underlying format neglect mechanism. Experiment 5 tests whether the use of a numerical rank claim versus an equivalent percentage rank claim affects actual purchase behavior. In this field experiment, a product that is part of a large set (i.e., >100) is described using a numerical rank claim or a percentage rank claim. We predicted that real shoppers will purchase the product more often when it is described using a percentage rank claim versus an equivalent numerical rank claim. MethodExperiment 5 was conducted at a cheese shop (Beecher's Handmade Cheese) in a popular tourist location (Pike Place Market) of a large U.S. city (Seattle). At the store's walk-up counter, patrons can order prepared foods (e.g., macaroni and cheese) and/or purchase wrapped cheese blocks from a large display case containing 67 different cheeses. Next to each cheese in the display case is a 4.5-inch by 2.5-inch sign that states the cheese's name and includes other descriptive information (e.g., flavor, production process). The shop's management team allowed us to conduct a 12-week field experiment in which we varied the information provided in the display case about a particular Camembert-style cheese known as Cirrus. Cirrus was chosen for this experiment because it had recently received an award for technical excellence and aesthetic quality at the 2017 American Cheese Society competition that could accommodate our desired rank format manipulation.The standard Cirrus sign in the store's display case (the control condition in our study) contained information about how and where the cheese was produced, along with the cheese's retail price ($9.95 per block). We created two new versions of the Cirrus sign containing numerical or percentage rank claims that were equivalent and truthful. The numerical rank claim stated, ""Of the 2,024 entrants in the 2017 American Cheese Society competition, only 411 were selected to receive awards for technical excellence and aesthetic quality. We're proud that Cirrus was one of the 411 cheeses to receive an award."" The percentage rank claim stated, ""Of the 2,024 entrants in the 2017 American Cheese Society competition, only 20% were selected to receive awards for technical excellence and aesthetic quality. We're proud that Cirrus was one of the 20% of cheeses to receive an award.""This field experiment was conducted over 12 consecutive weeks between July and October 2017. For the first two weeks and the last two weeks of the experiment, the control sign (without any American Cheese Society rank information) was displayed. For the eight weeks in between, we rotated the numerical rank signs and percentage rank signs on Sundays, prior to the store opening, on a predetermined weekly or bi-weekly basis. In total, the no rank (control) sign, numerical rank sign, and percentage rank sign were each displayed for 28 days (i.e., four weeks). Unit sales of Cirrus, as well as overall sales of all cheeses in the display stand (for use as a potential covariate), were tracked on a daily basis. Results and DiscussionWe analyzed the field data from Experiment 5 in two different ways, with and without a log transformation, and obtained similar results. Because these data were not positively skewed (skewness =.92), we use and report the untransformed measure (values of skewness between −2 and +2 are considered normal; [13]). A one-way ANOVA of experimental condition (no rank, numerical rank, percentage rank) on the daily unit sales of Cirrus cheese returned a significant result (F( 2, 81) = 9.71, p <.001, ηp2 =.19). Compared with the no-rank condition (M = 2.39 units, SD = 1.73), significantly more Cirrus cheese blocks were sold per day when either the percentage rank (i.e., top 20%) sign (F( 1, 81) = 19.41, p <.001) or the numerical rank (i.e., top 411) sign was displayed (F( 1, 81) = 4.71, p <.04). More germane to our theorizing, however, significantly more Cirrus cheese blocks were sold on the days when the percentage rank sign was displayed (M = 4.86 units, SD = 2.49), as compared with the numerical rank sign (M = 3.61 units, SD = 1.99; F( 1, 81) = 5.00, p <.03).In addition, we tested whether an equal proportion of total Cirrus cheese blocks was sold on the percentage-rank days compared with the numerical-rank days. In contrast, we found that a total of 136 Cirrus cheese blocks were sold on the 28 days when a percentage rank sign was displayed but only 101 Cirrus cheese blocks were sold on the 28 days when a numerical rank sign was displayed. These proportions (57.4% vs. 42.6%, respectively) differ significantly from 50% (χ2( 1) = 5.17, p =.023).Finally, to test whether our results were observed even after partitioning out daily variation in store traffic, we conducted an analysis of covariance (ANCOVA) of experimental condition on the daily dollar sales of Cirrus cheese, with the daily dollar sales of all other cheeses included as a covariate. We reasoned that daily variation in store traffic would be captured by differences in overall cheese sales per day. This ANCOVA returned a significant effect of experimental condition on Cirrus dollar sales (F( 2, 80) = 10.11, p <.001, ηp2 =.20). The covariate (i.e., dollar sales of all other cheeses) was also a significant predictor of Cirrus dollar sales (F( 1, 80) = 9.95, p <.01, ηp2 =.11). Compared with the no-rank condition, significantly more Cirrus cheese blocks were sold per day when either the percentage rank (i.e., top 20%) sign (F( 1, 80) = 20.08, p <.001) or the numerical rank (i.e., top 411) sign was displayed (F( 1, 80) = 6.42, p <.02). Furthermore, more Cirrus cheese blocks were sold on the days when the percentage rank sign was displayed as compared with the numerical rank sign (F( 1, 80) = 3.71, p <.06).The results of Experiment 5 indicate that equivalent numerical versus percentage rank claims influence consumers' actual purchase behavior. We found that consumers were more likely to purchase a product that is part of a large set when it was described using a percentage claim. Although this experiment represents an extreme test of our proposed effect given the large difference in nominal values between conditions (i.e., 20 vs. 411), it is important to note that these numbers were factually accurate and were obtained from the 2017 American Cheese Society competition. Our results suggest that consumers utilize available rank claim information to make real purchase decisions, and that equivalent numerical versus percentage rank claims can differentially affect consumers in a manner consistent with format neglect. Thus, we believe that Experiment 5 provides a useful demonstration of the external validity of this research. General DiscussionIn this research, we investigate how the claim format used to convey an item's position on a ranked list influences consumer evaluations. Although recent research examining consumer response to rankings and ranked list claims has identified several psychological factors that influence consumers' evaluation of a ranked item (e.g., [17]; [18]; [22]), it is unclear whether equivalent claims using a numerical (e.g., ""top 10,"" ""top 20"") versus percentage (e.g. ""top 3%,"" ""top 10%"") format will be evaluated differently. Given that both claim formats are frequently observed in marketing communications, understanding how different rank claim formats influence consumer evaluations is theoretically and managerially consequential.Across nine experiments (including those reported in Web Appendix A) that span multiple settings and contexts (including a field study), we find converging evidence of a shift in evaluations whereby consumers respond more favorably to numerical rank claims when set sizes are smaller (i.e., <100) but more favorably to percentage rank claims when set sizes are larger (i.e., >100), even when the claims are mathematically equivalent. To better assess the magnitude and robustness of this effect, we conducted a meta-analysis of claim format and set size on consumer evaluations across our experiments, using a statistical tool developed by [25]) for single-paper meta-analyses. For Experiments 1–5, the meta-analysis revealed significant contrasts when set sizes were large (estimate =.83, SE =.20; z = 4.21, p <.001) or small (estimate =.94, SE =.21; z = 4.50, p <.001), as well as a significant interaction (estimate = 1.77, SE =.29; z = 6.16, p <.001). A second meta-analysis that included the four experiments reported in the Web Appendix A revealed similarly strong effects. See Web Appendix D for additional detail on these meta-analyses.Despite the robustness of our findings, some caveats are in order. While both numerical and percentage ranks are used frequently, ranked lists are not available for all categories of products and services, which may limit the utility of our findings. For example, ranked lists are more popular in certain contexts (e.g., restaurants, hotels, travel locations, cities to live in or visit, universities to attend, firms to work for) and for certain offerings (e.g., cars, laptops). Furthermore, although we focus exclusively on rank claims in this research, consumers are likely to use other information to assess the relative performance of products and services. For example, they may use consumer or expert reviews or ratings in conjunction with or in place of rankings to guide their decision making. These alternative sources may limit the situations in which our findings can be directly applied. In addition, although we examined many different small set sizes (i.e., 20, 40, and 50 items) and large set sizes (i.e., 200, 300, 400, 500, 2,024) in the nine studies described in this article and Web Appendix A, we did not systematically examine set sizes that were close to the inflection point of 100 (e.g., 75 vs. 125 items). Thus, we cannot state definitively how consumers will respond to such set sizes. Finally, there may be discontinuities at certain ranks where an interaction between set size and claim format does not occur. For example, it may be the case that the numerical rank of 1 is always judged as being superior to its percentage rank counterpart, irrespective of set size, because being in the first position on a list holds special significance that attenuates our effect. Future research might investigate such moderators. Theoretical ContributionsAt a theoretical level, the present research makes four discrete contributions. First, whereas existing research has focused exclusively on numerical rank claims, we introduce the concept of percentage rank claims to the academic literature. Second, we elucidate how different rank claim formats (i.e., numerical vs. percentage claims) influence consumer evaluations. Our experiments indicate that even when an item's objective rank is unchanged, the use of a percentage claim format versus a numerical claim format can dramatically alter consumer evaluations. Furthermore, even inferior ranks may be evaluated more favorably relative to superior ranks depending on how they are communicated. For example, in Experiment 3, we found that a library ranked among the top 10 out of 20 libraries was evaluated more positively than a library ranked in the top 30% although the latter is objectively superior (top 6). Third, we uncover a novel bias—format neglect—that explains the shift in evaluations that we observe. We show that format neglect results from two related biases that occur when consumers evaluate rank claims: ( 1) the insufficient utilization of set size information and ( 2) the overreliance on nominal values that convey an item's rank. Fourth, by bridging the literatures on base rate neglect and the processing of percentage information, we provide insights on how consumers integrate nominal value and set size information in their evaluations and document consequences of this integration process. Next, we discuss how format neglect relates to and differs from base rate neglect.Prior research has identified conditions under which consumers underweight general, base-rate information when making predictions (e.g., [19]; [28]). We extend this research and demonstrate that a phenomenon akin to base rate neglect—the underutilization of set size—emerges in ranking contexts. Unlike previous work on base rate neglect, in which the bias occurs because consumers neglect base rates of different magnitudes and instead treat them as if they were equivalent (e.g., [ 6]; [19]; [24]; [28]), we find that consumers make inconsistent judgments related to rank claims even when base rates (i.e., set sizes) are the same. For example, we show that consumers differentially evaluate an item ranked in the ""top 10"" versus the ""top 20%"" out of 50, in spite of the base rate (set size) being identical (i.e., 50) across claims. While prior research on base rate neglect might have anticipated that consumers would rely relatively less on set size when encountering a rank claim, this literature is agnostic as to whether evaluations of the ""top 10"" versus ""top 20%"" claims in this example would differ.The second bias, which states that consumers will overrely on nominal values when making evaluations, is also a critical component of format neglect. According to this bias, ""top 10"" will be evaluated more favorably than ""top 20%"" because ""10"" is a smaller number, and small numbers are typically more favorable in the contexts of rankings. However, when set sizes are larger (e.g., 500), overreliance on nominal values predicts the opposite—a percentage claim should be judged more favorably. This is because a percentage rank claim of top 20% would correspond to a numerical rank of top 100, and ""20"" is a smaller number. Of course, this prediction is contingent on consumers insufficiently accounting for set size when making their evaluation. Thus, both the underutilization of set size and the overreliance on nominal values are needed for format neglect to emerge.Our research not only explains how format neglect (i.e., the overreliance on nominal values and the underutilization of set size) causes this shift in evaluations but also reveals when such a shift is less likely to occur. As we demonstrate, the inflection point for the shift in evaluations we observe is a set size of 100 items. For set sizes smaller than 100, using a numerical rank elicits more positive evaluations compared to an equivalent percentage rank; however, for set sizes larger than 100, a percentage rank elicits more positive evaluations.Our work also enriches knowledge in the area of numerical cognition by identifying a new context in which consumers process relatively complex forms of numerical information incompletely or inaccurately (e.g., [ 1]; [ 3], [ 4]; [ 5]; [ 8]; [ 9]; [11]; [12]; [20]; [41]; [42]). Extending prior research, which has mostly been limited to the areas of probabilities (e.g., [ 1]; [ 5]; [14]), ratios (e.g. [12]; [41]) and price discounts (e.g., [ 7]; [ 8]; [ 9]; [11]; [20]; [42]), we provide the first demonstration that consumers may be biased in their evaluations of rank claims as a result of the claim format employed. Our results show that consumers' overreliance on nominal values when encountering percentage information emerges in a context unrelated to probability or pricing.Future work might study whether other contexts are similarly susceptible to the effects that we observe in rank claims. For example, consider research on food and restrained eating (e.g., [34]; [37]). The same information (e.g., calorie intake) can be communicated with numerical values (500 calories) or percentages (25% of daily intake, assuming a recommended daily intake of 2,000 calories). It may be the case that presenting this information in one format versus another leads to different inferences. For example, if consumers overrely on nominal values when making magnitude judgments, 500 calories may be perceived as much larger. This effect might reverse for other nutrients, for which the recommended daily intake may be lower (e.g., Vitamin C has a recommended daily intake of 80 milligrams). In addition, it may be the case that prevention versus promotion orientations or gain versus loss frames are more compatible with a particular format. We leave all of this for future research to examine. Managerial and Public Policy ImplicationsThe present work has important implications in the managerial and public policy realms as well. Given the abundance of information to which we are exposed in our daily lives, we are often unable to process it fully. Ranked lists are useful because they enable consumers to efficiently compare consumption options (e.g., top televisions, best cars). This research shows that relatively subtle features of rank list claims, such as claim format and set size, may bias how we evaluate items referenced in rank claims.Of course, firms often aim to use rankings as a vehicle to promote the positive aspects of their products and services. This research illustrates that certain rank claim formats may at times help firms depict their products in a more favorable light depending on the set size of the item being promoted. Our findings suggest that marketers may potentially boost how consumers evaluate their products and services by merely selecting the appropriate rank claim format for a particular set size. Firms should abide by the following clear and prescriptive managerial guidelines if their goal is to maximize consumer evaluations—when set size is smaller than 100, use nominal rank claims, but when set size is greater than 100, use percentage rank claims. The effects documented in this research can be implemented by marketers in any communications that reference the rank of their products or services, including their advertising, company website, or social media.In addition, marketers may sometimes have the ability to alter the set size used in their claims (e.g., top Chinese restaurants in SoHo vs. top Chinese restaurants in Manhattan). Given the underutilization of set size information that we observed, it may be advantageous for marketers to focus on narrow (vs. broad) sets in their communications, because nominal values (irrespective of format) are typically smaller (i.e., the rank will be superior) for narrow sets. Future research might consider this possibility.A more charitable application of our results is that firms, public policy makers, and other entities can become better equipped to objectively convey rank information to consumers. Given the widespread use of ranked lists, as well as consumers' dependence on ranked lists to make decisions, it is important to educate consumers on how to avoid potential pitfalls in evaluating information about rankings and to draw their attention to those ranking aspects that may unknowingly be underemphasized in the evaluation process. For example, if consumers become aware of how different rank formats influence their judgments, they may be able to devise strategies to avoid falling prey to format neglect. Indeed, we identify and test several debiasing interventions that consumers and/or policy makers might implement to reduce the impact of claim format.It is worth mentioning that, in all our experiments, we provide respondents full information about the rank claims (i.e., the nominal value and set size) and still obtain robust support for format neglect. However, marketers are not required to reveal all this information in their claims—for example, they may only provide nominal value information bereft of the set size it is drawn from. Doing so may exacerbate the effects of format neglect. Indeed, because managers can strategically decide which format to use to showcase their offerings and may have an incentive to use approaches that present their product in a seemingly better light, it is important for them to be aware of the potential ethical repercussions of engaging in such practices given the robustness of this bias. Public policy makers should also be aware that format neglect emerges even when full information about nominal value and set size is presented; thus, the use of interventions to debias consumers such as those documented in the present research may be beneficial for consumers.More broadly, numerical and percentage formats are often used interchangeably outside of ranking contexts. Thus, our findings may be utilized by public policy watchdogs to illustrate and warn consumers about the fallacy of description or frame invariance. Increasing consumers' awareness of format neglect could potentially improve their decision making not only when evaluating rank claims but also when judging other types of information (e.g., nutritional information, financial information) that are commonly represented by both numerical and percentage formats. "
35,"Full Disclosure: How Smartphones Enhance Consumer Self-Disclosure Results from three large-scale field studies and two controlled experiments show that consumers tend to be more self-disclosing when generating content on their smartphone versus personal computer. This tendency is found in a wide range of domains including social media posts, online restaurant reviews, open-ended survey responses, and compliance with requests for personal information in web advertisements. The authors show that this increased willingness to self-disclose on one's smartphone arises from the psychological effects of two distinguishing properties of the device: ( 1) feelings of comfort that many associate with their smartphone and ( 2) a tendency to narrowly focus attention on the disclosure task at hand due to the relative difficulty of generating content on the smaller device. The enhancing effect of smartphones on self-disclosure yields several important marketing implications, including the creation of content that is perceived as more persuasive by outside readers. The authors explore implications for how these findings can be strategically leveraged by managers, including how they may generalize to other emerging technologies.KEYWORDS_SPLITAmong the many recent changes in consumer markets, two trends have been particularly transformative. The first is the emergence of online communication as a central medium through which firms and customers interact. This medium has yielded a wealth of textual data including social media posts, online reviews, and chats that can provide firms with real-time insights into customer opinions, needs, and preferences (e.g., [64]). The second trend is the emergence of the smartphone as the dominant platform on which these communications take place. Whereas online activities were once limited to at-home or in-office sessions on personal computers (PCs), with smartphones these activities can now occur at virtually any time and place. As a consequence of these two trends, when firms analyze user-generated content today, it is increasingly likely that it was created by consumers on their smartphone rather than their PC.In this article, we explore a question that lies at the intersection of these two trends: As consumers continue to generate content on their smartphone, might this shift be altering what consumers share about themselves—and thus what firms can learn about their customers? Across three field studies (and three replication studies) examining thousands of customer-generated posts from various contexts—as well as two preregistered experiments—we provide evidence that content created by consumers on their smartphones tends to be more self-disclosing than that created on PCs. We show, for example, that social media posts and customer reviews written on smartphones tend to be composed in a more personal, intimate linguistic style, and that consumers are more willing to admit certain types of personal information when using their smartphone, such as experiences with products that are private or embarrassing. This effect is robust across different measures (human judgments, automated measures) and different forms of disclosure (e.g., open-ended survey responses, online reviews, compliance rates with calls to action [CTAs] in web ads). Importantly, this effect has significant marketing implications; for example, the more personal and intimate nature of smartphone-generated reviews results in content that is more persuasive to outside readers, in turn heightening purchase intentions.We also investigate the mechanisms that underlie the observed differences in disclosure, demonstrating that the greater tendency to self-disclose on smartphones arises from the combination of two factors unique to the device. The first is that the highly personal nature of smartphones—resulting from both their constant accessibility and frequent use for personal or intimate activities (e.g., texting with family and friends)—fosters distinct feelings of psychological comfort on the device that facilitate users' willingness to self-disclose. Second, the difficulty of creating content on the smaller form of the device (screen and keyboard) leads consumers to narrow their attentional focus to the task at hand, which also facilitates disclosure. Theoretical Background What Factors Enhance Willingness to Self-Disclose?The study of self-disclosure is among the oldest in the social sciences, spanning the fields of psychology (e.g., [18]; [21]), human–computer interaction (e.g., [35]; [62]), and survey research (e.g., [27]; [65]). This topic is of growing interest to marketing researchers, who have explored how consumers' willingness to provide personal information over a computer is affected by factors such as the nature of the web interface (e.g., [31]), online privacy policies ([ 2]), and the degree of reciprocity in an interaction with an agent ([43]). Here, we follow [ 1] to define ""self-disclosure"" as the voluntary communication of feelings, thoughts, or other information deemed to be private and that might make the discloser feel vulnerable (see also [18]; [46]). For example, disclosure might involve expressing one's candid feelings about a service experience or admitting to incriminating consumption behaviors.A primary focus of research in this area has been to identify situational factors that drive people's willingness to self-disclose. For example, while people tend to be inherently protective of their private feelings and thoughts, they are more willing to share personal information when they feel a greater sense of privacy in their environment (e.g., [20]; [22]; [31]) or if they perceive their particular audience as more anonymous (e.g., [34]; [57]; [61]). Likewise, the degree of psychological comfort evoked by a context can drive self-disclosure. Therapists, for example, find that patients tend to be more self-disclosing in physical environments that foster feelings of security and familiarity, or when they feel more at ease with a counterpart in conversation (e.g., [13]; [25]). The Role of Modality in Self-DisclosureRelevant to our work is research examining how different communication modalities affect people's willingness to self-disclose, particularly in computer-mediated versus face-to-face environments (e.g., [ 8]; [32]; [35]; [62]; [65]). A consistent finding in this literature is that people are often willing to disclose more about themselves when communicating over a computer (e.g., email, instant messaging) than in person (e.g., [ 8]; [53]; [62]). While these effects have been discussed both in terms of depth of disclosure—the sensitivity of what people reveal—and breadth, or the amount revealed, a recent meta-analysis found that communicating through a computer (vs. face-to-face) has a larger effect on depth of disclosure ([53]). That is, while people may not necessarily disclose more when communicating through a computer compared to face-to-face, what they do disclose tends to be more intimate.Although a few explanations have been proposed for why people might disclose more through computers than in person (e.g., [35]), accounts generally point to the comparative anonymity of interacting through computer screens (e.g., [62]). Specifically, when people interact in person they receive a wealth of social cues that make them more concerned about how they come across to others ([55]). This concern about others' reactions then works to reduce willingness to disclose personal or intimate information in face-to-face interaction (e.g., [11]). When one expresses oneself through a computer, however, these social cues are less salient because of the physical distance or isolation of one's audience, which can increase willingness to disclose (e.g., [35]; [54]; [65]).While there is consistent evidence that consumers often disclose information that is more intimate or sensitive when communicating over computer-mediated interfaces relative to face-to-face, it is less clear whether depth of disclosure is influenced by the computing interface itself—specifically, smartphones versus PCs. For example, communication on both types of device involves interaction through a screen, suggesting that the salience of social cues—and in turn, users' willingness to disclose—might be similar across devices. Indeed, some limited support for this invariance has been provided by [ 3], [39], and [58], who found that when sensitive questions were first posed on a mobile device and then later on a PC (and vice versa), respondents showed high test–retest reliability, suggesting that device effects, if they exist, may be small. However, there have been no prior attempts to examine whether the use of smartphones (vs. PCs) affects degrees of self-disclosure in more complex real-world settings, such as when consumers post on social media, write reviews, or respond to open-ended questions on surveys. How Smartphones Enhance Self-Disclosure Relative to PCsIn this work, we hypothesize that while smartphones and PCs share some commonalities, they differ along two dimensions that, taken together, influence consumers' willingness to self-disclose: ( 1) the extent to which consumers experience psychological comfort while on the device and ( 2) the degree of attentional narrowing arising from the form factor of the device (e.g., size). These two elements, depicted in Figure 1, form the foundation of our main hypothesis:Graph: Figure 1. Theoretical process model showing how use of one's smartphone (vs. PC) can lead to greater depth of disclosure.Notes: The model hypothesizes two parallel causal paths of mediation: one stemming from greater focus on the disclosure at hand, and the other through feelings of enhanced psychological comfort on the device. H1:  Consumers will tend to exhibit enhanced depth of disclosure—sharing personal feelings, thoughts, and other information deemed to be more intimate and private—when creating content on their smartphone versus their PC.Next, we discuss the process by which we hypothesize that comfort and attentional narrowing lead to greater self-disclosure. The comforting role of smartphonesThe first factor that we argue enhances depth of disclosure on one's smartphone versus PC is the increased psychological comfort that consumers tend to derive from their phone (e.g., [17]; [59]). For example, [41] found that after an induction of stress, participants assigned to engage in a task on their smartphone reported a greater increase in psychological comfort (and thus, greater relief from stress) than those assigned to engage in the same task on their laptop, or even an otherwise similar smartphone belonging to someone else.The enhanced feeling of comfort associated with one's smartphone (vs. other devices) is thought to arise from a unique combination of positive, personal associations with the device ([41]). For example, whereas PCs tend to be used more for work purposes, smartphones are often relied on for texting with friends and family, watching entertaining videos, or catching up on social media updates (e.g., [47]; [56]). Moreover, given their portability, smartphones are almost always within arm's reach—in one's pocket or purse during the day, by one's bedside at night—such that consumers learn that they can rely on their smartphone to engage in these personal activities whenever and wherever they want (e.g., [14]). As a result, the device becomes a general source of comfort and security for owners ([41]).Critically, this difference in comfort bears important implications for depth of disclosure across devices. Prior work has shown that situational factors—such as mere differences in how a website is designed—can foster enhanced feelings of privacy and security and, thus, greater willingness to self-disclose in online surveys ([31]). These results are consistent with research showing that people are more likely to self-disclose in environments that evoke positive affect ([24]), such as feelings of comfort and security (e.g., [13]; [25]; [42]). We therefore propose the following: H2:  Consumers are more willing to share sensitive information on their smartphone (vs. PC) in part because they tend to experience greater psychological comfort while on the device. The smaller form of smartphonesPrior work shows that differences in form influence the process of content generation across smartphones versus PCs. For example, because it is more difficult to write on its smaller keyboard and screen, users tend to generate shorter content on their smartphone (vs. PC) when completing open-ended surveys (e.g., [10]; [39]; [66]) and writing online reviews ([40]; [50]). Given these form-factor constraints, the amount of information that people disclose—or the breadth of self-disclosure—should similarly be lower when sharing from a smartphone versus PC.Although the smaller form of smartphones (vs. PCs) may limit the amount of information that consumers share, we argue that it has the countervailing effect of enhancing depth of disclosure, or the intimacy of information disclosed. One line of evidence consistent with this prediction comes from [40], who found that when consumers write reviews on their smartphone versus PC, they tend to use a greater proportion of emotional words (e.g., ""love,"" ""amazing""). Although expressions of emotionality do not necessarily imply more self-disclosure per se, enhanced emotionality is widely considered one of several linguistic markers of greater depth of disclosure (e.g., [28]).The rationale for our hypothesis is as follows. A large body of research shows that when tasks are more difficult, people tend to respond by focusing more intently on the most essential aspects of the task in lieu of peripheral information (e.g., [12]; [15]; [44]). As such, the relative difficulty of engaging in activities on a smartphone due to its smaller keyboard and screen may similarly narrow users' attention to the task they are engaging in on the device. Consistent with this, research shows that smartphone users often experience ""inattentional blindness"" when using their device, narrowing their focus to the activity onscreen while blocking out external surroundings (e.g., [29]; [37]). As an illustration of this narrowing effect, a large-scale field study found that 46% of pedestrians who were on their smartphone failed to notice a unicycling clown passing within one meter of them ([16]).Building on this, we propose that when engaging in disclosure of personal information on one's smartphone (vs. PC), the relative difficulty of completing the task on its smaller keyboard and screen will similarly focus users on the most essential elements of the task—sharing one's personal thoughts and feelings—and less on peripheral thoughts and cues that might otherwise inhibit disclosure. This prediction is consistent with recent work demonstrating that people asked to complete an online survey while under cognitive load (i.e., remembering names)—possibly paralleling the difficulty of generating content on a smartphone (vs. PC)—were more willing to respond to sensitive survey questions ([60]). Formally, we hypothesize, H3:  The smaller form of smartphones (vs. PCs) narrows consumers' attention down to the communication at hand, which, in turn, enhances depth of disclosure when generating content on the device. Overview of StudiesWe report the results of five empirical studies that support the proposed effect of device use on depth of disclosure in user-generated content, as well as explore the mechanisms underlying the observed differences (Figure 1). We begin by offering large-scale field evidence for the basic effect in analyses of tweets about a variety of topics (Study 1) and analyses of online restaurant reviews (Study 2). In two preregistered experiments we then test for the causal effect of device use on self-disclosure, as well as the proposed underlying mechanisms (Studies 3 and 4). We conclude by demonstrating the real-world generality of the effect by examining consumers' compliance with sensitive CTAs in web ads across devices (Study 5). Study 1: Depth of Disclosure Across Devices on TwitterThe purpose of the first field study was to test for the proposed differences in depth of disclosure on a major social media platform, Twitter. To control for potential differences across devices in both the timing of posts and topical content, we analyzed a data set of 369,161 original tweets[ 5] that each included one of 203 ""trending hashtags"" within a single 12-hour period in December 2015.[ 6] The hashtags covered a wide range of topical domains—including pop culture, sports, and a terrorist attack that occurred in San Bernardino, California—which allowed us to test for the generalizability of any observed differences between smartphones and PCs. Data PreprocessingPrior to the main analysis, the data underwent four waves of preprocessing. First, a dichotomous indicator of whether a tweet was written on a smartphone or PC was created by identifying each tweet's originating platform (e.g., a tweet written on a smartphone would be evidenced by ""Twitter for iPhone,"" and on PC by ""Twitter Web Client""); tweets originating from ambiguous or unknown sources (e.g., third-party apps) were removed, leaving 296,473 original tweets. Because we were interested in analyzing tweets generated by typical users rather than commercial networks or professional bloggers, in the second stage of preprocessing we removed 1,305 tweets with known commercial usernames (e.g., CNN) or that were posted from accounts with exceptionally large followings, which we defined a priori to be the top 1% of the distribution of followers (more than 32,978 followers).[ 7] This yielded a final data set of 293,039 tweets (59.6% originating from smartphones). Next, to allow for a within-user analysis of differences in depth of disclosure, the third stage of preprocessing involved identifying the 2,121 users from the total set of qualified users who tweeted from both a PC and smartphone.In the final wave of preprocessing, the 203 ""trending hashtags"" in our data were topically categorized by human judges. To achieve this, we recruited a sample of 150 Amazon Mechanical Turk (MTurk) participants and first asked them to familiarize themselves with ten randomly assigned hashtags (of the possible 203) by clicking on a hyperlink that led them to a set of tweets that had recently included the hashtag. After participants felt familiar with the content associated with an assigned hashtag, they were instructed to indicate whether it belonged to one or more of seven possible categories: news (e.g., #CaliforniaShooting), sports (e.g., #ArmyNavy), entertainment/pop-culture (e.g., #GoldenGlobes), amusement (e.g., #ImAWreckCause), moral causes (#GenocideVictimsDay), technology (e.g., #GoogleDemoDay), and economy/finance (#futureofwork). This process yielded roughly 15 judgments for each of the 203 hashtags, and each hashtag was categorized on the basis of the grouping (e.g., ""news,"" ""sports"") to which it was most frequently assigned.[ 8] Method and ResultsWe undertook two approaches to analyzing whether tweets created on smartphones showed greater depth of self-disclosure than those created on PCs: an automated analysis of linguistic markers (e.g., use of first-person pronouns; see, e.g. [19]) as well as human assessments of the content. Each of these approaches will be described in turn. Automated measures of depth of disclosureMultiple researchers have sought to identify linguistic markers indicative of greater self-disclosure in text (e.g., [ 5]; [ 9]; [19]; [48]; [63]). To illustrate these markers, in Web Appendix 1 we provide examples of texts from each of our studies that were assessed by human judges as being high or low in self-disclosure. Consistent with prior work on the linguistic markers of self-disclosure, the examples show how texts judged to be self-disclosing tend to be accompanied by more extensive use of ( 1) first-person pronouns (e.g., ""I,"" ""me""), ( 2) references to family and friends, and ( 3) words that convey emotionality—particularly negative emotions ([28]; [45]). The presence of these common linguistic markers forms the foundation of algorithms designed to automatically detect depth of disclosure in online texts ([ 4]; [ 6]; [51]; [63]).Drawing on this extant literature, we subjected the tweets to analysis by Linguistic Inquiry and Word Count (LIWC; [49]), which contains dictionaries for each of the aforementioned linguistic markers (first-person pronouns, references to family and/or friends, and negative emotionality).[ 9] We also examined LIWC measures for ""authentic"" and ""analytical"" writing styles. According to [49], writers using a more authentic style create texts that are more personal and vulnerable, such that higher authenticity scores point to greater depth of disclosure. Moreover, texts written in a less analytical style, as indicated by more narrative language and references to personal experiences, would also be suggestive of greater depth of disclosure.Finally, to ensure that the LIWC analysis provided valid measures of the degree of self-disclosure in tweets, we undertook a cross-validation analysis that regressed human judgments of the depth of disclosure for a sample of the tweets on each of the six LIWC measures of disclosure (see Web Appendix 2 for a description of the method and results). The findings confirmed, for example, that tweets rated as more self-disclosing by human judges tended to include a greater proportion of first-person pronouns and references to family and friends, and were written in a more authentic—but less analytic—writing style as measured by LIWC. Results: differences in depth of disclosure based on automated measuresIn Table 1 we report the results of two statistical approaches to analyzing how the linguistic content of tweets created on smartphones differs from that created on PCs. One approach involved a set of six univariate analyses that modeled each of the LIWC dimensions of interest (e.g., use of first-person pronouns, analytical and authentic writing styles) as a function of the originating device. The second was a multivariate logistic regression that predicted the likelihood of a tweet being written on a smartphone versus PC as a function of the six LIWC dimensions simultaneously. The analyses controlled for the word count of the texts—which tends to be greater for content written on PCs versus smartphones ([40])—as well as for the hashtag categories.[10] Finally, the table reports separate results for the overall corpus of tweets as well as a within-user analysis of the subset of users who tweeted from both devices, which allowed us to better control for possible issues of self-selection across devices.GraphTable 1. Study 1: Univariate Least-Square Mean Differences Between Smartphone- and PC-Generated Tweets Along LIWC Dimensions and Coefficients of Multivariate Logistic Regression Modeling the Likelihood That Tweets Were Written on Smartphones (vs. PCs) as a Function of These Dimensions.  1 Notes: Analytical and authentic writing styles are measured as scores out of 100, whereas types of words are measured as percentages. The effects of writing styles were modeled separately because the scores are composites of some of the word-use measures.The results provide strong initial support for H1 among the full data set of tweets as well as the subset of users who tweeted from both devices. For the full data set, across hashtag categories tweets written on smartphones tended to contain greater proportions of first-person pronouns (Msmartphone = 3.29 vs. MPC = 2.23; F( 1, 293,038) = 2,742.45, p <.001), references to family (Msmartphone =.76 vs. MPC =.50; F( 1, 293,038) = 681.54, p <.001) and friends (Msmartphone =.32 vs. MPC =.25; F( 1, 293,038) = 142.79, p <.001), and negative emotional words (Msmartphone = 1.62 vs. MPC = 1.52; F( 1, 293,038) = 40.73, p <.001). These results are further supported by differences in writing style across devices: tweets written on smartphones tended to display a less analytical but more authentic writing style (analytical: Msmartphone = 69.08 vs. MPC = 72.98; F( 1, 293,038) = 1,059.54, p <.001; authentic: Msmartphone = 28.94 vs. MPC = 25.49; F( 1, 293,038) = 681.54, p <.001). As shown in Table 1, the within-user analyses of tweets written by the same users on their smartphone and PC yielded a similar pattern of results.Finally, we examined whether the size of these effects varied by the particular hashtag category (e.g., sports, politics). The results (reported in Web Appendix 3) suggest that while many of the aggregate results hold across the hashtag categories, there was some variance in the size and, in some cases, direction of the effects. For example, while the use of first-person pronouns and references to family and friends were consistently greater on smartphones in domains that are intuitively more amenable to self-disclosure—such as news (e.g., #CaliforniaShooting), moral causes (e.g., #GenocideVictimsDay), pop culture (e.g., #CouplesTherapy), and sports (e.g., #FIFA)—these same markers were less evident in the more impersonal domains of finance (e.g., #DiscussTheDeals) and technology (#bufferchat). These latter two categories also showed less authentic and more analytical writing styles, a reversal that is perhaps unsurprising given the conversational norms that generally surround discussions of finance and technology (e.g., a tone that is more objective than subjective). Human judgments of depth of disclosureTo provide further evidence for differences in disclosure, we subjected the full set of tweets from two of the hashtag categories—2,261 tweets about the San Bernardino terrorist attack (53% smartphone), and 1,009 tweets about pop culture (56% smartphone)—to assessment by human judges. These two categories were selected so that we could test for the effect across topics that differed substantively in context and valence. We recruited an independent sample of 1,925 MTurk participants to assess up to 10 randomly selected tweets, yielding an average of 7.08 judgments per tweet. Participants were blind to both the hypothesis of this study and the originating device of the tweet.To measure depth of disclosure in the tweets, participants rated their agreement with a set of items used in prior work (e.g., [ 7]; [33]; [63]) on a seven-point scale (1 = ""Not at all,"" and 7 = ""Very much so""):Self-focus: ""To what extent does the writer focus on him/herself in this tweet (e.g., how he/she felt, what he/she did)?""Internal states: ""To what extent does the writer reveal his or her personal feelings, thoughts, or opinions?""Personal information: ""To what extent does the writer disclose personal information about him/herself?""Vulnerable: ""To what extent does the writer disclose information that might make him/her feel emotionally vulnerable?""Controversial: ""To what extent is the writer expressing potentially controversial statements/views?""Offensive: ""To what extent is the writer expressing views that may be offensive to others?""Impulsive: ""To what extent does it seem like the writer was impulsive when writing his/her tweet?""An exploratory factor analysis revealed that these items loaded onto two dimensions of disclosure: ""intimate information"" (self-focus, internal states, personal information, vulnerable; α =.78) and ""lack of censorship"" (controversial, offensive, impulsive; α =.85). Web Appendix 1 shows examples of tweets that scored high versus low on intimacy of information disclosed. Results: differences in depth of disclosure based on human judgmentsParalleling the analyses of the automated measures, we undertook two sets of analyses of the human assessments of the tweets. The first set included two univariate analyses that separately modeled our measures of perceived depth of intimate disclosure and lack of censorship as a function of the originating device, and the second set involved a multivariate logistic analysis that predicted the likelihood that a tweet had been written on a smartphone or a PC as a function of the perceived depth of intimate disclosure and lack of censorship (simultaneously). As with the automated analysis, each model controlled for the word-count differences between devices.The results provide convergent validity for the central hypothesis that consumers express greater depth of disclosure when writing on their smartphone versus PC. First, tweets written on smartphones (vs. PCs) were assessed by judges as conveying more intimate information—an effect that held for the tweets about both the San Bernardino attack (Msmartphone = 3.04 vs. MPC = 2.84; F( 1, 7,854) = 34.47, p <.001) and the pop-culture topics (Msmartphone = 5.01 vs. MPC = 2.76; F( 1, 5,180) = 2,210.08, p <.001). In contrast, while tweets written on smartphones were judged as more uncensored on average (see Web Appendix 4), the effect was primarily driven by the tweets about pop-culture topics (Msmartphone = 5.73 vs. MPC = 3.71; F( 1, 5,169) = 1,297.90, p <.001), as there was no perceived difference in degree of censorship among tweets about the San Bernardino attack (Msmartphone = 4.15 vs. MPC = 4.16; F < 1). Thus, tweets posted from smartphones were consistently viewed as more intimately self-disclosing than those posted from PCs, and were less consistently seen as more unfiltered or uncensored. (The results of the multivariate logistic regression, which models the likelihood of the tweets being written on a smartphone versus PC as a function of the two human-judged dimensions of disclosure, mirror these results; see Web Appendix 4.) Discussion and Replication StudiesThe results of the first field study provide initial evidence that user-generated content written on smartphones tends to convey greater depth of disclosure than content written on PCs (H1). This effect was robust across both automated measures (e.g., percentage of first-person pronouns, references to family) and human judgments of disclosure. These results were also robust across a variety of contexts that ranged from serious breaking-news events (a terrorist attack) to frivolous online amusements (e.g., ""#TheWorstSecretSantaGifts"").Although we find that the pattern of results consistently held across the hashtag categories in our data set, to further test for the robustness of this effect we conducted the same analyses reported above for three additional Twitter data sets. Two were obtained from public sources: ( 1) a corpus of 67,408 tweets about the 2018 FIFA World Cup posted on Kaggle ([52]), and ( 2) a corpus of 201,258 tweets posted about the 2016 U.S. presidential election on election day ([36]). We also analyzed ( 3) an original corpus of 18,346 tweets on hashtags covering news, sports, and amusement/entertainment on a single day in January 2017. The results of these analyses, reported in Web Appendix 5, closely replicate those reported above: Whether users were tweeting about a sporting event, election, or entertaining topic, tweets written on smartphones (vs. PCs) consistently contained greater proportions of first-person pronouns, references to family and friends, and emotional words—particularly those conveying negative affect. They also tended to have a more authentic and less analytical style. Study 2: Does Self-Disclosure Matter? An Analysis of Online ReviewsStudy 1 offered initial evidence that at least one type of user-generated content—tweets about a variety of topics—tends to contain greater depth of disclosure when written on smartphones versus PCs. One important question, however, is whether the observed differences in depth of disclosure might yield meaningful downstream marketing implications. Prior work would suggest, for example, that content written in a more self-disclosing manner would lead readers to feel a greater sense of similarity to the writer, which may result in content that is more persuasive to outside readers ([30]; see also [23]). In Study 2 we therefore tested for the robustness of the effect in a domain in which self-disclosure might have material impacts on consumer behavior: online restaurant reviews on TripAdvisor. MethodThe data set contained a corpus of 10,185 TripAdvisor restaurant reviews written on smartphones or PCs between April 2014 and July 2017. The reviews were a random sample drawn from a larger corpus utilized in previous work ([40]) and were comparably balanced between smartphones (N = 5,097) and PCs (N = 5,088). The data included the name of the restaurant, date of the visit, and text of the review. Mirroring the approach to analysis in Study 1, we undertook two analyses to measure the depth of disclosure in the reviews. The first was to subject the texts to analysis by LIWC, which as in Study 1 yielded measures of a battery of linguistic markers of self-disclosure (e.g., first-person pronouns).The second approach subjected the same reviews to assessment by MTurk judges who rated the reviews along two dimensions. The first was the perceived depth of disclosure, measured along two items adapted from Study 1 that were relevant in the context of restaurant reviews: ""To what extent did the writer focus on him/herself in this review (e.g., how he/she felt, what he/she did)?"" and ""To what extent did the writer reveal his or her personal feelings, thoughts, or opinions?"" (α =.63). Importantly, two additional measures were now included to capture a potential downstream consequence: ""How persuasive would you find this review to be if you were considering going to this restaurant?"" and ""How interested would you be in visiting this restaurant?"" As in Study 1, all items were measured on a seven-point scale (1 = ""Not at all,"" and 7 = ""Very much so""), with participants blind to the hypothesis and originating device of the review. Results Differences in depth of disclosure based on automated measuresAs in Study 1, we undertook both univariate and multivariate logistic regression analyses of the degree to which content written on smartphones differed from that written on PCs along a battery of LIWC measures that are suggestive of self-disclosure: use of first-person pronouns, references to family/friends, negative emotionality, and authentic and analytical styles. Again, the analysis controls for differences in word count, which was significantly higher in PC-generated reviews (Msmartphone = 69.87 words vs. MPC = 96.28 words; F( 1, 10,182) = 347.76, p <.001).Note that because reviews are, by definition, personal—typically first-person—accounts of one's consumption experience, logically the vast majority of reviews should appear self-disclosing (at least to some extent). Still, even in this comparatively self-disclosing context, the results conceptually replicate those of Study 1. Reviews written on smartphones tended to include greater proportions of first-person pronouns (Msmartphone = 2.35 vs. MPC = 2.19; F( 1, 10,182) = 9.55, p =.002), references to family (Msmartphone =.38 vs. MPC =.32; F( 1, 10,182) = 9.55, p =.002) and friends (Msmartphone =.52 vs. MPC =.43; F( 1, 10,182) = 36.34, p <.001), and negative emotional words (Msmartphone =.75 vs. MPC =.67; F( 1, 10,182) = 7.09, p =.008). Finally, as in Study 1 smartphone-generated reviews had a less analytic writing style (Msmartphone = 64.43 vs. MPC = 65.95; F( 1, 10,182) = 9.40, p =.002), though here we did not find a difference in authentic writing style (Msmartphone = 34.63 vs. MPC = 35.07; F < 1). (Binary logistic regression analyses, presented in Web Appendix 6, yield similar results.) Differences in depth of disclosure and persuasiveness based on human judgmentsNext, we undertook the same analyses as in Study 1 to test for differences in human assessments of the content. Consistent with Study 1, the results show that reviews written on smartphones (vs. PCs) were rated as containing greater depth of disclosure (Msmartphone = 4.67 vs. MPC = 4.52; F(1, 9,551[11]) = 18.10, p <.001). The results also provide evidence for a key downstream implication of this effect: reviews written on smartphones were rated by judges as being more persuasive than those written on PCs (Msmartphone = 4.97 vs. MPC = 4.74; F( 1, 9,540) = 49.40, p <.001). Finally, readers were more interested in visiting restaurants reviewed by other customers on their smartphones than restaurants reviewed on PCs (least square Msmartphone = 4.68 vs. MPC = 4.61; F( 1, 9,840) = 3.90, p =.048)—an effect that was strengthened after we controlled for valence of the review (as captured by the percentage of negative emotional words; Msmartphone = 4.69 vs. MPC = 4.60; F( 1, 9,539) = 5.74, p =.016).Next, we conducted a serial mediation analysis (SAS Proc Calis) to test whether reviews with greater depth of disclosure in smartphone-generated content led to greater persuasiveness and, thus, greater interest in the restaurant under review. The results provided an excellent fit to the data (Bentler comparative fit index =.998; root mean square residual =.008) and, critically, supported the hypothesized model. Reviews written on smartphones (vs. PCs) contained greater depth of disclosure (bdevice → disclosure =.05; t = 4.79, p <.001); reviews containing greater depth of disclosure were more persuasive (bdisclosure → persuasive =.28; t = 30.06, p <.001); and more persuasive reviews heightened readers' interest in visiting the restaurant (bpersuasive → interest =.62; t = 96.67, p <.001). Finally, the model supported an overall positive indirect effect of device on interest in visiting the restaurant (total indirect effect: bdevice → disclosure → persuasive → interest =.03; t = 4.71, p <.001). DiscussionAcross two field studies we provide consistent evidence that customers tend to convey greater depth of disclosure when generating content on their smartphone than on their PC—as evidenced by tweets about a variety of topics (Study 1) and by restaurant reviews (Study 2). It is worth noting that the size of the effects was somewhat smaller among the restaurant reviews (e.g., Cohen's d for human-judged depth of disclosure =.09) compared with the tweets (Cohen's d =.14)—a result that is not surprising given that, by construction, customer-generated reviews are first-person accounts of personal consumption experiences, making it harder to observe differences in degree of disclosure across devices. Nevertheless, even in this context, reviews written on smartphones still exhibited greater depth of disclosure than those written on PCs.Study 2 also indicates that the greater depth of disclosure in smartphone-generated content carries important downstream consequences. Outside readers found reviews written on smartphones (vs. PCs) to be more persuasive, which, in turn, heightened their interest in visiting the restaurant under review. These results are broadly consistent with those of [26], who found that reviews containing a mobile indicator (e.g., a ""written on mobile"" label) are more persuasive to outside readers than those containing a PC indicator. They argued that this occurred because readers infer that mobile-generated reviews are more credible given the relative difficulty of writing on the device. It is important to emphasize, however, that the outside readers in our study were given no information about the device on which the content was written, such that reviews written on smartphones (vs. PCs) were rated as more persuasive based solely on their content.Finally, it is worth noting that one possible explanation for why reviews and/or tweets written on smartphones (vs. PCs) appeared more self-disclosing is that they were composed at the same time as an event or experience, when personal feelings may have been more salient. Two results, however, argue against such a timing explanation: ( 1) restaurant reviews written on smartphones included relatively more—not fewer—references to the past (M = 6.90) than those written on PCs (M = 6.33; F( 1, 10,182) = 42.39, p <.001) and ( 2) the tweets in Study 1 were posted nearly simultaneously from smartphones and PCs. Nevertheless, because the association between smartphone use and self-disclosure remains correlational, and the underlying mechanism remains uncertain, in the next two studies we attempt to increase our knowledge by investigating the effect in a more controlled setting. Study 3: Testing for Underlying MechanismsThe purpose of Study 3 was twofold. The first aim was to test whether the differences in depth of disclosure observed in the first two field studies replicate in a more controlled setting wherein participants are randomly assigned to generate content on their smartphone or PC. The second aim was to test whether the greater depth of disclosure in smartphone-generated content is driven by the proposed mechanisms for the effect: first, a greater sense of psychological comfort (H2), and second, greater attentional narrowing on one's smartphone versus PC (H3). MethodStudy 3 involved two data-collection phases. In the first phase, participants were randomly assigned to use their smartphone or PC to write about an upsetting personal experience; in the second phase, participants' descriptions of their personal experience were evaluated by an independent sample of judges for depth of disclosure. In this section, we describe each of these phases in turn. Phase 1: eliciting disclosures and measuring proposed mediatorsWe preregistered this study on AsPredicted.org, which included the preregistration of our predicted hypotheses as well as exclusion criteria.[12] Our final data set included responses from 715 participants from a Qualtrics panel (60% female) who were randomly assigned to complete a two-part survey on either their smartphone or their personal computer (for the complete survey instrument, see Web Appendix 7). In the first part of the survey—which served to administer the disclosure task—participants were asked to use their assigned device to describe an upsetting personal experience (in four to five sentences). The specific instructions were as follows:Think of a topic or event in your life that made you upset (e.g., an article you read that made you angry; an argument you had with a friend that upset you). In the space below please describe what made you upset, including your thoughts and feelings about the topic or event.After participants completed the disclosure task, they were asked to use the same device to respond to a set of scales that measured the proposed drivers of depth of disclosure:Psychological comfort. Participants responded to five items adapted from [41] that measured the extent to which they associated feelings of psychological comfort with the use of their assigned device (1 = ""Not true at all,"" and 7 = ""Very true""): ( 1) ""Using my smartphone (PC) provides a source of comfort,"" ( 2) ""Having my smartphone (PC) with me makes me feel secure,"" ( 3) ""When I am using my smartphone (PC) I feel I am in my safe space,"" ( 4) ""Just holding my smartphone (PC), no matter what I do with it, makes me feel comforted,"" and ( 5) ""Touching or holding my smartphone (PC) makes me feel calmer."" Responses to these items were averaged into an index of ""psychological comfort"" (α =.88).Attentional narrowing on disclosure. Participants indicated the extent to which they agreed with each of three statements about how they felt while writing about their personal experience: ( 1) ""I drowned out my environment when writing,"" ( 2) ""I got lost in what I was writing,"" and ( 3) ""I felt a sense of privacy when writing"" (1 = ""Not true at all,"" and 5 = ""Very true""). Responses to these items were averaged to create an index of ""attentional narrowing"" (α =.66).Next, although our theory does not make direct predictions about whether consumers accurately perceive the depth of disclosure of their own writing, to explore this we asked participants to rate their beliefs about the sensitivity of the information that they shared in their descriptions. This was measured in terms of their agreement with four items (on a five-point scale): ""I would hesitate to share this experience with someone I just met,"" ""I felt I was revealing something very personal about myself when describing this experience,"" ""The experience is a very private matter,"" and ""There were sensitive parts of that experience that I intentionally chose not to write about."" Responses to these four items were combined to form a ""self-judged disclosure"" index (α =.77).[13]Finally, to control for possible factors that might additionally influence depth of disclosure across devices, we asked participants to indicate ( 1) whether they had completed the study in a private or public setting, and 2) the extent to which they were generally concerned about privacy issues on their assigned device. Responses were based on their agreement with two items on a five-point scale: ""There are some things that I avoid doing on my smartphone (PC) (e.g., finance-related activities)"" and ""I worry a lot about the privacy of the data on my smartphone (PC)"" (""general privacy concern"" index; α =.83). Phase 2: human judgments of depth of disclosureTo measure the key dependent variable—depth of disclosure in the descriptions as perceived by outside judges—we recruited an independent sample of 649 judges from MTurk to assess up to ten randomly assigned texts written by respondents in the main study (judges were blind to both originating device and hypothesis). After reading each text, participants were asked to rate it along the same four items that were used to create the ""intimate disclosure"" index in Studies 1 and 2 (α =.76). We obtained three assessments for each description, yielding a total of 2,129 judgments. Analyses and Results Differences in depth of disclosureAs in the first two studies, to test for differences in depth of disclosure across devices we undertook analyses using both automated measures and human judgments of the texts. For the automated analysis, the 715 descriptions were subject to analysis by LIWC ([49]), from which we extracted the same set of linguistic markers of self-disclosure analyzed in the previous studies. Similar to the previous results, descriptions written by participants on their smartphones made greater use of personal pronouns (Msmartphone = 14.36 vs. MPC = 13.26; F( 1, 713) = 4.97, p =.026), contained more references to family (Msmartphone = 1.71 vs. MPC = 1.16; F( 1, 713) = 7.02, p =.008), and expressed greater negative emotionality, though this effect did not reach the a priori level of significance (Msmartphone = 5.57 vs. MPC = 5.00; F( 1, 713) = 3.29, p =.07; Wald χ2 = 3.38, p =.06). In contrast, here we did not see a significant difference in references to friends (Msmartphone =.65 vs. MPC =.62; F < 1) or in writing styles (authentic: Msmartphone = 37.72 vs. MPC = 39.70; F < 1; analytical: Msmartphone = 53.33 vs. MPC = 53.89; F < 1).External judges' assessments of the descriptions provided more direct evidence for differences in depth of disclosure. As we predicted, participants assigned to use their smartphone to write about an upsetting personal experience created content that was rated by outside judges as more disclosing (M = 4.85) compared with content created by participants assigned to use their PC (M = 4.55; F( 1, 1,911) = 22.09, p <.001). Importantly, this effect was sustained after controlling for factors that might covary with depth of disclosure, such as the length of the descriptions as well as the age and gender of the writers (depth of disclosure: least square Msmartphone = 4.84 vs. MPC = 4.56; F( 1, 1,908) = 17.08, p <.001). The results also hold after we control for the setting in which the writers completed the study—though it is worth noting that, across conditions, 93% of participants completed the study in a personal (vs. public) place. Evidence for proposed mechanismsTo investigate whether the effect of device use on depth of self-disclosure could be explained by the proposed mediation model (Figure 1), we estimated a structural path model that included the hypothesized drivers of the effect. The model hypothesized that the direct effect of smartphone (vs. PC) use on human-judged depth of self-disclosure is described by two causal paths: one in which smartphone use evokes greater feelings of psychological comfort, thereby enhancing depth of disclosure (device → psychological comfort → disclosure), and another in which smartphone use leads to more narrowed attention on the communication at hand, which also enhances depth of disclosure (device → attentional narrowing → disclosure).We obtained maximum-likelihood estimates of the path coefficients using SAS's Proc Calis, and they supported the hypothesized causal structure. Specifically, the analysis supported the parallel positive path from smartphone (vs. PC) to psychological comfort (bdevice → comfort =.05; t = 1.98, p =.024), and a positive path from comfort to depth of disclosure (bcomfort → disclosure =.35; t = 1.79, p =.037). Likewise, the analysis confirmed a significant positive effect of smartphone (vs. PC) use on degree of attentional narrowing on the disclosure task (bdevice → attentional narrowing =.07; t = 3.05, p =.001), and a significant positive path from attentional narrowing to depth of disclosure (battentional narrowing → disclosure = 1.27; t = 3.95, p <.001). The results also showed a significant total indirect effect of smartphone (vs. PC) use on depth of disclosure through the parallel paths of attentional narrowing and psychological comfort (total indirect effect: b =.10; t = 4.61, p <.001). Additional analyses: self-judged disclosure and privacy concernsWe undertook two additional analyses for which we did not make a priori predictions. We first examined whether the observed differences in depth of disclosure arose for participants' own perceptions of their descriptions. Notably, the results revealed that participants assigned to write on their smartphone indeed rated their description as more disclosing (M = 2.88) than did those assigned to write on their PC (M = 2.69; F( 1, 713) = 5.22, p =.023). Thus, both outside readers and the writers themselves appeared to perceive the greater depth of disclosure of smartphone-generated content.We next examined whether participants' general privacy concerns might influence depth of disclosure across devices. First, as might be expected, participants in the smartphone condition were more likely to agree with the statement, ""There are some things that I avoid doing on my [device]; e.g. finance-related activities"" compared with those in the PC condition (Msmartphone = 3.47 vs. MPC = 3.11; F( 1, 713) = 11.21, p <.001). Interestingly, however, this greater general privacy concern on smartphones did not seem to influence depth of disclosure on the device. Privacy concerns were not statistically correlated with outside judges' assessments of the depth of disclosure (Pearson r = −.01; p =.729; N = 1,913) but were positively correlated with participants' own perceptions of the depth of disclosure in their accounts (Pearson r =.15; p <.001; N = 715). As a result, inclusion of privacy concerns as a covariate did not alter the effect of device on external judgments of disclosure (least square Msmartphone = 4.86 vs. MPC = 4.54; F( 1, 1,910) = 22.84, p <.001), but it did temper the effect of device on self-perceptions of disclosure (least square Msmartphone = 2.98 vs. MPC = 2.72; F( 1, 712) = 2.85, p =.051). DiscussionStudy 3 provides a conceptual replication of the findings of the two field studies, showing that participants randomly assigned to write about an upsetting personal experience on their smartphone generated content that revealed greater depth of disclosure than did those assigned to use their PC. This effect was observed not only in terms of the automated measures and external human judgments analyzed in the prior studies, but in terms of the writers' own perceptions of depth of disclosure in their descriptions. The results also show that the effects observed in the prior studies generalize to another domain of user-generated content of potential interest to firms: a context wherein consumers are asked to reveal private information in an open-ended survey. Finally, and most importantly, the results of Study 3 provide initial evidence in support of the proposed mechanisms underlying the effect. As we hypothesized, the greater depth of disclosure in smartphone-generated content was driven by a greater sense of psychological comfort on the device (H2) as well as more narrowed attention on the disclosure task at hand (H3). Study 4: Disclosure of Sensitive Consumer InformationIn a second experiment, we explored whether the findings of the first three studies generalize to a context that is often of importance to marketers: customer compliance with requests for private or sensitive information. In Study 4 we therefore asked participants to describe a private and potentially embarrassing product experience, with a focus on whether those using their smartphone (vs. PC) would be more willing to comply with the request rather than opt out of doing so. MethodAn independent sample of 1,389 participants was recruited from a Qualtrics panel (71% female) and randomly assigned to complete the study either on their smartphone or their PC. We preregistered the study on AsPredicted.org,[14] which included the preregistration of our predicted hypotheses as well as the same exclusion criteria as in Study 3. The general procedure was similar to that used in Study 3, involving two data-collection phases. The first phase asked participants to disclose a product that they had purchased which they considered to be private and potentially embarrassing (by responding in an open-ended text box) and then to describe their experience with that product (in a second open-ended text box on the subsequent screen). The specific instructions were as follows:This survey is part of a market research study aimed at helping companies better understand consumers' experiences with different types of products. Think of a product that you use, or have used in the past, which you consider to be private and possibly embarrassing—that is, something that you might not want others to know about. For example, perhaps you have purchased products to prevent hair loss—or perhaps you sometimes buy certain foods to binge on when you're feeling sad. In the spaces below, please first indicate what this product is (e.g., ""weight loss supplement""). Then, please tell us about your experience with this product, such as what led you to buy it, and how you feel about using it.The key dependent variables of interest were ( 1) whether participants were willing to disclose such a product or whether they opted out of doing so (e.g., by writing ""N/A"") and, if they complied with the request for information, ( 2) the depth of disclosure expressed in their description of the product (which we measured in phase 2). Finally, participants were asked to use their assigned device to respond to the same items as those used in Study 3 to measure the proposed mechanisms—their psychological comfort (α =.86) and attentional narrowing on the task at hand (α =.68)—as well as a series of questions measuring possible covariates and demographic information (see Web Appendix 8).For phase 2 of the study, we first identified the subset of 975 participants who did disclose a private product (and met the preregistered inclusion criteria),[15] and we then recruited a separate sample of 374 MTurk judges to rate the descriptions on two dimensions. The first dimension was the sensitivity of the product described, which was measured based on judges' agreement with the following items (1 = ""Not at all,"" = 7 = ""Very much so""): ""This product was..."" ( 1) ""very private,"" ( 2) ""potentially embarrassing,"" ( 3) ""not one that would be discussed with a stranger,"" ( 4) reveals something personal about the user, and ( 5) ""very intimate."" Responses to these items were combined into an index of ""product sensitivity"" (α =.92). The second dimension was the depth of disclosure in the descriptions, which was measured using the same items as in Study 3 (α =.77). Thus, while our main analysis compared across devices the rates of compliance (or participants' willingness to disclose the personal product vs. opting out), this second phase enabled us to compare the depth of disclosure conveyed in participants' product descriptions, conditional on their having disclosed one. Results and Discussion Differences in response complianceAcross conditions, 134 (11%) of all participants refused to comply with the request to describe a private or sensitive product, as indicated by responses such as ""none"" (73%) or ""I do not buy these types of products"" (27%). More importantly, as we predicted, rates of compliance differed between conditions. Participants were significantly more likely to disclose a private or embarrassing product purchase when responding on their smartphone (93%) versus their PC (86%; likelihood-ratio χ2 = 13.35, p =.003). This effect still held after controlling for three measured factors that may have incidentally contributed to differences in compliance across devices: the gender of participants, their age, and whether the study was completed in a public place (least square Msmartphone =.94 vs. MPC =.85; likelihood-ratio χ2 = 23.14, p <.001). Differences in depth of disclosure (conditional on compliance)We next tested whether there were differences in the depth of disclosure in the product descriptions among the subset of participants who were willing to disclose such a product. Again, the results show that smartphone-generated content was assessed by outside judges as more self-disclosing than that generated on PCs. Specifically, products disclosed by participants on their smartphone were rated by outside judges as more sensitive in nature than those disclosed on PCs (Msmartphone = 4.54 vs. MPC = 4.35; F( 1, 4,803) = 4.54, p <.001), and the accompanying descriptions of their products were also rated as conveying greater depth of self-disclosure (Msmartphone = 4.83 vs. MPC = 4.70; F( 1, 4,741) = 13.41, p <.001).[16] In contrast, unlike in Study 3, where all participants complied with the writing task, members of this self-selected group of participants were unaware that they were being more self-disclosing when writing about their habits, as here we found no significant difference in self-perceptions of depth of self-disclosure between devices (Msmartphone = 3.29 vs. MPC = 3.31; F < 1). Evidence for mechanismsTo test for the proposed drivers of differences in depth of disclosure, we undertook two structural equation analyses: one for the decision to comply with the disclosure task (vs. opting out) and another for the depth of disclosure in the product descriptions provided by those who did comply. In this particular study, given that one of the proposed mechanisms—degree of attentional narrowing on the disclosure task—was meaningful only for participants who actually agreed to disclose, our analysis of participants' willingness to comply examined only the mediating effect of the degree of psychological comfort associated with the device. Our analysis for differences in depth of disclosure among participants who did comply, in contrast, examined the full set of mediators (as in the prior studies).For the analysis of differences in rates of compliance, path model estimates derived using SAS's Proc Calis supported the theorized mediating effect of psychological comfort. As we predicted, smartphones (vs. PCs) were associated with greater psychological comfort (bdevice → comfort =.12; t = 4.25, p <.001), and greater psychological comfort was associated with a higher probability of compliance (bcomfort → compliance =.08; t = 3.48, p <.001). Critically, we also observed a significant indirect effect of device on compliance through comfort (b =.01; t = 2.68, p =.007). Thus, the greater willingness to comply with a request for private information was in part driven by the enhanced feeling of psychological comfort that participants associated with their smartphone versus PC (H2).Next, to test for the proposed drivers of depth of disclosure as in Study 3, we collected MTurk assessments of the product descriptions provided by the subset of participants who disclosed the private product. As in Study 3, we estimated the model in which depth of disclosure was driven by two parallel paths: one in which smartphones yield greater psychological comfort (device → comfort → self-disclosure) and another in which smartphones induce greater attentional narrowing on the disclosure task (device → attentional narrowing → self-disclosure). For this study, we measured depth of disclosure as a latent construct with two manifest measures: sensitivity of the product described and sensitivity of the information conveyed in the product description (r =.54; α =.70).[17]The model provided an excellent fit to the data (Bentler comparative fit index =.998; root mean square residual =.000) and offered support for the hypothesized causal structure. Consistent with H2, we find support for positive path coefficients leading from smartphone (vs. PC) to psychological comfort (bdevice → comfort =.12; t = 8.37, p <.001), and from psychological comfort to depth of disclosure (bcomfort → disclosure =.39; t = 5.43, p <.001). Likewise, consistent with H3, we find support for positive path coefficients from smartphone (vs. PC) to attentional narrowing (bdevice → attentional narrowing =.03; t = 1.91, p =.028), and from attentional narrowing to depth of disclosure (battentional narrowing → disclosure =.10; t = 6.13, p <.001). Study 5: Compliance with CTAs in Web AdsTo test for the real-world generalizability of the results of Study 4, in the final field study we again examined whether consumers were more willing to provide sensitive information on their smartphone versus PC. To do this, we collaborated with the advertising technology company Taboola (https://www.taboola.com), which provided data on the daily performance of 19,962 CTA web ad campaigns that were run on both smartphones and PCs between November 2018 and August 2019. The data included a total of 631,013 observations representing each of the dates on which the 19,962 campaigns were run.Call-to-action ads are of interest because they request personal information from consumers to further interact with the firm or brand. The ad campaigns spanned 22 different categories (e.g., finance, music, family) and varied widely in the sensitivity of the product/service advertised (e.g., online games, fitness products, financial services) as well as the depth of personal information that was being requested (e.g., email addresses, estimated credit scores). Web Appendix 9 provides examples of the CTAs for several ad categories. For each ad campaign run on a given date, the data included information about the platform on which the ad was targeted (smartphone, PC), the ad category to which it belonged (e.g., health, dating, lifestyle), the number of consumers who were presented with the ad (i.e., impressions[18]), and the number of consumers who complied with the information request in the ad (i.e., conversions). To test whether consumers were more responsive to CTAs eliciting personal information on their smartphone or PC (H1), we calculated the conversion rate (i.e., the number of conversions divided by the number of impressions) for each ad category on each platform, which served as our main dependent variable. Results and DiscussionThe campaigns included in the data reached extremely large audiences, achieving 75.8 billion impressions over the ten-month period of study. As is commonly the case with web ads, however, the rate at which consumers clicked on the ads and provided all the requested information—recorded as a conversion—was quite low (e.g., [38]), with 84.68% of all ad-dates reporting no conversions on either PC or smartphone. To account for this highly skewed distribution of responses, we subjected the conversion rates across devices to a series of negative binomial regressions that modeled conversion rates as a function of ( 1) the device on which the ad was administered and ( 2) fixed effects that controlled for variance in conversion rates across ad categories, as well as interactions between the device and ad category.The results of these analyses, summarized in Web Appendix 10, robustly support H1. Consistent with the results of Study 4, consumers were more likely to comply with requests for personal information in ads when using their smartphone versus PC. Specifically, CTA ads on smartphones had an average conversion rate of.28%, whereas those on PCs had an average conversion rate of.02% (t = 12.48, p <.001; see Web Appendix 10). This effect was larger when analyzing just the subset of ad dates for which there were nonzero conversion rates. Here, the average conversion rate on smartphones was.54% relative to.03% on PCs (t = 25.89, p <.001). While the difference in the conversion rates is small in absolute terms, the.26% increase in conversions on smartphones (vs. PCs) translates to millions of customer responses when applied to the billions of impressions achieved across the campaigns.One natural concern with this analysis is that the higher conversion rates on smartphones may have accrued to factors other than users' willingness to self-disclose per se, such as differences in the contexts in which the devices are used or how the ads are displayed/formatted. If consumers are indeed more willing to self-disclose on their phone (vs. PC), differences in conversion rates should be larger among ad categories where the information elicited is more personal or sensitive in nature, such as ads for dating sites, financial services, and health products; in contrast, conversion rates should be more comparable for categories that are less sensitive, such as music, food, and news. We therefore examined how differences in conversion rates varied by ad category.To test the association between the sensitivity of the ads and compliance across devices, Taboola provided ad content for a random sample of 1,061 ads from each of the 23 categories that, importantly, included descriptive titles for each ad (e.g., ""Calculate Your Maximum Social Security Benefit Instantly"" from the ""finance"" category). We then recruited 686 MTurk participants to assess (on a seven-point scale) up to ten of the ad titles along three correlated items measuring the personal nature and sensitivity of the ad: ""This ad is about a very sensitive/private topic,"" ""If I responded to this ad I would expect to be asked a number of personal questions (e.g., my address, finances),"" and ""If I provided information requested in this ad I feel I would be disclosing something intimate or private about myself."" We averaged across these items to create a ""personal/sensitive nature"" index for each ad title (α =.70).We then estimated the following negative binomial regression, which modeled the observed conversion rates for each of the individual ad campaigns as a function of device, judged sensitivity of each ad category (from which ads were sampled), and the interaction between sensitivity and device: CRijk=bo+b1Dk+b2Sij+b3Dk×Sij Graphwhere CRijk is the observed conversion rate for ad campaign i from ad category j on device k, Dk is a dichotomous indicator for device k (smartphone = 1, desktop = −1), Sij is the judged sensitivity of ad campaign i from category j, and b0,..., b3 are empirical parameters. The key parameter of interest is the coefficient of the interaction between device and sensitivity (Dk × Sij).The results, presented in Web Appendix 11, confirmed that consumers were more likely to comply with calls to action in ad categories that were more personal and/or sensitive on a smartphone versus a computer. Specifically, we find a positive effect of smartphone (vs. PC) (b = 1.98; t = 4.27; p <.001), as well as a negative main effect of perceived ad sensitivity (b = −.34; t = −2.53; p =.011) on conversion rates. More importantly, we find a positive interaction between device and ad sensitivity on conversion rates (b =.40; t = 2.78; p =.005), suggesting that, as hypothesized, the tendency for conversion rates to be higher on smartphones (vs. PCs) is enhanced for ads that are more personal or sensitive in nature.As an illustration of this interaction effect, the three categories judged to be the most sensitive on average—dating (M = 4.64), financial services (M = 3.88), and health (M = 3.56)—were associated with the largest differences in average conversion rates between smartphones and PCs (dating: Msmartphone − PC diff. =.67%, p <.001; financial: Msmartphone − PC diff. =.51%, p <.001; health: Msmartphone − PC diff. =.40%, p <.001). In contrast, categories judged to be among the least sensitive in nature—music (M = 2.13), fashion (M = 2.87), and pets (M = 2.93)—showed very limited compliance overall, and no statistically significant difference between smartphones and PCs (music: Msmartphone − PC diff. = −.0004%, p =.164; fashion: Msmartphone − PC diff. = −.002%, p =.096; pets: Msmartphone − PC diff. = −.008%, p =.514; see Web Appendix 12). General DiscussionIn recent years, smartphones have increasingly come to supplant personal computers as the major medium through which consumers provide and share information. In this work we offer evidence that this change has served to alter not only how consumers communicate but also what they share: across five experimental and field studies, we find that consumers tend to be more self-disclosing on their smartphones than their PCs. We find this effect robustly across ( 1) multiple domains of user-generated content (e.g., six field data sets, open-ended survey responses), ( 2) different forms of self-disclosure (e.g., self-generated posts, admissions of embarrassing information), and ( 3) different measures of disclosure (automated measures, external human judgments, the writers' own perceptions of depth of disclosure, and compliance with sensitive CTAs in ads). We also find evidence for the two proposed parallel drivers of this effect, showing that enhanced disclosure on smartphones (vs. PCs) is driven by greater feelings of psychological comfort that consumers associate with their phone and the relative difficulty of generating content on the device, which narrows attention on the disclosure task at hand (and away from peripheral cues or thoughts).One question that remains is whether consumers are generally aware that they disclose differentially across their devices; for example, when consumers use their phone to tweet, are they aware that they may be revealing more about themselves? While in Study 3 participants reported being more disclosing after completing the disclosure task on their phone versus PC, it is not clear whether they were aware of a general effect of this device. To examine this issue more directly, we recruited an independent sample of 544 MTurk participants and asked them to indicate their beliefs about their willingness to disclose across devices.[19] We also administered similar scales to those used in Studies 3 and 4 to measure the degree to which participants believed that they experience greater psychological comfort and attentional narrowing on their smartphone versus PC. All responses were this time rendered on comparative scales, which were anchored at 1 (""Much more true of my laptop"") and 5 (""Much more true of my smartphone""), with 3 (""Equally true of my laptop and smartphone"") serving as the midpoint.The results suggest that consumers seem to be generally aware of the differences in self-disclosure observed in our studies. Respondents indicated that they tend to be more self-disclosing when creating content on their smartphone compared with their PC (Mdisclosure = 3.13; t(545) = 3.42, p <.001). Furthermore, participants reported that they generally associate stronger feelings of psychological comfort (Mcomfort = 3.69; t(545) = 21.36, p <.001) and tend to feel a greater attentional narrowing in activities (Mattentional narrowing = 3.52; t(545) = 16.99, p <.001) when using their smartphone versus PC. Thus, consumers seem to be at least somewhat aware of the distinct psychological experiences they undergo on their smartphone versus PC, as well as the differences in the types of information they tend to disclose across devices. Strategic Implications for ManagersThe finding that consumers are more willing to self-disclose on their smartphone (vs. PC)—and the identified mechanisms that give rise to it—hold several actionable implications for marketers. Perhaps the most direct is that if a firm wants to obtain sensitive or personal information from consumers, it should target them on their smartphone rather than their PC. We found evidence for this in Study 4, for example, where participants who were asked to admit to purchasing a private or embarrassing product were 6% less likely to do so when asked on their PC than when asked on their smartphone. While small in absolute terms, this difference would be quite meaningful for any firm relying on consumer self-reports to gauge consumption rates. Further evidence is provided from the large-scale field data in Study 5, where consumers were more compliant when ads requesting information were targeted on their smartphone (vs. PC)—a difference that was especially large for requests that were more sensitive in nature. Again, while the absolute size of this effect may seem small (Msmartphone − PC diff. =.19%; Msmartphone =.28% vs. MPC =.09%), when applied to the billions of impressions produced by the ad campaigns, this difference in compliance rates potentially translates to millions of additional customer leads for firms.The finding that social media posts and open-ended survey responses produced on smartphones were more self-disclosing also suggests that smartphone-generated content may offer more diagnostic or accurate insights into consumer preferences. Consistent with this, in Study 3 participants self-reported that they had disclosed information that was more private and personal on their smartphone than did participants on their PC. Building on this result, future work might explore whether the observed effects generalize to domains of disclosure with a measurable benchmark of ""truth"" or accuracy of information. For example, might consumer preferences disclosed on smartphones (vs. PCs) be more predictive of market outcomes?Finally, we found that the greater depth of disclosure in smartphone-generated content has the major downstream consequence of being more persuasive to outside readers. Study 2 demonstrated that restaurant reviews written on smartphones were perceived as 5% more persuasive on average than those written on PCs and, when positive, were associated with a 2% increase in readers' interest in visiting the restaurant. The effect of device use was even larger on the high extremes of the perceived persuasiveness and interest scales. Reviews written on smartphones were 33% more likely to receive a ""7"" on a seven-point scale of persuasiveness (raw difference +4.5%) and 28% more likely to receive a ""7"" on a seven-point scale of interest in visiting the restaurant (raw difference +3.9%). Thus, firms could identify which reviews will be more persuasive to outside customers by simply identifying their originating device. Leveraging the Psychological Drivers to Enhance Self-disclosureIn our research we showed that content produced on smartphones (vs. PCs) tends to be more self-disclosing because of two drivers: ( 1) the tendency for smartphones to be associated with heightened psychological comfort and ( 2) the narrowing of attention that often arises while completing a task on the device. We conceptualize these drivers as two independent factors with a relative influence that likely varies across consumers as well as contexts. For example, consumers vary in the degree to which they derive psychological comfort from their phone as a function of whether they use the device more for work versus hedonic purposes ([41]). Nevertheless, according to our theory, consumers who derive little psychological comfort from their phone might still be more self-disclosing when generating content on the device because of the narrowing of attention that tends to arise when writing on its smaller keyboard and screen. Similarly, one might conjecture that attentional narrowing would not arise when performing a simple task on one's phone, such as clicking a multiple-choice button when responding to survey questions; in such cases, however, consumers might still show enhanced depth of disclosure due to the feelings of comfort that often arise on the device.These two paths suggest actionable levers by which firms might influence consumers' willingness to self-disclose. For example, if firms wish to encourage consumers to be more self-disclosing in survey responses, our findings suggest that they should design surveys in a way that enhances respondents' feelings of psychological comfort—such as by exposing them throughout the survey to images or even sounds that are comforting or relaxing. From a consumer-welfare perspective, the mechanisms also have implications for consumers who want to avoid being too self-disclosing in certain contexts; for instance, they might consider eschewing their phone for their laptop when writing a work email or when responding to long, open-ended survey questions. Extensions to Emerging Technologies and Boundary ConditionsOne suggested area for future research is exploring whether the observed effects extend to newer technologies. Here, we argued that the small size of smartphone screens has an attention-narrowing effect that heightens consumers' willingness to self-disclose when generating content. Given that some new technologies—such as smart watches—have screens so small that they render typing extremely difficult, we predict that attempting to write on such small devices may prevent consumers from disclosing altogether. Another emerging technology to consider is voice-enabled assistants such as iPhone's Siri or Amazon's Alexa. Would the observed effects on smartphones versus PCs still hold if consumers used voice commands instead of written responses to disclose personal information? One might predict, for example, that sharing personal information verbally—rather than writing it—might evoke the psychological experience of face-to-face interaction, which (as noted previously) has been shown to reduce disclosure relative to written communication through computers (e.g., [32]). We believe that these are important and intriguing questions that merit future investigation.Future research could also further explore the role that the psychological comfort associated with one's smartphone plays in enhancing depth of disclosure on the device. For example, incidental experiences that precede disclosure—such as the extent of comfort derived from browsing certain online content on the device—may influence users' subsequent willingness to disclose in the short term, regardless of the device on which they are responding. In this case, firms may want to expose consumers to certain types of comforting or relaxing content prior to eliciting sensitive information from them. Moreover, to the extent that this psychological comfort arises from one's established associations with the device ([41]), consumers might be more willing to respond to personal or sensitive questions on their own smartphone than on an otherwise similar phone belonging to someone else. This bears important implications for medical professionals, for example, who have begun to increasingly administer surveys to patients using in-office tablets. Our findings suggest that medical professionals might consider sending sensitive survey questions to their patients so that they can respond instead on their personal smartphone.Future research could also test for boundaries of the types of information that consumers are willing to reveal on their smartphone versus PC. While we found evidence for the effect among some highly sensitive disclosures (e.g., providing one's bankruptcy history or issues with substance abuse in Study 5), many of the disclosures examined in our studies did not involve highly sensitive information—for example, descriptions of restaurant experiences (Study 1). While we do find that the effect extends to admissions of embarrassing or private purchases (Study 4), it is possible that the observed differences across devices may disappear when it comes to disclosures that could be more personally harmful, such as sharing one's social security number or financial information. "
36,"Gear Manufacturers as Contestants in Sports Competitions: Breeding and Branding Returns Several manufacturers make substantial investments to compete in sports contests, using the gear they develop and market. However, no systematic analysis of the breeding (i.e., innovation) and branding (i.e., marketing) returns from such investments exists. In this study, the authors conceptualize and empirically estimate the breeding and branding returns that such manufacturers obtain. The authors gather data for 30 car brands of 16 manufacturers over the period 2000–2015 regarding their participation, spending, and performance in Formula One championships, annual patent citations, and research-and-development (R&D) budgets as well as monthly vehicle registrations, advertising expenditures, and Formula One TV viewership. The authors find that only gear manufacturers with relatively high levels of R&D spending obtain a positive and significant breeding return from competing in sports contests. While most brands obtain positive branding returns, the lower the level of advertising spending for the brand, the greater the branding returns they obtain from competing in these contests. Thus, research-intense (compared with advertising-intense) gear manufacturers have more to gain from competing in sports contests. These findings can help guide manufacturers in budget allocation decisions on sports competitions, R&D, and advertising.KEYWORDS_SPLITMany firms are sports sponsors in one way or another. Marketing researchers have posed a historical interest in examining the branding returns from such sponsorships and have shown that professional sports sponsoring increases the brand's exposure, recall, recognition, affect, trust, loyalty, and sales (e.g., [13]; [30]; [33]; [34]; [39]; [50]; [59]). However, in quite a few cases, firms' interest seems to go beyond such branding returns to achieve what we call breeding (i.e., innovation) returns from athletes using their gear in sports competitions.Such breeding returns, beyond branding returns, from involvement in sports can be most easily envisioned in cases where gear manufacturers choose to go beyond mere sponsoring and actively compete in a sports contest. A gear manufacturer that competes in a sports contest participates with its own team that uses the manufacturer's gear and goes head-to-head against other participating contestants. Gear manufacturers may use the extreme conditions under which athletes in the team use their gear and closely cooperate in developing and testing new technologies that may improve their team's performance, entailing breeding returns. Moreover, branding returns from competing in a sports contest in this way may be different from branding returns from mere sponsoring. For example, one may envision that the performance of the firm as a contestant may affect its branding return.The breeding and branding returns gear manufacturers may obtain from entering sports competitions are relevant to many firms. Race bike manufacturer Trek invests $14 million annually to compete with its Trek-Segafredo team in the Union Cycliste Internationale (UCI) World Tour ([52]), car manufacturer Daimler invests around $200 million annually to compete with its Mercedes-AMG Petronas Motorsport team in the Formula One (F1) Championship ([54]), and ski manufacturer Atomic invests approximately $9 million annually to compete with its own team in the International Ski Federation (FIS) Alpine Ski World Cup ([48]). Competing firms may invest varying amounts in such sports competitions, with varying success, both of which may affect their breeding and branding returns. The allocation of resources to competing in sports contests is likely not independent of the firm's investment in other areas, of which research and development (R&D) and advertising seem most relevant as one considers the breeding and branding returns of competing in sports contests. This, in turn, raises the question of whether breeding and branding returns depend on the firm's R&D and advertising spending. Our key research question is therefore the following: To what extent do gear manufacturers that compete in sports contests gain positive outcomes in terms of breeding, branding, or both, and are these outcomes contingent on the gear manufacturer's R&D and advertising spending?To the best of our knowledge, so far no study has conceptualized or systematically analyzed both branding and breeding returns that gear manufacturers obtain from competing in sports contests, nor whether such returns depend on the manufacturer's R&D spending (for breeding returns) and advertising spending (for branding returns). This is what the current study aims to offer. Analyses of breeding and branding returns are of great interest to marketing managers, analysts, and academics because they relate to accountability of board-level strategic investments (e.g., [43]).Empirically, we constructed a novel data set on car manufacturers' participation, spending, and performance in the F1 World Championship. Our sample consisted of 30 automobile brands sold by 16 car manufacturers, among which 10 brands from 9 car manufacturers competed in F1 at some point during our sample period of 2000–2015. To examine the breeding effect, we supplemented F1 data with information on these 16 manufacturers' R&D spending and on their innovation performance (measured in terms of patent citations). To investigate the branding effect, we obtained the brands' advertising spending and sales performance, in terms of number of vehicle registrations in five countries (France, Germany, Italy, Spain, and the United Kingdom).Our study provides the following new insights. First, competing in sports contests and R&D spending are complements-competing in sports contests generates a significantly positive breeding return only for gear manufacturers with relatively high levels of R&D spending (more than €3.8 billion annually in our F1 context). Second, we find that competing in F1 and advertising spending are substitutes. Brands with low advertising budgets obtain greater branding returns from competing in sports contests than those with high advertising budgets. While all brands in our sample obtain positive branding returns from participating and increasing their spending in F1, only brands with less than €10.6 million in monthly advertising benefit from improving their performance in F1. In summary, research-intense gear manufacturers (i.e., firms that spend heavily on R&D but limitedly on advertising) have more to gain from competing in sports contests, as compared with advertising-intense gear manufacturers (i.e., firms that spend little on R&D but heavily on advertising).This article contributes to the existing literature in several ways. First, it shows that firms may obtain breeding and/or branding returns from their involvement in sports competitions, whereas prior literature has examined only branding returns and, thus, offers a partial view, at best. Second, it conceptualizes competing by a firm in sports contests as inherently different from mere sponsoring. It also provides an analytical framework for estimating the returns for firms that compete in sports contests and provides the first estimates of such returns ever reported in the literature. Third, we show that returns from competing in sports contests cannot be assessed without accounting for other related decisions of the respective firms, such as R&D and advertising spending. Fourth, for brand exposure, we are the first to empirically demonstrate that saturation effects occur even across greatly dissimilar exposure vehicles (in our case, car advertising and competing in F1 by car manufacturers). This complements prior literature that has demonstrated such saturation effects only among fairly similar exposure vehicles (e.g., [56]). It may also contradict managerial practice to leverage sports investments with greater advertising spending.The findings in this research are relevant not only to managers and analysts in the automotive industry specifically (including tier 1 suppliers) but also to other sports gear manufacturers, for which competing in sports contests is a relevant consideration (e.g., motorsports, cycling, skiing). They can use these findings to assess the potential economic outcomes of competing in sports contests. Moreover, these findings may guide gear manufacturers in a trade-off of budget allocation between contending in sports competitions on the one hand and R&D and advertising on the other hand. Manufacturers' Investments in Sports CompetitionsManufacturers' investments in sports competitions can be classified in terms of the following two dimensions: the type of involvement in the sports contest (sponsor vs. contestant) and the type of deployed resources that the manufacturer uses in the sports contest (gear vs. nongear). Type of Involvement: Contestant Versus SponsorThe manufacturer is involved as a contestant if it competes in the sports contest with a team. In contrast, a manufacturer that is involved as a sponsor in sport contests provides financial and/or in-kind assistance (e.g., a company's products) to an individual athlete, a team, or a competition in return for access to the commercial potential of the sponsored object ([28]; [35]).For manufacturers, competing in sports contests is different from being a sponsor in three ways. First, the manufacturer owns all or part of the team and, therefore, has greater responsibility for and more control over the team than a sponsoring manufacturer. For example, when Red Bull became the owner of the Jaguar F1 team instead of being a sponsor, it incorporated the company name in the team name and gained control over the design of the car's paint scheme, which helped the firm gain higher visibility ([25]).Second, manufacturers that compete in sports face off against other contestants from similar industries in the sports competition. For example, Mercedes competes against other car manufacturers in F1, and Trek competes against other race bike manufacturers in the UCI World Tour. This is different in case of sponsors. For example, Wilson is the racket sponsor of various tennis players (e.g., Roger Federer), but these players do not form a Wilson team that competes in tennis championships against, for example, a Babolat team; rather, the individual tennis players compete against each other (e.g., Roger Federer competes against Rafael Nadal).Third, the brand name of the manufacturer that competes in sports is strongly linked to the performance of the manufacturer's team in the competition. Contestants are ranked on the basis of their relative performance vis-à-vis competing brands in the sports contest (see, e.g., https://data.fis-ski.com/alpine-skiing/brand-ranking.html and http://www.skysports.com/f1/standings). In contrast, because sponsors do not compete themselves in the sports, they are not ranked on the basis of the performance of the athletes or teams they sponsor. For example, the Association of Tennis Professionals (ATP) rankings show the official singles rankings of the ATP World Tour, featuring the world's top-ranked players in men's professional tennis, but do not show the names of manufacturers whose rackets the players used. Type of Deployed Resources: Gear Versus NongearWe define ""gear"" as clothing, goods, and equipment made by the manufacturer to use in the sport. Gear manufacturers provide the set of tools that will enable the individual athlete or team to compete, whereas nongear manufacturers do not. For example, Nike sponsored Tiger Woods by providing him with Nike equipment, apparel, and shoes and is thus a gear sponsor. Trek, providing its own team with Trek race bikes to compete in the UCI World Tour, is a gear contestant.From the manufacturer's point of view, two important (related) differences exist between deploying gear and nongear resources to a sports competition. First, for the manufacturer, there is a strong fit between the gear deployed in the sports competition and the gear sold in the commercial market, thereby bridging these two markets. For example, Nike sold golf balls in the main market similar to those used by Tiger Woods in golf tournaments. Second, in case of gear sponsors and contestants, resources and competencies deployed for the competition may spill over to the main market and vice versa, which may lead to technology transfers. As an example, Wilson and Roger Federer cocreated a tennis racket, the Wilson Pro Staff RF97 Autograph, first to be used by Roger Federer in his matches, but later on, a commercial version of the racket was sold to the main market ([ 5]). There have also been many technology transfers from F1 race cars to cars for the general public (e.g., antilock brakes, electronic throttles, traction control). Positioning in Prior LiteratureThe distinctions we make help clarify the positioning of the present study in the existing literature on sports sponsoring. So far, most studies have focused on nongear sponsors and have shown that sports sponsoring by means of providing nongear support entails branding effects. [39], for example, show that extensive logo exposure from sponsoring a sports league increases brand recognition and likability equally as much as a 30-second TV ad. [59] show that brand recall and recognition for Heineken increased over the brand's years of sponsoring the Union of European Football Association (UEFA) Champions League, with the largest increase in the second year. And, [34] have shown that sponsoring a large sports event, such as the Summer Olympics, can increase brand trust and loyalty. The strength of such branding effects for nongear sponsors appears to vary depending on the fit between the sponsor and the brand and the successes of the sponsored objects ([30]; [34]; [39]; [50]; [59]).A few studies have focused on the effects of being a gear sponsor. [13] is the first study to investigate the relation between sponsoring and the sales performance of the sponsoring brand. It shows a positive effect of being a gear sponsor—that is, it examines the effects of Nike's gear sponsoring (golf balls, among other equipment) of Tiger Woods on the brand's sales performance of golf balls, and how this effect depends on Tiger Woods's performance in the competition. In a subsequent study, [18] shows that this endorsement effect is stronger for novice golfers than for experts.In contrast, the potential outcomes of manufacturers' investment in becoming a gear contestant have been completely ignored in the literature so far. This is an interesting context, because multiple gear contestants from the same industry typically participate in a particular sports competition (e.g., many large ski manufacturers participate with their own ski teams in the FIS Alpine Ski World Cup), with varying level of investments (e.g., ski manufacturer Head invests twice as much as ski manufacturer Atomic) and varying levels of success (e.g., both Head and Rossignol outperformed Atomic in the overall World Cup brand rankings in 2018). This provides a unique opportunity to investigate branding effects, as brands are shown to the audience in direct comparison to competitors' brands, which may lead to more pronounced branding effects. In addition, this context offers the opportunity to investigate breeding returns of firms' involvement in sports competitions. Because gear contestants own participating teams, the teams' performance is directly related to the gear contestant's brand, and therefore, these gear manufacturers are most likely continuously searching for new product technologies that may help improve the teams' performance. In line with these considerations, this study contributes to the literature by examining the breeding and branding effects that result from manufacturers' participation, investments, and successes in sports competitions as gear contestants. Conceptual FrameworkFigure 1 graphically summarizes our conceptual framework and shows the relation between a gear manufacturer competing in a sports contest and its innovation and sales performance (i.e., the breeding and branding effects, respectively). We operationalize competing in three ways: participation, spending, and performance in the sports competition. Participation denotes that the manufacturer is one of the contestants. Contingent on participation, we investigate the influence of ( 1) the amount manufacturers spend on competing (spending) and ( 2) the level of success in competing in the sports contest (performance), as different levels of spending and performance may affect the breeding and branding returns.Graph: Figure 1. Conceptual model.The theoretical base of our conceptual framework relies on the resource-based view (RBV) of the firm, which states that a firm's resources and capabilities (i.e., a firm's capacity to deploy these resources) help give it a sustained competitive advantage ([60]). We posit that the manufacturer's team is a resource, and competing in a sports contest can be viewed as a capability to leverage the manufacturer's asset of having its own team. The manufacturer's team is a resource that, either singly or with other manufacturer resources (e.g., R&D and advertising spending), can be the basis for a sustained competitive advantage in terms of an increase in the manufacturer's innovation performance and the brand's sales performance. Specifically, we hypothesize that R&D spending moderates the relationship between a gear manufacturer competing in a sports contest and its innovation performance because R&D is the most fundamental resource available to firms to produce technological know-how and generate innovations ([21]; [61]). In a similar vein, we hypothesize that advertising spending moderates the relation between a gear manufacturer competing in a sports contest and the sales performance of its brand(s) because advertising is generally an important source to increase a brand's sales performance (e.g., [55]). Breeding EffectWe define the breeding effect as the effect of a gear manufacturer competing in a sports contest on its innovation performance (i.e., the manufacturer's innovation outputs; e.g., patents; [ 4]). Atomic, for example, developed its ""Doubledeck"" ski technology first for use by professional athletes on the Atomic ski team that competes in the FIS Alpine Ski World Cup; after the technology was proven successful, the brand transferred this technology to its commercial skis. Main breeding effect of competing by a gear manufacturerCompeting in sports contests is valuable to a gear manufacturer because it offers the manufacturer the opportunity to develop and test new technologies under the most demanding circumstances. Competing generates valuable resources and competencies for converting new product ideas into innovations, increasing a manufacturer's innovation performance ([12]). The breeding effect of a gear manufacturer competing in a sports contest on its innovation performance can be explained as follows. First, a gear manufacturer competing in sports contests creates a parallel path of R&D activities in addition to its regular R&D processes. Because sports competitions are characterized as highly demanding in terms of both speed and accuracy, gear manufacturers that compete in such competitions develop and test new technologies specific to the demands of these sports competitions. This parallel path of R&D activities, along the technical frontier, can improve a firm's overall innovation performance ([ 3]; [16]). Second, the immediate performance feedback from competing in a sports contest stimulates learning through trial-and-error experiences. When a gear manufacturer is experimenting with new technologies, the performance feedback provides insights into these technologies' usefulness and quality. Such feedback facilitates the development of tacit knowledge and the discovery of otherwise unnoticed opportunities, which may increase the gear manufacturer's innovation performance ([ 8]; [ 9]).In summary, we expect that a gear manufacturer competing in a sports contest improves its innovation performance, leading to the following hypothesis: H1:  Competing in a sports contest by a gear manufacturer is positively related to the gear manufacturer's innovation performance. Moderating role of R&D spending on the breeding effectWe expect a direct relation between R&D spending and innovation performance as well as a positive moderating effect of R&D spending on the positive relation between a gear manufacturer competing in a sports contest and its innovation performance. Firms use R&D expenditures to create internal knowledge and to evaluate the potential outcomes of the created knowledge ([42]). Prior literature ([ 6]; [49]) has shown that a higher level of R&D spending entails a higher likelihood of patents being granted and/or the granted patents being intellectually valuable (in terms of citations), suggesting a direct effect of R&D spending on a firm's innovation performance.In addition to this direct effect, there are two reasons to expect a complementary effect between gear manufacturers competing in sports contests and these gear manufacturers' R&D spending. First, RBV theory emphasizes the role of firm-specific capabilities and competencies that stretch the firm's resources and help give it a sustained competitive advantage ([60]). Previous literature has shown that R&D spending is positively related to three important capabilities that may harness the innovation opportunities that result from having a team in a sports competition (i.e., absorptive capacity, product development capabilities, and patenting skills). By actively engaging in R&D in a particular field—in this case, innovation development in their focal industry—manufacturers increase their absorptive capacity (i.e., the capacity to acquire, assimilate, and exploit information they generate in another context, such as competing in a sports contest; [14]). They may also increase their product development capabilities (i.e., the capacity to turn this information and knowledge into breakthroughs; [14]; [58]). Finally, the higher the R&D spending, the greater a firm's patenting skills, which may help in patenting the breakthrough innovations, resulting from technology testing by the manufacturer teams in the sports competitions ([49]).Second, higher R&D spending is an important resource for the generation of creative innovation ideas ([ 9]). The new ideas from the regular R&D process may find their way into the equipment used by the manufacturers' teams, and competing by these teams in sporting contests may then provide valuable testing ground.In line with these arguments, we expect that R&D spending strengthens the positive effect of a gear manufacturer competing in a sports contest on its innovation performance, leading to the following hypothesis: H2:  The relationship between competing in a sports contest by a gear manufacturer and that gear manufacturer's innovation performance is positively moderated by the gear manufacturer's R&D spending. Branding EffectWe define the branding effect as the effect of a gear manufacturer competing in a sports contest on the sales performance of its brands. For example, Renault's F1 title in 2006 entailed a direct increase in its car sales ([23]). Main branding effect of competing by a gear manufacturerCompeting in a sports contest may positively influence a gear manufacturer's most important intangible resources (i.e., the brand's awareness, image, and reputation), thereby creating a sustainable competitive advantage and, eventually, higher sales ([ 2]; [15]; [31]). A gear manufacturer competing in a sports contest may generate branding returns for two main reasons. First, by entering sports competitions, gear manufacturers gain increased brand exposure because sports competitions have large viewership (e.g., 352.3 million people viewed the F1 championship globally in 2017; [53]). The brand's exposure increases with the brand's performance in the competition because the better-performing brands will receive more media attention than those at the back of the pack ([30]). Literature on the mere-exposure effect suggests that repeated exposure to a brand's stimuli, such as words, pictures, logos, and brands, will entail an affective response toward these stimuli, leading to higher brand preferences and higher brand equity, which subsequently leads to higher sales performance ([ 1]; [29]; [39]; [62]).Second, in the context of gear manufacturers competing in sports contests, signaling is an important additional logic beyond mere exposure for explaining the effect of competing on the sales performance of the gear manufacturer's brand(s). Signaling refers to the action a seller takes to convey information about the unobservable product quality to the buyer ([41]). Previous studies on signaling have focused on the transmission of quality signals in different forms, including brands ([20]), brand alliances ([41]), prices ([46]), advertising expenditures ([19]), and warranties ([10]). We argue that competing in sports contests, under the extreme conditions these contests entail and directly in comparison with competitors' products, enables the respective firm to demonstrate the performance and quality of its products and brands. A new technology that a competing firm introduces in such contests may yield strong reputational and quality returns to the main market. Thus, competing in a sports contest acts as a positive signal on the quality of the manufacturer's brand(s), which may result in higher sales, as perceived quality has been shown to be one of the most important universal brand benefits influencing a consumer's brand purchase intention and brand choice ([19]; [57]).In line with these arguments, we develop the following hypothesis: H3:  Competing in a sports contest by a gear manufacturer is positively related to the gear manufacturer's sales performance. Moderating role of advertising spending on the branding effectWe postulate that advertising spending will moderate the relationship between a gear manufacturer competing in a sports contest and the sales performance of its brand(s) for two main reasons. One argument for a negative interaction effect of a gear manufacturer competing in sports contests and its advertising spending is the saturation effect. Because gear manufacturers repeatedly show their brands during sports competitions, simultaneously increasing advertising spending will lead to saturation resulting from an increased number of brand exposures ([11]; [47]; [56]). This effect will be even more pronounced for brands with higher spending in sports competitions because higher spending leads to a more prominent display of the brand names, and logos. Similarly, the saturation effect will be greater for brands that perform well in the competition because better-performing brands receive more media attention and, thus, more brand exposures compared with brands that do not perform well ([13]).Second, according to [32], there is a negative interaction effect between two market signals that are similar in nature, owing to the reduced effectiveness of one signal in the presence of another signal of similar type. Because both competing in sports contests and advertising involve up-front expenditures, they are similar in nature. That is, both advertising spending and competing in sports contests can be viewed as (substitute) signals of high product quality, compared with rivals.[ 6] Therefore, we expect a negative interaction effect between them.In line with these arguments, we develop the following hypothesis: H4:  The relationship between a gear manufacturer competing in a sports contest and the sales performance of its brand(s) is negatively moderated by its advertising spending. Data Empirical ContextThe empirical context of our study is the F1 championship, which is the leading sports championship in single-seat auto racing, established by the Fédération Internationale de l'Automobile (FIA) in 1945. The F1 season runs from March to November and consists of a series of 19 Grand Prix races across different countries worldwide. Yet F1 has a strong heritage in Europe, where approximately 50% of the races still take place. Recent F1 race seasons have had an average of 11 teams participating with two cars, and every team enrolled in an F1 season competes in all the races of the year. At the end of the season, a world championship is awarded to one driver and one team with the highest total points earned during the races.The F1 context constitutes a perfect environment for testing our breeding and branding effects hypotheses. In terms of breeding potential, F1 teams, in which the car manufacturer's R&D personnel closely collaborate with the drivers and technical engineers, generate hundreds of ideas a year to improve automobile performance (e.g., aerodynamics, suspension setup, weight distribution, fuel efficiency). Because races are typically every two weeks, there is a rapid cycle of developing new ideas, testing them, and analyzing whether the modifications improve race performance. We believe that F1 is also an interesting area in which to investigate branding effects, because participating car brands gain a lot of brand exposure primarily due to the TV viewership of F1 races ([30]). Data Collection Procedure Level of data collectionWe collected data at the manufacturer-global-year level for the breeding analysis and at the brand-country-month level for the branding analysis. Patent data, which we use to measure innovation performance, is only unambiguously available at the manufacturer-global-year level, while data on car registrations, our measure of sales performance, is available at the brand-country-month level. Moreover, data for variables in the breeding part of the conceptual framework, such as R&D spending, are available only at the manufacturer-global-year level, whereas data for the variables in the branding part of the conceptual framework, such as advertising spending, are available at the brand-country-month level. Sample of countries, brands, and manufacturersWe decided to focus on Europe because F1 has its heritage in Europe and is still strongly European oriented, with many F1 drivers being of European origin, and approximately half of F1 races every year take place in Europe. Within Europe, we selected five countries—France, Germany, Italy, Spain, and the United Kingdom—on the basis of data availability on brands' monthly sales performance and the highest percentage of F1 TV viewership. Specifically, the percentage of the population within a country that has watched 15 or more minutes of at least one race during the 2010 F1 season was 52% for France, 51% for Germany, 60% for Italy, 71% for Spain, and 56% for the United Kingdom (Formula One Global Broadcast Report 2010). Moreover, many drivers participating in F1 between 2000 and 2015 are from one of these countries (11 drivers of German origin, 11 drivers of U.K. origin, 8 drivers of French origin, 6 drivers of Italian origin, and 6 drivers of Spanish origin), and these countries produce highly successful drivers (e.g., Michael Schumacher, Sebastian Vettel, Nico Rosberg, Giancarlo Fisichella, Fernando Alonso, David Coulthard, Jenson Button, Lewis Hamilton). We are aware that our sample of countries enhances the likelihood of finding a branding effect. The branding effect we identify may thus be lower in countries with less heritage in F1, with smaller F1 viewership, or in which no races take place.Table 1 lists the brands chosen in our sample countries for the branding analyses, the corresponding car manufacturers used in the breeding analyses, and the manufacturers' car brands that participated in F1 during 2000–2015, including the years in which they participated. We selected the 30 car brands (see Table 1, second column) using the following three criteria. First, we selected the top 20 brands in terms of vehicle registrations in our five sample countries. Second, we identified the brands that competed in F1 during our sample period. Among the top 20 brands, 7 brands competed in F1 during 2000–2015. We added Ferrari, Jaguar, and Lotus, which were not in the top 20 brands in terms of registrations but also competed in F1 between 2000 and 2015.[ 7] Third, we added seven niche brands that did not compete in F1 but are comparable to Jaguar, Ferrari, and Lotus in terms of the segments in which they operate: Aston Martin, Bentley, Lexus, Lamborghini, Maserati, Porsche, and Volvo. Our sample of 30 car brands accounts for more than 90% of passenger vehicle registrations in the five selected European countries. These 30 brands mapped into 16 car manufacturers (see Table 1, first column), among which 10 brands from 9 manufacturers competed in F1 at some point during our sample period (see Table 1, third column).[ 8]GraphTable 1. Selected Manufacturers and Brands.  1 aOutsourced development of the race team: BMW (2010), Jaguar (2000–2004), and Lotus (2010–2015). Our key findings are robust to the exclusion of these data points from our sample. See ""Robustness Checks"" subsection.2 bLancia brand is not sold in the United Kingdom.3 cDespite the connection between Porsche AG and the Volkswagen Group, we treat them as two separate entities. Until 2012, Porsche AG and Volkswagen had strong ties, including equity stakes in both directions. In 2012, the two companies actually merged, but they reported separately until the end of our data period. Measurement of Variables Operationalization of variables for the breeding modelThe dependent variable in the breeding model is a manufacturer's innovation performance, which we measured as the total number of citations obtained by the manufacturer's patents that were granted during a given year. Prior studies have used this method to measure innovation performance (e.g., [27]; [61]). There were, however, two issues in measuring the number of citations. First, for each patent, we observed only a portion of the period over which it could be cited. Specifically, it takes several years to realize patents' full citation potential. Second, the length of this observed citation period varied depending on when the manufacturer applied for the patent. For example, 13 years of citation data were available for patents applied for in 2001 (i.e., from 2001 to 2013),[ 9] whereas only 3 years of citation data were available for patents applied for in 2011 (i.e., from 2011 to 2013). We addressed this problem as follows. We first estimated the shape of the citation-lag distribution for each manufacturer using data on patents granted during 1986–1999. This distribution provides the fraction of citations that a manufacturer's patent obtains every year after the patent is granted. We used this distribution to calculate the total number of citations for patents granted in a particular year as follows. We first observed the total number of citations between the year in which the patent was granted until 2013 (e.g., three years for patents filed in 2011), and then we divided this value by the fraction of the citation-lag distribution that lies in this time interval ([27]).We measured the moderator variable (i.e., manufacturer's R&D spending) as the ratio of yearly spending in R&D in euros to the number of employees within the organization to control for different sizes of manufacturers. Prior studies (e.g., [45]) have found R&D spending per employee to be less sensitive to the spurious effects of business cycles, accounting manipulations, and asset sales than R&D spending as a proportion of sales.We operationalize the independent variable (i.e., competing as a gear manufacturer) in three ways: F1 participation, F1 spending, and F1 performance. F1 participation is a dummy variable that takes a value of 1 during the years in which a manufacturer participated in F1 as a gear contestant, and 0 otherwise. We operationalize F1 spending as the yearly spending of the gear manufacturer on its F1 team in euros. Such spending may cover out-of-pocket expenses, such as materials, contracts with first-tier suppliers on components, or employees on the manufacturer's payroll dedicated to the participating team. We measure F1 performance as the number of points the manufacturer's team accumulated during the entire F1 season. We distinguish between F1 participation and F1 spending because some manufacturers participate in F1 without any spending. For instance, the manufacturer Proton Holdings Berhad participated between 2010 and 2015 using the Lotus brand name. However, the manufacturer neither spent any money on the team nor managed the team. Similarly, Ferrari did not make any direct financial contribution to its team's budget between 2013 and 2015. This is because the payments that Ferrari received from the Formula One Group have increased in recent years; therefore, it was no longer necessary for the manufacturer to offer financial backing to the team. In these two cases, we treat the manufacturers (and the corresponding brands) as participants but treat their spending as zero. Operationalization of variables for the branding modelOne of the dependent variables in the branding analyses is the brand's sales performance, which we measured as the total vehicle registrations of the brand across all its models in a given country during a particular month. We measured advertising spending, which is both a dependent and a moderating variable in the branding analyses, as the total advertising spending of a brand in a country in euros across all media types in a given month.A brand as gear contestant in F1 is the independent variable in the branding model, which we also expressed in three levels: F1 participation, F1 spending, and F1 performance. F1 participation is a dummy variable that takes a value of 1 during months in which a brand participated in at least one F1 Grand Prix race and 0 otherwise. We measured the monthly F1 spending by multiplying the brand's annual F1 spending by the proportion of races in a year that took place in the particular month. All manufacturers except Ford participated with a single brand during our sample period. Therefore, we considered the manufacturer's F1 spending to be equal to the brand's F1 spending. Ford participated with the Jaguar brand name in 2000, 2001, and 2002, and with both the Ford and Jaguar brand names in 2003 and 2004. We allocated Ford's (manufacturer) F1 spending to its individual brands as follows. We allocated the entire manufacturer's F1 spending to the Jaguar brand for 2000–2002 and split the manufacturer's F1 spending between the Ford and Jaguar brands during 2003 and 2004. We assume that the manufacturer spent the same amount of money on the Ford brand in 2003 and 2004 as it did prior to 2000 when participating with only the Ford brand, and the rest of the manufacturer's spending was allocated to the Jaguar brand. Such allocation is also in line with publicly available information (e.g., [26]). We note, however, that our findings of the branding analyses are not sensitive to alternative allocations of F1 spending to the Ford and Jaguar brands (see the ""Robustness Checks"" subsection). We measured a brand's monthly F1 performance as the number of points accumulated by that brand during a particular month. Note that for F1 participation, F1 spending, and F1 performance, we assigned the same value for all countries.We included four control variables in the branding model. First, because different countries may have varying levels of exposure to the brands during different racing months, we controlled for country-specific monthly exposure of participating brands by including the monthly number of viewers that watched the live F1 races on TV in the particular country. Second, we included the number of new product introductions of each brand in a particular month as a control variable because we expected it to influence sales performance. We defined this variable as the number of new products, denoted as a unique combination of brand, segment, model, body group, and generation year, introduced by the brand during that month. Third, we included the effects of lagged advertising spending and lagged sales performance on current advertising spending to capture carryover effects and state dependence ([17]). Finally, we included competitors' sales performance and competitors' advertising spending and measured them as the total number of new vehicle registrations and total advertising spending of all other brands in that country during the particular month, respectively. Data SourcesFor the breeding analysis, we collected data on innovation performance, R&D spending, and F1 competing, in terms of participation, spending, and performance, for the 16 manufacturers. We obtained data on innovation performance in terms of yearly patent applications and their corresponding citations from the Worldwide Patent Statistical Database (PATSTAT) of the European Patent Office from 2000 to 2013, a period of 14 years. We obtained data on yearly R&D spending (2000–2013) from the car manufacturers' annual reports.[10] We converted all R&D spending figures reported in other currencies to euros using historical exchange rates. For most firms, the fiscal year matched the calendar year (January to December). We adjusted the R&D spending of other firms with a different fiscal year so that it matches with the calendar year. Finally, we gathered information on car manufacturers' yearly participation and performance in F1 from ESPN (www.espn.co.uk/f1/), and we obtained yearly spending in U.S. dollars for all car manufacturers that participated in F1 between 2000 and 2015 from Formula Money. We used historical exchange rates to convert U.S. dollars to euros.For the branding analysis, we collected monthly data on sales performance and advertising spending of the selected 30 brands across the five countries between January 2000 and December 2015 (192 months).[11] We obtained data on sales performance in terms of monthly new passenger vehicle registration for each car brand-model, in every segment, body group, and generation year across the five European countries from R.L. Polk & Co. We obtained data on country-specific brand level monthly advertising spending for all car brands from Nielsen Company. Advertising spending figures in France, Germany, Italy, and Spain are expressed in euros and those in the United Kingdom are expressed in British pounds. We converted pounds to euros using historical exchange rates. We use the same sources mentioned previously to obtain information on the brands' F1 participation, spending, and performance. Finally, we obtained information on the monthly number of television viewers who watched the live F1 races in every country of our sample from Eurodata TV Worldwide.[12] Descriptive StatisticsTables 2 and 3 provide means, standard deviations, and correlations among different variables we used in the breeding and branding analyses, respectively. Patent citations (innovation performance) are positively correlated with a manufacturer's F1 participation (r =.252), F1 spending (r =.479), and R&D spending (r =.094) but negatively correlated with a manufacturer's F1 performance (r = −.111). A brand's new vehicle registrations (sales performance) are positively correlated with a brand's F1 participation (r =.079), F1 spending (r =.113), F1 performance (r =.048), and advertising spending (r =.495).GraphTable 2. Descriptive Statistics for the Variables Used in the Breeding Analysis.  4 Notes: All variables in the breeding analysis are measured at the manufacturer-global-year level.GraphTable 3. Descriptive Statistics for the Variables Used in the Branding Analysis.  5 Notes: All variables in the branding analysis, except F1 participation, F1 spending, and F1 performance, are measured at the brand-country-month level. F1 participation is measured at the brand-global-year level and F1 spending and performance are measured at the brand-global-month level. Empirical Analysis Breeding Analysis Model specificationIn line with prior studies (e.g., [ 6]; [22]), we used a one-year lag between R&D spending and patent citations. Because we consider a gear manufacturer that competes in a sports contest as a resource that creates a parallel path of R&D, we consider a one-year lag also for the effect of a gear manufacturer competing in F1 on patent citations. Specifically, we modeled innovation performance, measured as the number of patent citations (InnPerfmy), of manufacturer m in year y as follows: ln(InnPerfmy)=μm+θ1F1my−1+θ2ln(R&Dmy−1)¯+θ3[F1my−1×ln(R&Dmy−1)¯]+ϵmy, Graph1where μm denotes a manufacturer-specific fixed effect, F1my−1 denotes manufacturer m as a gear contestant in F1 in year y − 1, expressed in terms of F1 participation (F1partmy−1), log of F1 spending (ln[F1spendmy–1]) or log of F1 performance (ln[F1perfmy–1]), R&Dmy−1 denotes the manufacturer m's R&D spending in year y − 1. We log-transform F1 spending, F1 performance, and R&D spending to allow for their decreasing marginal returns on the manufacturer's innovation performance. As we operationalized being an F1 contestant in three different ways, we estimated Equation 1 three times, each time with another operationalization of the F1 variable (participation, spending, or performance). Because we mean-centered ln(R&Dmy–1), θ1 captures the effect of F1 participation, spending, or performance for a manufacturer with mean level of R&D spending; θ2 captures the simple effect of R&D spending; θ3 captures the interaction between F1 participation, spending, or performance and R&D spending; ∊my∼N(0,σ2∊) and is the error term. We estimate the model in Equation 1 using ordinary least square regression. Estimation resultsTable 4 reports the parameter estimates of the breeding model. Columns 3, 5, and 7 contain the parameter estimates of the model that includes only the main effects of competing in F1 and R&D spending, but not the interaction between the two, when the F1 variable is measured as F1 participation, F1 spending, and F1 performance, respectively. It shows that the main effect of competing in F1 (i.e., the effect of competing independent of the R&D level) on innovation performance is significant when competing in F1 is operationalized as F1 participation or F1 spending, but not when it is operationalized as F1 performance. Therefore, we can confirm H1, except when we operationalize competing by performance.GraphTable 4. The Effects of Manufacturers Competing in F1 on Innovation Performance (Breeding Analysis).  6 *p <.10.7 **p <.05.8 ***p <.01.9 aBecause R&D spending per employee is mean-centered, the simple effect of F1 participation, spending, or performance denotes the effect for a manufacturer with mean level of R&D spending per employee.10 Notes: N.A. = not applicable.Columns 4, 6, and 8 of Table 4 contain the parameter estimates of the full model that includes the simple effects of competing in F1 (θ1) and R&D spending (θ2) as well as the interaction effect between them (θ3) when the F1 variable is measured as F1 participation, F1 spending, and F1 performance, respectively. Note that one cannot interpret these simple effects as main or marginal effects of F1 involvement, in the presence of the interaction effects between F1 and R&D spending (i.e., they cannot be used to test H1). We find that the interaction effect (θ3) is positive and significant at the.01 level in all three models.To interpret these findings, we plotted the effects of a car manufacturer competing in F1 on its innovation performance for the 5th percentile (€5,567) to the 95th percentile (€35,989) of annual R&D spending per employee in our data set (mean R&D spending per employee = €18,177). Figure 2, Panel A, shows the effect of manufacturers' participation on innovation performance across different levels of R&D spending. We obtained the mean effects (see the solid line) and the 95% confidence intervals (see the dotted lines). Similarly, Figure 2, Panels B and C, respectively plot the effect of 1% increase in F1 spending and the effect of 1% increase in F1 points on innovation performance across different levels of R&D spending. The breeding effect is significant at the 5% level when both the dotted lines indicating the 95% confidence interval are above or below the x-axis. All three figures show that there is a synergistic effect of car manufacturers competing in F1 and R&D spending. Combining the simple effect of F1 and the interaction effect between F1 and R&D spending, we find a significant, positive effect of F1 participation, spending, and performance on innovation performance for manufacturers with high (above-mean) levels of R&D spending (specifically, greater than €18,200, €17,800, and €23,500 annual R&D spending per employee or €3 billion, €2.9 billion, and €3.8 billion annual R&D spending for F1 participation, F1 spending, and F1 performance, respectively). This finding supports H2.Graph: Figure 2. Interactions between competing in F1 and R&D spending per employee on innovation performance.Notes: The dotted lines represent 95% confidence intervals. Innovation performance is measured as patent citations. Branding Analysis Model specificationWe modeled sales performance, measured as the number of new passenger vehicle registrations, (SalesPerfijt) for brand i in country j at month t, as follows: ln(SalesPerfijt)=αij+β1F1it+β2AdGWijt+β3(F1it×AdGWijt)+β4[F1it×ln(Viewersjt)¯]+β5∑k=012NPIijt−k+β6ln(CompSalesPerfijt)+∊ijt, Graph2where αij denotes the brand-country fixed effect, which captures time-invariant brand-specific and country-specific effects.[13] Incorporating such fixed effects alleviates the risk of endogeneity arising from idiosyncratic variations in brands (e.g., mainstream vs. niche brands) and countries ([40], p. 602). F1it denotes the brand's manufacturer being a gear contestant in F1, which we operationalize as either F1 participation (F1partit), log of F1 spending (ln[F1spendit]) or log of F1 performance (ln[F1perfit]) of brand i in a particular month t, and AdGWijt is the advertising goodwill of brand i in country j in month t. We employ the standard [37] exponential decay goodwill model for each country-brand combination. Specifically, we model the goodwill as  AdGWijt=ρAdGWijt−1+Adijt  , where Adijt is the advertising spending of brand i in country j at month t, and ρ is the carryover parameter, which we find using a grid search ([36]).[14] The squared-root term accounts for the decreasing marginal returns from advertising spending. Viewersjt denotes the number of people watching the live F1 races in country j in month t. We interact Viewersjt with F1it because we expect the effect of F1 TV viewership on sales performance to depend on the extent to which the brand competes in F1 (e.g., if it participates, spends a certain amount, and has won a certain number of points). NPIijt denotes the number of new products introduced by brand i in country j during the month. We model the effect on sales performance of new products introduced by the brand during the last 12 months (i.e.,  ∑k=012NPIijt−k  ). CompSalesPerfijt denotes the number of registrations of all other brands in the country.β1 captures the main/simple effect of the brand competing in F1 on sales performance, β2 denotes the main/simple effect of advertising goodwill on sales performance, and β3 denotes the interaction effect between the brand competing in F1 and advertising goodwill. Because we mean-center ln(Viewersjt), β1 and β3 capture the simple effect of the brand competing in F1 and interaction effect between the brand competing in F1 and advertising goodwill respectively for an average level of F1 TV viewership. Moreover, β4 captures the effect of the number of viewers for gear contestants depending on the level of the brand competing in F1, β5 denotes the effect of new product introductions during the past 12 months, β6 denotes the effect of competitors' sales performance, and ∊ijt is the error term.In Equation 2, advertising spending may be endogenous to sales performance because the unobservable monthly shocks in car registrations may be correlated with those affecting advertising spending (e.g., an important sales exhibition may occur in a given month in a given country). We model such endogeneity by including an instrumental variable and by modeling the error terms of the sales performance (Equation 2) and advertising spending equations (see Equation 3) to be correlated with each other. We use the brand's total monthly advertising spending in the other four countries as the instrumental variable. The brand's advertising spending in other countries is likely to be related to the brand's advertising spending in the focal country because manufacturers allocate advertising budgets across countries ([ 7]; [24]). However, we expect the advertising spending in other countries to be exogenous to sales performance in the focal country. The advertisements in each of the five countries in our analysis were most likely in different languages (i.e., French in France, German in Germany, Italian in Italy, Spanish in Spain, and English in the United Kingdom). Moreover, manufacturers customize advertising content to local markets (e.g., [44]). To check whether the advertising spending in other countries is a reasonable instrument for advertising spending, we carried out an auxiliary regression in which we used the log of advertising spending as the dependent variable and the log of the total advertising spending in the other four countries as the independent variable. The R2 of this regression is.590, indicating that using advertising spending in other countries to account for advertising endogeneity is a reasonable instrument ([51]).In addition to accounting for the endogeneity of advertising spending, we are interested in examining the effect of how the brand competes in F1 on advertising spending (see Figure 1). In line with this, we modeled the advertising spending for brand i in country j in month t (Adijt) (in logarithmic units) as follows: ln(Adijt)=γij+δ1F1it+δ2[F1it×ln(Viewersjt)¯]+δ3NPIijt+δ4ln(Adijt−1)+δ5ln(SalesPerfijt−1)+δ6ln(CompAdijt)+δ7ln(∑j′≠jAdij't)+ξijt, Graph3where γij denotes a country-brand specific fixed effect to account for time-invariant brand-specific and a country-specific advertising level, Adijt−1 denotes lagged advertising spending, SalesPerfijt–1 denotes lagged sales performance, CompAdijt denotes advertising spending of all other brands in the country during a particular month to account for competitive pressure in advertising spending or a common trend in advertising patterns, and  ∑j′≠jAdij't  denotes the total advertising spending for brand i in all four countries other than the focal country j in month t, which is the instrumental variable for advertising. We model the error terms of Equations 2 and 3 to be jointly distributed as  [∊ijt ξijt]′∼N(0,Σ)  . We estimate these equations using seemingly unrelated regression technique as suggested by [40], p. 591). Similar to the breeding analysis, we estimated the branding model three times, each time with another operationalization of the F1 variable (F1 participation, F1 spending, or F1 performance). Estimation resultsTable 5 reports the parameter estimates of the branding model. Columns 4, 6, and 8 contain the estimates of the model excluding the interaction effect between competing in F1 and advertising goodwill. Examining the parameter estimates of the sales performance equation (see the upper part of Table 5), we find for the main-effects-only model that the main effect of competing in F1 (i.e., the effect of competing independent of the level of advertising goodwill) is significant if we operationalize competing as participation and spending, but not in the case of performance. Therefore, H3 is confirmed, except when we operationalize competing as performance.GraphTable 5. The Effects of Brands Competing in F1 on Sales Performance and Advertising Spending (Branding Analysis).  11 *p <.10.12 **p <.05.13 ***p <.01.14 Notes: N.A. = not applicable; IV = instrumental variable.In the full model (see columns 5, 7, and 9 of Table 5), the simple effects of being a gear contestant (β1) and advertising goodwill (β2) are positive and significant at the.01 level in all three models. The size of the simple effects is calculated taking into account the level of the other independent variable. Because the models include an interaction effect between competing in F1 and advertising goodwill, we cannot separately interpret the simple effects (i.e., the simple effects do not offer an appropriate test of H3). The interaction effect between competing in F1 and advertising goodwill (β3) is negative and significant at the.01 level in all three models.To interpret these findings, we plotted the effect of competing in F1 on sales performance for the 5th percentile (€0)[15] to the 95th percentile (€21.8 million) of monthly advertising spending in our data set (assuming equal values of past advertising goodwill). In Figure 3, Panel A, we show the effect of a brand's participation in F1 on sales performance across different levels of monthly advertising spending. Similarly, in Figure 3, Panels B and C, we plot the effect of a 1% increase in F1 spending and the effect of a 1% increase in F1 points on sales performance, respectively, across different levels of monthly advertising spending. We obtained the mean effects (solid lines) and the 95% confidence intervals (dotted lines). The branding effect is significant when both the dotted lines indicating the 95% confidence interval are above or below the x-axis. These findings indicate that higher advertising spending lowers the effect of the brand competing in F1, thus supporting H4. This suggests a substituting effect between competing in F1 and advertising spending. Nevertheless, all brands have a positive branding effect from F1 participation and F1 spending and brands that spend less than €10.6 million on monthly advertising also have a positive branding effect from F1 performance.Graph: Figure 3. Interactions between competing in F1 and advertising spending on sales performance.Notes: The dotted lines represent 95% confidence intervals. Sales performance is measured as vehicle registrations.In addition to the effects of F1 and advertising, we find that β4 is positive and significant at the.01 level, indicating that an increase in the number of F1 TV viewers strengthens the positive effect of the brand competing in F1 on sales performance. β5 is positive and significant at the.01 level in all three models. This indicates that sales performance has a positive relationship with the number of new products introduced during the last 12 months. Competitors' sales performance (β6) is positively related to the sales performance of the focal brand and is significant at the.01 level for all three models. This could capture the trend in automobile registrations for all brands in the country.The lower part of Table 5 presents the parameter estimates of the advertising equation (Equation 3). We find that δ1 is positive and significant at the.01 level for the model in which competing in F1 is measured as F1 participation, positive and significant at the.10 level for the model in which competing in F1 is measured as F1 performance, and insignificant for the model in which competing in F1 is measured as F1 spending. This indicates that brands spend more on advertising when they participate in F1 or perform well in F1.Furthermore, regarding the control variables in the advertising equation, δ2, δ3, and δ5 are not significant. Lagged advertising spending (δ4) has a positive and significant (p <.01) effect on current advertising spending, for all three models. δ6 is positive and significant for all three models at the.01 level. This denotes that an increase in advertising spending of other brands in the country leads to an increase in the focal brand's advertising. This may capture either a competitive response or a trend in advertising spending among all brands within a country. Finally, we find that the instrumental variable (δ7) is positive and significant for all three models (p <.01), denoting that advertising spending in other countries has a significant effect on the brand's advertising spending in the focal country. Robustness ChecksWe checked the robustness of our results in four ways. First, we excluded data points from the breeding analysis in which the manufacturer outsourced the gear (engine) used during a racing season (see Table A1 in the Web Appendix). Specifically, although BMW withdrew from racing in 2010, the BMW Sauber F1 team competed using Ferrari engines ([38]). Similarly, Jaguar used Cosworth engines and Lotus used Cosworth, Renault, and Mercedes engines during the years they competed in F1. Second, we allocated an equal amount of F1 spending to Ford and Jaguar during 2003 and 2004, when both the manufacturer's brands participated in F1, and reestimated the branding model with F1 spending (see Table A2 in the Web Appendix). Third, we excluded the niche brands that competed in F1 during our sample period (Ferrari, Jaguar, and Lotus) from our branding analyses to examine whether we observe the negative moderation effect between a manufacturer being a gear contestant and its advertising spending due to the differences between large and small brands in our sample (see Table A3 in the Web Appendix). Finally, although we treat breeding and branding analyses differently, we checked for the robustness of the inclusion of innovation performance and R&D spending in the branding analysis. Following [ 6], we employed a three-year lag for the effect of R&D spending and a two-year lag for the effect of innovation performance (see Table A4 in the Web Appendix). We note that our findings are robust to all the aforementioned changes. The results of these analyses reaffirm our main findings that the manufacturer's R&D spending and competing in F1 are complementary of each other whereas the brand's advertising spending and competing in F1 are substitutes. Discussion Managerial ImplicationsWe provide useful insights for managers and analysts, specifically those in the automotive industry, including tier 1 suppliers in that industry, and more generally to those in sports gear industries, for which being involved as a gear contestant in sports competitions is a relevant consideration. First, we show that manufacturers with higher R&D spending stand to gain more from the breeding consequences of investments in sports competitions. For example, we show in Figure 2, Panel A, that car manufacturers that spend at least €3.8 billion annually on R&D (e.g., BMW, Honda) benefit from competing in F1, while manufacturers that spend less than that (e.g., Fiat, Renault) do not. Thus, if manufacturers decide to invest in F1 to enhance their patent base, they have to complement it with a high R&D budget to fully exploit the innovation potential that F1 offers.Second, we show that a gear manufacturer's brand competing in sports contests and the gear manufacturer's advertising spending for that brand are substitutes in inducing an increase in sales performance of that manufacturer's brand(s). The branding returns are the largest among brands that have the lowest advertising (e.g., Ferrari, Jaguar, Lotus). Competing in a sports contest clearly helps the gear manufacturer build its brand by showing its products and brand(s) in a relevant context. Therefore, manufacturers do not have to complement competing in sports contests with a large advertising budget.In summary, our findings may guide manufacturers in budget allocation decisions on sports competitions, R&D, and advertising. Our two main findings imply that firms that already spend a lot on advertising and relatively little on R&D have much less to gain from being a gear contestant in sports competitions as compared with firms that spend little on advertising and spend a lot on R&D. Thus, research-intense firms have more to gain from investing heavily in sports competitions as a gear contestant, as compared with advertising-intense firms. This study provides primary evidence from the automotive industry but is generalizable in logic to other industries. Both skiing and cycling, for example, have prime competitions of similar status as F1 in automotive to which our conceptual framework would generalize. Theoretical ImplicationsOur study adds to the literature on investments in sports competitions as follows. First, it shows that firms may obtain breeding and/or branding returns from competing in sports contests, whereas previous literature examined branding returns from only sponsoring and, thus, offers a partial view, at best. Second, our study conceptualizes how competing in a sports contest is inherently different from merely sports sponsoring. It also provides an analytical framework for estimating the returns for firms that compete in sports contests and provides the first estimates of such returns ever reported in the scholarly literature. Third, our findings also add to the RBV theory, as we show a new type of resource (i.e., owning a manufacturer team) as well as new type of capability (i.e., competing in sports contests), that together may lead to a competitive advantage (i.e., breeding and branding effects). Fourth, our evidence of significant interaction effects between different manufacturer resources suggests that the returns to a manufacturer's resource should not be studied in isolation but in combination with other resources that could be exploited to achieve the same outcome, which is in line with the RBV. Specifically, we report that a gear manufacturer's R&D spending strengthens the effect of the relation between competing in sports contests and its innovation performance, while a gear manufacturer's advertising spending for a competing brand weakens the effect of the relation between competing and the brand's sales performance. Competing in F1 and advertising heavily at the same time is less effective. We are the first to empirically demonstrate that saturation effects occur even across greatly dissimilar exposure vehicles (in our case, car advertising and competing in F1). This complements prior literature that has demonstrated such saturation effects only among fairly similar exposure vehicles (e.g., [56]). It may also contradict managerial practice to leverage sports investments with greater advertising spending. Research Agenda for Investigating Outcomes of Investments in Sports CompetitionsAs with any first exploration of a new phenomenon, several interesting future research directions remain, specifically for studies focusing on competing in as well as sponsoring sports contests. First, one could test the conceptual framework used in this article in another context, such as the Tour the France, or in other markets (e.g., emerging countries) to show the generalizability of the breeding and branding effects of competing by gear manufacturers in sports contests.Second, a useful extension of the current study would be to examine and compare branding effects (e.g., brand sales performance) between competing in and sponsoring of a sports contest. So far, studies have investigated the branding effects for gear and nongear sponsors separately, while our study focuses on the branding effects of gear contestants only. A comparison of the branding effects and the underlying theories that drive potential differences in consumer responses to sponsoring and competing might provide valuable new insights.Another interesting research topic related to the branding effects might be the extent to which a specific link in manufacturers' advertisements to the investments in the sports competitions (e.g., ""We sell on Monday what we race on Sunday,"" or in relation to success, ""If we can do it there, we can do it everywhere"") would positively elevate the branding effects. A related issue would be to investigate the mediating effect of advertising spending on the relationship between competing in sports contests and sales performance, especially when the relationships between the variables are nonlinear in nature.A fourth avenue for further research is to investigate the extent to which being a gear sponsor, rather than a gear contestant, would also lead to breeding effects in terms of a better innovation performance, and if so, if these breeding effects are comparable to, or stronger than, or weaker than the breeding effects of gear contestants. So far, studies on gear sponsors have only provided evidence for a positive branding effect (e.g., [13]), while breeding effects of being a gear sponsor have been totally neglected. However, because gear sponsors do collaborate with athletes to develop new products (e.g., Wilson collaborated with Roger Federer to develop new tennis rackets [[ 5]]), it is relevant to investigate whether and to what extent these collaborations between the gear sponsor and the sponsored athletes entails breeding effects (e.g., patents or patent citations) for the sponsoring manufacturer. And, relatedly, it would be worth investigating what role the strength of the linkage between the manufacturer's R&D and the sponsored or competing team's R&D plays in developing impactful corporate patents.To conclude, as many manufacturers have increasingly been involved in sports competitions over the past few centuries, either as a contestant or a sponsor, the return on these investments has become an important management priority. Although academic research has covered several relevant branding issues related to manufacturers being a sponsor, the aforementioned research areas suggest that the outcomes of manufacturers' investments in sports competitions would still be an important research area for years to come. "
37,"Gift Purchases as Catalysts for Strengthening Customer–Brand Relationships Gift giving is an effective means to strengthen interpersonal relationships; it also may initiate and enhance customer–brand relationships. Through a field study conducted with an international monobrand retailer of beauty products, a combination of propensity score matching with difference-in-differences estimations, and two experimental scenario studies, this research demonstrates that gift buyers spend 63% more in the year following a gift purchase than a matched sample of customers who purchase for their personal use. Specifically, gift buyers increase their purchase frequency (25%), spend more per shopping trip (41%), and engage in more cross-buying (49%). The sales lift is particularly pronounced among new customers. Identity theory suggests customer gratitude and public commitment as mediating mechanisms. Gift purchase design characteristics (i.e., assistance during gift purchase and branded gift wrapping) influence the strength of the mediating mechanisms.KEYWORDS_SPLITGift purchasing is an intriguing consumer behavior with great economic relevance; in the United States alone, consumers spend an average of $1,851 annually buying gifts for their family and friends ([71]). A gift purchase, or buying a product to give to another person through some form of ritual presentation ([ 3]), constitutes a special buying occasion and a relational investment ([13]; [39]). By giving and receiving gifts, people create and sustain their mutual interdependence ([61]), in that each gift creates social indebtedness that persists until the next instance of gift giving, when the recipient and donor can reverse roles. By evoking gratitude-based reciprocity ([51]), gifts act as relational catalysts that nurture and consolidate interpersonal relationships long after the actual gift exchange ([14]).Gifts not only create and reinforce social ties but also bond the donor to the gifted brand. Gift purchases often have symbolic meaning and evoke situational involvement, complex decision-making processes, and increased willingness to pay ([22]). They may engage customers deeply with the gifted brand ([37]) and promote their future brand loyalty ([72]). Yet empirical research has not addressed the consequences of gift purchases for building and sustaining customer–brand relationships. [ 7] links customers' past gift spending to future personal and gift spending, but otherwise, we know of no research that investigates the effects of gift purchases on customers' brand relationships.To advance understanding of this relevant and largely overlooked effect, we explore within a monobrand retailing context the link between gift purchases and customers' attitudes and future purchase behaviors toward the gifted brand. In so doing, we develop theoretical insights that can help managers leverage gift purchases as brand relationship catalysts. In contrast with most relationship marketing instruments (e.g., loyalty programs), encouraging customers' gifting behavior does not trigger any considerable costs and instead generates instant returns.We extend a relationship marketing perspective to the intriguing consumer behavior of gift purchasing with a mixed-method approach that combines a field study and two experimental studies employing monobrand retailing contexts. In a field study, we test whether gift purchases strengthen customers' brand relationships, using transactional customer data to quantify the performance ramifications of gift purchases. We further disentangle the effects on purchase frequency, spending per shopping trip, and cross-buying to evaluate how a gift purchase affects future buying behavior. Then, in a first experimental scenario study, we test our conceptual model to explain why the catalytic effect occurs. Gift purchasers, compared with consumers who purchase items for their personal use, appear to form stronger attitudes toward the brand, which enhance their future purchase behavior. In turn, we identify enhanced customer gratitude and public commitment as psychological mechanisms to explain the gift purchase–future purchase behavior link in a controlled setting. Moreover, we specify when gift purchases have more pronounced effects on future purchase behavior by considering prior customer relationships as a contingency variable. In a second scenario experiment, we examine how different gift purchase design characteristics (i.e., assistance during gift purchase and branded gift wrapping) differentially influence the attitude-strengthening mechanisms.With this unique and novel perspective, we contribute to marketing research and practice in three main ways. First, we theoretically propose and empirically demonstrate a catalytic effect of gift purchases for customer–brand relationships. With objective sales data from an international monobrand retailer of beauty products, we show that the mere act of purchasing a gift (cf. nongift purchase) leads to subsequent sales increases of 63%. In a post hoc analysis, we also determine that gift purchasers engage in more shopping trips (25% lift), more spending per shopping trip (41%), and more cross-buying (49%). Second, building on identity theory, we establish customer gratitude and public commitment as mediating mechanisms that explain the impact of gift purchases on customers' attitude strength and future purchase behavior. Third, we show that gift purchases are especially effective among less experienced buyers who engage in few or no shopping trips prior to their gift purchase. This insight is valuable for marketing practice because infrequent customers often represent a substantial proportion of firms' customer bases and are difficult to attract with traditional relationship marketing tools. Finally, we demonstrate that different gift purchase characteristics evoke unique psychological responses: when purchasers receive assistance during their gift selection process, they experience greater gratitude. A gift packaged in the provider's branded wrapping paper brings the customer's public commitment to the fore. By clarifying the catalytic effects of gift purchases for customer–brand relationships, this research thus offers novel insights for marketing research and practice. The Self-Enhancing Nature of Gift PurchasesTo shed light on the catalytic effect of gift purchases on customer–brand relationships, we contrast gift purchases with personal use purchases (Table 1) and delineate the implications of their differences for a purchaser from an identity perspective, which then serves as the theoretical foundation for our conceptual model development. Gift purchases involve buying a product (good or service) to give to another person as a present (i.e., no expectation of monetary compensation), using some sort of ritual presentation ([ 3], [ 4]; [ 6]). The primary objective is to please the gift recipient, for the purpose of ""establishing, defining, and maintaining [an] interpersonal [relationship]"" with him or her ([ 4], p. 100; [48]). In turn, a personal use purchase involves buying a product for personal consumption, with the underlying motive of satisfying one's own individual needs or wants.GraphTable 1. Differences Between Gift Purchases and Personal Use Purchases.  Gift purchases and personal use purchases are fundamentally distinct because of their differential level of interpersonal risk involved. Gift purchases constitute a relational investment and an effective way to nurture the interpersonal, social tie with the gift recipient ([13]). Thus, donors sense substantial interpersonal risk and may even feel anxiety when purchasing gifts ([79]). Gift purchases can affect the recipient's perceptions of the donor, such that a successful gift can evoke strong bonding, whereas an inappropriate gift can disrupt the interpersonal relationship between the donor and recipient ([62]). Products bought for personal use, even if they are visible to others, have only limited potential to threaten social, interpersonal relationships. The interpersonal risk that gift purchasers sense stems from two important challenges, such that their gift needs to ( 1) meet the recipient's preferences (choice accuracy risk) and ( 2) signal appropriate relational meaning to the recipient (signaling risk) ([39]).First, gift purchasers experience considerable choice accuracy risk, because identifying and selecting the right product is more complex and challenging in a gift purchase than in a personal use purchase situation. For gift purchases, the buyer and consumer diverge; the buyer is not the ultimate user. Gift buyers may have limited knowledge of recipients' preferences or might even feel forced to choose a product that is incongruent with their own taste in an effort to please the gift recipient ([48]; [77]). Given this enhanced complexity, gift purchasers are more activated and devote more cognitive effort to their purchase decision, such that they invest more time, visit more stores, study more information, and engage in more advice seeking ([16]; [28]). In contrast, even if the search effort is high when buying for personal use, the product selection overall is easier, because the buyer knows or develops individual preferences in the product selection process, rather than guessing at somebody else's preferences.Second, gift purchases involve notable signaling risk. Through gifts, donors try to signal their empathy, thoughtfulness, and closeness to and intimate knowledge of the gift recipient ([78]). Moreover, gift purchasers want to convey generosity and thus tend to display a higher willingness to pay ([22]; [44]), using gift prices to signal to the recipient the importance that the donor attaches to the relationship ([75]). Signaling in gift purchasing is both direct, in the one-to-one relationship between donor and recipient, and indirect, relative to a wider audience (e.g., when publicly presenting a gift at a birthday party, encompassing people the donor may know personally; [79]). In personal use buying, signaling plays a subordinate role. Even when this function exists, such as for conspicuous consumption instances, it occurs more indirectly (i.e., toward the general public rather than a specific, known person) than in gift giving.Identity literature proposes that the self consists of multiple identities ([68]). Being a purchaser of products constitutes an important component of overall identities; what we buy and consume defines us to both ourselves and others ([21]; [67]). In this research, we contrast two relevant, common consumer identities ([ 5]), gift purchaser (donor) and personal use purchaser, in terms of their impact on customer–brand relationships. A central, deeply ingrained human raison d'être is self-enhancement, such that people strive to maximize positive perceptions of the self ([38]; [70]). Identity theory ([12]; [69]) suggests that some identities are more salient for self-enhancement than others. Specifically, because ""humans are fundamentally a social species"" ([31], p. 96), social interactions and relationships are crucial to the construction, maintenance, and enhancement of the self ([19]; [42]). To make sense of ourselves, we rely on feedback and seek appraisals from relationship partners. Their real, perceived, or imagined reactions exert powerful influences on the self ([70]). Notably, because gift giving is a sensitive, self-relevant act, such that ""objects become containers for the being of the donor, who gives a portion of that being to the recipient"" ([62], p. 159), a consumer's identity as a donor—which entails the immediate involvement of a relevant other (i.e., gift recipient) and, thus, interpersonal risk—is potentially more self-enhancing or ""self-gratifying"" ([ 5], p. 158) than buying for personal use. The Catalytic Effect of Gift Purchases on Customer–Brand RelationshipsA successful gift purchase reinforces and elevates the customer's salient identity as a donor and provides self-enhancement, so it also could have an effect on the strength of the customer–brand relationship.[ 5] We employ an identity perspective ([69]; [70]) to derive the conceptual model in Figure 1.Graph: Figure 1. Gift purchases enhance customers' future buying behavior.Notes: H2a and H2b represent serial mediation hypotheses. Gift Purchases' Catalytic Effect on Customer Attitudes and BehaviorAn effective gift purchase augments an important social identity and evokes a more self-enhancing experience than an effective personal use purchase does ([ 5]; [62]), so a gift purchase may spur stronger reactions toward the brand. Specifically, donors, compared with personal user purchasers, should exhibit enhanced purchase behavior following a gift purchase because acting as a donor constitutes an important identity function, in which the purchaser tries to gain positive feedback and social approval and thereby enhance the self ([19]; [60]). To clarify the underlying psychological processes, we propose that gift purchases trigger donors' ( 1) gratitude and ( 2) public commitment toward the gifted brand, which in turn foster attitude strength and purchase behaviors.First, a gift purchase triggers customer gratitude, defined as ""emotional appreciation for benefits received"" ([51], p. 1). If a customer perceives a greater need for a benefit, his or her gratitude increases ([51]). Donors sense a pressing need to find an appropriate gift. The relevance of gifts to reinforce favorable donor identities and build and sustain social relationships further suggests that finding the right gift entails considerable choice accuracy risk and thus might be arduous ([ 2]), such that finding the right gift represents a solution to a salient problem and a valuable benefit. Therefore, gift purchasers, compared with customers purchasing for personal use, may experience more gratitude toward the brand that provides the gift.Second, gift purchases lead to greater public commitment than purchases for personal use. Public commitment involves publicly taking a positive position toward a brand ([34]; [47]), and a gift purchase is a publicly visible decision, at least to the gift recipient, involving signaling risk. By gifting a branded item, donors indicate that they deem the focal brand appropriate and commit themselves, through their donor identity, to the brand. The brand provides a public means for the donor to convey an identity as a capable gift giver ([62]), which makes the brand a part of the customer's self and bonds them. Personal use purchases instead involve less visibility, so these purchasers likely experience less public commitment and brand connections ([33]).Through increases in customer gratitude and public commitment, gift purchasers should form stronger attitudes toward the brand. We define attitude strength ""as the positivity or negativity (valence) of an attitude weighted by the [...] certainty with which it is held"" ([53], p. 1). Thus, attitude strength consists of two elements: attitude valence and attitude certainty. People's judgments are affected by their feelings ([51]), so if gift purchasers feel a positive emotion like gratitude toward the brand, their attitude valence should increase ([ 1]). Likewise, what people publicly commit to by passing it on to somebody else as a gift still becomes part of themselves and enhances their liking for the object ([ 5]). In addition, feeling gratitude and making public commitments increase people's confidence in their attitudes ([18]; [27]), thereby enhancing attitude certainty as well. Attitudes then guide behavior ([53]). In the context of gift buying, a gifted brand may not always be suitable for personal use (e.g., when a nonsporty person buys a sports gear brand as a gift), impeding personal use purchases. Still, a strong attitude may lead the gift giver to repurchase the brand in future gifting occasions. Thus, strong attitudes likely enhance future purchase intentions and behaviors toward the brand for additional gift and/or personal use purchases ([55]; [56]). H1:  Customers purchasing a gift (vs. product for personal use) display higher future purchase behaviors toward the selected brand. H2:  Customers purchasing a gift (vs. product for personal use) display higher future purchase behaviors toward the selected brand, serially mediated by (a) gratitude and then attitude strength and (b) public commitment and then attitude strength. Leveraging Gift Purchases' Catalytic Effect for Less Experienced CustomersThe effect of a gift purchase might vary according to the prior relationship between the customer and the focal brand, which reflects customers' previous purchase experience, or the number of shopping trips in which they engage before their first gift purchase. Customers with many past purchase experiences likely have more knowledge of and stronger attitudes toward the brand ([24]; [35]), whereas customers with less experience with the brand may have weaker attitudes. The catalytic effect of purchasing a self-enhancing gift then may be more influential for the latter group, whose weak attitudes at the time of their first gift purchase allow for a more notable increase in attitude strength and future purchase behavior.For gift purchasers seeking self-enhancement by reinforcing their donor identity, buying a brand that they have had little experience with is a particularly risky choice. If the gift succeeds (i.e., pleases the recipient), the risky purchase constitutes a positive, transformational relationship event for the donor ([29]). Depending on the customer relationship stage, different events exert varying impacts; initially, relational confidence is lower and risks are higher, so a transformational relationship event is more likely, which can spur strong customer reactions and alter the relationship trajectory. Thus, for less experienced customers early in the brand relationship, compared with more experienced ones, the gift purchase can create a positive transformational relationship event, which more likely induces favorable behavioral changes. H3:  The positive effect of purchasing a gift (vs. product for personal use) on purchase behavior is weaker among customers with more purchase experience. Leveraging Gift Purchases' Catalytic Effect Through Gift Purchase Design CharacteristicsBecause gift purchases foster customers' gratitude and public commitment toward the brand, we propose two design characteristics whose presence likely enhances gift purchasers' corresponding attributions to the brand. First, the focal brand could deliberately reduce the complexity of the gift purchase situation, such as by offering helpful assistance, thereby stimulating increased gratitude among gift purchasers. Supportive behavior by frontline employees increases customer gratitude in general ([ 9]). For gift purchasers in particular, dedicated assistance helps them solve the pressing problem of finding an appropriate gift ([51]). We anticipate a higher level of customer gratitude when the donor receives helpful consultations or assistance during the gift selection process. H4:  Gift purchasers who receive assistance (vs. no assistance) during their gift purchase process exhibit more gratitude toward the brand.Second, the brand can leverage gift purchasers' public commitment by offering branded gift wrapping. Gift wrapping represents an important aspect of ritual presentation ([62]; [63]). That is, high quality gift wrapping services that feature branded packaging (e.g., paper, bags, boxes) help the donor augment the gift presentation. They also make the joint effort by the donor and the brand to please the recipient more salient, to the donor, the recipient, and any potential audience. The donor visibly embraces the brand as a partner that enhances a donor identity, which emphasizes gift purchasers' public commitment to the brand ([27]; [34]). H5:  Gift purchasers who present their gift in branded gift wrapping (vs. in nonbranded gift wrapping) exhibit greater public commitment toward the brand. Study 1: Testing the Catalytic Effect of Gift Purchases in a Field SettingWith Study 1, we investigate the strengthening effect of gift purchases on customer–brand relationships in a field setting, using transactional, real-life data gathered from an international monobrand retailer of beauty products (H1). We also consider whether customers' prior relationship with the monobrand retailer influences the catalytic effect of gift purchases (H3). With a field study, we establish the external validity of our conceptual model and quantify the effect of gift purchases on customer–brand relationships, using actual firm data. Research Setting and DataThe field data come from an international monobrand retailer that produces and sells high-end beauty products through both physical and online stores. The beauty industry is an appropriate context, because gifts account for a high percentage of companies' overall sales ([46]). We analyze customer transactions with a monobrand retailer, such that the manufacturer, retailer, and salespeople all represent the same brand. This setting is well suited for our fundamental research questions, because we avoid commingling the relational effects of gift purchases across distinct manufacturer and retailer brands. The focal company provided information about which products consumers purchased and when, over an observation window from January 2011 to December 2013. A customer is identified in the database if (s)he has provided a valid email address to the monobrand retailer. Each customer included in the database receives a personal ID number. Transactions are matched to customers according to the email address or ID number that they indicate when checking out or the credit card used in previous transactions. The data set contains 84,112 customers, randomly sampled from six markets (United States, United Kingdom, France, Germany, Spain, and Italy). With these transactional customer data, we determine the extent to which gift purchases (vs. personal use purchases) affect donors' future purchase behaviors. Experimental DesignObservational field data provide insights with high external validity, but to use them to assess causal claims, we also must address endogeneity ([25]). In line with recent studies ([10]; [11]; [32]), we apply a quasi-experimental approach that combines propensity score matching (PSM) with a difference-in-differences estimation to test the causality of a gift purchase on future purchase behavior. With PSM, we address self-selection effects ([36]) before we analyze the matched sample using a difference-in-differences estimation that controls for cross-sectional and time-series effects ([45]). By combining PSM and difference-in-differences modeling, we can control for observed and unobserved confounds of the effect of purchasing a gift ([32]).The purchase of a gift constitutes the experimental treatment. We aim to establish whether this treatment causes a certain outcome (i.e., enhanced future purchase behavior) by identifying differences between customers in treatment versus control conditions. That is, we compare customers who purchased a gift (treatment group) with customers who never purchased a gift from the monobrand retailer (control group). Web Appendix A details the construction of treatment and control group from the original sample. We include only customers who made their first documented purchase in the data set during our observation window verified by their create date in the database; for customers in both groups, no prior purchases were gifts. We identify 547 customers who made their first gift purchase during a treatment period aligned with a holiday season (October–December 2012), which accounts for a large percentage of all gifts given each year ([14]; [40]). We coded a transaction as a gift purchase if the shopping basket contained at least one of the following items: a gift set, gift card, or gift wrapping. In the control group, 5,770 customers never purchased a gift but engaged in a transaction during the treatment period. Thus, customers in both the treatment and control groups engaged in purchases during the same period, but their intended uses (gift or personal) varied. Web Appendix B illustrates the windows for the sample selection, PSM, and difference-in-differences modeling. Propensity Score MatchingBecause participants were not randomly assigned to receive the treatment but self-selected into the treatment group, gift purchasers could differ systematically from purchasers in the control group. To account for this potential self-selection bias ([24]; [29]; [36]), we employ PSM and create an artificial control group. First, in a binary logistic regression, we calculate each customer's propensity to buy a gift (see Table 2, Panel A). Second, the matching procedure links each customer in the treatment condition with a statistical twin from the control group, who did not buy a gift but has statistically the same propensity to do so. With a caliper matching procedure, we match each treatment case to its nearest neighbor only if the two propensity scores fall within a prespecified tolerance zone ([17]; [73]). Limiting the propensity scores to differ by a maximum of.001—well below the recommended tolerance zone of.008, according to the [64] rule—we match 541 customers from the treatment condition with customers who never purchased a gift (Table 2, Panel B). Third, we evaluate matching quality and compute percentage reductions in bias (PRB) for the matches. The average PRB for all predictors is 96%, indicating a strong reduction of self-selection biases. Fourth, following [11], we compute standardized differences in means before and after the matching (Table 2, Panel B). After matching, the maximum standardized differences in means are.03, far below the recommend value of.25 ([11]).GraphTable 2. Study 1: Propensity Score Matching Results.  1 **p <.05.2 ***p <.01.3 aIn line with [24], we calculated the PRB using a formula from [58].4 bFor a fine-grained assessment of customers' relationship duration prior to their treatment period purchase, we counted the days between the first documented purchase and the first day of our treatment period (October 1, 2012). Thus, for a customer making his or her first overall purchase 30 days into the treatment period (i.e., around November 1, 2012), a negative relationship duration results (−30).5 Notes: Two-tailed tests of significance. Difference-in-Differences ModelNext, we analyze the matched sample of customers using a difference-in-differences approach, such that we compare sales differences (i.e., posttreatment sales − pretreatment sales) for customers in the treatment versus the control group. This approach controls for both customer characteristics that are time invariant and time trend effects that could confound the causal effects of a gift purchase ([ 8]; [26]; [32]). We use the following specification to test H1: (1)Salesigt=β0+β1Treatmentg+β2Time periodt+β3Treatmentg×Time periodt+β4Countryi+ ∊igt, Graphwhere Salesigt is customer i's sales from group g at time t; Treatmentg is a dummy variable that equals 1 for customers in the treatment group and 0 for customers in the control group; Time periodt is a dummy variable that takes a value of 1 in the posttreatment period and 0 in the pretreatment period; and β3, the interaction coefficient for Treatmentg and Time periodt, represents the primary coefficient of interest, capturing the causal effect of purchasing a product as a gift or for personal use on sales differences ([32]). Furthermore, as individual level covariates, we include five country dummy variables (i.e., Germany, Spain, France, United Kingdom, and Italy), where the United States serves as reference category. We thus capture sales levels for the treatment and control groups between October 2011 and September 2012 (i.e., pretreatment period) and then from January to December 2013 (posttreatment period) (Figure 2, Panel A). With these measures, one year before and one year after the treatment, we avoid seasonal effects.Graph: Figure 2. Study 1: Field study supports gift purchases' impact on future sales. ResultsTable 3 contains the estimation results for the difference-in-differences model. We find that β3 is positive and significant; purchasing a gift versus purchasing a product for personal use leads to a sales increase of $27.43 in the year after the gift purchase, in support of H1. Figure 2, Panel A, depicts the levels and changes in average sales for the treatment and control groups. After matching, in the pretreatment period, we find a nonsignificant difference of $1.01 in average sales; the two groups do not differ in sales levels before the treatment, substantiating the effectiveness of our PSM. After the treatment period, however, gift purchasers exhibit significantly higher average sales levels than nongift purchasers: With sales of $70.67, they spent $27.43 more on average than the counterfactual trend level of $43.24 (i.e., sales of the treatment group if they did not receive the treatment), representing a sales increase of 63%.GraphTable 3. Study 1: The Effect of Purchasing a Gift on Average Sales.  6 **p <.05.7 ***p <.01.8 Notes: One-tailed tests of significance.We also consider how a prior customer–brand relationship might influence the effect of gift purchases on future sales. Using a floodlight analysis, we determine levels of prior purchase experience at which the sales effect is stronger or weaker ([65]). We find that the interaction between the treatment (i.e., gift vs. personal use purchase) and purchase experience exerts a negative, significant effect on sales differences (b = −13.30, p <.05), in line with H3. As Figure 2, Panel B, shows, the sales effect of gift purchases significantly diminishes with more purchase experience, and a gift purchase exerts a stronger effect among less experienced customers. The floodlight analysis identifies a Johnson–Neyman point at a pretreatment purchase frequency of 1.18; the sales effect of a gift purchase is significant for customers who engaged in no more than one shopping trip before the treatment. Robustness ChecksIn line with [32], we conduct robustness checks to confirm the success of our quasi-experimental strategies, including a placebo regression that tests whether the parallel trend assumption holds before the treatment and a second placebo regression that uses a fake treatment group to validate the construction of our treatment group. First, the identifying assumption behind the difference-in-differences approach is that without the intervention, the two groups would behave the same way. To confirm whether the two groups display similar trends in their purchase behavior before the treatment, in a placebo regression, we use sales data from the first half of the pretreatment period (October 2011–March 2012) as ""fake"" pretreatment data and then consider the second half (April–September 2012) as ""fake"" posttreatment data. The β3 coefficient of the interaction between treatment and ""fake"" time period should not be significant if the pretreatment parallel trend assumption holds. As Table 3 shows, the difference-in-differences estimator for the placebo estimation is not significant (2.56, p >.05), affirming the parallel trend assumption for our study.Second, we validated the construction of our treatment group with another placebo estimation that uses a ""fake"" treatment group ([32], p. 101). From among the control group, we randomly chose 50% as fake treatment customers and 50% that remain as fake control customers. A significant difference-in-differences estimator would render the construction of the original treatment group questionable. However, as Table 3 shows, the difference-in-differences estimate of this second placebo estimation is not significant (3.20, p >.05), which helps validate the original construction of the treatment and control groups. Post Hoc AnalysesWe also conducted post hoc analyses of our field data to confirm the robustness of our findings and delineate how a gift purchase affects different facets of future customer behavior. We link gift purchases to other sales-related variables of interest; with additional difference-in-differences estimations, we assess how purchasing a gift affects the number of shopping trips, spending per shopping trip and cross-buying in the year after the treatment period. Overall, purchasing a gift (vs. a product for personal use) translates into enhanced future buying behavior. Gift purchasers return.21 times more often than nongift purchasers, on average. Relative to the counterfactual trend level of.81 shopping trips, it represents a 25% increase in the number of shopping trips. They spend $8.41 more per transaction than nongift buyers, for a 41% increase in spending per shopping trip. Finally, purchasing a gift positively influences cross-buying behavior. The relative gain in the number of product segments among gift purchasers is.42, for a cross-buying increase of 49%. These findings affirm the robustness of our results and also demonstrate that the mere act of purchasing a gift, instead of a product for personal use, has positive effects on various sales-related variables in the year after the gift purchase. Study 2: Testing the Catalytic Mechanisms of Gift PurchasesHaving established gift purchases' positive effect on customers' future purchase behavior, we conduct an experimental scenario study with three goals. First, we replicate and isolate the behavioral effect of gift purchases on the customer–brand relationship in a controlled setting with high internal validity. Second, we demonstrate how the behavioral effect identified in Study 1 unfolds, including the mediating mechanisms that explain why gift purchases affect customers' future purchase behavior (H2). Third, we specify gift purchases' effect in a consumer electronics setting—that is, a different, more utilitarian industry context. Experimental Design, Participants, and ProcedureWe employ a two-group design, such that the two groups vary in the type of purchase: gift or personal use. For the online data collection, we recruited 146 adult respondents from Germany, according to age and gender quotas. We distributed links to the online questionnaire through email and social media. Participation was voluntary, and as an incentive, all participants who completed the questionnaire entered a lottery to win one of two Amazon gift cards (value equivalent to $30). Their mean age was 33 years, and 56% were women. The data confirm that gift giving is a common, relevant consumer behavior; on average, these respondents purchased 18 gifts per year and spent $37.10 per gift.Each participant, assigned randomly to the experimental scenarios, received a short scenario and questionnaire. The scenario described the participant's relationship with a fictitious brand, ""FunTech,"" which produces and sells consumer electronics. All participants were told upfront that they had always been satisfied with the company, so their prior experiences were constant. After the description of their prior relationship with the brand, we measured participants' attitude strength before the manipulation; the two groups did not differ in their attitudes toward the brand (Mgift = 30.66, Mpersonal = 29.30; F =.48, p >.05). Next, both groups had to imagine that they purchased headphones, but for the manipulation, we varied the intended use of the product. Participants in the treatment group were told that they were searching for a product to give as a gift to their cousin, who is a close friend and to whose birthday party they had been invited. In the control group, participants read that they were searching for headphones for their personal use. The price of the headphones was held constant, at $59. A detailed description of the scenarios is in Web Appendix C.The manipulation checks support the effectiveness of our manipulation. On a seven-point Likert-type scale, ranging from 1 = ""strongly disagree"" to 7 = ""strongly agree,"" participants indicated accurately whether they purchased the headphones as a gift (Mgift = 6.63, SD = 1.08; Mpersonal = 1.67, SD = 1.44; t = 23.36, p <.01). The realism check also suggests that respondents easily imagined the described situation (M = 5.75, SD = 1.50). MeasuresWe adapted established multi-item scales to measure customer gratitude ([51]), public commitment ([34]; [47]), attitude valence, attitude certainty ([53]), and purchase intentions ([74]) (see Table 4). In line with [53], we multiplied attitude valence by attitude certainty to capture attitude strength. We assessed the convergent and discriminant validity of our scales using Amos 24.0. As shown in Table 5, all the convergent validity measures exceed their common thresholds. Fornell and Larcker's (1981) criterion confirms discriminant validity.GraphTable 4. Studies 2 and 3: Construct Measures and Item Loadings.  9 Notes: N.A. = not applicable. All items were measured on a seven-point Likert-type scale (1 = ""strongly disagree,"" and 7 = ""strongly agree"").GraphTable 5. Studies 2 and 3: Descriptive Statistics and Correlations.  10 Notes: N.A. = not applicable; AVE = average variance extracted. Study 2 (Study 3) values are reported before (after) the slash symbol (/). Cronbach's alphas are reported on the diagonal (Study 2/Study 3). ResultsCompared with those in the control condition, participants in the gift purchase condition reported higher levels of customer gratitude (Mgift = 4.45, Mpersonal = 3.96; F = 3.91, p <.05) and public commitment (Mgift = 5.51, Mpersonal = 5.01; F = 5.32, p <.05). To test whether these shifts also prompt stronger attitudes and purchase intentions, we use bootstrapping procedures ([30]). We conduct the mediation analysis with the PROCESS macro (Model 80; 5,000 bootstrapped samples) and simultaneously test the parallel and serial mediation properties of the conceptual model. The path coefficients are in Table 6. As recommended by [15], we use one-tailed testing for the directional hypotheses and rely on 90% confidence intervals (CI90%) to test the indirect effects, such that the resulting upper and lower bounds specify the 95% one-sided CI ([23]).GraphTable 6. Study 2: Path Coefficients.  11 **p <.05.12 ***p <.01.13 aWe rely on the bootstrapped 90% confidence intervals (CI), such that the resulting upper and lower bounds specify the 95% one-sided confidence interval ([23]).14 Notes: R2 Customer gratitude = 3%; R2 Public commitment = 4%; R2 Attitude strength = 40%; R2 Purchase intention = 57%. One-tailed tests of significance.Gift purchasers exhibit greater purchase intentions, mediated by customer gratitude and then attitude strength (bindirect_CusGra =.10, CI90% = [.02,.20]), in support of H2a. In line with H2b, gift purchasers also display greater purchase intentions mediated by public commitment and then attitude strength (bindirect_PubCom =.08, CI90% = [.01,.16]). When we control for the mediators, the remaining direct effects of gift purchase on attitude strength and purchase intentions are not significant, indicating that the effect is entirely mediated by customer gratitude and public commitment. The sum of the hypothesized indirect effects is significant (b =.18, CI90% = [.05,.31]), such that the two mediators significantly explain the positive effect of purchasing a gift, rather than a product for personal use, on future purchase intentions. The indirect effect through customer gratitude and then attitude strength explains 58%; that through public commitment and then attitude strength accounts for 42%. Thus, the affective response of customer gratitude and its subsequent influence on attitude strength is slightly more influential for explaining the indirect effect of purchasing a gift on future purchase intentions. Study 3: Testing Design Characteristics for Leveraging the Catalytic Effect of Gift PurchasesStudy 3 provides a second scenario-based experiment to extend our findings in three ways. First, we investigate when the proposed mediating mechanisms driving the behavioral effects of gift purchases are more or less pronounced (H4 and H5). We test two managerially relevant gift purchase design characteristics that monobrand retailers can proactively deploy to affect customer gratitude or public commitment. Second, we substantiate the chain of effects of our mediating mechanisms on attitude strength and then future purchase behavior. Third, we study gift purchases in another relevant retail context (i.e., specialty foods), to enhance the generalizability of our findings. Experimental Design, Participants, and ProcedureTo investigate how different gift purchase design characteristics influence the proposed mediating mechanisms, we focus on gift purchases and consider ( 1) the assistance provided by the focal firm during the gift purchase and ( 2) whether the gift purchaser presents his or her gift to the recipient in branded gift wrapping. The 2 (assistance during gift purchase: yes vs. no) × 2 (branded gift wrapping: yes vs. no) full-factorial between-subjects design includes four experimental groups. For the data collection, we recruited respondents from Prolific (https://prolific.ac/; [54]). Participants who completed the questionnaire received $1.10. Among the 159 respondents, the mean age was 36 years, and 53% were women. Only U.S. respondents were allowed to participate; they indicated they purchased 13 gifts per year and spent $37.31 per gift on average.As in Study 2, each participant received a short scenario and questionnaire. The scenario described the participant's relationship with a fictitious brand, ""NaturalYum,"" which produces and sells healthy and organic foods, snacks, and drinks. The scenario indicated they had always been satisfied with the brand. Next, all participants were told that they were searching for a product to give as a gift to their cousin, who is a close friend and to whose birthday party they had been invited. All groups had to imagine that they purchased a box of different food items for cooking and snacking, personalized to match their cousin's taste and priced at $50, which they presented to their cousin on the day of the birthday party.For the manipulations, we varied the gift purchasing experience. To manipulate assistance during gift purchase, we varied the level of support provided, such that participants in the (no-) assistance condition imagined that a NaturalYum employee (no employee) approached them to provide helpful assistance. To manipulate branded gift wrapping, we asked participants to imagine that their gift was wrapped in branded paper such that NaturalYum's brand logo was printed across the paper and thus visible to others. Participants in the no branded gift wrapping condition read that their gift was wrapped in plain paper that did not feature NaturalYum's brand logo. We report the full manipulations in Web Appendix C.The manipulation checks support the effectiveness of our manipulations. On a seven-point Likert-type scale, ranging from 1 = ""strongly disagree"" to 7 = ""strongly agree,"" participants indicated accurately whether they received assistance from shop personnel during their gift purchase (Massistance = 6.56, SD = 1.25; Mno assistance = 1.90, SD = 1.93; t = 18.09, p <.01) and if they presented the gift in branded gift wrapping (Mbranded wrapping = 6.87, SD =.43; Mno branded wrapping = 1.61, SD = 1.63; t = 27.94, p <.01). The realism check suggests that respondents were easily able to imagine the described situations (M = 6.42, SD =.90). MeasuresWe used the measures for customer gratitude and public commitment from Study 2. For control purposes, we also used the measures from Study 2 for attitude strength and purchase intentions. We assessed the convergent and discriminant validity of our scales in Amos 24.0. As we show in Table 5, all the convergent validity measures exceed their common thresholds. [20] criterion confirms discriminant validity. ResultsWe employ planned contrasts to test the hypothesized effects of assistance (branded gift wrapping) on customer gratitude (public commitment), while holding the other design characteristic constant, respectively. To establish the effect of assistance on customer gratitude, we compare our assistance and no branded gift wrapping condition with our full control condition (no assistance and no branded gift wrapping). Gift purchasers who receive assistance (vs. no assistance) express more customer gratitude (Massistance = 5.72, Mno assistance =4.39; F = 18.75, p <.01), in support of H4. For the effect of branded gift wrapping on donors' public commitment, we compare our branded gift wrapping and no assistance condition with our full control condition. The type of gift wrapping matters, such that donors who present their gift in branded (vs. not branded) gift wrapping exhibit greater public commitment (Mbranded wrapping = 5.77, Mno branded wrapping = 4.66; F = 11.38, p <.01), as we predict in H5. Figure 3 depicts the mean differences.Graph: Figure 3. Study 3: Gift purchase characteristics differentially affect gift givers' psychological responses.Notes: One-tailed hypothesis testing.As controls, we test whether increases in customer gratitude and public commitment translate into higher attitude strength and ultimately higher purchase intentions. In a mediation analysis in the PROCESS macro (Model 6; 5,000 bootstrapped samples), we find a significant, indirect effect of assistance during gift purchase on purchase intentions, mediated by customer gratitude and then attitude strength (bindirect_CusGra =.85, CI90% = [.47, 1.26]). The indirect effect of branded gift wrapping on purchase intentions, mediated by public commitment and then attitude strength, is also significant (bIndirect_PubCom =.46, CI90% = [.19,.77]). ImplicationsGifts are effective relationship-building catalysts. Extant research explores them almost exclusively from an interpersonal perspective, examining their impact on donor–recipient relationships. In contrast, we consider the catalytic effect of gift purchases from a novel relational perspective, with the prediction that the purchase of a gift can initiate and strengthen customer–brand relationships. We study this effect in a monobrand retailing context, because it avoids confounding the relational effects of gift purchases between a manufacturer and a retailer brand. Monobrand retailing is widespread in modern retail environments ([76]), and brands such as The Body Shop, Lush, and Zara rely on monobrand stores, both physical and online. However, this purposeful study design also represents a limitation, such that further studies are needed to generalize the results to retail settings in which the retailer and gift manufacturer are separate brands, as we discuss subsequently.Yet with this approach, we find support for our conceptual model across three different industries (beauty, consumer electronics, and specialty foods), encompassing both hedonic and utilitarian contexts. The empirical studies consistently indicate that purchasing a product as a gift strengthens donors' attitudes and boosts their future purchase behavior toward the gifted brand. Compared with buying a product for personal use, gift purchases enhance customers' gratitude and public commitment, which foster their attitude strength and future purchase intentions and ultimately manifest in higher-cost, more frequent, and more diverse purchases from the gifted brand. In addition to mediating effects, we establish prior purchase experience as an important contingency. That is, the sales effect of gift purchases is more pronounced for customers with less experience with the brand. Furthermore, assistance during the gift purchase process and branded gift wrapping represent two gift purchase design characteristics that augment customer gratitude and public commitment, respectively. Understanding Gift Purchases as Customer–Brand Relationship CatalystsBy establishing gift purchases as catalysts for building and strengthening customer–brand relationships, this study links the gift-giving and relationship marketing literature streams. In addition to the interpersonal relationship between the donor and the recipient, gifts can strengthen the relationship between the donor and the brand. This customer relationship perspective provides a fresh view on gift-giving behavior, which in turn could initiate a new stream of marketing research.In particular, our insights advance literature that refers to gifting as a form of customer engagement ([ 7]; [37]). Laden with symbolic meaning, gift purchases create buying situations with special importance for customers' identity and are critical touchpoints during customer journeys. With so much at stake, gift purchases can deepen the customer relationship with the brand, with positive impacts on key customer metrics such as attitude strength and future purchase behaviors. Thus, our research suggests gift purchases as an opportunity for retailers to engage customers with their brand, especially former light buyers with little purchase experience.We also identify mechanisms that explain why gift purchases strengthen customers' brand relationships. Drawing from identity theory ([12]; [69]), we derive two potential mediators that map onto the choice accuracy and signaling risks involved in gift purchases. Compared with customers who buy products for themselves, gift givers have stronger attitudes toward the brand due to their greater customer gratitude and public commitment. These findings enrich understanding of how customer–brand relationships grow. In line with prior literature ([49]; [66]), we find that both mechanisms simultaneously explain the impact of gift purchases on customers' brand relationships. That is, customer gratitude helps build strong brand relationships during the special purchase situation. For many customers, finding an appropriate gift is challenging and may even trigger gift anxiety ([44]; [79]). If the retail brand can help customers solve this problem and alleviate choice accuracy risk, it earns their emotional appreciation. Gift purchasers, compared with personal use purchasers, experience increased gratitude, which enhances their attitude valence and certainty (i.e., attitude strength) toward the brand, which then promotes future purchases ([51]). Public commitment instead arises in ritual presentation. By presenting gifts to recipients, donors face signaling risks and take a public stand for the purchased brand; the brand becomes a public means to convey the donor's identity as a capable gift giver ([62]). In response to this enhanced public commitment, gift purchasers develop stronger attitudes toward the brand, relative to personal use purchasers, due to increased liking of and certainty about the brand. The stimulation of public commitment and attitude strength then leads to augmented purchase behavior.Finally, we investigate when gift purchases strengthen customers' brand relationships. Gift purchases are distinct; we take different gift purchase design characteristics into account by investigating their impacts on the proposed mediating mechanisms. The assistance received and availability of branded gift wrapping significantly influence donors' psychological responses; their gratitude is higher if they receive assistance, and their public commitment is higher if the retailer packages the gift in branded gift wrapping. We thus advance extant literature by demonstrating that different psychological mechanisms come to the fore, depending on the characteristics of the gift purchase. Leveraging Gift Purchases as Relationship Marketing InstrumentsMarketing managers can leverage gift purchases as effective relationship marketing instruments in monobrand retail settings. We propose five means to do so. First, managers should identify products to position as gifts and promote them as such. Many gift promotions are driven just by holiday seasons, and thus brands fail to exploit their full potential for building strong customer–brand relationships. Marketing managers should systematically highlight selected products in marketing communications and offer promotional incentives for purchasers looking for a gift. The returns on such initiatives can be substantial. In our field study in the beauty industry, gift buyers spent 63% more in the year following their gift purchase than a matched sample of customers purchasing for personal use. Our results also clarify how this incremental spending effect emerges: compared with people purchasing for their personal use, gift buyers undertake more shopping trips (25%), spend more per trip (41%), and cross-buy more (49%). Thus, gift purchases have profound, multifaceted impacts on future buying behavior—a link that does not appear in any prior marketing studies to the best of our knowledge. In contrast with other relationship marketing instruments (e.g., loyalty programs) that require extensive immediate investments and costs in the hope of future returns ([43]), promoting gift purchases demands little additional cost, and it produces a dual effect: instant returns from the focal gift purchase, and then additional sales from gift buyers' expanded future purchase behavior.Second, managers should target new rather than experienced customers with gift purchase promotions, in recognition of the negative interaction effect of purchase experience on the link between gift purchases and future purchase behavior. In our field study, the sales lift is significant for customers who made no more than one shopping trip before their initial gift purchase. Relationship marketing literature already has identified varying effects at different customer relationship stages ([29]; [50]; [80]). In early stages, customers possess weaker brand attitudes and are more receptive to marketing actions ([24]; [35]), so the catalytic effect of gift purchases is stronger among new customers. Gift purchases also might help activate infrequent customers, who often represent a large proportion of firms' customer bases ([57]) but are not addressed by relationship marketing instruments that target more experienced, frequent customers (e.g., loyalty programs).Third, managers should take concerted actions to facilitate customers' gift selection process. Helping customers find the right gift spurs their gratitude, which helps explain the positive impact of gift purchases on the customer–brand relationship. Short-term feelings of gratitude can drive long-lasting performance outcomes in customer relationships ([51]), so retail store managers should train and encourage frontline employees to assist customers proactively in their gift selection process. Firms might also develop advanced online filters to help their customers identify an appropriate product for specific gift-giving occasions. For example, the European online shop Radbag offers a sophisticated ""gift finder"" on its website, enabling customers looking for a gift to filter products according to the recipient and his or her personality (e.g., party animal, globetrotter, workaholic, hipster), their own willingness to pay, and the gift's intended meaning (e.g., romantic, funny, exclusive, innovative).Fourth, as a low-hanging fruit, monobrand retailers should make their brand more prominent on gift packaging. Gift buyers' public commitment emerges as a second mediator that explains the positive impact of gift purchases on brand relationships. By providing high-quality, branded gift packaging, retailers can strengthen their customers' public commitment to the brand and stimulate long-lasting attitudinal and behavioral performance outcomes. Leading firms in various industries such as Hugo Boss, Rosenthal, and Normann Copenhagen rely on branded gift wrapping, which effectively enhances the brands' salience in gift presentation.Finally, retailers should encourage existing customers to recommend the brand to their peers as a good choice for upcoming gift-giving occasions. For example, retailers could provide digital or more traditional wish-list tools that facilitate voicing of customers' gift preferences. In addition to the primary sales effect of the gift, such an initiative could also bring new customers with strong attitudes toward the gifted brand. Limitations and Directions for ResearchWe find support for our overall conceptual model across two research formats and three industry contexts, such that we substantiate our prediction that gift purchases are effective catalysts of customer relationships in a monobrand retailing context. The study limitations also suggest some promising research avenues. First, we shed light on the favorable consequences of gift purchases for customer relationships, by treating the gift purchase as a given and investigating its effects on customers' subsequent attitudes and behaviors. A vital next step is to learn how to stimulate gift purchases among customers in the first place. Extant literature on gift selection provides initial insights into which store attributes become more important when choosing a gift or how, for example, the level of customer anxiety might be reduced (e.g., [41]; [44]).Second, our hypothesis development assumes the gift recipient responds positively to the gift. We consider this assumption justifiable; even if a recipient does not like the gift, social norms require gift recipients to react with gratitude ([59]; [78]). However, gift recipients could express dislike of a gift, such as when the gift giver and gift recipient have a close and trusting relationship in which honesty norms prevail. Studies of how gift recipients' negative responses affect the customer–brand relationship could investigate whether the catalyzing effect on the gift giver's brand relationship is impeded in this case. Depending on the giver–recipient relationship, the intensity of the prior customer–brand relationship, or the quality of the gift purchase experience provided by the brand, the gift giver might disregard the recipient's reaction and identify further with the brand, potentially leading to enhanced bonds.Third, in constructing our treatment and control groups, we included only customers who made their first documented purchase (i.e., became customers) from the monobrand retailer during the overall observation window, to ensure that the treatment and control group customers had made no prior gift purchases and to establish an undiluted effect of the gift purchase. Consequently, customers in our sample are relatively new (i.e., maximum tenure of one year and nine months at the start of the treatment period if a customer made his or her first purchase in January 2011). This necessary prerequisite affirms the unambiguous causal inference of our proposed effect, but it also might limit the generalizability of our findings beyond customers in early relationship stages ([29]). Research that builds on our findings could examine whether the positive effect of a gift purchase holds for customers who have patronized a focal firm for a couple of years before they make their first gift purchase, such that a gift purchase at a later relational stage might take a solidified brand relationship to the next level. Along this line, further research could also examine whether the sequence of purchasing a brand for personal use or as a gift plays an important role.Fourth, in our monobrand retailing context, the producer and retailer of the purchased gifts coincide. When gift products are manufactured by one company and marketed by another, an intriguing research question pertains to which company will reap the benefits of selling gifts to customers ([52]). Will customers change their attitudes and behaviors in favor of the company that produces the gift, the retailer that supports their gift selection process, or both? How can the brand manufacturer and retailer effectively team up to share the responsibility of providing the customer with an effective gift purchase experience (e.g., assistance, gift wrapping) and jointly reap the relational benefits of selling gifts? Further research should investigate gift purchases in which the gift producer and retailer differ to determine their simultaneous effects on the different types of customer relationships.Fifth, further research might explore the impact of gifts on recipients' brand relationships. Similar to purchasing and giving a gift, receiving a gift may initiate and foster a brand relationship, possibly mediated by recipients' public commitment to the brand. Insights into this effect would complement our relational view of gift giving with a third perspective (giver–recipient, giver–brand, recipient–brand). "
38,"Help Me Help You! Employing the Marketing Mix to Alleviate Experiences of Donor Sacrifice Nonprofit organizations often rely on individuals to execute their mission of addressing unmet societal needs. Indeed, one of the most significant challenges facing such organizations is that of enlisting individuals to provide support through the volunteering of time or donation of money. To address this challenge, prior studies have examined how promotional messages can be leveraged to motivate individuals to support the missions of nonprofit organizations. Yet promotional messages are only one aspect of the marketing mix that may be employed. The present study examines how donor-based nonprofit organizations can employ the marketing mix—product, price, promotion, place, process, and people—to influence the experiences of sacrifice associated with donation. The authors do so through an ethnographic study of individuals participating in living organ donation. First, they identify the manifestation of sacrifice in donation. Next, they define three complementary and interactive types of sacrifice: psychic, pecuniary, and physical. Then, they articulate how the marketing mix can be employed to mitigate experiences of sacrifice that emerge through the donation process. The authors conclude by discussing implications for marketing practice and identifying additional research opportunities for sacrifice in the realm of donation.KEYWORDS_SPLITNonprofit organizations contribute $985.4 billion to the U.S. economy ([43]) and serve the public interest by providing a wide array of crucial services, goods, and resources—from food and shelter to body parts. Organizations tend to employ the promotion element of the marketing mix to persuade individuals to donate; however, there may be opportunities to use additional elements. The greatest challenge such organizations have in executing their missions is that of securing sufficient donations from individuals ([ 9]; [67]). All types of donations from individuals entail sacrifice, yet those who provide anatomical parts in support of health care treatments make undisputed sacrifice. Because not all donations are born of the same degree or type of sacrifice, it is necessary to understand sacrifice in relation to donation so that organizations can better overcome this obstacle when recruiting donors. Thus, the question guiding this research is, How can organizations use marketing-mix variables to reduce experiences of sacrifice in donation?Studies on consumer shopping behavior have focused primarily on the monetary sacrifice made to obtain value imparted by organizations through the marketing mix ([30]; [33]; [34]); in contrast, the charitable giving literature has focused on promotion to increase the number of donors and size of donations ([21]; [38]; [49]; [66], [67]). Although there is recognition that the elements of the marketing mix influence shopping behaviors ([34]), there is little insight into how marketing-mix elements—product, promotion, price, place, process, or people—may be employed to support charitable giving. While promotion to attract donors is certainly important, it is likely insufficient to convey the full complement of donations needed. Consider, for example, the variance in degree of sacrifice sought. For some organizations, little effort is required (e.g., church usher, PTA member, Meals on Wheels driver); for others, the sacrifice is more extensive (e.g., Make-A-Wish granter, foster parent, organ donor). The present study examines living organ donation, a process in which one undergoes elective surgery to remove an organ for transplantation into another person. Given that any kind of organ donation represents an extreme form of sacrifice, the transplantation phenomenon serves as an excellent focal point for examining the sacrificial burdens involved in donation and opportunities to overcome them through the marketing mix.This research suggests that different elements of the marketing mix may be used to address sacrifice related to donating behavior. Our findings suggest that a combination of marketing-mix elements may reduce experiences of sacrifice and thereby increase donation behaviors. This research contributes to literature recognizing that consumer reluctance to donate must be overcome ([21]; [38]; [49]; [67]). This reluctance has been addressed by prior research, which has emphasized that promotional messages may be used to procure necessary donations. This study extends scholarship on donation by leading our inquiry beyond that of promotion. Specifically, we describe how sacrifice manifests in the donation process and identify roles for the marketing mix to overcome potential reluctance to make such sacrifices. Relevant to an examination of marketing mix are such variables as product, price, place, process, people, and promotion.In addition, this research contributes an understanding of anatomical parts as a particular type of possession separate from money, time, or other objects. While the donation of anatomical parts has been explored in the social sciences ([53]; [56]; [60]), it is not a focus of marketing literature, though the market for such parts is significant and growing. This research also contributes an understanding of how nonprofits may attract organ donors by more intentionally and systemically overcoming concerns of potential donors. Where prior research has considered donations of money, which can be replenished ([37]; [38]); possessions for which individuals have sentimental attachments ([67]); or time, for which all individuals have the same irreplaceable amount each day ([49]), this research investigates the growing market of anatomical parts for transplantation.In addition, this study offers practical applications by suggesting how marketing-mix elements can be employed to overcome the barriers that may hinder individuals from donating. By better understanding how individuals may experience sacrifice through donation, we provide insights and tools for nonprofit managers focusing on how to use the marketing mix to encourage donation and thereby increase supply to meet demand.To contextualize this study, we begin with a succinct review of the marketing and social science research on donation and sacrifice. We then present our methodology, including an overview of living organ donation within the U.S.-based transplantation market. We close with our findings, followed by a discussion of implications for practice and theory. Theoretical BackgroundNonprofit organizations deliver services to their clients made possible through donations from individuals ([ 9]; [67]). These donations are depicted as gifts of ""life"" or ""hope"" that support others in need ([54]). Such donations are most often provided by individuals who intentionally offer their support without receiving tangible rewards ([26]; [47]; [60]; [65]; [69]). These donations can be categorized as gifts to society that encompass the sacrifice of forgone opportunities ([36]; [41]; [56]). It is worth noting that these contributions are substantively distinct from contributions made to obtain some benefit for the self, such as with ""pay what you want"" pricing approaches ([17]). More specifically, contributions to nonprofit organizations are most often provided to deliver a benefit to others. Next, we provide a brief review of the marketing literature on donation and sacrifice. DonationMarketing and consumer researchers have primarily examined how the promotion element of the marketing mix can be employed to attract donors and increase donations. Studies provide insight into how messages may influence potential donors, turning a lens on the relative importance of the help sought ([23]), the role of individual identity ([14]; [48]), the motives for participating ([65]; [66]), or the impact on the donor ([23]; [66]). The focus of those studies has been to identify and understand conditions by which appeals may arouse sufficient interest for individuals to donate to an organization. Although promotion has a role in transforming individuals into donors, prior research does not illuminate how coupling other marketing-mix elements together with promotion may influence donation.Awareness of opportunities is an important factor in securing donations, particularly in the case of organ donations ([31]; [64]), and leads many organizations to focus on promotion. Knowledge acquisition is certainly a contributing factor for those who choose to donate anatomical parts, yet additional requirements are necessary to transform them into donors. For example, even after passing the first hurdle of developing a desire to donate, potential donors must still qualify to participate ([12]; [60]). Thus, it is necessary to investigate the donation experience to better understand the marketing mix's role in attracting and securing donors.Donations to organizations have been viewed as gifts to society ([11]; [56]; [60]). Like other types of gifts, these are born of sacrifice ([41]; [54]). Importantly, not all donations involve the same degree of sacrifice, as individuals possess several resources they may gift as donations. There are monetary gifts, which are viewed as replenishable and fungible. There are gifts of time, something qualitatively different from money in that time may not be stored or replaced ([23]; [32]). Other possessions that may be donated have value in the degree and source of individuals' attachment to them ([ 7]; [67]). Lacking in this conversation is an understanding of how the marketing mix can address the types or degrees of sacrifice that may be associated with the donation of possessions. SacrificeIn the marketing literature, the concept of sacrifice is focused primarily on price—what consumers give up to obtain value ([18]; [25]; [70]). Beyond money, research identifies consumer sacrifice as the expending of energy, effort, or time ([ 4]; [10]; [15]; [30]; [42]). The degree of sacrifice, conveyed by price, may serve as information to consumers ([19]; [25]), inform perceptions of alternative offerings, or provide indicators of offering quality ([58]; [63]). In addition to the sacrifice one may make to obtain an offering, there is the sacrifice that manifests as a consequence of forgoing other options ([36]; [63]). While individuals may feel minimally burdened by the particular form of sacrifice made, some sacrifices may be deemed too great, thereby reducing a consumer's willingness to purchase an offering ([ 8]; [19]). While price is often equated with sacrifice in the market, there also is recognition within the literature that consumers make sacrifices beyond price to attain desired outcomes.Extra-economic sacrifices are found in investments of time, effort, or energy ([ 1]; [ 8]; [22]; [30]; [70]). Time is a limited and perishable resource. It is most often viewed as that which may be monetized and is perceived as a cost ([ 4]; [27]; [70]), considered in relationship to search and intended patronage ([ 4]; [29]), or viewed as a precursor to attaining desired offerings ([13]; [33]). As a type of sacrifice, time is often described in conjunction with effort. Sacrifices of effort are depicted as labor or inconveniences necessary to attain benefits ([42]; [46]). Effort is evident in the cocreation of market-derived experiences where consumers are active participants ([16]; [20]; [51]; [61]). Sacrifices of effort may include that of choice when individuals opt to provide gifts in response to specific recipient requests ([15]; [39]; [68]).Sacrifices of energy are described as psychic or emotional expenditures encompassing the contemplation associated with a consumption opportunity ([ 1]; [ 3]; [ 4]; [ 6]). While a primary focus in the literature is on monetary sacrifice for value that is conveyed through the marketing mix, it is necessary to examine how the marketing mix can be used to address sacrifice experienced by donors. Although time and effort may emanate from the embodied self, sacrifice of the physical self is less often contemplated. Nonetheless, [22] examine the employment of physical and mental energy to transform a previously used object; [40] considers the physical nature of effort involved in providing relocation assistance; and [35] recognize the physical peril individuals accepted when they secretly shared additional food with other inmates in Nazi concentration camps. Together, those findings illustrate that monetary sacrifice alone may be insufficient for some forms of consumption and that promotions are likely insufficient to overcome sacrifices beyond those of awareness. MethodologyThe purpose of this study is to understand the nature of sacrifice in donation so as to guide organizations in overcoming obstacles to obtain donations. Because living organ donation indisputably involves great sacrifice, it provides a clear context in which to understand sacrifice in relation to donation. Furthermore, an organ must be donated voluntarily and may only be offered as a gift in the United States ([44]; [62][ 4]). Next, we provide an overview of the phenomenon followed by a discussion of data collection and analysis. Phenomenon of living organ donationLiving organ donation is orchestrated by medical personnel and associated transplant centers within the transplantation market. Whereas early transplants relied on organs from deceased individuals, living organ donation is increasing as health care innovations provide opportunities for transplanting organs from living, genetically unrelated individuals ([11]; [50]). Nonetheless, with demand for organs outpacing supply, living organ donors are increasingly sought. Without a transplant, individuals experiencing organ failure may undergo various treatments that sustain life, though often at diminished quality. All clinical costs associated with donation are funded by participating organizations (e.g., organ procurement organizations, insurers, transplant centers within hospitals) and are coordinated by a transplant team ([64]).Individuals may donate one kidney, a portion of their liver, a lung, or part of their intestine. We study the experiences of living kidney donation, as they are the most frequent type. The organ donation process is complex, requiring physiological and psychological clearances of donors. Living donors may be directed, meaning they donate to a known other (e.g., loved one, colleague), or nondirected, thereby donating to an unknown other. Nondirected donors provide an organ to the next individual on the transplant list with whom they are a match, or to support a donor chain. No matter the recipient, the donation process is the same.This process begins with education and culminates with surgery. The organ donation and transplantation process includes informing potential donors about the steps to qualify and the consequences of participation. Once they choose to participate in the process, individuals are assessed for their overall fitness. Qualification begins with procuring an extensive medical history, which provides for an assessment of overall health as well as evidence of current and potential (physical or mental) disease. Next is tissue and blood testing to assess the viability of a match to a recipient. When an individual is identified as a clinical match to a recipient, and it is determined that removing the organ is not likely to be detrimental to the donor, surgery is scheduled.Kidney transplants occur across two surgeries. First is the nephrectomy, removal of the kidney from the donor, a surgery that typically lasts four hours. Next is the transplantation, the insertion of the donated kidney into a recipient, which lasts approximately three hours. Surgery leaves a donor with an immediate and significant degradation of bodily functionality, coupled with the physical trauma of the procedure. Donors are hospitalized on average between two and four days after the procedure, followed by a recovery period at home of two to six weeks. Recipients often emerge from surgery feeling well due to the immediate functionality provided by the transplanted kidney. Both parties are required to participate in follow-up tests to monitor their respective kidney function, though the requirements differ. Data collectionBecause the present study focuses on the experience of living donation, we deemed ethnography to be the most appropriate research method. Given that the process to become a living organ donor is quite extensive, a larger number of people begin the screening process than actually donate. This is due to any number of reasons, including a prospective donor's current or projected health, willingness to proceed through various clinical tests, or decision to terminate the process. To better understand sacrifice within living donation, this study thus examines only those individuals who completed the living kidney donation process.Prior to beginning this study, the authors themselves participated in organ transplantation. The second author made her need known, as advised by her physician. The first author volunteered to be tested and ultimately became the second author's donor. The process, from the precipitating event through recovery, transpired over a period of nine months. Each author recovered without incident. Throughout this process, field notes were captured.Study participants were solicited through clinicians, online living organ donor support forums, and snowball sampling, with varying outcomes. They included individuals from different regions in the United States who participated in both directed and nondirected donations. Each author has a relationship with a nephrologist (kidney physician) with whom they shared the intention of this study. Those physicians were asked to share study information with their patients as they saw fit. The physician provided those patients who expressed an interest in participating in this study with the authors' contact information. Within online donor forums, the first author posted notices inviting willing participants to initiate contact through a social media platform, direct message, or email. In both recruiting approaches, more individuals expressed interest in participating in the study than actually followed through to participate in interviews. No compensation was provided to individuals who participated in this study.Our sample includes 20 individuals representing diversity in race, age, sex, sexual orientation, elapsed time since donation, donor and recipient outcomes, type of donation (i.e., directed, nondirected, or donor chain), and location (see Table 1). The participants include eight nondirected donors and six individuals who had complications or became aware of their recipients' complications. Although statistics indicate that the majority of donors continue to be in good health postdonation, some suffer donation-related complications. Our participants' clinical outcomes range from expected recoveries to varying degrees of acute or chronic physical and emotional disease. Most, but not all, recipients had resumed a healthy lifestyle free of dialysis.GraphTable 1. Overview of Study Participants.  We collected ethnographic data through semidirected phenomenological interviews, participant observation in living donation, and online donor forums. We downloaded the posts of individuals identified in online forums, which often provided an archived timeline of their experience, and these served as projective tasks within interviews. Given our own experiences, we quickly established rapport with study participants.We began interviews by asking individuals to describe how they became a living organ donor. Accounts shared in response to the initial question were probed using emic terms to facilitate interview continuity. In addition to learning of each unique circumstance, we asked individuals to describe how they learned of the need, made the choice to donate, and experienced testing, surgery, and recovery. They were also asked to describe the process, who was involved in the process, the emotional and physiological outcomes for themselves and the recipient (when known), and the timing of the transplant. Interviews ranged in duration from one to four hours, with an average of 90 minutes and some follow-up exchanges on social media and email. Data were collected by phone and through face-to-face meetings at the convenience of participants. Interviews were audiotaped and transcribed. Data analysisInterview transcripts and field notes provide the basis for our analysis and interpretation. Data analysis began with a review of the donation process as described by donors. This review revealed that the donation process was the same for all participants regardless of center type, testing protocol, or surgical method, thus allowing for comparison across phases in the process. Next, codes were generated from readings of the anthropology, theology, market, and consumer research on donation (e.g., time, money, effort). Those initial codes were supplemented with emic terms (e.g., wait, goal, endure) from the initial analysis of the transcripts and field notes.Analysis continued with each transcript being coded. Next, transcripts were analyzed across each phase of the donation process: learning about the opportunity, making the choice to participate, qualifying (i.e., determining the degree of match), and fulfilling the commitment to volunteer (i.e., surgery, recovery, and donor outcomes). In addition, we analyzed transcripts across outcomes in terms of meeting expectations (e.g., successful outcome), exceeding expectations (e.g., easier, faster), or falling below expectations (e.g., poor outcomes for the self or the recipient). Thus, two types of analyses—diachronic (i.e., across the process) and synchronic (i.e., within similar phases or outcomes of the process)—were performed ([ 2]; [57]; [59]).We identified emergent themes through an iterative process comprising analysis of the transcripts, the coded data, and the literature ([57]). Data collection and analysis continued until saturation was attained. We conducted member checking in follow-up discussions and emails with four participants. FindingsWe codify the living organ donation process in three key phases: deliberate, decide, and donate. Through our participants' experiences, we find that the marketing mix is the primary means by which organizations may support the donation process and, in particular, mitigate donor sacrifice that emerges as individuals become donors who offer their possessions for the benefit of others. We identify roles for six marketing-mix elements that aim to manage sacrifice experiences: product, promotion, place, price, process, and people. Furthermore, we identify three complementary and interactive types of sacrifice: psychic, which reflects the employment of mental or emotional energies; physical, which encompasses investments of components and functioning of the bodily self as well as modifications to behaviors; and pecuniary, which comprises investments of possessions, time, or money. We find the each of the three types of sacrifice may emerge during any of the phases within donation (see Table 2).GraphTable 2. Definitions of Sacrifice as Experienced Across the Three Phases of the Donation Process.  In line with this categorization, we find that there may be opportunities for organizations to address the types of sacrifice that may evince across any one of the three phases of the donation process. While both individuals and organizations participate in each of the phases in the process, the degree of relative influence varies, such that the deliberation phase is more heavily influenced by the individual and the donation phase by the organization. Next, we depict participant experiences through data excerpts to illuminate relationships between sacrifice and the marketing mix within each phase of the process. Although the phases are presented as discrete units, the experience is more of a continuum in that data may encompass aspects of more than one phase. DeliberateThe first phase in the process is one of deliberation, in which organizations prominently employ promotion to raise awareness of the donation opportunity. For many donor-reliant organizations, the product and process are entwined in delivering the intended outcomes and associated benefits. Here, too, we find that organizations may benefit when they more fully depict the product as comprising both the donation and the transplantation. Through our informant experiences, we identify roles for the product and the process that, together, provide donors with opportunities to contemplate the benefits and risks of participating (for themselves and for the recipient). The participants in our study come to learn of this particular volunteer opportunity in a variety of ways, from observing a loved one's decline in health to encountering promotional (and public relations) messages. Regardless of the means through which individuals learn of the donation opportunity, they necessarily employ psychic sacrifice to better understand the requirements and implications of participation in the process.One informant, Gregory, initially learned of living organ donation through a story on National Public Radio's This American Life program. He describes how that story prompted him to consider participating as a living organ donor, though he was not moved to act until he received a request for help. While promotion stirred his interest in the product, it was insufficient to motivate action to participate. He learned, through a group email, that his colleague's daughter was diagnosed with end-stage renal disease and was a candidate for transplantation. Though he did not know the daughter, he describes feeling compelled to offer to become her donor:I received an email from [a colleague] on a Sunday morning that his daughter had just gone onto the transplant list....It was a request [saying] that she needed a kidney—he was letting other people know. And [the email] stated her blood type, and it was mine. I spent about an hour wrestling with it, looking for a justifiable reason not to volunteer. And finding none, I decided that I would volunteer to be tested. (Gregory)The information from his colleague, coupled with knowledge garnered from a donor story in the media, compelled Gregory to donate. He describes learning of the opportunity to act along with the awareness of the product and process as integral to awakening his calling. Gregory's acknowledgment of his calling encompasses psychic sacrifices with respect to relinquishing a sense of control over the choice to participate. His sacrifice of choice was not due to any external forces but, rather, an alignment of his choice with his calling.Within the deliberation phase, individuals acquire additional knowledge about the process by which the donation will be used to deliver the product and associated benefits for the nonprofit's client. For most of our informants, the initial information requests are related to the specifics of donation in terms of what they contribute to the product and the process. That often begins with a desire to understand the requirements necessary to participate:I called [a transplant center in my city] just to see if I was even a candidate....I was going to be 61 in February, and I thought quite possibly I would be too old. They said that because of my age I would be considered a marginal donor in their system. I called [another transplant center] where the surgery was to be performed and they said, according to their system, I was fine. So, I began the long evaluation process. (Gregory)Across transplant centers, the product—retrieving a donated organ and transplanting it into one in need—is the same. Gregory pursued the donation opportunity in the face of mobility challenges, legal blindness, and the concern that he may be too old to participate. In fact, when he presented himself to a local center as a donor, he was rejected due to age. While it is uncommon for individuals to comparison shop for a transplant center, there are several instances in our data in which individuals found aspects of a center's process or people to more readily mitigate sacrifices posed by the donation. Thus, individuals might find one center to be more attractive than another, which may influence where or how they choose to participate.When individuals learn about donation opportunities through intimate relationships, as is the case with a spouse or siblings, they may experience a strong desire to donate even before fully understanding the product, process, or its impact on them. That desire also has the potential to stir psychic sacrifice as individuals pursue a known product with little information about the process around it. Wilma's brother was in need of a kidney, yet she had little understanding of what would be required of her. The transplant center personnel began educating Wilma from their first conversation when she requested information on how to become her brother's donor:I just called [the center], and [the transplant coordinator] sent me out my package and we went from there.... I think [my brother and sister-in-law] wanted to control [the process]. I think they just found out that [the transplant center] wasn't going to let them control it anyway. Their blind selection of a donor was to protect both ends, both the recipient and the donor. I felt very...taken care of, very considered. They were always looking out for me. They said, ""You can stop this process any time you want. Even if you're a perfect donor and you get the heebie-jeebies, it's okay, you can stop it.""...I knew at any time I could say no and so, therefore, I didn't feel like I wanted to say no....They were very kind, they were very helpful, very professional.... We do feel like we've been on a ride and I think it's not just me, 'cause I'm the donor. But it's the whole family—my dad, my brother Bill—just all of us feel like this has been a long process. (Wilma)As Wilma's knowledge increased, so did her comfort with donation. From the initial stages of the process, the people responsible for facilitating the process to deliver the product conveyed the ways in which they would help Wilma navigate and support her through the process. The people and their focus on Wilma's well-being helped mitigate experiences of psychic sacrifices even before they emerged.Promotion focuses predominantly on why one should donate, not on how messaging can help attenuate the psychic sacrifices individuals may make as they navigate relationships affected by donation. For example, an individual's decision whether to donate an organ can have major relational impacts within their network of family, friends, supporters, and naysayers due to the potential health risks and uncertain recovery period involved. Consider the experience of Gregory, who terminated his relationship with his longtime partner when she questioned his desire to donate. In addition to supporting potential donors, it is crucial for the process and promotion to attend to the support network of those donors. An example is found in Wilma's experience, in which she describes how the people in the center focused on communicating the process and her role within it to deliver the product as support for her as well as to alleviate her family's trepidation. Then there is Nancy, who incurred travel costs because she felt the need to communicate to her family in person regarding her intention to donate. The people in Nancy's center were less helpful in supporting her desire to understand the process in detail, which resulted in her incurring financial costs. Perhaps if the people and process were more supportive, Nancy would have been able to avoid pecuniary sacrifices in support of her donation.The process tends to focus on the potential donor, with some inquiries about their support system. This approach in organ donation is derived from laws that prohibit the sharing of medical information with people other than the patient. While legally compliant, such an approach often leaves potential donors lacking in assistance as they attempt to encourage their support system to come on board. Consider another informant, Kenneth, who described his wife's dismay as he aimed to initiate a kidney transplant donor chain. A donor chain is possible when donor–recipient pairs who are not clinical matches participate as part of a group of donors and recipients, where each donor contributes to another recipient such that at least two transplants result ([11]). Kenneth knew he had an opportunity to positively affect many lives through participation in the chain, as his donation would make subsequent transplants possible. He explains that he put his marriage at risk as a result of his decision to donate:I was part of the biggest chain that has been so far....I knew that I was starting it....I'm married and my wife told me she was going to leave me if I did it. I said, okay, and she didn't [leave me]. But I wasn't going to let that stop me because she's worried or whatever. I wasn't going to let that stop the benefit that it was going to be to other people, I didn't think that was right.... She never came around.... I think it still kind of bugs her that I went against what she wanted. Almost in a way, it's like I had an affair or something. (Kenneth)Potential donors often invest mental energy when contemplating becoming an organ donor and speaking to their close circle about it. The possibility that their health could be negatively affected may well produce personal stresses and, as was true for Kenneth, stress within their close relationships. Kenneth was driven to contribute what he perceived as the immeasurable good that would emanate from his cumulative psychic and physical sacrifices, and therefore he excluded his wife from a life-altering decision. He draws parallels between his kidney donation and an affair, a state of emotional and/or physical perfidy. Reconciling this requires him to sacrifice his wife's opinion and support, which are of great value in a peaceful marital union. Yet Kenneth, akin to many of our study participants, describes positive aspects that emerge through donation. The codification of those experiences would serve organizations in the development of promotions and process components to support potential donors and their support systems, as well as infuse opportunities within the process and the people supporting it to celebrate such experiences.The experiences of the previous informants underline how promotion, designed as it is to disseminate knowledge to potential donors about the opportunity to donate, is insufficient in addressing the various types of psychic sacrifice that emerge through the donation process. The process contemplates the clinical needs of an individual, yet organizational managers should consider and prepare for the types of psychic sacrifices donors make, from contemplating the opportunity, informing loved ones of their decision, and navigating support throughout transplantation, including the postdonation phase. There are a variety of products for which marketers commonly address potential fears (e.g., ""safe when used as intended""). Because messaging around organ donation does not typically address the various sacrifices that manifest, there is a large window of opportunity for tailoring the marketing mix to address this deficiency.The integration of promotion, product, and process also provides opportunities for organizations to support potential donors as they contemplate engaging in donation. Another participant, Sadie, learned of her husband's need for a transplant when accompanying him to a doctor's visit. During that discussion, she learned about and was motivated to consider becoming her husband's living organ donor:When you live with someone and all of a sudden you see them losing weight, you see them walking around like a zombie having no energy....He was doing the peritoneal dialysis, and he had to hook himself to the machine every night by eight o'clock.... The reason [the medical team] did this for him was because he liked to play golf. They were trying to make it so that he could maintain his lifestyle.... He was on dialysis for six months, but it was an awful six months.... When I went with him [for a checkup], the nephrologist informed me that a lot of wives are giving their husbands kidneys.... I thought, ""Well, I have one foot in the grave and one on a banana peel. I can do this!"" (Sadie)The same medical team that proffered in-home dialysis to address her husband's renal failure also offered organ donation as an alternative. The physician shared the benefits of living organ donation and also began to introduce information to enable Sadie to ponder such an option. Even though it was a more complex offering than dialysis, she welcomed an opportunity to take a more active role in improving her husband's health. When discussing it as a family, their son offered to donate instead of Sadie. She declined his offer as he was recently married, had a newborn, and had just started a new career. Thus, she enacted psychic sacrifice in her assessment of the opportunity, the relative risk to the possible donors (i.e., herself vs. her son), the potential impact to her own health, and the hope to enjoy a more spontaneous and active life than that which dialysis accommodated. These sacrifices are not accounted for within the process, leaving donors to manage them on their own when organizations can anticipate such experiences and should proactively address them.The deliberation phase is likely inspired, in some part, by the promotional element of the marketing mix. However, it is insufficient to address the multifaceted experiences of psychic sacrifice individuals bring to the deliberation phase. Prior research has found that psychic sacrifice may be enacted in response to promotional messaging. For example, a recent University of Pittsburgh Medical Center (UPMC) television commercial depicts a line of individuals slowly making their way through an ominous tunnel with the voiceover: ""At UPMC, living donor transplants put you first so you won't die waiting."" Similarly, the National Kidney Foundation initiated the promotion ""#BigAskBigGive,"" which provides individuals with guidance on how to talk with others about becoming a living donor. Promotional materials serve to inform, persuade, and invite action by individuals to consider participating in a process to deliver a specific product. Similar to for-profit organizations, which must align promotions with other aspects of the marketing mix, it is necessary for donor-reliant nonprofits to consider how other aspects of the marketing mix can be employed to address psychic sacrifices that may emerge in the deliberation phase of donation. Potential donors experience psychic sacrifice in contemplating what it means to undergo an elective surgery where the result is to remove functionality from their physical self and provide that functionality to another. Psychic sacrifices also serve as precursors for other types of sacrifices to manifest. Importantly, the mitigation of psychic sacrifices through a clearly and compassionately positioned and communicated product and process may provide encouragement to individuals to proceed to the decision and donation phases of the process—phases that likely require additional forms of sacrifice for which donors will seek support. DecideIndividuals undergo the decision phase of the process as they review the donation opportunity and determine their plans. While organ donation for transplantation, as a product, consists of a similar set of criteria and testing protocols across transplant centers and a consistent set of surgical procedures, there are some differences. These differences reflect each organization's approach to organ donation—specifically, the approaches of those with distinct roles associated with the entirety of the transplantation process. As individuals decide whether to donate, they assess not only the opportunity but also the organization. Thus, the decision to donate may be influenced by aspects of the product, the process to deliver it, the people who enable its delivery, and the place where the donation will occur. Where psychic sacrifice allows individuals to move forward with sincere contemplation, the decision phase finds individuals facing psychic, pecuniary, and physical sacrifices. Organizations have opportunities to mitigate these sacrifices, thereby likely contributing to the experience of donors and perhaps increasing the likelihood that individuals will choose to become donors.The opportunities for donor-reliant organizations to employ product, process, and people aspects of the marketing mix become more impactful as individuals assess the opportunity to make their decision. One informant, Penelope, donated to her brother after he survived a failed transplant from a deceased donor. She and her family were angst-ridden by his tenuous health and recount being summoned to the hospital because his physicians were uncertain if he would live. She aimed to better understand the impact of donation as part of the product, as well as the implications of participating on her lived experience:[The transplant center] had a reception for donors and recipients, a little cookie and cake thing where people who had [volunteered to donate] talk to those of us who are going to do it. [They] talk about their experience. That was great because I got to see people who had done it.... I was getting nervous. I was excited because I was going to help my brother but I was still nervous. That was my first surgery ever. (Penelope)In Penelope's case, the organization provided individuals considering where to donate an opportunity to learn about the experience from former donors. Sponsoring this event also provides an opportunity for the organization to help donors manage experiences of psychic sacrifice as they weigh saving another's life while risking their own. By expanding the process to include additional people, the organization has opportunities to provide additional support to potential donors and perhaps improve their decision process.The contributions of an organization's people in the decision phase are crucial to the process and to the donor's perceptions of it. Individuals who choose to donate, as well as their friends and family, may question the extent to which organizations recognize the depth of sacrifices required to do so. Wilma and her husband wanted to learn how people in the organization, and in particular the surgeon, viewed the process:My husband asked a question, ""What does that feel like once you take that kidney out of there and you take it over to the other person? How do you feel about it?...Do you kind of feel like God? Like you're saving this person's life?"" [The surgeon] said, ""Well, I am the physician who takes out the kidney. My patient is the donor. And, the donor comes in healthy....I am very particular about my job, because in the whole hospital, I'm the only person with patients who come in healthy and go out impaired."" And I thought, ""Wow! He understands.""...I felt relieved or assured by him saying that. I knew that he understood the gravity of the donation. (Wilma)Individuals considering donation recognize that transplantation provides significant benefits for both the recipient and the organization. However, potential donors are acutely aware that those benefits emerge through their sacrifices. As Wilma shared, individuals may encounter compassion in those who play roles in the provisioning of transplantation or in their initial contact with the organization. Such experiences facilitated by organizations through the people and processes that support the product allow individuals to receive validation of their sacrifices and enable their willingness to contribute the sacrifices required to fulfill the donation.With a decision to donate made, individuals begin the qualification phase. Potential donors are provided a detailed description of the process, including an overview of the criteria required for participation, the testing sequence, and the possible consequences of participation. The choice to donate is fraught with uncertainty, as it does not mean that an individual will be accepted as a donor. As such, individuals have different approaches to sharing their intentions with others. The first author described angst when contemplating with whom to share:I want to tell [my friend] about my plans [to donate]. She might think I'm crazy. I can't hear anything negative about [donating]. It's enough that [the transplant coordinator] said I could die! But what if [the transplant center] rejects me? How will I explain that? (Field notes)It is commonplace for individuals to share important happenings in their lives with others. The desire for acceptance of one's decision and support for it is common among the participants in our study, and many seek out such support in online donor forums.The experience of qualification feels more extensive than how it is presented to potential donors. Participants generally express astonishment at the degree of testing required:I thought it was just a blood test. I learned I had more tests to take.... I thought, ""Oh Lord, this is going to be impossible!""...Everything was going along and [the transplant coordinator] came back and said we are an identical match!...I think one of the difficult things is we don't know how to ask a sibling to donate. It's a sacrifice. (Reginald)The transplant coordinator orchestrates progression through the qualification phase based on clinical results from a series of escalating tests (e.g., blood tests to CT scan). These tests may be the first opportunity for individuals to experience the place where their donation will occur and, as such, leaves an indelible impression. One of our informants, Nancy, was deciding whether to conduct her tests at the local transplant center or the one where she was a potential match to a recipient. Ultimately, she felt it necessary to meet the people who would orchestrate and conduct her donation. She organized a visit incurring travel, accommodation, and vacation time costs to travel from one state to another in the Western part of the United States:I did online research.... The transplant center sent me a [video] and I read the literature that they gave me.... I decided to go to [the next state over] where the transplant happened—I wanted to do the blood matching there....I wanted to meet the people.... I read the possible adverse effects like pneumonia, blood clots, and death. I felt comfortable, but I still wanted to know more.... When I came to the appointment with the transplant surgeon, who's actually a cardio surgeon, he's not even a nephrologist!...I had lots and lots of questions. I wanted to know what was going to happen during the surgery and he just kind of waved me off and said, ""Oh you don't need to know that; let's not worry about that. We'll take the kidney out of your old caesarean scar. You won't have any new scars and the rest of it we won't worry about."" He just wouldn't give any more information and I even asked, ""Are there any other living donors? Is there somebody I could talk to?""...They said, ""Oh no, we don't do that."" (Nancy)As individuals proceed through the process, their awareness of the impending surgery and its associated risks becomes more of a reality. Where Penelope describes an opportunity to interact with former donors, Nancy was not allowed to do so. Thus, process contributed to Penelope's reduction in experiences of sacrifice by enabling her to see former donors, yet it accumulated additional sacrifices for Nancy. Furthermore, where Wilma's experience of sacrifice was attenuated by the health care staff, for Nancy it was not. Although these donors continued through the process, there are opportunities for organizations to mitigate experiences of sacrifice through marketing-mix elements that may also enhance the overall donation experience.An increasing awareness of the associated risks provides insights into the various sacrifices these individuals undergo. There are two that seem to be most angst producing: ( 1) the possibility of death and ( 2) the possibility of being rejected. In Nancy's visit, it becomes evident that although the product is similar, there were opportunities to employ alternatives for the communication of the process as well as interactions with transplantation staff to address her experiences of sacrifice related to her well-being and, ultimately, her life. Other participants spoke of the angst experienced as they pondered whether they would meet the criteria to donate. Participants stated that they have sufficient information about the process from the promotional and product materials as it relates to reasons why they may not be accepted as donors, or the rare but possible outcome of death. Yet their confidence in the process is influenced and experience of sacrifice altered when they are exposed to the people within it.Throughout the process, individuals often seek some affirmation that everything will work out satisfactorily. That is often evident in how individuals pursue the qualification process. For Nancy, it entailed travel to the transplant center to gather first-hand knowledge of the overall process and people within it. For others, like the first author, there are sacrifices made to ensure success with each step throughout qualification with the hopes of increasing the probability of acceptance. For example, the first author was required to complete the 24-hour urine volume test four times, as the results were different than expected by clinicians:Seriously? I drink a lot of water—the two jugs [of urine] are all mine! Off to get the new jugs and another [urine collection] hat.... I don't like the jugs at the [local clinic] so I will get them from the [local] transplant hospital—it's a drive, but anything is better than redoing this test! (Field notes)Testing often requires that individuals rearrange their lives to accommodate travel, clinical appointments, and testing procedures. As with the surgery costs, tests are covered by the recipient's insurance. However, some of these activities necessitate the expenditure of money (e.g., copay, gas, parking). In addition, the tests themselves typically require that individuals provide access to their body and bodily products to assess fitness for organ donation. Thus, testing to qualify may lead to psychic, pecuniary, and physical sacrifices. These sacrifices emerge during a fragile time in the process when individuals are anxiously awaiting to hear whether they can progress to the next phase of testing until they are accepted as donors.The nexus between place, people, and process in the decision phase represents an ideal point at which marketers can influence the donation experience. At this juncture, there is an escalation of commitment evident as individuals proceed from consideration to making a decision to actively pursuing the final phase of donation. Alison, a nondirected donor, wanted to donate in response to a story she heard on NPR. She is a busy mom with a career who wanted to donate on her terms. She identified a convenient location for her donation and prepared a schedule that negotiated necessary donation-related time commitments with the demands of her life. Alison describes her experiences of sacrifice and how the organization employed people throughout the process to attenuate anticipated anxiety as she passed from one level of clearance to the next round of testing:You kind of felt like you were on the show Survivor. Every time [the transplant coordinator] would email or call me, I would be like, ""Was our blood a match?"" Every time you had to have that blood draw, you were praying that you still were on the island! That you weren't going to get the call, ""Sorry, you've been rejected. You can't donate.""...Every time I knew I passed the next test, I was like, ""Yes! Okay! One step closer!"" (Alison)The presentation of the self for extractions of fluids and tissues serves to prepare individuals for ever-increasing physical sacrifices culminating with the nephrectomy. The relationship the transplant coordinator builds with the potential donor is key. The commitment to the process is commonly expressed because individuals anticipate progressing through to the donation stage. Ideally, the transplant coordinator supports this anticipation with commendations as potential donors undergo sequential tests and celebrations when they advance through stages in the process.The decision phase encompasses the full complement of sacrifices, but psychic sacrifices in particular usher in opportunities for additional experiences of sacrifice as individuals move through different phases. Key to the donation is the integration of process and people (surgeons, counselors, etc.) within place to deliver on the transplantation product. Furthermore, while incurred costs are pecuniary sacrifices, organizations provide a variety of alternatives to help individuals assuage or avoid incremental costs. Doing so likely requires additional donor confidence in the team communicating and managing the process. Within for-profit offerings, sacrifices associated with price may signal desirable attributes ([70]). However, incurred costs within donation tend to reflect a need for organizations to communicate more with potential donors such in order to mitigate such costs. Within the decision phase, donor sacrifices may be managed through a combination of marketing-mix elements to support donors as they make a crucial decision. DonateThe donation phase is reached when an organization's efforts to secure donors materializes. This phase culminates with the creation of a product (a donated organ for transplant) that provides valued benefits to clients. For those attracted to donate, organizations orchestrate the delivery of the product benefits through place, which houses the requisite people and processes. This phase of living organ donation then concludes with the emergence of the most critical sacrifice: the nephrectomy. While organizations cannot eliminate the totality of sacrifices associated with this phase, they can—through the careful specification of the product including roles for donation and thoughtful facilitation of product delivery through place, people, and processes—attenuate experiences of sacrifice.As the people within organizations prepare donors for surgery to complete the donation, there is an opportunity to contribute to donor confidence and comfort in order to reduce experiences of sacrifice. Increasing comfort with the part of the process that encompasses the details of surgery is a crucial component of the experience. One informant, Victoria, donated to her niece. Once credentialed as a donor, she recounts how she aimed to gather as much information as possible to better understand how her kidney would be removed:I'm one of those people that goes and does as much research as possible. As soon as they told me I was a match, I'm like, ""Okay, what's the surgery going to be like?"" I actually found on YouTube a video of the actual surgery so I sat and watched that.... The surgeon actually has to slide their hand [into the abdomen] to retrieve the kidney.... It wasn't long after that I was meeting the surgeons. I met the one gentleman who came in and the first thing I looked at—his hands were huge! I was just like, ""Oh my gosh, are you my surgeon?"" He says, ""No actually, I'm going to do the transplant [into the recipient]."" I'm like, 'Oh good!' He kind of looked at me funny, and I said, ""Your hands are huge!"" Shortly after that, I met my surgeon. It was a woman and she has these beautiful, little, tiny hands! (Victoria)In the deliberation phase, donors are most often concerned with factors related to transplant center successes. After deciding to pursue donation and being accepted to donate, individuals often turn their focus to the surgical process that results in donation. Like Victoria, donors express concern with recovery and factors that may influence it, including the size of the incision or degree to which organs are displaced. Though transplant centers do not assign surgeons based on hand size (or personality, or specialty area), it is crucial to understand the importance of people within the process. Organizations should have an awareness of what factors may increase perceived donor sacrifice and how they can proactively manage them.Once in the donation phase, sentiments about completing the process become more salient. Hannah, a hospice nurse who describes herself as one who avoids ""medical stuff,"" describes how interaction with her surgeon reduced her concerns with donation:I just love [my surgeon], there was something about him. And surgeons are usually so detached and so task-based. He was just a lovely man. We talked about different ways he could do the surgery. I said, ""Well, I'm going to be asleep so I want you to be comfortable with how you're doing this.""...He did end up doing the open [nephrectomy]. I have a six-inch scar.... And then he asked me, ""This is a nice thing you're doing. Is there something we can do for you?"" I said, ""Well this is going to sound a little strange, but I would love to have a picture of my kidney going to him..."" He just looked at me and said, ""Bring a camera!"" So we got a disposable camera and I have pictures of my kidney in the metal bowl with him working on it and [the other] surgeon coming to get [the kidney]. (Hannah)The organ donation and transplantation process involves people at every stage who take on crucial roles. For example, donors most frequently describe the transplant coordinator as an orienting figure in the process. Another central figure is the surgeon, whom people assume to be competent, albeit stereotypically impersonal. As surgeons show compassion toward donors and the sacrifices they experience through surgery and recovery, the donors feel cared for within the process. Conversely, recall Nancy's encounter where she felt the surgeon was dismissive toward her inquiries. Hannah's and Nancy's experiences underscore that just as health professionals can enable the progression of the process in a manner where donor sacrifices are managed, they can also amplify experiences of sacrifice.The nephrectomy, the most obvious physical sacrifice by donors, occurs during a surgical procedure with donors fully anesthetized. Physical sacrifice is thus experienced primarily through the recovery process. Penelope describes a postsurgical recovery experience that is common among living organ donors:When I came out of surgery, I felt like I had been run over by ten trucks. One after the other! They just kept running me over. One after the other. I was a mess, just a mess. But deep down, I was happy because I could hear them telling me that my brother was fine.... You can't look at the moment of surgery. You have to look at the end result. (Penelope)The transplant team provides an overview of all aspects of the process, including recovery. Recovery, both immediately after surgery and extending weeks afterward toward the goal of regaining full strength, is particularly challenging for donors given their high levels of health prior to donation. Recovery often requires that individuals refrain from several activities, including work, for anywhere from two to six weeks. The totality of sacrifices necessary by individuals to contribute to transplantation is most often deemed worthy, as exemplified by Penelope. The recovery portion of the donation process focuses primarily on clinical outcomes. While important, there are opportunities for organizations to support donors in the experiences of both physical sacrifice and psychic sacrifice as they strive to fully recover, in addition to pecuniary sacrifice through lost income and incurred costs.For most donors, the process ends once they obtain medical clearance to resume their regular activities. For the organization, the process comes to a close approximately six weeks after surgery with the postsurgical lab work. Although the likelihood of negative outcomes is low for kidney donors, when they do occur, a timely and appropriately compassionate response by the organization is important. Recall Nancy, who traveled to another state to donate her organ. During postsurgery recovery, she experienced unexpected outcomes that were not explained or anticipated by the clinicians or found in her research:The transplant was successful. They had told me in advance that I'd probably stay in the hospital six days because I had so far to travel to go home.... Before I was discharged, I noticed that I had lost feeling in my one leg, in my one upper thigh of my left leg. I mentioned it to the doctor and they said, ""It will disappear in six months."" So, I literally marked on my calendar for the six months. And, the pain did not go away—it was intensifying. I wrote [the transplant center] and insisted that they examine me again. And, they confirmed that I had neurological damage in that leg. (Nancy)Throughout the process, individuals are made aware of possible complications. While some complications from organ donation are resolved within the first year of surgery through additional clinical intervention (e.g., hernia repair) or lifestyle adjustments (e.g., fluid intake to address abnormal lab metrics), other complications may extend much longer (field notes). As with Nancy, Gregory experienced complications:It was in my exit interview, six days after surgery when I was released to come home, the same surgeon who operated on me said that it would probably be six to eight weeks before I would be out of the woods entirely.... At 8 weeks when I asked for a refill of pain medication, actually 8 weeks and 1 day, they said they had trouble with providing any pain medication after 8 weeks, and at 12 weeks when...the surgeon called me, I was surprised that he did, but a Saturday night he called me, and he said he had never had a patient who 12 weeks out was still in pain.... They never offered anything in terms of solution.... It felt to me like my internal organs were out of their normal position.... I asked if I could receive water therapy and [the surgeon] approved that. I asked if he would approve myofascial release work which I had learned about and I thought could help with what I was told was the scars were forming and the nerve tissue was probably entangled in the scars and myofascial release might work, and he denied that, he said no he wouldn't approve that. [He was] quite dismissive; as if I was saying, you know there's a witch doctor down the street. (Gregory)These experiences are similar to those of some donors who continue to be challenged as a result of surgical complications that may require accommodation for an extended period of time. Thus, it is important that health care providers equip themselves to manage donor experiences that encompass a range of outcomes, including those of prolonged and unexpected sacrifice.Although the process includes tests to assess mental and physical fitness and risks for donation, there are negative outcomes. As with Nancy and Gregory, medical complications that yield physical sacrifices are most often treated as exceptions to the process and may result in encounters with people who are not equipped to manage them within the context of the donation experience. Beyond physical complications, individuals may experience additional psychic sacrifices after donation. Consider Lizbeth, who donated to her brother with whom she had a standoffish relationship. Throughout the process, she describes feeling angst and frustration that she would have to donate to keep peace with her parents and brother. As a reluctant donor, she describes her experience:Donors, even donors who wanted to do this, feel like after, that ""I was just a kidney walking in there with arms and legs attached."" You'll find a lot of donors feel neglected and abandoned....I called the doctor and now they don't want to talk to me. Now that I gave up the organ, now I'm not important to them. [It's] kind of like a girl who goes out with a guy and he said, ""I love you! I love you!"" and she sleeps with him. Then afterwards, he doesn't call her. Like that feeling of, ""I gave something that was precious to me and now you don't even appreciate it."" (Lizbeth)Even years after the transplant and with great health, Lizbeth harbors resentment that neither the process nor the people within it did much to care for her emotionally and physically. While transplant organizations are in need of donors' organs, it is critical those donors are fully cared for in a manner that does not leave them feeling abandoned or exploited. It is thus imperative that organizations develop a supportive process staffed with compassionate people to mitigate sacrifices by individuals with less than ideal emotional or physical outcomes, a process that may well also enhance the product by heightening appeal to potential donors.Nonprofits typically focus on messaging that promotes their product, which in the case of organ donation organizations translates as engendering a desire among potential donors to sacrifice an organ for a person in need. The experiences of donors participating in the present study reflect the kinds of sacrifices that are common within the organ donor community and emphasize that such sacrifices need to be addressed by organ donor organizations. By pursuing mitigation strategies in the form of various marketing-mix elements, organizations can convey a cohesive value proposition in their quest to procure donors, one that speaks directly to the sacrifices that often accompany the donation of an organ. DiscussionThis study of living organ donation contributes to the literature by describing how elements of the marketing mix may be employed to attenuate donor experiences of sacrifice. Prior research has focused on how promotional messages may be employed to make individuals aware of donation opportunities and to overcome reluctance on the part of potential donors. While the aims of these promotions are crucial, we suggest how the marketing mix can be employed to mitigate concerns about the sacrifices often experienced by individuals as they advance through the donation process feeling valued as integral participants. As part of that strategy, we identify roles for the marketing mix—product, price, place, process, people, and promotion—that extend consideration beyond that of promotion. Thus, this research contributes an understanding of how organizations can more intentionally and systemically overcome potential donors' concerns and thereby increase the population of donors. Managerial implicationsNonprofits contribute significant value to society together with support from the individuals who contribute to them. Securing donations is a primary challenge and focus for the delivery of these organizations' missions ([ 9]; [67]). These findings are of particular interest to managers of nonprofit organizations who rely on individuals to offer contributions born of sacrifice that enable those organizations to deliver on their missions. Although these findings emerged from a particular type of donation, they are relevant to organizations that depend on contributions born of sacrifice, such as those seeking families to host foreign exchange students, those striving to facilitate the adoption of children who are difficult to place, those providing hospice support to individuals and their families during end-of-life transitions, or those offering compassionate care to individuals in crisis (e.g., sexual assault, domestic abuse, suicidal tendencies). These findings provide insight into how organizations can secure contributions, a necessary component of supply, to meet demand.Prior research primarily has focused on how nonprofit organizations may employ promotional messaging to inspire contributions from individuals. We agree that promotion is certainly necessary, yet the present findings provide evidence suggesting that managers may be better served in meeting their missions by considering how to effectively employ the entirety of the marketing mix to attract individuals for available donation opportunities. We suggest that managers consider the composite of sacrifices required from individuals as they proceed through each phase of donation, and that managers employ the marketing mix to proactively and compassionately address the various types of sacrifice that emerge.We identify actions for managers to employ the marketing mix—product, place, price, promotion, people, and process—in addressing each of the three types of sacrifice identified in the donation process (see Table 3). In addition to those specific actions identified, there are some general considerations for organizations. Product is reflected most clearly in a nonprofit organization's mission statement and manifests in the offering to which the donation supports. Place focuses on how disparate entities are integrated to support an individual's escalation of commitment from interested to committed as well as the delivery of the offering. Price is the component that conveys the costs incurred by donors to provide the contributions. Promotion is most often found in messages educating and persuading potential donors by conveying their importance to delivery of the offering. An organization's people are an important factor in delivering the entirety of the process and serve as a guide for donors throughout the process.GraphTable 3. Marketing and Organizational Considerations and Actions to Alleviate Sacrifices and Attract Organ Donors.  The process component reflects the steps required for individuals to transform from potential to actual donors, and it is the manifestation of the donation. The process we define is composed of three phases. In the deliberation phase of the process, individuals considering the opportunity are more involved in moving the process forward with some input from the organization. Within the decision phase, there is a balance of influence between individuals and organizations. As individuals move through to the donation phase, the balance of influence shifts toward the organization. Thus, an awareness of the process and perceptions of the organization to which individuals are contributing is also important. To an extent, donors are invited ""backstage"" ([28]) as they contribute to the creation of offerings for others. As such, it is imperative that organizations understand what they are asking of donors and how donors may experience sacrifice. Furthermore, it is important for donors to experience a degree of success, particularly when they are not able to readily observe the outcomes of their donations. Therefore, it is important that the processes to which donors contribute provide them with satisfaction that may be in some ways commensurate with the sacrifices they make to participate.Importantly, process and people influence each phase of the donation experience and should be audited regularly to ensure that the interfaces between them and each phase, as well as the other marketing-mix components, are integrated. Furthermore, it may be helpful for managers to examine the milestones within a donation experience by assessing the extent to which those milestones are critical transition points for an individual to continue with the process of becoming a donor. Prior research has suggested that recognition may not be impactful to those who already contribute to nonprofit organizations ([66]). However, it may be that when the process to become a donor is more involved, it may be useful for organizations to provide motivation that inspires individuals to continue through the process.The integration of each of the six marketing-mix elements is more likely to result in an environment in which individuals feel their donations are valued and respected. Each marketing-mix element should be aligned to engender the desired response to the organization: that of converting an individual into a volunteer. A great deal of marketing research focuses on the types of messages or individual characteristics that are more likely to yield larger contributions for nonprofits. In the present research, we instead focus on how the marketing mix can be engaged to prepare individuals to engage in a donation opportunity. We find that marketing-mix elements mitigate sacrifice, which serves to engage individuals in the donation task and thereby increases the likelihood that they will continue. For organizations where donation may continue, the enactment of such sacrifices is likely to engender loyalty and continuity.The implications of these findings are obviously important for organizations in need of tissue or organs to deliver on their mission. However, these findings are also relevant to organizations in need of donations generally. Consider the Center for the Homeless, a nonprofit serving those individuals without secure housing that is reliant on grants, fundraising, and donations. In particular, individuals donate clothing and food supplies, organize various life skills workshops for adults, and staff and equip a classroom for children. While there are various donors who contribute resources to support operations, the creation and maintenance of a Montessori classroom at this shelter is partially reliant on donor support. These donors contribute a significant amount of time, talent, and money annually to maintain a fully functioning classroom (e.g., books, computers, supplies) in addition to supporting training and funding for a full-time Montessori teacher.The Center for the Homeless generates much promotion to increase awareness that there are homeless children in need of support, yet these findings suggest that it may be more effective for the center to leverage the composite of the marketing mix to attract donations to the Montessori classroom. Promotions may be helpful to clearly articulate the intention of providing quality education for homeless children at the center in such a manner that manages the psychic sacrifice individuals may experience as they contemplate the opportunity. However, more is needed to explain the role of this particular product, as it is nontraditional in the realm of a homeless shelter as well as a school. The product is education that serves as a bridge, aiding students in catching up until they are once again enrolled in a school. Thus, the product may involve specialized processes and require additional people beyond the teacher to provide adequate education. The place—that is, the Montessori classroom within the center, is organized to aid children to be treated like students who are able to learn. Thus, place includes features of a traditional classroom (e.g., textbooks, reading pods) while accommodating the necessarily transient and multiple-grade-level nature of its students. As with other types of donation opportunities, there may be incurred costs for donors (e.g., background check to work with minors, art project supplies). The marketing mix could be employed by the Montessori classroom to attract not only donations but also volunteers. More specifically, the center could more fully employ the marketing mix to attenuate psychic sacrifice (as individuals recognize they have limited capacity to assist homeless children), pecuniary sacrifices to participate, or the physical sacrifice that stems from being in an environment (e.g., smells, security, equipment) different from what they typically imagine encountering.Organizations are not static, as evident in alterations to their operations, offerings, and positioning. As for-profit organizations alter their offerings, they often try to retain existing consumers and attract new ones, recognizing that each will invest differing psychic energies to consume the offering ([45]). Similarly, nonprofit organizations could adjust their offerings to remain relevant to those they serve, thereby maintaining or growing their client base. For example, Habit for Humanity could upgrade its offerings by adapting the marketing mix through product attributes (e.g., new houses, disaster recovery, retail outlet), distribution (e.g., local and global builds), market messaging (e.g., model home challenges, Women Build Week), processes (e.g., one-time vs. long-term), or people (e.g., retail staff, policy advocates, board members). As they do so, it is important that they assess how those changes affect the degree of sacrifice required for existing and potential donors and operationalize the marketing mix to address those sacrifices. These examples underscore the importance of understanding how the marketing mix can be employed to mitigate sacrifice that emerges in the donation process as well as to enhance the overall donation experience. The deft employment of the marketing mix to extend the tenure of donors may also accrue other benefits to organizations such as confidence in operational projections, service stability, or reduction in expenditures to delivery services.The extension of donor engagement may be viewed as a form of loyalty. Similar to brand loyalty, which has a positive impact on a firm's bottom line ([ 5]), it is likely that donor loyalty evident in their continued engagement with an organization also has a positive impact on an organization's performance. Consider blood donation, a relatively noninvasive procedure to obtain human tissue. Blood is donated to organizations that bank it for clinical usage. Some individuals consistently donate every eight weeks, often at the same facility. When individuals continue to donate to an organization, it is likely that they experience less psychic (e.g., contemplation), physical (e.g., blood draw, testing), and pecuniary (e.g., transport, time) sacrifice compared with their consideration and choice of new donation opportunities (e.g., child advocacy). Such continuity may also reduce the number of donors who switch their support to another organization or, worse yet, depart the donor marketplace. When organizations successfully communicate the value that donors help deliver to the marketplace and stimulate desire for individuals to donate and minimize sacrifices through the marketing mix, individuals are likely to engage as donors. Opportunities for future researchThe present study investigates living organ donors. While there are a growing number of living organ donors annually, the majority of transplants occur with organs offered by deceased donor families. Those families employ a different calculus when considering donation of their loved one's organs ([24]; [31]; [52]; [55]). Where the present research focuses on those who make a choice to donate, further research is warranted to assess how the marketing mix can be employed to mitigate sacrifices for deceased donor families.We focus on individual donors and their sacrifices. These individuals are embedded in social networks where those relationships likely influence the donation experience. The gift-giving literature provides insights into how the nature of social networks may shape the process and experience ([ 8]; [15]; [54]). An examination of donors' social networks and their influences may provide additional insights. Recall Lizbeth and Wilma. Lizbeth found her social network to be lacking in compassion and support, whereas Wilma found hers to be filled with care and consideration. Each recounted distinct experiences of the support they sought in the marketplace, which may be related to what was provided by their social network. Thus, there is an opportunity to understand how and to what extent social networks influence the donation experience providing donor-reliant organizations opportunities to understand and adequately prepare for donor support.The current research theorizes sacrifice at the individual consumer level. Scholars in anthropology and theology theorize sacrifice at a community level in relation to social cohesion. Similar to the indigenous Chukchee people, who worked together to attain benefits for the collective ([41]), it may be that members of contemporary societies can be inspired to work together in support of beneficial societal outcomes. For example, several movements requiring individuals to employ sacrifices to attain societal benefits have gained momentum in recent years (e.g., #BlackLivesMatter, #MeToo, Get Out The Vote, #NeverAgain). Participation in those movements most likely involves psychic (e.g., contemplation of consequences of action and inaction), pecuniary (e.g., donations), and physical (e.g., protests) sacrifices. Therefore, it may be that sacrifices related to consumer movements may be viewed as enhancing participant commitment. Thus, it is important to explore how entities pursuing societal benefits (e.g., movements, nonprofits, civic organizations) can employ the marketing mix to attract and retain participants. It may be that for some movements, experiences of sacrifice are part of the personal benefit in addition to the societal benefits donors seek, and thus, organizations will have to understand to what extent and under what conditions they are to attenuate experiences of sacrifice.Scholars have also suggested that organ donation may be viewed as a form of gift-giving to society ([11]; [56]; [60]). Such gift-giving contributes much to the public good. Even as the marketing mix may be employed to generate even greater degrees of gift-giving, it must also be recognized that the same tools also may result in less-than-ideal outcomes. For example, the Susan G. Komen Foundation raises funds for breast cancer research and makes grants for breast cancer screening to organizations. The foundation effectively employed the marketing mix to inspire donations as individuals paid to participate in three-day races and secured additional contributions from others. In recent years, the organization has reduced grant-making capacity due to the decreased number of individuals willing to make donations. While monies were employed for the organization's mission, a significant amount was used for what donors perceived to be excessive non-mission-critical expenditures. Thus, it is imperative that scholars also consider factors that influence the relationship between marketing-mix elements, donor sacrifice, and perceived organizational effectiveness. ConclusionConsumer sacrifice allows donor-reliant organizations to attain their missions. We expand prior theories of sacrifice with an explanation of its three types and how they may be managed through the marketing mix. This explanation provides opportunities for managers to better understand how to more fully leverage the marketing mix to inspire individuals to partner with them by reducing experiences of sacrifice. Thus, those seeking more effective ways to procure donations for their organizations will benefit from understanding the nature of the relationship between sacrifice and the employment of the marketing mix to position their offerings.  "
39,"Highlighting Effort Versus Talent in Service Employee Performance: Customer Attributions and Responses Firms often attribute their service employees' competent performance to either dedicated effort or natural talent. However, it is unclear how such practices affect customer evaluations of service employees and customer outcomes. Moreover, prior work has primarily examined attributions of one's own performance, providing little insight on the impact of attributions of others' performance. Drawing on research regarding the warmth–competence framework and performance attributions, the current research proposes and finds that consumers expect a more communal-oriented and less exchange-oriented relationship when a service employee's competent performance is attributed to dedicated effort rather than natural talent, as effort (vs. talent) attribution leads consumers to perceive the employee as warmer. The authors further propose customer helping behaviors as downstream consequences of relationship expectations, finding that effort (vs. talent) attribution is more likely to induce customers' word-of-mouth and idea provision behaviors. The findings enrich existing literature by identifying performance attributions as a managerially meaningful antecedent of relationship expectations and offer practical guidance on how marketers can influence consumers' relationship expectations and helping behaviors.KEYWORDS_SPLITWhen firms communicate information about their service employees' competent performance, they often attribute it to either dedicated effort or natural talent. For example, on their websites, financial services firms such as Citigroup state that ""Citi works tirelessly.... We strive to create the best outcomes,"" and Partners Group Holding asserts that ""we work hard and deliver outstanding results."" In contrast, Manulife Financial highlights that the ""talent of our employees is what makes Manulife Financial a successful organization,"" and BlackRock states, ""Our best solutions come from the contributions of a group of talented and smart people"" (for more examples, see Web Appendix W1). We systematically examined the company websites of the top service firms on the 2018 Forbes Global 2000 list and found that many top financial and health care services firms mention these two types of performance attribution on their websites (see Figure 1). Despite the real-world prevalence of references to these two types of performance attribution, it is unclear how firms' promotions of performance attributions affect customer evaluations of service employees and customer outcomes.Academic research suggests two types of performance attribution: one to dedicated effort and the other to natural talent ([11]; [16]; [56]). Psychology and marketing literature has mainly examined a fixed or malleable view of people's own performance and its impact on how they judge themselves (e.g., judgments of one's own intelligence or personality; [16]; [17]), their own performance (e.g., academic performance; [31]; [56]), or brands/products ([40]; [42]), providing little insight on relationship judgments and behaviors toward others. However, in service relationships, beliefs about others' performance can influence relationships with those others (e.g., how a customer views a service employee's performance can influence the customer's relationship with the employee; [ 9]). We recognize this characteristic in service relationships, as well as a lack of research on the attributions of others' performance and their impact on relational aspects; thus, in this article we examine how attributions of service employees' performance influence consumers' relationship expectations with and behaviors toward service employees.We propose that attributions of service employees' competent performance can change the extent to which customers expect a more communal-oriented (or less exchange-oriented) relationship. Prior work has conceptualized consumers' relationship expectations with service employees along the communal–exchange continuum ([ 3]; [13]). In a communal relationship, consumers expect a service employee to take genuine care of them and understand their needs as a friend or family member would. In an exchange relationship, consumers consider a service employee strictly as a business partner and expect the employee to provide services that will be worth their money. We propose that consumers will expect a more communal and less exchange-oriented relationship when a service employee's competent performance is attributed to dedicated effort rather than natural talent, because effort (vs. talent) attribution leads consumers to perceive the employee as warmer. We further examine customer helping behaviors toward firms (i.e., voluntary and discretionary behaviors that aid firms beyond those required in the purchase of products and services; [ 8]; [10]) as downstream behavioral consequences of relationship expectations. In particular, we propose that highlighting service employees' effort (vs. talent) can increase customer helping behaviors such as word of mouth (WOM) and idea provision.Our investigation of service employees' performance attributions makes several theoretical and managerial contributions. First, the current research broadens our understanding of social judgments in commercial relationships. Although a considerable body of research has investigated the relation between judgments of competence and warmth ([32]; [58]), this prior work has mainly examined how a certain level of competence is related to warmth perception. The current research examines the attributions of competence as a new dimension that influences warmth perception, holding the objective level of competence constant. Second, the marketing literature has focused on the downstream consequences of a communal versus exchange relationship with consumers ([ 3]; [ 5]; [54]), but very few studies have proposed firm tactics that could induce a certain type of relationship expectation (communal or exchange). For instance, [41] showed that a company's communal obligations (e.g., providing medical care on the basis of need instead of ability to pay) can influence consumers' relationships with the company. The current research enriches the existing literature by examining performance attributions as an antecedent of relationship expectations. Third, as we have mentioned, whereas most prior work in the marketing literature has focused on attributions of one's own performance and their effect on product evaluation and choice ([40]; [42]), our work examines the attributions of others' performance.Our findings also provide important marketing insights. Figure 1 indicates that firms often attribute their employees' performance to effort or talent. Our research proposes that firms can strategically implement such performance attributions to evoke a type of relationship expectation that they want to promote (e.g., highlighting employees' effort when a firm wants to promote a communal-oriented relationship with customers). Thus, performance attribution is a managerially meaningful antecedent of relationship expectations, because it can be embedded in communication messages without requiring customers to have direct interactions with service employees. Our research also provides implications on customer attention to communication messages. We suggest that customers' relationship expectations can be manifested in their attention to service employee information. If firms want to attract customers' attention to person-related information (e.g., personal background information about service employees), they can highlight the effort of their employees, whereas if they want customers to focus on job-related information (e.g., what service employees do), they can highlight the talent of their employees. Finally, our research suggests that promoting different types of performance attribution can shape customer behavior. Specifically, by highlighting employees' effort (vs. talent), firms can increase customer helping behaviors such as sharing the firms' information on social networks or providing new product ideas.Graph: Figure 1. Prevalence of service employee performance attributions among top service firms.Notes: The figures represent the percentages of top service firms on the 2018 Forbes Global 2000 list that explicitly communicate either dedicated effort or natural talent (or both) or that do not provide performance attribution information on the company websites. Two independent coders were instructed to code performance attributions on the web pages in which firms deliver communication messages toward their customers. Agreement between the coders was high (83%), and disagreements were resolved by discussion. Social Judgments and Performance AttributionsResearch in social psychology as well as marketing has supported the notion that when people form impressions about others, they tend to make judgments along two fundamental dimensions: competence (e.g., capability, skillfulness, efficacy) and warmth (e.g., friendliness, helpfulness, trustworthiness) ([ 2]; [19]). For example, judgments of competence and warmth shape consumers' relationships with commercial partners, such as nonprofit and for-profit firms ([ 1]), salespeople ([48]), and brands ([33]). This line of research has investigated the relations between judgments of competence and warmth, mainly by examining how a certain level of competence is related to warmth perception. Some studies have reported that a higher level of competence results in greater warmth perception ([46]; [51]), whereas others have shown that a lower level of competence leads to greater warmth perception ([32]; [59]).Extending the existing literature, the current research examines the attributions of competence as a new dimension of competence influencing warmth perception, holding the objective level of competence constant. Research in social psychology has corroborated dedicated effort and natural talent as two internal sources of people's performance ([11]; [53]). In the case of dedicated effort, competent performance is believed to be the result of commitment, perseverance, and hard work. In the case of natural talent, competent performance is believed to be the result of innate aptitude. This typology is also in line with implicit theories suggesting that people's performance can be attributed to malleable traits such as effort or to fixed traits such as natural talent ([16]; [42]). In addition, prior work on attribution theory has made it clear that competence can be attained through either dedicated effort or natural talent. [57] suggests that ""when associated with aptitude [natural talent], the concept of competence is conceived as mainly uncontrollable, whereas when associated with effort expended, the attainment of competence is conceived as controllable"" (p. 79). Thus, conceptually, effort and talent are two different attributions of competence. Bridging these two streams of research on the warmth–competence framework and performance attributions, we examine how information on different attributions of competent performance changes warmth judgments and, in turn, relationship expectations. Employee Performance Attributions and Relationship ExpectationsWe posit that attributing a service employee's competent performance primarily to dedicated effort (vs. natural talent) makes consumers perceive the employee as warmer. Extant research indirectly supports this proposition. Prior work has found that when a person's performance is attributed to effort, that person is more likely to be seen as ""one of us"" ([30]), because most people generally believe that they also need to exert high effort to succeed ([35]). Indeed, when students learn about successful scientists' hard work in their scientific discoveries, they are more likely to see the scientists as ordinary people ([38]). Prior work also has indicated that those who are socially close are perceived to be warmer than those who are socially distant ([37]). For instance, compared with out-group members, in-group members are rated as having a greater capacity to experience emotions and being higher in warmth ([27]). Therefore, we propose that compared with talent attribution, effort attribution will lead the customer to perceive the service employee as warmer.In contrast, because most people tend to believe that only a few individuals possess natural talent ([18]), talent attribution can increase perceived social distance. Geniuses and exceptionally talented individuals are typically perceived to ""have"" something that most people do not have and, thus, are seen as different ([22]). In addition, [38] note that viewing scientists as individuals with a special aptitude for science discourages students from feeling connected with the scientists. When one feels disconnected from another individual, one is less likely to attribute the ability to feel to that person ([37]). For instance, compared with in-group members, out-group members are rated as lacking emotional capacity and as being more self-centered ([27]). Furthermore, people tend to see naturally talented others as disconnected from human experiences and emotionally inert ([35]), and gifted intellectuals are considered to be more antisocial than others ([43]). Teachers often view gifted and talented students as emotionless, antisocial, and insensitive to the feelings of others ([ 7]; [24]). Therefore, compared with effort attribution, talent attribution that can increase perceived social distance between a customer and an employee will make customers perceive a service employee as less warm.Our research further posits that the perceived warmth of service employees is the basis for consumers' relationship expectations with those employees. Consumers in a communal relationship expect a service employee to take care of them and consider their needs ([ 3]; [13]). In contrast, in an exchange relationship, parties understand that the benefits received should correspond to the benefits given, focusing on self-interest ([13]; [36]). Although commercial relationships always involve elements of exchange relationships, such as monetary exchange, consumers' relationship expectations can vary on the communal–exchange continuum, because consumers can expect different degrees of communality in commercial relationships depending on the situation ([ 4]; [ 5]).When consumers perceive a service employee to be warm, they will likely expect that the employee will be cooperative, have other-profitable (rather than self-profitable) intentions, and show genuine concern for consumers' needs ([ 5]; [33]). Such expectations are consistent with the norms of communal relationships. In contrast, people tend to expect a cold person to show less empathy for others and care more about him- or herself than about others ([33]). In addition, when people see others as being low in warmth and lacking emotional responsiveness, they can more readily perceive those others as instruments for their own goals ([28]). For example, viewing others as emotionless helps managers make decisions in difficult situations (e.g., layoff decisions) by seeing those individuals as objects or instruments to achieve their goals ([29]). Perceiving others' self-centered intentions and focusing on the instrumentality of others are behaviors in line with the characteristics of exchange relationships. Thus, when customers perceive an employee as warmer (less warm), they will expect a more (less) communal-oriented relationship with him or her along the communal–exchange continuum. Thus, H1:  Consumers expect a more communal-oriented (i.e., less exchange-oriented) relationship when a service employee's competent performance is attributed to dedicated effort rather than to natural talent. H2:  The effect of service employees' performance attributions on consumers' relationship expectations is mediated by warmth judgments regarding the service employees.Figure 2 depicts our conceptual framework and the flow of the studies. We first present five studies providing empirical evidence for the link between performance attributions and relationship expectations. Study 1a shows that when a service employee's competent performance is attributed to dedicated effort rather than to natural talent, consumers expect a more communal- and less exchange-oriented relationship with the employee. In Study 1b, we examine simultaneous attribution to both effort and talent. We then test whether perceived warmth underlies the effect of performance attributions on relationship expectations by directly measuring the variable (Study 2a) and by manipulating the perceived warmth of the service employee (Study 2b). Study 3 uses eye-tracking technology to show that effort attribution leads consumers to pay more attention to person- than to job-related information about the service employee. We then develop our hypothesis for customer helping behaviors as downstream consequences of relationship expectations and present two studies, one using a real firm context (Study 4) and the other in a field experiment (Study 5), to support the hypothesis.Graph: Figure 2. A conceptual framework of the current research. Study 1a: Service Employee Performance AttributionIn Study 1a, we attribute a service employee's competent performance either to dedicated effort or to natural talent and test whether effort attribution leads participants to expect a more communal-oriented relationship with the employee. We also examine a control condition in which no information about performance attribution is provided. MethodTwo hundred seventy participants (106 women; mean age = 37.59 years) were recruited online from Amazon Mechanical Turk in exchange for monetary compensation. Performance attribution manipulationParticipants were told that the medical society in a U.S. city periodically featured the city's top physicians and were asked to provide feedback on an article. All participants read identical information on performance, such that a physician had received a peer review rating in the top 10% of general physicians in the city. Then, in the effort attribution condition, participants read statements attributing the physician's performance to effort (e.g., ""[He/she] puts a lot of effort into the work""), whereas in the talent attribution condition, they read statements attributing the physician's performance to talent (e.g., ""[He/she] is naturally skillful at the work""; see Web Appendix W2). The control condition article only stated the physician's performance without any information on performance attribution. As a manipulation check, participants indicated the extent to which they thought the physician had achieved his or her level of performance because of effort or talent with three items (e.g., ""Put a lot of effort into his or her work/Was naturally talented at his or her work""; α =.96). Dependent variableNext, participants rated the degree to which they would expect their relationship with the physician to be communal- or exchange-oriented using eight items adapted from [ 3]. Five items tapped into communal relationship expectation (e.g., ""a person with whom I would want to interact outside of business"") and three tapped into exchange relationship expectation (e.g., ""a person with whom I would interact only for business purposes""; 1 = ""not at all,"" and 7 = ""very much""). In all studies, we followed prior work ([ 3]; [48]) and combined the reverse-coded items on exchange relationship expectation with the items on communal relationship expectation (α =.89). Web Appendix W3 lists measurement items for all studies. Control variablesParticipants responded to questions related to the design (""I like the design of the article""), credibility (""I think the content is credible""), and understandability (""I think the content is easy to understand"") of the article (1 = ""strongly disagree,"" and 7 = ""strongly agree""), as well as their knowledge of health care services (""How much do you know about health care services in general?""; 1 = ""not at all,"" and 7 = ""very much""), attention to the study (1 = ""paid little attention,"" and 7 = ""paid a lot of attention""), and mood (1 = ""feel bad,"" and 7 = ""feel good"") as control variables. The control variables did not differ across the conditions (ps >.10). Results Manipulation checkA one-way analysis of variance (ANOVA) revealed a significant effect of performance attributions among the conditions (F( 2, 267) = 42.85, p <.001,  ηp2  =.24). Participants in the effort attribution condition (M = 2.35, SD = 1.71) were more likely to attribute the physician's performance to dedicated effort than those in the talent attribution condition (M = 4.83, SD = 2.05; t(267) = −8.70, p <.001, d = −1.31). Moreover, performance attribution in the control condition (M = 2.94, SD = 1.89) scored in the middle and was significantly different from that in the effort attribution condition (M = 2.35, SD = 1.71; t(267) = 2.03, p <.05, d =.33) and that in the talent attribution condition (M = 4.83, SD = 2.05; t(267) = −6.85, p <.001, d = −.96). Therefore, neither effort nor talent attribution seems to be a default attribution in the absence of attribution information. Relationship expectationsA one-way ANOVA revealed that performance attributions had a significant effect on relationship expectations (F( 2, 267) = 8.50, p <.001,  ηp2  =.06). Planned contrasts revealed that participants expected their relationship with the physician to be more communal when the physician's performance was attributed to effort (M = 3.62, SD = 1.15) than when it was attributed to talent (M = 2.88, SD = 1.18; t(267) = 4.12, p <.001, d =.64), in support of H1. In addition, participants' relationship expectations in the control condition (M = 3.24, SD = 1.25) were significantly lower than those in the effort attribution condition (M = 3.62, SD = 1.15; t(267) = −2.08, p <.05, d = −.32) and higher than those in the talent attribution condition (M = 2.88, SD = 1.18; t(267) = 2.07, p <.05, d =.30). DiscussionStudy 1a offers preliminary evidence for our primary proposition that individuals expect a more communal-oriented relationship with a service employee whose performance is attributed to effort rather than to talent. The findings also show that either effort or talent attribution changes relationship expectations, compared with when there is no attribution, which indicates that neither of the performance attributions may be the default attribution in consumers' minds. Rather, firms can strategically create communication messages to highlight effort or talent, which can move customers' relationship expectations with their service employees along the communal–exchange continuum. Some might argue that there may be other more direct ways to develop communal relationships, such as by treating customers well and satisfying them. However, these tactics require actual interactions with customers. The current research suggests that communication messages that do not involve interactions with customers can still create a certain type of relationship expectation. In the next study, we additionally examine a situation in which the performance is simultaneously attributed to both effort and talent. Study 1b: Performance Attribution to Both Dedicated Effort and Natural TalentAlthough researchers have agreed that effort and talent attributions are on opposite ends of a continuum ([31]), and our research focuses on the relative emphasis on effort or talent, firms might communicate both effort and talent, as Figure 1 illustrates. Thus, in Study 1b, we examine simultaneous attribution to both effort and talent. Prior work on attribution theory has shown that people tend to perceive that naturally talented people's achievements come without effort ([52]; [53]). Therefore, providing information about a service employee's natural talent without any information about his or her effort can increase social distance ([38]) and lower warmth perception. However, prior work has also shown that learning that even talented people (e.g., great scientists like Einstein) had to exert high effort to succeed can increase people's sense of relatedness with those talented people ([30]; [38]). Thus, we argue that, compared with talent attribution only, simultaneous attribution to dedicated effort and natural talent can help consumers understand that even a talented employee is someone like them—that is, someone who needs to put in a lot of effort to achieve good performance—which will enhance warmth judgments of and a communal relationship expectation toward the employee. MethodOne hundred twenty-five undergraduate students (81 women; mean age = 20.38 years) from a large university in Hong Kong participated in this laboratory experiment in exchange for monetary compensation. Effort and talent attribution was similar to that in Study 1a. Participants read an article about an accountant whose competent performance (e.g., ""has ranked Jesse in the top 15% among CPAs in Hong Kong"") was attributed to either effort (e.g., ""Jesse puts a lot of effort into the work"") or talent (e.g., ""Jesse is naturally skillful at the work""). In the effort-and-talent attribution condition, participants read statements attributing the accountant's performance to both effort and talent (e.g., ""Jesse puts a lot of effort and is naturally skillful at the work""; see Web Appendix W4). After reading the article, participants indicated their relationship expectations with the accountant as in Study 1a. ResultsA one-way ANOVA revealed that performance attributions had a significant effect on relationship expectations (F( 2, 122) = 3.08, p <.05,  ηp2  =.05). In a replication of the previous findings, effort attribution (M = 3.47, SD = 1.00) induced a more communal relationship expectation than talent attribution (M = 2.99, SD =.96; t(122) = 2.22, p <.05, d =.49), further supporting H1. In addition, the effort-and-talent attribution (M = 3.43, SD =.92) induced a more communal relationship expectation than the talent attribution (M = 2.99, SD =.96; t(122) = 2.11, p <.05, d =.47), but it was not different from the effort attribution (M = 3.47, SD = 1.00; t(122) = −.18, p =.86, d = −.04). DiscussionStudy 1b reveals that attributing a service employee's performance to both dedicated effort and natural talent yields an effect similar to that of effort attribution only. As long as effort is made salient, consumers perceive a more communal (or less exchange-oriented) relationship with the service employee compared with a situation in which effort information is not salient. Note, however, that our findings do not imply that highlighting both effort and talent is always preferable to highlighting only one or the other. For example, compared with talent attribution only, attribution to both effort and talent can create expectations of a more communal relationship, and such expectations may not align with the service propositions of a firm that tends to engage in exchange-oriented relationships. In the next study, we test the mechanism for the effect of performance attributions on relationship expectations by directly measuring the perceived warmth of a service employee. Study 2a: Mediating Role of Perceived WarmthIn Study 2a, we investigate the mechanism underlying the effect of performance attributions on consumers' relationship expectations. We predict that attributing a service employee's performance to effort (vs. talent) leads participants to perceive the employee as warmer and therefore to expect a more communal-oriented relationship with that employee. MethodTwo hundred thirty-five undergraduate students (150 women; mean age = 19.98 years) from a large university in Hong Kong participated in this laboratory experiment in exchange for monetary compensation. Participants were told that a bank on campus was promoting an investment program for university students. They then read an advertisement featuring an investment manager whose competent performance (e.g., ""winner of best employee of the year award and ranked in the top 1% in performance"") was attributed to either effort (e.g., ""I work very hard to pick my investments"") or talent (e.g., ""I am talented at picking my investments""; see Web Appendix W5). In this study, we used ""top 1%"" to reduce the range of the performance level in participants' mind to control competence perceptions. Manipulation checkParticipants indicated the extent to which they thought the investment manager had achieved his or her level of performance because of effort or talent, using a semantic differential scale with three items (e.g., ""Put a lot of effort into his or her work/Was naturally talented at his or her work""; α =.97). MeasuresParticipants then indicated their relationship expectations with the investment manager as in Studies 1a and 1b (α =.89). We also measured the extent to which participants perceived the investment manager to be warm with six items (e.g., ""friendly,"" ""warm""; 1 = ""not at all,"" and 7 = ""very much""; [25]; α =.89). Control variablesTo ensure that the performance attribution manipulation did not induce different competence perceptions, we measured perceived competence of the investment manager with six items (e.g., ""competent,"" ""capable""; 1 = ""not at all,"" and 7 = ""very much""; [25]; α =.89). We also measured perceived attractiveness of the investment manager to check whether the performance attribution manipulation affects attractiveness perceptions. Neither of the variables differed across conditions (all ps >.20). Results Manipulation checkParticipants in the effort attribution condition (M = 2.10, SD = 1.22) were more likely to attribute the investment manager's performance to effort than those in the talent attribution condition (M = 5.53, SD = 1.53; t(233) = −18.99, p <.001, d = −2.48). Relationship expectationsAgain, in support of H1, participants expected a more communal relationship when the investment manager's performance was attributed to effort (M = 3.21, SD = 1.21) rather than to talent (M = 2.75, SD = 1.14; t(233) = 2.98, p <.01, d =.39). Perceived warmthParticipants perceived the investment manager as warmer when his performance was attributed to effort (M = 4.47, SD = 1.05) rather than to talent (M = 3.87, SD = 1.17; t(233) = 4.15, p <.001, d =.54), in support of H2. To establish discriminant validity between perceived warmth and relationship expectations, we performed a confirmatory factor analysis. For each construct, the average variance extracted exceeded.50 (perceived warmth = .51, relationship expectations = .57). [20] test also revealed that both average variances extracted were higher than the shared variance of.15, confirming that they represent distinct constructs. Mediation analysesWe tested perceived warmth as a possible mediator with a bootstrapping analysis using PROCESS Model 4 ([45]; see Figure 3). Results revealed that the indirect effect of performance attributions on relationship expectations through perceived warmth was significant (indirect effect =.20, SE =.07, 95% confidence interval = [.09,.36]).Graph: Figure 3. Mediation analysis (Study 2a). DiscussionStudy 2a shows that consumers expect a more communal relationship with a service employee when the employee's performance is attributed to effort rather than to talent, because they perceive such an employee to be warmer. This study also established discriminant validity between perceived warmth and relationship expectations. Warmth judgment and communal relationship expectation, though correlated, are conceptually distinct constructs. [25] conceptually and empirically separated warmth perceptions (perceptions of a trait) and inferred communal intent (perceptions of a motive behind a trait or action). Perceived warmth of a service employee is a perceived trait of that employee that is not specific to a given service context, whereas a communal relationship expectation involves the predicted norms in the relationship with a service employee in a specific service context. This study also shows that performance attributions do not necessarily change perceived competence of the service employee, which is in line with prior work suggesting that effort and talent are two different types of attribution of competence ([56], [57]). In the next study, we test our proposed mechanism by directly manipulating the warmth of the employee. Study 2b: Manipulating Service Employee WarmthStudy 2b uses a moderation-of-process strategy ([50]) to manipulate the warmth of a service employee to provide further evidence for warmth as a mediator for the effect of performance attributions on relationship expectations. If effort (vs. talent) attribution leads consumers to expect a more communal (or less exchange-oriented) relationship because the employee is perceived as warmer, information signaling that the employee is warm should attenuate the proposed effect. We employ a 2 (performance attribution: effort vs. talent) × 2 (warmth: yes vs. no) between-subjects design. MethodThree hundred seventy-one undergraduate students (233 women; mean age = 20.30 years) from a large university in Hong Kong participated in this laboratory experiment. Participants read website information about a physician whose competent performance (e.g., ""Dr. Lee received a peer review rating in the top 5% among general practitioners in Hong Kong"") was attributed to either his effort (e.g., ""Dr. Lee spends a lot of time [and] works really hard to develop personalized health improvement programs"") or talent (e.g., ""Dr. Lee has a sharp instinct [and is] naturally skillful at developing personalized health improvement programs""; see Web Appendix W6). Warmth manipulationTo manipulate the warmth of the physician, we provided additional information that can increase warmth perceptions but is not directly related to the employee's behavior toward his or her customers. Warmth is particularly relevant to the prosocial domain, because people rely on warmth judgments to predict whether a person is well-intentioned toward other people ([19]). Thus, we manipulated the warmth of a service employee by informing participants that the employee donates a part of his earnings to various charity organizations. No such information was mentioned in the control condition.To test the effectiveness of the warmth manipulation, we conducted an independent pretest (n = 170; 106 women; mean age = 20.88 years). After reading the website information (excluding information on performance attributions), participants indicated the extent to which they perceived the physician to be warm and competent, as in Study 2a. A t-test revealed that participants perceived the physician as warmer in the warmth condition (M = 5.11, SD =.77) than in the no-warmth condition (M = 4.44, SD = 1.13; t(168) = 4.45, p <.001, d =.69). However, perceived competence did not differ across the two conditions (p =.22). MeasuresWe measured relationship expectations with the physician as in the previous studies. We also measured participants' expectations about the employee's service process quality with four items (e.g., ""unfavorable/favorable,"" ""bad/good""; α =.93) and service outcome quality with four items (e.g., ""unfavorable/favorable,"" ""bad/good""; α =.92). Performance attribution manipulation did not change these expectations (all ps >.30). ResultsWe ran a 2 (performance attributions: effort vs. talent) × 2 (warmth: yes vs. no) ANOVA on relationship expectations. The results revealed a significant main effect of performance attributions (F( 1, 367) = 8.07, p <.01,  ηp2  =.02), no significant effect of warmth (F( 1, 367) =.49, p =.48,  ηp2  =.001), and a significant interaction (F( 1, 367) = 4.07, p <.05,  ηp2  =.01). Planned contrasts revealed that our previous findings were replicated in the no-warmth condition; specifically, effort attribution (M = 4.03, SD = 1.15) induced more of a communal relationship expectation than did talent attribution (M = 3.47, SD = 1.05; t(367) = 3.41, p =.001, d =.51), in support of H1. In contrast, this effect was attenuated in the warmth condition (Meffort = 3.88, SD = 1.09 vs. Mtalent = 3.79, SD = 1.14; t(367) =.59, p =.56, d =.08), in support of H2. DiscussionIn this study, we directly manipulated the mediating variable (i.e., warmth of a service employee). The results support our mechanism that effort (vs. talent) attribution leads consumers to perceive a service employee to be warmer by showing that information signaling that the employee is warm attenuates the effect of performance attributions on relationship expectations. This study also shows that performance attributions do not change participants' expectations about the employee's service process quality and service outcome quality, thus ruling these out as possible alternative explanations for our proposed effects. In the next study, we examine the effect of performance attributions on customer attention. Study 3: Customer Attention to Service Employee InformationTo enhance the validity of our findings, Study 3 provides further evidence for the effect of performance attributions by using an alternative, more objective measure of relationship expectations: consumers' attention to service employee information. We argue that consumers' relationship expectations can be manifested in their attention while reading advertisements. Prior work has shown that under an exchange relationship, individuals focus on their counterparts' instrumental function to ensure that the benefits they are to receive fulfill their own goals ([ 2]; [ 3]). Furthermore, [47] argue that when individuals focus on others' instrumental function, they tend to overlook the facts relating to the personal lives and experiences of those others. Therefore, we predict that if talent attribution leads to greater expectation of an exchange relationship, consumers will pay more attention to information pertaining to the service employee's instrumental function (e.g., what the service employee can do for them) than to personal information about the employee (e.g., personal background information). We use an eye-tracking technique to capture participants' attention toward service employee information, which allows us to measure a subconscious or preconscious reflection of relationship expectations ([44]). MethodOne hundred forty-seven undergraduate students (110 women; mean age = 20.82 years) from a large university in Hong Kong participated in this laboratory experiment. We used an eye-tracking device, The Eye Tribe, powered by the software GazeLab (30 Hz), which collects raw eye movement data points every 33.3 milliseconds. This eye tracker was integrated into a 15.4-inch monitor at a resolution of 1,680 × 1,050 pixels. As participants viewed the stimuli shown on the screen, a discreet infrared camera located below the screen unobtrusively recorded participants' attention. Performance attribution manipulationParticipants were told that their university's medical society was editing a newsletter, and they were asked to read an article featuring an interview with a physician from the university's health clinic. On the first page of the article, we manipulated performance attributions as in Study 2b. When we defined the performance attribution information as an area of interest (i.e., a selected region of the stimulus of which eye-movement metrics are extracted), participants in the two conditions did not differ in terms of the attention they paid to the manipulation stimuli (Meffort = 4.58 seconds, SD = 3.70; Mtalent = 4.23 seconds, SD = 3.02; t(145) =.61, p =.54, d =.10). We excluded any participants who did not fix their attention on the performance attribution information because they were neither exposed to the effort nor the talent attribution manipulation.To test the effectiveness of our manipulation, we conducted an independent pretest (n = 92; 67 women; mean age = 20.60 years). After reading an article about the physician, participants indicated the extent to which they thought the physician had achieved his level of performance because of effort or talent, using a semantic differential scale as in previous studies. Participants in the effort attribution condition (M = 2.41, SD = 1.37) were more likely to attribute the physician's performance to effort than those in the talent attribution condition (M = 4.60, SD = 1.53; t(90) = −7.26, p <.001, d = −1.51). Participants also indicated the extent to which they perceived the physician to be warm and competent, as in Study 2a. We also measured participants' expectations about the overall quality of the physician (1 = ""very bad,"" and 7 = ""very good""). A t-test revealed that participants perceived the physician as warmer when his performance was attributed to effort (M = 5.42, SD =.82) rather than to talent (M = 5.07, SD =.83; t(90) = 2.04, p <.05, d =.42). However, perceived competence and expected overall quality did not differ across the two conditions (ps >.30). Dependent variable and control variablesParticipants were then presented with two columns of additional information about the physician (see Web Appendix W7). One column presented person-related information about the physician, such as the physician's background (e.g., ""Dr. Lam is 32 years old and was born and raised in Hong Kong""). The other presented job-related information, such as information about what the physician could do for the participants (e.g., ""Dr. Lam investigates [students'] current health states and conducts physical examinations to establish risk factor levels""). We counterbalanced the presentation of each column. Each of the two columns of service employee information was defined as a separate area of interest. For each participant, we calculated the ratio of time spent fixating on person-related information to the time spent fixating on job-related information. Because this ratio was positively skewed (skewness = 8.55, SE =.20; Shapiro–Wilk's W =.32, p <.001), we used the log-transformed ratio as the dependent measure. Moreover, we measured participants' knowledge of health care services, mood, and arousal as control variables and found that these variables did not differ across conditions (all ps >.40). ResultsA 2 (performance attributions: effort vs. talent) × 2 (presentation order: person-related information on the left vs. right) ANOVA revealed that the log-transformed ratio of fixation time was higher when the physician's performance was attributed to effort (M =.26, SD = 1.12) rather than to talent (M = −.05, SD = 1.24; F( 1, 143) = 4.00, p <.05,  ηp2  =.03), in support of H1. Thus, when the performance was attributed to effort (vs. talent), participants spent a relatively greater proportion of time attending to the physician's person-related information than to the physician's job-related information. The main effect of the presentation order was significant; the log-transformed ratio of fixation time was higher when person-related information was presented on the left (M =.63, SD = 1.23) than on the right (M = −.44, SD =.85; F( 1, 143) = 38.53, p <.001,  ηp2  =.21), consistent with the tendency to read English text from left to right ([49]). However, the interaction between performance attributions and presentation order was not significant (F( 1, 143) =.02, p =.90,  ηp2  <.001). DiscussionThis study validates the theoretical and managerial importance of relationship expectations by showing that it can be reflected in consumers' attention to advertisements, not just in self-reported relationship expectation measures. Specifically, effort attribution leads consumers to spend a greater proportion of time attending to person-related information compared with job-related information about the service employee, consistent with the norms of communal relationships. This study provides practical insights on how to utilize performance attributions in communication messages. For instance, firms often communicate their service employees' personal background information to enhance consumers' connection with the employees ([55]). Our findings suggest that in such a situation, firms can attribute their employees' performance to effort rather than to talent. We also showed that the observed effects cannot be attributed to changes in competence or quality perceptions. In the next section, we develop a hypothesis regarding downstream consequences of relationship expectations and present two studies to provide empirical evidence supporting the hypothesis. Service Employee Performance Attributions and Customer Helping BehaviorsTo demonstrate the managerial and practical importance of service employee performance attributions, we examine downstream consumer behaviors resulting from relationship expectations. Specifically, we examine customer helping behaviors for firms as a result of relationship expectations. Drawing on prior work, we define customer helping behaviors as voluntary and discretionary behaviors toward firms that aid the firms beyond those required in the purchase of products and services ([ 8]; [10]), which can include spreading WOM (e.g., sharing product/service information on one's social networks), providing suggestions for product and service improvements, participating in firm activities, and helping other customers ([ 8]; [23]; [26]). Although the link between relationship expectations and customer helping behaviors has not been directly tested, prior research has suggested that customers are more likely to engage in helping behaviors when they believe a service employee places the welfare of the customers above the employee's own immediate self-interest ([10]), which is consistent with characteristics in communal relationships ([ 3]). Therefore, we predict that when an employee's performance is attributed to effort, thus inducing more of a communal relationship expectation, consumers will have a higher likelihood of engaging in helpful behaviors. Formally, H3:  Consumers are more likely to engage in customer helping behaviors toward a firm when its service employees' competent performance is attributed to dedicated effort rather than to natural talent.Prior research has identified both WOM and idea provision as important customer helping behaviors that can promote firm interests. Scholars have found that WOM can influence the way consumers make purchase decisions and, thus, affect sales ([ 6]), and that customers' participation in idea provision can enhance new product financial performance ([12]). In the next two studies, we test the effect of performance attributions on these two customer helping behaviors. In Study 4, we used a real firm context and measured individuals' WOM behaviors. We show that customers are more likely to help a firm share information on social networks when the employees' performance is attributed to effort than to talent. In Study 5, we conducted a field experiment to examine customers' provision of new product ideas. The findings indicate that effort attribution makes customers more likely to provide new product ideas. Study 4: Performance Attributions and WOM BehaviorsIn Study 4, we explore WOM behaviors as a downstream consequence of relationship expectations. We predict that when a firm highlights its service employees' dedicated effort (vs. natural talent), thus inducing a more communal-oriented relationship expectation, customers will be more likely to share the firm's information on social networks. MethodOne hundred fifty-five undergraduate students (98 women; mean age = 20.21 years) from a large university in Hong Kong participated in this laboratory study for monetary compensation. To increase realism of the experimental context, we used a real fitness center in Hong Kong, which operates in multiple locations and offers two types of classes with trainers: one combining yoga and fitness training, and the other combining Thai boxing and fitness training. Performance attribution manipulationParticipants were given website information about this fitness center and its trainers. They were told that the fitness classes were instructed by a team of highly qualified fitness trainers who have won awards and championships in Hong Kong and overseas. We attributed these performances to either effort (e.g., ""A group of hardworking trainers...will dedicate their efforts"") or talent (e.g., ""A group of talented trainers...have good natural skills""; see Web Appendix W8).We also conducted an independent pretest (n = 80; 55 women; mean age = 20.69 years). Participants in the effort attribution condition (M = 2.75, SD = 1.37) were more likely to attribute the fitness trainers' performance to effort than were those in the talent attribution condition (M = 4.89, SD = 1.36; t(78) = −6.97, p <.001, d = −1.57), indicating that our manipulation was successful. Participants also indicated the extent to which they perceived the fitness trainers to be warm and competent, as in Studies 2a and 3, and how experienced the trainers seemed to be (1 = ""not at all,"" and 7 = ""very much""). A t-test revealed that participants perceived the fitness trainers as warmer when their performance was attributed to effort (M = 4.85, SD =.92) than to talent (M = 4.26, SD = 1.25; t(78) = 2.44, p <.05, d =.54). However, perceived competence and experience did not differ across the two conditions (ps >.10). Sharing of website on social networks (WOM behaviors)Participants then read a message from the fitness trainers asking participants for their help to share the fitness center's website on social networks. Following [14] measure of WOM behaviors, participants were led to believe that by clicking a share button, they would share the website on a social network of their choice. After choosing their favored social network(s), participants were informed that they would not actually share the website. As an incentive, customers who chose to share the website could enter a lucky draw for a chance to win a free trial class at the fitness center (worth HK$200 or US$25). MeasuresWe measured relationship expectations with the fitness trainers as in the previous studies. We also measured participants' general tendency to share information on social media (1 = ""never,"" and 7 = ""very frequently""), which did not differ across conditions (p >.50). Results Sharing of website on social networks (WOM behaviors)A cross-tabulation analysis revealed that participants in the effort attribution condition (58.97%) were more likely to share the fitness center's website on social networks than those in the talent attribution condition (42.86%; χ2( 1) = 4.03, p <.05), in support of H3. Relationship expectationsA t-test analysis revealed that participants expected a more communal relationship when the fitness trainers' performance was attributed to effort (M = 3.93, SD = 1.30) than to talent (M = 3.31, SD = 1.30; t(153) = 2.95, p <.01, d =.48), in support of H1. Mediation analysesWe tested relationship expectations as a mediator for the effect of performance attributions on sharing behavior with a bootstrapping analysis using PROCESS Model 4 ([45]). Results revealed that the indirect effect of performance attributions on sharing behavior through relationship expectations was significant (indirect effect = −.41, SE =.19, 95% confidence interval = [−.85, −.13]). DiscussionUsing a real firm context, Study 4 offers important marketing implications by examining WOM behaviors as a customer outcome of relationship expectations. The findings support our prediction that when a firm highlights its service employees' dedicated effort as opposed to their natural talent, thus inducing a more communal relationship expectation, customers are more likely to engage in helpful behaviors by sharing the firm's information on social networks. Instead of providing an exact performance level as in previous studies, we offered a description of the fitness trainers' achievements (i.e., a team of highly qualified fitness trainers who have won awards and championships) to generalize our findings. In this study, we also showed that the trainers described as hardworking were perceived to be warmer, but not more competent or more experienced, than those described as talented. In the next study, we examine the effect of performance attributions on another type of customer helping behaviors. Study 5: A Field Experiment on Customer New Product Idea ProvisionIn Study 5, we conducted a field experiment at the coffee shops of an international coffee chain to test the effect of performance attributions on customers' provision of new product ideas. This coffee chain employs user-design philosophies to generate new product ideas through its website and has implemented many crowdsourced ideas. We predict that customers will be more likely to provide new product ideas when firms highlight their service employees' dedicated effort (vs. natural talent), thus inducing more of a communal relationship expectation. Method ProcedureOver a two-week period, we launched a ""Share Your Ideas"" campaign (hereinafter, ""campaign"") at two locations of the coffee chain. In the shops, we prominently displayed marketing materials (e.g., posters on walls, poster stands, table stickers) highlighting the baristas' dedicated effort for one week, and those highlighting their natural talent for another week. To control any confounding effects associated with particular dates, we simultaneously ran the campaign at two coffee shops, each located in a different large university in Hong Kong. We counterbalanced the performance attribution conditions between the two shops (i.e., talent attribution condition in Shop A and effort attribution condition in Shop B in the first week, and vice versa in the second week).We contracted a professional graphic designer to create the campaign's marketing materials (for sample materials, see Web Appendix W9). The marketing materials in the effort attribution condition highlighted the baristas' effort (e.g., ""We are a group of hardworking baristas! Please share your beverage ideas with us. We put a lot of effort into creating perfectly composed drinks""), whereas those in the talent attribution condition highlighted the baristas' talent (e.g., ""We are a group of talented baristas! Please share your beverage ideas with us. We are naturally skillful in creating perfectly composed drinks""). We displayed the marketing materials throughout the shops (see Web Appendix W10). Feedback formsWe placed feedback forms throughout the shops that customers could voluntarily pick up, fill out with their ideas and suggestions, and submit to a collection box. The feedback forms included a performance attribution manipulation (for samples of the feedback forms, see Web Appendix W11). We measured participants' general liking of the coffee chain (1 = ""not at all,"" and 7 = ""very much"") and frequency of visits (1 = ""never,"" and 7 = ""very frequent"") as control variables. We also measured perception of the baristas' beverage-making skill level (1 = ""not good at all,"" and 7 = ""very good"") to ensure that the performance attribution manipulation did not lead to differences in perceived competence of the baristas. As an incentive for their participation, customers who submitted a feedback form could enter a lucky draw for a chance to win a HK$300 (US$38) coffee chain coupon. Results Submission of feedback formsTo test the effect of performance attributions on customers' likelihood of submitting a feedback form, we examined the number of submitted feedback forms as a percentage of the total number of sales transactions. We obtained the numbers of weekly sales transactions of the two coffee shops from their managers and found that the number of total transactions was not significantly different across the two shops.We conducted three types of analyses on customers' likelihood of submitting a feedback form. First, a cross-tabulation analysis indicated that customers were more likely to submit a feedback form when they were exposed to effort attribution information than to talent attribution information (5.24% vs. 3.36%; χ2( 1) = 40.21, p <.001), in support of H3. In addition, two separate analyses showed that the finding was consistent for both Shop A (4.38% vs. 2.90%; χ2( 1) = 16.99, p <.001) and Shop B (6.48% vs. 3.99%; χ2( 1) = 24.49, p <.001).Second, we ran a binary logistic regression of the submission of feedback forms (1 = submitted, 0 = not submitted) on performance attributions (dedicated effort vs. natural talent), shop dummy (Shop A vs. Shop B), and their interaction. There was a significant main effect of performance attributions (b = −.51, SE =.10, Wald( 1) = 24.04, p <.001, Exp(B) =.60). Thus, customers in the effort attribution condition were more likely to submit a feedback form than were those in the talent attribution condition, in support of H3. There was a main effect of the shop dummy (b = −.41, SE =.09, Wald( 1) = 20.05, p <.001, Exp(B) =.66) and a nonsignificant interaction (b =.08, SE =.15, Wald( 1) =.31, p =.58). Two separate logistic regression analyses (one for each shop) indicated that customers in the effort attribution condition were more likely to submit a feedback form than were those in the talent attribution condition for both Shop A (b = −.43, SE =.11, Wald( 1) = 16.76, p <.001, Exp(B) =.65) and Shop B (b = −.51, SE =.10, Wald( 1) = 24.04, p <.001, Exp(B) =.60).To further enhance the robustness of our findings, we adopted the rare events logistic regression method (ReLogit; [34]). Given that our binary event of interest (i.e., submission of feedback forms) was relatively rare (4.31% of the sample), ReLogit corrects for rare event biases and standard error inconsistency, thus providing more accurate estimates than traditional logistic regression models. The ReLogit results were consistent with those from the logistic regression models. Number of suggestions providedWe further tested the effect of performance attributions on the number of suggestions provided. Two research assistants blind to the research hypotheses independently counted the number of suggestions provided on the submitted feedback forms (Cohen's kappa =.71, p <.001), and disagreements were resolved through discussion. They were instructed to count only the related suggestions and exclude suggestions unrelated to the given question on the coffee chain's beverage offerings (e.g., ""I love you [the name of the coffee chain]"").We used a Poisson regression, because the dependent variable was count data ([15]). We regressed the number of suggestions on performance attributions, shop dummy, and their interaction. The results revealed a significant main effect of performance attributions (b =.28, SE =.10, z = 2.91, p <.01), in support of H3. There was also a significant main effect of shop dummy (b = −.23, SE =.11, z = −2.07, p <.05) and a significant interaction (b = −.41, SE =.15, z = −2.73, p <.01). Split-group Poisson regressions showed that participants at Shop A provided a greater number of suggestions in the effort attribution condition (M = 1.62, SD = 1.33) than in the talent attribution condition (M = 1.23, SD = 1.21; b =.28, SE =.10, z = 2.91, p <.01). However, the effect was not significant at Shop B (Meffort =.86, SD = 1.14; Mtalent =.97, SD =.95; b = −.13, SE =.11, z = −1.13, p =.26).These effects persisted after we controlled for participants' liking of the coffee chain and frequency of visits. Therefore, the effects could not be attributed to individual differences in these factors. Moreover, performance attributions did not change the extent to which participants perceived the baristas to be skillful (p =.44). DiscussionIn a natural field setting, Study 5 shows that when a firm highlights its service employees' dedicated effort (vs. natural talent), customers ( 1) are more likely to submit feedback forms and ( 2) provide a greater number of suggestions, though the latter effect was significant at only one shop. Moreover, in this study we did not provide the exact performance level so as to generalize our findings, although we believe that customers consider the coffee chain's baristas to be a competent group among coffee shop employees in general (especially among our participants, who actually visited the coffee chain). In addition, our performance attribution manipulation did not change perceptions of the baristas' beverage-making skill level. Thus, our effect cannot be attributed to participants' perception that baristas depicted as naturally talented (vs. hardworking) were more skillful and competent and, thus, were less likely to need suggestions from customers. We replicated the findings in a laboratory experiment, in which we also measured relationship expectations (for details, see Web Appendix W12). General DiscussionThe current research demonstrates that message cues that attribute a service employee's competent performance to dedicated effort (vs. natural talent) lead consumers to expect a more communal and less exchange-oriented relationship due to an increase in the perceived warmth of the employee. Study 1a showed that participants expected a more communal relationship with a service employee whose competent performance was attributed to effort rather than to talent, whereas Study 1b revealed that simultaneous attribution to both effort and talent yields an effect similar to that of effort attribution only. In directly measuring the perceived warmth of an employee, Study 2a showed that the effect of performance attributions on relationship expectations is mediated by this construct. We manipulated the perceived warmth of an employee in Study 2b and showed further support for the mediating role of warmth. In Study 3, we used eye-tracking technology and found that effort attribution led participants to pay more attention to person- than job-related information about the service employee, reflecting expectation of a more communal-oriented relationship.Studies 4 and 5 explored customer helping behaviors as downstream consumer outcomes of relationship expectations. In Study 4, we used a real firm context (i.e., fitness center) and showed that participants were more likely to spread WOM for a firm when its service employees' performance was attributed to effort than to talent. Finally, in Study 5, we conducted a field experiment and showed that effort attribution, which induced a more communal relationship expectation, made participants more likely to provide new product ideas. Theoretical ContributionsThe marketing literature has focused on attributions of one's own performance and demonstrated their impact on brand or product evaluations ([40]; [42]). The current research highlights the importance of studying attributions of others' performance, because even for the same level of performance, people's beliefs about performance attribution can change judgments of those others ([11]; [52]). For instance, people expect hardworking others to perform better on novel tasks ([11]). We suggest that it is also important to understand the role of attributions of others' performance in consumer outcomes in service relationships, because how a customer views a service employee's performance can determine the customer's relationship with the employee ([ 9]). Therefore, this study fills the gap in prior work by examining how attributions of service employees' performance influence consumers' relationship expectations with and behaviors toward the service employees.The current research also augments existing knowledge on the two fundamental dimensions of social judgment—competence and warmth—by linking the warmth–competence framework ([19]) with the literature on performance attribution ([16]; [57]). Prior work has investigated relationships between judgments of competence and warmth ([32]; [58]), mainly by examining how a certain level of competence is related to warmth perception. Extending the existing literature, the current research examines the attributions of competence as a new dimension of competence influencing warmth perception, holding the objective level of competence constant.The current research also enriches the existing literature by identifying performance attributions as an antecedent of relationship expectations. The marketing literature has focused mainly on the downstream consequences of a communal versus exchange relationship with consumers—for example, whether consumers' perceptions of a communal versus exchange relationship influences their evaluation of brands ([ 3]; [ 4]), loss aversion tendency ([ 5]), and responses to service failures (Wan, Hui, and Wyer 2011). However, given the lack of research on the antecedents of relationship expectations, marketers may have little practical guidance on how they can shape expectations about a particular type of relationship in the minds of consumers. Addressing this gap, we find that the attributions of service employees' competence can alter consumers' expectations about their relationships with the employees along the communal–exchange continuum. In addition, the current research suggests that relationship expectations can be reflected in consumers' attention, not just in self-reported relationship expectation measures. Our use of eye-tracking technology allowed us to measure the subconscious or preconscious reflection of relationship expectations.In addition, the current research contributes to the literature on customer helping behaviors by identifying performance attributions as a new antecedent of such behaviors ([ 8]; [23]; [26]). As customer helping behaviors (e.g., spreading WOM, providing new product ideas) are becoming notable marketing goals for brands and firms, factors that encourage such behaviors are both theoretically and managerially important. We have shown that effort attribution, as opposed to talent attribution, increases the likelihood of customer helping behaviors. Marketing ImplicationsOur findings offer practical implications, because firms can highlight either effort or talent as the primary source of service employees' competent performance to induce a relationship expectation that corresponds to their service propositions. For instance, firms that emphasize communality in their services (e.g., Disneyland, Starbucks) can attribute their employees' performance to effort, leading consumers to expect a more communal relationship with their employees. In contrast, if these firms attribute employee performance to talent, thus inducing a more exchange relationship expectation, the discrepancy between consumers' relationship expectations and their actual service experience may hurt service satisfaction.Our findings also demonstrate that, depending on whether a firm attributes its service employees' performance to effort or talent, consumers will pay attention to different types of service employee information, reflecting their expected relationships with the employees. This helps guide firms in designing their marketing materials. For example, when firms want their consumers to pay attention to a service employee's personal (job-related) information, they might want to attribute the employee's performance to effort (talent).Moreover, this research shows that the effect of performance attributions on relationship expectations has consequences for customer helping behaviors that offer managerial insights. Specifically, we gathered empirical evidence suggesting that marketers can implement effort or talent attributions in their communication messages to influence customers' actual WOM and idea provision behaviors. Marketers regard WOM—electronic WOM in particular—as ""one of the most significant developments in contemporary consumer behavior"" due to its ability to influence the way consumers make purchase decisions and affect sales ([ 6], p. 297). Marketers are also increasingly involving customers in idea generation for new products, because such a tactic can enhance new product financial performance ([12]). As firms strive to achieve these marketing goals, our research findings offer insights into how firms can motivate these customer helping behaviors using their communications messages. According to our findings, firms are advised to attribute their employees' performance to effort, rather than talent, when they want to encourage customers to share firm information on social networks or to suggest new products or services. We believe our proposed effect of performance attributions on relationship expectations can also influence other types of customer helping behaviors, such as participating in firm activities and helping other customers.What factors shape consumers' expectations about their relationship with a service employee is an important practical question, because it can have significant consequences on consumer outcomes ([ 3]; [ 4]; [54]). Although it is true that firms can develop communal relationships through other methods—for example, by generally treating customers well and satisfying them—these tactics require actual interactions with customers. The current research suggests that communication messages that do not involve interactions with customers also can move customers' relationship expectations along the communal–exchange continuum, in turn influencing consumer behaviors. Marketing practitioners can utilize this knowledge about highlighting effort and/or talent to design their website communications, print advertisements, and social media strategy going forward, or to reevaluate the effectiveness of their current communication strategies. Future ResearchThis article offers several fruitful directions for future research. First, future studies can examine whether the performance attribution effects can be extended to other contexts. For instance, our proposed effects may not be limited to person perception. Because people tend to view a relationship with a brand, product, or firm similarly to a relationship with a person ([21]; [39]), the attributions of brands' or firms' competent performance might influence consumers' perceived relationships with those brands or firms. Future studies could also explore service failure contexts. For example, researchers can investigate whether attributing poor service performance or negative service outcomes to an employee's lack of effort (or natural talent) can lead to differences in a consumer's willingness to forgive. In addition, future studies could explore how consumers might interpret information on performance attributions of firms whose performance is uncertain (e.g., startups).Even though our last two studies show that effort (vs. talent) attribution is more likely to increase customer helping behaviors, we do not argue that effort attribution is always more beneficial to firms than talent attribution. In a supplementary study (Web Appendix W13), we measured membership sign-up behavior as a different downstream behavior in the same fitness training context. The findings show that because customers who generally do not want to proactively interact with service employees during a service process (e.g., by offering their own opinions about the training program) prefer a more exchange-oriented (i.e., less communal-oriented) relationship with a service employee, firms are more likely to acquire them if the service employee's performance is attributed to talent rather than effort. Future studies could explore other consequences of service employees' performance attributions for consumer behaviors, such as loyalty to the same service employee and reactions to service recovery, as well as other individual and situational factors that influence customers' relationship preferences.In addition, future research could explore how relationship expectations may interact with actual service experience to affect customer satisfaction. For instance, customers who experienced an exchange-oriented relationship with a service employee in digital interactions may be less satisfied with the same experience when they are exposed to effort attribution (vs. talent attribution) that induces a more communal relationship expectation. Future research could also explore consumer heterogeneity in terms of attributions of service employee performance. Because the focal point of this research was to delineate the effects of service firms' performance attributions, we did not directly explore consumers' heterogeneity in their attributions, which might depend on the industry or context. This heterogeneity may be presumed to interact with service firms' endogenous attribution decisions. "
40,"How Consumers' Political Ideology and Status-Maintenance Goals Interact to Shape Their Desire for Luxury Goods This research distinguishes between the goal of maintaining status and advancing status and investigates how consumers' political ideology triggers sensitivity to a status-maintenance (vs. status-advancement) goal, subsequently altering luxury consumption. Because conservative political ideology increases the preference for social stability, the authors propose that conservatives (vs. liberals) are more sensitive to status maintenance (but not status advancement) and thus exhibit a greater desire for luxury goods when the status-maintenance goal is activated. Six studies assessing status maintenance using sociodemographic characteristics (Studies 1, 2, and 3a) and controlled manipulations, including ad framing (Study 3b) and semantic priming (Studies 4 and 5), provide support for this proposition. The studies show that the effect is specific to status maintenance and does not occur ( 1) in the absence of a status goal or ( 2) when the status-advancement goal (a focus on increasing status) is activated. Overall, the findings reveal that conservatives' desire for luxury goods stems from the goal of maintaining status and offer insights into how luxury brands can effectively tailor their communications to audiences with a conservative ideology.KEYWORDS_SPLITA key function of luxury goods—a €262 billion market in 2017 ([19])—is to signal consumer status ([26]; [29]; [67]). Status, broadly defined as the respect and admiration received from others ([46]), is a fundamental human goal ([ 4]) that drives consumers' desire for luxury goods ([10]; [25]). This research formally distinguishes between two goals related to status, one reflecting the desire to maintain one's status (hereinafter the ""status-maintenance goal"") and one reflecting the desire to advance one's status (hereinafter the ""status-advancement goal""). In practice, the notion of status maintenance frequently permeates luxury brand communications. Consider, for example, a watchmaker's famous slogan, ""You never actually own a Patek Philippe. You merely look after it for the next generation."" Similarly, DAMAC Properties, a Dubai-based luxury real-estate company, informs prospective buyers that its properties will ""complement [their] stature,"" and Rolex reminds potential buyers that ""Class is forever"" (Web Appendix A). When will consumers be more sensitive to messages emphasizing such status maintenance and subsequently desire luxury goods more?One answer to this question begins with the idea that people's views on status often stem from their broader social beliefs (e.g., [13]; [32]), the core of which is political ideology ([ 7]). Building on the finding that conservative (vs. liberal) ideology emphasizes the need to sustain the current social order ([36]), we posit that conservative political ideology (hereinafter ""political conservatism"") increases the importance of status maintenance but not status advancement or a general sensitivity to status (i.e., the value put on status in general). Because consumers tend to act on goals that are both important (i.e., valuable and prioritized; [24]) and activated (i.e., cognitively salient and accessible; [44]), we predict that political conservatism increases the desire for luxury goods when the status-maintenance goal is activated. We also propose that activating the status-maintenance goal among conservatives heightens their preference for social stability, increasing their desire for goods viewed as helping to maintain the social order, such as luxury goods. We probe whether this effect ( 1) stems from consumers' preference for social stability and ( 2) occurs when the status-advancement goal is activated or in the absence of a status goal.This research contributes to the literature in two ways. First, we contribute to the status literature by distinguishing between status-maintenance and status-advancement goals. Whereas the bulk of the work tends to treat status as a single construct and focuses on how the presence or absence of a status goal affects luxury consumption (e.g., [58]; [62]), our research reveals how status goals with different foci (status maintenance vs. status advancement) differentially affect luxury consumption. Notably, our work empirically demonstrates the importance of this conceptual distinction by showing that political conservatism increases sensitivity to the status-maintenance goal but not the status-advancement goal, subsequently inducing a greater desire for luxury goods.Second, we contribute to the nascent literature on political ideology and luxury consumption ([54]) by going beyond previous efforts tied to sociopolitical views and marketing, particularly the areas of political consumerism ([76]), political campaigns ([33]), and prosocial or environmental behaviors ([40]; [75]). [54] find that conservatives differentiate themselves through products that signal that they are better than others (vertical signaling) while liberals differentiate themselves through products that signal their uniqueness (horizontal signaling). In contrast with their study, which explores the different types of signaling strategies people employ through luxury goods, our approach examines how different status goals may lead consumers to engage in vertical signaling (i.e., desire for luxury products) or not (i.e., desire for nonluxury products). In doing so, we complement previous work by shedding light on the motivations of conservatives when purchasing luxury goods. By demonstrating that conservatives desire luxury goods more than liberals under the status-maintenance goal, we reveal that their aspiration for vertical signaling stems from their desire to keep their current status as it is.Finally, our findings also hold several practical managerial implications by offering a more sophisticated approach to luxury market segmentation. Because luxury products appeal to specific audiences, knowing how to segment and target them has long been central to the management of luxury brands ([20]). In practice, however, luxury brands often use vertical segmentation (e.g., by income), resulting in increasingly complex submarkets (e.g., ""true luxury, masstige, premium, ultrapremium, opuluxe, hyperluxe, affordable luxury""; [38]). Our study pinpoints an accessible, easy-to-measure variable—namely, political ideology—and identifies how and when it predicts consumers' appetite for luxury goods. Indeed, political ideology is regularly assessed through opinion polls (e.g., Pew Research Center, Gallup), is easily identifiable along a geographic map ([12]) and media outlets ([34]), and offers more granular consumer insights (e.g., town level; for examples of available data sources, see Web Appendix B).Overall, our findings imply that rather than targeting a segment on the basis of mere wealth or even status, a luxury brand that emphasizes status maintenance (e.g., Patek Philippe) may be more successful when targeting a wealthy conservative segment. This goal may be achieved by running targeted marketing and communications campaigns ( 1) on media platforms patronized by conservatives (e.g., Fox), ( 2) in conservative geographic areas (e.g., Texas), or ( 3) online, particularly on social media, by leveraging digital footprints indicative of conservatism (e.g., [15]) (e.g., of how brands match the media used to their consumers' political orientations, see Figure 1; for managerial guidelines, see Figure 2).Graph: Figure 1. Examples of media sponsorship by political leaning. Notes: Information collected from SponsorFeedback.com. Left-leaning platforms include MSNBC, NBC, CNN, CBS, and ABC. Right-leaning platform includes Fox. For each brand, we computed the percentage reflecting media sponsoring on the basis of the number of shows sponsored by each brand across left- and right-leaning media platforms. For example, AT&T sponsors three shows (The Rachel Maddow Show, Andrea Mitchell Reports, and Morning Joe) exclusively on MSNBC. Therefore, 100% of AT&T's sponsorship goes to left-leaning media.Graph: Figure 2. Decision guidelines for luxury brand managers. Status Goals and Luxury ConsumptionGoals are internal representations of desired states ([ 6]), be they physical needs (e.g., the need to eat) or self-actualization ([73]). Considered a fundamental human goal ([ 4]; [25]), status confers many psychological and social benefits to consumers ([23]; [50]). As such, status is a key factor in marketing because of its important role in shaping consumers' desire for luxury goods ([10]; [25]; [67]). Simply making status salient through reminders of successful similar people ([47]) or in the form of power threats ([22]) can increase the desire for luxury options, traditionally defined as high-quality, exclusive, and (often) conspicuous ([20]; [38]).Departing from the view that a single status goal (i.e., status maximizing) underlies consumers' desire for luxury, the current research proposes that the status goal has a dual nature, reflecting a desire to maintain or advance one's social standing, depending on the extent to which the desired status state is currently being or has yet to be experienced. When consumers believe they are currently experiencing the desired social status they have in mind (e.g., they hold a high position in their community because of their advanced degrees), they may focus on maintaining their social status (i.e., status-maintenance goal). By contrast, when consumers believe they have yet to experience the desired social status they have in mind, they may focus on advancing social status (i.e., status-advancement goal). Although most research has largely ignored this distinction, managers in the luxury industry recognize and appeal to both status goals. That is, while luxury brands' taglines often emphasize status maintenance, luxury marketers also appeal to consumers' desire to ""climb the ladder."" For example, Audi calls on consumers to ""update [their] status,"" Aston Martin announces that its car will ""add value"" to consumers' lives, and India-based tailor High Status Fabrics asserts that its tailor-made suits will ""elevate"" consumers (Web Appendix A).Building on this distinction, we investigated how political ideology, a key variable shaping consumers' view of the social strata, may uniquely influence the importance of retaining status and ultimately guide the desire for luxury goods. We begin with the idea that luxury goods represent status signals ([29]) that help ""conserve"" the social hierarchy by reducing uncertainty about consumers' roles and prerogatives. In other words, consumers ""read"" others' statuses through their consumption, which reinforces the social hierarchy over time ([ 3]) and helps maintain consumers' status in relation to others' ([10]; [14]; [67]). In seventeenth- and eighteenth-century Europe, for example, the so-called sumptuary laws governed the ownership and display of fashion (e.g., gold embroidery) for the purpose of sustaining the existing social order ([18]; [67]). Because consumers' views of status often rest on their broader social beliefs (e.g., [13]; [32]), we examine how political ideology, which lies at the core of broader social beliefs, systematically predicts the importance consumers put on status maintenance (vs. status advancement). Next, we turn to prior work on political ideology to build our hypotheses. Political Conservatism and Status MaintenanceThe term ""political ideology"" refers to beliefs and principles that reflect a person's views on how society should be governed ([ 7]). Political ideology is typically measured on a spectrum ranging from liberal to conservative, a classification that is judged as the most parsimonious ([51]) and also predictive of consumers' behavior ([35]). Conservatism emphasizes the importance of keeping things as they are ([17]), and as such, conservatives often engage in the same daily routines ([37]). For example, conservatives tend to prefer familiar to unfamiliar music ([27]) and favor established brands over nameless or new brands ([39]). Conservatism also triggers a greater sensitivity to the existing social structure. For example, conservatives tend to judge others on their position in the social strata rather than question the fairness of the social system. Accordingly, they tend to evaluate those with high status more favorably than those with low status, regardless of their own status, resulting in in-group favoritism among high-status conservatives and out-group favoritism among low-status conservatives ([45]).Building on these findings, we propose that political conservatism increases the importance of pursuing and satisfying the status-maintenance goal. The more conservative the person is, the more he or she will view the pursuit of status maintenance as important. Given the pervasiveness of people's tendency to ""look upward"" ([21]; [53]), however, there is no obvious reason to expect that political ideology would affect the status-advancement goal. In other words, we would expect status maintenance to be increasingly important to consumers as their political conservatism increases but status advancement to be invariant in relation to political conservatism. Confirming these predictions, a pilot study (for details, see Web Appendix C) showed that conservatives (M = 4.83, SD = 1.40) viewed status maintenance as more important than liberals (M = 3.67, SD = 1.79; F( 1, 76) = 7.61, p =.007) but conservatives (M = 3.70, SD = 1.77) and liberals (M = 3.60, SD = 1.94) did not differ in how important they viewed status advancement (F( 1, 76) =.04, p =.84).As consumers typically pursue multiple goals simultaneously ([44]), goal importance alone does not guarantee that a goal will be pursued. Instead, the extent to which a goal guides actual behavior will depend on its motivational (goal importance) and cognitive (goal activation) properties ([24]; [44]). Thus, a goal is most likely to shape consumer behavior when it is both important and activated ([24]; [44]). For example, although impulsive people put more importance than nonimpulsive people on satisfying the pursuit of pleasure, they may not automatically exhibit greater preferences for all pleasurable items (e.g., sweet food) unless that specific goal is activated ([55]). Similarly, although conservatives put greater importance on status maintenance than liberals, they may not automatically have a greater desire for goods that help maintain the social hierarchy (i.e., luxury goods) if there are other goals (e.g., relationship goals, health goals) that are more salient and cognitively accessible at any given moment. Therefore, we reason that the status-maintenance goal will motivate luxury consumption most when it is both important and activated; thus, we predict the following: H1  : Political conservatism increases the desire for luxury goods when the status-maintenance goal is activated, but it does not affect the desire for luxury goods when the status-advancement goal is activated or when there is no status goal. The Role of a Preference for Social StabilityWe further propose that the effect of political ideology on the desire for luxury goods when the status-maintenance goal is important and activated stems from consumers' increased motivation to keep things as they are—that is, a preference for stability. Prior research suggests that the motivation to keep things as they are permeates both personal (i.e., the desire to keep one's life regular and predictable, namely, preference for personal stability) and social domains (i.e., the desire to keep the social structure as is, namely, preference for social stability; [17]; [37]). If conservatives' desire for luxury stems from their aim to keep the social structure as it is by visibly communicating their own status to others, we predict that their preference for social stability (vs. personal stability) will drive the effect. Although it is unclear how purchasing luxury goods may satisfy the desire to stick with the same daily routines (behaviors tied to personal stability), consumers may be keen to turn to visual status symbols that help them keep the social structure as it is. Therefore, we predict that the preference for social stability will underlie conservatives' (vs. liberals') greater desire for luxury goods when the status-maintenance goal is important and activated (Figure 3): H2  : When the status-maintenance goal is activated, increased preference for social stability mediates the effect of political conservatism on the desire for luxury goods.Graph: Figure 3. Conceptual map. Overview of Studies and MethodologySix studies test the hypotheses by employing different measures of political conservatism and desire for luxury (Table 1). Studies 1–3a measure the role of status-maintenance activation through status position, while Studies 3b–5 use external manipulations. Goal activation may depend on internal chronic factors (i.e., typically stable features such as character traits) or external manipulations ([44]). For example, although an indulgence goal (i.e., focusing on immediate enjoyment, such as spending on luxury goods, over long-term considerations) is typically more strongly activated among people low than high on hyperopia, momentarily activating an indulgence goal through a manipulation (e.g., a writing task asking participants to focus on the role of enjoyment in their lives when deciding how to spend money) raises the desire to indulge for any person regardless of their hyperopia score ([30]).GraphTable 1. Summary of Study Design and Results.  10022242918799700 a Status-maintenance goal activation.20022242918799700 b Study 3a does not measure desire for luxury, but the process (i.e., preference for social stability); therefore, its results reflect the preference for social stability.30022242918799700 Notes: IV = independent variable; DV = dependent variable; NR1 and NR2 indicate two studies not reported herein but included in the Web Appendix. NR1 is a replication of Study 4 with an American sample (Web Appendix W), and NR2 is a replication of Study 5 with a French sample (Web Appendix V). Spotlight analysis examines the difference between conservative and liberal at one standard deviation above the status position mean (i.e., high) and one standard deviation below the status position mean (i.e., low). Simple effect of political conservatism examines the slope of political conservatism in the status-maintenance goal condition (i.e., high) and in the status-advancement goal condition (i.e., low).Studies 1 and 2 establish that the desire for luxury goods increases with political conservatism when the degree of status-maintenance activation is high but remains unchanged when it is low. Specifically, Studies 1 and 2 use consumers' current status position in the social strata to infer the degree of status-maintenance activation. Given that consumers tend to protect and maintain favorable conditions ([64]), a high-status position should trigger greater activation of status maintenance than a low-status position. Indeed, high-status consumers express greater concerns about maintaining their status than low-status consumers ([60]). Furthermore, a pretest reveals that the degree of status-maintenance activation increases along with status position (Web Appendix D). Study 1 examines 21,999 car purchase decisions and finds that Republicans tend to purchase more luxury cars than Democrats when the status-maintenance goal is positionally activated (i.e., high status position). Study 2 replicates the effect using a different measure of status position.Subsequent studies aim to rule out a potential alternative explanation for the findings in Studies 1 and 2. That is, Studies 1 and 2 compare a condition when the degree of status-maintenance activation is high with one when it is low, and thus it could be argued that the effect comes from the difference in the level of status activation (i.e., how much consumers focus on and think about status in general) rather than in the level of status-maintenance activation (but see Web Appendix D for the pretest result showing that the level of status activation does not differ across status positions). Studies 3b to 5 further address this account by directly manipulating the status-maintenance goal. In addition, they use a condition in which a status goal is activated, but without an emphasis on maintenance (i.e., status-advancement activation condition) as another control condition to strengthen the argument that maintaining status (not just any status goal) drives the effect. To this end, we manipulate both status-maintenance and status-advancement goals and show that political conservatism triggers a greater desire for luxury goods when the status-maintenance goal is activated; however, this effect does not occur when the status-advancement goal is activated or in the absence of a status goal.Studies 3a and 3b provide evidence for the underlying role of preference for social stability. Specifically, these studies show that when the degree of status-maintenance goal activation is high (i.e., high-status position), political conservatism increases the preference for social stability (Study 3a) and that this shift in preference mediates the effect of political conservatism on the desire for luxury goods (Study 3b). Finally, Studies 4 and 5 activate status goals using a manipulation independent of the consumption task and demonstrate how they can spill over to luxury consumption. Study 5 also directly varies the product framing.Across the studies, we interpret political ideology as a generalized personality orientation along the liberal–conservative spectrum and capture the construct both categorically as a party affiliation (Republican vs. Democrat) and continuously as a degree of political conservatism ([37]; [74]). This approach follows the use in prior research of continuous measures of political ideology ranging from liberal to conservative (e.g., [40]; [54]). Thus, we use the terms ""conservatives"" and ""Republicans"" (when measured categorically) or ""political conservatism"" (when measured continuously) interchangeably. Note that though we use the term ""political conservatism"" for the simplicity of language, we view liberalism and conservatism as the two ends of a single spectrum capturing variation in preference for stability, the mechanism at work in our focal effect. Therefore, although we interpret the results by focusing on political conservatism, the opposite interpretation focusing on political liberalism is also possible. Across all studies, we systematically apply the same sample filtering criteria and include the same set of covariates (i.e., age, gender, and income) in our analyses (Web Appendix E). Study 1: How Does Political Conservatism Affect Luxury Car Purchases?This study tests the main hypothesis (H1) by leveraging a unique secondary data set measuring political ideology, status position, and actual car purchases. We assess the degree of status-maintenance activation from a consumer's current status position. We predicted that political conservatism would increase the likelihood of purchasing a luxury car among consumers with high status but not low status. Overview and Data DescriptionWe analyzed car purchase data between October 2011 and September 2012 from a survey conducted by a U.S.-based consulting company (Strategic Vision). Specifically, the survey was sent to car buyers across 50 states and the District of Columbia 3 to 4 months after the date of purchase. Consumers voluntarily filled out the survey at their homes or offices. Of the 416,571 consumers in total, 38,939 disclosed their political affiliation. After we accounted for the control variables, the final sample size was 21,999 consumers (35.36% female; Mage = 53.64 years). The survey also included other questions not tied to our hypotheses. Political conservatismCar buyers revealed their political affiliation categorically: Republican, Democrat, Independent, Libertarian, Green, Tea Party, and other. Of those who disclosed their political affiliation, 12,881 (33%) identified themselves as Republicans and 12,000 as Democrats (31%). We excluded 36% of consumers who did not identify themselves as Republican or Democrat (a percentage similar to prior work; e.g., 32% in [49]) from the main analyses. After we included the control variables, the final sample consisted of 11,324 Republicans and 10,675 Democrats. Desire for luxury brandsWe classified the cars in the data set as nonluxury or luxury (for the full list, see Web Appendix F) using the classification published by Luxury Society, a leading Switzerland-based analyst and news publisher for the luxury industry. This classification relies on the volume of luxury-related online queries performed on search engines such as Google in 2011 in the United States. Among all brands, 22% (78%) are classified as luxury (nonluxury). Status positionEducation and income are two key foundational facets of status ([14]; [70]), respectively reflecting focal intangible and tangible positional assets tied to people's rank in the social hierarchy ([ 4]). Following this perspective, we assessed consumers' status position through their socioeconomic status (SES), or the sum of their standardized education and income ([ 5]; [42]). This approach is similar to that of prior studies in the status literature that combine education and income to measure status position ([ 1]; [ 5]; [43]). In our data, education fell into five categories: ( 1) did not finish high school, ( 2) high school graduate, ( 3) did not finish college, ( 4) college graduate, and ( 5) postgraduate degree; income included 25 intervals (i.e., less than $10,000 = 1, over $500,000 = 25). ResultsWe performed a logistic regression on political conservatism (Republican = 1, Democrat = 0), SES, and their interaction to predict luxury car purchases, with age and gender as covariates. As expected, political conservatism (β =.38, z = 10.74, p <.001) and SES (β =.54, z = 37.94, p <.001) increased luxury car purchases. There was also a significant political conservatism × SES interaction (β =.09, z = 2.98, p <.01; Table 2, Model 2). To probe the interaction, we conducted a spotlight analysis to examine the effect of political conservatism on luxury car purchases at both high and low levels of SES. As this study measures actual purchases, we used objective indicators of SES based on [65], [66]) to determine high and low levels of SES in our sample (Web Appendix G). As hypothesized, political conservatism significantly increased luxury car purchases among high-SES consumers (β =.35, z = 9.65, p <.001), while it had no significant impact on luxury car purchases among low-SES consumers (β = –.03, z = –.18, p >.8).GraphTable 2. Study 1: Luxury Car Purchase Likelihood.  40022242918799700 ***p <.01. ****p <.001.50022242918799700 Notes: Standard errors are in parentheses. Political conservatism is coded as 1 if Republican and 0 if Democrat. Gender is coded as 1 if male and 0 if female.To probe the robustness of the findings, we conducted additional analyses using education and income separately as single measures of status position. The focal finding that political conservatism increases luxury car purchase among consumers having high-status positions holds regardless of whether the status position is assessed through education (β =.35, z = 7.67, p <.001) or income (β =.20, z = 5.44, p <.001) alone (for details, see Web Appendix H, Robustness Analysis 3). Further tests of robustness entailed ( 1) employing a different classification of luxury cars (Web Appendix H, Robustness Analysis 1) and ( 2) widening the political ideology categorization to the Green and Tea parties (Web Appendix H, Robustness Analysis 2). The results from these tests systematically replicate the focal results. In addition, we empirically address the possibility that the effect stems from a greater desire for conventional brands, rather than luxury brands per se. Indeed, conservatives exhibit greater sensitivity to conventions and traditions ([37]), and luxury perceptions often rest on tradition and history ([38]). Ruling out this possibility, additional analyses (Web Appendix I) show that the effect holds for both luxury brands perceived as conventional and nonconventional. Overall, Study 1 provides robust evidence for H1 that political conservatism increases the desire to purchase a luxury car among consumers with high status but not among those with low status, presumably because high status activates the status-maintenance goal. DiscussionThe results of Study 1 show that high-SES Republicans were 9.8% more likely to purchase a luxury car than high-SES Democrats. To obtain a more specific estimate of the effect, we collected the average price for each car model in the data set. On average, high-SES Democrats spent $29,022 and high-SES Republicans $33,216 to purchase a new car. Practically, a luxury car seller may expect to gain a 14.45% increase in sales from high-SES Republicans than from high-SES Democrats.Of note, we found an unexpected main effect of political conservatism (i.e., the effect of political conservatism at the mean of SES). Given that we did not find a similar main effect in any of the other studies, we conjecture that this effect may stem from the average SES of the sample used in this study being higher than that of ( 1) the samples used in the other studies and ( 2) the U.S. population. Specifically, the average education level was college graduate, while less than 40% of Americans between ages 25 and 64 years had at least a 2-year college degree when the study was conducted. The mean income level was $72,500 (approximately within the 70th percentile of the U.S. population). Therefore, it is likely that the spectrum of SES in Study 1 mostly captured medium- to high-SES consumers. This interpretation is consistent with the survey participants being actual buyers of new cars. Study 2 aims to address this concern by using a different sample and measure of status position. While Study 1 shows real-life evidence of the effect, its nonexperimental nature makes assessing causality difficult. In addition, given that Study 1 contains only buyers of new cars willingly volunteering to take the survey, there is a potential for sample selection bias (i.e., a focus on wealthy customers). We address these issues in the following studies by replicating the effect in a more controlled setting (Studies 2 to 5) and directly manipulating status goals (Studies 3b to 5). Study 2: Assessing Consumers' Status-Maintenance Goal Activation Through Perceived Status Pos... MethodOne hundred ninety-four participants (56% female; Mage = 38 years) recruited from Amazon Mechanical Turk (MTurk) completed the survey for a small monetary compensation. They responded to measures of political conservatism, desire for luxury brands, status position, and demographics (i.e., age, gender and income), in that order. Political conservatismParticipants chose between one of three options: ""conservative,"" ""liberal,"" or ""neither"" ([28]). In line with [49], our analyses focused on 136 participants (57% female; Mage = 38 years) who reported being either Republican (N = 51) or Democrat (N = 85). Desire for luxury brandsParticipants indicated their desire for seven luxury and seven nonluxury U.S. fashion and car brands, which we pretested to vary the extent of perceived luxury and high status (Web Appendix J). We included only U.S. brands, as political conservatism can affect perceptions of foreign brands ([ 8]). We presented the brands sequentially, in random order, and participants indicated how much they liked each brand on a seven-point scale (1 = ""not at all,"" and 7 = ""very much""). Status positionWe used the McArthur scale (Web Appendix D; [ 1]). ResultsFollowing prior research ([69]; [72]), we used a difference score between evaluations of luxury and nonluxury brands as our dependent variable (DV). We first averaged participants' evaluations of luxury (El, α =.82) and nonluxury (Enl, α =.75) brands before computing a difference score (El – Enl) for each participant. Higher values indicated a greater desire for luxury than nonluxury brands.We regressed participants' desire for luxury brands on political conservatism (conservative = 1, liberal = 0), status position, their interaction, and the three covariates. The results revealed a significant political conservatism × status position interaction (β =.30, t(129) = 2.72, p =.007; Table 3). As predicted, a spotlight analysis conducted at one standard deviation above the mean of status position revealed a significantly greater desire for luxury brands among conservatives than liberals (β =.45, t(129) = 1.93, p =.055). A spotlight analysis at one standard deviation below the mean of status position revealed that conservatives had a lower desire for luxury brands than liberals (β = –.52, t(129) = –2.17, p =.032). Simple effect analyses revealed that status position significantly predicted the desire for luxury brands among conservatives (β =.32, t(129) = 3.27, p =.001) but not among liberals (β =.02, t(129) =.29, p =.77).GraphTable 3. Study 2: Preference for Luxury Brands Relative to Nonluxury Brands.  60022242918799700 ***p <.0170022242918799700 Notes: Standard errors are in parentheses. Status position varies from 1 (bottom) to 10 (top) and are mean centered. Political conservatism is coded as 1 if conservative and 0 if liberal. Gender is coded as 1 if male and 0 if female.Replicating Study 1, Study 2 showed that political conservatism increased the desire for luxury brands among consumers with high but not low status, providing further support for our hypothesis (H1). In contrast with Study 1, there was no main effect of political conservatism at the mean status position. In Studies 3a and 3b, we test the underlying role of preference for social stability. Study 3a: Preference for Social Stability as a Function of Political Party and Current StatusStudy 3a aims to show that when the status-maintenance goal is important (i.e., when holding conservative political ideology) and activated (i.e., when having a high-status position), preference for social stability increases. That is, we expected conservatives to exhibit greater preference for social stability than liberals at high- but not low-status positions. MethodOne hundred seventy-four participants (50% female; Mage = 34 years) recruited on MTurk completed the survey for a small monetary compensation. They completed measures of political conservatism, preference for social stability, status position (McArthur scale), and demographics (i.e., age, gender and income), in that order. Political conservatismParticipants indicated the political party they identify with by choosing between one of three options: ""Republican,"" ""Democrat,"" or ""neither"" ([28]). Again in line with [49], our analyses focused on 123 participants (51% female; Mage = 34 years) who reported being either Republican (N = 41) or Democrat (N = 82). Preference for stabilityA series of seven-point scales (1 = ""not at all me,"" and 7 = ""very much me"") assessed preference for social stability. Items are ""I don't like when the social order changes too rapidly around me,"" ""Seeing too many changes in society tends to make me worry,"" and ""Too many changes and reforms to the current social structure makes me feel uneasy"" (α =.91). ResultsWe regressed participants' preference for social stability on political conservatism (Republican = 1, Democrat = 0), status position, their interaction, and the three covariates. The results revealed a significant effect of political conservatism (β = 1.15, t(116) = 3.74, p <.001) and a marginally significant political conservatism × status position interaction (β =.32, t(116) = 1.90, p =.059). A spotlight analysis conducted at one standard deviation above the mean status position revealed a significantly greater preference for social stability among conservatives than liberals (β = 1.73, t(116) = 4.34, p <.001). At the mean status position, conservatives exhibited a greater preference for social stability than liberals (β = 1.16, t(116) = 3.74, p <.001). However, a spotlight analysis at one standard deviation below the mean status position revealed no difference between conservatives and liberals (β =.58, t(116) = 1.26, p =.209). Simple effect analyses showed that status position significantly predicted the preference for social stability among conservatives (β =.45, t(116) = 2.94, p =.004) but not liberals (β =.13, t(116) = 1.19, p =.235).Study 3a provides initial evidence that preference for social stability may underlie the effect by showing that conservatives exhibit greater preference for social stability than liberals at high- but not low-status positions. Study 3b provides additional evidence for the role of social stability through moderation and mediation. Study 3b: Consumers' Preference for Social StabilityThe objectives of Study 3b were threefold. First, we used two standards of comparison: a condition of status-advancement activation and a condition that does not activate any status goal. Consistent with prior research showing that activating a status goal increases the desire for luxury compared with a no-status goal condition (e.g., [47]), we expected that activating either the status-maintenance goal or the status-advancement goal would increase the desire for luxury compared with a condition without any status goal activation. In addition, we expected that political conservatism would only affect the desire for luxury when the status-maintenance goal was activated, not when the status-advancement goal was activated or in the absence of a status goal. Second, to provide stronger support for the proposed causal relationship, we directly manipulated status goals, rather than inferring goal activation from status position as in Studies 1, 2, and 3a. Third, we provide support for our hypothesis (H2) by examining the mediating role of consumers' preference for stability. We measured preference for stability in both personal and social domains and expected that the desire to maintain the existing social structure (i.e., preference for social stability) would underlie the effect. MethodWe randomly assigned 403 participants (52% female; Mage = 36 years) recruited on MTurk to one of three conditions (status goal: status-maintenance vs. status-advancement vs. no-status). After indicating their political conservatism as part of an initial survey, participants took a second survey framed as a print ad evaluation task during which they viewed one of three versions of the same eyewear product (our status goal manipulation). Next, they indicated their willingness to pay (WTP) for the eyewear product in U.S. dollars (the main DV) before reporting their age, gender, income, and preference for personal and social stability. Political conservatismParticipants reported their political ideology (1 = ""extremely liberal,"" and 7 = ""extremely conservative"") and political affiliation (1 = ""strong Democrat,"" and 7 = ""strong Republican"") on seven-point scales. We averaged these items to form a measure of political conservatism (α =.91; M = 3.54, SD = 1.56; [39]). Status goal manipulationParticipants viewed one of three different versions of a print ad for an eyewear product (Web Appendix K). The no-status condition featured the eyewear as an economical line that was functional and affordable, while the status-maintenance and status-advancement conditions both featured the eyewear as a luxury line that was state-of-the-art and in limited supply. All three print ads featured the same image and layout, but the tagline varied according to the status goal condition: ""Eyewear for everyone"" (no-status), ""Update your status with status"" (status-advancement), and ""Keep your status with status"" (status-maintenance). A pretest confirmed that each of the ads successfully activated the target status goal, without varying the extent of emphasis on social status (Web Appendix L). Preference for stabilityA series of seven-point scales (1 = ""not at all me,"" and 7 = ""very much me"") assessed preference for personal stability (""I prefer life to be regular and predictable,"" ""I just want to stick to the same regular routine in my life,"" and ""I do not like changes in life""; α =.91). The same three-item measures as in Study 3a assessed preference for social stability (α =.94). ResultsA mixed general linear model performed on WTP, with status goal as a categorical variable, political conservatism as a continuous variable, and the covariates, revealed a main effect of status goal (F( 2, 394) = 16.98, p <.001) and a status goal × political conservatism interaction (F( 2, 394) = 2.50, p =.083). Consistent with the perspective that having a status goal increases consumers' WTP compared with the absence of a status goal ([47]), participants in both the status-maintenance (M = 64.30, SD = 58.30; F( 1, 394) = 33.97, p <.001) and status-advancement (M = 50.00, SD = 34.54; F( 1, 394) = 7.61, p =.006) conditions were willing to pay significantly more than participants in the no-status condition (M = 37.54, SD = 19.52) for the eyewear.To probe the interaction, we examined the slopes of political conservatism in each status goal condition. As expected, political conservatism predicted WTP for the eyewear emphasizing status maintenance (β = 5.45, t(396) = 2.33, p =.02). By contrast, there was no effect of political conservatism on WTP for the eyewear emphasizing status advancement (β = –1.65, t < 1, p =.487) or the eyewear without a status emphasis (β =.67, t < 1, p =.754; Figure 4). Furthermore, spotlight analyses revealed that WTP for the eyewear emphasizing status maintenance was significantly higher than WTP for the eyewear emphasizing status advancement among conservatives (one standard deviation above the mean of political conservatism; β = –25.21, t(256) = –3.00, p =.003) than among liberals (one standard deviation below the mean of political conservatism; t < 1, p =.627). Conversely, WTP for the eyewear emphasizing status maintenance was significantly higher than WTP for the eyewear without a status emphasis at all levels of political conservatism (all ps <.01).Graph: Figure 4. Study 3b: WTP (USD) for an eyewear product.We next examined the mediating role of preference for social stability (N = 401 for this analysis after removing two participants who did not complete these measures). Because status goals moderate the effect of political conservatism on WTP, as reflected in the significant effect of political conservatism in the status-maintenance condition, we expected social stability to mediate the effect only in this condition. Therefore, we coded the status-maintenance condition as 1 and the status-advancement and no-status conditions as –1 and conducted a moderated mediation analysis by using a bootstrapping procedure ([31], Model 15) with a generated sample size of 5,000, including the same covariates. This model estimated the effect of political conservatism on WTP directly as well as indirectly through social stability, with both direct and indirect effects moderated by status goals (Table 4). The first part of the model regressed preference for social stability on political conservatism and showed a significant main effect of political conservatism (β =.50, t(396) = 10.46, p <.001). The second part regressed WTP on political conservatism, status goals, preference for social stability, the political conservatism × status goals interaction, and the preference for social stability × status goals interaction. The results revealed a significant social stability × status goals interaction (β = 4.44, t(392) = 3.25, p =.001), while the political conservatism × status goals interaction was no longer significant (β =.83, t < 1, p =.59). Importantly, the bootstrapping analysis showed that the conditional indirect effect of political conservatism on WTP was significantly mediated by preference for social stability in the status-maintenance condition (β = 4.11, SE = 2.28; 95% confidence interval [CI] = [.45, 9.53]) but not in the other two conditions (β = –.35, SE =.52; 95% CI = [–1.38,.65]).GraphTable 4. Study 3: Test of Moderated Mediation by Social and Personal Stability.  80022242918799700 Notes: Political conservatism is mean centered. Status goal is coded as 1 if status-maintenance and –1 if status-advancement or no-status. Gender is coded as 1 if male and 0 if female.As an additional robustness check, we conducted the same analysis by comparing ( 1) status maintenance with status advancement and ( 2) status maintenance with no status. All results held at a standard level of significance (Web Appendix M). However, the same bootstrapping analysis using preference for personal stability as a mediator showed that preference for personal stability did not mediate the effect of political conservatism on WTP in the status-maintenance condition (β = 1.50, SE = 1.05; 95% CI = [–.05, 4.27]) or in the other two conditions (β = –.08, SE =.25; 95% CI = [–.64,.39]; for details, see Table 3). Overall, the indirect effect of political conservatism on WTP for luxury goods was mediated by preference for social stability and moderated by status goals, providing support for H1 and H2. Study 4: Momentary Manipulation of Status GoalsTo test the generalizability of our effect, in Study 4 we employed a status goal manipulation independent of the evaluation task—namely, a writing task that induced participants to focus on status maintenance or status advancement. In addition, we used another well-established measure of political conservatism consisting of multiple items ([48]). MethodWe randomly assigned 264 participants (48% female; Mage = 35 years) recruited on MTurk to one of two status goals conditions (status-maintenance vs. status-advancement). After indicating their political conservatism as part of an initial survey, participants took part in a pretest for a future study on written language (our status goal manipulation). Next, as part of a consumer survey, they indicated their desire for six car brands. Finally, they answered the same demographic questions as in the previous studies. Political conservatismWe assessed political conservatism using a scale ([48]) successfully employed in prior research (e.g., [75]). Sample items include ""I am politically more liberal than conservative"" and ""I cannot see myself ever voting to elect conservative candidates"" (1 = ""strongly disagree,"" and 7 = ""strongly agree""; α =.88). We averaged these scores to form an overall political conservatism score (M = 3.90, SD = 1.57). Status goal manipulationParticipants engaged in a short writing task. In the status-maintenance (status-advancement) condition, participants read:A recent analysis of global socioeconomic insight revealed that Americans are expected to experience a decline [improvement] in their status in the next 5 years. This means that relative to citizens of other developed countries, Americans are expected to face more challenges and difficulties in maintaining [more chances and opportunities to improve] their social standing. Now please think about 2–3 ways you may be able to maintain [improve] your social standing in the next few years and list them in the space below.A pretest confirmed that the status goal manipulation successfully activates different goal foci, while keeping the emphasis on status constant across conditions (Web Appendix N). Desire for luxury brandsThe format was the same as in Study 2, except the DV comprised only car brands (for pretest details, see Web Appendix O). Participants indicated the extent to which they wanted a product from six car brands (1 = ""not at all,"" and 7 = ""very much""). ResultsAs in Study 2, our DV was the difference score between participants' desire for luxury (M = 3.91, SD = 1.52; α =.63) and nonluxury (M = 3.31, SD = 1.63; α =.82) brands. We regressed the desire for luxury brands on political conservatism, status goal (status-maintenance = 0, status-advancement = 1), and their interaction as well as the three covariates. There was a significant political conservatism × status goal interaction (β = –.35, t(257) = –2.91, p =.004; Web Appendix P). To probe the interaction, we examined the slopes of political conservatism in each condition. In the status-maintenance condition, political conservatism predicted the desire for luxury brands (β =.21, t(257) = 2.27, p =.024). Unexpectedly, we observed a marginally significant effect in the status-advancement condition, such that political conservatism negatively predicted the desire for luxury brands (β = –.14, t(257) = –1.76, p =.079). A spotlight analysis further revealed that the desire for luxury brands among conservatives (one standard deviation above the mean of political conservatism) was higher in the status-maintenance condition than in the status-advancement condition (β =.58, t(260) = 2.21, p =.03). Among liberals (one standard deviation below the mean of political conservatism), the desire for luxury brands was higher in the status-advancement condition than in the status-maintenance condition (β = –.66, t(260) = –2.52, p =.012).Overall, Study 4 further demonstrates that political conservatism increases consumers' desire for luxury when the status-maintenance (but not status-advancement) goal is activated (H1) in a context in which the goal is activated independent of the consumption task. One limitation of Study 4 is that the status-maintenance goal condition might have triggered a feeling of loss by prompting participants to think that they may experience a decline in status (unlike in the status-advancement goal manipulation). Although findings on whether losses are more motivating than gains are mixed ([57]), in Study 5 we address this concern by employing a nonloss framed manipulation for the status-maintenance goal. Study 5: Framing a Product as Luxury Versus NonluxuryThe objectives of Study 5 were twofold. First, we employed a nonloss-inducing status goal manipulation. Second, we varied the framing of a single product as luxury or nonluxury to demonstrate the luxury-specific nature of the effect. Indeed, if a preference for social stability drives the effect, we should observe the effect only for goods that act as stabilizers of the social hierarchy (i.e., luxury goods), not for goods that do not typically have this association (i.e., nonluxury goods). Practically, the study offers a vivid example of how luxury managers can leverage the findings by changing their product framing. MethodThree hundred three students (45% female) from a large U.S. college on the West Coast were approached on campus by experimenters and voluntarily participated in exchange for a free snack. Participants were randomly assigned to one of four conditions of a 2 (product framing: luxury vs. nonluxury) × 2 (status goal: status-maintenance vs. status-advancement) between-subjects design with political conservatism as a continuous variable. After assessing political conservatism with the same two-item measure as in Study 3b (α =.72; M = 3.55, SD = 1.06), we manipulated status goal. Finally, participants indicated their WTP for a set of headphones framed as a luxury or a nonluxury product, which served as our DV. We chose headphones because this product is relevant, status signaling, and accessible to our population ([11]). Status goal manipulationWe randomly assigned participants to one of two writing tasks. In the status-maintenance (status-advancement) condition, they read:Recent research on college well-being has revealed that one key to a satisfying and successful college life is to maintain stable [improve] social status. By ""social status,"" we mean a relative social standing or the level of respect and admiration received by others in a society. Now, please think about all the respect and admiration you receive [ways you lack respect and admiration] from people around you in various domains of your life. When you're done reflecting, please think about 2–3 ways you can maintain [enhance] your current social status for next few years and list them in the space below.A pretest confirmed that our status goal manipulation successfully activates different goal foci, while keeping the emphasis on status constant across conditions (Web Appendix Q). Product framingWe randomly assigned participants to read and evaluate the new headphones framed as luxury or nonluxury. Common to both conditions was the image of the product (Web Appendix R). However, in the luxury condition the tagline read ""Top of the Top, the L-Pro line,"" and the description used luxury-related words such as ""luxurious"" and ""prestigious."" In the nonluxury condition, the tagline read ""Made for comfort, the for-all headphones,"" and the description used words such as ""convenience"" and ""handiness."" A pretest confirmed that our product framing manipulation was successful (Web Appendix S). ResultsA three-way analysis of variance on WTP, with status goal (status-maintenance vs. status-advancement) and product framing (luxury vs. nonluxury) as categorical variables and political conservatism as a continuous variable, revealed a significant three-way interaction (F( 1, 295) = 5.66, p =.018; Figure 5). There was a main effect of product framing, such that participants were willing to pay more in the luxury condition (M = 77.58, SD = 74.80) than in the nonluxury condition (M = 32.56, SD = 29.68; F( 1, 295) = 54.52, p <.001), but no main effect of status goal (Mmaintenance = 58.29, SD = 62.03; Madvancement = 50.83, SD = 59.26; F( 1, 295) = 1.92, p =.167).Graph: Figure 5. Study 5: WTP (USD) for headphones framed as luxury.In addition, the status goal × political conservatism interaction was significant only in the luxury condition (F( 1, 144) = 9.25, p =.003; nonluxury condition: p =.297). To explore this interaction further, we examined the slopes of political conservatism in each status goal condition. When the status-maintenance goal was activated, political conservatism positively predicted WTP (β = 22.33, t(144) = 2.93, p =.004), but when the status-advancement goal was activated, political conservatism did not predict WTP (β = –.11.20, t(144) = –1.40, p =.163). A floodlight analysis revealed that the effect of the status-maintenance (vs. status-advancement) goal on WTP turned significant at.3 standard deviation above the mean of political conservatism (β = 26.45, t(144) = 2.13, p =.035).Overall, Study 5 demonstrated that political conservatism increases consumers' WTP for products framed as luxury when their status-maintenance goal is activated, providing further support for our hypothesis (H1). By contrast, political conservatism did not predict WTP for the product framed as nonluxury (β = 5.14, t(151) = 1.51, p =.133), even when the status-maintenance goal was activated. In addition, WTP for the product framed as nonluxury did not differ across status goal conditions among conservatives (at one standard deviation above the mean of political conservatism; t < 1, p =.356). This finding confirms the proposition that the effect is specific to luxury goods because consumers view these goods as reinforcing the social hierarchy ([21]). By varying the framing of a single product, this study provides vivid evidence of when such framing may increase consumers' responses to advertising depending on the dominant political ideology in the target market. General DiscussionSix studies reveal that political conservatism increases the desire for luxury goods when a status-maintenance goal is activated but not when a status-advancement goal is activated or in the absence of a status goal. To increase the generalizability of our findings, we used four different product categories (cars, fashion clothes, eyewear, and headphones). Furthermore, to provide convergence on our effects, we employed multiple measures of political conservatism (Table 1). Finally, a meta-analysis ([68]) revealed that political conservatism significantly increased the desire for luxury goods across status-maintenance goal conditions (dmaintenance =.24, z = 3.65, p <.001) but not across status-advancement conditions (dadvancement = –.12, z = –1.82, p =.07; for details, see Web Appendix T). A p-curve analysis ([61]) showed that the studies contain evidential value (significantly right skewed; p <.001) and are sufficiently powered, such that the evidential value is not inadequate (not flatter than 33% power, p >.999; Web Appendix U). Theoretical ContributionsOur research makes two important theoretical contributions. First, we contribute to the literature on social status by providing empirical evidence for when status-maintenance versus status-advancement goals may matter in consumption contexts. Despite the influence of social status on consumption, research has mostly treated status as a single construct (e.g., [ 4]; [46]). By showing that consumers engage in status-driven consumption in response to different status goals, our work provides first empirical support for the idea that consumers' need for status might be multidimensional.Second, we contribute to the nascent literature ([54]) that links consumers' views on social hierarchy with their purchases facilitating their expression of status (i.e., luxury goods; [21]). Going beyond the question whether conservatives may desire luxury products more than liberals, we investigate when this pattern is likely to occur. In doing so, we shed light on the motivational underpinning behind conservatives' desire for luxury goods—that is, their desire to maintain, rather than advance, their status. Managerial ContributionsInvestigation of political ideology carries important implications for managers because it provides a powerful segmentation tool (e.g., [71]). Indeed, people of varying political ideologies differ in the brands they favor and media outlets they follow. As a result, brands tend to match the media used to their consumers' political orientations (Figure 1). For example, Jeep, the most desired car brand among Democrats, only sponsors left-leaning media (e.g., http://sponsorfeedback.com), and Apple, another patron of left-leaning media, actually stopped advertising on the Fox Network during the 2012 season because the company judged the broadcasted content as conflicting with its core philosophy ([ 2]). Apart from one-time surveys mapping political party to brands, however, managers lack both resources giving them a systematic understanding of how political ideology may influence brand choices and guidelines on how to leverage political ideology as a segmentation tool. As luxury products appeal to specific audiences, refining the segmenting and targeting of consumers is central to the management of luxury brands ([20]).To this end, we offer a more sophisticated but practical approach to luxury market segmentation. Our findings support an approach that takes into account two factors that together predict consumers' appetite for luxury goods: political ideology and status-maintenance activation. Importantly, data on both factors are accessible and identifiable at granular levels, making it easy for managers to leverage our findings when designing their segmentation and targeting strategies. First, managers can easily identify political ideology along geographic segmentation ([12]) or through agencies (e.g., Pew Research Center, Gallup, ICPSR) in great detail (e.g., town level; see Web Appendix B for examples of available data sources). They can also assess political ideology through consumers' preferences for media outlets ([34]) or recognizable digital footprints on online platforms such as social media (e.g., ""likes"" for a political issue, following of a political figure; [15]; [41]; [56]). Second, they can easily assess status-maintenance activation by ( 1) using positional metrics such as education, income, and SES, which are often available in survey data (e.g., Consumer Expenditure Survey, U.S. Census); ( 2) identifying socioeconomic contexts that activate status-maintenance motives (e.g., economic downturns, when consumers may attempt to reassure their standing through luxury consumption; [52]); or ( 3) momentarily heightening activation by altering the framing used in brand communications.We also provide detailed guidelines on how managers of luxury brands can motivate luxury consumption (Figure 2). First, managers should consider whether their brands currently leverage status maintenance. If so, our findings suggest that simply identifying and targeting a conservative segment can increase their effectiveness. To do so, the brand could run targeted marketing campaigns ( 1) on media platforms patronized by conservatives (see Figure 1; [34]), ( 2) in conservative geographic areas (see Web Appendix B; [12]), or ( 3) online by targeting individual consumers associated with digital footprints indicative of conservatism ([15]). If the brand does not currently emphasize status maintenance, managers could consider leveraging status-maintenance messages in their communications or targeting individual or situational contexts with high status-maintenance activation (e.g., consumers having high-status positions). In summary, our findings provide insights into how managers can drive luxury brands using political ideology or differential status goals rather than targeting a segment on the basis of wealth or status. Limitations and Future Research DirectionsOur research is not without limitations, which offer potential avenues for further research. First, our efforts center only on the political spectrum from conservative to liberal; however, a growing number of people are finding it difficult to identify with either mainstream political ideologies or new ones (e.g., Independents). In Study 2, for example, 36% of the consumers who disclosed their political ideology identified themselves as neither Republican nor Democrat. Little is known about how political orientations other than Republican or Democrat might influence the way people construe status and engage in luxury consumption. Consistent with prior research (e.g., [54]), we treated liberalism and a low degree of conservatism synonymously, given our focus on conservatism. Furthermore, although research has shown that the single spectrum ranging from liberalism to conservatism can effectively capture variations in personal and sociopolitical perspectives, including a preference for social stability ([51]), liberalism and conservatism may differ on other dimensions for which a continuous scale measurement may be ill-suited. Additional research is necessary to further unpack the relationship between liberalism and conservatism and investigate their interplay with consumption practices.Second, we limited our research to the U.S. context. Given previous findings that political conservatism carries different meanings across different countries ([ 9]; [16]), the extent to which our results may replicate in a non-U.S. context is unclear. As an initial exploration, we conducted a study using the same design as in Study 5 in France. Although the study did not reveal a significant political conservatism × status goal interaction (see Web Appendix V), a crucial difference was that political conservatism was significantly lower in the French sample (M = 3.53, SD =.88) than the U.S. sample (M = 3.90, SD=1.57, t(296) = –7.24, p <.001). While average political conservatism did not differ from the scale midpoint in the U.S. sample (t(263) = –1.07, p =.287), this difference was significant in the French sample (t(296) = –9.19, p <.001). In addition, the reliability of the Mehrabian scale was much lower in France (α =.52) than in the United States (α =.88), indicating that assessing political conservatism in France may involve different processes. Thus, an important boundary condition for the effect may lie in the characteristics of the distribution of political conservatism among the population of interest.Third, future studies could investigate the conditions that may lead liberals to desire luxury goods more than conservatives. Across two of our studies, liberals showed a greater desire for luxury than conservatives when their current status position was low (Study 2) and when the status-advancement goal was activated (Study 4). These mixed results suggest that there are conditions in which political liberalism triggers luxury consumption. Of note, this effect was significant only in the studies employing real brands as the DV (Studies 2 and 4). Although we pretested brands on key dimensions tied to our investigation (e.g., luxurious, liking; Web Appendix J), the luxury brands may also differ from nonluxury brands on other dimensions not captured by the pretest. For example, consumers may perceive these brands as more horizontally differentiating (e.g., unique, innovative, creative) than nonluxury brands, leading to the observed effect ([54]).A fourth potential avenue, equally appealing to managers and researchers, would be investigating contexts that activate status maintenance. As an initial attempt, we used the high-status position to activate status maintenance when testing our hypotheses in Studies 1 and 2. Beyond consumers' current status—a broad construct that may influence factors other than the degree of status-maintenance activation—researchers could try to identify individual or group variables that naturally induce changes in status-maintenance activation. As status-maintenance goal activation stems from a person's current status state overlapping with his or her desired status state, situations that increase the overlap between states may foster status-maintenance activation. For example, a person who just obtained a promotion and was given new positional assets such as a company car may be more likely to focus on maintaining his or her current position than a person who was not promoted. In addition, among those holding a desired status position, increases in the instability of their current status position may activate status maintenance ([63]). Thus, making salient the prospect of sliding back from a current status position (e.g., an elected official unsure about being reelected, an executive manager approaching the end of a contract term) may help activate the status-maintenance goal.Finally, our work reflects a resurgence of interest in political ideology in the social sciences ([37]) and is a response to the emerging interest in political ideology in marketing ([35]; [59]). As [59], p. 500) notes with regard to political ideology, ""understanding the psychology of liberals and conservatives can inform a range of managerial decisions."" In this spirit, we hope our initial steps will pave the way for new efforts that increase understanding of the role of political ideology in marketing. "
41,"How Do Specialized Personal Incentives Enhance Sales Performance? The Benefits of Steady Sales Growth The authors study specialized personal incentives (SPIs), which are cash rewards granted to salespeople for meeting interim performance goals within the regular sales quota period (monthly, quarterly, etc.). Because firms often institute multiple SPIs, the authors are able to investigate whether different sales achievement trajectories have differential impacts on salespeople’s period-end sales performance. The authors find that a steadily growing sales trajectory in a sales period is more strongly associated with period-end success than a sales trajectory that is relatively flat early but has a sharp spike later in the period. Furthermore, although salespeople who had high performance in the prior month (i.e., high-performance state) may be able to draw on superior selling strategies (compared with other salespeople), they too experience a boost in sales performance in the current month by earning SPIs. Notably, the authors also find that although earning SPIs benefits all salespeople, there is a U-shaped relationship between a salesperson’s performance state and his or her month-end sales performance. For any specific number of SPIs earned, the probability of meeting and exceeding month-end quotas is boosted more for salespeople with low- and high-performance states than for salespeople with a medium-performance state.One of the important issues we investigate in this research is whether different types of sales production trajectories of salespeople in a sales period are associated with different period-end sales performance in the context of the quota in that period. Notably, we compare the period-end performance of a salesperson whose sales trajectory is steadily growing throughout the sales period with that of a salesperson who had relatively low sales production early in the period followed by very rapid sales production success later in the period. Consider two salespeople, A and B—with exclusive territories, equal abilities, and equal quotas—at a firm where the sales period is one month. Assume that both A and B achieved 50% of their current month’s quota by the 20th of the month. Assume as well that A achieved 20% of the month’s quota by the 10th of the month and another 30% between the 10th and the 20th. In contrast, B achieved only 5% of the month’s quota by the 10th of the month and another 45% between the 10th and the 20th. So, A had a relatively steady growth in her sales production from the 1st through 20th of the month, whereas B had lower sales until the 10th but experienced a steep spike from the 10th through the 20th. Thus, A’s sales trajectory was steadily growing through the first two-thirds of the month, whereas B’s sales trajectory was barely rising in the first third of the month but grew very rapidly in the second third of the month.Suppose that, on the 20th of the month, their sales manager wants to know who between A and B will have better sales performance in the context of exceeding their sales quotas at the end of that month. Because both A and B had earned 50% of their quota by the 20th of the month, one perspective might suggest that A and B will have equal performance at the end of the month. A second perspective suggests that because B has experienced recent momentum as of the 20th of the month, she will have better month-end sales performance than A. Unlike A, B has recency and momentum of sales production; having made more sales between the 11th and the 20th of the month, most of B’s sales are recent. Both recency and momentum have been found to be important predictors of future outcomes in many different contexts (e.g., momentum investing, “hot hand” phenomenon). Thus, one could infer that B might have superior month-end performance to A. A third perspective is based on the fact that A has been more consistent than B, as A had a steadily rising sales production until the 20th of the month. In line with findings such as the “spacing effect” (Janiszewski, Noel, and Sawyer 2003) as well as subgoals literature (Fishbach, Zhang, and Dhar 2006), A will have better month-end sales performance than B. Indeed, given the theoretical rationales presented, both the second and third perspectives are relevant and reasonable outcomes, even if one of the two salespeople (either A or B) had covered a higher proportion of her equal month-end quota by the 20th of the month than the other.In this research, we rely on specialized personal incentives (SPIs) to test the association between sales trajectory types and period-end performance. An SPI is an interim (within a sales period) incentive that some firms incorporate for their sales force. Such “special incentives” can take the form of “spiffs” (defined by Zoltners, Sinha, and Lorimer [2006] as “special performance incentive for the field force”). The specific type of SPIs we focus on are intermediate, extra incentives that salespeople can earn in addition to their period-end bonus for exceeding interim quotas (which we refer to as “SPI quotas”). These SPI quotas require salespeople to achieve a specific percentage of the period-end quota by a specific time in that sales period. In a context similar to SPIs, Chung, Steenburgh, and Sudhir’s (2014) pioneering study shows that quarterly bonuses help mainly the weak performers by serving as pacers to keep them on track in achieving their regular annual bonuses. In contrast, we investigate whether different types of sales-production trajectories differentially affect salespeople’s period-end performance. We also investigate whether salespeople’s performance state (performance at the end of the prior period) interacts with different types of sales-production trajectories in the next period to determine their performance at the end of the next period.Conceptually, SPI quotas are best understood in the context of the relationship of subgoals with superordinate goals (Fishbach, Zhang, and Dhar 2006). Subgoals are lesser goals that form a part of the superordinate goal. Specialized performance initiative quotas have the same relationship to regular period-end quotas that subgoals have to superordinate goals. Fishbach, Zhang, and Dhar state, “Setting goals and monitoring progress toward goal achievement is fundamental to theories of self-regulation” (p. 232). If an interim milestone increases commitment for the superordinate goal, then that interim milestone is beneficial in reaching the superordinate goal (Austin and Vancouver 1996; Devezer et al. 2014; Fishbach and Dhar 2005; Fishbach, Shah, and Kruglanski 2004). In line with this, the SPIs at our data-provider firm, being explicitly tied to the period-end quota and distinctively expressed as increasing cumulative percentages of the period-end quota, should enhance commitment to meeting the period-end quota. The sales quota period at our data-provider firm is one month. This firm has instituted two explicit SPI quotas, namely ( 1) “reaching 20% of the monthly quota by the 10th of the month for the early SPI,” and ( 2) “50% of the monthly quota by the 20th of the month for the later SPI” (see Figure 1).Regarding the scenario presented previously, salesperson A has earned both SPIs (i.e., the early SPI as well as the later SPI) that month, whereas B earned only the later SPI. In the context of our data-provider firm, without a systematic empirical analysis, it is not obvious a priori whether earning both SPIs (steadily growing sales trajectory) has a higher association with superior period-end performance than does earning just the later SPI (a recent spike in the sales trajectory conveying recent momentum). Alternatively, does earning both SPIs have a lower comparative association with period-end performance? The unique aspects of our data enable us to test the association between the nature of the salesperson’s sales trajectory and his or her period-end sales performance without confounding it with absolute dollar sales achieved. In contrast, if the SPIs at a firm are not cumulative (e.g., if they are expressed as achieving $x in sales by the 10th of the month, and $y in sales between the 10th and 20th), as is the case with quarterly quotas, then earning more SPIs necessarily leads to higher sales than does earning fewer SPIs.Our research contributes by examining the following research questions: ( 1) Is a steadily growing sales trajectory (in which both SPIs are earned) associated with higher end-ofperiod performance compared with a sales trajectory with a recent spike (in which only the later SPI is earned), or vice versa? ( 2) Does salespeople’s performance state (i.e., their performance level at the end of the prior month) determine the extent to which they will benefit from earning SPIs in the subsequent month? ( 3) Is earning SPIs in one period associated with earning SPIs in the next period?A preview of our results: First, earning both SPIs in a period is more strongly associated with superior period-end performance than is earning only the later SPI in that period (even though, a priori, the two may be indistinguishable on the 20th of the month). So, a steadily growing sales trajectory enhances the chances of success relative to a trajectory with a sharp spike. Second, we find that earning SPIs benefits all salespeople. Yet, notably, there is a U-shaped relationship between a salesperson’s performance state and his or her sales performance at the end of the next month. For any specific number of SPIs earned, the probability of meeting and exceeding month-end quotas is boosted more for salespeople with low- or high-performance states than for salespeople with a medium-performance state. This result highlights the importance of earning SPIs even for salespeople with a high-performance state. This finding also indicates that although salespeople with a high-performance state likely develop better sales strategies than do other salespeople, these superior strategies alone do not provide the upper bound of possible performance for the salespeople with a high-performance state. The salespeople with a high-performance state nevertheless benefit by earning SPIs (i.e., being self-regulated). Third, we find that earning SPIs in one sales period is associated with earning SPIs in the next period.This article is organized as follows. First, we review relevant literature and present our hypotheses. We then present the empirical setting. Next, we respectively describe and present the results of our first empirical analysis, which offers support for the effects of earning SPIs on the period-end sales achievement. Then, we respectively describe and present the results of our second empirical analysis, which tests whether earning SPIs in one period is associated with earning them in the next period. Finally, we present the general discussion and managerial implications. Conceptual Background and HypothesesThe literature on the psychology of goal achievement suggests that people find it difficult to start a task when they face a challenging goal (Gollwitzer and Brandstaetter 1997; Heath, Larrick, and Wu 1999). This literature notes that one way to overcome the “starting problem” is to use subgoals. Heath, Larrick, and Wu (1999, p. 93) state that “proximal subgoals are more likely to produce eventual success.” In extending this to the sales force context, the within-period interim SPI quotas are subgoals and the period-end quotas are superordinate goals.However, it is not always optimal to instill subgoals, and the literature presents mixed evidence on the motivational abilities of subgoals. Experiencing early success in reaching a subgoal can create complacency and thereby can lead to a reduction in effort (Schunk 1983, 1984; Vancouver, Thompson, and Williams 2001). Extant research has shown that achieving early subgoals leads to weaker performance on the superordinate goals. In a celebrated example, cab drivers in New York were found to drastically reduce effort after some “earning” goals were met early in the day (Camerer et al. 1997).Given the conflicting evidence about the efficacy of sub-goals, how should a sales manager perceive SPIs? The key lies in how salespeople infer subgoal achievement, and how managers can influence it. Fishbach, Zhang, and Dhar (2006) and Fishbach and Dhar (2008) argue that the achievement of the superordinate goal can be diminished or enhanced on the basis of whether the subgoal’s success is respectively framed and inferred as progress or as commitment. If achieving the subgoal is viewed as progress already made, people decrease effort due to a sense of complacency. Yet if subgoal achievement is viewed as increasing commitment toward the superordinate goal, people put in greater effort, and subgoal success positively affects the superordinate goal. We argue that the SPIs at our data-provider firm are designed to instill commitment to the superordinate goal (we provide more details in the “Empirical Setting” section), and therefore, for the remainder of this article and the subsequent hypotheses, we focus on such SPIs. We thereby hypothesize,H1: Earning any SPI in a month (i.e., only the early SPI, only the later SPI, or both SPIs), versus not earning any SPI in the month, is associated with higher month-end sales performance.The next question that arises is whether earning only the later SPI (vs. only the early SPI) in a sales period is more effective at positively influencing period-end performance, or vice versa. This issue relates to the distance of the subgoal from the period-end superordinate goal. By the time the salesperson has achieved the later SPI, (s)he is closer to the superordinate goal than (s)he would be after achieving the early SPI. In this context, Bonezzi, Brendl, and De Angelis (2011) provide helpful guidance. They argue that if a person uses the initial state as the reference point for monitoring advancement (i.e., “to-date frame”), the perceived marginal value of incremental advances made decreases. For example, reading one more page of a lengthy book is perceived as yielding less advancement after having read 200 pages than after having read 50 pages. In addition, getting closer to the end-goal can galvanize the person to expend greater effort. In such a context, Bonezzi, Brendl, and De Angelis argue that if a person uses the desired end state as the reference point (i.e., “to-go frame”), the perceived marginal value of advancement increases. Reading one more page is viewed as yielding more advancement when 50 pages remain than when 200 pages remain.The issue, therefore, is whether salespeople use the “to-date frame” versus “to-go frame” when reviewing their advancement toward their period-end sales quota. It is reasonable to argue that SPIs focused on the period-end superordinate goal (as is the case at our data-provider firm, because the SPIs are presented as percentage of month-end quota) should trigger the “to-go frame” in salespeople. Compared with when a salesperson earns only the early SPI in a sales period, the (s)he is closer to the period-end sales quota reference point when (s)he earns only the later SPI in a sales period. Following the argument presented by Bonezzi, Brendl, and De Angelis, any marginal advancements in sales production made after earning only the later SPI in a period (being closer to the end of the period) should be viewed as being more motivating and effort inducing than the marginal advancements in sales production after earning only the early SPI in that period. Thus,H2: Compared with earning only the early SPI in a month, earning only the later SPI in the month is associated with higher month-end sales performance.Next, we turn to the issue of earning two SPIs versus only one SPI in a period, in terms of their association with meeting and exceeding end-of-period quotas. It is important to determine whether earning both SPIs (i.e., steadily growing trajectory) has a stronger association than earning only the later SPI (i.e., slow growth in sales early in the month with a very rapid rise later) with superior period-end performance, or vice versa. Note that if earning more SPIs were to automatically imply more sales, then this would be a foregone conclusion. However, in our case, it is not clear a priori whether this supposition holds. Recall again the two salespeople A and B, both of whom have achieved exactly 50% of the current month’s quota by the 20th of the month. Salesperson A (salesperson B) has achieved 20% (5%) of this month’s quota by the 10th of the month and another 30% (45%) between the 10th and the 20th. Although their sales production is at 50% of the month-end quota on the 20th of the month, salesperson A has earned both SPIs in that month. In contrast, salesperson B has earned only the later SPI in that month. Essentially, looking back from the 20th of the month, for the same amount of sales, the sales of salesperson B are more “bunched” toward the end compared with those of salesperson A, whose sales are more “spaced out” and therefore steadier.Literature on learning has found evidence of a “spacing effect,” which suggests that, for the same lump sum, spacing events apart results in better outcomes than amassing them together at the end (Janiszewski, Noel, and Sawyer 2003; Kornell 2009). Dempster (1988) states, “The spacing effect—which refers to the finding that for a given amount of study time, spaced presentations yield substantially better learning than do massed presentations—is one of the most remarkable phenomena to emerge from laboratory research on learning” (p. 627, emphasis added). To apply the spacing argument to our example of salespeople A and B, judged from the 20th of the month, salesperson A has better learning and therefore will likely have better performance with respect to month-end quota. Why does the spacing effect work? Many studies on the spacing effect are in the context of learning and are based on the spreading of the “repetition effect,” whereby more opportunities to learn are valuable, implying that last-minute “cramming” is often counterproductive (Dempster 1987; Schmidt 1983). There is an extensive literature on the “learning orientation” of salespeople, and selling effectively to achieve quotas triggers salespeople’s learning orientations (Ahearne et al. 2010; Sujan, Weitz, and Kumar 1994). Thus, trying to achieve the multiple quotas associated with earning multiple SPIs gives the salespeople more learning opportunities, thus enhancing their performance. So,H3: Compared with earning only the later SPI in a month, earning both SPIs in a month is associated with higher month-end sales performance.Note that, as we discussed in the introduction, on the 20th of the month, salespeople who earn only the later SPI (salespersonB) have recency in sales production momentum (during the second third of the month). Extant literature has also presented evidence suggesting that coming in later with a burst of recent momentum is more beneficial than having early success. The most telling among these likely is the “hot hand” phenomenon. Evidence from literature streams such as behavioral decision theory and others has found that both recency and momentum are important predictors of future outcomes in many contexts. For example, recency of purchase is one of the prime factors in the recency, frequency, and monetary model of customer lifetime value. The literature on innovation has also shown that late entrants (which have momentum on their side) can “leapfrog” over early pioneering firms (Golder and Tellis 1993). In addition, momentum investing in stocks is a strategy aimed at capitalizing on the belief that large recent increases in the price of a stock will be followed by additional increases (Investopedia 2017). Furthermore, recent findings in the literature have shown evidence for the “hot hand” phenomenon (i.e., a person who has recently experienced success may have a greater chance of further success in additional attempts; e.g., Andrews 2014; Gelman 2015; Green and Zwiebel 2013; Miller and Sanjurjo 2015). These arguments provide evidence for outcomes that contradict H3. This evidence suggests that salesperson B, due to her recency and momentum around the 20th of the month, should have better month-end performance than salesperson A and thus that earning only the later SPI in the month will be associated with higher month-end sales performance than earning both SPIs. However, to avoid confusion, we refrain from formally presenting an alternate hypothesis that contradicts H3. Instead, we simply rely on the results of our empirical analysis to test whether H3 is supported.It is also noteworthy that expectancy theory (Oliver 1974; Vroom 1964) could have been used to arrive at some of our hypotheses, such as H1. Expectancy theory addresses motivation in terms of value and likelihood associated with outcomes. Expectancy theory posits that as the probability of an event increases, people will be more motivated to put in effort. However, it is difficult to present H2 or H3 by relying only on expecting theory. For example, we are not aware of anything in expectancy theory that would have enabled us to make the predictions that we have presented in H3. Expectancy theory does not provide clear guidance on whether the subjective probabilities of exceeding the period-end quota are enhanced to the same extent (or to different extents) if the salesperson earns only the later SPI (vs. both SPIs) in a period. Thus, in deriving the logic for our hypotheses, we preferred to rely on self-regulation theory and concepts such as subgoals.Next, we investigate whether salespeople with different performance states from the prior month (i.e., low-performance vs. medium-performance vs. high-performance states) benefit differentially from earning both SPIs in the next month in the context of exceeding their performance target at the end of the next month. In education research, it is well-established that weaker students benefit from frequent testing. Crooks (1988, p. 469) argues, “Weaker students may benefit from identification of more attainable intermediate goals, thus making possible the pattern of repeated successes that leads to improved self-efficacy.” Likewise, Chung, Steenburgh, and Sudhir (2014) find that instead of holding only annual quotas for salespeople, instituting quarterly quotas in addition to the annual quotas is associated with the highest improvement in annual performance among weak salespeople. In comparison, the improvement among medium- and high-performing salespeople was modest. Extending these findings to our context, we propose that earning both SPIs in the next month will be associated with the highest boost in the next month’s performance in salespeople with a low-performance state (i.e., salespeople with low performance in the immediately preceding sales period) compared with the boost experienced by salespeople with high-performance or medium-performance states. Thus,H4: Compared with salespeople with high- or medium-performance states, salespeople with a low-performance state experience a higher boost in month-end performance by earning both SPIs (vs. no SPIs) in that month.Finally, we focus on understanding whether earning SPIs in the prior period is likely to enhance the likelihood of doing so again in the next period. Lal and Srinivasan (1993) argue that the salesperson’s decision to put in effort has an intratemporal component, such that efforts in one period could affect efforts in the next. In the psychology literature on goals, a large body of work has investigated the persistence of goal-directed behaviors (Austin and Vancouver 1996; Locke and Latham 1990a, b). In our context, with the month-end quota being the superordinate goal, the effort to earn SPIs is a goal-directed behavior, which literature suggests should have some persistence. The literature on routinization also mentions the automatic mental associations among superordinate goals and goal-directed actions (earning SPIs), which can be instrumental in attaining these superordinate goals (Aarts and Dijksterhuis 2000; Ohly, Sonnentag, and Pluntke 2006). Extant research has suggested that once the goal is activated, the accessibility of such goal-directed actions depends on the frequency or recency with which they are applied in the future (Bentler and Speckart 1979; Wyer and Srull 1989), thus suggesting that earning one or more SPIs (i.e., incorporating goal-directed action) in a month is likely to increase chances of earning SPIs in the subsequent month. Indeed, Aarts and Dijksterhuis (2000, p. 54) conceptualize that “frequent and consistent performance of a goal-directed action in a specific situation facilitates the ease of activating the mental representation of this behavior (and hence the resulting action itself) by the situation.” In the sales context, the persistence of earning SPIs in subsequent periods is relevant, as sales managers would like to know whether instituting SPIs could induce persistence effects of the action of earning SPIs. Note that compared with earning only the early SPI or later SPI, earning both SPIs (repetition of earning SPIs in a month) has higher recency and frequency. Thus,H5: Compared with not earning any SPI in the prior period, earning only the early SPI in the prior period is more strongly associated with earning both SPIs in the current period.H6: Compared with earning only the early SPI in the prior period, earning only the later SPI in the prior period is more strongly associated with earning both SPIs in the current period.H7: Compared with earning only the later SPI in the previous period, earning both SPIs in the prior period is more strongly associated with earning both SPIs in the current period.Empirical SettingThe data for this study are from a medium-sized nationally branded personal care CPG manufacturer selling products in an emerging economy. The firm has national-level brand recognition and a respectable market share in a few of its product categories. It has a team of more than 400 salespeople selling to retailers in geographically exclusive territories all over the country. More than 90% of retailers in this emerging economy are independent and unorganized—mostly small family-owned businesses with little to no bargaining power. Such retailer customers do not have specific temporal patterns to their buying cycles and purchase their merchandise only when their stock is depleted. They do not have the available cash required to time their purchases to enable the salespeople to play timing games. The salespeople visit each store in their territory twice each month (roughly 15 days apart), and the transactions occur during their visit to the store.The data-provider firm has a “monthly” sales horizon period for the regular quota-bonus incentives for its sales force. Figure 1 provides a visual depiction of the bonus plan for the sales force at this firm. The data set contains details on the monthly sales quotas (i.e., targets) imposed on each salesperson, as well as the monthly sales achievement (i.e., production) of that member. As indicated previously, this firm also has two SPIs in place every month. The early SPI is for meeting 20% of the regular monthly sales quota by the 10th of the month, and the salesperson is granted a cash award (300 units in the local currency) for doing so. The later SPI is for meeting 50% of the regular quota by the 20th of the month, and the salesperson is granted a cash award (800 units in the local currency) for doing so. Each salesperson can earn neither, either, or both SPIs each month. On average, salespeople at this firm had earned SPI cash bonuses of 501 each month (SD = 503) in the local currency.The explicit nature of the SPIs at this firm (stated as percentages of the month-end quota) makes it very clear to the salespeople that the month-end quotas are the main target of interest, and the SPIs are the means to that end. Therefore, the design of the SPIs is such that the superordinate goal is always salient in the salespeople’s minds: after achieving the early SPI, for example, they are likely to think, “I just have accomplished 20% of the monthly quota.” Such a design draws their attention to the month-end quota as the superordinate goal and strengthens their commitment to it (Fishbach and Dhar 2008; Fishbach, Zhang, and Dhar 2006). The psychological forces described here should hold after earning the early SPI, the later SPI, or both SPIs.In addition, at the end of each month, all salespeople are evaluated on whether they have met or exceeded their regular monthly quotas, and it is on this basis that they receive month-end bonuses. At the end of every month, this firm categorizes all of its salespeople into one of the following four performance categories on the basis of their performance relative to quota: ( 1) the “fold” category, if they fail to meet their sales quota that month (no cash bonus); ( 2) the “minimum” category, if their sales achievement is between 100% and 110% of their quota that month (cash bonus of 2,250 units in local currency); ( 3) the “stretch” category, if their sales achievement is between 110%–120% of their quota that month (cash bonus of 3,200 units); and ( 4) the “outstanding” category, if their sales achievement is greater than 120% of their quota that month (cash bonus of 4,200 units). The dependent variable in the first of our two empirical analyses is the ratio of each salesperson’s sales production to his or her sales quota each month (referred to as sales-to-quota ratio [SQR]). On average, salespeople at this firm had sales achievements of 324,035 (SD = 224,488) each month in the local currency of the country. Furthermore, on average, the salespeople at this firm earned end-of-month cash bonuses of 2,619 each month (SD = 1,563) in the local currency.This CPG manufacturing firm provided us a data set that held details for 33 months, starting from April 2009. This data set holds 9,384 observations for 813 salespeople, in the form of a panel. We executed two empirical analyses on these data (referred to herein as the First Analysis and the Second Analysis), as we explain in the following sections. Table 1 presents an explanation of several variables, including the covariates(i.e., independent variables), used in the two analyses as well as descriptive statistics for continuous covariates. First AnalysisThe First Analysis is focused on testing support for H1–H4. For the remainder of this article, we refer to salespeople with “outstanding” performance in the prior month state as having a high-performance state, those with “stretch” performance in the prior month state as having a high-mediumperformance state, those with “minimum” performance in the prior month state as having a low-medium-performance state, and those with “fold” performance in the prior month state as having a low-performance state. Furthermore, the high-medium-performance state and low-medium-performance state are together referred to as the medium-performance state. These states are parallel to those used by Steenburgh (2008), who categorizes salespeople in line with their state from the perspective of performance against quota. Steenburgh studies how salespeople’s states in the context of performance to date (i.e., before and after quarter-end dates) affect revenue production in a given period.TABLE: TABLE 1 Covariates Used in the AnalysesTABLE:  Notes: The subscript p denotes a salesperson; we have data for 813 salespeople. The subscript t denotes the month; we have data for 33 months, so t has a value of 1–33.We investigate the association among the interaction of the type of SPI earned by each salesperson in the current month and the salesperson’s performance state in the prior month on the dependent measure, SQR in the current month. Recall that SQR (M = 1.10, SD = .315) is each salesperson’s current month’s sales production measured against the quota for the month. The first set of substantive covariates is the “type of SPI earned in the current month” (i.e., no SPI vs. early SPI vs. later SPI vs. both SPIs) by each salesperson (“SPI Type” hereinafter). The second set of substantive covariates is “sales performance state in the prior month” (i.e., fold vs. minimum vs. stretch vs. outstanding; “Performance State” hereinafter). The last set of substantive covariates is the interaction between SPI Type and Performance State.To operationalize the SPI Type covariate, we rely on indicator variables EarlySPI, LaterSPI, and BothSPI, which respectively assume a value of 1 if the salesperson earned only the early SPI, only the later SPI, or both SPIs in the month. When all indicators (EarlySPI, LaterSPI, and BothSPI) have a value of 0, it implies that the salesperson did not earn any SPI in that month. Furthermore, to operationalize the Performance State covariate, we rely on indicator variables lag_Min, lag_Stretch, and lag_Outs, which respectively assume a value of 1 if the salesperson had achieved “minimum,” “stretch,” and “outstanding” sales performance category at the end of the prior month. When all three indicators are 0, it implies that the salesperson had achieved “fold” category at the end of the prior month. Consequently, the first observation in each salesperson’s panel had to be discarded because it did not have data on sales performance categorization achieved by the salesperson in the lagged period. Table 2, Panel A, presents frequency cross-tabulation of “performance category in current month” and of “type of SPI earned in current month” (i.e., SPI Type). Similarly, Table 2, Panel B, presents a frequency cross-tabulation of “performance category in current month” and of “performance category in prior month” (i.e., Performance State). The Web Appendix presents further details on the data.Note that the prior month’s Performance State indicator variables (lag_Min, lag_Stretch, and lag_Outs) enable us to check whether this sales force is playing timing games. As Steenburgh (2008) has highlighted, timing games imply that salespeople move orders from one period to the next, or vice versa. If this has occurred, per Steenburgh’s logic, lower levels in the prior month’s sales Performance State (e.g., lag_Min) should be more strongly associated with SQR in the current month than higher levels in prior month’s sales Performance States (e.g., lag_Stretch, lag_Outs). Alternatively, if salespeople have moved orders from the upcoming period to the earlier period, then higher levels of Performance State (e.g., lag_Stretch, lag_Outs) should have a lower association with SQR at the end of the upcoming month compared with that observed for lower levels of Performance State (e.g., “fold,” “minimum”). We empirically checked for timing games, and we did not find any evidence to support it. We provide additional details on this in the next section.On this issue, one might wonder whether salespeople might be inclined to move orders to the next month, thereby enhancing chances of earning the early SPI in the next month, once they cross their earnings ceiling at 120% of the quota at the end of a month. However, given that this firm is operating in an emerging economy, the customers of this firm mostly are small family-owned enterprises that are usually cash-strapped. Such buyers do not have the reserve funds and therefore have little ability to strategically time their purchases to acquiesce to such requests from the salespeople to manipulate order timing.Furthermore, even if a salesperson is categorized as belonging to “outstanding” or “stretch” performance categories in four or more months over the previous six months, the sales managers at the firm do not necessarily increase that salesperson’s quotas in future periods. With such ongoing monitoring, salespeople at this firm are, in general, unlikely to time their orders to hit the early SPI in the next month. In addition, the bonus amount of 300 currency units of the early SPI is too small (specifically, less than one-fifteenth of the highest month-end bonus of 4,200 currency units) to risk playing timing games, in the face of monitoring by sales managers. Another issue is whether salespeople might play timing games with the SPI quotas and delay in registering sales production. However, salespeople are unlikely to delay registering sales because there are no benefits (financial or otherwise) of doing so.We also consider whether there exist any Performance State–related momentum (or persistence) effects implying that success (failure) in exceeding the quota in the prior month is associated with success (failure) in the current month as well. If this is supported empirically, it likely suggests that, compared with salespeople with medium- and low-performance states, salespeople with high-performance states may be able to (at least temporarily) develop superior strategies about making a sale in the next period. These superior strategies may enable a high-performance-state salesperson to have superior performance in the next month as well. If this is the case, the magnitudes of the regression coefficients of lag_Stretch and lag_Outs should be positive and higher than the magnitude of the coefficient of lag_Min. In such a case, the magnitude of the regression coefficients of lag_Outs should also be higher than the magnitude of the coefficient of lag_Stretch. Managerially speaking, if salespeople’s Performance State has an association with their performance in the next period, it can have important implications for managers in developing new and effective types of compensation plans.If Performance State–related momentum effects exist, then another substantive issue is to understand whether the SPI Type earned in the current month is more effective than the prior month’s sales Performance State in determining the current month’s SQR. If the SPI Type earned in the current month is less important than Performance State, then being self-regulated in the next month and earning SPIs may not even matter for the salespeople with a high-performance state. If salespeople with a high-performance state are adept at selling and have superior strategies than others, they may be able to continue being high performers in the future, without the need for self-regulation or learning through spacing effects. In this perspective, high performers can produce results anytime and in relatively little time.Alternatively, there is the perspective that all salespeople—even those with a high-performance state—should benefit by being self-regulated. If high-performance-state salespeople need to inculcate self-regulation and earn SPIs in the next month to experience high performance in the next month, it will further underscore the importance of SPIs. Understanding this issue is important because extant literature has not investigated whether high-performance-state salespeople benefit from earning SPIs and being self-regulated.Finally, we also want to study the impact of the interaction between SPI Type and Performance State on the current month’s SQR. Thus, we also incorporated nine (i.e., 3 · 3) interaction effect terms, given that the SPI Type covariate has three indicator variables (EarlySPI, LaterSPI, and BothSPI) and the Performance State covariate has three indicator variables (lag_Min, lag_Stretch, and lag_Outs). In addition, we incorporated several control variables (e.g., geographic location, quarter) into the model. For a list of all variables used in our two analyses, see Table 1.We also tested for evidence of ratcheting of sales quotas. To that end, we undertook a series of three regressions presented in Chung, Steenburgh, and Sudhir (2014) and did not find any significant effects, suggesting that it is unlikely that this firm ratchets its salespeople’s quotas. Managers at the firm also shared that they did not believe in this practice, as it can demotivate high performers. Thus, following Chung, Steenburgh, and Sudhir, we treat each salesperson’s quotas as exogenous. The random intercepts linear regression model used in the First Analysis isIt is noteworthy that some of the covariates in the analysis in Equation 1 may be potentially endogenous. We provide details on the strategy that we incorporated to address this endogeneity concern in the context of the regression model in Equation 1 in the Web Appendix.TABLE: TABLE 2 Cross Tabulations   Results of First AnalysisWe executed the sampler for a total of 90,000 draws. The first 35,000 draws were discarded as burn-in. Subsequently, 55,000 of the Markov chain Monte Carlo (MCMC) draws were obtained for every parameter. Table 3 summarizes these posteriors for the relevant covariates in the random intercepts linear regression model in Equation 1.Main Effect of SPI Type on SQRNext, we focus on the main effects of earning early SPI, later SPI, or both SPIs in a month (vs. not earning any SPI) on SQR. Compared with not earning any SPI, earning the early SPI in a month (i.e., EarlySPI covariate; b = .242, p < .05) is associated with increasing the SQR earned by the salesperson in that month. Thus, H1 is supported. Furthermore, compared with not earning any SPI in a month, earning the later SPI in a month (i.e., LaterSPI covariate; b = .282, p < .05) is associated with a higher increase in SQR earned by the salesperson in that month, thereby suggesting additional support for H1. However, the magnitude of the coefficient of LaterSPI covariate (b = .282) is higher than that of the EarlySPI covariate (b = .242). This indicates support for H2,1 thereby indicating that earning the later SPI in a month is better than earning just the early SPI at enhancing SQR at the end of that month. From the standpoint of achieving overall success toward a superordinate goal, the finding that late success is more successful at achieving a subgoal than early success is an interesting outcome from a behavioral perspective.Compared with not earning any SPI in a month, earning both SPIs in a month (i.e., BothSPI covariate; b = .339, p < .05) is also associated with an increase in the SQR earned by the salesperson in that month. Because the coefficient of the BothSPI covariate (b = .339) is higher than that of the LaterSPI covariate (b = .282), earning both SPIs in a month is better than earning just the later SPI at enhancing SQR at the end of that month. Thus, H3 is supported. In addition, recall that, at this firm, a salesperson who earns only the later SPI and a salesperson who earns both SPIs need to cross the same sales threshold (50% of quota) by the 20th of the month.Main Effect of Performance State on SQRTable 3 suggests that the main effect of the “outstanding” performance state in the prior month (lag_Outs) is larger than that of the “stretch” performance state (lag_Stretch), which, in turn, is larger than that of the “minimum” performance state (lag_Min). This is because the magnitude of the coefficient of lag_Outs (b = .187, p < .05) is higher than that of lag_Stretch (b = .149, p < .05), which, in turn, is larger than that of lag_Min (b = .067, p < .05). Therefore, we cannot infer that this sales force is playing any timing games, a result that is consistent with Steenburgh (2008). The result also suggests that there are performance state–related momentum effects. In line with our previous discussion, we infer that compared with salespeople with medium-performance and low-performance states, salespeople with high-performance states are likely able to incorporate superior selling strategies, enabling them to continue having high performance in the next period.Furthermore, our results suggest that both increasing levels of SPI Type (from EarlySPI to BothSPIs) and increasing levels of Performance State (from lag_Min to lag_Outs) are associated with increasing SQR in the current month. However, our results also indicate that SPI Type in the current month is more effective than Performance State at enhancing SQR in the current month. This is because the regression coefficient of even the lowest level of SPI (i.e., EarlySPI coefficient; b = .242) is larger than the coefficient of the highest level of performance state (i.e., lag_Outs; b = .187). Essentially, from the perspective of enhancing salespeople’s performance in the current sales period, having a high-performance state in the prior period is less beneficial than earning SPIs from early on in the current period. However, to get an overall idea of the complete effects, we also need to consider the interaction effects.Interaction Between SPI Type and Performance State on SQRBecause the model in Equation 1 is a linear model, the regression coefficients themselves also depict the marginal effects. Note that we will need to rely on the estimate of the main effects as well as that of the interaction effects to understand the complete effects. As an example, for a salesperson who had an “outstanding” performance state in the prior month and who has earned both SPIs in the current month, the prediction for SQR in the current month is the sum of the following estimates presented in Table 3:• .815 (adjusted estimate for the intercept);• .187 (the main effect of the “outstanding” performance state in the prior month; i.e., the coefficient of the lag_Outs covariate);• .339 (the main effect of earning both SPIs in the current month; i.e., the coefficient of the BothSPI covariate); and• -.039 (the coefficient of BothSPI · lag_Outs interaction term covariate).TABLE: TABLE 3 Estimates for SQR Linear Regression in the First Analysis  Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (i.e., p <.05) for the coefficient.The resulting value is an SQR prediction of 1.302. We interpret this result as follows: a salesperson with an “outstanding” sales performance state in the prior month who has earned both SPIs in the current month will have an SQR of 1.302. Likewise, a salesperson with an “outstanding” performance state in the prior month but who did not earn any SPI in the current month will have an SQR prediction of 1.002 (i.e., the sum of .815 [estimate of the intercept] and .187 [the coefficient of lag_Outs]). The predicted SQR values for these two salespeople (1.302 vs. 1.002, respectively) highlight that salespeople with a high-performance state (“outstanding” state in the prior month) benefit by earning both SPIs relative to not earning any SPI. Essentially, a salesperson who has an “outstanding” performance state in the prior month has a boost in his or her SQR of .299 (i.e., 1.302 – 1.002) in that month if (s)he were to earn both SPIs (vs. no SPIs) in that month. Thus, we infer that although a high-performance state may enable the salesperson to have confidence and superior strategies in the next period, these superior strategies alone do not guarantee that high-performance-state salespeople will achieve the upper bound of their performance. By regulating themselves through earning SPIs, these salespeople were able to enhance their sales performance and reach their full potential.Table 4 presents an overview of the model-based predictions for SQR under the different conditions that we have discussed. All the predictions are based on the regression coefficients of the substantive covariates that are estimated from the model in Equation 1 and were presented in Table 3. For example, Table 4 suggests that the predicted SQR in the current month is 1.100 (sum of .815 [intercept], .149 [coefficient of lag_Stretch], .282 [coefficient of LaterSPI], and -.146 [coefficient of LaterSPI · lag_Stretch interaction term]) for a salesperson who had a “stretch” performance state and who earned only the later SPI in the current month.Furthermore, for all the performance states (i.e., “fold,” “minimum,” “stretch,” and “outstanding”), the last row in Table 4 presents the boost in SQR if the salesperson had earned any specific SPI (vs. no SPIs) in the current month. For example, a salesperson who has a “minimum” performance state will experience a boost in SQR of .150 (i.e., 1.032 – .882) in that month if (s)he earns the early SPI (vs. no SPIs) in that month.Table 4 suggests that, in general, the amount of boost in SQR that salespeople receive in earning (vs. not earning) SPIs is U-shaped going from high-performance state to low-performance state. For example, compared with salespeople who had “stretch” or “minimum” performance states in the prior month, salespeople who had either “outstanding” or “fold” performance states in the prior month experience a higher boost in their SQR when they earn both SPIs versus when they do not earn any SPI. Specifically, the boost in SQR by earning both SPIs in a month is higher for salespeople with an “outstanding” performance state (i.e., .299) or a “fold” performance state (i.e., .339) compared with that for salespeople with “minimum” (i.e., .250) or “stretch” (i.e., .212) performance states. Incidentally, the U-shaped relationship is more pronounced in the case of earning both SPIs compared with earning either the early SPI or later SPI.Recall that H4 was stated specifically in the context of earning both SPIs in a month.Furthermore, this general U-shaped relationship in the boost to SQR is observed even in the context of earning higher SPIs (e.g., only the later SPI) versus earning lower SPIs (e.g., only the early SPI); however, for brevity, we do not show the calculations for this context. In general, the lowest levels of boost in SQR were experienced by salespeople with the “stretch” (i.e., medium) performance state in the prior month.Overall, though we find some support for H4, our results also run partially counter to H4. Our finding that, like low-performance-state salespeople, even high-performance state salespeople benefit highly by earning an SPI is a novel one. Such U-shaped or inverted U-shaped nonlinear relationships have been observed in the literature. For example, Malhotra (1983) finds that low-knowledge and high-knowledge people are less likely to rely on extrinsic information in their decision making than are medium-knowledge people.As we have stated, we did not anticipate this U-shaped relationship between performance state and the boost in month-end performance. Nonetheless, given the novelty and potential importance of this finding, we provide a theoretical (albeit post hoc) explanation for our finding. In so doing, we rely on two well-established constructs in social psychology, ( 1) self-esteem (i.e., one’s perceived sense of self-worth; Heatherton and Polivy 1991; Schunk 1991) and ( 2) self-efficacy (i.e., self-confidence in one’s abilities; Bandura and Cervone 1983; Locke and Latham 1990a, b). Note that prior research in psychology has shown that these constructs interact with positive and negative external feedback (Bandura and Cervone 1983; Silverman 1964), which is akin to salespeople receiving feedback on the basis of achieving different SPIs in a sales period (i.e., no SPI vs. early SPI only vs. later SPI only vs. both SPIs) as a means of affecting their future performance. These conceptual parallels facilitate our efforts to provide a plausible explanation for our findings.Schunk (1991) proposes that high levels of achievement enable people to maintain high self-esteem. Accordingly, we suggest that high-performance-state salespeople should a priori have the highest expectation for doing well in the next month, and earning the SPI(s) provides validation of their high expectations. That is, earning SPI(s) in the next month acts as a positive signal and a reinforcement that enhances the self-esteem of such salespeople. Consequently, they will maximize their efforts to further boost their performance going forward (Baumeister and Tice 1985).Conversely, we speculate that the low-performancestate salespeople will a priori have the lowest expectations of doing well in the next month. In such a scenario, earning SPI(s) in the next month leads to a violation of their priors in a positive direction such that their perceptions of self-efficacy will increase. Education research has shown that students with enhanced self-efficacy expend more effort and persist with a task longer than those with low self-efficacy (Schunk 1984). Thus, we argue that low-performance-state salespeople who earn SPI(s) in a month will be motivated to work very hard to boost their performance at the end of the month.Finally, we argue that the medium-performance-state salespeople suffer merely because they are stuck in the middle. Medium-performance-state salespeople may not experience as much of an ego boost from earning SPIs as their high-performance-state counterparts, because the former have less a priori grounds to construe the achievement of SPIs as self-esteem-oriented credible signals of their competence and self-worth. Moreover, medium-performancestate salespeople may not experience as much of an efficacy-boost from earning SPIs as their low-performancestate counterparts, because self-efficacy issues may not have been salient to medium-performance-state salespeople in the first place. As a result, medium-performance-state salespeople attain a relatively smaller boost in performance from earning SPIs. Second AnalysisThe Second Analysis investigates support for H5–H7. Specifically, it tests whether earning SPIs in the prior month is associated with earning both SPIs in the current month. Our dependent variable for the Second Analysis is SPI Type in the current month, a disordinal nominal variable with four categories (no SPI, early SPI, later SPI, or both SPIs). Thus, we executed an MCMC-based random-effects multinomial probit regression, wherein not earning an SPI in the current month is the base category. We use the type of SPI earned in the prior month (identified by indicator variables lag_EarlySPI, lag_LaterSPI, or lag_BothSPI), Performance State (identified by indicator variables lag_Min, lag_Stretch, or lag_Outs), and the nine interaction terms among them as substantive covariates. The control variables we used in this Second Analysis were almost the same as the ones used as in the First Analysis (see Table 1).We executed the following random-effects multinomial probit regression: where SPI is a four-category nominal dependent variable representing the SPI Type earned by the salesperson in the current month. We executed a random-effects multinomial probit regression with “no SPI” as the base category. Some of the covariates in the analysis in Equation 2 are potentially endogenous as well. We addressed the endogeneity of such covariates in this model as we did in the First Analysis.TABLE: TABLE 4 Predictions for SQR and Performance Boost Under Different Conditions  aEffect of salary has been adjusted in the magnitude of intercept. Notes: N.A. = not applicable. Results of Second AnalysisWe executed the sampler for a total of 75,000 draws for the model in Equation 2. The first 35,000 draws were discarded as burn-in. Subsequently, 40,000 draws of the MCMC chain were obtained for every parameter. Table 5 summarizes the posteriors for the regression coefficients of relevant covariates in the model in Equation 2. Table 6, Panel A, presents the marginal main effect of SPI Type earned in the prior month (lag_EarlySPI, lag_LaterSPI, and lag_BothSPI) on each of the four categories of the SPI Type earned in the current month (no SPI, early SPI, later SPI, and both SPIs). Table 6, Panel B, presents the marginal main effect of Performance State achieved in the prior month (lag_Min, lag_Stretch, and lag_Outs) on the four categories of the SPI Type earned in the current month. Recall that we had 9 (i.e., 3 · 3) interaction effect terms; thus, their marginal effects on the four performance categories will lead to 36 (i.e., 9 · 4) marginal effects.Panels A–C of Table 7 present these marginal interactions for the four categories of the dependent variable (type of SPI earned in the current month), respectively, for when lag_EarlySPI, lag_LaterSPI, and lag_BothSPI had values of 1.As we did in the First Analysis, here, too, we need to use the main effects (in Table 6, Panels A and B) as well as the interaction effects (in Table 7, Panels A–C) to understand the complete effects. For example, for a salesperson who earned both SPIs and had an “outstanding” Performance State in the prior month, the probability of earning both SPIs in current month is the sum of the following:• 0 (main effect of lag_BothSPI on earning both SPIs from Table 6, Panel A)—although the estimate has a mean value of -.151, we use 0 because this estimate is nonsignificant;• -.073 (main effect of lag_Min on earning both SPIs in the current month, from Table 6, Panel B); and• .470 (interaction effect of earning both SPIs in the prior month; i.e., lag_BothSPI and earning “outstanding” performance in the prior month [lag_Outs] from Table 7, Panel C).The resulting value is .397 (i.e., 39.7%). That is, a salesperson who earned both SPIs and had an “outstanding” Performance State in the prior month has a 39.7% higher probability of earning both SPIs in the current month compared with one who did not earn any SPI and had a “fold” Performance State in the prior month.The marginal probabilities presented in Table 6, Panels A and B, and Table 7, Panels A–C, suggest that for all Performance States except “fold” in the prior month, a salesperson’s probability of earning both SPIs in the current month increases, provided that the salesperson earned the later SPI or both SPIs in the prior month. We make this inference because, in general, the marginal effects in the rightmost column in Table 7, Panels B and C, are positive and significant. Note that all the marginal main effects in Table 6, Panel A, are nonsignificant. Combined, these results suggest that earning the early SPI (compared with not earning any SPIs) in the prior month does not increase the probabilities of earning an SPI in the current month. However, earning the later SPI or both SPIs (rightmost column in Table 7, Panels B and C, respectively) in the prior month positively enhances the probability of earning both SPIs in the current month. Note also that this period-to-period SPI persistence is relatively stronger for salespeople with high-performance states (such as “Outstanding” or “Stretch”) in the context of earning both SPIs in a period. Furthermore, not earning any SPI or even earning only the early SPI does not enhance the probability of earning both SPIs in the next month. Overall, we find support for H6 and H7, but not for H5. Discussion, Managerial Implications, and LimitationsIn this research, we investigate whether different types of sales production trajectories of salespeople during a period are associated with different levels of sales performance at the end of that period. Most notably, we compared the period-end sales performance of a salesperson whose sales production trajectory steadily grew until a certain point within the sales period with that of a salesperson who had relatively low production early in the period followed by very rapid sales production success later in the period. This is an empirical issue because extant literature has presented conflicting evidence on which of the two types of trajectories should have a stronger association with enhanced period-end sales production performance. To that end, we empirically investigate the effect of earning SPIs in a sales period on salespeople’s period-end performance.The SPIs have certain similarities with quarterly bonuses in the context of the annual quota-bonus plan. Both situations present salespeople with intermediary goals in the context of the superordinate period-end quota. Thus, because there is a sub-goal, the setting is similar in the sense that working toward an intermediary goal has an effect on the achievement of the overall goal. Yet a key aspect of our data is that the SPIs at our data-provider firm are cumulative and specified as a percentage of the period-end quota that must be achieved by a certain time within the period, whereas quarterly quotas are specified only as dollar-amount targets. As such, at firms with only quarterly quotas in an annual quota-bonus plan, salespeople will achieve more sales by frequently (vs. less frequently) meeting quarterly quotas. In contrast, the cumulative nature of the SPIs at our data-provider firm enabled us to investigate some unique issues that have not yet been identified in the extant literature. This characteristic of the SPIs at our data-provider firm allowed us to make two important contributions.First, we were able to investigate whether there are benefits to earning two SPIs (vs. only the later SPI) in the context of period-end performance, even when, at some point in the sales period, earning two SPIs may have the same level of absolute sales as earning only the later SPI. Essentially, we investigated the role of incurring a steadily growing sales trajectory versus one with recent spikes on period-end sales performance. We find that salespeople will have better performance when they earn both SPIs rather than only the later SPI. Managerially speaking, for any level of sales achievement, a steadily growing sales trajectory is better than making few sales initially and then making a lot of sales to catch up, even though the latter approach involves the benefits of recency and momentum through a rapid spike in sales production. In addition, we find that earning only the later SPI is superior to earning only the early SPI for enhanced period-end sales performance. Managerially speaking, the firm could emphasize the later SPI, either by increasing the cash reward for earning the later SPI or by coaching salespeople to make it more salient. Alternatively, this also means that firms that are in a position to incorporate only a single SPI should have the SPI be triggered later, rather than earlier, in the sales period (thus mimicking the later SPI).TABLE: TABLE 5 Estimates for Multinomial Probit Regression in Second Analysis  Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficientSecond, we highlight that salespeople’s Performance State (i.e., the performance that salespeople experienced in the prior period) interacts with the shape of the sales production trajectory in the next period to differentially affect sales performance at the end of the next period. Essentially, earning both SPIs has a stronger benefit for salespeople with a high-performance state or a low-performance state, as compared with the benefits accruing to the salespeople with medium-performance states. The benefit of SPIs for the weak performers has been established in the education psychology and sales literature; however, our finding that SPIs benefit high-performance-state salespeople is novel. The observation that salespeople’s Performance State interacts with their sales production trajectory in the upcoming period to affect their performance in that period has implications for managers in the context of developing more effective incentive schemes. A potentially helpful intervention that managers could undertake is to implement a higher cash reward for the SPI in the next period for salespeople who have high performance (vs. medium performance) in a period. The higher cash reward for the SPIs in the next period should lead high-performance-state salespeople to put more effort into earning both SPIs in the next period (i.e., incur a steadily growing sales production trajectory in the next period), thus incurring the enhanced boost in period-end sales performance, thereby also benefiting the firm.The design of the SPIs at our data-provider firm (stated explicitly as percentages of the month-end quota) makes it very clear to salespeople that the month-end quotas are the main target of interest, and the SPIs are the means to that end. Our empirical analysis suggests that the SPIs at our data-provider firm indeed increase commitment toward the period-end quota. However, drawing on arguments made by Fishbach and Dhar (2008), it is likely that a different pattern of results could potentially emerge if SPIs end up being perceived as highlighting progress made rather than enhancing commitment to the period-end quota.The implication is that managers should design SPIs such that salespeople continually focus on the end-of-period quota even when they achieve the SPI quotas. This could be done in various ways, but we find that a simple framing in which the interim quotas are explicitly expressed in percentage terms of the end-of-period quota is very effective in maintaining salespeople’s focus on the end-of-period quota. In contrast, we sense that specifying targets in dollar amounts, as is done in the quarterly quotas of the traditional annual quota-bonus plan, is likely less effective. The implications are wide if future research can test whether SPIs that explicitly incorporate period-end quota (such as those implemented at our data-provider firm) garner higher commitment for period-end quotas than the implicit quarterly pacers in the popular quarter-annual quota-bonus scheme. If so, then this finding would suggest that managers may be better off redesigning the popular quarter-annual quota-bonus scheme to ensure that the quarterly quotas are specified as an explicit percentage of the annual quota rather than in dollar terms, as is the current practice.A limitation of this research is that we have not been able to clarify why high-performance-state salespeople benefited by earning SPIs in our study, whereas high-performing salespeople did not benefit much when quarterly quotas were put in place inChung, Steenburgh, and Sudhir’s (2014) study. There might be multiple reasons for this. First, we rely on performance states, whereas Chung, Steenburgh, and Sudhir identified performance categories by undertaking latent-class analysis in their counterfactual analysis. Second, in contrast to the SPIs at Chung, Steenburgh, and Sudhir’s data-provider firm, the unique design of the SPIs at our data-provider firm might be instrumental in increasing the strength of this phenomenon. Third, there are industry, cultural (both firm and social), and country differences between our studies. Our data-provider firm is a medium-sized CPG firm (with a respectable market share in a few of its product categories) in an emerging economy experiencing rapid growth, whereas Chung, Steenburgh, and Sudhir’s data-provider firm is a large office-products vendor based in the West.TABLE: TABLE 6 Marginal Main Effects  Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficient.Future research should test our conjectures, because several factors might be moderating the phenomenon described in this article. At our data-provider firm, salespeople were clearly categorized as belonging to one of the four categories at the end of each month. As such, their Performance State was salient to salespeople in the subsequent month. In addition, during each month, each salesperson received feedback through SPIs and thus was cognizant of the SPIs (s)he had earned in each month. Therefore, as a countercheck, future research could test relevant outcomes when salespeople’s Performance State and the SPI Type earned were not made explicit and salient to them. Finally, future research could also confirm whether our post hoc explanation for the U-shaped relationship indeed holds.TABLE: TABLE 7 Marginal Interaction Effects  Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficient.At our data-provider firm, the costs associated with instituting SPIs are modest: the payout for the early SPI (late SPI) is less than one-tenth (approximately one-fourth) of the average payout for achieving the regular monthly quota. Thus, relative to their costs, SPIs seem to have a larger “psychic value” in instilling self-regulation to our data-provider firm.Specialized personal incentives are managerially powerful interventions because they have some state persistence effects: salespeople who earn SPIs in a period are more likely to earn SPIs in the next period than those who do not. Thus, earning SPIs in any sales period has a double benefit: it has a direct positive association with enhanced sales performance in the current period, and it is positively associated with earning SPIs in the next period, thereby indirectly enhancing sales performance in the next period. Consequently, salespeople may be able to increase their chances of earning future SPIs and enhance their future period-end performance if they are able to meet SPIs in the current period.For that to happen, one potential intervention that sales managers could undertake is simply to increase the cash incentive associated with earning SPIs. By doing so, a greater proportion of the firm’s sales force is likely to start taking SPIs seriously, which, in turn, should enhance their period-end performance. Another intervention that sales managers could implement is to shift an increasing proportion of cash incentives from the end of the period to the point when the SPI is triggered within the period. Specifically, future research could undertake a field experiment in which cash incentives are split in different proportions across within-period SPIs versus at the end of the period bonus. This might enable managers to arrive at an optimal allocation scheme of splitting cash across within-period SPIs and the period-end bonus. In addition, because the salespeople with high- and low-performance states experience the greatest performance boost by earning both SPIs, another intervention could be for managers to put in place higher incentives to earn SPIs, as compared with incentives provided for salespeople who have a medium performance state.There are several other limits to the extent to which explicit SPIs can increase salespeople’s sales achievement. Other aspects of SPIs that future research could investigate include the optimal number of SPIs a firm should institute, whether that number depends on the length of the quota period, and whether SPIs can be nonmonetary (e.g., coupons, vouchers, gifts, recognition/commendations from the sales manager). In addition, the spacing between SPIs within the sales period is another important factor that we leave for further investigation. Future research could also study salesperson-level profitability to determine whether it has a similar pattern of effects as that observed in this research for sales performance. Finally, future research could investigate whether SPIs induce forward-looking behavior and timing games in situations wherein the magnitude of the SPIs offered to the sales force is high or comparable to the period-end bonus.Notes: The subscript p denotes a salesperson; we have data for 813 salespeople. The subscript t denotes the month; we have data for 33 months, so t has a value of 1–33.Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (i.e., p <.05) for the coefficient.aEffect of salary has been adjusted in the magnitude of intercept. Notes: N.A. = not applicable.Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficient.Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficient.Notes: The numbers in parentheses are standard deviations of the estimates. A boldfaced cell indicates a statistically significant estimate (p < .05) for the coefficient.  "
42,"How Does Consumers' Local or Global Identity Influence Price–Perceived Quality Associations? The Role of Perceived Quality Variance Globalization has substantially influenced the world economy. However, managers have a limited understanding of how local–global identity influences consumers' price perceptions and behavior. In this research, the authors propose that consumers' local (vs. global) identity leads to a greater tendency to make price–perceived quality (PPQ) associations. Perceived quality variance among comparison brands is a key mechanism underlying these effects. Two field studies (Studies 1 and 7), seven experiments (Studies 2–6, 9, and 10), and a systematic review of secondary data (Study 8) provide converging and robust evidence for the effect of local–global identity on PPQ. Consistent with the perceived quality variance account, when quality differences among the brands are made salient, PPQ associations of consumers high in global (but not local) identity significantly increase, compared with baseline conditions. However, when perceived quality similarities are made salient, PPQ associations of consumers high in local (but not global) identity significantly decrease. Product type and distribution of customer ratings represent natural boundaries for the relationship between local–global identity and PPQ. The authors conclude with the implications for managers' targeting endeavors. We also provide specific tools that marketers can use in ads and point-of-purchase materials to encourage or discourage consumers in making PPQ associations.KEYWORDS_SPLITMost marketers strive to find ways to charge high prices for their products. However, it is often difficult to do so without improving objective product performance or adding more attributes. For example, Netflix recently faced a huge uproar when it tried to raise prices without increasing perceptions of value. Its management team could have avoided this reaction by segmenting its market and starting the price increase in consumer segments that equate higher prices with higher quality. In the current research, we propose that if marketers focus on consumers with a local (vs. global) identity, their odds of success can drastically increase, as these consumers tend to view higher prices as signals of superior quality.Nevertheless, researchers are only starting to understand the role of local and global identities in consumer behavior (e.g., [ 7]). For example, it is unclear whether these identities differentially influence one of the most important relationships found in the pricing literature—namely, consumers' tendency to use product price to judge quality—that is, make price–perceived quality (PPQ) associations ([13]). Given its importance, there is renewed interest among researchers in examining the phenomenon ([44]).Indeed, managers currently seem to be puzzled about the potential role that consumers' local–global identity may play in their tendency to use price to judge quality. Our recent in-depth interviews with 15 senior level managers from Fortune 500 corporations revealed that managers across industries considered local or global communities in their pricing decisions, but none knew when such strategies might be effective and why (for key quotes, see Appendix A). This notion is illustrated by the following quote from a director of a firm's pricing division:When we try to introduce local flavors...it makes people think of their local communities....Here, we are careful to make sure that our product is seen as premium. You know...having a twist on the local ingredient is important. Similarly, it is important to have a reasonably higher price since it communicates premium-ness, and then reinforce it with advertising and packaging. But we don't know for sure why such consumers prefer premium brands. That is largely a mystery.So, the question is, how and why may consumers' local–global identity influence their PPQ associations? Extant findings seem to suggest that consumers with a global (vs. local) identity tend to have an abstract (vs. concrete) construal (as implied by [32]]), which in turn positively affects PPQ ([44]). In contrast, we propose that consumers with a local (vs. global) identity are more likely to make PPQ associations. Although there can be several reasons for this relationship, we focus on one—namely, greater perceived quality variance. We propose that a salient local (vs. global) identity is associated with a general dissimilarity-focus mindset. The enhanced salience of quality variance, in turn, leads people to focus more on price—one of the most direct and obvious cues used to compare brands—to infer product quality ([14]; [15]). We further demonstrate that contextual and product-related factors that influence perceived quality variance (e.g., services vs. goods, hedonic vs. utilitarian products, and convergent vs. divergent reviews) moderate the influence of local–global identity on PPQ.The issues we address have significant implications for the cross-cultural and pricing literature streams. First, by examining the role of local–global identity, we bring a fresh perspective to the cross-cultural literature, which is dominated by the individualism–collectivism dimension ([15], [17]; [18]; [19]; [37]). Second, we contribute to the pricing literature by examining how an important but underexplored factor, local–global identity, influences PPQ associations. Third, we are the first to uncover perceived quality variance as a new consequence of local and global identity. Fourth, we show that the strength of the association between local–global identity and PPQ associations varies by factors that influence perceived variance in brand quality.Managerially, our findings suggest that marketers of relatively high-priced products should situationally activate consumers' local identity, which facilitates PPQ. Furthermore, in line with the perceived quality variance account, for products that charge a premium price over competing products, marketers can use situational cues to increase perceived quality variance and facilitate consumers' PPQ. In contrast, for products that adopt a low-price strategy, marketers can use situational cues to reduce perceived quality variance. Our findings also suggest the importance of adapting marketing strategies to different regions: in rural areas where local identity is likely to be salient, consumers likely have high levels of PPQ, whereas in metropolitan areas where global identity is more salient, marketing campaigns are needed to enhance consumers' PPQ so that consumers perceive higher prices to be signals of superior quality. Similar strategies can be applied to countries around the world that are high in local or global identity. These insights also help address a current debate on whether companies should be more locally oriented, and how this may affect consumers. Next, we discuss the link between local–global identity and PPQ, followed by hypothesis development and empirical testing using both field and lab studies. Local–Global Identity and PPQ AssociationsRecent research delineates two distinct consumer identities (i.e., local identity and global identity), reflecting how strongly people associate with the local and the global community, respectively ([34]). Individuals whose local identity is salient (""locals"") are faithful and respectful of local traditions, interested in local events, and identify with people in their local community, whereas those with a salient global identity (""globals"") favor globalization, view the world as a ""global village,"" and blur the lines of distinction between local and nonlocal people and events ([ 1]; [45]). Furthermore, consumers high (vs. low) in local identity prefer local products and brands, whereas those high (vs. low) in global identity prefer global products and brands ([45]).Individuals from more globalized countries, such as the United States and Canada, tend to have a stronger global identity because they are more likely to meet different types of people, encounter different cultures, and access stories and news from other countries. In contrast, those from more localized countries (e.g., China, India) tend to have a stronger local identity because of their restricted access to other cultures ([ 1]; [ 6]). Research has further suggested that global and local identities can also be fruitfully activated through priming procedures (e.g., [40]; [45]).At the national level, there is evidence that people in countries with different levels of local–global identity differ in their tendency to use price to infer product quality. For example, Chinese and Indian consumers (who are high in local identity) make stronger PPQ associations than do U.S. and Canadian consumers (who are high in global identity) ([42]). Similarly, Polish (high in local identity) make higher PPQ associations than Germans (high in global identity) ([46]). However, these findings are inconsistent with those of another study, which shows that there is no difference in PPQ across different countries ([ 4]). Yet because these studies do not focus on cultural differences, we do not know whether local–global identity was responsible for these results. Some previous research has attributed these national differences to cultural dimensions other than local–global identity ([14]; [17]). More importantly, no previous research has offered theoretical explanations for the possible effect of local–global identity on PPQ. A clearer theorization of the mechanism through which local–global identity affects PPQ will advance our understanding of how consumers differ in their propensity to make price–quality inferences, and why. We propose that perceived quality variance is a key mechanism through which local–global identity affects PPQ, as discussed next. Local–Global Identity and Perceived Variance Among Comparative ObjectsThe ability to make comparative judgments is a fundamental human characteristic ([31]). People tend to follow one of two comparison processes—namely, dissimilarity focus and similarity focus—to make judgments ([30], [31]). We propose that locals (vs. globals) are more likely to focus on dissimilarities than similarities, because locals (vs. globals) tend to discern greater differences between local and nonlocal communities, which motivate them to associate more values with local traditions and local events. In contrast, because globals view the world as a ""global village"" and blur the lines of distinction between local and nonlocal people and events, they are more likely to focus on similarities. For example, Koreans (who are high in local identity) draw clear distinctions between in-group and out-group members, whereas Americans (who are high in global identity) do not ([35]). In addition, prior studies have also pointed to an association between high (vs. low) degrees of local identity and perceived dissimilarity from out-group members. In particular, activating one's own traditions and values can enhance intergroup aggression, especially when the in-group and out-group are in conflict ([39]). Conversely, research has suggested a link between openness to diversity (a characteristic of globals but not locals) and a similarity-focus mindset. For example, openness to diversity reduces perceived difference from other group members ([11]).The dissimilarity focus among locals (vs. globals) also extends to nonsocial domains. For example, when asked to answer partially redundant questions (e.g., to rate both academic satisfaction and general life satisfaction), Chinese (high in local identity) spontaneously recognize the redundancy problem (e.g., academic satisfaction is part of general life satisfaction) and adjust their responses accordingly; however, Germans (high in global identity) do not detect the redundancy ([36]). Similarly, [22] showed that, when evaluating two videos, individuals with overseas experiences (high in global identity) are able to identify more similarities than those without overseas experiences (high in local identity).In the context of product evaluations, when a local identity is salient, we propose that individuals will have a dissimilarity-focus mindset and perceive greater variance among brands in the marketplace. The perception that brands are dissimilar should motivate locals to look for cues to make sense of the distinctions. However, when a global identity is salient, we propose that individuals will have a similarity-focus mindset and view things as homogeneous, leading to lower perceived quality differences among brands. The perception that brands are similar discourages consumers from expending effort to differentiate them (see [31]) and to look for cues that enable such distinction. Next, we discuss how these differences may influence the tendency to use price as an indicator of product quality. Local–Global Identity, Perceived Variance, and PPQ AssociationsOur focal hypothesis that perceived quality variance mediates the relation between local–global identity and PPQ associations (see Figure 1) relies on the proposed link between perceived variance among comparative brands and PPQ. We expect this association for several reasons.Graph: Figure 1. The impact of local–global identity on PPQ associations.Consumers who perceive greater variance among comparative brands may be more motivated to look for cues to mentally differentiate the brands, as doing so may enable them to satisfy the fundamental human need to make sense of the world ([14]). In situations where nonprice cues are not diagnostic, such as when performance-related attributes are not alignable, perceived dissimilarity among comparative brands drives consumers to rely on alignable cues (e.g., price) that readily enable comparison between brands to infer quality. Indeed, price is intuitively one of the most important alignable product attributes ([14]; [27])—a dominant and salient attribute that enables consumers to directly and quickly compare brands ([15]; [27]; [33]). Thus, people who want to make sense of dissimilar objects (i.e., locals) are more likely to use price as a cue. When they need to determine brand quality, these consumers may be more likely to make PPQ associations.In contrast, those who perceive low variation in quality tend to view high- and low-priced brands as not differing much in quality and therefore are less motivated to look for and use cues that distinguish quality. Such individuals may be less likely to use price as a cue for inferring product quality. Accordingly, when consumers perceive the difference between two brands to be obvious, they selectively access information that supports the dissimilarity ([43]). However, when perceived difference across brands is low, consumers are likely to view the quality of high- and low-priced brands to be similar and are thus less likely to use any cues (e.g., price) to differentiate the brands. H1  : When evaluating brand quality, locals have a greater tendency than globals to make PPQ associations. H2  : The effect of local (vs. global) identity on PPQ associations is mediated by perceived variance among comparative brands in the marketplace. Boundary ConditionsTo advance our understanding of the underlying role of perceived quality variance, we also examine potential boundary conditions for the effect of local–global identity on PPQ associations. We have argued that locals (vs. globals) perceive greater variance in the quality of brands, which increases their tendency to use price to judge a product's quality. Thus, when quality differences among brands are made salient through a contextual cue (compared with a control condition wherein they are unchanged), globals—who, by nature, perceive less quality variance and have greater potential for increase—should be more likely to notice the differences among the brands and thus use price as an indicator of brand quality. However, such a contextual cue is less likely to increase the PPQ associations of locals, whose tendency to see variation (and thus, to make PPQ associations) is already high (""ceiling effect"").Similarly, when quality similarities among brands are made salient, locals—whose baseline tendency to discriminate among brands is high and has a greater potential for decrease—should be less likely to perceive brands as different and, therefore, have a lower tendency to make PPQ associations, compared with a control condition in which quality variance is unchanged. However, globals' baseline tendency to discriminate among brands is low and is difficult to decrease further (""floor effect""). Thus, their tendency to make PPQ associations should be unchanged when quality variance is reduced, relative to a control condition. We hypothesize the following: H3a:  When the quality difference among brands is made salient (compared with a control condition in which quality variance is unchanged), globals' tendency to make PPQ associations is elevated, whereas locals' tendency to use PPQ associations is unaffected. H3b:  When the quality similarity among brands is made salient (i.e., quality variance is reduced, compared with a control condition in which quality variance is unchanged), locals' tendency to make PPQ associations is decreased, whereas globals' tendency to use PPQ associations is unaffected.In real-life situations, consumers make choices not just about physical goods but also about services. Given that services are intangible and heterogeneous, their perceived quality difference is inherently greater than that of goods ([24]). Greater variation in the quality of services (vs. goods) should increase globals' tendency to make PPQ associations because their baseline tendency to differentiate brands is low and has greater potential for increase. However, because locals' tendency to make PPQ associations is already high, there is little room to increase it further (the same ""ceiling effect"" argument outlined previously). As a result, they should exhibit little change in PPQ when evaluating services (vs. goods). H4:  When evaluating services (vs. goods), globals' tendency to make PPQ associations is significantly higher, whereas locals' tendency to make PPQ associations does not differ.Beyond product type, another context that naturally changes consumers' perceived quality difference is when they see divergent or convergent customer ratings on products that interest them. Online reviews increasingly influence consumer purchase decisions ([38]). However, these reviews do not necessarily agree with one another. Convergent customer ratings in a product category (i.e., when most people leave similar ratings for products in that category) are likely to give customers an impression that various products in this category are of similar quality (i.e., low quality variance). In contrast, divergent customer ratings (i.e., people's opinions are all over the place and there is no dominant view) are likely to give customers an impression that the quality of products in this category differs greatly. Drawing on H3, we predict the following: H5a:  When the distribution of customer product reviews is divergent (compared with a control condition), globals' tendency to make PPQ associations is significantly increased, whereas locals' tendency to make PPQ associations does not differ. H5b:  When the distribution of customer product reviews is convergent (compared with a control condition), locals' tendency to make PPQ associations is significantly reduced, whereas globals' tendency to make PPQ associations does not differ.We tested our hypotheses in eight studies. Study 1 provided initial evidence on the link between local–global identity and PPQ associations in a shopping mall with real consumers (H1). Study 2 replicated Study 1's findings in a different context and demonstrated perceived quality variance as a key mechanism underlying these effects (H2). The next three studies examined several contextual moderators, including salience of quality variance/similarity (Study 3), product type (services vs. goods; Study 4), and distribution of customer ratings (convergent or divergent; Study 5). Study 6 primed both local–global identity and construal level to examine their differential effects on reliance of price as an indicator of quality and reconciled the seemingly contradictory predictions between our theory and those of construal level theory. Study 7 brought our theory to the field to examine how situationally activated local/global identity affects consumers' monetary expenditures. Finally, Study 8 provides the results of a meta-analysis of previous studies on PPQ associations conducted across different countries. Notably, consistent with prior research (e.g., [ 6]), our empirical work addresses the relative effects of local (vs. global) identity. Study 1: The Shopping Mall StudyWe designed Study 1 to test the effect of local (vs. global) identity on PPQ with real consumers in a shopping mall and to assess whether local–global identity can be situationally activated in a real consumption setting. Respondents were 164 shoppers at a shopping mall in the city of Hohhot, China, who were intercepted by the researchers and shown a brochure that described either a ""Think Local Movement"" or a ""Think Global Movement"" to manipulate local and global identity, respectively ([ 6]; for stimuli, see Web Appendix 1). Thereafter, participants were told that a well-known apparel company was considering releasing some shoes and caps to be sold at the mall and had hired us to conduct a test on consumers' quality perceptions of their products. The researchers then showed them three pairs of running shoes and three caps, with price tags attached (Shoe A: ¥299; Shoe B: ¥599; Shoe C: ¥799; Cap A: ¥39; Cap B: ¥69; Cap C: ¥99). Following [17], participants rated all six products on quality, reliability, and dependability (1 = ""Very Low,"" and 7 = ""Very High""), which were averaged to form a quality evaluation for both shoes (αs =.89 to.90) and caps (αs =.88 to.89).Following [45], we assessed the validity of the identity manipulation using a three-item scale, anchored by 1 = ""Global Citizen,"" and 7 = ""Local Citizen"" (e.g., ""For the time being, I mainly identify myself as a...""; α =.86; for the full scale and other measures used in this article, see Web Appendix 2). Results indicated that participants assigned to the local (vs. global) identity condition perceived themselves more as local citizens (for the local–global identity manipulation check results in this study and other studies, see Web Appendix 3). Participants also reported their age, gender, and household income.A 2 (identity) × 2 (product category; dummy coded 1 = shoes and 0 = caps) repeated-measure analysis of variance (ANOVA) on the correlation between retail prices and subjective quality evaluations (i.e., PPQ associations) revealed a significant main effect of identity (F( 1, 162) = 8.36, p <.01) but nonsignificant effects of product category and its interaction with identity (ps >.15), suggesting that PPQ associations did not vary by product category. Thus, the data were pooled across the product categories. For both product categories, participants in the local (vs. global) identity condition made significantly higher PPQ associations, as predicted in H1 (shoes: Mlocal =.68 vs. Mglobal =.40; t(162) = 2.98, p <.01; caps: Mlocal =.71 vs. Mglobal =.50; t(162) = 2.15, p <.05). Rerunning the analyses with age, gender, and household income as covariates did not change the pattern of results, and none of these demographic variables were significant (all ps >.40). Follow-Up StudyWe designed a follow-up study to replicate Study 1's finding in the United States, using 69 consumers (49 men; Mage = 31–40 years) shopping at an apparel store in an upscale mall. Respondents were guided to a table where they saw four caps marked with different prices (Cap A: $10; Cap B: $20; Cap C: $30; Cap D: $40). They were asked to rate the quality of each cap on a 0 to 100 scale. For each participant, the correlation between retail prices and quality ratings served as our dependent variable. Local–global identity was manipulated by the T-shirt the employee was wearing. The local-identity T-shirt contained the logo ""Think Local"" and the phrase ""supporting the link to local community,"" whereas the global-identity T-shirt contained the logo ""Think Global"" and the phrase ""supporting the link to the whole world"" (for a picture of these T-shirts, see Web Appendix 4). After completing quality ratings for each cap, participants rated the three-item local–global identity manipulation check questions (α =.91) as in Study 1. Results showed that participants in the local (vs. global) identity condition made significantly higher PPQ associations (Mlocal =.50 vs. Mglobal =.02; t(67) = 3.19, p <.01).In a real-life setting, Study 1 supported H1's prediction that locals (vs. globals) have a greater tendency to make PPQ associations. We conducted another study (Study 9 in Web Appendix 5) to test the generalizability of our findings over single-quality-cue and multiple-quality-cue formats. Results of this study replicated the findings of Study 1 and demonstrated that the effect of local–global identity on PPQ held in both multiple- and single-quality-cue conditions. In the next study, we aimed to test the mechanism underlying the link between local–global identity and PPQ. Study 2: The Role of Perceived Quality Variance Participants, Design, and ProcedureOne hundred ninety-six Amazon Mechanical Turk (MTurk) workers (89 men; Mage = 37.25 years, SD = 12.32) from the United States participated in Study 2, which entailed a 2 (identity: local vs. global) × 2 (price level: high vs. low) between-subjects design. Following [32], we manipulated local–global identity using a sentence-unscrambling task with ten sentences (the first ten items in Web Appendix 6). Those assigned to the local (global) identity condition were instructed to construct ten grammatically correct sentences using such sentences as ""Events know I local (global)."" The manipulation check questions (α =.94) were as in Study 1 (for results, see Web Appendix 3).Then, participants answered three questions on dissimilarity focus (e.g., ""At this time, I feel that I could easily identify differences in a set of comparative objects""; α =.60), and seven questions on perceived quality variance using a scale adapted from [ 2]; e.g., ""The quality of alarm clocks in the marketplace varies a lot""; α =.90). Both scales were anchored by 1 = ""Strongly Disagree,"" and 7 = ""Strongly Agree.""Next, following [14], participants viewed information about three brands of alarm clocks—the target brand and two comparison brands—which provided baseline price information. Participants were randomly assigned to either the high- or low-price condition, using identical product descriptions. The target brand was priced the highest (lowest) in the high (low) price condition, with equal relative price range (from 43% [15/30] to 75% [15/20], see Web Appendix 7). In addition, we used fictitious brand names to minimize the potential confounds. Afterward, participants rated the target brand on the same three-item quality measure as in Study 1 (α =.84). Results and Discussion Local–global identity and PPQ associationsA 2 (identity) × 2 (price) ANOVA on the quality index revealed no effect of local–global identity or price (ps >.11) but, more importantly, showed a significant identity × price two-way interaction (F( 1, 192) = 4.55, p <.05). Consistent with H1, locals rated the target brand as having significantly higher quality in the high-price condition (M = 5.54) than in the low-price condition (M = 5.03, t(102) = 2.63, p <.01). In contrast, the quality ratings for globals did not vary across the two price conditions (Mlow price = 4.98 vs. Mhigh price = 4.92; t(90) =.29, p =.77). Mediation analysisA bootstrapping procedure with 10,000 iterations using Model 15 of [ 9] PROCESS showed that the indirect effect of local (vs. global) identity on PPQ associations through perceived quality variance was positive (.11) and significant (95% confidence interval [CI] = [.02,.29], excluding zero), in support of H2.[ 5]Study 2 demonstrated that the effect of local (vs. global) identity on PPQ associations is mediated by perceived quality variance, in support of H2. Relative to globals, locals perceived higher levels of quality difference among comparative brands in the marketplace, which in turn led to greater PPQ associations. As we show in Study 10 (Web Appendix 8), price sensitivity and risk aversion cannot be alternative explanations of our findings.Our theorization suggests that local (vs. global) identity induces a general dissimilarity-focus mindset, which in turn enhances perceived quality variance, leading to higher PPQ. To assess the proposed serial mediation, we followed [28] to test two mediation models. We first tested whether dissimilarity focus mediates the effect of local–global identity on perceived quality variance. We then tested whether perceived quality variance mediates the effect of dissimilarity focus on PPQ (mediated-moderation model). As expected, for the first model, a bootstrapping with 10,000 iterations using Model 4 showed that the indirect effect of local–global identity on perceived quality variance through dissimilarity focus was positive (.18) and significant (95% CI = [.04,.36], excluding zero). Furthermore, the second mediated-moderation model (Model 15) showed that the indirect effect of dissimilarity focus on PPQ through perceived quality variance was also positive (.12) and significant (95% CI = [.01,.28], excluding zero).[ 6] These results provide support for our conceptualization. Next, we provide further evidence of the mechanism by manipulating the mediator ""perceived quality variance."" Study 3: Salience of Quality Variance Participants, Design, and ProcedureThree hundred eighty-seven MTurk workers (134 men; Mage = 39.84 years, SD = 12.82) from the United States participated in exchange for a small monetary incentive. The experiment consisted of a 2 (identity: local vs. global) × 2 (price level: high vs. low) × 3 (quality variance: enhanced, reduced, unchanged) between-subjects design.We manipulated local and global identity as in Study 2. Participants were then randomly assigned to one of the three quality variance conditions, which used a news report from a reputable magazine. In the quality variance–enhanced (reduced) condition, participants read a report from an interview with an expert regarding the quality of products in the marketplace, which included an excerpt stating the expert's opinion that ""durable appliances offered by different manufacturers in fact do (do not) have significant differences in product quality."" In the quality variance unchanged (control) condition, no such news was presented. Afterward, participants were shown the same three brands of alarm clocks as in Study 2. We added microwaves (for the product stimuli, see Web Appendix 8) as an additional product to enhance the generalizability of our findings. Participants were asked to rate the target brands on the same three-item quality index as in Study 1 (αalarm clock =.90 and αmicrowave =.93).Finally, as a manipulation check for quality variance prime, participants were asked to recall the news and indicate the expert's opinion about product quality (1 = ""has significant differences across products,"" 2 = ""does not have much difference across products,"" and 3 = ""I don't know about this information""). Results showed that most participants in the variance-enhanced condition selected 1 (93.8%), whereas most participants in the variance-reduced condition selected 2 (89.5%), and most participants in the variance-unchanged (i.e., control) condition selected 3 (73.6%; χ2 ( 4) = 504.48, p <.01). Thus, quality variance was successfully primed. Results and Discussion Local–global identity and PPQ associationsWe conducted a 2 (identity) × 2 (price) × 3 (quality variance) × 2 (product category; dummy coded 1 = alarm clock, and 0 = microwave) repeated-measure ANOVA on the quality index. Results revealed only a significant main effect of product category (F( 1, 385) = 16.93, p <.01); no other effects were significant (ps ranged from.11 to.51), suggesting that PPQ associations did not vary by product category. Thus, the data were pooled across the product categories. Results of the pooled data revealed no main effect of identity (F( 1, 385) = 1.96, p =.16), a significant main effect of price (F( 1, 385) = 20.79, p <.01) and variance (F( 2, 385) = 3.00, p =.05), no effect of identity × variance two-way interaction (F( 2, 385) =.82, p =.44), and significant two-way interactions between identity and price (F( 1, 385) = 6.40, p <.05) and between price and variance (F( 1, 385) = 7.77, p <.01). More important and consistent with H3a and H3b, there was a significant three-way interaction among identity, price, and quality variance (F( 2, 385) = 3.17, p <.05).In the control (i.e., variance-unchanged) condition, a 2 (identity) × 2 (price) ANOVA revealed no effect of identity or price (ps >.18), and a significant identity × price two-way interaction (F( 1, 385) = 9.44, p <.01). Locals rated the target brands as superior in quality in the high (vs. low) price condition (Mhigh price = 4.88 vs. Mlow price = 4.21, t(68) = 3.93, p <.01). However, globals did not rate the brands as significantly different across the price conditions (Mlow price = 4.53 vs. Mhigh price = 4.26, t(53) = 1.26, p =.21), in support of H1. Test of H 3aNext, we compared the PPQ associations in the variance-enhanced (vs. unchanged) conditions among locals and globals separately. For globals in the variance-enhanced and unchanged conditions, a 2 (variance) × 2 (price) ANOVA revealed no effect of salience (F( 1, 385) =.18, p =.68) or price (F( 1, 385) = 1.33, p =.25), and a significant quality-variance × price two-way interaction (F( 1, 385) = 8.40, p <.01), suggesting that enhancing the salience of quality variance significantly influenced globals' tendency to make PPQ associations. Contrasts suggested that globals made PPQ associations in the variance-enhanced condition (Mlow price = 4.02 vs. Mhigh price = 4.65; t(63) = −3.75, p <.01), but not in the variance-unchanged condition (Mlow price = 4.53 vs. Mhigh price = 4.26; t(53) = 1.26, p =.21; Figure 2).Graph: Figure 2. The moderating effect of salience of quality variance on the relationship between local–global identity and PPQ associations (Study 3).For locals in the variance-enhanced and unchanged conditions, a 2 (variance) × 2 (price) ANOVA revealed no effect of salience (F( 1, 385) =.01, p =.91) and a significant effect of price F( 1, 385) = 36.61, p <.01). Consistent with our hypothesis, there was no effect of variance × price two-way interaction (F( 1, 385) = 2.03, p =.16), suggesting that enhancing the salience of quality variance did not change locals' tendency to make PPQ associations. As shown in Figure 2, locals in both variance-enhanced (Mlow price = 4.02 vs. Mhigh price = 5.11; t(62) = −5.39, p <.01) and variance-unchanged (Mlow price = 4.21 vs. Mhigh price = 4.88; t(68) = −3.93, p <.01) conditions made PPQ associations. Taken together, these results supported H3a. Test of H 3bFurthermore, we compared the PPQ associations in the variance-reduced (vs. unchanged) conditions among locals and globals separately. For globals in the variance-enhanced and unchanged conditions, a 2 (variance) × 2 (price) ANOVA revealed no effect of variance, price, or the variance × price two-way interaction (all ps >.05), suggesting that reducing the salience of quality variance did not change globals' tendency to make PPQ associations. Contrasts showed that globals did not make PPQ associations in the variance-reduced (Mlow price = 4.61 vs. Mhigh price = 4.77; t(77) = −.68, p =.50) or variance-unchanged (Mlow price = 4.53 vs. Mhigh price = 4.26; t(53) = 1.26, p =.21) conditions (see Figure 2).For locals in the variance-reduced and unchanged conditions, a 2 (variance) × 2 (price) ANOVA revealed no effect of salience (F( 1, 385) =.64, p =.42) but a significant effect of price (F( 1, 385) = 6.25, p <.05) and a significant variance × price two-way interaction (F( 1, 385) = 4.47, p <.05), suggesting that reducing the salience of quality variance significantly influenced locals' tendency to make PPQ associations. As Figure 2 illustrates, contrasts showed that locals did not make PPQ associations in the variance-reduced condition (Mlow price = 4.63 vs. Mhigh price = 4.69; t(62) = −.25, p =.80), but did so in the variance-unchanged condition (Mlow price = 4.21 vs. Mhigh price = 4.88; t(68) = −3.93, p <.01). These results support H3b.Our framework suggests that locals (vs. globals) perceive greater quality variance among comparative brands, which in turn leads them to rely on price to infer the quality of these brands. Accordingly, situationally enhancing the salience of quality variance increased globals' but not locals' tendency to make PPQ associations, compared with a control condition in which quality variance was not changed. Similarly, situationally increasing the salience of quality similarity (compared with a control condition in which quality variance was unchanged) reduced locals' tendency to use price to indicate quality but did not affect globals' tendency to make PPQ associations, because globals already perceived low variance in quality to begin with.We designed the following two studies to extend Study 3 by using natural moderators, including product type (Study 4) and the distribution of customer ratings (Study 5). If our proposed mechanism holds, when the evaluation objects are services (vs. goods) or when the ratings from other customers are divergent (vs. control), we should replicate the findings in the variance-enhanced condition, as stated in H4 and H5a. However, when the ratings are convergent (vs. control), we should replicate the findings in the variance-reduced condition (H5b). Study 4: Services Versus Goods Participants, Design, and ProcedureTwo hundred seventy-eight MTurk workers (101 men; Mage = 39.89 years, SD = 12.22) from the United States participated in a study comprising a 2 (identity: local vs. global) × 2 (price: high vs. low) × 2 (product type: services vs. goods) between-subjects design. The procedure, manipulation of local–global identity, and measures were the same as in Study 3, except for three important differences: ( 1) we included three services (carpet cleaning, landscaping, and airline services; for stimuli, see Web Appendix 9); ( 2) in addition to the two products used before (i.e., alarm clock and microwave), we added sewing machines to ensure equivalence with the number of services; and ( 3) instead of keeping relative price range constant, we kept the same prices for the two baseline brands (e.g., $20 and $30). After examining descriptions of the three brands (i.e., the target brand and two other brands) for each product, participants rated the target brands on the same three-item quality index as in Study 1 (αs ranged from.82 to.93).[ 7] ResultsFor goods, we analyzed the data using a 2 (identity) × 2 (price) × 3 (category of goods; dummy-coded as 2 = sewing machine, 1 = alarm clocks, and 0 = microwave) repeated-measure ANOVA with quality index as the dependent variable. The analysis revealed that none of the effects related to category of goods were significant (ps >.26). For services, we analyzed the data using a 2 (identity) × 2 (price) × 3 (service type) repeated-measure ANOVA with quality index as the dependent variable. The analysis revealed a significant main effect of service category (F( 1, 131) = 3.83, p =.05), but none of its interactions with other factors were significant (ps >.50). Thus, we pooled the data separately for goods and services.Using the pooled data, we conducted a 2 (identity) × 2 (price) × 2 (product type) ANOVA on the quality index. Results revealed no effect of identity (F( 1, 270) =.35, p =.58) but did show significant effects of price (F( 1, 270) = 13.20, p <.01), product type (F( 1, 270) = 21.06, p <.01), product type × price two-way interaction (F( 1, 270) = 4.83, p <.05), and price × identity two-way interaction (F( 1, 270) = 5.23, p <.05); however, there was no effect of product type × identity two-way interaction (F( 1, 270) =.01, p =.94). Consistent with H4, there was a significant three-way interaction among identity, price, and product type (F( 1, 270) = 4.05, p <.05).For goods, a 2 (identity) × 2 (price) ANOVA revealed no effect of identity or price (ps >.33), but we did find a significant identity × price two-way interaction (F( 1, 270) = 9.13, p <.01). Locals rated the target brands as superior in the high- (vs. low-) price condition (Mlow price = 4.45 vs. Mhigh price = 4.94, t(71) = −2.93, p <.01), whereas globals rated the target brands as equivalent in quality across price conditions (Mlow price = 4.78 vs. Mhigh price = 4.52, t(66) = 1.51, p =.14). These findings replicated those of Studies 1 and 2. Test of H 4Next, we compared PPQ associations for services (vs. goods) among globals and locals separately. For globals, a 2 (product type) × 2 (price) ANOVA revealed no effect of price (F( 1, 270) =.99, p =.32) but a significant main effect of product type (F( 1, 270) = 10.82, p <.01) and a significant product type × price interaction (F( 1,270) = 8.87, p <.01). Globals made PPQ associations when evaluating services (Mlow price = 4.80 vs. Mhigh price = 5.31; t(61) = −2.66, p =.01) but not goods (Mlow price = 4.52 vs. Mhigh price = 4.78; t(66) = −1.51, p =.14; Figure 3). For locals, a 2 (product type) × 2 (price) ANOVA revealed significant effects of product type (F( 1, 270 = 11.34, _I_p_i_ <.01) and price (F( 1, 270) = 17.74, p <.01). More important and consistent with H4, there was no effect of two-way product type × price interaction (F( 1, 270) =.04, p =.85). Locals made PPQ associations when evaluating both services (Mlow price = 4.84 vs. Mhigh price = 5.39; t(70) = −2.98, p <.01) and goods (Mlow price = 4.45 vs. Mhigh price = 4.94; t(71) = −2.93, p <.01; Figure 3). Thus, these results supported H4.Graph: Figure 3. The moderating role of services versus goods on the relationship between local–global identity and PPQ associations (Study 4). Study 5: Convergent Versus Divergent Customer Reviews Participants, Design, and ProcedureParticipants were 785 MTurk workers (278 men; Mage = 39.33 years, SD = 13.13) from the United States who were randomly assigned to a 2 (identity: local vs. global) × 2 (price: high vs. low) × 3 (customer rating distribution: convergent, divergent, control) between-subjects design. The procedure, manipulation of local–global identity, product stimuli, and measures were as in Study 2 except for two differences: ( 1) we used microwaves in this study, and ( 2) before making judgments on the target brand, participants saw a summary table of customer ratings, which we used to manipulate the distribution of customer ratings. In the divergent-rating condition, the customer reviews were almost equally distributed across the ""poor,"" ""good,"" and ""excellent"" categories, whereas in the convergent-rating condition, customer reviews concentrated on the ""good"" category (for stimuli, see Web Appendix 10). Although the distribution of customer ratings differed, the average rating was the same across convergent and divergent conditions. In the control condition, there was no information about customer reviews.Thereafter, participants viewed information about three brands (i.e., the target brand and two other brands) of microwaves and evaluated the target brand on the three-item quality measure as in Study 1 (α =.90). Participants were then asked to rate perceived differences between microwaves in the marketplace using the perceived quality variance measure as in Study 4 (α =.81). Participants in the divergent-rating condition (M = 5.22) perceived more quality variance than those in the control condition (M = 4.97; t(526) = 2.22, p <.05), whereas those in the convergent-rating condition (M = 4.67) perceived less quality variance than those in the control condition (M = 4.97; t(519) = −2.41, p <.05), suggesting that our manipulation was successful. Results and DiscussionA 2 (identity) × 2 (price) × 3 (rating distribution) ANOVA on the quality index revealed no effect of identity or rating distribution (ps >.10), a significant effect of price (F( 1, 773) = 51.55, p <.01), no significant two-way interactions (ps >.21), and, importantly, a significant three-way interaction among identity, price, and rating distribution (F( 1, 773) = 5.32, p <.01).In the control condition, we expected to replicate the findings of Study 2. A 2 (identity) × 2 (price) ANOVA revealed no effect of identity (F( 1, 773) =.12, p =.73), a significant effect of price (F( 1, 773) = 16.75, p <.01), and a significant identity × price two-way interaction (F( 1, 773) = 10.90, p <.01). Participants primed with local identity rated the target brand as having higher quality in the high- (vs. low-) price condition (Mlow price = 3.71 vs. Mhigh price = 4.56; t(138) = −5.50, p <.01). However, those primed with global identity rated the target brand equivalently in the two price conditions (Mlow price = 4.10 vs. Mhigh price = 4.19; t(122) = −.56, p =.58). Test of H 5aNext, we compared PPQ in the divergent (vs. control) conditions among locals and globals separately. For globals in the divergent and control conditions, a 2 (rating distribution) × 2 (price) ANOVA revealed no effect of rating distribution (p >.11), a significant effect of price (F( 1, 773) = 16.31, p <.01), and a significant ratings distribution × price two-way interaction (F( 1, 773) = 10.70, p <.01). Contrasts showed that globals made PPQ associations in the divergent condition (Mlow price = 3.53 vs. Mhigh price = 4.39; t(122) = −5.44, p <.01), but not in the control condition (Mlow price = 4.10 vs. Mhigh price = 4.19; t(122) = −.56, p =.58; Figure 4). For locals in the divergent and control conditions, a 2 (rating distribution) × 2 (price) ANOVA revealed no effect of rating distribution (p >.15), a significant effect of price (F( 1, 773) = 59.68, p <.01), and no effect of rating distribution × price two-way interaction (F( 1, 773) =.01, p =.92). Contrasts showed that locals made PPQ associations in both the divergent (Mlow price = 3.85 vs. Mhigh price = 4.72; t(138) = −5.49, p <.01) and control (Mlow price = 3.71 vs. Mhigh price = 4.56; t(138) = −5.50, p <.01; Figure 4) conditions, in support of H5a.Graph: Figure 4. The moderating role of convergent versus divergent ratings on the relationship between local–global identity and PPQ associations (Study 5). Test of H 5bFurthermore, we compared PPQ in the convergent (vs. control) conditions among locals and globals separately. For globals in the convergent and control conditions, a 2 (rating distribution) × 2 (price) ANOVA revealed no effect of rating distribution, price, or the rating distribution × price two-way interaction (ps >.19), suggesting that providing convergent customer reviews did not change globals' tendency to make PPQ associations. Contrasts showed that globals did not make PPQ associations in the convergent (Mlow price = 3.90 vs. Mhigh price = 4.12; t(126) = −1.25, p =.21) and control (Mlow price = 4.10 vs. Mhigh price = 4.19; t(122) = −.56, p =.58; Figure 4) conditions. For locals in the convergent and control conditions, a 2 (rating distribution) × 2 (price) ANOVA revealed no effect of rating distribution (p >.13), a significant effect of price (F( 1, 773) = 14.84, p <.01), and a significant rating distribution × price two-way interaction (F( 1, 773) = 13.10, p <.01), suggesting that providing convergent customer reviews influenced locals' tendency to make PPQ associations. Contrasts showed that locals did not make PPQ associations in the convergent condition (Mlow price = 4.29 vs. Mhigh price = 4.31; t(127) = −.15, p =.88), but did so in the control condition (Mlow price = 3.71 vs. Mhigh price = 4.56; t(138) = −5.50, p <.01; Figure 4). Taken together, these results supported H5b.Using product type (Study 4) and distribution of customer ratings (Study 5) as natural boundary conditions, these studies provided additional evidence for the ""perceived quality variance"" account. We also conducted a study (Study 11 in Web Appendix 11) to examine hedonic (vs. utilitarian) product type as another natural moderator. Hedonic (vs. utilitarian) products by nature have greater perceived quality variance because different consumers tend to evaluate hedonic products using divergent criteria, whereas the evaluation of utilitarian products is mainly based on well-defined criteria ([12]). Our framework suggests that when evaluating hedonic (vs. utilitarian) products, globals' tendency to use PPQ associations will be elevated, whereas locals' tendency to use PPQ associations will be unaffected. Our results supported this prediction. These studies enhanced the external validity of our findings and showed direct evidence of the managerial implications of this research.In the next study, we aim to reconcile the seemingly contradictory findings predicted by our theory and those of [44]. These authors found that an abstract (vs. concrete) construal enhances PPQ associations. If globals (vs. locals) have a greater abstract (instead of concrete) construal (as implied by [32]]), this account predicts that they would be more likely to make PPQ associations, which is opposite to our prediction.We believe that the seemingly contradictory predictions are due to the conceptual distinction between local–global identity and construal level. Our theorization predicts that a local (vs. global) identity induces a dissimilarity-focus mindset, which in turn motivates the search for, and use of, diagnostic cues to make sense of the quality differences between brands. In contrast, construal-level theory suggests that abstract (vs. concrete) information such as price tends to exert greater impact on representations and judgments when construal level is high (vs. low; [44]). Thus, although a local identity and low-level construal both may lead to greater perceived differences among comparative objects ([21]), locals are driven by their innate dissimilarity-focus mindset, which motivates them to look for and use diagnostic cues such as price to justify brand differences. However, a low- (vs. high-) level construal reduces the tendency to use abstract cues such as price to judge product quality.We tested the distinction between local–global identity and construal level in the context of product choices. Specifically, we manipulated the diagnosticity of product attributes through trade-offs among product features. As an example, take three features of a digital camera: megapixels, optical zoom, and price. When attributes do not contain trade-offs (e.g., ""low in price but high in both megapixels and optical zoom"" vs. ""high in price but low in both megapixels and optical zoom""), the decision scenario is quite similar to the stimuli of [44], Experiment 2), in which the comparison was between a low-price, high-quality option and a high-price, low-quality option. In such a situation, perceived quality variance among comparative brands is made salient by the diagnosticity of product attributes. When construal level is experimentally made high, we expect to replicate Yan and Sengupta's findings (i.e., price has more impact in the high- than in the low-construal condition). However, the prediction of local–global identity can have two possible directions, depending on whether the construal-level account or our proposed quality variance account holds. The construal-level account predicts that price, being an abstract cue, will be used as a quality cue more by globals (vs. locals) because they are abstract (vs. concrete) thinkers. However, the quality-variance account suggests that the impact of price will not differ across locals and globals (as in H3a).Given that trade-offs significantly lower the diagnosticity of product features ([ 5]; [10]; [25]), when attributes contain trade-offs (e.g., low price, high in megapixel, low in optical zoom, representing a low-price, mixed-quality option), perceived quality variance among the comparative brands is not made salient (similar to the control condition in Study 3). In such a situation, if the quality-variance account holds, price should affect locals (vs. globals) more, as specified in H1. If the construal-level account holds, we predict price, being an abstract cue, to have more of an impact on globals (vs. locals), who are abstract (vs. concrete) thinkers. In addition, according to [44], quality attributes are concrete product cues (i.e., low-level construal), whereas price is an abstract cue (i.e., high-level construal). Because the manipulation of diagnosticity is only on quality (and not on price) cues, we expect diagnosticity to moderate the effect of construal level on PPQ in the low-construal-level condition, but not in the high-construal-level condition. The next study tests these predictions and rules out decision-making effort as another alternative explanation. Study 6: The Role of Construal Level Participants, Design, and ProcedureWe randomly assigned 470 college students (239 men; Mage = 26.60 years, SD = 10.88) to one of the conditions in a 4 (local identity, global identity, high-level construal, low-level construal) × 2 (diagnosticity of quality cues: high vs. low) between-subjects design. Local and global identities were manipulated as in Study 2; the manipulation check items were the same as in Study 2 (α =.88). Following [ 6], we primed construal level by asking participants to think and write about why they should improve their academic performance (high construal) or how to improve their academic performance (low construal). To check the manipulation, we used the Behavior Identification Form (BIF; [41]; see Web Appendix 2).Participants were then given a description of two cameras and asked to determine which was of higher quality. The two cameras differed in price and two other nonprice cues (megapixels and optical zoom). The diagnosticity of nonprice cues was manipulated through consistency in megapixels and optical zoom (see Web Appendix 12). In the high-diagnosticity condition, the two nonprice cues were in the same direction: the high-price ($240) camera was low in both megapixels (15 MP) and optical zoom (10×), and the low-price ($200) camera was high in both megapixels (18 MP) and optical zoom (12×). This design is consistent with [44]; Experiment 2). Because one option had a higher price but was of lower quality than the other option, the quality variance between these two options was salient, as shown by [ 5]; [10]; [25]). We used a pilot study (N = 78) to validate the manipulation of diagnosticity. Participants were randomly assigned to either the high- or low-diagnosticity condition and rated perceived quality variance using two items (α =.85): ( 1) ""The quality of cameras in the marketplace varies a lot,"" and ( 2) ""There are huge differences among cameras."" Results showed that participants in the high- (vs. low-) diagnosticity condition perceived more variance in quality of cameras (Mhigh diagnosticity = 5.61 vs. Mlow diagnosticity = 4.99; t(76) = 2.31, p <.05).Participants also completed a two-item measure of task involvement (α =.85): ( 1) ""How involved were you when judging the two cameras?"" (1 = ""Not at all,"" and 7 = ""Very much so"") and ( 2) ""How much thought did you put into the task of evaluating the two cameras?"" (1 = ""Not at all,"" and 7 = ""A lot""). We also recorded the actual time that participants spent making the choice as another measure of effort. Results and Discussion Manipulation checkAs we expected, participants in the local (vs. global) identity condition were more likely to perceive themselves as local citizens (Mlocal = 4.64 vs. Mglobal = 4.12; t(231) = 2.25, p <.05). However, participants in the high- and low-construal level conditions did not differ in this aspect (Mhigh construal = 4.30 vs. Mlow construal = 4.40; t(235) = −.41, p =.68). Those in the high-construal condition (M = 18.14) scored higher on the BIF than those in the low-construal condition (M = 15.57; t(235) = 3.66, p <.01), indicating that construal level was primed successfully. Interestingly, consistent with [32], participants in the global identity condition (M = 16.26) scored higher on the BIF than those in the local identity condition (M = 14.03; t(231) = 3.09, p <.01), suggesting that local–global identity prime indeed affects construal level. Choice of the higher-quality cameraIn the low-diagnosticity condition, consistent with our prediction that price would have more impact in the local (vs. global) identity condition, the proportion of participants who selected the high-price camera as superior was higher in the local (31.67%) versus the global (10.91%) identity condition (χ2( 1) = 7.27, p <.01). However, the proportion of participants who selected the high-price camera as superior did not differ across the high-level (28.33 %) and low-level (18.33%) construal conditions (χ2( 1) = 1.68, p =.20; Figure 5).Graph: Figure 5. The effect of local–global identity and construal level on PPQ associations (Study 6).Notes: The y-axis indicates choice of the high-price option as having better quality.In the high-diagnosticity condition, the proportion of participants who selected the high-price camera as having better quality was higher (χ2( 1) = 7.44, p <.01) in the high-level construal condition (22.41%) than in the low-level construal condition (5.08%); this is consistent with [44] finding that price has more of an impact in the high-level construal condition than in the low-level construal condition. However, the proportion of participants who selected the high-price camera as superior did not differ between the local identity condition (29.63%) and the global identity condition (26.56%; χ2( 1) =.14, p =.84).To test our prediction that when diagnosticity is high (vs. low), globals will perceive the high-price item to be of better quality (i.e., elevated PPQ), whereas locals' quality perceptions will be unaffected (H3a), we compared the choice of the high-price option in the high- (vs. low-) diagnosticity condition among locals and globals separately. The proportion of globals who selected the high-price camera as being of better quality was higher (χ2( 1) = 4.65, p <.05) in the high-diagnosticity condition (26.56%) than in the low-diagnosticity condition (10.91%). However, the proportion of locals who selected the high-price camera as being of better quality did not differ (χ2( 1) =.06, p =.84) between the high- (29.63%) and low- (31.67%) diagnosticity conditions (see Figure 5).To test our expectation that diagnosticity (high vs. low) will moderate the effect of construal level on PPQ in the low-construal level condition but not in the high-construal level condition, we conducted additional analysis across construal levels. Consistent with our expectations, in the high-level construal condition, the proportion of participants who selected the high-price cameras as having better quality did not differ across the low- (28.33%) and high- (22.40%) diagnosticity conditions (χ2( 1) =.55, p =.46), indicating that they were not affected by diagnosticity; however, in the low-level construal condition, the proportion of participants was higher in the low- (18.33%) than in the high- (5.08%) diagnosticity condition (χ2( 1) = 5.03, p =.03), suggesting that they were significantly influenced by diagnosticity of nonprice cues. Ruling out decision-making effort as an alternative explanationWe used two measures to assess the effort participants invested in the decision task: ( 1) a self-reported task involvement measure and ( 2) processing time (in seconds). Results showed that neither task involvement (Mlocal = 5.29 vs. Mglobal = 5.49; t(231) = −1.02, p =.31) nor processing time (Mlocal = 40.62 vs. Mglobal = 36.12; t(231) =.39, p =.70) differed across the identity conditions. Therefore, decision-making effort cannot explain our findings.This study provided direct evidence on the difference between local–global identity and construal level and reconciled the seemingly contradictory findings. Moreover, it ruled out effort in decision task as another alternative explanation for our findings. Next, we report a field experiment with real behavioral measures to test the external validity of the findings. Study 7: Field Study with Actual Monetary ExpendituresThe purpose of this study was to investigate a behavioral consequence of local–global identity and PPQ associations in a real choice task involving monetary expenditures. [ 3] found that consumers who make stronger PPQ associations spend more money on purchases to acquire higher-quality products. In the context of choosing a water bottle from four options at different prices, we expect that locals (vs. globals) are more likely to purchase expensive water bottles and that this effect is mediated by PPQ associations. Participants, Design, and ProcedureEighty-one U.S. consumers (33 men; Mage = 23.65 years, SD = 6.76) shopping at a local bookstore were recruited with an offer of $20 in total compensation, which could include a water bottle of their choice with the remaining amount in cash. As in Study 1, participants were given a brochure that described either a ""Think Local Movement"" or a ""Think Global Movement,"" which was used to manipulate local and global identity, respectively (Web Appendix 13).Next, participants were instructed that the study would involve consumers' evaluation of water bottles and were reminded of the compensation scheme. They were also told that if they so chose, they could receive $20 in cash and no water bottle (two consumers chose this option, one from the local identity condition and one from the global identity condition).[ 8] Thereafter, we asked participants to evaluate four different water bottles actually sold in the bookstore (priced at $4.99, $9.99, $14.99, and $19.99) and administered the four-item PPQ associations scale from [23]; adapted to assess state, rather than chronic, PPQ associations for water bottles; sample item: ""At this moment, I believe that the higher the price of a water bottle, the higher the quality""; α =.89). Participants were then asked to choose one of the four water bottles and were paid the remaining amount of $20 in cash. Finally, participants rated the three-item local–global identity manipulation check questions (α =.92) as in Study 1 (for results, see Web Appendix 3). ResultsAs we predicted, participants assigned to the local (vs. global) identity condition spent more on the water bottle (Mlocal = $14.52 vs. Mglobal = $9.43; t(77) = 4.44, p <.001) and had significantly higher PPQ associations (Mlocal = 5.12 vs. Mglobal = 4.34; t(77) = 2.28, p <.05), indicating that participants primed with local (vs. global) movements perceived a much stronger relation between the price of a water bottle and its quality; this, in turn, influenced their choice and spending behavior. Indeed, participants with a situationally activated local (vs. global) identity spent 53.98% more. Although PPQ is not a theorized mediator (which is perceived quality variance), we ran a mediation test to provide evidence that the amount spent is driven by PPQ, and not by other variables. A bootstrapping procedure with 10,000 iterations using Model 4 of PROCESS showed that the indirect effect of local–global identity on amount of money spent through PPQ associations was positive (.79) and significant (95% CI = [.12, 1.99], excluding zero), suggesting that individuals with an accessible local (vs. global) identity were willing to spend more money on purchases because of higher PPQ associations. Study 8: A Systematic Review of Previous StudiesTo enhance the generalizability of our findings, we performed a systematic review on PPQ associations documented in previous studies (for database development, coding procedures, and detailed results, see Web Appendix 14). Given that these studies were conducted in different countries, we used country-level local–global identity as an explanatory factor for PPQ. Following [ 6], we used the KOF Index of Globalization (http://globalization.kof.ethz.ch/) to capture country-level local–global identity, with a higher score reflecting a greater degree of global identity (and a lower degree of local identity).The mean standardized r across the studies in our database was.208 (95% CIBS = [.199,.218], p <.001), suggesting that, in general, consumers use price to infer brand quality. However, there was substantial heterogeneity in PPQ associations (χ2 = 2,681.54, p <.001). Thus, we conducted moderation analysis through a meta-regression using the Comprehensive Meta-Analysis 3.0 software, with standardized r as the common effect size metric, country-level Globalization Index as the independent variable, and other country-level variables (i.e., gross domestic product per capita, competitive environment, and Hofstede's five cultural dimensions [individualism–collectivism, power distance, uncertainty avoidance, masculinity, and long-term orientation]) and study-level factors (price range, product durability, study type, and publication type) as covariates.Consistent with our theorizing, results showed a negative relationship between the Globalization Index and PPQ (β = −.02, Z = −3.07, p <.01). Among the country-level variables, competitive environment was positively related to PPQ associations (β =.08, Z = 10.41, p <.001), whereas gross domestic product per capita had a negative effect (β = −.05, Z = −4.99, p <.001). Of the five cultural dimensions, only uncertainty avoidance (β = −.04, Z = −5.73, p <.001) was significantly associated with PPQ associations. Of the study-level factors, there were significant effects of product durability (β = −.09, Z = −7.11, p <.001), study type (β = −.10, Z = −7.02, p <.001), and publication type (β =.06, Z = 4.71, p <.001) but no significant effect of price range (p =.14). General DiscussionAs we show in Appendix B, all studies provide converging evidence for the effect of local–global identity on PPQ, using a variety of measures and manipulations of the key variables. In a shopping mall with real consumers, Study 1 showed that locals (vs. globals) have a greater tendency to make PPQ associations. Study 2 shed light on the mediating role of perceived quality variance. Study 3 revealed that when the quality difference among brands is made salient, globals' (but not locals') tendency to make PPQ associations is elevated, whereas when the quality difference among brands is reduced, locals' tendency to make PPQ associations is lowered, whereas globals' tendency to use PPQ is unaffected. The next two studies examined the moderating roles of product type (services vs. goods; Study 4) and online reviews (convergent vs. divergent; Study 5). Study 6 reconciled the seemingly contradictory predictions between our theory and those of construal-level theory. Study 7 reported a field experiment with real behavioral measures to prove the external validity of our findings. Study 8 presents secondary evidence, further showing how local–global identity may affect PPQ at the national level, lending additional support for external validity. Study 9 (Web Appendix 5) showed that the effect of local–global identity on PPQ is held in both multiple- and single-quality-cue conditions. Study 11 (Web Appendix 11) revealed that hedonic (vs. utilitarian) product type represents another natural moderator of the relation between local–global identity and PPQ associations. Theoretical ContributionsOur findings offer contributions to the price–quality judgments and local–global identity literature streams. Previous cross-cultural research has mainly focused on the dimensions of individualism–collectivism ([16], [17]; [20]; [37]) and power distance ([ 8]; [14]). Although the world has been moving toward globalization in recent years, we know little about how this trend may affect consumers' use of price as a signal of quality. From the limited evidence in cross-country studies ([ 4]; [42]; [46]), it is unclear whether the effect of local–global identity on price–quality judgments even exists. Our research is the first to demonstrate the existence of this effect.Furthermore, our research contributes to the local–global identity literature by identifying perceived variance among comparative objects as a new qualitative difference between these two identities. This important discovery can advance our understanding about why locals are faithful to local traditions: local identity heightens perceived differences, driving locals to focus on the uniqueness of their traditions and overlook the common elements between their traditions and those of other communities. This discovery likely has implications beyond PPQ associations, such as on categorization and brand extensions. Finally, our research also contributes to the price–quality judgments literature by identifying a novel mechanism that drives consumers to use price to judge quality—that of perceived quality variance. Because of this mechanism, situational factors that make quality variance salient or reduced—such as product type, expert opinions, or distribution of customer ratings—can change consumers' tendency to make PPQ. Managerial ImplicationsAs presented in Appendix A, managers actively consider the likelihood that consumers would use PPQ in their product evaluations and use such information in their marketing strategies. They are also aware of the role that local or global communities play in pricing decisions. However, none of our informant managers had a clear idea of when such strategies might be effective and why. This research helps address some of these questions. Our findings indicate that when promoting high-price products, marketers can situationally activate consumers' local identity, because consumers tend to use price to judge a product's quality when their local identity is salient. Communication appeals or contextual cues, such as ""Think Local"" movement (Studies 1 and 7) or T-shirt (the follow-up study to Study 1), can be used to achieve this goal. Ads or messages that feature local cultural symbols may enhance the accessibility of the local identity. TV channels that feature local traditions can be effective as well. Conversely, when promoting low-price products, marketers can activate consumers' global identity to reduce PPQ. Contextual cues (e.g., ads that feature multicultural symbols and globalization) may enhance the accessibility of global identity.Another approach to increase consumers' PPQ associations is to alter consumers' perception of dissimilarity among brands to match with a pricing strategy. For products that charge a premium price over competing products, marketers can use situational cues (e.g., expert opinion, as in Study 3; distribution of customer ratings, as in Study 5) to increase perceived quality variance and facilitate consumers' associations between price and product quality. In contrast, for products that take a low-price strategy, marketers can use these situational cues to reduce, rather than increase, perceived quality variance.Our findings on how product type (service vs. goods, hedonic vs. utilitarian products) affects customers' perceived quality variance provide insight into marketing strategies associated with services, hedonic products, and new products. Marketers of these products can capitalize on our findings by wisely allocating their ads budget: there is no need to build up price–quality associations in the minds of target consumers, because these products naturally induce perceived quality variance, which in turn leads to enhanced PPQ. Previous research has argued that consumers have more diversified views on innovations than on existing products, especially the radically new innovation with first-of-its-kind, groundbreaking technologies ([26]). Our theory suggests that consumers are prone to make PPQ associations when adopting these products.Our research is the first to show the important role that distribution of customer ratings plays in influencing consumers' PPQ. When people post similar ratings for products in a category, potential buyers may have an impression that products in that category are of similar quality. In contrast, when people's opinions are all over the place and there is lack of a dominant view, potential buyers tend to perceive high quality variance among the products in that category. Armed with this information, marketers using skimming pricing should welcome, rather than suppress, different opinions from previous users, as divergent online reviews can actually enhance consumers' PPQ. However, firms with penetration pricing may need to strive for consumers' convergent opinions, as similar customer ratings can reduce consumers' tendency to view the product's low price as an indicator of its low quality.Our findings also provide useful guidelines for firms to adapt their strategies to different regions and address the question about whether companies should be more locally or globally oriented. For products to be marketed to the places where people tend to have a salient local identity (e.g., rural areas), local flavors and ingredients can be used in the products. In addition, because these consumers are more likely to make PPQ associations, marketers may not need to allocate much ad budget to convince consumers about price–quality associations. However, when marketers enter places where people are high in global identity (e.g., metropolitan areas), they should know that consumers in these places do not have an established mental connection between price and quality. Thus, additional effort is needed to increase perceived dissimilarity among brands in the marketplace to enhance price–quality associations. Similar strategies can be used for international marketing strategies. Previous research ([ 1]; [ 6]) has shown that individuals in globalized countries are more likely to have a stronger global identity, whereas those from more localized countries tend to have a stronger local identity. Limitations and Future ResearchFirst, although treating the country-level Globalization Index as a proxy of local–global identity in Study 8 is in line with previous research ([ 6]), it may violate the conceptualization that these two identities are orthogonal. Second, this study may suffer from alternative explanations, such as product life cycle. Although this concern is alleviated by the variety of product stimuli used in our studies, we need to be cautious of Study 8's conclusions. Third, while a sacrifice mindset ([ 6]) cannot explain our moderation studies, future research should examine whether sacrifice mindset can account for the relationship between local–global identity and PPQ in domains not examined in the current manuscript. Finally, in this research we focused only on price–perceived quality. Given that price–quality judgments can also be quality–perceived price, it may be fruitful for future researchers to apply our theory to examine how quality levels affect consumers' price expectations. "
43,"How to SHIFT Consumer Behaviors to be More Sustainable: A Literature Review and Guiding Framework Highlighting the important role of marketing in encouraging sustainable consumption, the current research presents a review of the academic literature from marketing and behavioral science that examines the most effective ways to shift consumer behaviors to be more sustainable. In the process of the review, the authors develop a comprehensive framework for conceptualizing and encouraging sustainable consumer behavior change. The framework is represented by the acronym SHIFT, and it proposes that consumers are more inclined to engage in pro-environmental behaviors when the message or context leverages the following psychological factors: Social influence, Habit formation, Individual self, Feelings and cognition, and Tangibility. The authors also identify five broad challenges to encouraging sustainable behaviors and use these to develop novel theoretical propositions and directions for future research. Finally, the authors outline how practitioners aiming to encourage sustainable consumer behaviors can use this framework.KEYWORDS_SPLIT""We are jeopardizing our future by not reining in our intense but geographically and demographically uneven material consumption...By failing to adequately limit population growth, reassess the role of an economy rooted in growth, reduce greenhouse gases, incentivize renewable energy, protect habitat, restore ecosystems, curb pollution, halt defaunation, and constrain invasive alien species, humanity is not taking the urgent steps needed to safeguard our imperilled biosphere.....—World Scientists' Warning to Humanity: A Second Notice ([266])I always make the business case for sustainability. It's so compelling. Our costs are down, not up. Our products are the best they have ever been. Our people are motivated by a shared higher purpose—esprit de corps to die for. And the goodwill in the marketplace—it's just been astonishing.—Ray [90], Founder and CEO of Interface CarpetOur behaviors as individual consumers are having unprecedented impacts on our natural environment ([305]). Partly as a result of our consumption patterns, society and business are confronted with a confluence of factors—including environmental degradation, pollution, and climate change; increasing social inequity and poverty; and the growing need for renewable sources of energy—that point to a new way of doing business ([213]). In response, many companies are recognizing the need for a sustainable way of doing business, and across industries we see firms such as Interface Carpet, Unilever, Nike, and Starbucks embedding sustainability into the DNA of their brands ([138]). The current research provides a review of the literature regarding sustainable consumer behavior change and outlines a comprehensive psychological framework to guide researchers and practitioners in fostering sustainable behavior. Marketing and Sustainable Consumer BehaviorThere are many reasons why understanding facilitators of sustainable consumer behavior should be of interest to marketers. One reason is reflected in the [266] quote: marketers should be cognizant that the consumption mindset that conventional marketing encourages is a key driver of negative environmental impacts ([73]; [248]). Second, as the Ray Anderson quote suggests, businesses able to adapt to the demands of our changing world, including the urgent demand for sustainability, will be more likely to thrive in the long term and enjoy strategic benefits ([30]). A sustainable business focus has advantages such as identifying new products and markets, leveraging emerging technologies, spurring innovation, driving organizational efficiency, and motivating and retaining employees ([147]). Moreover, research suggests that socially and environmentally responsible practices have the potential to garner more positive consumer perceptions of the firm, as well as increases in profitability ([50]; [198]; [231]; [285]).Firms that are able not only to operate more sustainably but also to consider new models of business that offer and encourage sustainable consumption can potentially earn greater long-term profits ([174]). In one example, the growth of the ""sharing economy"" demonstrates the substantial environmental and economic gains possible through shifting consumers sustainably—in this case, from owning products to accessing existing products and services. Although the question of how marketing relates to sustainable consumption has historically received attention in the form of identifying the ""green consumer"" segment ([ 8]; [170]), scholars now call for work on the predictors of sustainable consumption ([173]; [213]; [214]). Rather than merely targeting the green consumer segment, marketers can expand their market for the long-term mutual benefit of the firm and the planet. Thus, as firms operate and offer products and services in a more sustainable manner, they might simultaneously wish for consumers to recognize, embrace, and reward their sustainable values and actions in ways that spur sustainable consumption and maximize the firm's sustainability and strategic business benefits.The current work is motivated by the need for a comprehensive review and framework related to the key drivers of sustainable consumer behavior change. We build on existing work that has aptly outlined the steps marketers can take to identify, foster, and evaluate sustainable behavior ([211]; [248]). Although this existing work details the social marketing concept and spotlights examples, it does not provide a comprehensive psychological framework for influencing consumer behavior change. Extant work often concentrates on a more focused set of factors that motivate sustainable behavior ([116]; [247]; [303]).[ 6] The first intended contribution of the present work, then, is to outline a comprehensive framework to help both practitioners and researchers encourage sustainable consumer behavior. On the practitioner side, access to a broader framework (including all the major factors from the literature) will allow practitioners to develop the most effective interventions. Second, the unique, process-driven focus of our framework (as opposed to the intervention focus of previous work) ensures that as technologies and societies change, practitioners can easily apply our framework to new situations. Thus, a key contribution is that we offer a comprehensive set of tools firms can use as they pursue their sustainability and strategic business goals. Third, undertaking a more complete review allowed us to delineate a broader set of challenges to sustainable consumer behavior change that can inform both practitioners and researchers. We discuss these challenges—the self–other trade-off, the long time horizon, the requirement of collective action, the problem of abstractness, and the need to replace automatic processes with controlled processes—in the theoretical contribution section. Finally, we use these challenges to sustainable consumer behavior change to introduce a set of novel theoretical propositions to guide further conceptual development and future research. Shifting Consumers to Behave SustainablyAt first glance, it might appear that the goals and assumptions of marketing are incompatible with the goals and assumptions of sustainability. Traditional marketing encourages growth, promotes an endless quest for satisfying needs and wants, and seems to view resources as ever abundant ([73]; [314]). In contrast, a sustainability focus suggests that utilized resources can be renewed by mimicking the circular flows of resources in nature, and it respects the fact that capacity of both resources and the environment are limited ([209]; [217]). We argue that, because of this apparent contradiction, marketing and sustainability are inextricably intertwined. Furthermore, we take the optimistic view that marketing and behavioral science have much to say about how we might influence consumption to be more sustainable. We review the literature and highlight ways in which consumers can be encouraged to behave more sustainably. Our review of the literature has led to the emergence of the acronym SHIFT, which reflects the importance of considering how Social influence, Habit formation, Individual self, Feelings and cognition, and Tangibility can be harnessed to encourage more sustainable consumer behaviors.The SHIFT framework can help address the ""attitude–behavior gap"" that is commonly observed in sustainability contexts. Although consumers report favorable attitudes toward pro-environmental behaviors ([323]), they often do not subsequently display sustainable actions ([18]; [111]; [172]; [360]). This discrepancy between what consumers say and do is arguably the biggest challenge for marketers, companies, public policy makers, and nonprofit organizations aiming to promote sustainable consumption ([155]; [257]).Thus, although consumer demand for sustainable options is certainly on the rise ([113])—for example, 66% of consumers (73% of millennials) worldwide report being willing to pay extra for sustainable offerings ([225])—there is room to further encourage and support sustainable consumer behaviors. We define sustainable consumer behavior as actions that result in decreases in adverse environmental impacts as well as decreased utilization of natural resources across the lifecycle of the product, behavior, or service. Although we focus on environmental sustainability, we note that, consistent with a holistic approach to sustainability ([228]), improving environmental sustainability can result in both social and economic advances ([63]; [270]). We examine the process of consumption including information search, decision making, product or behavior adoption, product usage, and disposal in ways that allow for more sustainable outcomes. Thus, sustainable consumer behaviors could include voluntarily reducing or simplifying one's consumption in the first place ([184]; [208]); choosing products with sustainable sourcing, production, and features ([193]; [253]); conserving energy, water, and products during use ([189]; [354]); and utilizing more sustainable modes of product disposal ([353]).Unlike typical consumer decision making, which classically focuses on maximizing immediate benefits for the self, sustainable choices involve longer-term benefits to other people and the natural world. Although broader marketing strategies can be useful in this domain, marketers also need a unique set of tools to promote sustainability. We endeavor to outline the key drivers of sustainable consumption with one comprehensive framework. Our review of existing literature on sustainable consumption began with an initial selection of top marketing journals: Journal of Marketing, Journal of Marketing Research, Journal of Consumer Psychology, and Journal of Consumer Research. These behavioral marketing and consumer behavior journals are the most highly regarded in the field, having high impact factors (above 3.0), and they are all featured on the Financial Times Top 50 list. Using this set of journals, we conducted a literature search using specific keywords on Web of Science. The keywords included: sustainab* or ecolog* or green or environment* or eco-friendly and consum* or behavi* or choice or usage or adopt* or disposal.This set of papers was then read and grouped into themes, which formed the five factors in the SHIFT framework. We used these five categories because they emerged in our initial review as being the most frequently occurring concepts, and because they allowed us to summarize the literature on sustainable behavior change in an inclusive manner. To extend our review, we then searched the literature more broadly by using our first set of search terms and replacing the third search word with more specific labels that were relevant to our five themes. We refined our search to include behavioral sciences, business, psychology multidisciplinary, economics, and management journals. For example, for the first section on social influence, we searched ""social influence"" and ""norms."" Our results allowed us to identify additional articles in peer-reviewed academic journals in marketing, psychology, and economics. We then read and reviewed these articles in terms of quality and relevance, which were determined through consensus among the authors before inclusion in our analysis. Our review identifies a set of 320 articles, some of which are used to frame the introduction (n = 40) and the rest represent the SHIFT factors (n = 280). Next, we discuss the five identified routes to sustainable consumer behavior change (refer to Web Appendix G for a summary of articles representing our SHIFT factors). The SHIFT Framework Social InfluenceThe first route to influencing sustainable consumer behaviors is social influence. Consumers are often impacted by the presence, behaviors, and expectations of others. Social factors are one of the most influential factors in terms of effecting sustainable consumer behavior change ([ 1]). We examine how three different facets of social influence—social norms, social identities, and social desirability—can shift consumers to be more sustainable. Social normsSocial norms, or beliefs about what is socially appropriate and approved of in a given context, can have a powerful influence on sustainable consumer behaviors ([68]; [247]). Social norms predict behaviors such as avoiding littering ([69]), composting and recycling ([238]; [353]), conserving energy ([89]; [120]; [152]; [277]), choosing sustainably sourced food ([86]), selecting eco-friendly transportation ([141]), choosing green hotels ([317]), and opting for solar panels ([44]). The Theory of Planned Behavior suggests that, along with subjective norms, attitudes and perceived behavioral control shape intentions, which predict behavior. This framework has been applied to sustainable behaviors ([136]; [143]).Cialdini and his colleagues use the term ""descriptive norm"" to refer to information about what other people are doing or commonly do ([69]; [263]). Descriptive norms can be stronger predictors of sustainable consumer behaviors than other factors such as self-interest, and people tend to underestimate how influential such norms can be ([227]). Descriptive norms are most effective when combined with reference to similar contexts ([104]). In one example, descriptive norms communicating that others were taking part in a hotel energy conservation program were more effective than a traditional environmental message, especially when the descriptive norms referred to the same hotel room as the guest's ([120]). Although descriptive norms are often very influential, if the majority of people are not engaging in the desired sustainable behavior, highlighting a descriptive norm might unintentionally lead to decreases in the desired action ([66]; [277]). One field study sheds light on an exception to this: when community organizers themselves installed (vs. did not install) solar panels on their homes (a behavior that reflects low norms), they were able to recruit 62.8% more residents to do the same ([175]).In contrast, ""injunctive norms"" convey what behaviors other people approve and disprove of. Such norms can thereby influence sustainable behaviors ([152]; [263]; [277]), but they should be used carefully ([177]). Injunctive norms are most effective when combined with thoughts about the ingroup and when they do not threaten feelings of autonomy, which can lead to ""reactance"" responses ([353]). Thus, both descriptive and injunctive norms can affect sustainable behaviors, but they should be used with care. Social identitiesThe impact of social influence depends on people's ""social identities"" or sense of identity stemming from group memberships ([315]). For example, consumers are more likely to engage in sustainable actions if ingroup members are doing so ([120]; [136]; [347]). Moreover, viewing the self as a member of a pro-environmental ingroup is a key determinant of pro-environmental choices and actions ([102]; [133]; [329]). Seeing the self as similar to a ""typical recycler"" predicts recycling intentions, over and above other factors such as attitudes, subjective norms, and perceived behavioral control ([204]).One additional implication of social identities is that individuals desire to view their ingroups positively ([258]) and do not wish to see their ingroup outperformed by other groups ([100]). This is particularly true of outgroups that the consumer does not wish to be associated with, known as ""dissociative groups."" In one example, researchers examined intentions to undertake sustainable actions such as water conservation, composting organics, and recycling ([354]). When people learned that a dissociative reference group had performed better on a positive, sustainable behavior (thus casting the ingroup in a negative light), the focal group members increased their own positive behaviors. These effects were augmented in public settings, because this is a condition under which the collective self is most relevant. One practical implication of this work is that friendly challenges could be encouraged between competing groups ([339]), such as cities, neighborhoods, organizations, or business units.Another finding stemming from the social identity literature is that social identity effects are heightened for those high in ""ingroup identification."" Identifying with being ""an organic consumer"" or ""a green consumer,"" for example, predicts organic purchases ([32]; [33]). Moreover, majority group members, as well as minority group members who are high in ingroup attachment, receive messages encouraging sustainable consumption more positively ([124]). Highlighting a shared, superordinate ingroup identity can increase acceptance of information related to sustainable actions, especially for those who are high in ingroup identification ([279]). Social desirabilityAnother means by which social influence can impact sustainable behaviors is through ""social desirability."" Consumers tend to select sustainable options to make a positive impression on others ([123]), and they endorse high-involvement sustainable options (e.g., hybrid vehicles) to convey social status to others ([126]). However, observers sometimes view sustainable behaviors negatively, leading some consumers to avoid pro-environmental actions ([49]; [216]; [232]; [269]; [287]). In one instance, males avoided appearing ""eco-friendly"" because it was associated with feminine traits ([49]). One implication, then, is to make sustainable products or behaviors socially desirable and to buffer against potential negative perceptions linked to sustainable consumption.Moreover, consumers are more likely to act in a socially desirable manner in public contexts in which other people can observe and evaluate their actions ([123]; [128]; [249]). In addition, encouraging public commitments to engage in sustainable consumer behavior can increase such actions ([52]; [121]). For example, those who committed to participate in a hotel energy conservation program and wore a pin as a public symbol of this commitment were the most likely to engage in the program ([20]). Habit FormationWhereas some sustainable behaviors (e.g., installing an efficient showerhead) require only a one-time action, many other sustainable behaviors (e.g., taking shorter showers) involve repeated actions that require new habit formation. Habits refer to behaviors that persist because they have become relatively automatic over time as a result of regularly encountered contextual cues ([178]). Because many common habits are unsustainable, habit change is a critical component of sustainable behavior change ([335]). Many behaviors with sustainability implications—such as food consumption, choice of transportation, energy and resource use, shopping, and disposal of products—are strongly habitual ([84]; [337]). Interventions that break repetition, such as discontinuity and penalties, can disrupt bad habits. Actions that encourage repetition, such as making sustainable actions easy and utilizing prompts, incentives, and feedback, can strengthen positive habits. Discontinuity to change bad habitsThe habit discontinuity hypothesis suggests that if the context in which habits arise changes in some way, it becomes difficult to carry out the usual habits that would occur. In other words, a disruption in the stable context in which automatic behaviors arise can create ideal conditions for habit change. Life changes (e.g., a recent move) make people more likely to alter their eco-friendly behaviors ([24]; [338]; [341]). Thus, combining context changes with habit formation techniques can be one way to encourage sustainable behaviors. PenaltiesPenalties are essentially types of punishment that decrease the tendency to engage in an undesirable behavior. A penalty might take the form of a tax, a fine, or a tariff on an unsustainable behavior. Fines can encourage behavior change in domains that can be monitored, such as the disposal of waste ([107]), whereas taxes and tariffs can be effective in domains that involve strong habits (e.g., driving gasoline-powered vehicles; [176]). Although penalties can certainly deter unsustainable behaviors in some instances, they can trigger backfire effects if the penalty seems unreasonable ([107]) and can lead to negative affect and defensive responses ([42]; [112]; [303]). Moreover, penalties can be difficult to enforce and monitor ([42]). Thus, it is often desirable to turn to positive behavior change strategies instead, which we discuss next. Implementation intentionsOne means of transitioning people from an old habit to a new one is to have them consider implementation intentions, or thoughts about what steps they will take to engage in the action ([178]). Such intentions can positively influence recycling ([146]) and sustainable food-purchasing habits ([99]). Then the new behavior can be encouraged through repetition and by positive habit formation techniques such as making it easy, prompts, feedback, and incentives. Making it easyMany sustainable actions are viewed as effortful, time-consuming, or difficult to carry out, which can be a barrier to sustainable actions ([210]). Thus, one strategy to encourage sustainable habit formation is to make the action easier to do (Van [331]). Contextual changes that improve the ease of engaging in sustainable behaviors, such as placing recycling bins nearby, requiring less complex sorting of recyclables, and offering showerheads with ""low-flow"" settings, encourage such behaviors ([48]; [108]; [197]). One means of making sustainable actions easier is to make them the default ([106]; [318]). In one example, when sustainable electricity was set as the default option, individuals were more likely to stick with it ([252]). Because consumers are often low on cognitive resources, simplifying the decision-making process can allow them to more automatically form sustainable habits ([303]) PromptsAnother means of encouraging sustainable habit formation is the use of prompts: messages that are given before the behavior occurs to remind the consumer what the desired sustainable behavior is ([182]). Prompts can positively affect many sustainable behaviors including waste disposal, energy usage, and recycling ([236]). Prompts to engage in sustainable behaviors work best when they are large, clear, easy to follow, and placed in proximity to where the behavior will be performed ([19]; [348]). Because prompts are easy to employ and cost-effective, they can be a good initial behavior change strategy ([278]), but they are best utilized in combination with other strategies ([76]). IncentivesRewards, discounts, gifts, and other extrinsic incentives can increase desired behaviors and positive habit formation. Monetary incentives such as rebates, tiered pricing, and cash can encourage people to adopt and maintain sustainable behaviors ([80]; [291]; [357]). Incentives have been shown to influence sustainable behaviors such as waste disposal and cleanup ([23]), energy usage ([ 2]), and transportation choices ([97]). Although incentives can encourage the adoption and maintenance of sustainable behaviors, they do have potential drawbacks ([43]). Smaller monetary rewards are often less motivating than other types of incentives such as a free gift, a lottery entry, or social praise ([137]; [150]). Second, incentives to engage in sustainable behaviors can lead to actions that are short-lived ([168]). Consumers initially respond positively to rewards, but the sustainable behavior often disappears once the incentive is removed ([53]). Thus, one-time sustainable actions are easier to encourage with incentives than are longer-term changes ([112]). Furthermore, incentives can have the unintended consequence of decreasing the desired behavior because the intrinsic motive to engage in the action is reduced ([46]). FeedbackAnother means of encouraging sustainable habit formation is to use feedback. This involves providing consumers with specific information about their own performance on a task or behavior. Feedback can be given for actions like water and energy usage, and it can be provided with reference to the consumer's own past behaviors or in comparison to the performance of other individuals ([ 3]; [103]; [320]). Research suggests that feedback is more effective when it is presented over an extended period of time, in real-time, and in a clear manner ([65]; [103]; [166]). Sharing group feedback with households and in work settings can also be an effective behavior change strategy ([75]; [275]; [277]; [290]). The Individual SelfFactors linked to the individual self can have a powerful influence on consumption behaviors. The concepts discussed in this section include positivity of the self-concept, self-interest, self-consistency, self-efficacy, and individual differences. The self-conceptIndividuals desire to maintain positive self-views and can reaffirm the positivity of the self-concept through consumption ([88]). As a result of the desire to view the self positively, people often exhibit self-defensive reactions to learning that their own behaviors have negative environmental impacts ([82]; [101]) and derogate others displaying more sustainable actions ([216]; [361]). Moreover, people display motivated biases including the tendency to seek out and reinforce information that confirms preexisting views ([346]). Furthermore, people avoid some forms of sustainable behavior change (e.g., travel behaviors) because changing can threaten the self ([221]). In one example, threats to Republican self-identity led to backfire effects such that Republicans decreased support for climate change mitigation policies in response to climate change communications ([142]) or were less likely to choose an eco-friendly option ([129]). Thus, positively associating sustainable behaviors with the self-concept and buffering against self-threatening information can be critical for sustainable behavior change. For example, self-affirmation, or the endorsement of important self-values, mitigates self-protective responses and leads to greater endorsement of sustainable actions ([49]; [256]; [297]).The self-concept also relates to sustainable behaviors in that the possessions people own can become extensions of their identity ([36]). One way this sense of extended self manifests is that people can be unwilling to part with possessions that are linked to the self because of a sense of identity loss ([358]). Winterich and her colleagues showed that this identity loss was mitigated by having the consumer take a picture of a sentimental product before considering donating, which led to increased possession donation. Giving possessions to others not only has positive sustainability implications but it can also lead to greater well-being for the giver ([85]). Finally, consumers take better care of and are less likely to trash (vs. recycle) identity-linked products ([322]). Self-consistencyIn addition to wanting to see the self in a positive light, people want to see the self as being consistent. Self-consistency research shows that a consumer reaffirming a component of the self-concept (e.g., being environmentally concerned) or engaging in a sustainable behavior at one time point often leads to consistent sustainable behaviors in the future ([330]). Similarly, initial personal commitments to act sustainably can increase the likelihood of subsequently behaving in a sustainable manner ([41]; [168]), especially when they are made in writing ([190]). Along with individual consistency, a firm adhering to green values can lead to increased consumer conservation behaviors ([343]). Furthermore, evidence suggests that people who engage in a sustainable action in one domain are often more likely to perform sustainably in other domains as well (i.e., positive spillover; [158]; [179]; [190]; [230]; [324]). Self-assessments of the consumer's behavior can also affect consistency. For example, those who felt that the end sustainability goal was unimportant were less motivated to pursue the end goal when they were unable to achieve subgoals (e.g., failing to recycle a newspaper; [77]). Moreover, cuing people that a given behavior has positive sustainability outcomes leads them to see themselves as being more environmentally concerned and to be more likely to choose eco-friendly products ([71]). Finally, simply reminding consumers of a time when their behavior was inconsistent with a personally held value related to sustainability can subsequently lead the consumer to behave in a manner consistent with those sustainable values ([81]; [249]).Although there are many examples of self-consistency effects, inconsistency effects can also arise. Licensing effects may occur wherein individuals who have engaged in a sustainable action at one time point will later be less likely to engage in another sustainable or positive behavior ([251]; [268]; [321]). For example, researchers found that people who took part in a ""green"" (vs. conventional) virtual shopping task that asked them to select from sustainable products were subsequently more likely to behave in an antisocial manner ([207]). The availability of pro-environmental technologies and resources also can lead to negative spillover effects ([292]; [296]). For example, [58] found that consumers used more resources when they knew that a recycling option was available.Moreover, both inconsistency and consistency can emerge in the same context. People who brought a reusable shopping bag to the market subsequently spent more money on both sustainable and indulgent food options ([167]). Furthermore, making a sustainable choice decreases subsequent sustainable behaviors for those low in environmental consciousness but increases these behaviors for those highly conscious of environmental issues ([110]). Consistency rather than inconsistency effects may be more likely to occur when connected to transcendent rather than self-interested values ([96]). Self-interestEconomic and evolutionary theories both suggest that appeals to self-interest can be leveraged to influence pro-environmental behaviors ([125]; [240]). One strategy is to highlight the self-benefits associated with a given sustainable product, service, or behavior ([123]; [227]). Research shows that sustainable attributes have a greater influence on consumers if self-relevant motives are fulfilled (vs. not fulfilled; [273]). Another means of appealing to consumer self-interest is to highlight self-benefits that can counteract the barriers to sustainable action ([119]; [179]). Such barriers include the belief that sustainable attributes can have negative implications for aesthetics ([194]), functional performance ([196]; [224]; [324]), effort ([155]), or affordability ([60]; [119]; [149]). Messages that appeal to self-interest are most effective in private ([123]) and when the individual self is primed in some way ([353]). Research suggests that a focus on self-interest is not always effective alone ([210]). Moreover, self-interests can crowd out pro-environmental motivations ([280]), especially when appeals include self-focused and environmentally focused reasons for acting sustainably ([91]). Self-efficacyAccording to [27], self-efficacy involves beliefs that the individual can engage in the required action and that carrying out the behavior will have the intended impact. Consumers' feelings of self-efficacy predict their sustainable attitudes as well as their tendencies to continue to enact sustainable behaviors over time ([13]; [70]; [93]; [171]; [351]). According to [245], [246]), consumers are most likely to choose sustainable options when consumer compromise is low and when there is high confidence that a particular behavior will make a difference (i.e., self-efficacy is high). Individual differencesAn important individual difference is ""personal norms"" or beliefs regarding a sense of personal obligation that are linked to one's self-standards ([25]; [153]; [282]; [307]). Individual differences in personal norms around sustainability predict sustainable behaviors, including recycling ([132]), selecting sustainable food ([356]), and being willing to pay more for sustainable options ([131]; [308]). Other research has focused on differences in environmental concern ([ 6]; [244]; [283]). Marketers can find success targeting those with strong personal norms and values around sustainability or by strengthening existing personal norms through priming ([249]; [301]; [302]; [336]). In addition, individual differences in mindfulness ([21]; [31]; [241]; [288]) as well as perceptions of feeling connected to nature ([226]) have been shown to predict environmental concern and sustainable behaviors. Furthermore, traits such as extraversion, agreeableness, conscientiousness, and environmental concern predict green buying behaviors ([105]; [199]).Finally, demographics have been shown to relate to sustainable consumption behaviors ([79]; [117]; [220]). Gender differences in which women exhibit more sustainable consumer behaviors are sometimes noted. This may occur partly because women tend to be higher in traits such as agreeableness, interdependence, and openness to experience ([83]; [90]; [195]). Other work finds that those who are younger, more liberal, and highly educated are likely to engage in pro-environmental behaviors ([118]; [122]; [267]; [284]). It makes sense to target responsive segments with sustainability appeals ([ 8]; [171]; [180]), and interventions should be tailored to reflect the specific needs and motivations, barriers, and benefits of the target consumer ([ 3]; [22]; [74]). Feelings and CognitionWe introduce the concepts of feelings and cognition together because, generally speaking, consumers take one of two different routes to action: one that is driven by affect or one that is more driven by cognition ([289]). This proposition is consistent with theories suggesting that either an intuitive, affective route or a more deliberative, cognitive route can dominate in decision making ([94]; [160], [161]). We note that this distinction is likely to be highly relevant in the domain of reacting to information about ecological issues ([206]). We first outline how negative and positive emotions can impact pro-environmental behaviors. Then we discuss the role of cognition in determining sustainable actions by considering information and learning, eco-labeling, and framing. Negative emotionsConsumers often consider the negative emotional consequences of either engaging or not engaging in sustainable behaviors (Rees, Klug, and Bamberg 2015). Generally speaking, it is important to avoid creating negative emotional states that are too intense ([172]). Instead, more subtle activation of negative emotions can be effective ([212]; [249]). We next address the impact of three specific negative emotions: fear, guilt, and sadness.Communications regarding sustainable behavior often use ""fear appeals"" that highlight the negative consequences of a given action or inaction ([29]). On the one hand, communications that leave the individual feeling as though the consequences are uncertain and temporally distant can make the situation seem less dangerous and can lead to inaction ([192]). On the other hand, using strong fear appeals can lead to a sense of being unable to overcome the threat and can result in denial ([233]). Because of this, it is best to use moderate fear appeals and to combine these with information about efficacy and what actions to take ([187]; [237]).Guilt can influence sustainable intentions and behaviors ([57]; [154]; [195]; [201]; [218]; [234]). This is largely due to the consumer assuming individual responsibility for the unsustainable outcomes ([185]), leading people to feel morally responsible for the environment ([163]). Research shows that ""anticipated guilt"" can also influence people to act in a pro-environmental manner ([127]; [162]; [200]; [299]). Anticipated guilt is more effective at encouraging sustainable behavior when consumers are subtly asked to consider their own self-standards of behavior rather than when they are exposed to explicit guilt appeals, which can backfire ([249]). ""Collective guilt"" can also be a motivator of pro-environmental action ([100]). Information conveying that one's country has a significant carbon footprint leads to a sense of collective guilt, and such feelings predict willingness to support sustainable causes and actions ([100]; [201]).In addition to fear and guilt, researchers have examined sadness as a driver of sustainable attitudes and behaviors ([286]). Sadness was shown to lead to more pro-environmental behaviors such as using an energy footprint calculator and allocating higher donation amounts to a sustainable cause ([281]). However, once the emotion dissipated, differences in sustainable actions were eliminated between those who had received the sadness message versus a non-affective message. Thus, emotions such as sadness are more influential while consumers are experiencing them. Positive emotionsConsumers are more inclined to engage in pro-environmental actions when they derive some hedonic pleasure or positive affect from the behavior ([72]). Sustainable behaviors can both decrease negative and increase positive emotions ([234]; [264]; [312]). On the one hand, engaging in sustainable actions has been shown to result in ""warm glow"" feelings that can spill over and lead to more favorable evaluations of the overall service experience ([114]). Positive emotions such as joy and pride have been shown to influence consumer intentions to decrease plastic water bottle usage, and optimism can motivate the maintenance of sustainable behaviors over time ([250]). On the other hand, research suggests that positive emotions can work to negatively impact sustainable consumer behaviors. For example, unsustainable actions such as driving gas-powered automobiles are linked to positive affective benefits ([300]).Meanwhile, feelings of ""affinity towards nature"" predict sustainable attitudes and intentions ([165]). Studies demonstrated positive sustainable actions in response to ""cute"" appeals (e.g., communications featuring cute animals), particularly when the consumer exhibits ""approach"" motivational tendencies ([342]). This is driven by increased feelings of tenderness in response to such appeals.The role of specific positive emotions such as pride in determining sustainable consumer behaviors is also relevant ([40]). Pride is a self-conscious and moral emotion stemming from a sense of responsibility for a positive outcome ([185]). Those who feel a sense of pride have been shown to be more likely to subsequently engage in sustainable behaviors, in part because pride enhances feelings of effectiveness ([ 9]). Finally, positive environmental actions can lead to feelings of hope, which can increase climate activism and sustainable behaviors ([98]; [293]). Feelings of hope can be augmented by framing climate change as a health issue as opposed to an environmental issue ([222]). Information, learning, and knowledgeOne basic means of persuading consumers to engage in eco-friendly actions is to present information that conveys information regarding desired (and undesired) behaviors and their consequences ([210]). Some have lamented that people's dearth of understanding and knowledge—due to lack of exposure to information ([115]), information overload ([148]; [223]), and confusion ([62])—can contribute to low uptake of sustainable behaviors. Moreover, intelligence ([16]), education ([117]), and knowledge ([186]) are linked to greater responsiveness to environmental appeals and engagement in eco-friendly behaviors. In many ways, knowledge is relevant across all our SHIFT factors. The consumer must have knowledge of the social norm, must be aware of and understand the prompt or feedback, and must comprehend information related to self-values, self-benefits, self-efficacy, etc.Providing information through appeals that highlight why the desired behavior or product is sustainable can be effective in giving consumers the initial knowledge they need regarding actions and consequences ([248]; [313]). Indeed, one is unlikely to engage in more deliberate forms of sustainable behavior change if one is not informed about the problem, potential positive actions, and possible consequences ([117]). Meta-analytic reviews suggest that information has a significant albeit modest influence on pro-environmental actions ([76]; [236]). However, research also reveals that interventions providing information only are often not enough to spur long-term sustainable changes ([ 2]; [236]). Because of this, combining information with other tactics can be more effective ([159]; [211]; [248]; [304]). Some work even suggests that detailed knowledge can backfire. Those with the highest levels of science literacy displayed more ideology-reinforcing bias than their counterparts, which was attributed to their science knowledge making them better able to support their own pre-existing viewpoints ([159]). Eco-labelingEco-labeling is one means of conveying information about the sustainable attributes of a product ([242]). Labels that are attention-grabbing, easily understandable, and consistent across categories can enable consumers to make better informed eco-friendly decisions ([45]; [316]; [319]). It has been suggested that eco-labels would be more effective if they were contrasted against negative labels that highlight products with environmentally harmful attributes ([45]). Eco-labeling can seem more transparent and unbiased if it is certified by a third party that validates the sustainability claims ([203]). However, it is important to note that some work suggests eco-labels do not play a strong role in predicting consumer food selections ([130]). FramingMarketers can strategically choose message framing to encourage sustainable choices ([325]). Because consumers care more about future losses than about future gains ([140]), labels on energy-efficient appliances should compare energy costs rather than savings ([51]; [215]). Furthermore, marketers can aggregate information to make a bigger impact, using lifetime (vs. annual) energy costs for appliances ([164]) and cost per 100,000 miles labeling to promote sales of efficient cars ([54]). Loss-framed information is especially effective when combined with concrete information on how to engage in the behavior. For example, loss-framed messages were most effective in improving the quantity and accuracy of residential recycling behaviors when they were combined with detailed information about how to recycle (vs. more general reasons regarding why we should recycle) ([351]). Also, framing can have differential effects on different segments of consumers. In the United States, framing a carbon price as a carbon offset (vs. a tax) has a strong effect on Republicans but has little impact on Democrats and a moderate impact on Independents ([139]). In another example, framing an appeal in terms of ""binding moral values"" (e.g., duty, authority, consistency with ingroup norms) leads to more positive recycling intentions and behaviors among Republicans, whereas appealing to ""individualizing moral values"" (e.g., fairness, empathy, individuality) leads to more positive reactions among Democrats ([169]). Notably, such matching effects in message framing are often driven by perceptions of fluency or the ease of processing and comprehending the meaning of stimuli ([169]; [351]). TangibilityOne unique facet of sustainable consumption is that eco-friendly actions and outcomes can seem abstract, vague, and distant from the self ([259]). Most sustainable consumer behaviors involve putting aside more immediate and proximal individual interests to prioritize behaviors with ill-defined consequences that are focused on others and are only realized in the future ([ 7]; [298]). Moreover, consumers are not likely to act on issues that are impalpable in nature ([125]). Pro-environmental outcomes are difficult to track and measure because changes emerge slowly over time and uncertainty surrounds problems and their solutions ([56]; [115]; [345]). Uncertainty can also emerge due to firm actions such as greenwashing ([62]). Next, we outline some solutions to the tangibility problem. Matching temporal focusWhereas sustainability is naturally future-focused, consumers are often present-focused. Moreover, when consumers judge a future environmental payoff to be distant, it becomes less desirable in the present ([140]; [339]). One solution to this mismatch is to encourage the consumer to think more abstractly and/or to focus on future benefits of the sustainable action ([259]). Those who have a greater focus on the future engage in more pro-environmental behaviors ([14]; [157]). Asking individuals to focus on future generations can reduce present-focused biases ([340]), and prompting the consideration of legacy increases sustainable choices ([362]). Communicate local and proximal impactsCommunications that relate the more immediate consequences of pro-environmental behaviors for a given city, region, or neighborhood can make environmental actions and outcomes seem more tangible and relevant ([183]; [271]). Drawing on people's attachments to a specific place ([78]; [116]), emphasizing personal experiences with climate change impacts ([345]), and using current issues such as extreme weather events can lead to more sustainability-oriented beliefs and actions ([188]). Concrete communicationsAnother way to tackle intangibility is to make sustainability issues more relevant and concrete for the self ([ 4]; [14]; [188]; [259]; [298]). This can be done by communicating the immediate impacts of environmental problems such as climate change ([243]) and outlining clear steps to make a difference ([351]). Communications can make the consequences of inaction (or action) clear by using techniques such as vivid imagery, analogies, and narratives ([206]). Encourage the desire for intangiblesA challenge for sustainable behaviors is that consumers often have a desire to own material goods. One means of moving toward more sustainable consumption is to promote dematerialization ([73]) in which consumers decrease emphasis on the possession of tangible goods. This could include consumption of experiences ([326]), digital products ([17]; [37]), or services ([191]). This is consistent with the notion that marketing is evolving to be more focused on the provision of services, intangible resources, and the cocreation of value ([332]). Trends such as the ""sharing economy,"" with its ideal of collaborative consumption of idle resources ([85]), and ""voluntary simplicity"" in which consumers simplify their lifestyles rather than focus on possessions ([64]) indicate that consumers can fulfill their needs without the possession of tangible products being a focal goal. Theoretical Implications and Directions for Future ResearchIn our literature review, we identified five routes to sustainable behavior change while delineating specific behavior change strategies within each route. The focus of the review portion of this article has been to identify what the main drivers of sustainable consumer behavior are according to existing research. In the next section, we will go further to highlight a set of theoretical propositions regarding when and why each of the routes to sustainable behavior change (i.e., the SHIFT factors) will be most relevant. We do so by outlining a set of key challenges that make sustainable consumption distinct from typical consumer behaviors: the self–other trade-off, the long time horizon, the requirement of collective action, the problem of abstractness, and the need to replace automatic with controlled processes. We examine each of these challenges to sustainable consumer behavior change through the lens of our SHIFT framework and outline key theoretical propositions and directions for future research. The Self–Other Trade-OffOur first challenge to sustainable consumer behavior is that consumers often perceive such actions as having some cost to the self, such as increased effort, increased cost, inferior quality, or inferior aesthetics ([194]). At the same time, sustainable consumer behaviors lead to positive environmental and social impacts that are external to the self ([55]). Thus, although the traditional view of consumer behavior holds that consumers will choose and use products and services in ways that satisfy their own wants and needs ([295]), views of sustainable consumer behaviors often imply putting aside wants that are relevant to the self and prioritizing and valuing entities that are outside of the self (e.g., other people, the environment, future generations, etc.).The self–other trade-off has implications for how social influence might operate in the context of encouraging sustainable consumer behaviors. Although sustainable consumption often comes at some cost to the self, we suggest that identity signaling can be a self-relevant positive repercussion that can outweigh the costs of sustainable action. This assertion is supported by work showing that consumers are more likely to select sustainable options when the setting is public or status motives are activated ([123]; [126]). A novel proposition building on this work is that product symbolism might have more impact on consumer attitudes and choices when a product is positioned on sustainable versus traditional attributes. By the term ""symbolic,"" we refer to the notion that some products are better able to convey important information about the self to others ([38]; [349]). The marketer could highlight either symbolic benefits (i.e., convey relevant information about the self to others) or functional aspects (i.e., information about satisfying practical needs) linked to a product ([39]). Because there may be fewer direct self-benefits related to a sustainable action, linking a sustainable option with symbolic benefits could be a fruitful strategy. P1:  When a given behavior or product is positioned on the basis of its symbolic attributes (vs. functional attributes), consumers may exhibit more positive attitudes and behaviors if the option is framed in terms of being sustainable versus a traditional product.Another way of overcoming the self–other trade-off is to consider the individual self ([109]). In particular, how the individual views his or her own self-concept might predict sustainable consumer behaviors. Whereas some individuals tend to have a more independent view of the self (i.e., the self is separate and distinct from others), some have a more interdependent self-construal (i.e., the self is connected with others; [205]). One possibility is that those who think of the self in terms of an interdependent self-construal (both as a measured individual difference and as a primed mindset; [350]) might be more inclined to engage in sustainable behaviors ([15]), particularly when such actions assist ingroup members ([87]). Moreover, research could examine how to activate even broader, more transcendent construals of the self that encompass not only the self and close others but also other species and the biosphere. Encouraging such transcendent self-views might effectively increase eco-friendly actions. P2:  Encouraging the self-concept to be seen as broader than the self (either interdependent or transcendent) will lead to increases in sustainable behaviors.At the same time, a specific focus on the individual self might be linked to sustainable actions in a way that overcomes uncertainty and is motivating. Giving people a sense of agency (i.e., allowing individuals to perceive themselves as the causal agents of behavioral outcomes) offers them a perception of empowerment and the ability to actually effect change. This might be done through priming of agency to motivate individuals to achieve a given sustainable goal ([328]). Because outcomes of sustainable actions are often abstract and uncertain, agency priming might be a relevant motivational tool in the domain of sustainable behavior change. Thus: P3:  Agency primes will lead to an increased tendency to engage in sustainable behaviors.Research on the individual self in prosocial contexts also highlights the potential importance of moral identity in overcoming the self–other trade-off. Moral identity refers to a cognitive schema around moral traits, goals, and values ([12]). The strength of moral identity can vary as an individual difference (e.g., moral identity centrality), and it can be activated by situational priming ([10]). Moral identity predicts altruistic and ethical behaviors ([12]), and those higher in moral identity appear to have an expansive ""circle of moral regard"" that includes entities further from the self such as outgroup members ([260]). Because of this, individuals who are high in moral identity or who have moral identity primed in some way might be more likely to endure some costs to the self to contribute to a greater good. Although research has looked at moral identity in the domain of prosocial behaviors ([261]), to our knowledge no prior work has examined whether individuals view sustainable behaviors as moral obligations that are predicted by moral identity. P4:  Both individual differences in moral identity and moral identity primes will increase sustainable consumer behaviors.The self–other trade-off is also linked to how consumers perceive the costs and benefits of sustainable consumption. The literature lacks sufficient work examining the positive consumer associations with sustainability. Although there are a number of studies on the negative associations of sustainable consumption, there are very few that explicitly examine the positive associations. For example, sustainability might be linked to positive feelings about design when it is in the context of innovative, out-of-the-box thinking. Tesla, for example, capitalizes on such associations. Furthermore, it seems likely that sustainability has positive associations with health, local and fresh food, and the outdoors and nature. Sustainable options that connect to growing trends such as healthy and vibrant living, being a ""foodie,"" and being an outdoor enthusiast might do well. Although some research shows that the concept of ""organic"" is linked to positive associations around health and even being lower in calories ([274]), more work could certainly examine implicit positive associations of sustainability in other domains as well. P5:  Sustainable options and behaviors might have unique positive associations when compared to traditional options, including being healthier, more innovative, and being linked to the outdoors and nature.The self–other trade-off highlights a heavier research emphasis on the role of ""negative self-related"" emotions such as guilt and fear. Future work might look further at the role of ""positive feeling states that are related to entities outside of the self"" in influencing sustainable consumption. For example, researchers have examined the impact of awe—a sense of wonder we feel in the presence of something vast that transcends the individual self—on prosocial behaviors more generally ([254]). However, to our knowledge no work looks at how awe impacts sustainable consumer behaviors. Extant work does show that empathy might be linked to prosocial behaviors ([334]). Although empathy is defined in different ways, it is often conceptualized as an affective state ""that stems from the apprehension of another's emotional state or condition, and that is congruent with it"" ([92], p. 91). Moreover, outwardly focused emotions such as moral elevation might also predict sustainable actions. Moral elevation refers to feelings of warmth and expansion that are linked to admiration and affection in response to seeing exemplary behavior on the part of another individual ([11]; [135]). Examining emotions like awe, empathy, and moral elevation are all directions for future research. P6:  Outwardly focused positive emotions such as awe, empathy, and moral elevation will predict positive sustainable consumer behaviors.Another possibility, linked to focusing on the self versus others, is to examine the role of aspirational social influence in sustainable consumer behavior change. Is it possible to make the sustainable option or behavior socially desirable to the self by connecting it to aspirational role models such as celebrities and athletes? Although research covers the motivational roles of both ingroup members ([120]) and dissociative outgroup others ([353]), there is a paucity of research on the impact of aspirational others on influencing sustainable consumer behaviors. One possibility is that aspirational branding could be harnessed to create positive, socially approved associations around the notion of sustainable lifestyles. Marketers could accomplish this by linking sustainable actions to aspirational others in a way that fosters a sense of desirability, luxury, and value linked to sustainable products and behaviors. P7:  Connecting sustainable products and behaviors to aspirational role models in a way that cultivates a sense of inspiration and luxury might increase sustainable behaviors. Long Time HorizonOur second challenge to sustainability involves the reality that sustainable behaviors require a long time horizon for outcomes to be realized. Invariably, asking individuals to engage in a pro-environmental behavior means that some of the consequences will be achieved only at a future point in time ([ 7]). As we have seen, consumers view payoffs to be less desirable the further off the payoffs are in the future ([140]). Relative to sustainable behaviors, most traditional consumer behaviors have consequences that are more immediate. Many payoffs linked to sustainability are so far off in the future that they will not even be observed in the consumer's own lifetime. We call this challenge the ""long time horizon.""The notion of the long time horizon is related to the individual self in that it is linked to self-control. Indeed, self-regulation research demonstrates that people have a difficult time regulating the self to forgo benefits in the present for longer-term payoffs in the future ([35]; [219]). Sustainable behaviors present a unique self-regulation dilemma. Whereas most self-regulatory acts involve holding off on some positive reward now in order to receive a later payoff that reflects a self-relevant goal (e.g., not eating ice cream in the present so one can fit into a favorite dress on an upcoming vacation), sustainable behaviors involve putting off something positive now for a future positive outcome that is not only temporally distant but broader than the self (e.g., not purchasing a sporty car to reduce carbon emissions, the effects of which will only be realized in the future and will benefit the environment and other people). Although one would think that the self-control literature has much to say about sustainable behavior change, little work has explicitly looked at the role of self-regulation in determining sustainable actions. Existing work shows that those who have their regulatory resources depleted are more susceptible to temptations and impulse buying ([34]). Given that many sustainable behaviors require an effortful cost to the self in the short term for an uncertain future payoff, examining the dynamics of self-control in this domain could be productive. It is possible that sustainable behaviors require even more self-control than other self-control behaviors. For example, the same action (e.g., being vegan) could be positioned in terms of sustainability versus health goals, and it may be that self-regulation is more likely to fail for sustainability reasons given that such behaviors have fewer clear future implications for the self. Research might examine this and consider how to enhance self-regulation in the sustainability domain. One idea involves interventions to make the natural world part of the extended self, thereby transforming future environmental benefits into self-benefits, which could improve self-regulation. P8:  Those whose regulatory resources are somehow limited will be more likely to lapse in terms of engaging in sustainable behaviors (vs. other types self-control behaviors).The long time horizon associated with sustainable behavior is related to feelings in that people often have to undergo hedonic costs to the self in the present to maximize some positive sustainable outcome in the future. Needless to say, this is often difficult, as people are usually hesitant to give up their own affective benefits. However, acting in a manner that helps others has been shown to provide positive affect, which is sometimes termed the ""warm glow"" effect ([114]). Focusing on how sustainable behaviors can create positive affect in the present might increase sustainable behaviors. We propose that: P9:  Sustainable behaviors that provide greater immediate (vs. long-term) warm glow feelings or positive affect will lead to decreased perceptions of the long time horizon and increase the likelihood of sustainable actions.The long time horizon is linked to tangibility as well. Although people generally care less about future outcomes, the degree to which they care varies across individuals. People with higher ""discount rates"" care less about future outcomes ([140]). Likewise, people with lower consideration of future consequences ([309]) express weaker pro-environmental intentions ([156]). Therefore, tangibility interventions (such as communicating local and proximal impacts) may be especially effective for these individuals. In contrast, those with low discount rates and high consideration of future consequences are already attuned to future outcomes and may be less influenced by tangibility interventions. Thus: P10:  Individuals with higher discount rates and low consideration of future consequences might be more sensitive to heightening the tangibility of environmental outcomes.In addition, the long time horizon and self–other trade-off are both linked to how tangibility could play a role in determining sustainable consumer behaviors. Environmental impacts are not likely to be observed until the future, most likely by future generations. As such, interventions that increase the tangibility of the effects of acting (or not acting) sustainably on future generations might encourage more sustainable actions. One possibility involves perspective-taking interventions ([202]) that encourage the consumer to adopt the viewpoint of future generations. Thus, we propose that: P11:  Individuals will be more motivated to engage in sustainable consumer behaviors when they either dispositionally or situationally take the perspective of future generations.A final implication of the long time horizon is linked to all of the SHIFT factors. One striking facet of the current review is that most of the existing research involves surveys or experiments that take place at a single point in time ([151]). Future research could profitably examine the longitudinal effects of different interventions on sustainable behaviors. Moreover, a dichotomy that our framework highlights is the short-term versus long-term focus of the different behavior change strategies. Although some of the constructs are driven by the immediate context and lead to short-term behavior change, other constructs lead to more enduring behavior change over the long term. For example, although tools related to feelings and cognition and habit-formation tools that focus on in-the-moment behavior shaping can be effective in the current context, sustainable actions can disappear once they are removed. It may be optimal to ensure a balance of in-the-moment behavior-shaping tools (e.g., incentives, penalties, making it easy) with ways of making these behaviors last over time (e.g., relating the actions to the consumer's morals, values, self-concept, self-consistency). Future research could test this possibility. P12:  Sustainable consumer behaviors may be best promoted over the long term by using a combination of in-the-moment tools and lasting-change tools. The Challenge of Collective ActionSustainable behaviors often require collective as opposed to individual action ([26]). A large group of people must undertake sustainable behaviors for the benefits to be fully realized. This differs from traditional consumer behaviors in which the outcome is realized if the individual engages in the action alone. This is also distinct from other behaviors with a long time horizon like health promotion behaviors (e.g., exercising and eating healthy) because these can be enacted at the individual level with observable results.The ""challenge of collective action"" is relevant to how social influence might operate when considering sustainable (vs. conventional) actions. When people observe others engaging in an action, this may increase perceptions of collective efficacy or ""a group's shared belief in its conjoint capabilities to organize and execute the courses of action required to produce given levels of attainments"" ([28], p. 477). Although collective efficacy has received little attention in the sustainability domain, researchers have examined it in the contexts of organizational leadership ([61]) and political action ([333]). Drawing on this work, we suggest that collective efficacy can be a compelling motivator of sustainable consumer behavior. In fact, because sustainable outcomes require that actions be undertaken on a very large scale, it may be that collective action is more motivational in the domain of sustainability than other positive behavior domains. This is an open question for future research to examine. Thus: P13:  Messages communicating both the behaviors of others (collective action) and collective efficacy will increase the tendency to engage in sustainable actions.The consideration of feelings has potential implications for how to overcome the challenge of collective action. Although some research has looked at the role of collective emotions (i.e., feelings that group members widely share as group-level goals are pursued or thwarted; [311]), the types of emotions studied in this domain have been limited to past group actions resulting in guilt or pride ([ 9]; [40]). Meanwhile, sustainable actions might be better fostered using other types of collective emotions. For example, collective feelings of anger and hope have been shown to predict collective action ([359]). Thus, we propose: P14:  Collective, future-oriented emotions such as anger and hope might foster sustainable consumer behaviors.In a similar vein, cognitions about collective actions might also facilitate sustainable behaviors. Because sustainable behaviors have the unique property of requiring collective action, one possibility is that communicating collective-level outcomes such as climate justice could be influential in encouraging such behaviors. Although thoughts about perceived ability to restore justice have been shown to lead to actions such as selecting fair-trade products ([352]), it might be the case that conveying collective notions of justice (e.g., communicating information about collective impacts and consequences of unjust, unsustainable actions) would be impactful in the domain of encouraging sustainable consumer behaviors. In particular, communication about inequitable distributions of negative environmental threats and how these are felt by communities that are the most vulnerable might be a compelling message ([181]). P15:  Communicating information about climate justice might motivate sustainable consumer behavior change.Collective action is also linked to tangibility. Anecdotally, a popular technique for motivating green behavior is to advertise the collective impact. For example, ""If everyone in the United States washed their clothes with cold water instead of hot, we would save around 30 million tons of CO2 per year"" ([294]). Despite the popularity of this type of messaging to promote green behavior in an applied context, to the best of our knowledge it has not been tested in the academic literature. We predict that this type of messaging has differential impacts for tangible versus intangible outcomes due to two opposing forces. On the one hand, collective impact framing highlights the collective action problem (e.g., ""There's no way everyone in the U.S. would do this!""), which might decrease sustainable action. On the other hand, it scales up the perceived size of the impact, which could increase sustainable behavior ([54]). Because people are often insensitive to large numeric changes in environmental outcomes ([272]), such that ""3 million"" tons of CO2 would be treated the same as ""300 million,"" it may be more effective to use tangible representations featuring visual images and analogies (e.g., ""a garbage heap the size of the Empire State Building""). P16:  Tangible (vs. intangible) collective impact framing increases pro-environmental behavior. The Need to Replace Automatic with Controlled ProcessesWe note that many unsustainable behaviors have become learned in ways that make them automatic rather than controlled in nature. Engaging in sustainable consumption thus often means (at least initially) replacing relatively automatic behavioral responses with more effortful new responses (e.g., carrying one's own shopping bag). This challenge can be related to habit formation. Recall that one means of influencing habitual change is by leveraging discontinuity, or the notion that major life change events can allow for other forms of habit change to occur. It is also possible that a certain mindset (beyond rare major life changes) can lead to habit change ([255]). Individuals who have a ""fresh start"" mindset exhibit more positive attitudes toward products that allow for a fresh start, and they hold more positive intentions to donate to charities focused on giving recipients a new beginning ([255]). The authors define a fresh start mindset as ""a belief that people can make a new start, get a new beginning, and chart a new course in life, regardless of their past or present circumstances"" (p. 22), and they show that it can be both measured and manipulated. A fresh start mindset might be applicable in terms of habit formation. Taking a ""fresh start"" view of a new behavior might serve as a form of discontinuity that makes habit change more likely. P17:  Those who have a fresh start mindset (measured or manipulated) will be more inclined to change to sustainable consumer behavior habits.Although the adoption of sustainable behavior often requires overriding an automatic habit with a controlled one, this process may be facilitated by tangibility. Because tangible outcomes are more vivid and immediate, they may provoke more experiential (rather than analytic) processing ([59]), leading people to base their decisions more on emotions and heuristics. Therefore, tangibility may increase the effectiveness of heuristic-based interventions (such as defaults or framing) and decrease the effectiveness of calculation-based interventions (such as attribute scaling; [54]). For example, when buying a car online, representing the fuel efficiency as cost per 100,000 miles may be more effective, whereas when buying a car in person, a personal anecdote from the salesman about rarely needing to fill up the tank might be more effective. Thus, we propose: P18:  Tangibility interventions shift people from analytic to experiential processing and will therefore moderate the effectiveness of other interventions. The Problem of AbstractnessOur last challenge to encouraging sustainable consumer behaviors is that such actions are often characterized as being abstract, uncertain, and difficult for the consumer to grasp ([259]). Furthermore, the consequences of sustainable actions can involve uncertain and fuzzy outcomes ([345]). Although distant future outcomes are often abstract, immediate and local environmental outcomes are also frequently abstract (e.g., energy efficiency, air quality, biodiversity). Although traditional consumer behaviors can carry different elements of risk and uncertainty, the outcomes of choices in traditional consumer contexts are usually more clear and certain than they are in sustainable consumer contexts.The problem of abstractness can be addressed by considering social influence. One reason why people are influenced by social factors is because we often look to the expectations and behaviors of others when the situation is uncertain ([67]). There is evidence, for example, that unfamiliar behaviors are more likely to be influenced by norms than are more familiar behaviors ([349]). Thus, when the sustainable consumer behavior is in some way ambiguous (e.g., ""Exactly what is the most sustainable option for baby diapers?"") or uncertain (e.g., ""Will engaging in this behavior really have the desired impact?""), people may be more influenced by social factors. Those who are high in the individual differences of uncertainty avoidance ([145]) might be more influenced by social factors when abstractness is high. Thus: P19:  When the sustainable action or the outcome is ambiguous, uncertain, or new in some way (vs. being clear, certain, and well-established), social factors such as the presence of, behaviors of, and/or expectations of others will be more influential in determining behavior. This might be pronounced among those high in uncertainty avoidance.Habit formation can also be relevant in tackling the problem of abstractness. Climate change and other issues are serious, nebulous, and can have large-scale consequences, making the acts carried out by individuals seem small and inconsequential. This can lead to green fatigue, or demotivation that is the result of information overload and lack of hope for meaningful change ([310]), and such hopelessness can be demotivating to consumers ([134]). One solution may be to celebrate small and concrete wins that can positively reinforce further sustainable actions and keep consumers engaged. P20:  Rewarding small milestones will encourage consumers to continue engaging in environmentally friendly behaviors and help avoid green fatigue.The problem of abstractness also relates to the individual self. In fact, one way to combat the problem of abstract and uncertain outcomes might be to directly consider how they could impact the individual self. As we have seen, making sustainable impacts and outcomes seem local and relevant to the self can encourage sustainable consumer behaviors. However, future research might consider other means of connecting sustainable outcomes more clearly to the self. For example, [144] manipulated a focus on the future self by showing people a digital image of what their future self might look like. These researchers found that increasing connectedness to the future self increases willingness to invest in retirement savings ([144]). It is possible that manipulations that create a connection between the current and future self will lead to increases in sustainable consumer behaviors. P21:  Those consumers who are encouraged to focus on the future self will be more likely to engage in sustainable consumer behaviors.Sustainable behaviors can also be made to feel less abstract by making the current emotional benefits and costs more concrete. Future work might examine which different communication modes are most appropriate for making individuals feel emotions linked to sustainable behaviors. Images are known to activate emotions more readily in contexts such as communicating about intergroup conflicts ([47]). Visual information may best communicate how environmental issues will affect others in order to elicit concrete emotions, and these communications may potentially have an enhanced effect on those who are visualizers ([265]). P22:  Visual communications (vs. text) will be effective at eliciting other-focused emotions such as love and empathy and lead to greater participation in sustainable actions. This effect will be enhanced for individuals who are visualizers.The problem of abstractness can be related to feelings. Allowing consumers to understand the impact of their actions might help facilitate relevant emotions and reduce perceived abstractness. In the domain of charitable giving, highlighting the impact has been shown to lead to greater emotional rewards attached to the behavior ([ 5]). Previous work, however, has not looked at the specific emotions tied to impact in sustainable consumer behaviors. For example, making the potential impact clear and concrete may be more likely to lead to anticipatory pride (vs. other anticipatory states) linked to the sustainable action. P23:  Making the positive impact of sustainable behavior more certain in the present will result in greater pride and lead to greater likelihood of carrying out such behaviors in the future.Feelings might also be linked to the problem of abstractness in another way. The ubiquity of social media and sharing exposes consumers to others who might communicate their actions linked to sustainability. For instance, people may share pictures of their commute by bike or by carpool, along with how they are feeling during the journey. Experiencing positive emotions leads to greater feelings of closeness (Van [327]; [344]), and we tend to feel greater empathy for and thus more strongly experience the emotions of close others ([95]). Thus, close others sharing their emotions involved in carrying out sustainable behaviors should be more effective at reducing abstractness by increasing the strength of the emotions we expect to feel when we engage in the behavior. P24:  Social distance will lead to emotional contagion when emotional responses to sustainable behaviors are shared with others, such that close (vs. distant) others sharing how they experience positive emotions when carrying out sustainable behavior will make the benefits of the behavior seem more concrete.Finally, the problem of abstractness is linked to tangibility. One possible way to increase tangibility of actions and outcomes (and to make information less abstract) is to employ analogies. Because sustainability is an abstract and intangible concept, comparing a sustainable action or outcome to a familiar experience or example unrelated to sustainability might facilitate greater connection between the consumer and the concept of sustainability. Thus, future work might examine the following: P25:  When the action or behavior is sustainable (vs. traditional), analogies will be more likely to encourage consumer behavior change. How to Use the SHIFT Framework in PracticeOur SHIFT framework shows different tactics that can be used to influence sustainable consumer behaviors (see Web Appendix A). We note that no single route to behavior change identified by the framework works ""best."" Rather, we suggest that practitioners should understand the specific behavior, the context in which the behavior will occur, the intended target of the intervention, and the barriers (and benefits) associated with the behavior (see Web Appendix B; for more detailed information on how to think about the relevant factors to encourage behavior change, see [210]; [246]; [276]; [279]). We note that there are often multiple barriers to sustainable behavior change, and therefore combining strategies can be impactful ([239]; [306]).Although our framework highlights the different drivers of sustainable behavior change, it can also be used to think about potential barriers to sustainable action. In particular, one way to use the framework is to consider the primary and secondary barriers to engaging in the desired behavior and then select relevant tactics to overcome them. A primary barrier is the barrier that elicits the strongest avoidance response in the target consumer, and a secondary barrier is the factor that elicits the next strongest avoidance response. Thinking about barriers in terms of the SHIFT factors (e.g., a barrier can be linked to social influence [the sustainable action is seen as socially undesirable] and habit [the existing unsustainable action is highly habitual]) can help the practitioner draw connections to the tools within the framework that might facilitate change. We provide examples of possible focal consumer behaviors in Web Appendix C, and Web Appendix D shows potential strategies that can be drawn from our framework based on the primary and secondary barriers to action.In one example of identifying primary and secondary barriers that explicitly relied on the SHIFT framework, [353] gathered data on the motives of residents who were hesitant to engage in grasscycling (i.e., composting grass clippings by allowing them to decompose naturally). The researchers discovered that residents' hesitance was due to barriers related to social norms (primary barrier: the norm was that nobody was engaging in the behavior and that it did not seem approved of) and individual factors (secondary barrier: the behavior was perceived to be costly to the self). The authors developed and tested two different solutions that addressed the key barriers by using strategies related to social norms and the individual self. These researchers created messages that were delivered to residents on door hangers, and they tracked residential grasscycling practices over time (both before and after the intervention). First, when the individual was prompted to think of the collective self (""Think about how we as a community can make a difference""), descriptive norms (""Your neighbors are grasscycling—you can too"") and injunctive norms (""Your neighbors want you to grasscycle"") were most effective. Second, when the person was prompted to think about the individual self (""Think about how you as an individual can make a difference""), highlighting relevant self-benefits worked best (""Grasscycling improves your lawn quality""). By tackling the key barriers linked to social influence and the individual self, the authors increased sustainable behaviors in a large-scale field study.Another example involves Our Horizon, which is a nonprofit with a mandate to discourage gasoline consumption caused by driving automobiles. Two focal barriers to decreasing gasoline usage are social factors (it is both socially normative and socially desirable to drive) and tangibility (consumers report uncertainty about the impacts of driving less). Our Horizon has responded by developing a strategy to target both social norms and tangibility. Our Horizon encourages local governments to implement warning labels on gas pumps similarly to the way many nations now place warning labels on tobacco packaging. The labels that the organization plans to implement serve to both ( 1) help communicate what is normatively approved of and ( 2) describe concrete and personally relevant local impacts (see Web Appendix F). Although we offer examples to illustrate the SHIFT principles in practice, it is important to recognize that different behaviors and segments will have unique barriers and benefits to behavior change. We include more examples of using barriers to identify tactics based on our Framework in Web Appendix E.As we have seen, thinking about the primary and secondary barriers to pro-environmental behavior change is one means by which marketers, policy makers, and nonprofits can use the SHIFT framework. However, there is one important nuance: the practitioner should make sure that the tools employed are complementary rather than oppositional to each another. For example, in the grasscycling study described previously, messaging that reflected the individual self along with social norms was less effective than communicating about the individual self and self-benefits (or the collective self and social norms), because consistent messaging leads to goal-compatible outcomes ([353]). In another example, highlighting the extrinsic benefits of engaging in a sustainable action along with intrinsic benefits can be less impactful than communicating intrinsic benefits alone, because extrinsic motives are not compatible with intrinsic motives ([42]; [91]). Concluding ThoughtsA question with practical and theoretical significance is whether our framework can be applied to other behaviors, such as prosocial actions or health behaviors, or if the factors are unique to sustainable behaviors. We conjecture that many of the facets of our framework may apply to the other positive behaviors as well. However, we note that there are some elements that may be unique to sustainable consumption. For example, health behaviors are not subject to the challenge of collective versus individual action to the same degree that sustainable behaviors are. Although health behavior changes can collectively have positive economic and societal benefits ([355]), health behavior change also undeniably primarily has individual benefits ([229]). Although health and prosocial behaviors (e.g., charitable giving) both carry problems of tangibility, sustainable behaviors and outcomes are likely perceived as being even less tangible than health and prosocial behaviors. This is an open question for future research to explore, and applying the framework in other domains certainly has theoretical and practical potential.In summary, we have reviewed and categorized the behavioral science literature, uncovering five broad psychological routes to encouraging sustainable consumer behavior change: Social influence, Habit formation, the Individual self, Feelings and cognition, and Tangibility. We anticipate that this SHIFT framework will be helpful in guiding practitioners interested in fostering sustainable consumer behavior. Moreover, we expect that this framework will assist researchers in conceptualizing different means of influencing sustainable consumer behavior and will spur further research in this essential domain. At the end of the day, we hope that our framework will help stimulate sustainable consumer behavior change and allow firms wishing to operate in a sustainable manner to do so in ways that can maximize both their sustainability and strategic business goals. "
44,"Immediate Responses of Online Brand Search and Price Search to TV Ads This study aims to deepen the understanding of evaluating TV ad spots by their immediate effects on important online activities. The authors merged minute-by-minute brand search and price search data with spot-level TV advertisement data for the three leading pickup truck brands in the United States over an 11-month period. They presented a generalizable modeling framework and used it to estimate the size and variation of immediate online responses to TV ads. The average elasticity of brand search to a brand's own national ads is.09, and the average elasticity of price search to a brand's own national ads is.03. Given ad audience size, immediate search responses vary with ad creative characteristics, audience category interest, slot of the break, program genre, and time factors. Overall, the results show that ordinary TV ads lead to a variety of immediate online responses and that advertisers can use these signals to enrich their media planning and campaign evaluations.KEYWORDS_SPLITAlthough TV ad spend in the United States was surpassed by digital in 2016, TV remains an important medium, accounting for approximately 37% of total ad spend ([ 8]). In 2020, advertisers in the United States are projected to spend $70 billion on TV advertising ([10]). Digital advertising has overtaken TV and other offline media for many reasons; for example, the perceived ease with which digital advertisers can quantify the relative effectiveness of different ad insertions on the basis of behavioral responses such as click-throughs and conversions. Such a capability allows digital advertisers to have greater confidence in their selections of ad creative and media placements.By contrast, with the exception of informercials and other direct-response-oriented ads, most traditional TV advertisers have not relied on behavioral response measures to determine the relative effectiveness of different ad copy or media placements. Instead, in evaluating ad creative, TV advertisers have relied on either ""gut feel"" or attitudinal measures collected through focus groups or sample surveys. In planning media placements, TV advertisers have long relied on program ratings and basic audience demographics such as age and gender.Meanwhile, consumers' self-reported television usage has not fallen: it was reported at 2.77 hours per weekday per person in both 2013 and 2017 ([ 1], [ 2]). So-called ""second screening"" behaviors, particularly during commercial breaks, have rapidly become pervasive, with 178 million Americans regularly using a second-screen device while watching TV ([ 9]). Ready access to a second screen empowers TV viewers to take immediate actions after seeing an ad, such as searching for product reviews, attributes, or prices; expressing opinions on social media; or placing an order on the advertiser's website. Given rampant ad blocking, ad fraud, and nontransparency in digital advertising markets, advertisers that aim to influence online actions may wish to continue advertising in offline media such as television. They may further wish to use detailed online response data to help refine media plans and campaign evaluations.Both practitioners (e.g., [29]; [43], [44]) and researchers (e.g., [17]; [30]) have recognized that TV ads can cause immediate—within minutes—post-ad spikes in various online activities (e.g., searches or app downloads for the advertised brand, visits to the advertiser's website). The prevalence and immediacy of such ad-driven online responses raise a tantalizing question: Can TV advertisers use post-ad spikes in online activities to assess the relative effectiveness of different ad spots? If the answer is affirmative, it would have the potential to dramatically improve how TV ad copy and media placements are chosen, which would ultimately lead to enhanced cost-effectiveness of TV as an advertising medium.Indeed, recognizing such potential, many attribution vendors have introduced services that promise to link spikes in online activities to the individual TV ads that caused them, helping advertisers select ad copy and media placements to maximize immediate online response. We are aware of more than a dozen vendors that offer such services. For example, Google Analytics 360 TV Attribution pairs minute-level ad airing data with search and website traffic data and uses a machine learning algorithm to ""accurately attribute digital activity to your TV ad spots...to help you make smart choices about your advertising investment.""[ 5] Similarly, Neustar MarketShare's TV attribution application, in collaboration with comScore Rentrak, measures the impact of TV ad spots on website visits and inbound calls, which ""shows why your ads work. Or don't. (Was it the network? Creative? Timing? A combination of those?)"" TVSquared ADvantage claims to have helped more than 700 brands, agencies, and networks improve TV campaign effectiveness by tracking ""how TV drives response via phone, app, mobile, web and SMS,"" thereby allowing advertisers to ""understand spot-level and campaign-wide performance by day, daypart, network, genre, program, creative and audience.""[ 6] In 2018, Adobe Advertising Cloud TV, a leading platform for programmatic TV (""automated, data-driven planning and buying of television advertising""; [39]), launched a partnership with TVSquared ADvantage to allow advertisers to optimize national, cable, and local TV buys on the basis of spot-level online response.[ 7]In addition to advertising attribution vendors, many start-ups and other digital-first marketers have developed similar capabilities in-house. Private conversations with practitioners indicate that many are using these practices to mechanically refine their TV ad creative and media schedules without a deeper understanding of the TV-to-online spillovers. It is against this backdrop that we conducted the current study, with the following intended contributions.Methodologically, all the TV attribution vendors have kept the core algorithm behind their proprietary solution a trade secret, providing little detail to the public for an independent and impartial evaluation. Getting attribution right is notoriously difficult, even more so for mass media such as television. It is one matter to show that TV ads can cause statistically significant immediate post-ad spikes in online activities; it is another to measure spot-level responses with such precision that one can quantify the relative performances of different ad creative and media placements. We see many challenges that need to be adequately addressed:Establishing a proper baseline at the minute level. The baseline must be flexible enough to account for complex trend and seasonality (e.g., minute-of-hour, hour-of-day, day-of-week patterns). In addition, many other factors can influence the baseline and thus cause potential misattribution. For example, a portion of the post-ad spike in online activities could have been caused by the absence of programming content during a commercial break, as opposed to the presence of a particular ad spot. Alternatively, many correlated observables and unobservables can influence both the focal advertiser's ads and online activities. For example, a competitor's media schedule could be correlated with the focal advertiser's, and competitive ad spots could have an immediate spillover on the focal advertiser's online metrics.Separating signal from noise. Many online activities are inherently very noisy, and many TV ad insertions may produce subtle signals, especially when the number of impressions generated by a TV spot is small or the response rate per impression is low.Assigning attribution across overlapping spots. It is not uncommon that multiple spots of the same TV advertiser may be aired on different networks at approximately the same time—in the same minute or with overlapping durations (e.g., ad insertions by a heavy prime-time advertiser or during a blitz campaign). One therefore must be able to assign attribution across overlapping spots in a logically coherent manner.Accounting for a multitude of moderating factors. The amount of immediate online response to an ad spot is determined by the number of ad impressions and the response rate per impression. The former requires a reliable measure of ad audience size (as opposed to just the program rating or ad spend). The latter can be a function of ad creative characteristics, media placement, and audience composition. Because these moderating factors can be correlated with one another, one needs to account for them simultaneously to minimize omitted variable biases.We aim to develop a rigorous, yet practical, approach to addressing these challenges. We propose a modeling framework that links ad insertions to minute-by-minute online metrics and illustrate it with ""real-world"" size data compiled from multiple sources. We intend to provide practitioners and researchers alike a transparent and replicable tool for TV attribution based on immediate online response. Advertisers, agencies, and networks can use our method as a benchmark in evaluating the proprietary solutions offered by attribution vendors.In addition to making a methodological contribution, we intend to make a substantive contribution by answering the following empirical questions, which can potentially serve as reference points for future research in this area: What is the typical rate of immediate online response to a regular TV ad spot? How does the elasticity compare with those reported in prior studies? How long does the immediate response last? How does the response rate vary by minute after an ad insertion? Does it peak in the minute the ad is shown and then decay exponentially, or does it peak in the minute after the ad is aired and then fade away gradually? How is the response rate similar or different for brands from the same product category? Is there immediate online response to competitors' TV ads? Are competitive spillovers positive or negative? How do spillovers between brands compare with own-brand effects? Are spillovers asymmetric between leader and follower brands? How is the immediate online response affected by ad creative quality? All else being equal, is there more immediate online response to ads that are deemed as more informative? What about ads that are rated as more likable, or ads that make the advertised product more desirable? How do media placement factors affect the rate of immediate online response to an ad spot? All else being equal, how much higher is the response rate for the first slot in a commercial break? What about prime time versus non–prime time, broadcast versus cable, live sports versus other programs, weekend versus weekdays? How does audience category interest affect the rate of immediate online response? How may answers to the previous questions vary depending on the nature of the online activity in question (e.g., brand search vs. price search)?As the empirical context for our study, we focus on three top pickup truck brands in the U.S.—Ford F-Series, Chevy Silverado, and Ram Trucks—for four key reasons. First, car shoppers engage in various online activities before making a purchase, with a purchase funnel that can last for weeks or months ([19]). Car shoppers are exposed to numerous ads and promotions from a myriad of online and offline sources. These exposures make it nearly impossible to quantify the impact of any regular TV ad spot on sales or brand attitudes ([ 7]). This, in turn, makes the automotive industry highly relevant for testing the potential of refining TV media planning and campaign evaluation by using immediate online responses to TV ads.Second, these three pickup truck brands represent a set of well-defined direct competitors, allowing us to compare and contrast effect estimates to identify similarities and differences as well as to quantify the direction and degree of competitive spillovers. According to Motor Intelligence, Ford F-Series had a market share of about 31% in 2016, followed by Chevy Silverado at 22% and Ram Trucks at 18%, continuing a 33-year trend of stable market share rankings in a $40 billion category ([41]).Third, these three brands offer a fertile ground for investigating how various ad creative–, media placement–, and audience-related factors may moderate the rate of immediate online response to TV ads. During the period under study (a span of 493,920 minutes), the three brands ran 27,562 ad spots on national TV, deploying 169 distinct pieces of creative and spanning a wide range of dayparts, pod positions, broadcast and cable networks, and program genres. This allows us to quantify immediate online response to TV ad insertions under a wide variety of conditions. Furthermore, we have access to ad audience data and a measure of audience interest in the pickup truck category for each national spot. This allows us to separate, for the first time in research in this area, the effects of ad creative and media placements from those of audience characteristics.Fourth, these three brands present a conservative test of our modeling framework for quantifying the immediate online response attributable to individual ad spots. Ford F-Series, Chevy Silverado, and Ram Trucks are all mature brands that are well known to U.S. consumers (in contrast to newer or lesser known brands, for which ad viewers may exhibit a stronger tendency to respond immediately by searching online). Furthermore, many prior studies in this area have examined ad spots in ""must-see"" TV programs that had tens of millions of viewers (e.g., the Super Bowl, the Olympic Games). The average and median audience per spot for the ""ordinary"" national TV ads included in our study are.5 million and.2 million, respectively. In addition, no prior study has examined local TV ads, which tend to have a much smaller audience per spot, presumably causing a much smaller and thus harder-to-detect post-ad spike in online activities. Unlike national spots, we do not directly observe the audience size for each local spot. As a result, we use the spend estimate of each local spot as a proxy. In our study, the three pickup truck brands had in total 750,672 local ad insertions, with an average (median) spend of $348 ($159) per spot. The upshot is that the empirical context of our study enables us to test whether our modeling framework is sensitive and reliable enough to quantify immediate online response to ordinary national and local TV ads, thus making our findings more generalizable to everyday circumstances encountered by the majority of TV advertisers.Before proceeding, it is important to acknowledge that while it is useful and insightful to model how and when TV ad spots can drive immediate brand and price searches, such midfunnel performance metrics are only part of a bigger picture because advertisers are ultimately interested in driving bottom-line performance metrics such as sales. Although it is beyond the scope of the current study, more research is needed in linking the former with the latter.The rest of the article consists of the following. The next section discusses how our study relates to and extends the existing research. We then present the proposed modeling framework and the data used to illustrate it. We report the empirical findings and results from what-if analyses. We conclude with a discussion of the managerial implications and directions for future research. Relationship to Prior LiteratureAs the second-screen phenomenon during television advertising has become more prevalent, a growing body of research has documented some of its effects on various online metrics. Table 1 provides an overview of this stream of research and identifies the key dimensions that distinguish the current study from prior work.GraphTable 1. Literature on Online Response to Offline TV Ads.  1 a[43] published several case studies with only data visualizations and no formal econometric analysis.[43], [44]) published the first case studies that documented large post-ad spikes in Google search for the advertised brands following TV ads during the opening ceremonies of the 2008 and 2010 Olympic Games. Since then, several studies have found a similar positive effect of TV ads on online search ([ 5]; [17]; [20]; [22]; [26]; [29]). Further research has shown that TV ads lead to other online responses as well, including brand website traffic ([24]; [30]), online word of mouth (WOM; [13]; [25]; [38]), and online conversions ([14]; [16]). These results stem from various types of analyses using monthly, weekly, daily, hourly, or minute-level data. Given that over 90% of TV ad spots are shorter than one minute, the most granular analysis at the minute level is more desirable because a smaller data interval could better eliminate potential aggregation bias ([37]). We therefore focus on comparing the current research with previous studies that have also conducted analyses at the minute level (see Table 1).Even though there seems to be a broad consensus that television advertising leads to a variety of behavioral responses online, only a handful of previous studies have investigated how factors related to ad creative and media placements moderate those effects. [30] found that the effects of TV ads on brand website traffic and subsequent online purchases vary depending on whether the ads have an action, information, emotion, or imagery focus. [13] showed that featuring a hashtag or the web address in the call to action increases subsequent online brand WOM for ads that air in the first slot of a commercial break, but featuring a phone number reduces subsequent online chatter.Both articles used innovative ad content measures—[30] employed research assistants to code content, and [13] brought in data produced by a firm called iSpot by analyzing advertisement videos. We expand this small number of studies by examining how consumer attitudinal responses to ad creative (collected by a firm called Ace Metrix from large panels of survey respondents) moderate immediate online brand and price search response, in addition to other media- and audience-related moderators. Therefore, the moderating effects of ad content enter the analysis as ad creative quality ratings, rather than specific content elements within individual ad creatives. Given the large and diverse nature of stimuli encoded within TV ads, it is possible that these summary evaluations are both more parsimonious and more complete measures of content than prior studies were able to access.More broadly, the current study contributes to the literature in five notable ways. First, it is the first article to study the effect of television advertising on price search (i.e., requesting price quotes at car shopping websites), and it further allows for direct comparison of those effects to the effects of TV ads on brand search from Google. By differentiating between brand search and price search, it helps improve our understanding of how TV-to-online spillovers vary across different stages of the purchase funnel. The answers can influence brands' media planning and buying practices to reach targeted audience at the ""right"" moment of the shopping journey.Second, most of the existing studies measured advertising exposure using either ad expenditures ([17]; [18]; [20]; [22]; [26]) or ad gross rating points ([14]; [16]), which are typically measured at the telecast or quarter-hour level. To the best of our knowledge, we are the first to use spot-level ad audience size data in quantifying the rate of immediate online response to regular TV ads. This is important because consumer ad avoidance varies throughout commercial breaks and sharpening the resolution of the number of viewers exposed to each ad spot facilitates greater statistical power and estimation precision.Third, we were able to gain access to an important measure at the spot level—namely, the proportion of viewers who were contemporaneously in the market for a new pickup truck. It seems likely that TV-to-online spillovers will be strongly influenced by the proportion of viewers who are category shoppers, but this has never been quantified in any similar context. The possibility was previously tested in the TV/YouTube context by [ 6], who showed that accurate evaluation of ad effects depends critically on viewers' preexisting brand knowledge. As far as we know, we are the first to quantify how audience interest in the advertised category affects viewers' immediate online search response after seeing a TV ad.Fourth, the current study contributes to the literature on advertising competitive spillovers. Perhaps the most similar paper from this literature is [33], who found that advertisements for restaurants increased phone referrals to competing restaurants. Our results complement a larger literature showing positive/negative/no competitive spillovers, including [ 3] in catalog retailing, [21] in cruises, [36] in prescription drugs, [28] in online display advertising, [11] and [12] in targeted promotion, and [34] in brick-and-mortar store feature advertising. The direction and degree of competitive spillovers have not been reported in the context of TV ads and online responses. The current research aims to uncover how TV ads spill over to brand and price search for competitors' products and further to quantify possibly asymmetric effects between competitors.Finally, we offer a generalizable modeling framework that should prove useful to brands that want to quantify the immediate online behavioral consequences of their TV ad creative, the programs during which the ads run, and the people who view the ads. ModelIn this section we present a framework for modeling online activities at the minute level, which is decomposed into the sum of a baseline, an immediate response caused by TV ads, and an error term. The baseline is allowed to have, among other things, an hourly fixed effect and a within-hour trend that can vary by hour of the week. The immediate response to an ad spot is modeled to have a duration and a flexible decay pattern that are determined empirically. The immediate impact of each ad on online activity is modeled as the product of the ad audience size (or cost, when audience data are not available) and a response rate, which in turn depends on the characteristics of the ad creative, media placement, and audience. The error term is serially correlated, with the pattern determined empirically. Although parts of the model are tailored to the automotive industry (e.g., we separate ad spots into own national, competitor national, own local, and dealers associations), the framework is readily adaptable to other empirical contexts.Let us assume online activity  l∈{brand search, price search}  for brand b in minute t,  Sbtl  , consists of the following components: Sbtl=τbtl+∑i=0M[ϕbt, t−ilNAb, t−i]+∑i=0N∑c=1Cb[χbct, t−ilNAc, t−i]+∑i=0N[ψbt, t−ilLAb, t−i]+∑i=0N[ωbt, t−ilDAb, t−i]+ϵbtl, Graph1where τbtl  denotes the baseline of activity l for brand b in minute t (i.e., what would have been the volume of brand or price search if there had been no TV ads), which we specify as a function of fixed hour effects, within-hour trends that can vary by hour of the week, and the volume of search for a control keyword (""SUV"") in minute t, as described in more detail subsequently; NAb, t−i  and  NAc, t−i  denote the total audience (in millions) exposed to national TV ads in minute  t−i  for, respectively, brand b and each of its competitors  c∈Cb  ; ϕbt, t−il  and  χbct, t−il  denote the rates at which ad audiences  NAb, t−i  and  NAc, t−i  , respectively, respond to an ad exposure at minute  t−i  with online activity l for brand b in minute t; LAb, t−i  and  DAb, t−i  denote the spend (in $10,000s) on local TV ads by, respectively, brand b and its dealers associations in minute  t−i  [ 8]; ψbt, t−il  and  ωbt, t−il  denote the rates at which ad spend  LAb, t−i  and  DAb, t−i  , respectively, generate, after an i-minute delay, online activity l for brand b in minute t; and ϵbtl  denotes the error term, which is given a moving-average representation,  ϵbtl=ebtl+∑i = 159ρbileb, t−il  with  ebtl∼i.i.d. N(0,σl, b2)  , to allow for a flexible pattern of serial correlation.Of key interest is  ϕbt, t−il  —that is, for every one million exposures to a national TV ad of brand b in minute  t−i  , the number of online responses of type  l   in minute t, which we specify as ϕbt, t−il=αnatl, bilexp(γblweek(t)+∑j=1JβjlXbj, t−i), Graph2where  αnatl, bil  denotes a baseline rate of i-minute delayed response,  γbl  captures a long-term trend in viewers' tendency to respond immediately to brand b's national TV ads, and  βjl  captures the moderating effect of the jth ""lift factor,""  Xbj, t−i  , which characterizes brand b's TV ads in minute  t−i  . There are three broad types of factors that can moderate the rate of immediate online response to a TV ad spot: those related to the ad creative, the media placement, and the audience.[ 9] For minutes with overlapping ads (i.e., spots aired in the same minute but on different networks),  Xbj, t−i  is calculated as the audience size-weighted average. Note that the exponential formulation implies that the jth lift factor has a multiplier effect—all else being equal, for one unit increase in  Xbj, t−i  , the response rate would be scaled by a multiple of  exp(βjl)  .[10]Compared with the rate of immediate response to a brand's own national TV ads (  NAb, t−i  ), we adopt a simpler specification for the rate of immediate response to competitors' national ads (  NAc, t−i  ) to obtain a more parsimonious investigation of the competitive spillover of TV advertising on immediate online response. Due to the lack of data on all three types of moderating factors, we model the rate of immediate response to own local ads (  LAb, t−i  ) and own dealers association ads (  DAb, t−i  ) in a similar fashion: χbct, t−il=αnatl, bcil,ψbt, t−il=αloc, bil, and ωbt, t−il=αdealer, bil. Graph3It is critical to specify the baseline flexibly to avoid conflating advertising effects with correlated unobservables. To minimize such concerns, we formulate  τbtl  as follows: τbtl=μb, hour(t)l+λb, hour of week(t)lt+κblSUVt, Graph4where μb, hour(t)l  denotes a fixed effect for the specific hour containing minute t, accounting for the average baseline activity in each given hour of the sample period. λb, hour of week(t)l  denotes a fixed effect that accommodates a distinct local trend in baseline activity for each hour of the week (i.e., Monday 12 a.m., Monday 1 a.m.,..., Sunday 11 p.m.). It is included to control for unobservables that have within-hour trends and may correlate with within-hour TV ad insertion patterns.[11] SUVt  denotes the number of searches containing the keyword ""SUV"" in minute t, which serves as a control for consumers' general tendency to search for large automobiles in any given minute of the sample period.In summary, we see three main ways in which endogeneity could bias the estimates of immediate online response to TV ad insertions. In Web Appendix A, we discuss these main threats and explain how we alleviate those concerns through a combination of model specification and data richness.To calibrate the model described in Equations 1–4, we first take the difference between pairs of consecutive minutes within each hour, canceling out the hour-of-sample fixed effects  μb, hour(t)l  . This relieves us of the need to estimate these fixed effects, which are numerous but not of primary interest. Formally, by applying the first-difference operator (i.e.,  Δxt=xt−xt−1  ), we can transform the original model into the following mathematically equivalent representation: ΔSbtl=λb, hour of week(t)l+κblΔSUVt+∑i=0Mαnatl, bil{exp[γblweek(t)+∑j=1JβjlXbj, t−i]NAb, t−i−exp[γblweek(t−1)+∑j=1JβjlXbj, t−1−i]NAb, t−1−i}+∑i=0N∑c=1Cbαnatl, bcilΔNAc, t−i +∑i=0Nαloc, bilΔLAb, t−i+ ∑i=0Nαdealer, bilΔDAb, t−i+Δebtl+∑i=159ρbilΔeb, t−1l. Graph5We estimate Equation 5 using nonlinear least squares with serially correlated residuals. For all three brands, brand search response becomes statistically undetectable nine minutes after the start of own national TV ads (i.e., M = 9), and five minutes after the start of competitor national ads, own local ads, and dealers association ads (i.e., N = 5). Price search response becomes indistinguishable from zero after six minutes for own national ads (i.e., M = 6) and four for competitor national, own local ads, and dealers association ads (i.e., N = 4). DataFor each of the three pickup truck brands, we compiled a rich set of data from multiple sources, from February 15, 2015, through January 23, 2016, a span of 493,920 minutes, avoiding Super Bowl outliers to focus on regular TV spots. The rest of the section describes data from each source and how we merged them for our empirical analyses. Brand searchWe obtained minute-by-minute brand search volume data by combining extracts from Google Trends and AdWords Keyword Planner.[12] Google Trends provides brand search indices by week-within-sample period, hour-within-each-week, and minute-within-each-hour. Google AdWords Keyword Planner provides monthly total brand search volume estimates. We apportioned Keyword Planner's monthly total brand search volume estimates according to Google Trends' brand search indices to obtain, sequentially, brand search volume estimates for each week, each hour within each week, and finally, each minute within each hour. Price searchWe obtained minute-by-minute price search volume data from Autometrics (www.autometrics.com), which has agreements with major car shopping websites in the United States to process records of car shoppers requesting online price quotes from local dealerships. Each record consists of the time stamp of an online price quote request, the car shopping website through which the request was made, the brand and the model of the vehicle requested, and the zip code entered by the car shopper who made the request. We are able to access these records aggregated by brand and minute, thus forming the price search data used in this study. Private conversations with Autometrics and automotive executives indicate that the amount of online price quote requests is a common key performance indicator in the industry, often used as a proxy for the number of car shoppers who are close to the end of the purchase funnel. SUV searchUsing the same method we used to obtain minute-by-minute brand search volume data for the three pickup truck brands, we obtained the number of Google search queries containing the word ""SUV"" in each minute. The purpose of collecting this data was to improve baseline search volume estimates for each focal truck brand by using SUV search volume as a control for factors that vary by the minute and may influence online searches for large automobiles such as SUVs and pickup trucks (e.g., the presence of a TV commercial break). Ad audience size for national spotsDuring the sample period, the three focal truck brands ran a total of 27,562 ad spots on national TV at a cost of $210 million. For each of these national spots, we obtained audience size data from comScore's ""TV Essentials"" database. ComScore collects TV viewing data passively from 52 million digital set-top boxes in 22 million households. ComScore has nearly a thousand-fold advantage over Nielsen's sample size of 26,000 households,[13] enabling it to provide reliable audience size estimates for ""long-tail"" television networks and programs. By having a reliable estimate of the actual audience size of each ad spot (as opposed to using program ratings or cost estimates as proxies), we are in a position to quantify, for the first time in the literature, the amount of immediate online response to TV ad spots on a per impression basis (analogous to click-through rates of display ads). Ad creative scores for national spotsWe obtained ad creative scores from Ace Metrix, a provider of competitive intelligence on advertising content. Ace Metrix identifies new national TV ad creative and, within 24 hours of its first airing, exposes each creative to 500 online panelists and records their attitudinal responses through a standardized survey. The panelists were asked to indicate their level of agreement with a battery of statements about the ad creative in question on a scale of 0–100 (0 = ""Not at all,"" and 100 = ""Very much""). We were able to obtain survey ratings from Ace Metrix for ad creatives that accounted for 92% of the national TV impressions in our sample. Three scores are of particular interest: AdInfo (how informative an ad creative is), AdLike (how likable an ad creative is), and AdDes (how much an ad creative has made the advertised brand desirable), which map roughly into the three broad stages in the hierarchy of effects—cognitive, affective, and conative ([27]). The survey statements used to generate these three scores are, respectively, ""I learned something,"" ""I like this ad,"" and ""I want that! (whatever you think the commercial is about)."" Media placements for national spotsFor each national TV ad insertion in our sample, we obtained media placement data from Kantar Media's ""Stradegy"" database, a comprehensive source for competitive advertising intelligence that covers all ad spots run on major national networks and local broadcast stations. For each national spot, we observe the date, start time, duration (30 seconds for 99% of ads in the sample), advertised brand, ad creative identifier, pod position, TV network, program genre, and a cost estimate. Ad audience category interest for national spotsPolk Automotive Intelligence (Polk, hereinafter) collects data on all new automobile registrations in the United States. In partnership with comScore, for each national TV ad spot, Polk uses proprietary algorithms to estimate what fraction of the ad's audience was contemporaneously a potential purchaser or lessee of a new pickup truck.[14] We obtained these spot-level estimates and refer to them as AudienceCategoryInterest, the sample averages of which are 17.2% for Ford's ad audience, 16.7% for Chevy's, and 17.7% for Ram's. Ad spend for local spotsDuring the sample period, the three focal truck brands ran a total of 750,672 ad spots (318,238 from manufacturers and 432,434 from dealers associations) on local TV stations at a cost of $261 million, with $106 million spent by manufacturers and $155 million spent by dealers associations. For each of these local spots, we obtained from Kantar Media's ""Stradegy"" database the date, start time, duration, advertised brand, and a cost estimate. Unfortunately, comScore's ""TV Essentials"" database does not cover local spots. As a result, we used spot-level cost estimates as a proxy for spot-level audience sizes. In addition, neither Ace Metrix nor Polk covers local TV ad spots, preventing us from having ad creative scores or ad audience category interest estimates for these local spots. Merging spot-level data by minuteIn our proposed modeling framework, national ad audience sizes, local ad spend, and national ad lift factors are minute-level measures. To convert spot-level data to minute-level measures and merge them across sources, we do the following.For national spots that straddled consecutive minutes, we assume a constant number of viewers at each second of the spot's duration. For example, for a 30-second spot that started at 19:50:45 and had an audience size of 1 million, we assume it generated 15 million impression-seconds from 19:50:45 to 19:50:59 and 15 million impression-seconds from 19:51:00 to 19:51:14. For each minute, we aggregate impression-seconds from all the national spots that had exposures during any second of that minute. We then divide the total impression-seconds in each minute by 60 to arrive at our minute-level measure of average national ad audience size (i.e.,  NAb, t  and  NAc, t  in Equation 5).For local spots that straddled consecutive minutes, we split the cost estimate of each spot into each minute, proportional to the number of seconds run in each minute. We then aggregate the costs by minute to arrive at our minute-level measure of local ad spend (i.e.,  LAb, t  and  DAc, t  in Equation 5).Finally, for minutes with exposures from multiple national ad spots, we calculate our minute-level lift factors (i.e.,  Xbj, t  in Equation 5) by taking the weighted averages across all the ad spots that had any exposures in each minute, with the weight being the impression-seconds each spot had in each minute.Table 2 presents descriptive statistics of the minute-by-minute brand and price search data. Overall, the variation in brand and price searches across brands conforms to the three brands' relative position in market share. During the sample period, Ford F-Series was searched 36 million times on Google (or 72 times per minute) and 8 million times on major car shopping websites (or 16 times per minute); Chevy Silverado was searched 21 million times on Google (or 43 times per minute) and 7 million times on major car shopping websites (or 13 times per minute); Ram Trucks was searched 19 million times on Google (or 39 times per minute) and 3 million times on major car shopping websites (or 6 times per minute).GraphTable 2. Descriptive Statistics of Minute by Minute Brand and Price Search Data.  Table 3 presents descriptive statistics of the spot-level advertising data. During the sample period, Ford F-Series spent $191 million on television advertising, 42% of which on 1,777 national manufacturer spots, 8% on 39,229 local manufacturer spots, and 50% on 264,488 dealers association spots. Chevy Silverado spent $135 million on television advertising, 47% of which on 12,653 national manufacturer spots, 26% on 112,209 local manufacturer spots, and 27% on 125,885 dealers association spots. Ram Trucks spent $145 million on television advertising, 45% of which on 13,132 national manufacturer spots, 40% on 166,800 local manufacturer spots, and 15% on 42,061 dealers association spots. During the sample period, Chevy Silverado aired 72 unique pieces of national ad creative, followed by Ram Trucks with 67 and Ford F-Series with 30.GraphTable 3. Descriptive Statistics of Spot-Level Advertising Data.  Table 4 presents descriptive statistics of the minute-level advertising data that was merged across sources and used in model estimation.[15] Ford F-Series had far fewer minutes with national ads ( 2,562) than Chevy Silverado and Ram Trucks (each with more than 18,000) but much larger audiences per ad minute (540,000 vs. 13,000–14,000, on average). This is because Ford's national spots were far more concentrated in broadcast networks, especially during professional football games, which also tend to be more expensive on a per impression basis, leading to a much higher average spend per national spot for Ford (about $45,000) than Chevy (about $5,000) and Ram (about $5,000). Ace Metrix data indicate that Ford ads were rated as the most informative and likable on average and Chevy ads induced the most desire to purchase. Median audiences per ad minute were far smaller than the averages, with the medians ranging from about 60,000 for Chevy and Ram to 150,000 for Ford, underscoring the ""ordinary"" TV ad spots that predominate the sample.GraphTable 4. Descriptive Statistics of Minute-Level Advertising Data.  2 aThe three Ace Metrix scores are standardized across ad creative.Figure 1 visualizes the patterns of minute-by-minute brand searches for three one-hour periods—for each focal brand, we zoomed in on the hour containing ad insertions that had the highest spend in the sample period, which all occurred during nationally televised professional football games. Each gray bar in Figure 1 depicts a commercial break during the telecast, and each dash vertical line indicates an ad insertion by a focal brand.Graph: Figure 1. National TV ads and post-ad brand search spikes.Notes: The panels present three one-hour windows that contain national TV ad insertions with the largest audience size for each of the three brands. In all three panels, gray bars indicate the time windows for commercial breaks. Dashed vertical lines mark the starting time of the ad insertions for a focal brand.In Figure 1, Panel A, we see two ad insertions for Ford F-Series. The first began at 9:16:20 PM, lasted for 30 seconds, and had a middle pod position and an average audience of 21.9 million. In the minute before the ad insertion, there were 152 own-brand searches; in the minute after the ad insertion, there were 664 own-brand searches, a 4.4-fold spike. A back-of-the-envelope calculation suggests that the immediate own-brand search response rate, one minute after the ad insertion, could be approximately 23 per million [= (664 − 152)/21.9].The second Ford ad insertion, with different creative, began at 9:22:04 PM, lasted for 30 seconds, had a first pod position and an average audience of 22.3 million. The volume of own-brand searches had a five-fold spike, from 144 in the minute before the ad insertion to 722 in the minute after, suggesting an immediate own-brand search response rate of roughly 26 per million [= (722 − 144)/22.3]. From Figure 1, Panels B and C, we see spikes of similar magnitudes in minute-by-minute brand searches for Chevy and Ram, after their respective ad insertions.Besides the immediate post-ad spikes in searches for the advertised brands, there are several other patterns in Figure 1 that are remarkable. First, all the focal ad insertions (especially the ones by Chevy and Ram) seem to have preceded spikes in brand searches for their direct competitors (Ford in particular), suggesting positive competitive spillover in immediate online response to TV advertising. Second, we see no noticeable spikes in searches for the three focal brands during commercial breaks that did not have any of their ad insertions. This suggests that the brand search spikes are caused mainly by the presence of the focal brands' TV ads, rather than by the absence of the game. Third, brand search volume reverted to its pre-ad baseline within five minutes or less. Finally, no noticeable dips appear below the pre-ad baseline following the post-ad spikes, which might imply that the ads produced truly incremental search rather than accelerating search that would otherwise have occurred a few minutes later.The striking visualization presented in Figure 1 offers clear but anecdotal evidence of immediate online response to TV ads. The patterns we observe in Figure 1 could prove to be the exception rather than the rule, because the vast majority of ad spots have audiences that are two orders of magnitude smaller. Can one reliably quantify the immediate online response to regular TV ads and how the response rate may be moderated by various lift factors? The next section presents our empirical findings by applying our proposed modeling framework to the comprehensive data we have managed to stitch together from multiple sources. ResultsThis section presents the parameter estimates for the main effects (  αnatl, bil  ,  αnatl, bcil  ,  αloc, bil  , and  αdealer, bil  ) and the moderating effects (  βjl  ) based on Equation 5, the estimating equation. It concludes with what-if analyses based on the calibrated model. Web Appendix C presents the parameter estimates related to the baseline (  λb, hour of week(t)l  ,  κbl  , and  ρbil  ), which are not of primary interest but are important from the standpoint of model calibration. Main Effects of Own National SpotsTable 5 reports the main effect estimates of own national TV ads (  αnatl, bil  ), averaged across spots by minute following an ad insertion. In terms of own-brand search response, from the minute the ad was aired to the ninth minute afterward, one million ad impression-minutes (i.e., an average ad audience of one million over a span of 60 seconds) would generate, on average, 40.2 immediate brand searches for Ford F-Series, 33.8 for Chevy Silverado, and 17.8 for Ram Trucks, following the order of the brands in total brand search volume and market share. These effect estimates indicate that the rate of immediate own-brand search response per viewer is, respectively,.0040% for Ford,.0034% for Chevy, and.0018% for Ram, which are smaller than the typical click-through rates for online display ads (.05%) ([ 4]). That said, given the large number of total national ad impression-minutes (1,379 million for Ford, 2,421 million for Chevy, and 2,585 million for Ram), the total number of immediate own-brand searches attributable to national TV ad spots are still substantial (about 55,000 for Ford, about 82,000 for Chevy, and about 46,000 for Ram).GraphTable 5. Main Effects of Own National Spots.  3 *p <.01.In terms of price search response, from the minute the ad was aired to the sixth minute afterward, one million ad impression-minutes would generate, on average, 6.2 immediate price searches for Ford, 1.7 for Chevy, and.6 for Ram, following the order of the brands in total price search volume and market share. These effect estimates indicate that the rate of immediate price search response per viewer is much lower than the rate of brand search response:.0006% for Ford,.0002% for Chevy, and.0001% for Ram. This is not surprising, in that there tend to be more shoppers at the upper funnel, who are more likely to conduct brand searches, than shoppers at the lower funnel, who are more likely to conduct price searches. Nevertheless, because the total number of ad impression-minutes is large, the total number of immediate price searches attributable to national TV ads is nontrivial: about 8,600 for Ford, about 4,000 for Chevy, and about 1,400 for Ram. It is also a testament to the power of the data and modeling framework in detecting weak signals.How do these effect estimates compare with what has been reported in the literature? To facilitate comparison, we report at the bottom of Table 5, summarized across all the ad minutes and by brand, the average and median elasticities of minute-level brand and price searches to national TV ads. We see heterogeneity across the brands and between the types of search response. Following the order in market share, the average elasticities of brand search are, respectively,.22 for Ford,.10 for Chevy, and.06 for Ram. The average elasticities of price search are, respectively,.20 for Ford,.02 for Chevy, and.02 for Ram.Across all the ad minutes and brands, the average elasticities of brand search and price search are, respectively,.09 and.03, which are comparable to the average elasticity of sales to advertising (.12) reported by [35] and those that have been reported in the literature of online response to offline TV ads. For example, [18] find that the average elasticity of brand search to advertising (across 21 vehicles) is.04; [20] report an average elasticity of.17; [22] report an average elasticity of.07 for less established brands; [14] find that the elasticities of conversion to advertising range from.05 to.11 in car insurance, health insurance, and banking industries; and [17] report elasticities of mobile search between.13 and.17.Figure 2 plots the percentages of total immediate search response realized by minute following an ad insertion. For own-brand search, on average about 12% of the cumulative effect is realized in the minute the ad is aired, followed by approximately 58% in the following minute and 17%, 7%, and 4% in the second, third, and fourth post-ad minutes, respectively. For price search, the vast majority of response occurs between the first and the fifth post-ad minutes, with each of the five minutes accounting for about 20% of the cumulative effect. These temporal patterns suggest that ( 1) for both brand search and price search, nearly all of the immediate response takes place within five minutes of a TV ad insertion, and ( 2) brand search response arises and dissipates more quickly than price search response, which is intuitive because, on average, it takes more time to conduct a price search through a car shopping website than a brand search through Google.Graph: Figure 2. Percentage of cumulative search response to own national ads by minute after ad insertion (averaged across brands). Moderating Effects of Lift Factors for National SpotsIn addition to quantifying the average effects of ad spots, TV advertisers are equally interested, if not more so, in quantifying how contextual factors may moderate immediate online response, which can help them assess the relative effectiveness of different ad creative, media placements, and audience targeting criteria. We allow the contextual factors (i.e.,  Xbj, t  in Equation 5) to moderate the response rate multiplicatively. Thus, all else being equal, for one unit increase in  Xbj, t  , the response rate and the elasticity are expected to be lifted by  exp(βjl)  times. In other words, one can interpret  exp(βjl)  s as the multiplier effects of the contextual factors, whose estimates we report in Table 6. A multiplier significantly different from 100% indicates that the corresponding factor has a significant impact on immediate brand or price search response.GraphTable 6. Moderating Effects of Lift Factors.  4 *p <.01.5 aThe multipliers are calculated as  exp(βjl)  .6 bThe scores are standardized to have a standard deviation of one.7 cAudienceCategoryInterest is measured in percentage points. Ad creative–related factorsIn terms of brand search response, the multipliers associated with the three ad creative scores (standardized to have a standard deviation of one) are all significantly greater than 100%, suggesting that, all else being equal, ad creative deemed by viewers as more informative (""I learned something from the ad""), likable (""I like this ad""), or desirable (""I want that!"") generates more immediate brand searches. This is reassuring in the sense that advertisers selecting ad creative on the basis of either traditional survey-based copy testing scores or immediate brand search response would make similar choices. The estimated multipliers (119% for informativeness, 108% for likability, and 110% for desirability) indicate that, on average, one standard deviation of improvement in an ad creative's attitudinal response could lead to approximately 10% to 20% improvement in brand search response.In terms of price search response, the multipliers associated with the three ad creative scores are further away from 100%, but none are significant at the 99% confidence level. We see two potential explanations. It could simply mean that the signal-to-noise ratio is not high enough to reliably quantify the moderating effects of ad creative scores on immediate post-ad price search. An alternative explanation could be that price search is more likely a lower-funnel behavior, whereas national TV ads are more often used to further upper-funnel goals, which makes the creative scores of national TV ads a less reliable predictor of immediate price search response.The contrast between the results for brand search and price search suggests that advertisers should be cautious in relying on any single online response measure in assessing the relative effectiveness of ad creative. Although it appears that ads with more favorable attitudinal response are associated with more immediate post-ad brand searches, they do not seem to generate more immediate price searches. Thus, it is important to ascertain ( 1) whether the signal-to-noise ratio is high enough to reliably quantify the moderating effects of ad creative–related factors and ( 2) how critical favorable attitudinal response is in generating the behavioral response the marketer seeks. Media placement–related factorsFor both brand and price search, all else being equal, spots run in the first slot of a commercial break generate significantly higher rates of immediate online response (+22% and +54%, respectively). Note that we obtain these strong effects after controlling for the audience size of each ad spot. In other words, these effects are not due to the fact that more viewers may have watched the first ad in a commercial break before they changed channels. We speculate that these positive first-slot effects resulted because ad viewers are more attentive during the first ad in a commercial break, before their cognitive capacity is depleted by subsequent ads in the break. It could also be the case that viewers have more time to conduct online searches after watching the first ad in a commercial break, having to worry less about missing the TV programming after the break. In short, our results are consistent between brand and price search and suggest that the first slot in a commercial break could be worth a double-digit premium due to a more attentive/responsive audience.Similar to the first-slot effect on brand search response, we observe that ad spots run during prime time or a professional football game generate significantly more immediate brand searches (+23% and +55%, respectively), after having controlled for ad audience size. The positive lift of prime time could be due to the fact that viewers are more attentive to the commercials and TV programming during the daypart that is typically associated with TV viewing. Another intuitive explanation is that the second-screening phenomenon is the strongest during prime time because more TV viewers have ready access to their mobile devices, enabling them to conduct immediate post-ad search online. It could also be that prime time coincides with when most car shoppers conduct online research for cars and are thus more likely to respond to car ads. The strong positive lift of professional football games is also intuitive. We suspect that viewers are more attentive to the commercials during live sports programming.The effects of prime time and professional football on price search response are also positive (+11% and +27%, respectively), but not significant at the 99% confidence level. The lack of statistical significance is another sign that the signal-to-noise ratio in the price search data may not be high enough to reliably quantify the moderating effects of some lift factors.Unlike the effects of first slot, prime time, and professional football, which are directionally consistent between brand search and price search, the effects of broadcast and weekend diverge between the two types of online response. Ad spots run on broadcast networks generate significantly fewer immediate brand searches per viewer (−12%) and significantly more immediate price searches per viewer (+54%). We speculate that these divergent effects occur because broadcast viewers are, on average, less affluent than cable viewers and are therefore more price sensitive, which makes broadcast viewers (relative to cable viewers) more likely to conduct price searches and less likely to conduct brand searches.Ad spots run on weekends generate significantly fewer immediate brand searches per viewer (−9%) and significantly more immediate price searches per viewer (+64%). We speculate that these divergent effects occur because car shoppers are more likely to visit dealerships and make purchases on weekends than on weekdays. As a result, relative to weekdays, car shoppers are, on average, more likely to conduct price searches (operationalized as requesting price quotes from local dealerships in our study) and less likely to conduct brand searches on weekends.The divergent broadcast and weekend effects on brand versus price search show that media placements that can generate more of one type of online response may generate less of other types of digital activity. This cautions TV advertisers against relying on any single immediate online response metric in selecting media placements, as there is unlikely a media plan that can optimize all types of online response. That said, if the advertiser does have one type of online response that it intends to focus on for a particular campaign, large lifts in performance and cost effectiveness can accrue from quantifying the multiplier effects of various media placement factors and then making media buys accordingly. Audience-related factorsAll else being equal, for every one-percentage-point increase in AudienceCategoryInterest, the number of immediate brand searches per ad viewer increases by 2%, which is significant at the 99% confidence level. The amount of immediate price searches per ad viewer also increases but the increase is not significant. To put the effect size of AudienceCategoryInterest on brand search response into perspective, consider an ad spot with AudienceCategoryInterest at, say, 27%, which is ten percentage points above the average of 17%. Our effect estimate  (βjl=.16)  indicates that, all else being equal, one would expect to see a brand search response rate that is 17% higher  [=exp(.016×10)−1]  than the average. This finding suggests that spot-level audience characteristics data furnished by third-party vendors (e.g., Polk, comScore, Acxiom, Datalogix, Experian, Nielsen) can be validated through their correlation with immediate post-ad online response. In our empirical context, the spot-level audience category interest estimates have demonstrated strong face validity, which is reassuring for TV advertisers that increasingly rely on rich audience data for targeted media buys. Main Effects of Competitor National SpotsTable 7 reports the effect estimates of competitor national TV ads (  αnatl, bcil  ) on focal brand search and price search, averaged across spots by minute following an ad insertion. In terms of total brand search response (cumulative from the minute the ad was aired to the fifth minute afterward), we see positive and significant spillover across all six directional dyads. These significant and consistent effect estimates suggest that TV ads can trigger not only immediate searches for the advertised brand but also its competitors. We speculate that this occurs because TV ads can remind viewers of alternatives to the advertised brand, which in turn could spur them to search the competitor brand for comparison. It also might be that TV ads remind consumers of category needs, thereby leading consumers interested in competing brands to search those brands directly, without a comparison.GraphTable 7. Main Effects of Competitor National Spots.  8 *p <.01.In terms of magnitude, the estimated main effects on own brands are much larger than competitive spillovers. For one million impression-minutes, an average Ford spot generates 40.2 Ford searches versus 8.4 Chevy/Ram searches, an average Chevy spot generates 33.8 Chevy searches versus 12.0 Ford/Ram searches, and an average Ram spot generates 17.8 Ram searches versus 5.4 Ford/Chevy searches. It is remarkable that the data and modeling framework reliably quantified the sizes of competitive spillovers, even though the competitor brand search response rate is extremely low:.0008% for Ford,.0012% for Chevy, and.0005% for Ram. The implied average elasticities of brand search to competitor national TV ads range from.003 to.05.Ford receives the most competitive spillovers (8.2 from Chevy and 4.4 from Ram). This suggests, unsurprisingly, that the category leader is probably the default or the reference option in most shoppers' consideration set. As a result, it receives the most comparison searches.[16]Finally, in terms of competitive spillovers in price search response, we find mostly insignificant effect estimates. This could be another sign that the signal-to-noise ratio in the price search data may not be high enough for our model to reliably quantify immediate post-ad competitor price search. It could also be that, as car shoppers approach the end of the purchase funnel, they are less likely to comparison shop between brands and more likely to comparison shop between local dealerships of the same brand for the best price. Main Effects of Local SpotsTable 8 reports the effect estimates of local manufacturer ads  (αloc, bil)  and dealers association ads  (αdealer, bil)  , averaged across spots by minute following an ad insertion. In terms of brand search response, from the minute the ad was aired to the fifth minute afterward, local manufacturer/dealers association ads costing about $10,000 would generate, on average, 8.1/7.0 immediate brand searches for Ford, 6.4/8.7 for Chevy, and 6.4/−1.2 (insignificant at the 99% confidence level) for Ram. Averaged across the three brands, the implied elasticity of brand search to local manufacturer ads and dealers association ads are, respectively,.002 and.001. In terms of price search response, the effects are, respectively, 6.3/2.0 for Ford, −2.2/2.8 for Chevy, and.6/1.0 for Ram. Averaged across the three brands, the implied elasticities of price search to local manufacturer ads and dealers association ads are, respectively,.0002 and.002.GraphTable 8. Main Effects of Local Spots.  9 *p <.01.10 Notes: It is a bit counterintuitive that the immediate price search response rate for Chevy local manufacturer ads is negative and significant (−2.21). It could simply be a type I error. Or it could be that Chevy local manufacturer ads have already provided sufficient information that it makes price search unnecessary.Several aspects of the results are worth noting. First, because the total spend on local ads is large ($110 million for Ford, $71 million for Chevy, and $80 million for Ram), the total number of immediate searches attributable to local TV ads is substantial: about 78,000 for Ford, about 54,000 for Chevy, and about 37,000 for Ram in brand searches; and about 28,000 for Ford, about 10,000 for Chevy, and about 6,000 for Ram in price searches. Summed across the three brands, there was a total spend of $261 million on local ads, which generated about 169,000 immediate post-ad brand searches and about 44,000 price searches.It is also instructive to compare the immediate post-ad searches attributable to local spots with those attributable to national spots, which are presented in Table 9. Relatively speaking, in terms of generating immediate brand search response, national spots are the most cost effective (on average 8.7 per $10,000 spend), followed by local manufacturer spots (6.6 per $10,000 spend) and local dealers association spots (6.4 per $10,000 spend). The opposite is true when it comes to generating immediate price search response: local dealers association spots are the most cost effective (on average 2.1 per $10,000 spend), followed by local manufacturer spots (1.2 per $10,000 spend) and national spots (.7 per $10,000 spend).GraphTable 9. National Versus Local Spots in Immediate Post-Ad Search Response (Averaged Across All Insertions).  This reversal of relative cost effectiveness in generating brand versus price search has an intuitive explanation in that the content of TV ads for these three truck brands typically varies systematically between national and local spots. National spots are purchased exclusively by the manufacturers and, according to a content analysis by [40], typically carry brand-oriented messages with relatively few price-oriented messages. Local TV spots are purchased by both manufacturers and local dealers associations, with both parties designing ads that extensively communicate current market-specific pricing and promotion terms. As a result, TV viewers respond accordingly: the ratio between price and brand search response is the highest for price-focused local dealers association spots (1:3) and the lowest for brand-focused national spots (1:13). We view this intuitive finding as another testament to the face validity of our effect estimates and, in turn, the power of the data and modeling framework.Finally, it is worth remembering that, unlike national spots, we do not have access to reliable audience measures for local spots, which requires us to rely on spot-level cost estimates provided by Kantar Media as a correlate of local ad audience size. As a result, we can only quantify immediate search response rate on a per impression-minute basis for national spots. To the extent that the same amount of spend can purchase more impressions on local TV than on national TV, the amount of immediate search response per viewer is likely lower on local TV than on national TV. That said, because there is likely greater measurement error in local ad exposure than in national ad exposure, our local spot effect estimates are likely to have more downward error-in-variable bias than their national counterparts. What-If AnalysisHow can TV advertisers leverage our modeling framework and the resulting effect estimates to assess the relative effectiveness of different ad spots and thereby refine their selection of ad creative and media placements? This subsection presents several what-if analyses to demonstrate the potential usefulness of our approach in practice.Given the calibrated model, we can simulate the amount of incremental brand and price searches if the TV advertiser were to have a different allocation of ad spend across media placements, target audiences, and ad creative. Because media placement factors and target audiences tend to be correlated with one another, for simplicity, we focus our what-if analyses on ad creative selection. We simulate what could have happened to immediate search response if Ford had reallocated its national TV impression-minutes across ad creative while maintaining the allocation across media placements and target audiences.For the ten pieces of Ford ad copy with creative scores, we simulate the immediate search response under the scenario in which 100% of the national TV ad impression-minutes that accrued to the ten pieces of ad copy had been allocated instead to only one piece of ad copy. Figure 3 presents, for each of the ten pieces of ad copy, the percentage differences (relative to the average across the ad copy) in generating immediate brand and price searches. The first ad copy from the left, which has the highest score in informativeness and below-average scores in likability and desirability, could have generated 15.6% more brand searches and 31.8% more price searches. However, none of the other nine pieces of ad copy could have generated more of one type of search without generating less of the other. This exercise again highlights a key takeaway: TV advertisers should be cautious if they rely on only one particular type of online response in evaluating and selecting ad creative, because it can be difficult for any single piece of ad creative to excel in driving all types of online response. Rather, TV advertisers should monitor a variety of online activities and align the performance metric with the specific objective of each campaign (e.g., brand building vs. price promotion).Graph: Figure 3. Percentage difference in search response if Ford had used only one ad copy for national TV.Notes: Each bar represents the percentage difference (relative to the average across the ten pieces of Ford ad copy for which we observe ad creative scores) in generating brand/price searches if 100% of the national TV ad impression minutes that accrued to the ten pieces of ad copy had been allocated to just one of them.To make the previous simulation more realistic, we consider an alternative scenario: What would have happened if Ford had allocated 20% of the national TV ad impression-minutes to each of the five top-performing pieces of ad creative (out of the ten)? When we use immediate brand search response as the selection criterion, the top five pieces of ad copy could have generated 9.4% more brand searches while producing only 1.5% fewer price searches. When we use immediate price search response as the selection criterion, the top five pieces of ad copy could have generated 12.5% more price searches while producing only 3.5% fewer brand searches. These simulations demonstrate that substantial gains could be made by applying our proposed modeling framework in ad creative selection. Equipped with additional information in real world applications, TV advertisers could conduct similar what-if analyses in refining their plans of media placement and audience targeting. Conclusions and Future DirectionsCompared with digital media, most TV advertisers have traditionally been unable to access behavioral response measures at the spot level, frustrating efforts to select ad creative or media placements on the basis of their relative effectiveness in achieving particular behavioral objectives. Thanks to the increasing prevalence of the second-screening phenomenon, a new class of attribution vendors has emerged, promising that TV advertisers can measure immediate post-ad spikes in online activities and use those measures to assess the relative effectiveness of ad spots.It is against this backdrop that we conducted our study. We focused on three top pickup truck brands, for which we compiled a rich data set by stitching together information from multiple sources, covering a span of nearly half a million minutes. We focused on two types of online activities: brand search and price search. We observed 27,562 ad spots on national TV and 750,672 spots on local TV. By merging the spot-level ad data with the minute-level search data, we built a comprehensive testing ground to demonstrate the worth and insights available from estimating the linkage between TV ad spots and immediate online response.Our research offers several key takeaways. First, for both brand search and price search, there is a detectable spike immediately after a regular ad insertion, be it on national or local TV. The rate of response follows the order of the brands in total search volume and market share. We believe our focal brands offer a conservative setting because they are decades old and many, if not most, category consumers are intimately familiar with them. We suspect that brands that are newer or lesser known, or transact primarily online, would likely see even greater responses.[17]Second, nearly all of the immediate response occurs within five minutes of an ad insertion, with brand search response peaking in the minute after the ad is aired and then dissipating quickly, while price search response is spread out more evenly over the five post-ad minutes.Third, in addition to generating immediate own-brand searches, national TV ad insertions also lead to significant competitor-brand searches. The category leader receives larger positive competitive spillovers than its rivals. For price search, however, we detected little competitive spillover, probably because as car shoppers approach the end of the purchase funnel, they are less likely to comparison shop between brands and more likely to comparison shop between local dealerships of the same brand for the best price.Fourth, relatively speaking, national spots appear to be more cost effective in generating immediate brand search response, whereas local spots appear to be more cost effective in generating immediate price search response. Although this reversal of relative cost effectiveness is a novel finding, it is intuitive in the sense that the three focal brands' national spots are typically more brand-oriented, whereas their local spots are mostly focused on price promotions.Fifth, ad creative with more favorable attitudinal response seems to be associated with more immediate post-ad brand searches. On average, a one-standard-deviation improvement in ad creative quality (as measured by survey-based ratings of ad informativeness, likability, and desirability) could result in a 10% to 20% improvement in post-ad brand search response. However, the moderating effects of ad creative characteristics are muted when it comes to generating immediate price searches. This suggests that TV advertisers should be cautious in replacing survey-based creative ratings with any single online response measure, especially when the indicator pertains to a lower-funnel activity such as online price quote requests.Sixth, media placement factors and audience category interest can also moderate the rate of immediate search response. TV ads ( 1) placed in the first slot of a commercial break, ( 2) aired during prime time, and ( 3) aired during professional football games cause more immediate brand and price searches. Ad spots run on broadcast networks or weekends generate significantly fewer immediate brand searches but significantly more immediate price searches. A one-percentage-point increase in audience category interest leads to a 2% increase in immediate brand search, providing support for the practice of TV advertisers relying on increasingly rich audience characteristics data for targeted media buys.Managerially, our findings about positive lifts of certain media placements (e.g., first slot, prime time, live sporting event) and audience category interest suggest that when TV advertisers intend to focus on maximizing one particular type of online response, large gains in effectiveness could accrue from quantifying and balancing the multiplier effects of various media and audience factors against their cost differentials. That said, the findings about divergent effects of broadcast/cable, weekend/weekday, national/local, and ad creative characteristics on brand versus price search caution advertisers against relying on any single immediate online response metric in assessing media placements and ad copy, as there is unlikely to be a media plan or ad creative that would be optimal for all types of online response.Practically, unlike the proprietary methods used by advertising attribution vendors, our proposed framework for modeling behavioral response at the minute level is transparent and readily replicable. The brand search data used to estimate the model are accessible to any brand, both for itself and for its competitors. The price search data represent a type of online response that has not been studied in the prior literature. Admittedly, because our sources for search data (Google and Autometrics) are unlikely to capture all the relevant search responses, our estimates of response rates are likely downward biased. The estimates of elasticities and moderating effects should be more robust to the fact that our data are unlikely a census of brand and price searches.TV advertisers could further extend our modeling framework to include website traffic, online transactions, social media activities (as in [13]]) or other important behavioral indicators that vary at the minute level. We suspect that the reliability of spot-level attribution will depend on the signal-to-noise ratio. The strength of the signal will depend on the size of the ad audience and the tendency of ad viewers to respond immediately, which can be weaker, for example, for brands that compete in low-involvement categories. The level of noise shall depend on variability, relative to the mean, of minute-by-minute online activity. One way to overcome a low signal-to-noise ratio is to include a large number of ad spots over an extended period of time, as we demonstrated in the current study. Directions for Future ResearchA deeper understanding of immediate online response to TV ad spots opens up multiple areas for further research. While advertisers may ultimately care about the impact of advertising on sales, it remains a challenge for many TV advertisers, such as the ones in the current study, to quantify the impact of any regular TV spot on sales because consumers can be exposed to a myriad of ads and promotions from both online and offline sources over weeks or even months. To close the attribution loop, following immediate online responses through to purchases or other types of transactions is a critically important step forward. Future research needs to address the question of whether the rate of immediate online response is positively correlated with the amount of online and offline response accrued over time and, ultimately, with incremental sales attributable to a single spot. If such positive correlation could be established, TV advertisers could be more confident in the validity of using the relative sizes of immediate online response to assess the relative effectiveness of different ad spots.Besides driving sales in the near term, TV campaigns often have long-term brand-building goals. Although our study has examined the correlation between three survey-based attitudinal measures and immediate search response, it remains a fertile ground to systematically investigate the relationship between an ad's efficacy in changing various mindset metrics (e.g., awareness, value and quality perception) and its efficacy in generating different online activities.Methodologically, the current study relies solely on a time-based identification strategy to detect the immediate effects of TV ads on online search. A powerful direction for future research would be to combine time-based identification with a spatial identification strategy, as exemplified by [15]. This seems applicable to national advertisements, as exogenous variation across time zones may allow for even more accurate predictions of counterfactual online response. Similarly, dividing online response by geographic origin and merging local response with local ad exposure may greatly enhance the signal-to-noise ratio.The dependable and sizable influence of TV ads on online brand and price search cautions marketers against the use of simplistic ""last-touch"" attribution strategies, as they may overestimate the effect of search engine marketing and underestimate the generative influence of TV advertising. Traditional and digital advertising budgets are still commonly divided between siloed agencies with little or no coordination between them. The TV-to-online spillover observed in this study renews the call for holistic integration and evaluation of ad campaigns and cross-media synergies (e.g., [23]; [31]).To conclude, our study contributes to a larger effort to understand how measurable funnel actions correspond to the reach, placement, and content of TV advertising. It could be fruitful to extend the current literature on TV-to-online spillovers to other broadcast media, such as radio, where the same fundamental challenge of spot-level ad performance assessment and attribution exists. We are confident that the drive for marketing accountability will continue and that multitaskers' immediate online response to traditional advertising will be prominently featured as marketers refine their understanding of how advertising affects the customer journey. "
45,"Improving Cancer Outreach Effectiveness Through Targeting and Economic Assessments: Insights from a Randomized Field Experiment Patients at risk for hepatocellular carcinoma or liver cancer should undergo semiannual screening tests to facilitate early detection, effective treatment options at lower cost, better recovery prognosis, and higher life expectancy. Health care institutions invest in direct-to-patient outreach marketing to encourage regular screening. They ask the following questions: ( 1) Does the effectiveness of outreach vary among patients and over time?; ( 2) What is the return on outreach?; and ( 3) Can patient-level targeted outreach increase the return? The authors use a multiperiod, randomized field experiment involving 1,800 patients. Overall, relative to the usual-care condition, outreach alone (outreach with patient navigation) increases screening completion rates by 10–20 (13–24) percentage points. Causal forests demonstrate that patient-level treatment effects vary substantially across periods and by patients' demographics, health status, visit history, health system accessibility, and neighborhood socioeconomic status, thereby facilitating the implementation of the targeted outreach program. A simulation shows that the targeted outreach program improves the return on the randomized outreach program by 74%–96% or $1.6 million to $2 million. Thus, outreach marketing provides a substantial positive payoff to the health care system.KEYWORDS_SPLITIn 2018, over 1.7 million new cases of cancer were diagnosed in the United States, and the cost of cancer care surpassed $147 billion ([49]). Following the guidelines of the [50], health care institutions encourage at-risk patients to undergo regular screening, as this opens the door for early detection, more cost-effective treatment options, and better recovery prognosis. Regular screening reduces mortality rates for lung (20% drop; [33]), breast (20%–40% drop; [52]), and liver (37% drop; [71]) cancers. Moreover, cancer screening can reduce annual treatment costs by nearly $5,000 ([10]).Health care institutions invest heavily in direct-to-patient outreach interventions to increase screening completion among at-risk patients. For example, Johns Hopkins Hospital's cancer center uses emails, letters, seminars, and community events to encourage screening completion among patients ([34]). With 1.7 million outreach interventions launched in 2015, and $123 million spent on prevention and education efforts,[ 5] only 8% of U.S. adults over 35 years old utilize preventive services ([13]). This percentage is too low. Health care institutions face three challenges in improving outreach effectiveness.First, most studies examine only the main effects of medical interventions (e.g., [57]; [58]), neglecting variation due to patient demographics, health status, visit history, health system accessibility, and neighborhood socioeconomic status ([26]). By incorporating this heterogeneity in patient response to outreach interventions, health care institutions can implement ""personalized health care marketing"" and boost outreach effectiveness. Second, medical scholars typically compare the relative efficacy of outreach interventions using a single-period research design (e.g., [ 8]). Given the importance of regular screening compliance over multiple periods ([19]), it is critical to evaluate screening compliance over multiple periods. Third, quantifying the return on outreach interventions to incorporate the health benefits and financial cost of interventions will help health care institutions communicate the tangible value they bring to the community and enable funding agencies to sustain these interventions ([ 2]). As the director of cancer education for the Stanford Cancer Center notes, ""durable, long-term solutions will require a substantial investment in academic/community partnerships to improve cancer education"" ([21]).To addresses these three challenges, we use a multiperiod randomized field experiment conducted at a large hospital system with at-risk patients for hepatocellular carcinoma (HCC), the most common type of primary liver cancer ([58]). Patients were randomly assigned (1:1:1) to three conditions: usual care, outreach alone, or outreach with patient navigation. Usual care is the baseline condition in which physicians offer preventive care recommendations at their discretion during a patient's usual care visits. As we describe subsequently, outreach alone and outreach with patient navigation provide two different levels of marketing using outreach mails, outreach calls, and customized motivational education by trained patient navigators. The focal outcome is the patient's screening completion status within 6 months (Period 1), 6–12 months (Period 2), and 12–18 months (Period 3) of the initial randomization. This enables an investigation of the impact of outreach interventions on regular screening compliance. We evaluated screening completion status every 6 months, as this interval has been demonstrated to increase early detection and survival compared with longer screening intervals ([56]). To incorporate patient heterogeneity, we iteratively construct the focal covariates based on the extant medical literature and pragmatic considerations that the study design affords, including patients' demographics, health status, visit history, health system accessibility, neighborhood socioeconomic status, and prior screening compliance.Relative to the baseline condition, outreach alone (outreach with patient navigation) increases screening completion rates by 10–20 (13–24) percentage points, but the effectiveness of the two outreach interventions does not significantly differ. Central to this article, the similarity in these main effects masks considerable heterogeneity in outreach effectiveness due to patient-level differences ([11]). We uncover patient-level treatment effects of these two interventions using causal forests, a state-of-the-art development in the machine learning and economics literature ([66]). Results show the following: ( 1) compared with outreach alone, outreach with patient navigation induces a higher proportion of patients with significant positive heterogeneous treatment effects in Periods 2 (9%) and 3 (23%); ( 2) the increased screening completion from outreach alone or outreach with patient navigation is higher for patients who are female, are part of a racial/ethnic minority, have a better health status, have a more frequent visit history, are covered by medical-assistance insurance, reside in closer proximity to clinics, and reside in a more populated neighborhood; ( 3) the increase in screening completion due to outreach alone is higher for patients who are younger, commute faster, and reside in a neighborhood with more public insurance coverage; in contrast, the increase in screening completion as a result of outreach with patient navigation is higher for patients who are older and reside in a higher-income neighborhood.Incorporating patient-level differences in their responsiveness to outreach interventions, and a well-established scheme of cost–benefit calculation that quantifies health benefits and financial costs associated with outreach interventions (e.g., [28]), we assign patients to the baseline, outreach-alone, or outreach-with-patient-navigation condition in each period on the basis of their predicted treatment effect and predicted net return. As a result, the commensurate return on the patient-level targeted outreach program is $3,704,270–$4,167,419 when extrapolated to 3,217 eligible patients in the hospital's database. The targeted outreach program improves the return on the randomized outreach program ($2,130,921) by 74%–96%.We make several contributions to marketing theory and health care practice. First, the literature on marketing interventions in health care (Table 1, Panel A) typically relies on experimentally manipulated moderators, such as test accuracy ([41]) or consumer goals ([68]); while theoretically interesting, they are impractical for health care institutions to implement. Health care institutions can readily utilize observable patient characteristics—such as ethnicity, visit history, and insurance coverage—that are of theoretical relevance. The bulk of the marketing literature has focused on attitudinal consequences using self-reports of behavioral intentions (e.g., [12]), risk perceptions (e.g., [47]), and attitudes (e.g., [ 9]) in a lab setting. While insightful, they are of little practical relevance to addressing actual health care behaviors such as screening completion among real patients.GraphTable 1. Literature Review on Cancer Care and Preventive Care.  1 a The authors note that they were unable to find the heterogeneity of treatment effect due to the small sample size in each subgroup.2 Notes: References are provided in Web Appendix H.Second, we extend the medical literature on cancer outreach effectiveness, which has focused primarily on the main effects of cancer outreach interventions from randomized field studies (Table 1, Panel B). The causal forests approach provides a practical way to improve the efficacy and external validity of field experiments by systematically exploring the treatment-effect heterogeneity across intervention types, across patient subgroups, and over time without prespecifying the sources of heterogeneity ([23]; [42]).Third, we provide insights into what patient subgroup benefits more (less) from outreach interventions, offer ways to customize the interventions, and help practitioners allocate limited financial resources to those with the largest potential gains. For example, while outreach programs typically target diverse, socioeconomically difficult-to-reach disadvantaged patient populations to improve their health outcomes ([58]), patients more responsive to outreach interventions tend to be female, be part of a minority, be in good health status, have more frequent visit history, be covered by medical-assistance insurance, reside in closer proximity to clinics, and reside in more populated neighborhoods. Thus, simply targeting one or two patient characteristics may not maximize the gains from the outreach interventions.Fourth, our approach provides a roadmap for implementing personalized health care marketing by customizing outreach interventions and quantifying the return on such interventions. Using patient-level treatment effect estimates with valid confidence intervals, we not only provide a tool that can recommend the most suitable intervention for each patient given their profile but also provide an individual-level cost–benefit analysis to measure the return on personalized health care marketing investments. Institutional Setting, Study Design, and Data Institutional Setting: Cancer Outreach and the Importance of Regular ScreeningOur field experiment is based on the cancer outreach efforts of a large hospital system to increase regular screening completion for early detection of HCC, the most common form of liver cancer, among patients with higher risk of HCC. Most patients with liver cancer do not display symptoms until it reaches an advanced stage; they often miss the time window during which treatment options, such as transplant and surgical resection, are effective. The five-year survival rate for early-stage liver cancer patients who undergo surgery is 60%–70%, while the five-year relative survival rate for liver cancer is 18% ([ 1]). Yet, the utilization rate of HCC screening is below 20% in the general cirrhotic population, and even lower among low-socioeconomic-status and non-Caucasian patients ([59]).The outreach program was designed to promote regular screening (i.e., obtain a screening every six months). The six-month screening interval is in line with the evidence-based recommendations issued by the American Association for the Study of Liver Diseases and National Comprehensive Cancer Network (e.g., [44]; [65]). It was initially based on tumor doubling times and ( 1) is better for early detection than longer intervals (e.g., 12 months; [56]) but worse than shorter intervals (e.g., 3 months; [63]) and ( 2) minimizes patient and provider burden ([15]).The hospital system conducted a randomized trial between December 2014 and March 2017. The study was approved by the University of Texas Southwestern Medical Center Institutional Review Board. The trial protocol is available on clinicaltrials.gov (NCT02312817), where the study is registered. The random assignment (1:1:1) consisted of one baseline condition (no outreach) and two conditions with outreach interventions (outreach alone and outreach with patient navigation), with the outcome being HCC screening completion status.[ 6] Study Design SampleThe eligibility criteria for patient inclusion using established norms have been developed in the medical field (for details, see Web Appendix A1). From the 3,217 eligible patients in the hospital's database, 1,800 patients were randomly selected for the study.[ 7] Focal independent variable: intervention typeAs we summarize in Figure 1, each patient was randomly assigned to one of three conditions in a 1:1:1 ratio: No outreach or usual care (baseline condition)  : Patients received visit-based HCC screening as recommended by primary or specialty care providers and were not contacted by the outreach marketing team. For patients who scheduled ultrasounds, the hospital system placed automated reminder telephone calls two days before the ultrasound appointments. Outreach-alone intervention  : As in the baseline condition, patients were eligible for usual care, as offered through their usual outpatient encounters. Patients were also mailed a one-page letter, which contained information on the risk of HCC in patients with cirrhosis and the benefits and risks of HCC screening, a brief summary of the screening procedure, and a recommendation to the patient to make an appointment for an ultrasound (for details, see Web Appendix A2). To increase participation, the staff then made outreach calls to nonresponders (i.e., patients with returned mail and those who did not respond to mailed invitations within two to four weeks). During telephone calls, trained research staff followed standardized scripts. Mails and telephone calls were in English or Spanish, depending on patients' preferences. In addition, the hospital system placed automated reminder telephone calls two days before appointments for patients who scheduled ultrasounds. Outreach-with-patient-navigation intervention:  Like patients in the baseline condition and those in the outreach-alone condition, patients in this condition were eligible for care as offered through their usual outpatient encounters. Patients in this condition had an experience identical to those in the outreach-alone condition, with two additions: ( 1) a telephone script used during outreach telephone calls and ( 2) an additional reminder call from the research staff. During telephone calls, if patients in this condition declined to make an appointment for screening, the research staff used a standardized telephone script to identify potential barriers and then provided customized motivational messages to encourage screening participation. Examples of barriers include preparation involved, pain during the test, and so on (for details, see Web Appendix A3). For instance, if a patient is concerned about the preparation required for the screening, the research staff alleviates this concern by stating, ""A liver ultrasound is a quick procedure. The ultrasound usually takes less than 30 minutes and the appointment should take around one hour from start to finish."" For scheduled ultrasounds, the hospital system's research staff called the patients five to seven days before the appointments to provide a reminder, address any concerns, and reschedule the appointment if needed. For these patients, the hospital system also placed automated reminder telephone calls two days before the ultrasound appointments. Overall, as shown in Figure 1, this condition is the most intense and comprehensive intervention in the study.Graph: Figure 1. Study design: Cancer outreach interventions.aThe following patients were excluded: 185,539 patients who did not meet cirrhosis criterion, 9,921 patients with comorbid conditions or with Child C cirrhosis, 405 patients with the history of HCC or suspicious mass on imaging, 78 patients whose language is not English or Spanish, and 42 patients with no contact information.bPatients in the outreach-alone and outreach-with-patient-navigation conditions could receive usual care. Multiperiod study design and sample sizes in periods 2 and 3To encourage regular screening completion, the study repeated the outreach-alone and outreach-with-patient-navigation interventions during each of the three periods. We define Period 1 as the time within 6 months of the first randomization, Period 2 as the time between month 6 and month 12 since the first randomization, and Period 3 as the time between month 12 and month 18 since the first randomization. In summary, the hospital system undertook the outreach interventions in all three periods, each period being six months apart, and each patient belonging to the same condition across the three periods. The goal of this design is to encourage screening in each period.Once a patient has completed the screening in the first period, the patient does not exit the pool and is contacted in the second and third six-month periods. There are two exceptions to the repeated interventions: ( 1) if the patient completes the screening and is diagnosed with HCC during the experiment, the patient exits the pool as the providers must refer the patient for HCC treatment instead of routine screening; ( 2) if the patient completes the screening and dies during the course of the experiment, the patient cannot complete the screening in later periods. As a result, the sample size is 1,800 for Period 1, 1,772 for Period 2, and 1,743 for Period 3.[ 8] The sample sizes in the baseline, outreach-alone, and outreach-with-patient-navigation conditions are (600, 600, 600) for Period 1, (591, 592, 589) for Period 2, and (577, 584, 582) for Period 3. Dependent variable: screening completion statusScreening completion status is measured as a patient getting an abdominal imaging screening test ( 1) or not (0).[ 9] We observe the dependent variable for each patient in Periods 1, 2, and 3. Constructing Focal Covariates: An Iterative ApproachTaking theoretical and pragmatic considerations into account, we followed a four-step iterative approach to determine the focal covariates that inform patient heterogeneity in response to outreach interventions. This process of including covariates starts from original yet tentative variables available to researchers and is informed by a multifaceted understanding of theory models, prior studies, research questions, and practice. The approach resembles a theory-in-use process (e.g., [70]) that iteratively intertwines exploratory and confirmatory research to incorporate the interplay of heterogeneity with treatment.Step 1: Utilize original variables. We begin with the variables that are available in the electronic medical record system (EMR) and are relevant to practitioners and well-documented in medical research (and thus relevant to academic scholars).[10] These systems store and track key patients' information such as patient demographics (e.g., [69]), health status (e.g., [25]), and visit history record ([60]). As Table 1 shows, previous studies in health care have analyzed these ""ready-for-use"" variables in the EMR (e.g., [32]; [45]).Step 2: Construct theoretically relevant variables. We use the information available in the EMR to construct new variables that are not captured by the raw unrefined data but draw on theories such as health belief model and protection motivation theory (e.g., [40]; [48]). Thus, a patient's health insurance and location information proxy their insurance coverage (financial access to care) and proximity to clinics (geographical access to care). This is consistent with the research showing that health system accessibility and ""improving health system accessibility across the socio-economic spectrum"" ([ 7], p. 19) is a strategic priority for policy makers ([53]).Step 3: Explore external secondary data sources. To supplement the previous steps, we also gather additional data from external secondary sources. Socioeconomic factors can help marketing researchers develop a better understanding of understudied and underserved consumers ([43]). We collect data on each patient's neighborhood socioeconomic status—including educational attainment, income, commute time, private/public health insurance coverage, employment status, and population—by collecting zip-code-level data from American Community Survey.Step 4: Incorporate contextually relevant variables. Along with variables that are static in nature, we include each patient's screening compliance in prior periods. Incorporating cancer screening compliance across different periods ([19]) captures the temporal variation in screening completion. It also informs us how outreach effectiveness might vary due to patients' prior behavioral pattern.In summary, we include six sets of patient characteristics: ( 1) demographics including age, gender (coded as 1 if a patient is female, 0 otherwise), ethnicity (non-Hispanic Caucasian, Hispanic, Non-Hispanic African American, or other/unknown), and primary language (English, Spanish, or other); ( 2) health status, which includes Child-Pugh B (coded as 1 if Child-Pugh score is higher than 6, 0 otherwise), Charlson Comorbidity Index, presence of documented cirrhosis (coded as 1 if yes), etiology of liver disease (hepatitis C, hepatitis B, alcohol, nonalcoholic steatohepatitis, or other); ( 3) visit history, which includes the number of primary care visits in the year prior to cohort entry and receipt of hepatology care (coded as 1 if the patient received the hepatology care prior to cohort entry, 0 otherwise); ( 4) health system accessibility, which includes insurance coverage (commercial, Medicaid, medical assistance/charity, Medicare, self-pay, or unknown) and proximity to clinics (coded as 1 if there are more than three clinics in the zip code that matches the first three digits of the zip code[11] where the patient resides, 0 otherwise); ( 5) neighborhood socioeconomic status, which includes educational attainment (percentage with a bachelor's degree or higher), income (per capita income), average commute time, insurance coverage (percentage with a private or public health insurance plan), unemployment rate, and population measured at the three-digit zip code level[12]; and ( 6) screening completion status in the prior period(s) (coded as 1 if a patient completes the screening test in Periods 1 or 2, 0 otherwise). Table 2 describes each variable, its operationalization, and descriptive statistics. Web Appendix B compares the means of all variables across three conditions. Differences are statistically nonsignificant, showing that random assignment was successful.GraphTable 2. Summary Statistics.  3 a We used zip code to identify the neighborhoods. All zip-code-level covariates are aggregated to the level of the first three digits by calculating the sum (i.e., population) or mean (bachelor's degree or higher, mean travel time to work, and per capita income) across all five-digit zip codes that share the same first three digits.4 Notes: After we exclude patients who were diagnosed with HCC or deceased, the screening completion rate is 38.5% in Period 2 and 35.4% in Period 3. Model-Free EvidenceFigure 2, Panel A, shows the number of patients who completed screening in different periods. Whereas 435 patients (24%) completed the screening only once during the three periods, 660 patients (37%) did so more than once. For all three periods, more patients completed the screening with outreach alone (102) and outreach with patient navigation (134) than the usual care (36) condition. The evidence suggests that both interventions increase HCC screening.Graph: Figure 2. Model-free evidence.Figure 2, Panel B, shows screening completion rates in each condition in each period after excluding the patients who were deceased or diagnosed with HCC in the previous period(s). In Period 1, 25% in the no-outreach condition, 45% in the outreach-alone condition, and 48% in the outreach-with-patient-navigation condition underwent screening. The screening completion rate in the outreach-alone condition (difference =.198, p <.01) and the outreach-with-patient-navigation condition (difference =.232, p <.01) is significantly higher than the no-outreach condition. Results in Periods 2 and 3 show a similar pattern. Comparing the screening completion rate in the outreach-alone condition and the outreach-with-patient-navigation condition, there is no statistically significant difference in Period 1 (difference =.033, n.s.) or Period 2 (difference =.033, n.s.), and Period 3 (difference =.051, p <.10). The model-free evidence suggests that both outreach conditions outperform the baseline condition but do not differ in effectiveness relative to each other.[13] Empirical Strategy Causal Forest Estimation of Patient-Level Treatment EffectsTo draw inferences about the causal effect of different interventions, researchers typically estimate and compare the average treatment effects (i.e., main effects) of randomized interventions. Such a comparison may not consider that treatment effects vary across subgroups within and across treatment conditions. Moreover, to avoid searching for particularly responsive subgroups, medical researchers must register preanalysis protocols for clinical trials to specify which subgroups will be analyzed. Such protocols may fail to identify strong but unexpected treatment-effect heterogeneity, especially in emergent fields in which moderators are ex ante ambiguous. We use causal forests to address these two challenges ([66]). Causal forests enable nonparametric estimation of patient-level treatment effects with valid asymptotic confidence intervals, without restrictions on the number of covariates or the need for a larger number of experimental conditions or repeated measures. Causal forests also alleviate concerns regarding spurious treatment-effect heterogeneity due to searching for particularly responsive subgroups (Web Appendix D1 and D2 compare causal forests with several established approaches). Next, we outline the potential outcome framework, followed by an overview of causal forests. Potential outcome frameworkFor illustration purposes, we consider the case of one period and the outreach-alone intervention (treatment condition) compared with no outreach/usual care (control condition). For a set of independent and identically distributed patients i = 1,..., n, we observe the outcome of interest Yi (screening completion), treatment assignment Wi (i.e., whether the patient is assigned to the outreach-alone or no-outreach condition), and vector of patient characteristics Xi (e.g., patient demographics, health status). Following the potential outcome framework ([55]), for each patient i, there are two potential outcomes: if a patient is assigned to the treatment condition, we observe the outcome Yi = Yi1, and if the patient is assigned to the control condition, we observe Yi = Yi0. We define the conditional average treatment effect (CATE) (i.e., treatment effect at x) to assess whether the treatment effect is heterogeneous among subgroups: τ(x)=E[Yi1−Yi0|Xi=x]. Graph1The fundamental challenge to identifying the CATE is that we only observe one of the two potential outcomes: Yi1 and Yi0. Thus, we must invoke the assumption of unconfoundedness to estimate the CATE ([54]). As patients are randomly assigned to one of the experimental conditions, the treatment assignment Wi is independent of the potential outcomes conditional on Xi (i.e.,  {Yi1,Yi0}⊥Wi|Xi  ). This assumption implies that the treatment is as good as random within each subpopulation indexed by Xi = x. Thus, given the data (Xi, Yi, Wi), we can revise Equation 1 to the following: τ(x)=E[Yi1−Yi0|Xi=x] =E[Yi|Wi=1,Xi=x]−E[Yi|Wi=0,Xi=x]. Graph2Common approaches to estimate the function  τ(x)  include nearest neighbor matching and kernel methods, but these methods do not perform well in the presence of many covariates or complex interactions among covariates ([66]). Causal forestsCausal forests combine causal inference in economics with random forests in machine learning. Random forests ([14]) deploy supervised machine learning algorithms to achieve high out-of-sample prediction accuracy with very little tuning, particularly with high dimensional data with underlying nonlinear relationships ([31]). Random forests ( 1) build a large collection of individual decision trees such that each tree predicts the outcome variable given the vector of covariates and ( 2) average the predictions from those trees. First, each tree is trained on a bootstrap training sample (not on the original sample) with a randomly chosen subset of covariates (not with all the covariates), and it is built by recursively partitioning the chosen covariate space into splits, determining each split by minimizing the mean squared error of the prediction of outcomes in the case of regression trees. Given the tree split, each tree clusters the most similar observations into a terminal node known as a leaf. To predict the outcome of an observation outside of the estimation sample, each tree makes a prediction using the mean of outcomes in the leaf where this new observation belongs. Finally, a random forest averages the prediction from those trees.Researchers have recently adapted random forests to draw inferences. The technique known as causal forests utilizes an algorithm for flexible modeling of interactions in high dimensions by building many causal trees and averaging their predictions to estimate the treatment effect function τ(x). Causal forests provide valid asymptotic confidence intervals for the treatment effects ([66]).Given a profile of patient characteristics x, tree-based models help identify the most similar patients locally in the patient characteristics space with an adaptive neighborhood metric (i.e., similar patients are in the same leaf). [66] adapt the regression tree to estimate the within-leaf treatment effects by taking the difference between the mean outcomes of treated and control units in the same leaf: τ^(x)=1|{i:Wi=1, Xi∈L}|∑{i:Wi=1, Xi∈L}Yi−1|{i:Wi=0, Xi∈L}|∑{i:Wi=0, Xi∈L}Yi. Graph3To ensure consistency and asymptotic normality, [66] prove a bias-reducing condition called honesty: a tree achieves honesty if each bootstrap training sample only uses the outcome of interest Yi to estimate the within-leaf treatment effect based on Equation 3 or to determine where to split the covariate space, but not both. In other words, the bootstrap training sample is further split into two subsamples: one used to build the tree (i.e., understand where the treatment heterogeneity is given the vector of covariates),[14] and the other used to estimate the treatment effects given the tree structure.Using this process, causal forests produce an ensemble of B such trees ([14]; [66]), each of which outputs an estimate  τ^b(x)  and averages the predictions from those trees to compute an estimated CATE:   τ^(x)=B-1∑b=1Bτ^b(x)  .This aggregation scheme also helps reduce variance and smooths sharp decision boundaries ([16]). The variance estimate of causal forests is defined as follows ([24]; [66]; [67]): V^(x)=n-1n(nn-s)2B-1∑i=1nCOV[τ^b(x),Nib]2, Graph4where  τ^b(x)  is the treatment effect estimate from the bth tree. Nib ∈ {0, 1} indicates whether the bootstrap training sample i is used for the tree b, n(n − 1)/(n − s)2 is a finite-sample correction for forests grown by subsampling without replacement, and the covariance is taken with respect to all B trees in the forest. Equations 3 and 4 produce a treatment effect estimate and a confidence interval for each patient.In marketing, causal forests have been applied in the context of customer retention ([ 5]), information disclosure and physician payments ([30]), and adoption of voice-activated shopping devices and consumers' purchase quantity, spending amount, and search activities ([61]). To our knowledge, this is the first study to use causal forests in the context of randomized health care field experiments. Application to our contextFollowing [58], we have two different treatment conditions (outreach alone and outreach with patient navigation) and three different periods. We use the following procedure to perform six causal forest estimations. Additional aspects of the estimation are summarized in Web Appendix D3.Step 1. Using patient characteristics as covariates,[15] we applied causal forests to obtain each patient's treatment effect estimate in the sample that includes patients in the baseline (condition 1, sample size = 600) and those in the outreach-alone condition (condition 2, sample size = 600) in Period 1. For each patient i in condition 1 in Period 1, the patient-level treatment effect estimate is  τ^i,P1,1→21  (i.e., the difference between the outcome we observe for the patient i in condition 1 and the outcome that would be realized if this patient were in condition 2); for each patient in condition 2 in Period 1, the patient-level treatment effect estimate is  τ^i,P1,2→12  (i.e., the difference between the outcome we observe for the patient i in condition 2 and the outcome that would be realized if this patient were in condition 1). We term this first causal forest estimation  ForestP112  , where P1 refers to Period 1, and the superscript 12 refers to the comparison of the baseline condition ( 1) and the outreach-alone condition ( 2).Step 2. After excluding the patients who were deceased or diagnosed with HCC in the previous period(s), we repeated Step 1 to obtain  τ^i,P2,1→21  and  τ^i,P2,2→12  (condition 1, sample size = 591; condition 2, sample size = 592) in Period 2 and  τ^i,P3,1→21  and  τ^i,P3,2→12  (condition 1, sample size = 577; condition 2, sample size = 584) in Period 3. As discussed, we included one (two) additional covariate(s) indicating whether a patient has completed the screening test in the prior period(s) in the causal forest estimation of Period 2 ( 3). We term these second and third causal forests  ForestP212  and  ForestP312  , where P2 and P3 refer to Period 2 and Period 3, respectively, and the superscript 12 refers to the comparison of the baseline condition ( 1) and the outreach-alone condition ( 2).Step 3. We repeated Step 1 to obtain each patient's treatment effect estimate in the sample that includes patients in the baseline (condition 1, sample size = 600) and those in the outreach-with-patient-navigation condition (condition 3, sample size = 600) in Period 1. For each patient in condition 1, the patient-level treatment effect estimate is  τ^i,P1,1→31  (i.e., the difference between the outcome we observe for the patient i in condition 1 and the outcome that would be realized if this patient were in condition 3); for each patient in condition 3, the patient-level treatment effect estimate is  τ^i,P1,3→13  (i.e., the difference between the outcome we observe for the patient i in condition 3 and the outcome that would be realized if this patient were in condition 1). We term this fourth causal forest  ForestP113  , where P1 refers to Period 1, and the superscript 13 refers to the comparison of the baseline condition ( 1) and outreach-with-patient-navigation condition ( 3).Step 4. We repeated Step 2 to obtain  τ^i,P2,1→31  and  τ^i,P2,3→13  in Period 2 (condition 1, sample size = 591; condition 3, sample size = 589) and  τ^i,P3,1→31  and  τ^i,P3,3→13  (condition 1, sample size = 577; condition 3, sample size = 582) in Period 3. We term these fifth and sixth causal forests  ForestP213  and  ForestP313  , where P2 and P3 refer to Period 2 and Period 3, respectively, and the superscript 13 refers to the comparison of the baseline condition ( 1) and the outreach-with-patient-navigation condition ( 3).Table 3 and Figure 3 show the distribution of the patient-level treatment effect estimates based on the causal forest estimation. Relative to the baseline condition, outreach alone (outreach with patient navigation) increases screening completion rate by between 10 and 20 (13 and 24) percentage points (Table 3, Panel A). Causal forests enable us to construct confidence intervals for patient-level treatment effect estimates. As we report in Table 3, Panel B, outreach-alone intervention induces positive and statistically significant treatment effects among 100%, 74%, and 66% of the patients in Periods 1, 2, and 3, respectively (p <.05), while the outreach-with-patient-navigation intervention does so among 100%, 83%, and 89% of the patients in Periods 1, 2, and 3, respectively (p <.05).GraphTable 3. Summary of Average Treatment Effects (ATEs) and Patient-Level Conditional Average Treatment Effects (CATEs) by Outreach Type.  5 a Statistical significance is at the 95% level.6 Notes: CI = confidence interval.Graph: Figure 3. Distribution of patient-level CATEs.Notes: ATE refers to the average treatment effect; SE refers to the standard error; CATEs refer to conditional average treatment effects.Germane to the focus of this article, there is substantial heterogeneity in those significant patient-level treatment effects: ( 1) compared with outreach alone, outreach-with-patient-navigation intervention induces a higher proportion of patients with significant positive treatment effect estimates in Periods 2 (83% − 74% = 9%) and 3 (89% − 66% = 23%), and ( 2) patient-level treatment effect estimates of outreach-alone (outreach-with-patient-navigation) intervention range from 5–31 (5–37) percentage points. Next, we investigate the sources of heterogeneity. Incorporating Heterogeneity in Patient-Level Treatment EffectsWe examine the treatment effect heterogeneity by correlating treatment effect estimates with patient characteristics. Accordingly, we estimated the following equations: τ^ijt={P1,P2,P3}2=α20+ α21Ageij+ α22Genderij + α23Ethnicityij+ α24Languageij + α25Child-PughBij+ α26Charlsonij + α27Cirrhosisij+ α28Etiologyij + α29Prior_visitij+ α210Hepatology_careij + α211Insurance Coverageij+ α212Proximityij + α213Educationj+ α214Incomej + α215Commutej+ α216Privatej + α217Publicj+ α218Unemployj + α219Populationj+ ηt+ε2ijt, Graph5a τ^ijt={P1,P2,P3}3= α30+ α31Ageij+ α32Genderij + α33Ethnicityij+ α34Languageij + α35Child-PughBij+ α36Charlsonij + α37Cirrhosisij+ α38Etiologyij + α39Prior_visitij+ α310Hepatology_careij + α311Insurance Coverageij+ α312Proximityij + α313Educationj+ α314Incomej + α315Commutej+ α316Privatej + α317Publicj+ α318Unemployj + α319Populationj+ηt+ε3ijt, Graph5bwhere  τ^ijt={P1,P2,P3}2  and  τ^ijt={P1,P2,P3}3  refer to patient-level treatment effect estimates of outreach alone and those of outreach with patient navigation, j denotes the three-digit zip code, and t denotes the period. We pooled the estimates across periods and included period-fixed effects (ηt) to capture common time-varying observables that may affect them and clustered standard errors at the patient level to allow for heteroskedasticity and correlated errors within patients over time.Columns 1 and 2 of Table 4 report the results showing the sources of heterogeneity in patient-level treatment effects. Next, we discuss the patient characteristics associated with the treatment effect heterogeneity.GraphTable 4. Sources of Heterogeneity in Patient-Level CATEs.  7 *p <.05.8 **p <.01.9 ***p <.001.10 Notes: The baseline categories of main effects are male, non-Hispanic Caucasian, Hepatitis C, English, medical assistance/charity, and Period 1. We scaled continuous variables to zero mean and unit variance. As we pooled the estimates of three periods, sample sizes are 3,544 (1,200 + 1,183 + 1,161) and 3,539 (1,200 + 1,180 + 1,159) (see sample size columns of Table 3, Panel B). AgeOlder patients are less responsive to the outreach-alone intervention than younger patients (  α^21   = −.003, p <.001) but they are more responsive to outreach-with-patient-navigation intervention than younger patients (  α^31  =.006, p <.001). A possible explanation is that older adults prefer to use information that is customized to their needs rather than generic information that can be overwhelming ([20]), which makes them less responsive to direct mails than younger adults ([35]). The interactive and personalized nature of the navigation over the telephone provides targeted and useful information to older patients, making it more effective ([37]). GenderFemale patients are more responsive to both outreach interventions than male patients (outreach alone:  α^22  =.014, p <.001; outreach with patient navigation:  α^32  =.007, p <.001). This is likely due to the higher prevention and loss-minimization focus among women ([64]). According to agency-communion theory ([18]), men focus on maximizing gains while women focus on minimizing the downside potential of their decision. Outreach messages for cancer screening, by design, approach health care from a prevention and loss-minimization focus. Ethnicity and languageHispanic patients are more responsive to both outreach interventions than Caucasian patients (outreach alone:  α^23 H =.011, p <.001; outreach with patient navigation:  α^33 H =.004, p <.001). Likewise, non-Hispanic African American patients are more responsive to both outreach interventions than Caucasian patients (outreach alone:  α^23 AA =.010, p <.001; outreach with patient navigation:  α^33 AA =.011, p <.001). Similarly, patients whose primary language is Spanish are more responsive to both outreach interventions than those whose primary language is English (outreach alone:  α^24  =.014, p <.001; outreach with patient navigation:  α^34  =.015, p <.001). Due to language and access barriers, such patients may have relatively fewer opportunities to learn about the health screening information than ethnic majority groups ([17]; [62]). Given the lower baseline access, outreach interventions that provide information on screening opportunities should be more effective among such groups than their counterparts ([39]). Health statusPatients in a poorer health status (those with Child-Pugh B) are less responsive to both outreach interventions than patients with a better health status (outreach alone:  α^25  = −.012, p <.001; outreach with patient navigation:  α^35  = −.003, p <.001). The pattern is consistent when Charlson Comorbidity Index and the presence of documented cirrhosis are used as indicators of health status. A possible explanation is that outreach interventions might make patients fearful of finding out they have cancer ([ 4]) and experience death anxiety ([29]). Those with poor health will experience higher death anxiety due to lower optimism about their health ([ 3]), which reduces adaptive coping and thus decreases the utilization of health care services ([48]). We also find that compared with patients with Hepatitis C, those with Hepatitis B are less responsive to outreach alone (coefficients ranging from −.004 to −.002), but this is not the case for outreach with patient navigation. Visit historyPatients with a higher number of prior primary care visits are more responsive to both outreach interventions than those with fewer prior primary care visits (outreach alone:  α^29  =.010, p <.001; outreach with patient navigation:  α^39  =.011 p <.001). Similarly, patients who previously received hepatology care are more responsive to both outreach interventions than patients with no prior hepatology care (outreach alone:  α^210  =.003, p <.001; outreach with patient navigation:  α^310   =.003, p <.05). At its core, a patient's prior visit history signifies the extent to which a patient has a favorable attitude toward utilizing health care services to pursue their health goals ([38]) and has familiarity with the utilization process ([27]). This should motivate patients to get screened. Health system accessibilityPatients with insurance coverage through medical assistance/charity are generally more responsive to both outreach interventions than patients with other types of insurance (outreach alone:  α^211  = ranging from −.016 to −.009, p <.001; outreach with patient navigation:  α^211  = ranging from −.008 to −.001, p <.001 through n.s.). Patients who receive health care at a low cost due to medical assistance/charity, with access to the corresponding insurance, are more likely to respond to outreach interventions because of their ability to overcome financial hardships to utilize screening services. A patient's ease of accessing health care services is based on not only their ability to pay for the service but also their proximity to health care providers. We find that patients with closer proximity to care are more responsive to both outreach interventions than patients with further proximity to care (outreach alone:  α^212  =.008, p <.001; outreach with patient navigation:  α^312  =.005, p <.01). Neighborhood socioeconomic statusPatients who live in more educated neighborhoods are not necessarily more or less responsive to interventions (outreach alone:  α^213  =.002, n.s.; outreach with patient navigation:  α^313  = −.011, n.s.). Yet patients who reside in a higher-income neighborhoods are more responsive to outreach with patient navigation (outreach alone:  α^214  = −.000, n.s.; outreach with patient navigation:  α^314  =.012, p <.01), which implies that those in low-income neighborhoods are less responsive to this intervention. Patients in low-income neighborhoods face unique challenges such as higher rates of obesity, chronic disease, environmental pollutants, and incarceration ([36]). The prevalent health and environmental challenges in these communities might cause anxiety among community members and lead them to be pessimistic about their health ([22]), thus making them less responsive to outreach intervention. Patients in neighborhoods with longer average commute times are less responsive to outreach alone (outreach alone:  α^215  = −.004, p <.001), but this is no longer the case for outreach with patient navigation (outreach with patient navigation:  α^315  = −.000, n.s.). Patient navigation alleviates perceived costs associated with a screening by providing the information on the estimated duration for the appointment, so patients who live in a highly trafficked community will no longer show resistance to a screening.While patient-level insurance coverage should capture the impact of health system accessibility, the neighborhood-level health insurance coverage can also offer additional insights. Patients' responsiveness to the outreach interventions does not vary by the degree of private health insurance coverage in their neighborhood (outreach alone:  α^216  = −.000, n.s.; outreach with patient navigation:  α^316  =.002, n.s.). However, patients in a neighborhood with a greater public health insurance coverage are more responsive to the outreach-alone intervention but not to the outreach-with-patient-navigation intervention (outreach alone:  α^217  =.011, p <.01; outreach with patient navigation:  α^317  =.006, n.s.).Neighborhood unemployment rate does not significantly affect patients' responsiveness to the interventions (outreach alone:  α^218  =.002, n.s.; outreach with patient navigation:  α^318  =.003, n.s.). Yet patients from neighborhoods with more dense populations are more responsive to both interventions (outreach alone:  α^319  =.005, p <.05; outreach with patient navigation:  α^219  =.005, p <.05), implying that patients in rural areas are less responsive to interventions. These results bear a notable caveat: a potential aggregation bias due to the measurement of neighborhood variables at the three-digit-zip-code level may have led to the null effect. Additional post hoc analysis combining patient characteristicsAs described in Columns 3 and 4 of Table 4, we explore possible combinations of patient characteristics with the interactions between prior primary care visits and other patient characteristics. This is akin to examining higher-order interactions in an analysis of variance. This analysis offers several insights.The marginal benefit of additional primary care visits diminishes such that a patient's primary care visit has a nonlinear effect on outreach intervention effectiveness. Referring to Columns 3 and 4 of Table 4, there is a positive linear coefficient (outreach alone: b =.020, p <.001; outreach with patient navigation: b =.022, p <.001) and a negative quadratic coefficient (outreach alone: b2 = −.003, p <.001; outreach with patient navigation: b2 = −.004, p <.001) for the effect. Jointly, the coefficients capture diminishing returns such that a patient's first few primary care visits yield large marginal returns. Given the importance of the initial visits, health care professionals can enhance outreach effectiveness by targeting patients who have made fewer than numerous visits in the past.The interactions between prior primary care visits and patient characteristics can help practitioners further identify the responsive subgroups. For example, Spanish-speaking patients' responsiveness to outreach interventions is attenuated as primary care visits increase (coefficient = −.005 p <.001 for both outreach alone and outreach with patient navigation). It could be that Spanish-speaking patients perceive that outreach interventions are less informative than primary care visits. Practitioners may target Spanish-speaking patients who have no prior primary care visits in the past. Patients with Child-Pugh B are even less responsive to outreach interventions as primary care visits increase (outreach alone: coefficient = −.003 p <.001; outreach with patient navigation: coefficient = −.003, p <.01), suggesting that increased primary care visits compound the perception of a fear and death anxiety of having cancer triggered by outreach interventions and thus decrease the use of screening. Overall, our post hoc analysis highlights the need to understand how outreach effectiveness varies by the combination of patient characteristics. Dynamics in the Proportion of Treatment Effects that Are Statistically SignificantThe dynamics in the proportion of treatment effects that are statistically significant across each condition display an interesting pattern. As Figure E1 in Web Appendix E shows, there is very little heterogeneity in Period 1 in terms of the proportion of treatment effects that are statistically significant. Specifically, 100% (100%) of the treatment effects due to outreach alone (outreach with patient navigation) are statistically significant in Period 1. However, compared with outreach alone, outreach with patient navigation induces a higher proportion of patients with significant positive treatment effect estimates in Period 2 (83% vs. 74%) and Period 3 (89% vs. 66%). The effectiveness of the outreach with patient navigation relative to outreach alone improves over time. Medical and health care professionals believe that exposure to outreach nurtures the motivation for screening compliance, which is the crucial first step. Once patients are compliant to screening, potential barriers to getting screening may be addressed in subsequent periods. Thus, it is possible that the heterogeneity in the proportion of significant treatment effects could be related to the repetition of the treatment over different periods.As a thought experiment, we conducted an analysis investigating the extent of heterogeneity in the proportion of treatment effects that are statistically significant when we ""turn off"" the repeated nature of the treatment. In this analysis, we define the dependent variable as whether a patient undergoes a screening at least once in the short (0–6 months), medium (0–12 months), and long (0–18 months) runs. The goal of this approach is to study whether interventions can bring at-risk patients in for a screening at least once during the three periods investigated. We also wanted to understand if heterogeneity, in terms of proportion of treatment effects that are statistically significant, manifests when we ""turn off"" the repeated nature of the treatment. Web Appendix Figure E2 reports the screening completion rates and plots the patient-level treatment effects, and Table E2 reports the summary of the treatment effects across each condition. The main takeaways are as follows:There is little heterogeneity in the proportion of treatment effects that are statistically significant. Web Appendix Table E2 reports that outreach-alone intervention induces positive and statistically significant treatment effects among 100%, 100%, and 99.9% of the patients in Periods 1, 2, and 3, respectively (p <.05), while the outreach-with-patient-navigation intervention does so among 100%, 99.7%, and 98.5% of the patients in Periods 1, 2, and 3, respectively (p <.05).There remains substantial heterogeneity in the magnitude of treatment effects. Web Appendix Table E2 reports that patient-level treatment effect estimates of outreach-alone (outreach-with-patient-navigation) intervention range from 10–32 (11–37) percentage points.Web Appendix Figures E1, E2, and Table E2 jointly show that the heterogeneity in the proportion of treatment effects that are statistically significant is related to the dynamics induced by the repeated nature of the treatment. Future research should investigate the sources of these dynamics. Return on Cancer Outreach InterventionsWe evaluate the return on outreach interventions among patients and across three periods: Returnk=∑t=13∑i=1Nt={1,2,3}{Pr(Screeningikt=1)×[Benefitikt−Screening Costikt−Pr(Early Tumorikt=1|Screeningikt=1)× Treatment Costikt]−Pr(Screeningikt= 0) × Opportunity Costikt−Outreach Costikt }, Graph6where  Pr(Screeningikt=1)  refers to the probability that patient i assigned to outreach type k completes the screening in period t. If a patient completes the screening test: ○ The health care institution generates Benefitikt for patient i receiving intervention type k in period t, captured by the quality-adjusted life years of a patient attributable to the screening (typically expressed in the financial value in the medical literature). ○ The health care institution incurs Screening Costikt for patient i receiving intervention type k in period t, which includes the costs of an ultrasound/MRI/CT test or a combination of these tests (i.e., each patient can complete multiple tests). ○ Conditional on being detected with an early tumor, the health care provider incurs Treatment Costikt for patient i intervention type k in period t, which includes the costs of tumor resection, liver transplantation, and local ablative therapies. If a patient does not complete the screening test: ○ The health care institution incurs Opportunity Costikt if patient i receiving intervention type k in period t develops advanced HCC, which creates costs. Irrespective of whether the patient completes the screening test: ○ Outreach Costikt is incurred if the health care institution employs an outreach program. The outreach costs are higher for the outreach-with-patient-navigation than the outreach-alone condition and are zero for the baseline condition.Research has documented that HCC screening completion with biannual ultrasound extends patients' quality-adjusted life expectancy by 1.3 months, and HCC screening utilization with MRI does so by 2 months ([28]). Patients may complete an ultrasound, an MRI, a CT scan, or a mix of these tests. We assume the average quality-adjusted life expectancy to be 1.65 months for each patient who completes the screening. The medical literature posits that the financial value per quality-adjusted life year is $50,000 ([ 2]; [28]). Thus, total benefits can be obtained by multiplying the number of quality-adjusted life years by the financial value per quality-adjusted life year ($50,000) (i.e., multiply total number of patients who complete the screening by average quality-adjusted life expectancy).Table 5 presents the results of the benefit–cost calculation using Equation 6 for each condition in each period. We use the observed values in the data (e.g., actual number of patients who visit) in conjunction with parameters (e.g., early detection rate) from the medical literature to calculate the return in each condition in each period. Web Appendix F documents details on parameters from the medical literature.GraphTable 5. Return on Nontargeted Outreach Interventions.  11 Notes: We extrapolated the calculation to the 3,217 patients eligible for randomization. Outreach costs = Number of call hours × Cost per hour. Screening costs = Number of ultrasounds completed × Cost per ultrasound + Number of CTs/MRIs completed × Average unit cost (CT/MRI). Treatment costs = Early detection probability × Average cost of treatment (if detected early). Opportunity costs = Annual HCC probability × Annual cost of advanced HCC. Health benefits = Quality-adjusted life gain × Financial value per quality-adjusted life year. No-outreach conditionWe observe that 150/600, 156/591, and 126/577 patients in no-outreach condition completed the screening in Periods 1, 2, and 3, respectively. The total benefits of the no-outreach condition across the three periods are estimated to be $2,970,000.The screening costs are the total costs of ultrasound, CT, and MRI tests completed. The number of ultrasound (CT and MRI) tests completed is 127 (69), 149 (68), and 113 (59) in Periods 1, 2, and 3. The cost per ultrasound is $143, while the average cost of CT and MRI is $1,020. Thus, the total screening costs are estimated to be $255,547. Focusing on treatment costs, 5% of the total number of screening tests typically result in early tumor detection. The average treatment cost per patient for early tumor detection is $74,397 ([28]). Given that 150/600, 156/591, and 126/577 patients in the no-outreach condition completed the screening in Periods 1, 2, and 3, 5% of them would undergo treatment costs, giving a total treatment cost of $1,606,967. Opportunity cost is incurred if patients who have not completed the screening develop advanced HCC. The annual cost of advanced HCC is $41,320, and the annual HCC probability is 2.9% ([28]). Multiplying the number of patients who have not completed the screening in the no-outreach condition by the probability of HCC (2.9%) gives us a total opportunity cost of $1,600,902. Finally, outreach costs are zero in the no-outreach condition. Subtracting the total cost from the total benefit, the total return in the no-outreach condition is −$493,416, which translates to a loss of $840 per patient to the health care system. Outreach-alone conditionCompared with the no-outreach condition, there are higher benefits in the outreach-alone case ($5,183,750 vs. $2,970,000), as there are 269/600 patients, 254/592 patients, and 231/584 patients who have completed the screening in Periods 1, 2, and 3, respectively. Using the same approach as the one used for no-outreach condition to calculate the costs, the screening costs, treatment costs, and opportunity costs, respectively, are $323,919, $2,804,752, and $1,224,642 in the outreach-alone condition. In addition, the total number of hours devoted to outreach calls is 3,477, 2,607, and 2,135. Assuming a $15 hourly wage, the total outreach cost in the outreach-alone condition is $123,285. Thus, the total return in the outreach-alone condition is $707,152, which translates to a gain of $1,192 per patient to the health care system. Outreach-with-patient-navigation conditionCompared with the no-outreach condition, there are higher benefits in the outreach-with-patient-navigation condition ($5,651,250 vs. $2,970,000), as there are 289/600 patients, 273/589 patients, and 260/582 patients who would complete the screening. Following the same calculation approach, the screening costs, treatment costs, opportunity costs, and outreach costs are $332,871, $3,057,700, $1,137,168, and $157,548, respectively, in the outreach-with-patient-navigation condition. Thus, the total return in the outreach-with-patient-navigation condition is $965,963, which is substantially greater than that in the no-outreach condition and translates to a gain of $1,635 per patient to the health care system. Summary of returns on nontargeted cancer outreach interventionsNo outreach results in a net loss of $840 per patient to the medical hospital, whereas outreach alone (outreach with patient navigation) generates a monetary gain of $1,192 ($1,635) per patient. When extrapolated to the 3,217 patients eligible for randomization from the hospital's patient database, the cancer intervention results in a loss of $900,718 from no outreach or usual care, a gain of $1,278,402 from outreach alone, and a gain of $1,753,237 from outreach with patient navigation. In this scenario, the total gain is $2,130,921. Return on Patient-Level Targeted Cancer Outreach Interventions: A SimulationThus far, the calculation of return on cancer interventions has been based on the random assignment of patients and on patients remaining in the same condition over three periods. However, ( 1) there is heterogeneity in patient-level treatment effects of outreach-alone intervention and in those of outreach-with-patient-navigation intervention, ( 2) not all patient-level treatment effect estimates are statistically greater than 0, ( 3) treatment effect heterogeneity varies across periods, and ( 4) the net return on outreach interventions varies across intervention types and over time. As such, given each patient's characteristics in a particular period, outreach with patient navigation is unlikely to be uniformly more effective than outreach alone. This poses two questions: ( 1) Given each patient's observed characteristics, which intervention type is most suitable for each patient? and ( 2) For the same patient, does the most suitable intervention vary across periods? Accordingly, we conduct a simulation that assigns each patient to the most suitable condition in each period based on two types of allocation schemes: ( 1) predicted treatment effect and ( 2) predicted net return (see detailed procedure in Web Appendix G). Recommended allocation based on predicted treatment effectConceptually, in each period, given each patient's profile, we compare each patient's treatment effect estimate in their corresponding condition (e.g., condition 2) with their simulated treatment effect estimate in the counterfactual condition (e.g., condition 3). Then we assign this patient to the best-suited intervention that generates a significantly higher treatment effect estimate (e.g., condition 3, p <.05). If none of these estimates is significantly larger than 0, we assign this patient to condition 1.As we show in Table 6, Panel A, the recommended allocation for each period is as follows: Period 1: 0%, 99.9%, and.1% of patients are assigned to the no-outreach, outreach-alone, and outreach-with-patient-navigation conditions, respectively. Period 2: 9.0%, 74.0%, and 16.9% of patients in each condition, respectively. Period 3: 8.4%, 66.9%, and 24.7% of patients in each condition, respectively.GraphTable 6. Simulation Results Based on Patient-Level Treatment Effect Estimates.  There are four noteworthy takeaways from this recommendation. First, the recommended split deviates from the original allocation based on the randomized controlled trial (1:1:1), suggesting that targeting induces asymmetric allocation of patients to different conditions. Second, there is a fraction of patients who stay in the baseline condition in Periods 2 and 3. For these patients, neither of the interventions is more effective than the baseline. Third, we reallocate most patients to the outreach-alone condition, suggesting that health care institutions can achieve the same level of effectiveness by aligning only moderate outreach efforts with these patients. Fourth, over time, the outreach-with-patient-navigation condition seems to be more effective, given the higher allocation to this condition (.1% in Period 1, 16.9% in Period 2, and 24.7% in Period 3).Table 6, Panel B, shows the return on patient-level targeted outreach interventions. When extrapolated to the 3,217 patients eligible for randomization from the hospital's patient database, patient-level targeted outreach program across conditions generates a gain of $1,547,662, $1,204,318, and $952,290 in Periods 1, 2, and 3, respectively. The total net return on patient-level targeted outreach program is $3,704,270, or 74% higher than that on nontargeted outreach program based on the random assignment ($3,704,270–$2,130,921 = $1,573,349). Recommended allocation based on predicted net returnWhat if the health care system aims to maximize the overall return derived from assigning each patient to the most suitable intervention? As such, we can assign each patient to the intervention that gives the highest predicted net return based on Equation 6 rather than only the highest predicted treatment effect. Specifically, in each period, we compare each patient's predicted net return in the corresponding condition (e.g., condition 2) with their simulated patient's estimated net return in the counterfactual condition (e.g., condition 3). Then we assign the patient to the best-suited intervention that generates a significantly higher net return (e.g., condition 3, p <.05). If none of these estimates is significantly larger than 0, we assign this patient to condition 2 because the net return in the baseline condition is negative across all three periods (recall Table 5).As we show in Table 7, Panel A, the recommended allocation for each period is: Period 1: 0%, 87.2%, and 12.8% of patients are assigned to the no-outreach, outreach-alone, and outreach-with-patient-navigation conditions respectively. Period 2: 0%, 79.3%, and 20.7% of patients in each condition, respectively. Period 3: 0%, 68.7%, and 31.3% of patients in each condition, respectively.GraphTable 7. Simulation Results Based on Patient-Level Estimated Return.  There are two takeaways from this recommendation. First, no patient stays in the baseline condition under this allocation scheme, reflecting the goal of maximizing overall return. Second, while we still reallocate most patients to the outreach-alone condition, the outreach-with-patient-navigation intervention seems to be even more effective over time given the higher allocation to this condition than the previous allocation (e.g., 31.3% vs. 24.7% in Period 3).Table 7, Panel B, shows the return on patient-level targeted outreach interventions. When extrapolated to the 3,217 patients eligible for randomization from the hospital's patient database, the total net return on the patient-level targeted outreach program is $4,167,419, or 96% higher than that on the nontargeted outreach program based on the random assignment ($4,167,419 − $2,130,921 = $2,036,498). In summary, patient-level targeted outreach interventions improve the payoffs to the health care system by 74%−96%, or $1.6 million to $2 million. DiscussionThe difference in allocation based on predicted treatment effect versus predicted net return shows the versatility of our approach in providing practical guidance to medical professionals and policy makers. The nature and magnitude of benefits can shift based on the goals that a health care institution sets for itself. Using this approach, an organization can set its strategic goals to maximize the benefits from personalized outreach. These results also confirm that the cumulated benefits from repeated and upgraded health education through outreach with patient navigation can be enhanced using individually tailored outreach over time. DiscussionRelying only on the main-effects analysis, scholars might conclude that the outreach with patient navigation and outreach alone are equally effective. However, our application of causal forests uncovers patient heterogeneity in outreach effectiveness and leads to different conclusions and important practical implications. Specifically, patients with different characteristics respond very differently to each intervention. For example, patients who are more responsive to outreach alone or outreach with patient navigation tend to be female, be part of minority populations, be in better health status, be covered by medical assistance, have closer proximity to clinics, and reside in a populated neighborhood. Patients who are more responsive to outreach alone tend to be younger, have faster commutes, and reside in neighborhoods with more public insurance coverage. Patients responsive to outreach with patient navigation tend to be older and reside in a higher-income neighborhood. Over time, the outreach-with-patient-navigation intervention becomes more effective for an increased proportion of patients. As such, we illustrate time-varying heterogeneity in the outreach effectiveness.A cost–benefit analysis shows that the baseline condition results in a net loss of $840 per patient, whereas outreach alone (outreach with patient navigation) generates a gain of $1,192 ($1,635) per patient. When extrapolated to the 3,217 eligible patients, the total net gain of the nontargeted cancer outreach program across conditions is $2,130,921, which implies that outreach marketing provides a substantial positive payoff to the health care system. Our simulation shows that targeted outreach interventions can enhance this return by 74%−96%. Research ImplicationsFor the marketing discipline, this article provides a framework for better understanding and analyzing sufficiently powered field experiments that are based on random assignment of heterogeneous customers to different treatments. Instead of focusing only on the main effects of the treatment or a subset of individual-level covariates, causal forests flexibly predict personalized treatment effects based on high-dimensional, nonlinear functions of those covariates. Such an approach also obviates the need for choosing several one-way interactions a priori to test for heterogeneity or searching over many interactions for particularly responsive subgroups. Accordingly, this article provides a methodological solution to the field's concern of external validity. Many empirical findings are typically much less generalizable than we imagine, because researchers lack a process and corresponding insights to identify moderators (i.e., the interaction of treatment and unmodeled/unmanipulated background factors) ([23]; [42]).For the emerging discipline of personalized health care, we show that causal forests can identify particularly responsive subgroups without the need for a larger number of experimental conditions. While modern health care has implemented personalized medicine using genetic information, most health care outreach and educational programs still rely on untailored communications. Practitioners who manage these programs should recognize that the use of a large number of patient characteristics can substantially improve the outreach responsiveness through a tailored approach.Our research also responds to a recent call for boundary-breaking marketing-relevant research ([43]) in several ways. First, our covariates are motivated by ""real-world phenomena, rather than the constructs and theories in the marketing"" (p. 11). Our findings that treatment effects vary across covariates not only engage ""academics in other disciplines"" (p. 1) but also offer important implications to the extant literature and theory going forward. Second, our findings have ""life and death implications"" (p. 9)—they help detect liver cancer at early stages. Third, our covariates, such as ethnicity, language, insurance coverage, and neighborhood socioeconomic status, elucidate how outreach effectiveness may vary among ""understudied consumers such as minorities, privileged or impoverished classes, and marginalized consumers (e.g., special needs populations)"" (p. 5). Patient-Centric Health Care Marketing ImplicationsWe urge hospitals and medical centers with outreach programs to leverage patient information to improve the effectiveness of outreach investments. Hospitals and health care practitioners should realize that a ""one-size-fits-all"" outreach program is neither effective nor economic. The use of machine learning can power data-driven patient-centric outreach programs that are also dynamically adaptive. Practitioners should consider both cross-sectional and temporal adaptation of outreach programs to maximize the benefit of health care interventions.We urge policy makers in the federal, state, and local health departments, American Hospital Association, American Cancer Society, and American Liver Foundation to financially support personalized outreach programs. More hospitals should reach out to the underrepresented populations as they are more responsive to outreach messages; however, this requires additional resources and training. Incentivizing hospitals to reach out to patients with varying personal, clinical, structural, and socioeconomic backgrounds can also be effective. They should also engage a multidisciplinary group from health care, marketing, computer science, and other disciplines to fund an accumulation of comprehensive databases to facilitate even better targeting of patients to improve outcomes. Limitations and Future ResearchFirst, because patients have different barriers to screening, future research should test the effectiveness of different barrier-reduction strategies by analyzing the nature of communication between patients and the staff with the use of call recordings. Second, our study focuses on the endpoint outcome: screening completion. Future research could apply the notion of customer journey to disentangle which parts of the intervention (e.g., barrier discussion during an outreach call vs. reminder calls) are more effective at not only increasing completion but also reducing no-show rates or time to response, further enhancing the return. Third, although we track individual patients, outreach designed to serve an individual may have influenced other members of the household. Future research could study possible spillover effects of outreach interventions. "
