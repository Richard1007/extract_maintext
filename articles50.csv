Number,Body
1,"A New Livestream Retail Analytics Framework to Assess the Sales Impact of Emotional Displays At the intersection of technology and marketing, this study develops a framework to unobtrusively detect salespeople's faces and simultaneously extract six emotions: happiness, sadness, surprise, anger, fear, and disgust. The authors analyze 99,451 sales pitches on a livestream retailing platform and match them with actual sales transactions. Results reveal that each emotional display, including happiness, uniformly exhibits a negative U-shaped effect on sales over time. The maximum sales resistance appears in the middle rather than at the beginning or end of sales pitches. Taken together, the results show that in one-to-many screen-mediated communications, salespeople should sell with a straight face. In addition, the authors derive closed-form formulae for the optimal allocation of the presence of a face and emotional displays over the presentation span. In contrast to the U-shaped effects, the optimal face presence wanes at the start, gradually builds to a crescendo, and eventually ebbs. Finally, the study shows how to objectively rank salespeople and circumvent biases in performance appraisals, thereby making novel contributions to people analytics. This research integrates new types of data and methods, key theoretical insights, and important managerial implications to inform the expanding opportunity that livestream e-commerce presents to marketers to create, communicate, deliver, and capture value.Keywords: deep learning; emotions; face detection; livestream e-commerce; salesperson effectivenessLivestream retailing augments traditional go-to-market strategies by reaching consumers via screen-mediated sales presentations for a variety of products. Amazon Live, Facebook Live, Taobao Live, and QVC serve as prominent exemplars. This type of retailing blends technology and marketing: the technology integrates video stream broadcast platforms, electronic payment systems, and forward and reverse logistics for efficient delivery and hassle-free returns, and the marketing combines entertainment and retailing, enhances reach via influencer marketing, shortens the purchase journey to the duration of the sales presentation, and permits value capture via innovative payment plans. In a typical video sales pitch, a host (salesperson) nudges a prospective customer through the purchase funnel by building awareness of an item's features, benefits, price, and discounts, as well as instilling urgency to buy. For example, on Taobao Live, which reaches over 37 million Chinese viewers monthly, top influencer Wei Ya helped Procter & Gamble accelerate its customers' journey from awareness to purchase and Tesla generate customer leads ([19]).The continuous stream of sales presentation videos can be captured using advanced computing capabilities ([40]; [59]). Using video footage, marketing scientists can apply artificial intelligence technologies to automatically detect a salesperson's face in each frame, extract emotional expressions, and relate them to actual customers' behavioral data —all unobtrusively at a large scale— to generate novel insights ([34]), thereby augmenting the sparse knowledge on the business impact of emotional displays.Recent studies have investigated the effects of emotional displays on various marketing metrics. For example, [50] offer the first study to automatically extract the emotions of joy and surprise that viewers experience when watching television commercials and then relate these emotions to attention and ad avoidance behavior. Similarly, [32] examine the impact of emotional displays on sales using the Facial Action Coding System ([13]) to categorize a set of emotions (e.g., happiness, surprise, disgust) based on facial expressions of viewers watching movie trailers, compute the average watching intention for each movie trailer, and relate it to box-office revenues. They find that viewers' emotional displays of happiness positively influence both watching intentions and box-office revenues. However, although [32] incorporate prospective consumers' emotional displays into their study, the role of emotional displays on the seller side of the exchange dyad remains an unexplored cue that shapes consumers' purchases. Indeed, in their survey of the literature on emotions, [ 3], p. 184) emphasize that ""much of what we do know is confined to consumer behavior, as opposed to the behavior of salespeople or marketing managers.""To our knowledge, the current study is the first to assess the sales impact of product, price, sales force, advertising, and promotion in the presence of a salesperson's face and emotional displays. Specifically, we address the following knowledge gaps: Do salespeople's emotions in livestream presentations impact sales? If so, to what extent? How do the effects of emotions vary over an item's presentation span? What is the optimal allocation of face presence and emotions over the presentation span?To answer these and related questions, we collaborate with a livestream retailer that broadcasts television shows 24 hours each day of the week, deploys salespeople to deliver live sales presentations of hedonic products, receives payments by credit cards, and ships orders by mail. A typical show lasts one hour and presents about eight items. We analyze the video data consisting of 62.32 million frames over two years. To put this scale in perspective, this footage exceeds two million 30-second TV ads. Then, we apply two machine-learning algorithms: real-time face detection ([56]) and real-time emotion classification ([ 1]). Specifically, the face detection algorithm discovers the presence or absence of a face in every frame of the video, while the emotions classifier (based on a convolutional neural network with rectified linear unit activation) assigns probabilities to the facial expressions in each frame with a face. Thus, we unobtrusively extract the display of emotions of each salesperson in one-to-many screen-mediated marketing communications in consumer markets.Next, across 99,451 sales pitches, we match the salespeople's six emotional displays —happiness, sadness, surprise, anger, fear, and disgust— to how long each item was shown, the product category to which it belongs, the number of units sold, the price charged, and whether shipping fees were waived. Finally, we extend marketing mix models on two frontiers: the inclusion of emotional displays and salespeople's effectiveness. We emphasize that the literature on marketing mix models is vast, as is the literature on emotions; however, they do not overlap. This study bridges the two distinct domains.Our analysis of large-scale video data shows that salespeople's emotions negatively impact sales across all six emotions, including happiness. The magnitude of sales decline across all the emotions is.47%, which is more than double the free-shipping effect (.20%). Happiness constitutes more than one-third of the total sales decline. Thus, we uncover a new maxim: sell with a straight face (i.e., reduce facial expressions).Furthermore, the level of the optimal face presence reduces over the initial 10% span, then gradually increases as the presentation progresses, and subsequently tapers down in the last 15% span. Finally, most marketing mix models ignore the role of the sales force (e.g., [ 2]; [39]), and when they do include it (e.g., [16]), the sales force variable is operationalized at an aggregate level (e.g., number of salespeople, number of calls). Consequently, companies are limited in their insights into an individual salesperson's effectiveness. By contrast, the proposed framework uses person-specific dummy variables to estimate individual salesperson's impact, yielding valuable information on salespeople's performance, which circumvents managers' cognitive biases (e.g., homophily) in recognizing excellence and identifying candidates for retraining, thus contributing to people analytics in a sales setting. Next, we describe the conceptual background needed to interpret the empirical results. Conceptual BackgroundFacial expressions are social displays that a sender strategically deploys to elicit a desired response from a receiver ([11]; [15]). In other words, facial expressions are ""declarations that signify our trajectory in a given social interaction, that is ... what we would like the other to do"" ([15], p. 130). They are social communicative moves that serve as ""tools for social influence"" ([11], p. 393).A sender may mask true intentions. The onus is, therefore, on the receiver to decipher the sender's intent. The Emotions as Social Information (EASI) model ([51], [52]; [53]) asserts that buyers scrutinize the seller's expressions in commercial exchanges in an attempt to discern the seller's strategic intentions. For example, [57] show that sellers sporting a broad smile during an encounter are perceived as less competent and that perceived incompetence is more likely to be evident among prevention-focused customers and in high-risk consumption settings. Furthermore, their field study in a crowdfunding context reveals that a project creator with a broad smile is perceived as less competent, which reduces the total amount of money pledged for a project, the total number of large-scale donations made by backers, and the average amount of money pledged per backer. Similarly, [10] find that displays of intense happiness (e.g., a broad smile) by customer-facing employees can undermine trust and reduce satisfaction with the product.[44], [45], [46]) provides a theoretical explanation for why receivers are likely to draw certain inferences with respect to a sender's facial expression, and the EASI model offers clues on how receivers are likely to react to the cue. Salespeople's expressions elicit customers' inferences about sellers' characteristics such as competence, trustworthiness, and persuasive intent. Such inferences, in turn, impact customers' purchasing behaviors. Drawing on extant theory, Table 1 presents the seller's facial expressions, intent, consumers' inferences about sellers, and consumers' action tendency with examples. According to [53], consumers' action tendencies are to ( 1) move toward (i.e., consumers experience a positive emotional reaction toward the influence attempt and thereby seek to cooperate with the seller); ( 2) move away (i.e., consumers experience a slightly negative emotional reaction toward the influence attempt and thereby seek to temporarily ignore or avoid the sender); or ( 3) move against (i.e., consumers experience a highly negative emotional reaction toward the influence attempt and thereby seek to terminate the interaction). Thus, Table 1 explains how salespeople's emotional displays trigger buyers' inferences in one-to-many broadcast communications.GraphTable 1. The Implications of a Sender's Strategic Social Communicative Moves. Seller's Facial Expressiona,bSeller's IntentaConsumers' Inference about SellerConsumer Action Tendency Illustration of Consumer Action TendencyHappiness (smile)Influence consumer to affiliateA seller's happiness may be taken as a sign that the seller is gaining in the negotiation at the target's expense (Van Kleef et al. 2010).Move againstAn entrepreneur displaying a broad (slight) smile in a profile photo on a website is likely to receive less (more) financial backing for a crowdfunded project (Wang et al. 2017).Sadness (pouting)Influence consumer to provide supportA seller's sadness may be taken as a sign that the seller is recruiting succor (Scarantino 2017a) and trying to get consumers to lower their guard.Move away or againstA service provider displaying intense sadness during a cell phone purchase is likely to result in the customer registering lower satisfaction with the product received and service (Cheshin, Amin, and Van Kleef 2018).Surprise (startled)Influence consumer to engageA seller's surprise may be taken as a sign that the seller is trying to garner attention/liking in an attempt to make them more amenable to persuasion.Move toward or awayA host on livestream video displaying surprise to customers is likely to garner greater interest in an offering. Alternatively, the surprise may serve as an unwelcome distraction and cause the customer to lose interest in the item.Anger (scowling)Influence consumer to submitA seller's anger may be taken as a sign that the seller is losing in the negotiation and engaging in aggressive action to reassert dominance (Fridlund 1994).Move toward or awayA manager displaying anger in response to employees' competence-based violations diminishes perceptions of leader effectiveness (Wang et al. 2018).Fear (gasping)Influence consumer to help and protectA seller's fear may be taken as a sign that the agent is recruiting empathy and trying to get consumers to lower their guard.Move away or againstA fear appeal in a television ad (e.g., Nationwide Insurance's ""Make Safe Happen"" 2015 Super Bowl Ad featuring a young boy who is no longer alive) led to negative social media posts and reduced liking of the ad (Bharadwaj, Ballings, and Naik 2020).Disgust (nose wrinkling)Influence consumer to reject current situationA seller's disgust may be taken as a sign that the seller is seeking to violate behavioral norms in the negotiation (Heerdink et al. 2019).Move away or againstA third-party observer of scenario featuring disgust (vs. neutral expression) finds that it triggers inferences that a norm has been violated (Heerdink et al. 2019). 1 a[11].2 b[53].More specifically, Table 1 provides theoretical bases to interpret our empirical results. First, positive facial expression (i.e., happiness) invokes the action tendency to move against. In a competitive buyer–seller exchange setting, a seller's happiness expression engenders consumers' inference that the seller is gaining an advantage, thereby reducing the seller's trustworthiness as well as consumers' purchasing tendency ([51], [52]; [53]) — a finding consistent with [57] that a seller's broad smile results in the inference of low competence and the action tendency of reduced buying activity. Second, negative facial expressions (i.e., sadness, anger, fear, and disgust) invoke the action tendency to move away. For instance, a seller's sadness expression in a selling context invokes the consumers' inference of garnering empathy as an attempt to lower their guard, which can be off-putting to consumers, who might then either ignore or avoid the seller. Consistent with this expectation, [10]) show that the display of sadness by frontline employees undermines trust and lowers satisfaction. Last, surprise can be either a positive or negative facial expression: consumers can infer that the seller is trying to garner attention, which invokes the action tendency to either move toward or move against, depending on whether consumers believe the expression is appropriate. Livestream Retail Analytics FrameworkThe proposed framework circumvents limitations such as simulated interactions in laboratory studies, survey-based studies relying on self-reports, manual observation and coding of facial displays, small sample sizes, limited set of emotional displays, and lack of business metrics as the response variable. For example, [25] investigate the impact of service representatives' happiness expressions on subjects in lab settings (using trained student actors). [17] conduct surveys with airline passengers' assessments of flight attendants, and [27] rely on surveys completed by sales managers self-reporting their own ability to perceive their salespeople's emotions. [42] manually evaluates a small sample of bank employees' smiles in salesperson–customer service encounters. These studies lack the full spectrum of emotional displays and sales performance as the response. [26], a notable exception, seek to understand the content effects on sales. Specifically, they analyze a small sample of 275 sales pitches from Home Shopping Network and incorporate minute-by-minute cumulative sales; however, they ignore the role of facial expressions. To circumvent the aforementioned limitations, we develop a framework that unobtrusively collects nonsimulated market interactions, does not rely on self-reports, does not require manual observation or coding of facial displays, involves a large sample size, covers a broad spectrum of six emotional displays, and, most importantly, uses sales transactions as the response variable. Figure 1 presents the ten-step framework to capture and analyze the structured and unstructured data from livestream retailing.Graph: Figure 1. Livestream retail analytics framework. Data CaptureThe first two pennants in Figure 1 list three steps that pertain to data capture. Transaction data contain structured information such as quantity of items sold, prices of items, duration of display, shipping cost, and product category. Video footage of salespeople's presentations offers the unstructured data. We process the video footage as follows. Each video frame is a colored image with a resolution of 480  ×  360 pixels. For every second of the video footage, we select a frame, convert it to grayscale, and present it to a pretrained OpenCV frontal face detection model based on the Haar cascade algorithm ([56]). For each detected face, the grayscale frame bounded by the face's region of interest is forwarded to an emotion classification model to infer the emotional state of the salesperson by producing probabilities for happiness, sadness, surprise, anger, fear, and disgust. We classify emotional displays using a pretrained mini-Xception model developed by [ 1]). Thus, we unobtrusively extract data on whether a face exists in 62.32 million frames and the probabilities of emotional displays. The third pennant in Figure 1 uses time stamps (i.e., the start and end times of item displays) to compute the display duration. We match 25,565 distinct items across 6,065 shows to the accounting data on orders placed, selling prices, and free shipping waivers. Analytics PipelineThe fourth pennant in Figure 1 consists of dynamic time warping (step 4), dimension reduction (step 5), feature engineering (step 6), and mixed models (step 7). To understand dynamic time warping, let  Eisk(t)  denote the time pattern of the emotional display  k,  where  k=1,...,6  for an item i in show s. Data analysis commonly uses variable transformation such as squaring, which alters the magnitude of  Eisk(t)  but keeps its x values (time) unaltered. In contrast, we transform  Eisk(t)  by shifting, stretching, or shrinking the time argument, denoted by t in  Eisk(t)  , but keeping its magnitude (y values) the same. This shifting, stretching, or shrinking applies to each item i in show s and emotion k. For example, Sin(t) and Cos(t) are different curves, yet if we replace t in Cos(t) by (  π2−t)  , we shift the cosine curve along the time dimension to overlap it with the sine curve exactly. The function  ϕ(t)=π2−t  is called a ""warping"" function that performs shifting; however, more generally, it can stretch or shrink time, differentially at various instants, to align curves more closely with each other with respect to their landmarks such as peaks, valleys, and inflection points. This process of dynamic time warping is also known as curve registration or landmark alignment. The resulting aligned curves serve as inputs for analysis rather than the raw curves  Eisk(t)  . Subsequently, we present the empirical results with and without curve alignment to understand the benefits of this optional step.Dimension reduction enables us to capture the dynamic effects of emotional displays on quantity sold. Specifically, for each item-show, the time t in  Eisk(t)  spans over 30 epochs, which are defined as 1/30th of the total duration of item i displayed in show s. Consequently, we have 180 additional variables (i.e., 30 epochs for six emotions). Furthermore, we generate an additional 180 variables by including its quadratic terms and yet another 360 variables by interacting them with price and promotion. To maintain parsimony and mitigate collinearity, we extract the principal component to capture ""happiness"" (say, when  k=1  ) via the principal scores  zkis=∑t=130e^ktEisk(t)  , where  e^k=(e^k,1,e^k,2,...,e^k,30)′  is the principal eigenvector that reduces the dimensionality from 30 epochs to the scalar  zkis  . Then, we regress the quantity sold  Qis  on the principal scores  zkis  to estimate the trajectory of sales impact of an emotional display together with its confidence intervals. For details, see the Web Appendix.Feature engineering supplements the new features to represent the quadratic and interaction effects. To this end, we included  zkis2  for each emotion k and interaction terms such as  zkis×Xis  , where  Xis  denotes a moderating variable of interest (e.g., price). To generate the outputs listed in the fifth pennant in Figure 1, we formulate a set of mixed models. Model DevelopmentFigure 2 illustrates how marketing mix — product, price, sales force, display duration (advertising), and free shipping (promotion) — together with face presence and emotional displays (happiness, sadness, surprise, anger, fear, and disgust) affect the focal outcome representing customers' purchase behavior (i.e., sales). In Model 1, we formulate the marketing mix model with marketing mix variables, time effects, and random effects for items and shows. Model 2 extends Model 1 by incorporating the face presence and six emotional displays. Model 3 adds the quadratic effects of emotional displays. Model 4 further augments Model 3 with interactions of price and promotion with the emotional displays.Graph: Figure 2. Modeling framework. Model 1: Incorporating Salespeople and Time Effects in Marketing Mix ModelsWe investigate how the number of units of an item sold on a given show varies with the item's price, its duration of display, whether free shipping was waived, the product category to which it belongs, the salesperson who presented it, and the time effects (day effect, week effect, and year). The model specification is as follows: Ln(Qis)=β1Ln(Pis)+β2Ln(Dis)+β3Sis+β4Cis+∑j=122λjHjs+τ′Tis+μ0+μ1i+μ2s+ϵis, Graph( 1)where  Qis  denotes the quantity sold of an item i in the show s with (  i,s)=1,⋯,N=99,451  ;  Pis  represents the item's price in dollars;  Dis  is the display duration in seconds;  Sis  captures free shipping (  Sis=1)  or not  (Sis=0)  ; and  Cis={0,1}  indicate one of the two types of products (whose names are not disclosed for confidentiality). These five variables represent the proxies for the traditional marketing mix variables: product, price, sales force, marketing communications (i.e., length of ad), and promotion (i.e., free shipping). Given the log-log specification,    the parameters (  β1,β2)  respectively yield the price and duration elasticity, which quantifies the percentage sales impact associated with a 1% increase in price or duration. The parameters (  β3,β4)  measure the percentage change in sales due to free shipping and product category. In addition,  Hs  is a  22×1  dummy vector, with unity for the element j and zero elsewhere, that identifies individual salesperson j hosting the show s. The corresponding  λj  furnishes the sales lift due to various salespeople,  j=1,...,22  , relative to the baseline salesperson 23. A single salesperson owns the entire show in our data. The parameters  (μ0,μ1i,μ2s,ϵis)  represent the fixed intercept, random intercept for items, random intercept for shows, and the usual zero mean and constant variance normal error term, respectively. The random effects parsimoniously reflect the variability about the intercept  μ0  due to heterogeneous impact of items and shows.Time flows across 62.32 million seconds of the video footage in our analysis, and it exhibits periodicity for the seconds across days and for the days across weeks. To clarify, consider time in seconds since midnight. A total of 86,400 seconds elapse by the midnight of the next day, and then the clock resets to zero (00:00:00 hours). At 23:59:59 hours, the elapsed time is 86,399 seconds, and it is 5 seconds at 00:00:05 hour. Although the instants 23:59:59 and 00:00:05 differ by just 6 seconds, these two instants would be represented as if they are 86,394 seconds apart under a linear scale. Thus, to account for periodicity of the days and weeks, sine and cosine terms should be used as follows. Let  tis  represent the seconds of a day when an item i in show s is displayed, where the full day of 86,400 seconds equals 360˚ or  2π  radians. Then the two periodic regressors for the day effect are  Dis=(Sin[2πtis86400],Cos[2πtis86400])′  . Similarly, let  dis  represent the day of a week when an item i in show s is displayed, where the full week equals seven days. Then the two periodic regressors for the week effect are  Wis=(Sin[2πdis7],Cos[2πdis7])′  . Because calendar years are not periodic, unlike seconds or days, let the dummy variable  Yis  indicate the years. Thus,  Tis=(Yis,Wis′,Dis′)′  in Equation 1 includes these five regressors with the conformable parameter vector  τ  that constitutes the year effect, the week effect, and the day effect on item sales. Model 2: Incorporating the Presence of Face and Emotional DisplaysThe proposed livestream retail analytics framework provides the fraction of the frames containing a face when item i was displayed in show s, which we denote by  Fis  . In addition, when item i was displayed in show s, it furnishes the principal score  zkis  for happiness (  z1is  ), sadness (  z2is  ), surprise  (z3is  ), anger  (z4is  ), fear  (z5is  ), and disgust (  z6is  ). Incorporating them in Model 2, we extend the right-hand side (RHS) of Model 1 as follows: Ln(Qis)=α0Fis+α1z1is+α2z2is+α3z3is+α4z4is+α5z5is+α6z6is+RHS(Model1), Graph( 2)where  αk  are the effects of face presence and six emotional displays on sales. Because the score  zkis=∑t=130e^ktEisk(t)  , the sales impact  α^ke^kt  exhibits the trajectory over  t=1,...,30  epochs. Model 3: Incorporating Quadratic EffectsThe effects of emotional displays may wax and wane. For example, moderate happiness may be effective, but limited or excessive happiness display may not be. To investigate such intensity effects, we extend Model 2 by incorporating the quadratic effects of facetime and emotional displays. Then, Model 3 is given by Ln(Qis)=α0Fis+γ0Fis2+∑k=16αkzkis+∑k=16γkzkis2+RHS(Model1), Graph( 3)where  zkis=∑t=130e^ktEisk(t)  , and  γk  represent the quadratic effects for the face presence and emotional displays, respectively. Equation 3 also includes the simple effects of face presence (  α0)  and emotions (  αk)  , the marketing mix effects, and salesperson's effectiveness, time, and fixed and random intercepts via Model 1. Optimal allocation of face and emotionsWe derive the optimal face presence and emotional displays over time in the Web Appendix, which shows that the optimal number of frames to devote to face presence and each emotion k in every epoch t is given by Ft*={−α0e0t2γ0ifα0e0t>0,γ0<0,0otherwise.andEkt*={−αkekt2γkifαkekt>0,γk<0,0otherwise. Graph( 4)Thus, for every epoch t in the presentation span T, the optimal allocation of face presence is  Ft*∑tFt*=e0t∑t=1Te0t  ; and the optimal allocation for each emotional display k is  Ekt*∑tEkt*=ekt∑t=1Tekt  . To gain intuition, observe that  Ft*∝e0t  and  Et*∝ekt  in Equation 4, which reveals that face and emotions allocation are proportional to the eigenvector weights: the larger the weight, the greater the intensity of face presence or emotional expressions. Model 4: Incorporating Interaction EffectsVarious factors can moderate the impact of sellers' affective displays on customers' attitudinal and behavioral outcomes. For example, some studies investigate boundary conditions from perceivers' characteristics, such as emotional receptivity ([30]) and epistemic motivation ([57]). Others examine the moderating roles of the selling context, such as store busyness (e.g., [18]). To complement, we explore the moderating role of factors under managers' control such as price and promotion. Specifically, we augment Model 3 as follows: Ln(Qis)=∑k=16δkzkisPis+∑k=16ωkzkisSis+RHS(Model3). Graph( 5)In Equation 5, the free shipping effect equals  β3+ωkzk,  which depends on the level of emotional display,  zk  . Similarly, each emotion  zk  moderates the price elasticity  ηk=β1+δkzkP  . The preceding discussion completes the inclusion of emotions in marketing mix models. Empirical Analysis ContextA livestream retailer, whose identity remains confidential, broadcasts shows 24 hours a day, seven days a week, on its own television channel and sells exclusive hedonic products in multiple product categories. The salesperson hosting the show presents information on products and encourages viewers to place orders by telephone. Each show lasts for 60 or 120 minutes, is planned weeks in advance, and contains live sales pitches of items (i.e., not scripted or prerecorded). Besides selling, the salespeople attempt to build parasocial relationships with viewers so that they feel a bond with virtual personalities analogous to those with television celebrities or news anchors ([49]). DataOur direct-to-consumer retailer sells items from two product categories using 23 hosts as salespeople. The salesperson pitches multiple items during a show, and the item appears throughout the presentation span. We observe 99,451 sales pitches at an item-show level on salespeople's presence of face, their facial expressions, item prices, duration of display, shipping fee waivers, and, most importantly, actual sales as the dependent variable. Table 2 presents the descriptive statistics, and Tables 3 and 4 contain the estimation results for Models 1–4 obtained via the R package lme4.GraphTable 2. Descriptive Statistics. VariableDescriptionMeanSDRangeDependent VariableQuantity, QisNumber of items sold in a show69.6493.21[1, 2024]Marketing MixPrice, PisPrice of the item110.48196.22[6.29, 8,000]Duration, DisDisplay duration in seconds487.1333.09[61, 2,886]Free shipping, SisDummy variable for free shipping.21.29{0,1}Product category, CisDummy variable for two categories.04.19{0,1}Emotional DisplaysFace Presence, FisFraction of the frames with a face.19.12[0, 1]Happiness, Eis1Grand mean of probability of happiness display in all the frames with a face.23.21[0, 1]Sadness, Eis2Grand mean probability of sadness display in all the frames with a face.10.06[0,.62]Surprise, Eis3Grand mean probability of surprise display in all the frames with a face.08.12[0,.78]Anger, Eis4Grand mean probability of anger display in all the frames with a face.09.07[0,.76]Fear, Eis5Grand mean probability of fear display in all the frames with a face.11.06[0,.58]Disgust, Eis6Grand mean probability of disgust display in all the frames with a face.02.03[0,.52]Sales ForceSalesperson 1, H1Dummy variable for Salesperson 1.004.065{0,1}Salesperson 2, H2Dummy variable for Salesperson 2.004.065{0,1}Salesperson 3, H3Dummy variable for Salesperson 3.034.182{0,1}Salesperson 4, H4Dummy variable for Salesperson 4.002.044{0,1}Salesperson 5, H5Dummy variable for Salesperson 5.047.212{0,1}Salesperson 6, H6Dummy variable for Salesperson 6.036.187{0,1}Salesperson 7, H7Dummy variable for Salesperson 7.072.258{0,1}Salesperson 8, H8Dummy variable for Salesperson 8.079.270{0,1}Salesperson 9, H9Dummy variable for Salesperson 9.008.091{0,1}Salesperson 10, H10Dummy variable for Salesperson 10.000.020{0,1}Salesperson 11, H11Dummy variable for Salesperson 11.078.268{0,1}Salesperson 12, H12Dummy variable for Salesperson 12.082.274{0,1}Salesperson 13, H13Dummy variable for Salesperson 13.012.108{0,1}Salesperson 14, H14Dummy variable for Salesperson 14.126.332{0,1}Salesperson 15, H15Dummy variable for Salesperson 15.055.228{0,1}Salesperson 16, H16Dummy variable for Salesperson 16.054.226{0,1}Salesperson 17, H17Dummy variable for Salesperson 17.070.255{0,1}Salesperson 18, H18Dummy variable for Salesperson 18.052.223{0,1}Salesperson 19, H19Dummy variable for Salesperson 19.028.166{0,1}Salesperson 20, H20Dummy variable for Salesperson 20.041.198{0,1}Salesperson 21, H21Dummy variable for Salesperson 21.002.039{0,1}Salesperson 22, H22Dummy variable for Salesperson 22.040.196{0,1}Salesperson 23, H23Dummy variable for Salesperson 23.072.259{0,1}Time EffectsYear, YisYear of the display (annual)2018.50[2017, 2019]Day of week, WisDay of the display (weekly)3.982.0[1, 7]Time of day, DisSeconds since midnight4150825614[3.16, 86400] GraphTable 3. Sales Impact of Marketing Mix, Sales Force, and Time Effects. Model 1Model 2Model 3Model 4Est.t-val.Est.t-val.Est.t-val.Est.t-val.Marketing MixLn(Price), β1–.756–142.28–.765–148.54–.769–151.28–.769–150.43Ln(Duration), β2.552127.77.626132.10.671136.02.671135.98Free shipping,β3.21820.36.19818.78.19018.20.18917.97Product category, β4.4145.91.3865.47.3605.11.3575.07Sales ForceSalesperson 15, λ15.43814.92.41813.82.42714.19.42614.16Salesperson 20, λ20.4126.59.3986.18.3856.01.3856.01Salesperson 18, λ18.35111.37.38812.20.35811.32.35811.34Salesperson 12, λ12.31311.61.2378.50.2388.57.2378.57Salesperson 8, λ8.31111.01.2649.04.2327.97.2317.96Salesperson 7, λ7.2889.92.32110.69.29910.01.30010.04Salesperson 11, λ11.2709.40.2628.83.2418.16.2418.18Salesperson 19, λ19.2556.48.2997.35.2676.60.2676.60Salesperson 22, λ22.2497.34.3088.81.3038.70.3038.71Salesperson 3, λ3.2276.07.1453.75.1734.49.1734.52Salesperson 17, λ17.1695.57.1715.45.1464.70.1474.73Salesperson 21, λ21.140.84.085.49.098.57.096.56Salesperson 6, λ6.0832.37.0752.07.0922.55.0902.52Salesperson 16, λ16–.036–1.15.0641.95.0551.68.0541.68Salesperson 5, λ5–.099–2.95–.103–2.97–.057–1.67–.058–1.67Salesperson 13, λ13–.168–3.04–.200–3.50–.187–3.29–.187–3.30Salesperson 23, λ23–.183–6.11–.122–3.93–.090–2.92–.090–2.92Salesperson 14, λ14–.212–7.90–.237–8.57–.201–7.27–.200–7.25Salesperson 2, λ2–.312–3.17–.186–1.83–.179–1.77–.181–1.80Salesperson 1, λ1–.488–6.17–.495–6.08–.496–6.14–.496–6.14Salesperson 4, λ4–.539–4.22–.662–5.02–.663–5.06–.662–5.05Salesperson 10, λ10–.679–2.25–.697–2.23–.690–2.22–.688–2.22Salesperson 9, λ9–.790–11.16–.735–10.03–.749–10.29–.749–10.29Time EffectsYear Effect, τ1–.204–16.81–.180–14.63–.167–13.60–.167–13.63Week Effect (Sin), τ2–.113–13.38–.115–13.48–.112–13.24–.112–13.22Week Effect (Cos), τ3.09611.44.09911.67.10512.36.10512.37Day Effect (Sin), τ4–.292–32.77–.302–33.52–.314–34.95–.314–34.97Day Effect (Cos), τ5–.541–57.35–.563–58.98–.569–59.95–.570–59.99Intercept414.29816.94347.09713.78320.51212.80321.78012.86VIF Median (Max)1.443 (5.110)1.304 (5.172)1.604 (9.721)1.616 (9.724)Distinct Items25,56525,56525,56525,565Unique Shows6,0656,0656,0656,065 GraphTable 4. Sales Impact of Face and Emotional Displays. Model 2Model 3Model 4Estimatet-valuesEstimatet-valuesEstimatet-valuesMain EffectsFace Presence, α^0.33812.663.13843.353.14043.37Happiness, α^1–.033–40.39–.025–25.26–.024–20.23Sadness, α^2–.003–3.56–.001–.90.001.62Surprise, α^3–.001–1.07.0077.32.0087.09Anger, α^4–.033–45.82–.031–29.70–.031–26.03Fear, α^5–.005–6.58–.003–3.13–.003–2.92Disgust, α^6–.012–16.63–.011–10.47–.010–8.52Quadratic Effects (Estimate×103)Face Presence, γ^0–4,641–41.66–4,644–41.69Happiness, γ^1–.769–6.79–.764–6.73Sadness,γ^2–.296–4.31–.303–4.41Surprise, γ^3–.613–7.80–.629–7.99Anger, γ^4.010.17.017.28Fear, γ^5–.442–6.20–.440–6.17Disgust, γ^6–.060–1.93–.061–1.97Moderation Effects (Estimate×103)Happiness × Price, δ^1–.009–2.04Sadness × Price, δ^2–.012–3.55Surprise × Price, δ^3–.012–4.10Anger × Price, δ^4.0051.27Fear × Price, δ^5.001.32Disgust × Price, δ^6–.001–.27Happiness × Shipping, ω^12.248.84Sadness ×Shipping, ω^2–.411–.18Surprise × Shipping, ω^33.6991.80Anger × Shipping, ω^4–1.671–.76Fear × Shipping, ω^51.144.51Disgust × Shipping, ω^6–1.882–.82  Results Sales impact of face presence and emotional displays Face presence[31]) apply convolutional neural networks to detect the presence of a person's face in a Kickstarter crowdfunding video and show empirically that the presence of a human face makes a difference in shaping the desired funding outcomes. But does it impact sales? If so, to what extent? Our study answers these questions. The estimate of.338 in Table 4 (Model 2) means that sales increase by.34% when a face is present, an effect common to all the hosts. For a specific salesperson, say salesperson 15, the impact of sales pitch is  (.338+.418)=.756  , which means sales increase by.76%. This magnitude explains why the livestream retailer prefers live broadcasts even when items could have been posted on the internet in a faceless manner. Emotional displaysTable 4 for Model 2 partially presents the sales impact of happiness, sadness, anger, fear, and disgust. The estimates  α^k  for happiness (–.033), sadness (–.003), surprise (–.001), anger (–.033), fear (–.005), and disgust (–.012) are uniformly negative and statistically significant for all emotions except surprise. Thus, we conclude that emotional displays decrease sales.We present the dynamic pattern of emotional displays with (see Figure 3) and without (see Figure 4) dynamic time warping. These dynamic patterns emerge from the elements of the eigenvector  e^kt  across the 30 epochs  t=1,...,30  . For clarity, Figures 3 and 4 present the epochs on the unit interval. The elements of the eigenvector  e^kt  , together with the estimates  α^k  , yield the total sales impact of emotions given by  a^kt=α^ke^kt  . Summing across all the epochs, the sales impact of emotional displays are as follows: happiness (–.18%), sadness (–.02%), surprise (–.004%), anger (–.18%), fear (–.03%), and disgust (–.06%). Happiness and anger induce the largest sales decline; surprise the smallest. Summing across these emotions, the magnitude of total sales decline (.47%) is more than twice the free-shipping effect (.198%). Happiness contributes more than one-third to the total sales decline.Graph: Figure 3. Time-varying sales impact of emotional displays with dynamic time warping.Graph: Figure 4. Time-varying sales impact of emotional displays without dynamic time warping.What accounts for the negative sales impact? As discussed in the ""Conceptual Background"" section, sellers' emotional displays trigger buyers' inference and action tendencies. Specifically, Table 1 shows that positive facial expressions such as happiness negatively impact sales because consumers infer that the seller is gaining an advantage at their expense, thereby reducing sellers' trustworthiness and, in turn, buyers' tendency to purchase ([51], [52]; [53]). This expectation corroborates [57]) findings, which show that a seller's broad smile results in a buyer's inference of a seller's low competence and reduces buying activity. A similar situation occurs with politicians sporting a ""permasmile"" (i.e., maintaining a smile for an extended period of time); they are not perceived as genuine, which induces distrust and leads to lost votes ([60]). As for negative facial expressions (i.e., sadness, anger, fear, and disgust), they invoke the action tendency to move away, which corroborates [10]) finding that frontline employees' displays of sadness undermine trust and reduce satisfaction. Last, surprise can be either positive or negative, and it results in an insignificant effect on sales. Thus, this large-scale evidence supports recent studies ([10]; [57]), and so we caution that emotional displays are bad for livestream retailing business.Over an item's presentation span, the magnitude of sales impact  a^kt  builds up, attains a maximum in the middle, and recedes toward the end. Across the six emotions, this dynamic pattern holds uniformly. Are the U-shaped patterns significant? Using the expressions in the Web Appendix, we plot the confidence intervals in Figures 3 and 4. We conclude that because zero does not belong in it, except for surprise, the rest of the emotional displays, including happiness, exert significantly negative effects on sales.What accounts for the U-shaped dynamics? The literature on advertising repetition (e.g., [ 5]; [ 9]; [38]; [41]) provides a plausible interpretation. As the sales pitch progresses, the repetitiveness of facial expressions exacerbates the negative sales impact (i.e., becomes more negative) and drives it to the lowest level. After that, often due to the tedium ([ 5]) of a protracted sales pitch, viewers' attention drifts from the message-related thoughts to their own thoughts ([ 9]) of purchase consideration, namely, balancing the benefits and costs of the presented item and deciding whether to buy. Consequently, the negative effect ameliorates during purchase consideration. [38] find a similar U-shaped pattern for the effectiveness of television commercials (see their Figures 4 and 9 for chocolate and cereal brands, respectively).Graph: Figure 9. Optimal face allocation. Quadratic effectsModel 3 specifies the quadratic effects to explore whether emotional displays can be optimized. Table 4 shows that the conditions  α^k>0  and  γ^k<0  are not satisfied by happiness, sadness, anger, fear, and disgust. Consequently, their resulting optimal level  Ekt*=0  according to Equation 4. Although surprise satisfies the conditions  α^3>0  and  γ^3<0  , the salesperson cannot express only surprise throughout the presentation in the absence of other emotions; thus, this corner solution does not seem practically useful. In contrast, the face presence satisfies the conditions  α^0=3.138>>0  and  γ^0=−4.641<<0  , and the optimal  F*=−3.138(−2×4.641)=  .34. For comparison, the average face presence in Table 2 is.19. Thus, face presence should be increased from 19% to 34% to maximize sales. Moderation effectsModel 4 specifies the interactions of emotional displays with free shipping and price. Table 4 shows that the estimated  ω^k  are not significant for all k. Hence, the main effects of emotional displays hold regardless of the shipping fee waiver. Similarly, the price interaction effects of fear (  δ^4  ), anger (  δ^5  ), and disgust (  δ^6  ) are not significant, thereby generalizing their main effects across various prices.In contrast, the interaction effects of happiness (  δ^1  ), sadness (  δ^2  ), and surprise (  δ^6  ) are significant and negative. They moderate price elasticity:  ηk=∂Ln(Q)∂Ln(P)=β1+δkzkP  . Substituting  δ^1=−.009  for happiness from Table 4 and the average price of $110.48 from Table 2, we get price elasticity  η1=−.77−.99z1  , which means viewers become more price sensitive as the intensity of sellers' happiness increases. Why? Because the buyers suspect that the seller is gaining at their expense ([53]), and they exhibit ""move against"" tendencies (see Table 1). The qualitatively similar results hold for sadness and surprise. These interaction effects generalize our previous findings: emotional displays are bad for business. Marketing mix effectiveness in the presence of emotionsAccording to the log-linear specification, the estimated coefficient of.386 (see Model 2 in Table 3) means that, ceteris paribus, a product from category 1 sells 1.47 (  =Exp(.386))  times more than a product from category 2. The estimated price elasticity equals –.765 (see Model 2 in Table 3), which means a 10% increase in price corresponds to a 7.65% decrease in sales. Similarly, the estimated display duration elasticity equals.626 (see Model 2 in Table 3), which means a 10% increase in display duration corresponds to a 6.26% increase in sales, which is about 2 to 6 times larger than advertising elasticity (see [47]). The free shipping increases the quantity sold by.198% (see Model 2 in Table 3). Using the average price of $110.48 and the average quantity of 69.64 (see Table 2), the shipping waiver increases revenues by $15.23  (=$110.48×.198100×69.64)  and is profitable when the shipping costs are below $16. Ranking salespeopleHuman biases affect the performance appraisal process (e.g., pay, bonus, advancement rate, prestige). We propose that to mitigate these biases, salespeople should be ranked on their individual effectiveness (objective attribution) rather than average sales (naïve attribution). Model 1 facilitates the estimation of the effectiveness of an individual salesperson by controlling for prices, duration, free shipping, time of day, and week. Table 3 reports the estimates of percentage sales increase for an individual salesperson relative to the group average based on effects coding of the dummy variables (see [22]). Consider the estimate of –.488 for salesperson 1 from Model 1 in Table 3. That estimate means salesperson 1's performance is.488% below the group average. Similarly, salesperson 6's performance is.083% above the group average. These estimated effects are not affected by human cognitive biases.Graph: Figure 5. Salesperson performance appraisal.We compare the salesperson's performance rank based on the naïve versus objective attributions. Panel A in Figure 5 shows the ranking of 23 salespeople based on the average sales, which ignores the effects of prices, duration, free shipping, and time of day and week. In contrast, Panel B shows the ranking of the same 23 salespeople based on their individual effectiveness. The top and the bottom three salespeople remain the same under both metrics, thereby showing that the ranking attains convergent validity by identifying the same set of best and worst performers. However, the majority of salespeople (∼75%) reside in the middle, where the rank ordering differs across metrics. Thus, the objective attribution based on salesperson's effectiveness after controlling for prices, duration, free shipping, and time of day and week should guide supervisors in more objectively selecting salespeople for rewards, recognition, and retraining. Understanding moderation effectsFigures 6 and 7 depict the sales impact of emotional displays based on the full model (Model 4). In these figures, a low (high) price refers to the 25th (75th) percentile of the price distribution. First, emotional displays decrease sales. This finding holds uniformly for negative and positive emotions. Because  Ln(Q)  serves as the dependent variable, the marginal change in it equals  ΔQ/Q  , which represents ""percentage change in sales."" Thus, a marginal increase in emotional display corresponds to a sales decline that ranges from.004% to.18%. Across the six emotions, the magnitude of the total sales decline (.47%) is more than double the free-shipping effect (.20%). Second, because the tangent to the curves in Figures 6 and 7 becomes steeper as the intensity of emotional display increases, the sales decline accelerates. In other words, the sales decline increases at an increasing rate. Thus, not displaying emotions emerges as the optimal course of action. So, salespeople should sell with a neutral face, although how consumers interpret ""neutral"" depends on the sellers' gender, facial morphology, and contextual factors (e.g., [23]). Finally, the parallel curves in Figures 6 and 7 reveal the modest magnitude of moderation effects: sales decrease as price increases or promotion decreases (see the dashed curves).Graph: Figure 6. Emotional displays by price interactions.Graph: Figure 7. Emotional display by free shipping interactions. Time effectsIn the week effect, sine and cosine variables jointly identify the sales variations across days of the week. The cosine variable differentiates the first half of the week (Monday to noon Thursday) from the second half of the week (noon Thursday to Sunday). The sine variable differentiates the middle of the week (9 p.m. Tuesday to 9 p.m. Friday) from the end of the week (9 p.m. Friday to 9 a.m. Tuesday). Similarly, in the day effect, the sine and cosine variables identify the sales variations across hours of the day. Specifically, the cosine variable differentiates post meridiem (p.m.) from ante meridiem (a.m.), while the sine variable captures the rhythms across the late evening (6 p.m. to midnight) through the night hours (midnight to 6 a.m.) to the morning hours (6 a.m. to noon) and the afternoon (noon to 6 p.m.). Because the empirical results indicate that the cosine variable is less important than the sine variable, the a.m./p.m. distinction is not critical. As expected, sales occur 24 hours a day, including the nights; peak during the day; and are larger during the weekends. Relative variable importanceFigure 8 presents the relative contribution of marketing mix and nonmarketing variables: the former contributes 71%, whereas the latter accounts for 29% of the total  R2=80%  . The time of day, the day of the week, and the week of the year explain 20%. Emotional displays and face presence further explain 9% of the explained variance. Thus, nonmarketing variables boost explanatory power.Graph: Figure 8. Variable importance. Robustness checks Parameter stabilityA glance across the columns in Table 3 indicates a remarkable robustness. The columns reveal the estimated marketing mix effects in the presence of various operationalizations of emotional displays. For example, across Models 1–4 the price elasticity varies from –.76 to –.77, and the duration elasticity ranges from.55 to.67. Likewise, shipping and product estimates are (.22,.41), (.20,.39), (.19,.36), and (.19,.36) across Models 1–4, respectively. Salesforce effectiveness across the four models is also stable; for example, the percentage sales increase due to salesperson 15 hovers around.42. Even the rhythms of daily and weekly sales deviate only marginally. These results hold even when we replaced the static face variable  Fis  in Equation 3 with the dynamic component  z0,is=∑t=130e^0tFis,t  across the epochs  t=1,⋯,30  . Furthermore, we tested for heterogeneous effects of emotional displays and found that the effects were homogeneous across the two product categories (see Figures 6 and 7). Thus, the broad robustness —for all the variables and across all the models— enhances confidence in these results. Tercile analysisWe also analyzed data by splitting the presentation span of an item i displayed in a show s into three time segments. We discovered V-shaped effects across the beginning, middle, and end of the presentation span for all the six emotions, including happiness. We then extended this analysis tenfold to 30 epochs and found that not only does the parameter stability hold in both analyses, but also qualitatively similar results persist: negative U-shaped effects of emotions over the presentation span. Furthermore, the average variance inflation factor across all independent variables was 1.66, ranging from 1.01 to 5.17, which is far below 10 and thus rules out multicollinearity concerns. Dynamic time warpingTo our knowledge, this study marks the first time dynamic time warping appears in marketing. To further assess robustness, we reestimated Model 2 without dynamic time warping. As mentioned previously, dynamic time warping aligns landmarks such as the peaks, valleys, and inflections of the raw emotional curves  Eisk(t)  . Such landmark alignment homogenizes the timing of peaks, valleys, and inflections in  Eisk(t)  . Consequently, the estimated trajectories,  a^kt  , in Figure 3 are smoother than those in Figure 4 without landmark alignments. More importantly, the overall pattern remains the same: the sales impact  a^kt  is negative, U-shaped, and similar across the six emotions. In summary, the U-shaped patterns as well as other results are robust. We close this section by comparing the performance of models on multiple metrics. Model comparisonWhich one of the four models is the best? Although the adjusted  R2  of about 80% is remarkable, especially given 99,451 sales pitches, it does not discriminate among the four models as the log-likelihood, Akaike information criterion, and Bayesian information criterion do. Therefore, we compared the models using these metrics and present the results in Table 5. Specifically, Models 3 and 4 dominate Models 1 and 2 on all the metrics. The Bayesian information criterion selects Model 3, whereas both the other metrics (log-likelihood and Akaike information criterion) indicate that Model 4 outperforms the rest. We used Model 4 to plot Figures 6 and 7. We next discuss the implications of these findings.GraphTable 5. Models Comparison. Model 1Model 2Model 3Model 4R279.10%79.7%80.0%80.0%Log Likelihood–110,560–108,447–107,472–107,456AIC221,184216,972215,036215,027BIC221,488217,343215,473215,579Fixed Parameters32394658Observations99,45199,45199,45199,451  DiscussionThis research offers important insights into livestream retailing by addressing two foundational questions identified in this special issue dedicated to understanding the interface of technology and marketing: ( 1) How can managers use new types of data to improve marketing decision making? and ( 2) What new methods can deliver the best consumer insights to improve marketing strategy? To address the first question, the second pennant in Figure 1 captures the new type of data available from streaming videos of sales presentations, which can identify a large-scale, unobtrusive, and comprehensive set of emotions. To address the second question, the fourth pennant in Figure 1 contributes the new methods to create six emotional trajectories via functional principal components analysis and dynamic time warping to align them. Incorporating them as quadratic and moderating variables, we then assess the value of emotional displays. Building on Models 1–4, we discuss the following theoretical and managerial contributions. Theoretical Contributions Livestreaming technology opportunities in marketingLivestream e-commerce, which features hosts promoting and selling goods and services in real time via screen-mediated sales presentations, represents an emerging opportunity for marketers to create, deliver, and communicate content so as to monetize in ways not possible previously. Specifically, marketers can, first, reach customers via new channels such as social messaging apps (Facebook, WeChat), livestreaming services (e.g., Twitch), and internet platforms (e.g., Taobao Live) that integrate shopping and entertainment. Second, these technology platforms facilitate purchases from wherever and whenever customers are seeking to buy. Third, they shorten the purchase funnel by demonstrating a product and describing why it is a must-have item; conveying that only limited quantities are available; counting down the time remaining on the item before the next item is to be introduced; and injecting such calls to action as ""grab it before its gone."" Finally, technology allows value capture in formats not possible previously: noncash payments (e.g., PayPal, Venmo), installment payments (e.g., Klarna unsecured loans), and barter payments (e.g., BarterOnly.com, which provides exchanges of used products). Marketers need to imagine how they can integrate such value creation and value capture opportunities made possible by technological advances. Large-scale unobtrusive emotions dataEarlier studies used human intervention to collect data on emotional displays at a small scale (e.g., [ 8]; [30]; [42]). In contrast, applying artificial intelligence (see, e.g., [33]; [35]), we extract face presence and facial expressions from 62.32 million frames of streaming video sales presentations automatically and unobtrusively, thereby responding to calls to harness machine learning to generate meaning from big data (e.g., [ 4]; [29]; [40]; [59]). Multiple emotionsEarlier studies consider either a single (e.g., [32]; [57]; [58]) or a select few emotions (e.g., [10]; [50]; [54]; [55]), potentially resulting in biased estimates due to omitted variables. Hence, Model 2 specifies a comprehensive set of six emotional displays simultaneously. Our focus on salespeople's emotional displays is also responsive to an earlier call to devote greater attention to emotions on the seller side of the exchange dyad (see [ 3]). Dynamic effectsEarlier studies focus on static episodic expressions (e.g., [10]; [55]). Our Model 2 permits capturing the dynamic trajectories of emotions at a more granular level, thereby revealing time-varying patterns of sales impact (see Figure 3). More importantly, the Web Appendix makes original contributions to the theory of inference on the effects of functional principal components. Optimal emotionsVirtually all studies on emotions have used customer mindset metrics as the dependent variables (e.g., [54]; [57]; [58]). While [32] use box-office revenues, they specify happiness to monotonically affect sales, ruling out the possibility of that an optimal level of emotions exists. Given the nonmonotonic effects in Models 3 and 4, the theoretical existence of the optimal mix arises. Equation 4 presents the optimal emotions to display so as to maximize sales. These results not only make original contributions to the extant literature but also offer guidance to design technology-inspired service agents (e.g., avatars, virtual news anchors) to be more humanlike ([11]; [37]). They also inform the discussion about technology and marketing in that artificial intelligence can be used to monitor the seller's facial activity, provide real-time coaching, and thus assist in training salespeople to improve business outcomes ([20]; [33]). Salesperson's impact in screen-mediated exchangesA marketplace increasingly characterized by greater technological connectivity and interactivity has prompted calls to investigate the business impact of a seller's facial expressions in screen-mediated commercial interactions ([ 7]). [28], for instance, underscore the need to evaluate whether their results from in-person, face-to-face customer encounters involving ""emotionally calibrated"" salespeople will hold in digital exchanges. The authors contend that this type of salesperson exhibits calmness and that exuding calmness builds rapport, which in turn drives favorable sales performance outcomes. Our theorizing, which is steeped in EASI's predictions about the inferences that buyers draw about a seller's facial expressions in a competitive exchange (see Table 1), and findings from a one-to-many livestream broadcast setting reaffirm the importance of reducing emotional displays in driving sales effectiveness. We thereby contribute to understanding the communicative role of facial expressions in screen-mediated exchanges and elaborate further in the following section on ""selling with a straight face."" Managerial Contributions Optimal face allocationHow should face presence be allocated over an item's presentation span? Should frames containing a face be displayed uniformly or in chunks? If the latter, should they be concentrated in the middle, when sales resistance is highest? To this end, we evaluate Equation 4 and present the optimal percentage allocation of the total number of frames with a face over an item's presentation span in Figure 9. We observe that the optimal allocation is neither uniformly displayed nor chunked in the middle. Rather, the optimal number of face frames wanes at the start, gradually builds to a crescendo, and eventually ebbs. Specifically, the optimal allocation decreases on the initial 10% span, then gradually increases as the presentation progresses, and finally decreases in the last 15% span. Remarkably, this optimal allocation conforms to the three-part structure of stories: the beginning, the middle, and the end (see [36]). Sell with a straight face?Figures 6 and 7 uncover the novel and provocative findings that, first, positive emotional displays reduce sales. Second, the greater the intensity, the larger the decline. To mitigate the negative effect, salespeople should consider toning down their facial expressions. To mitigate the quadratic effects of intensity, they can abate their exaggerated expressions. Together, these findings indicate a new maxim: sell with a straight face. Consistent with this maxim, [12], p. 82) advocates that direct marketers should use a ""journalist approach"" to answer the ""who, what, why, when, where of a product. Whom is the product for? What does it do? Why is it beneficial? When can it be used? Where can it be bought?"" In other words, livestream salespeople should broadcast their pitch with a stoic expression akin to that of news anchors, though we acknowledge that this implication may not generalize to face-to-face communications in business markets. Sales resistance curveFigures 3 and 4 can be interpreted as the sales resistance curve. The maximum sales resistance is near the middle of an item's presentation; the least sales resistance is at the beginning and end of presentations. This U-shaped sales resistance curve provides actionable guidelines to practitioners. Emotional displays at the beginning and end of presentations help engage consumers and build rapport. However, during the livestream show, hosts should monitor the frequency and intensity of their emotional expressions. Because genuine interactions involve less emotional and more neutral expressions, salespeople can make emotional connections with the audience with neutral expressions and lessen the insidious effects of sales resistance. Although hosts cannot completely avoid emotions, they should take advantage of livestreaming platforms to emotionally connect the brands with customers. People analytics in sales performance appraisalA naïve assessment of a salesperson's performance is based on the actual quantity sold. However, this quantity depends on factors such as prices, duration, free shipping, and time of day and week. In other words, a salesperson can validly object that another salesperson's larger actual sales are not reflective of his/her performance alone because price, duration, free shipping, and time of day and week also impacted sales. Cognitive biases further compound such performance appraisals. Prominent factors driving cognitive biases include the fundamental attribution error, halo effects, the leniency bias, the recency bias, selective perception, the self-serving bias, and the similarity bias (e.g., [48]). Fundamental attribution error refers to supervisors underestimating the influence of external factors and overestimating the influence of internal factors when judging a salesperson's performance; halo effects arise when a general impression of a salesperson overshadows the relevant metrics; leniency bias refers to a supervisor's tendency to rate all salespeople positively (or negatively), reducing the difference between top and bottom performers; recency bias creeps in when recent events (e.g., bumper sales, sharp declines) influence supervisors' judgments; selective perception refers to the supervisor's tendency to notice certain metrics and filter out others; self-serving bias emerges when a salesperson attributes own successes to internal factors and failures to external factors; and similarity bias (i.e., homophily) shapes the evaluation when supervisors reward a salesperson similar to themselves.Using Table 3, managers can objectively rank salespeople (see Panel C in Figure 5) to circumvent the effects of the aforementioned biases. Indeed, the CEO and senior leadership team of the livestream retailer we examined found our proposed framework valuable to recognize excellence and identify candidates for retraining. Both recognition and training, in turn, help improve future sales performance ([61]). Thus, the framework in Figure 1 unlocks the power of data and contributes to sales performance analytics. Future Research Content effects[26]) recent study suggests that information content can matter. Specifically, they analyze 275 sales pitches from the Home Shopping Network, manually code the minute-by-minute content on the cumulative sales thus far during an item's presentation span, and show that the intermittent availability of this information increases item sales by.084 units at the onset and decreases linearly to.015 units at the end for a unit increase in the displayed cumulative sales. We encourage future researchers to automate such content analysis to extract and incorporate facial expressions. Two-way communicationsOur empirical study pertains to one-to-many screen-mediated competitive exchanges, and it shows that the salesperson's emotional expressions evoke negative inferences by viewers about the salesperson's intentions. One explanation may be the absence of social interaction. When the salesperson smiles, the viewers may not reciprocate because the salesperson's emotions are not targeted to a specific viewer. Such differences provide the impetus to study screen-mediated face-to-face interactions in the presence of social others. For example, [21] contend that the customer purchase journey involves traveling with social others, which necessitates investigations into the various influences that members of the social network can have on buyers' appraisals, intentions, and actions. In a livestream e-commerce setting, the host becomes an important social other. Viewers can readily communicate with the influencer via live chat texts, emojis, voice, and/or video and further enhance their sense of connection with that celebrity (i.e., parasocial relationship). Does the host's verbal and nonverbal communications influence viewers' behavior in such communal, two-way screen-mediated exchanges? Do purchases by social others induce ""fear of missing out""? [43] suggests conversion rates of 30% in livestream shopping versus 3% in traditional marketing. To incorporate such two-way communications in the models, researchers should augment the regressors with the characteristics of not only the items and sellers (as in this study), but also the network of social others and the hosts. We encourage further research to shed light on two-way communications in livestream shopping. Authentic emotions[25] manipulate authenticity (i.e., surface or deep acting) and emotional intensity in simulated service encounters (i.e., actors played the role of employees) with 223 consumers to understand the effects on customer satisfaction, customer–employee rapport, and loyalty intentions. They show that authenticity rather than intensity influences customers' reactions (for similar results, see [57]). We encourage future researchers to design emotion recognition algorithms that can classify facial expressions on the basis of emotional authenticity in addition to intensity. ConclusionPrevious studies predominantly focus on marketing mix effects on sales because when they were conducted, machine learning technology was not available to detect faces and extract emotions at scale. This study combines machine learning technology and marketing. Specifically, we develop the retail analytics engine (see Figure 1) to unobtrusively collect data on face presence and emotional displays. Applying this technology to livestream retail data, we found that facial expressions, including happiness, adversely impact sales. This counterintuitive and provocative finding suggests that salespeople should sell with a straight face. These negative effects exhibit U-shaped dynamics over an item's presentation span, uniformly across six emotions, revealing that the largest sales resistance occurs during the middle of the presentation. Furthermore, the presence of a face matters because it impacts sales positively; therefore, it should be present more than is currently the case. Yet, its optimal allocation over time should be reduced over the initial 10% span, then gradually increased as the presentation progresses, and subsequently tapered down in the last 15% span. Finally, the retail analytics engine empowers managers to more objectively assess the effectiveness of each individual salesperson (see Figure 5), thereby circumventing cognitive biases in performance appraisals.This study highlights the importance of monitoring and managing facial expressions. One implication is to train new salespeople. The firm can analyze the video footage, much like sports teams watch films of critical moments in previous games to learn what individual players did well and not so well, and sales coaches can help discern the extent to which they displayed emotions and the proportion of each emotion expressed. The feedback from such debriefing sessions could be used to modify sales pitches. Another implication is to retrain experienced sales professionals. The firm can compare each salesperson with the top performer (see Figure 5) and identify which emotions the salesperson ought to tackle. Happiness is the first one that should be addressed. While previous research advocates ""service with a smile,"" we suggest selling with a straight face. Smiling may be off-putting because it lacks authenticity ([25]), reducing trust in the seller ([10]). Subsequently, salespeople should address displays of anger, then fear, and other negative emotions. Last, this study has implications for bot marketing. As technology advances, bots will more closely mimic human facial expressions and supersede humans in monitoring and managing facial expressions. Chat bots, like humans, provide voice assistance to customers. Similarly, three-dimensional audiovisual bots, like salespeople, can engage with customers. For example, HSBC Bank in Northern California employs Pepper, a social humanoid robot ([14]). Further technological advances will bestow bots with the ability to express and reciprocate emotions, thereby assisting livestream retailers to nudge prospective customers through the purchase funnel by explaining features and benefits, instilling urgency to buy, and entertaining them along the way. "
2,", Science Direct, and Google Scholar) to find studies published during the 1990–2020 period. To identify research involving avatars that are consistent with our definition, we first excluded articles that did not provide detailed information about avatar definitional elements or scope. In addition, we excluded research on topics that fall outside our definitional boundary (e.g., consumers' self-avatars). Purely technical articles (e.g., programming of avatar) and studies that do not address real-time consumer–avatar interactions were removed as well. To strike a good balance between research diversity and quality, we sampled only reputable journals across various disciplines (i.e., impact factor of at least 3).[ 6] In total, we compiled 98 empirical research articles (the full list of reviewed research is available in the Web Appendix). Table 2 provides a summary of select illustrative research.GraphTable 2. Avatars in Empirical Research. Illustrative ResearchOriginal LabelContextTheoretical PerspectiveMediator VariablesModerator VariablesKey FindingsAl-Natour, Benbasat, and Cenfetelli (2011)Automated shopping assistantOnline shopping for a laptop computerComputers as social actors (CASA) framework; similarity-attraction hypothesisPerceived decision process similarityNonePerceived decision process similarity mediates the effect of perceived personality similarity on several beliefs (enjoyment, social presence, trust, ease of use, and usefulness).Bickmore et al. (2016)Embodied conversational agentCancer patients identifying and learning about clinical trials on the internetNot specifiedNoneNonePatients were more satisfied with the conversational agent compared to the conventional web form-based interface, and patients with low health literacy had a higher success rate in finding relevant trials.Brave, Nass, and Hutchinson (2005)Embodied computer agentCasino-style blackjack gameCASA frameworkNoneNoneEmpathic emotion of the agent leads to greater user-rated likeability, trustworthiness, perceived caring, and perceived support.Chattaraman et al. (2019)Digital assistantOnline purchase of athletic shoes by older consumersSocial response theoryTrust in online store; information overload; perceived self-efficacy; ease of use; usefulnessInternet competencyUsers' internet competency interacts with the digital assistant's conversational style (social- vs. task-oriented) in affecting social, functional, and behavioral intention outcomes.Chattaraman, Kwon, and Gilbert (2012)Virtual agentOnline purchase of apparel by older consumersSocial response theory; CASA frameworkPerceived social support; trust in online store; perceived riskNoneVirtual agents can increase older users' patronage intentions by enhancing perceived social support and trust in the online store while reducing perceived risk.Derrick and Ligon (2014)Embodied conversational agentAn avatar-based screening checkpoint experimentCASA frameworkNoneGenderSelf-promoting agents are perceived as more powerful, more trustworthy, and more expert, whereas ingratiating agents are perceived as more attractive. Ingratiation impression management techniques are viewed less (more) favorably by females (males) than self-promotion techniques.D'Mello, Graesser, and King (2010)AutoTutorComputer literacy learning with a fully automated computer tutorSocial agency theoryNoneNoneStudents who interacted with the AutoTutor through a spoken dialogue used more cognitive resources and completed more problems than students who had to type.Go and Sundar (2019)ChatbotOnline digital camera purchaseCASA frameworkSocial presence; homophilyAnthropomorphic visual cues; agencyChatbot's message interactivity has positive effects on customers' attitude toward the website and return intention mediated by perceived social presence and homophily; anthropomorphic visual cues and agency moderate these effects.Holzwarth, Janiszewski, and Neumann (2006)AvatarOnline purchase of shoes that are customizable via online consultationSocial response theoryEntertainment value; information value; likeability of avatar; credibility of avatarProduct involvementUse of an avatar sales agent increases satisfaction with the retailer, attitude toward the product, and purchase intentions, mediated by perceived entertainment and information value; an attractive avatar is more effective at moderate levels of product involvement, mediated by likeability of the avatar, whereas an expert avatar is more effective at high levels of product involvement, mediated by credibility of the avatar.Keeling, McGoldrick, and Beatty (2010)AvatarOnline experiments of retail websites selling books/CDs and travel insuranceCASA frameworkTrust perceptionGoods/services high in credence vs. search qualitiesAvatars' social orientation and task orientation increase customers' trust perception, which subsequently has a positive effect on patronage intention. Effects of task- (social-) oriented communications are stronger for search (credence) goods/services.Kim, Hong, and Cho (2007)Intelligent conversational agentOnline electronic product (e.g., cellular phone) information searchNot specifiedNoneNoneAgents capable of the probabilistic inference and the semantic inference show superior performance in providing suitable responses to user inquiries with only a few interactions.Lee and Choi (2017)Conversational agentInteractive movie recommendation systemCASA framework; media equation theory; uncertainty reduction theoryIntimacy; trust; interactional enjoymentNoneSelf-disclosure and reciprocity of the conversational agent have positive impacts on user satisfaction and intentions to use, mediated by intimacy, trust, and interactional enjoyment.Mimoun and Poncin (2015)Embodied conversational agentFurniture purchase with Anna on IKEA's websiteTechnology acceptance modelUtilitarian value;hedonic valueNoneAnna increased consumers' satisfaction and behavioral intentions through utilitarian and hedonic value.Nunamaker et al. (2011)Embodied conversational agentAutomated kiosk-based interviewsNot specifiedNoneNoneMale-embodied agents are perceived as more powerful, more trustworthy, and more expert than female ones; however, the latter are more likeable. Avatars with neutral expressions are perceived as more powerful, whereas smiling avatars are more likeable.Von der Pütten et al. (2010)Embodied conversational agentInteractions involving personal questionsThreshold model of social influence; Ethopoeia conceptNoneNoneBeliefs about whether a participant is interacting with a human-controlled or a computer-controlled agent lead to almost no differences in the evaluation of the virtual character or its behavioral reactions; higher behavioral realism affected both.Qiu and Benbasat (2009)Anthropomorphic interface agentOnline recommendation system for complex and attribute-intensive digital camerasCASA framework; social agency theorySocial presenceNoneHumanoid appearance and human voice-based communication of avatars significantly increase participants' perceived social presence, which has a positive effect on trust, perceived usefulness, perceived enjoyment, and the decision to use the avatar as a decision aid.Schuetzler et al. (2018)Conversational agentResponses to sensitive questions to a person vs. a conversational agent vs. online surveySelf-disclosure; social desirability; social presence theoriesNoneNoneConversational agents with better conversational abilities prompt more socially desirable responses from participants, with no significant effect found for embodiment.Verhagen et al. (2014)Virtual customer service agentInquiries about online mobile phone serviceSocial response; implicit personality; primitive emotional contagion; social interaction theoriesSocial presence; personalizationCommunication style (socially vs. task-oriented); anthropomorphismFriendliness and expertise have positive effects on participants' service encounter satisfaction mediated by social presence and personalization; the effect of friendliness on personalization is stronger for socially oriented agents than for task-oriented agents, as is the effect of expertise on social presence.Wang et al. (2007)Virtual characterOnline travel information serviceSocial response theory; stimulus-organism-response framework; cognitive mediation theoryArousal; pleasure; flow; hedonic and utilitarian valuesProduct involvementSocial cues from interacting with the avatar increase perceptions of website socialness, feelings of arousal, pleasure, and flow, leading to greater hedonic and utilitarian values, which then increase patronage intentions. The effect of arousal on pleasure is stronger when product involvement is high; the influence of arousal on hedonic value is stronger for women. Flow does not lead to pleasure for older consumers, and pleasure has a much weaker impact on utilitarian value for those consumers.Yokotani, Takagi, and Wakashima (2018)Virtual agentMental health interviewsThreshold model of social influenceNoneNoneParticipants revealed more sex-related symptoms to the virtual agent than to a human expert, whereas they disclosed mood and anxiety symptoms more often to the human expert than to the virtual agent.  Form realismThe overarching theoretical framework that guides empirical studies of human–avatar interactions is social response theory, which is sometimes referred to as the CASA paradigm ([68]; [70]; [84]). It suggests that anthropomorphic characteristics of avatars elicit consumers' socialness perceptions, often in an automatic, spontaneous, mindless process that induces varying degrees of cognitive, affective, and social responses to avatars ([ 4]; [45]; [96]; [98]). Although the theory suggests that an anthropomorphic appearance positively affects customer outcomes, empirical results indicate some mixed effects. In various situations, lower or higher levels of form realism appeared more effective, but in other cases, no differences emerged. For example, visually static, cartoonish avatars with very low form realism increased satisfaction with a retailer, attitude toward products, and purchase intentions in some studies ([31]; [45]). However, [83] find that avatars with more realistic, humanlike appearance increased customers' perceptions of social presence, leading to higher usage intentions. [96] found no significant differences between avatars that are low or high in form realism in terms of service satisfaction. Similarly, [86] reported that a more anthropomorphic appearance of an avatar had no effect on participants' disclosure of information about sensitive topics, such as drinking behaviors.Two factors may help explain these inconsistent effects. First, prior studies have not investigated all of the design elements identified in our typology that help establish avatars' form realism (e.g., visually static vs. dynamic avatars, avatars' age, gender) or how these underlying elements might induce specific effects. By focusing on only a subset of visual characteristics and, thus, failing to account for the totality of the elements that establish form realism of an avatar, these studies may have produced biased estimates. Second, we propose that avatars' form and behavior must be considered simultaneously, because form realism is meaningful only in the context of behavioral realism ([ 5]); however, few studies have done so. Behavioral realismExtant research consistently shows positive effects of greater behavioral realism. For example, [98] report that an avatar's scripted text or spoken communications can enhance customers' hedonic and utilitarian benefits when shopping online, as well as increasing their patronage intentions. Similarly, [59] find that when users interact with an avatar that has a high degree of behavioral realism, trust between the parties is higher. Other studies offer similar results, noting that avatars' behaviors, such as decision-making style ([ 4]) and socially oriented communication ([96]), significantly affect avatar trustworthiness and the overall customer experience ([12]; [18]).Advancements in AI technology have enabled avatars to exhibit higher levels of cognitive and emotional intelligence. For example, they can engage in autonomous conversations by analyzing and responding to customers' requests in real time, thereby significantly increasing customers' trust in them ([64]). Using avatar interviewers equipped with video, audio sensors, and advanced analytical software, [75] demonstrate how intelligent avatars can detect, interpret, and respond to human interviewees' emotions, cognitive effort, and potential deceptions. [97] examine the effects of an avatar that can collect and analyze a human's voice and upper-body movement and coordinate its own responses accordingly. Results show that the avatar's intelligent behavior led to positive evaluations of the avatar, regardless of whether it was controlled by a human or a software.Even in light of these consistent findings related to avatar behavioral realism, some important research issues remain unresolved. First, few studies have investigated the underlying behavioral realism elements identified in our typology of avatar design (e.g., communication modality, social content, response type) to determine which are most critical or how they might interact with other form or behavioral realism elements. For example, [ 7] found that an avatar nurse incorporating social content in its scripted conversations produced better patient experiences, but [86] reported that a scripted, task-focused avatar interviewer elicits more socially biased responses. Second, a few studies revealed some unexpected negative effects of behavioral realism (e.g., [86]), but research has yet to identify the conditions in which detrimental effects are more likely or design strategies to address them. Integrated perspective on form and behavioral realismOur review of extant literature thus reveals a key limitation: lack of consideration of the alignment between form and behavioral realism of avatars. If the levels of form and behavioral realism are mismatched, the consequences for avatars' effectiveness may be profound and can help explain inconsistent past findings. Yet some misaligned avatars (e.g., the REA avatar is high in behavioral realism but low in form realism) seem equally as effective as well-aligned avatars (e.g., SK-II's skincare advisor YUMI is very high in both behavioral and form realism). However, other misaligned avatars have failed (e.g., Nordnet's Amelia, with high form realism but low behavioral realism). A systematic analysis of avatar effectiveness thus seems warranted and requires identifying different categories of avatars along the form and behavioral realism dimensions as a first step.We suggest that avatars can be parsimoniously grouped into a 2 × 2 taxonomy, according to their form and behavioral realism (Figure 2). This taxonomy provides a foundation for predicting the success or failure of avatars in business practices and can inform avatar design strategies. We identify four distinct categories of avatars: simplistic, superficial, intelligent unrealistic, and digital human avatars. A simplistic avatar has an unrealistic human appearance (e.g., 2D, visually static, cartoonish image) and engages in low intelligence behaviors (e.g., scripted, only task-specific communication). For example, in the Netherlands, ING Bank uses a 2D, cartoonish-looking avatar, Inge, to provide responses to simple customer inquiries with a set of predetermined answers. In contrast, a superficial avatar has a realistic anthropomorphic appearance (e.g., 3D, visually dynamic, photorealistic image), such as Natwest Bank's Cora, but low behavioral realism, in that it is only able to offer preprogrammed answers to specific questions. An intelligent unrealistic avatar (e.g., REA) is characterized by humanlike cognitive and emotional intelligence but exhibits an unrealistic (e.g., cartoonish) human image. These avatars can engage customers in real-time, complex transactions without being mistaken for real human agents. Finally, a digital human avatar such as SK-II's YUMI is the most advanced category of avatars, characterized by both a highly realistic anthropomorphic appearance and humanlike cognitive and emotional intelligence, and is designed to provide the highest degree of realism during interactions with human users.Graph: Figure 2. Form realism versus behavioral realism taxonomy. Insights and propositions derived from academic literatureTo advance extant literature, we propose the need to consider the interrelationship of form and behavioral realism. Visual information (i.e., what an avatar looks like) often gets processed automatically and almost immediately, requiring minimum cognitive resources ([65]). This visual appearance then becomes the basis for probabilistic consistency inferences, where consumers form an expectation of some unknown attribute, based on a known attribute with which it is believed to be correlated ([27]). For example, consumers often make inferences about an unfamiliar brand's quality by using price as a signal of quality, in the belief that the two are correlated. Similarly, when an avatar looks more like a human, consumers may expect it to also behave like a human. Thus, the visual characteristics of an avatar may influence consumers' judgments of its behavioral competence, even before an interaction takes place ([73]). A more realistic anthropomorphic appearance suggests a higher level of behavioral realism, leading to a greater expectation that the avatar will behave like a real human might. Proposition 1:  As the form realism of an avatar increases, so do customers' expectations for its behavioral realism.Customers will use an avatar's form realism as a frame of reference for forming initial expectations about its behavioral realism. The expected level of behavioral realism will then serve as a benchmark, against which consumers will form comparative judgments of their subsequent experience. According to expectation confirmation theory ([76]), when the actual outcome is worse than expected, consumers experience a negative disconfirmation, leading to decreased overall satisfaction. A better-than-expected outcome instead results in a positive disconfirmation, which increases customers' overall satisfaction ([33]; [100]).Consistent with this theory, if an avatar's behavioral realism exceeds the consumer's initial expectations, which were based on the avatar's form realism, a positive disconfirmation likely occurs, and the consumer should perceive the avatar as more credible and attractive, as well as feel increased trust or confidence in it ([ 2]). Ellie, an avatar that assesses depression and PTSD symptoms in veterans, serves as a good illustration of a positive disconfirmation: her cartoonish appearance paired with highly intelligent, humanlike behavior has proven highly effective with vulnerable individuals ([85]). Conversely, if the avatar's behavioral competency falls short of the expectations that users formed on the basis of the avatar's form realism, it may lead to a negative disconfirmation and dampen customers' satisfaction ([65]). Nordnet's Amelia exhibited minimal competence in providing customized advice for high-risk transactions (e.g., stock purchase), which proved disappointing. Amelia's realistic anthropomorphic appearance might have led customers to develop high behavioral expectations, which Amelia could not deliver, giving rise to a negative disconfirmation. Therefore, we expect asymmetric effects of misaligned avatar form and behavioral realism. Proposition 2:  Differences between the avatar's form and behavioral realism have asymmetric effects, such that customers experience positive (negative) disconfirmation when an avatar's behavioral realism is greater (less) than its form realism. Mediation mechanisms of avatar effects on performanceIntegration of evidence across multiple research streams suggests that avatars affect performance outcomes (e.g., customers' likelihood to purchase a product) indirectly through customers' cognitive, affective, and social responses, depending on the context ([45]; [59]). Customers form cognitive responses to avatars according to the avatars' informativeness or competence in helping them make well-informed decisions ([45]; [98]). Cognitive trust, or willingness to rely on another entity's help to achieve goals in uncertain situations, is another key dimension of customers' cognitive response ([53]; [63]).Interactions with avatars can also evoke affective responses in customers, such as by providing them with unique entertainment experiences ([45]). Avatars can deliver pleasurable experiences independent of their ability to facilitate a specific functional goal, such as a shopping task, by offering entertainment and emotional value during the shopping process ([98]). Human–avatar interactions are also social in nature. Avatars can enhance customers' perceived social presence (i.e., the feeling of being with another person) and create feelings of human contact or connection ([18]; [83]). Moreover, the use of an avatar can provide a sense of personalization, so customers receive information that appears tailor-made to their specific needs ([96]; [98]). The CASA framework argues that avatars can also induce feelings of reciprocity in human–computer interactions, which can strengthen perceived rapport with the avatar and enhance the users' social experience ([18]; [59]).When the levels of an avatar's form realism and behavioral realism are aligned, customers' behavioral expectations tend to be confirmed. This simple confirmation, together with high initial behavioral expectations induced by form realism, may have a strong, positive, additive effect on performance outcomes, such as customers' purchase likelihood ([76]; [90]). We expect that avatars that are aligned in their form and behavioral realism can affect customer performance outcomes through all three types of mediating responses: cognitive, affective, and social. However, when the levels of form and behavioral realism are misaligned, the outcomes might be mediated through different responses. Consider the misalignment that occurs when form realism exceeds behavioral realism. In this situation, customers may find the avatar to be especially entertaining, because its realistic anthropomorphic appearance and characteristics can also serve as hedonic elements that can increase perceived entertainment, which often is intrinsically enjoyable in its own right, regardless of performance outcomes ([24]; [103]). To the extent that perceived enjoyment creates a pleasant mood, the hedonic aspect of high form realism can improve performance outcomes such as impulsive online purchases ([78]). Yet customers also might experience negative disconfirmation, stemming from the disappointment with an avatar's cognitive and social capabilities, resulting in weakened customer performance outcomes.Alternatively, when misalignment arises because the avatar's behavioral realism exceeds its form realism, the positive effects on customer outcomes might be mediated primarily by cognitive and social responses. Although typically this avatar's lower form realism is unlikely to provide much entertainment for the customer,[ 7] the positive disconfirmation of avatar's behavioral competence may significantly boost customers' confidence in the avatar's overall ability to provide valid information, offer personalized service, or build customer rapport. Proposition 3:  When an avatar's form realism exceeds its behavioral realism, it has a positive effect on performance outcomes (e.g., purchase likelihood), through customers' (a) affective responses, but a negative effect on performance outcomes through customers' (b) cognitive and (c) social responses. Proposition 4:  When an avatar's behavioral realism exceeds its form realism, it has a positive effect on performance outcomes (e.g., purchase likelihood) through customers' (a) cognitive and (b) social responses. Business PracticesAs we noted previously, many companies are adopting avatars to humanize their brands with a scalable personalized human touch, but managers lack guidance for how to design these avatars to ensure their effectiveness ([10]; [101]). In this section, we use business examples to illustrate the avatar categories from our taxonomy (Figure 2) and thereby clarify which factors and conditions make them effective. We also generate theoretical insights and managerial implications. Simplistic avatarAn obvious benefit of using avatars is the firm's improved efficiency and scalable customer service. For example, ING Bank's cartoonish avatar, Marie, answers common debit and credit card questions with preprogrammed information and solutions (www.ing.com). In the Los Angeles Superior Court, an animated cartoon avatar, Gina, which speaks multiple languages, successfully handles 1.2 million new traffic citations a year ([62]). A start-up company called TwentyBN has introduced an animated cartoon sales avatar, Millie, which can understand and answer simple questions while presenting various products ([49]) and seems to be especially effective in promoting low-ticket items, such as sunglasses. Simplistic avatars thus seem most effective in providing hassle-free, convenient options for completing quick, specific tasks (e.g., information inquiries), especially when relatively little risk is involved (e.g., inexpensive online purchases). Superficial avatarThe use of superficial avatars in various industries shows more mixed results. Among the successes, HSBC Hong Kong's Amy, a photorealistic avatar that handles routine customer inquiries similar to ING's Marie, was well-received by customers ([93]). A very realistic-looking 3D avatar, Cora of Natwest Bank in the United Kingdom, can answer 200 basic questions, such as how to open an account or complete a mortgage application ([79]). In the insurance industry, Lemonade Insurance's avatar Maya and Progressive's Flo, both very humanlike, are programmed to provide category-specific information and handle simple transactions, such as onboarding customers and giving online quotes ([13]; [82]). However, other superficial avatars have been less effective. The Swedish bank Nordnet had to discontinue its realistic-looking avatar Amelia, presumably due to her failure to provide intelligent stock purchasing advice. At IKEA, the decision to eliminate its avatar Anna stemmed from a recognition that her realistic anthropomorphic appearance led to complex customer questions, which required responses beyond the predetermined set available in the avatar's programming ([11]; [87]). Overall, superficial avatars can entertain customers while enhancing efficiency in low-risk transactions (e.g., bank account information inquiries), but they also can produce detrimental effects for customers seeking high-risk or complex transactions (e.g., financial investments) because these avatars lack the level of intelligence that their realistic anthropomorphic appearance leads users to expect. Intelligent unrealistic avatarThis type of avatar is relatively rare but generally successful. For example, the REA avatar has been effective in providing virtual showings of homes for sale; Ellie, an avatar therapist, has been useful in detecting PTSD and depression symptoms in military veterans. With its humanlike intelligence, Ellie can engage in context-appropriate, autonomous conversations and build rapport with subtle, supportive, and sympathetic gestures when listening to a veteran's sensitive story. In turn, veterans disclose significantly more PTSD symptoms to her than to a human therapist ([43]). Thus, intelligent unrealistic avatars seem especially effective for complex relational transactions involving sensitive personal information (e.g., finances, health) as they can provide a sense of nonjudgment because customers recognize that these avatars are not human but are still competent in their tasks. Digital human avatarWith advanced digital and computing technologies, pioneer avatar companies such as Soul Machines are breaking new ground for digital human avatars in marketing applications (www.soulmachines.com). For example, skincare brand SK-II uses an incredibly realistic looking and behaving avatar, YUMI, whose AI-powered digital brain enables advanced cognitive and emotional intelligence. In addition, YUMI can recognize users' gestures and features, such as eye color; communicate via speech or text; and deliver customized tips with credible, highly personalized beauty advice ([14]). Another digital human avatar, modeled after Daniel Kalt, the chief economist of UBS investment bank, can forecast financial data and present investment advice to high-wealth customers (www.nanalyze.com). Overall, digital human avatars seem most effective for building long-term customer relationships in contexts that feature substantial complexity or risk (e.g., financial investments), where users prioritize realistic, trustworthy, and personalized service. Insights and Propositions Derived from Business PracticesObservations from business practices suggest that avatars' effectiveness may be highly contingent on the level of perceived uncertainty users experience during their interactions with avatars. This uncertainty might arise from contextual factors, such as functional risk, financial risk, or price ([25]). Functional risk refers to the concern that the product/service may fall short of performance expectations. Financial risk refers to a possible loss of money, independent of purchase price, due to a poor decision (e.g., stock performance) ([25]). In addition, as the purchase price increases, the need for information about the quality of products or services also becomes more important to manage perceived risk ([102]). Overall, we predict that when customers feel greater uncertainty, they develop heightened expectations that an avatar that offers a realistic anthropomorphic appearance will also have a comparable level of behavioral realism, because they rely on the avatar's informativeness and ability to provide personalized advice to reduce the perceived risks associated with the purchase. Proposition 5:  The positive effect of an avatar's form realism on customers' behavioral realism expectations is stronger when (a) functional risk is higher, (b) financial risk is higher, and (c) the product is more expensive.We previously posited that when form realism exceeds behavioral realism, the avatar can have both positive (via customer's affective responses) and negative (via customer's cognitive and social responses) effects on performance outcomes. These mediated effects also might be moderated by perceived uncertainty. When perceived uncertainty is high, the avatar's entertainment value may become less salient to the consumer because an entertaining avatar cannot overcome the perceived risks associated with the purchase. The negative disconfirmation induced by the avatar's lack of behavioral competence should become more salient and detrimental to consumers' cognitive and social responses, resulting in weaker performance outcomes. For example, whereas the realistic, anthropomorphic appearance of HSBC's Amy seems effective because she was programmed to provide basic information to routine customer questions that do not involve high-risk transactions, the very realistic appearance of Nordnet's Amelia could not compensate for her lack of competence to offer stock advice (i.e., high financial risk transactions). Thus, we expect the following moderation effects: Proposition 6:  When an avatar's form realism exceeds its behavioral realism, its positive effect on customers' affective responses is weakened if ( 1) functional risk is higher, ( 2) financial risk is higher, and ( 3) the product is more expensive. Proposition 7:  When an avatar's form realism exceeds its behavioral realism, its negative effects on customers' cognitive and social responses are strengthened if ( 1) functional risk is higher, ( 2) financial risk is higher, and ( 3) the product is more expensive.Conversely, when an avatar's behavioral realism exceeds its form realism, it may have a stronger positive effect on customer performance outcomes under high perceived uncertainty. This is because an avatar's form realism may induce a positive disconfirmation regarding its behavioral competence, which can reassure customers of the usefulness of the information and personalized service provided by the avatar, thereby boosting customers' confidence in their risky purchase decisions. Another pertinent risk in today's world is that of privacy violations ([25]). If an avatar's behavioral realism exceeds its form realism, the avatar provides reassurance that customers are dealing with an intelligent, nonhuman entity that will not judge them, so it should mitigate social unease and embarrassment. The PTSD therapist Ellie works with highly private matters, involving patients disclosing emotionally and psychologically sensitive information. Such situations might evoke greater concerns if patients cannot tell whether they are dealing with a real human therapist behind the screen. Proposition 8:  When an avatar's behavioral realism exceeds its form realism, its positive effects on customers' cognitive and social responses are stronger if (a) functional risk is higher, (b) financial risk is higher, (c) the product is more expensive, and (d) privacy risk is higher.As avatars are also increasingly used in mobile apps (e.g., ING's Inge, Progressive's Flo, Bank ABC's Fatema), the choice of mobile or fixed devices as platforms on which the firm decides to provide an avatar also may be pertinent. Compared with fixed devices (e.g., desktops), mobile devices are particularly entertaining as hedonic information technologies (e.g., video games, MP3 players) have made their way into these portable devices ([103]). Moreover, consumers spend more time in online communities when they use mobile rather than fixed devices ([66]), suggesting that mobile devices are the preferred channel for online social experiences. Thus, avatars that are entertaining and capable of establishing personalized social connections with customers may be especially effective on mobile devices. Proposition 9:  As form realism increases, relative to behavioral realism, the use of mobile devices (compared with fixed devices) strengthens the positive effect of avatars on customers' affective responses. Proposition 10:  As behavioral realism increases, relative to form realism, the use of mobile devices (compared with fixed devices) strengthens the positive effect of avatars on customers' social responses.Finally, the effects of customers' cognitive, affective, and social responses on performance outcomes may depend on the consumer relationship phase, which refers to the relational trajectory of a customer–seller exchange ([29]; [77]). During consumer–avatar interactions, three relationship phases are particularly important: exploration, build-up, and maturity ([47]). During the exploration phase, a consumer is primarily concerned with the potential value and benefits of dealing with the seller, so the perceived informativeness of the avatar becomes especially critical. During this initial stage, entertainment or social engagement provided by the avatar might even detract from the consumer's task objectives and compromise customer outcomes. In this stage, avatar design should allow for behavioral competence, so that the avatar can provide accurate, task-specific, customized information rather than focusing on designing highly attractive or socially engaging avatars. Such an approach should produce a positive disconfirmation for customers' cognitive experience. As the relationship proceeds to the build-up phase, the consumer has experienced some benefits from interactions with the firm's avatars, so socialization processes (e.g., building rapport) become more important. Engaging and fulfilling social experiences can help customers develop long-term commitment to the firm. A positive disconfirmation about the avatar's ability to deepen social bonds might prove especially effective. Finally, during the maturity phase, a consumer satisfied with the cognitive and social benefits of interacting with an avatar may focus less on these factors and instead seek more entertainment value, prioritizing affective responses. Proposition 11:  The effects of customers' cognitive, affective, and social responses on performance outcomes are moderated by the relationship phase, such that (a) the effects of cognitive responses are amplified in the exploration phase but suppressed in the build-up and maturity phases, (b) the effects of affective responses are enhanced in the maturity phase but suppressed in the exploration and build-up phases, and (c) the effects of social responses are strengthened in the build-up phase but weakened in the exploration and maturity phases. Integrated Framework of Avatar PerformanceTo synthesize these insights, we offer an integrated framework of avatar performance (Figure 3). This framework provides a visual summary of key insights from our review of extant research and business practices, as well as our development of the avatar taxonomy and propositions. With this framework, we work to advance thought in this emerging, contemporary marketing area by integrating three mediation mechanisms (customers' cognitive, affective, and social responses) together with theory-driven moderators to test theory as well as offer managerial implications.Graph: Figure 3. An integrated framework of avatar performance. Managerial Implications and Research Directions for Avatar-Based MarketingOur integrative analyses of academic research and business practices generate practical implications and research directions for avatar-based marketing, which we group into five key managerially relevant areas: ( 1) when to deploy avatars, ( 2) avatar form realism, ( 3) avatar behavioral realism, ( 4) form–behavioral realism alignment, and ( 5) avatar contingency effects (Table 3).GraphTable 3. Managerial Implications and Research Directions for Avatar-Based Marketing. Key Issues/DecisionsImplicationsDirections for Future ResearchAvatar deploymentAvatars can be used to humanize a brand with scalable, cost-effective, responsive (24/7), humanlike interactions.How can avatar–human collaborations be optimized?Avatars can be used when the scale of service requirements and customer inquiries overwhelm company employees (e.g., financial, travel, telecom services). The employees can use their new free time for more complex issues.When the customer encounters a problem with the avatar, which type of ""exit ramp"" to a human employee is most effective: avatar-initiated, employee-initiated, or customer-initiated, and when during an interaction should it be deployed?Avatars can be used to enhance customer engagement and relationship building through emotional connection, personalization, and service consistency.In the event of an avatar service failure, what service recovery strategies should be employed?Avatars can be used to offer multichannel flexibility based on segment preferences (e.g., mobile social media, company website, dedicated apps).To what extent can a successful/failed service recovery experience shape customers' (dis)confirmation of the avatar's effectiveness?Avatar form realismAvatars' anthropomorphic appearance is a double-edged sword. A more humanlike appearance appeals to consumers, due to enhanced entertainment value, but it also raises consumers' expectations of the avatar's behavioral competence.Which dimension of the avatar's anthropomorphic appearance (i.e., spatial dimension, movement, and other human characteristics) has the greatest impact on consumers' expectations for the avatar's behavioral realism?If behavioral realism falls short of expectations, a negative disconfirmation will be produced, resulting in lower levels of customers' cognitive and social responses to the avatar.When might a digital assistant without a visual representation (e.g., Amazon's Alexa) outperform an avatar with an anthropomorphic appearance?Avatars' form realism should not exceed the level of its behavioral competence to avoid unfavorable customer experiences.Which avatar form realism elements create the most entertaining avatar experience?Avatar behavioral realismAn avatar's behavioral competence is a more impactful design factor than its appearance. In case of a budget constraint, more resources should be allocated to improving avatar behavioral competence than to enhancing its visual appearance.What is the role of avatar emotional intelligence, relative to its cognitive abilities, in shaping customers' expectations and overall experience?The higher the avatar's behavioral competence relative to its appearance, the more favorable the customer's cognitive and social experiences, due to a positive disconfirmation.What corrective actions can be taken to redress a negative disconfirmation stemming from avatar's behavioral realism?As a caveat, high levels of behavioral competence may produce negative effects (e.g., social desirability bias) when the avatar also has a very realistic anthropomorphic appearance.How might other types of avatars (e.g., customers' self-avatars) facilitate social media-based marketing campaigns?Which behavioral realism elements have the greatest impact on customers' expectations?Form realism–behavioral realism alignmentHigh form realism induces high behavioral realism expectations, which will then be confirmed.What are the unique benefits, challenges, and risks associated with using high-realism avatars in brand campaigns?The additive nature of high expected behavioral realism and its subsequent confirmation produces satisfactory customer experience with the avatar.When would avatar virtual influencers (e.g., Lil Miquela) be more effective than human endorsers in brand promotion?Alignment of high form realism–high behavioral realism results in high levels of customers' affective, cognitive, and social experiences, which subsequently increase firm performance.How should hyper-realistic avatars be deployed in marketing campaigns?Avatar contingency effectsConsumers' expectations that the avatar's anthropomorphic appearance reflects a comparable level of behavioral competence will be more pronounced when consumers' perceived uncertainty (e.g., the product's functional performance, financial risk) is high.How might customer segmentation strategies (e.g., psychographics, benefits sought) inform effective avatar designs?Behavioral realism should account for more weight in avatar design decisions than form realism, especially when customers' perceived uncertainty is high.What form realism–behavioral realism elements are most relevant and impactful, given a specific customer segment profile?When the avatar is designed with a low level of behavioral competence, companies should manage customers' expectations by giving the avatar a less realistic, less humanlike appearance (i.e., simplistic avatar). Avoiding a design with high form realism–low behavioral realism (i.e., superficial avatar) becomes even more important when customers' perceived uncertainty is high.How do avatar mediation mechanisms differ across customers in different segments?When the exchange entails privacy concerns (e.g., mental health), an avatar characterized by low form realism–high behavioral realism (i.e., intelligent unrealistic avatar) may be more effective than an avatar with high form realism–high behavioral realism (i.e., digital human avatar), because it reassures customers that they will not be judged and promotes more honest responses.In which circumstances will avatars likely distract from, rather than contribute to, customers' experience?Use of mobile devices (e.g., smartphones), compared with fixed devices (e.g., desktops), can enhance avatars' impact on consumers' affective and social experiences.What contextual factors determine the relative effectiveness of avatars vs. other digital entities (e.g., anthropomorphized products, brand mascots) in online shopping experience?Avatar designs should account for the consumer relationship phase. During the exploration phase, avatar behavioral realism should focus on providing the best cognitive experience; during the build-up phase, avatar design should be directed at enhancing consumers' social experience (e.g., rapport) to promote commitment; during the maturity phase, emphasizing the entertainment value of the avatar (e.g., funny, attractive) may prove more effective in sustaining the established relationship. In terms of when and where avatars should be used, avatars seem to be most effective in service-oriented industries (e.g., financial, travel, telecom services), in which the sheer scale of service requirements and customer inquiries can easily overwhelm a company's employees. Avatars can free up employees' time, so that the employees can focus on complex customer needs and offer more value-added services, with greater productivity. Avatars can provide consistent, personalized service and help build emotional connections between the firm and its customers ([22]; [52]). For companies that serve a large portfolio of customers, avatars can also make it feasible to launch a segmented, multichannel strategy. By offering customers opportunities to engage with avatars through different channels (e.g., social media, company websites, dedicated apps), the firm ensures that it meets each customer's unique needs at the right time and in the right place.After confirming that an avatar should be implemented, the firm should determine the design of avatar's appearance, with the clear recognition that its form realism is a double-edged sword. On the one hand, a more realistic, anthropomorphic appearance appeals to consumers, because it offers greater entertainment value ([78]). On the other hand, it elevates customers' expectations of the avatar's behavioral competence, which is much more difficult and costly to develop. If the avatar's behavioral competence falls short of customers' expectations, they experience a negative disconfirmation, which can decrease their satisfaction. To avoid negative performance outcomes, an avatar's anthropomorphic appearance should not exceed the level of its behavioral competence.Having decided on the avatar's appearance, companies can design the avatar's behavioral competence. If companies lack the resources to develop high form–high behavioral realism avatars (i.e., digital human avatars), they should allocate more resources to the avatar's behavioral intelligence than to its appearance. A positive disconfirmation of the avatar's cognitive and social competence likely produces an above-average level of customer satisfaction or even customer delight ([34]), which can increase firm performance.Managers should account for form realism–behavioral realism alignment too. If an avatar has high levels of both form and behavioral realism, customers' high initial expectations about the avatar's behavioral performance will be confirmed. Because customer satisfaction is an additive function of ( 1) initial expectations of the avatar's behavioral competence and ( 2) subsequent confirmation or disconfirmation of this expectation ([76]), the high form realism–high behavioral realism alignment will likely produce high levels of affective, cognitive, and social responses in consumers, as well as better outcomes overall. Additional research is needed to determine the ""zone of tolerance"" and the precision required when aligning form and behavioral realism of avatars.Finally, in addition to considering uncertainty factors and media channel choice, design efforts should take the customer relationship phase into account, because the relative effects of customers' cognitive, affective, and social responses differ across relationship stages. For example, during the exploration phase, a positive confirmation regarding an avatar's behavioral realism can ensure good cognitive experiences (e.g., cognitive trust), but during the maturity phase, a more entertaining avatar (e.g., funny, attractive appearance) may prove more effective for sustaining the relationship. Future research that takes a lifecycle approach to avatar design and use could determine avatar effectiveness at each stage and the strategies needed to adapt in accordance with customers' dynamic needs. Research DirectionsOur analysis of avatar design strategies indicates some promising research opportunities. First, propositions derived from our conceptual framework provide opportunities for empirical research, which can be tested using different methods. For example, researchers might collaborate with an avatar design company to manipulate form and behavioral realism in a 2 × 2 full-factorial experiment, consistent with our taxonomy in Figure 2. These results would provide evidence of the effects of form realism on expectations for behavioral realism (P1), as well as (dis)confirmations induced by any (mis)alignment (P2). Moderation tests also might be carried out with lab experiments that allow for manipulations of uncertainty factors (e.g., risk, price, privacy) or channels (e.g., mobile app vs. desktop) (P5–P10). To test the mediation effects, researchers might use a difference-in-differences field experiment to examine the aggregate main effect. Using a low form realism–low behavioral realism avatar as a baseline (control group), researchers could increase form (treatment 1 group) and behavioral (treatment 2 group) realism. Any significant differences in daily sales across the treatment groups and control group, between the pre- and posttreatment periods, would indicate the external validity of the asymmetric effects of form realism–behavioral realism misalignment (P3, P4). To confirm the distinct mediation effects, customers also could be surveyed regarding their cognitive, affective, and social responses, shortly after the treatments, followed by tests of the effects on performance outcomes (e.g., purchase intentions, word of mouth). Alternatively, a lab experiment can be used for differential mediation tests. As for the test of the relationship phase (P11), a cross-sectional survey posted on a retail website that uses an avatar might enable researchers to conduct subgroup analyses or moderated regressions to detect differential effects of mediators across relational phases. A more demanding and rigorous approach would secure panel data from a collaborating retailer that uses avatars and agrees to let the researchers track customers' relational trajectories through longitudinal surveys, while also granting them access to objective customer sales data.Research opportunities also exist in areas that our framework does not cover. Avatars are designed to enhance the productivity of company employees, rather than replace them altogether, so continued research might investigate how to optimize avatar–human collaborations, especially if problems arise. Various approaches allow customers to switch to a human representative when necessary, using avatar-initiated, employee-initiated, or customer-initiated ""exit ramps."" These approaches can differ in their nature (proactive vs. reactive) and timing (early vs. late in the interaction). Thus, determining how and when human intervention is introduced could provide significant insights for achieving service recovery and ensuring customers' overall satisfaction with and commitment to the firm. In this article, we have highlighted the influence of an avatar's form realism on the overall customer experience. Additional research should uncover which specific dimensions of the avatar's anthropomorphic appearance exert the strongest impacts on customers' behavioral realism expectations, which form realism elements create the most entertaining avatar experience, or when a digital assistant without an anthropomorphic appearance (e.g., Amazon's Alexa) would perform better than an avatar. For example, if the avatar's form realism exceeds its behavioral realism, which subset of behavioral elements is most likely to lead to customers' expectation disconfirmation? What corrective actions would be effective in addressing negative disconfirmations? Research also could delve deeper into the effects of avatars' emotional intelligence, relative to their cognitive intelligence, in shaping customers' expectancy (dis)confirmations and overall experience.Another type of avatar that is growing in popularity is customers' self-avatars, created with virtual model technology ([89]). To inform research into these applications, the typology we developed would need to be modified to reflect the unique characteristics of self-avatars (e.g., resemblance to self). Some designer brands (e.g., Gucci) have successfully engaged customers to dress their self-avatars in branded products, then share them on social media ([16]). The limited research on self-avatars has focused primarily on enclosed online environments (e.g., retailer's website), not social media ([20]; [35]), indicating the pressing need for insights into how, why, and when self-avatars could perform in social media marketing campaigns.Brands also have turned to virtual influencers (3D, computer-generated personalities) instead of or in addition to human influencers for online marketing campaigns. Powered by advanced AI, these avatars can attract significant followers on social media platforms. For example, with almost 3 million Instagram followers, Lil Miquela has endorsed brands such as Prada and Calvin Klein ([ 1]; [ 6]). This avatar actively replies to social media comments, appears in publications like Vogue, and even participates in live media interviews ([19]). While virtual influencers offer unique benefits to brands such as content control and versatility, they also create potential risks. For example, 61% of consumers assert that authentic, relatable content is the primary appeal of human influencers ([80]), but only 15% of followers of virtual influencers describe them as credible ([21]). Moreover, because the virtual influencer avatar is not a human, the brand it endorses ultimately is held responsible for its actions. Academic research has yet to investigate the unique benefits, risks, and operational mechanisms associated with avatar-based virtual influencer marketing.Future research might also explore avatar-based targeting strategies. Demographics, psychographics, and benefits sought are widely used customer segmentation bases ([95]); they also might be used to predict which customers will be best served by a given type of avatar. For example, customer's demographic traits might interact with the avatar's demographic attributes or behavioral elements to influence customer's cognitive, affective, and social responses to the online experience. Creating a personality or decision-making style for an avatar that matches those of the customer might be an effective, psychographics-based design strategy ([ 4]). Segmenting markets on the basis of in-depth analyses of the motives that lead certain people to interact with avatars could also inform benefits-based avatar deployment strategies ([11]). Distinct mediation mechanisms could be uncovered across different customer segments to inform idiosyncratic avatar designs. Research is also needed to determine when avatars may distract from, rather than contribute to, the customer's experience, as well as find strategies to address these challenges. Finally, to extend beyond our focus on the relative effects of the different types of avatars established by our taxonomy, future research might compare the effects of avatars with the impacts of other digital representations such as emoji, anthropomorphized products, brand mascots, or voice-only digital assistants. Understanding when and how avatars, versus these alternative marketing tools, are more effective in influencing online shopping experience and performance outcomes demands further scientific inquiry. ConclusionRapid increases in the use of avatars have been fueled by two main factors: advances in digital technologies and increasing reliance on online experiences among both consumers and firms. The use of avatars is projected to grow by 35% annually ([41]). However, the effectiveness of avatars continues to be uncertain, so we offer an integrated theoretical framework to establish definitional and conceptual clarity, synthetize academic research and business practices, and offer insights and propositions that provide managerial implications and an agenda for future research. The proposed definition of avatars, as digital entities with anthropomorphic appearance, controlled by a human or software, with an ability to interact, helps us establish a design typology, which in turn gives academics and managers insights into how to isolate elements that make avatars more or less effective for specific goals.We synthesize academic literature and business practices by offering a 2 × 2 form realism–behavioral realism taxonomy, which in turn enables us to derive propositions regarding the effectiveness of avatars in marketing. The level of alignment between an avatar's form realism and behavioral realism, according to several contingencies, can provide a parsimonious explanation of when an avatar will be most effective. With insights gained from our investigation of fundamental avatar elements, extant research, and business practices, we develop an integrative framework of avatar performance that offers theoretical insights, research propositions, managerial implications, and future research directions. "
3,"Analyzing the Cultural Contradictions of Authenticity: Theoretical and Managerial Insights from the Market Logic of Conscious Capitalism This research analyzes the cultural contradictions of authenticity as they pertain to the actions of consumers and marketers. The authors' conceptualization diverges from the conventional assumption that the ambiguity manifest in the concept of authenticity can be resolved by identifying an essential set of defining attributes or by conceptualizing it as a continuum. Using a semiotic approach, the authors identify a general system of structural relationships and ambiguous classifications that organize the meanings through which authenticity is understood and contested in a given market context. They demonstrate the contextually adaptable nature of this framework by analyzing the authenticity contradictions generated by the cultural tensions between ""conscious capitalism""—a market logic that encompasses both global brands and small independent businesses, such as a farm-to-table restaurant or an organic food co-op—and the elitist critique. The Slow Food movement provides a case study for analyzing how consumers, producers, and entrepreneurs who identify with conscious capitalist ideals understand these disauthenticating, elitist associations and the strategies they use to counter them. The authors conclude by discussing implications of the analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist brands.Keywords: authenticity; brand image; conscious capitalism; ethical consumerism; consumer identity; market logics; semioticsConsumers crave authenticity—so much so that their quest for authenticity is considered ""one of the cornerstones of contemporary marketing"" ([11], p. 21). This has created an enormous challenge for the field, considering that marketing itself is typically considered inherently inauthentic. —[67], p. 1)In the field of marketing, little doubt exits that ""authenticity"" is highly desired by consumers and thereby is a crucially important strategic resource for marketing management. Consumers are more likely to form stronger emotional attachments to a brand, business, or tourist site they perceive as being authentic ([20]; [30]; [57]; [86]) and to incorporate these market resources into their identities ([ 7]; [11]; [45]). On the managerial side, [31], p. 610) conclude that authenticity is ""the most rare and coveted asset in the contemporary branding landscape."" Their assertion is supported by an array of studies indicating that authenticity is integral to the enhancement of brand equity ([60]), effective brand extensions ([82]), persuasive marketing communications ([ 5]), success in relationship marketing ([22]), and emotionally engaging person and celebrity brands ([31]; [87]).Although there is a clear consensus that authenticity profoundly matters to both consumers and marketers, the marketing literature also presents a recurrent concern that authenticity is a nebulous concept that has eluded precise definition ([ 5]). [67], p. 2) proclaim that this conceptual ambiguity poses a significant barrier to creating ""a coherent theory of authenticity."" Accordingly, they aim to redress this dilemma by presenting a general definition of authenticity based on six key perceptual components. In contrast, [81], p. 3) proposes that ""authenticity is a polysemous and multilayered concept"" and thus ""it might not [emphasis added] be helpful to compress the wealth of disparate meanings associated with the concept into a single definition.""As Södergren further notes in his meta-analysis, ""the majority of the research [on authenticity] has focused on characteristics that distinguish the 'real thing' from the fake"" (p. 11). To further elaborate on this conceptual tendency, marketers' efforts to define authenticity almost invariably invoke some variant of genuineness, such as brands (via their management teams) staying true to ideals of timeless tradition, heritage, craftsmanship, and quality (see also [ 6]; [82]). In this spirit, [55], p. 30) propose that the authenticity of a brand's social media communications hinges on perceptions of honesty, sincerity, and being ""real."" [ 7] similarly contend that the core cultural meanings of authenticity are truth, genuineness, and reality. [67] comprehensive definition of authenticity also incorporates a series of veracity-oriented constructs, such as originality (i.e., not being a copy), accuracy (i.e., being true to others), and integrity (i.e., being true to oneself).While the analytic goal of distinguishing the authentic from the inauthentic makes intuitive sense, it is a Sisyphean undertaking that attempts to specify an ambiguous cultural category by referring to other semantic terms whose meanings are also contextually contingent and malleable (i.e., honesty, sincerity, originality, genuineness, and truthfulness). Furthermore, informing marketing managers that their brand lacks authenticity because consumers see it as being unoriginal, insincere, or dishonest offers little guidance on how to resolve the deeper cultural tensions that drive these unfavorable perceptions. Rather than a checklist of definitional attributes, we argue that marketing managers need an analytic approach that can enable them to answer questions such as ( 1) why is their brand or business susceptible to certain kinds of authenticity challenges?, ( 2) what cultural meanings and contradictions underlie those challenges?, and ( 3) what responses could they take to mitigate the disauthenticating associations that ensue from these tensions?Returning to our opening vignette, we can reframe [67], p. 1) statement that marketers face an ""enormous challenge"" because their profession is ""typically seen as inauthentic"" (see also [ 5]) as a realization that marketing, as a business practice, also occupies an ambiguous cultural position. On the one hand, marketing aims to advocate for the needs (and voices) of customers ([41]) and, yet, it is also means for companies to enhance their profits and market share. This tension readily gives rise to concerns that short-term (and potentially exploitive) profitability goals might take priority over serving customers' best interests. Accordingly, consumers are inundated with cultural narratives (ranging from journalistic reports about deceptive marketing tactics to portrayals of unscrupulous marketers by entertainment media) that encourage cynicism and distrust toward marketers' branding claims and persuasive communications ([32]; [45]; [66]).However, the specific cultural meanings and associations that lead to perceptions of authenticity or inauthenticity vary across brands and markets. For example, consumers are likely to deploy different configurations of meanings, beliefs, and evaluative norms when judging the authenticity of a high-fashion retailer ([23]), a café owner who promotes their establishment as a home-away-from-home ([20]) or a global brand that positions itself as an advocate for environmental justice (e.g., Patagonia; [49]).In this article, we explain and demonstrate how the semiotic square ([40]) can be used to systematically analyze such culturally heterogeneous authenticity contradictions and to develop contextually appropriate responses to the specific authenticity challenges that arise in a given market. The semiotic square is an analytic tool that has often been used to delineate cultural meanings and semantic contradictions that are manifest in both consumer perceptions and marketing strategies ([29]; [36]; [50]; [51]; [54]; [68]; [69]). From a semiotic perspective, the cultural categories of the authentic and the inauthentic are not just contrasting or oppositional terms. Rather, they are anchor points in a broader network of relationships through which the authenticity of a given brand, business, brand ambassador, social media influencer, and the like is culturally constructed and potentially contested.Our market context is conscious capitalism, which refers to a ""way of thinking about capitalism and business that better reflects where we are in the human journey, the state of our world today, and the innate potential of business to make a positive impact on the world"" ([62], p. 273). Conscious capitalism is particularly vulnerable to the broader authenticity–inauthenticity tension that all marketers confront to varying degrees. Therefore, it serves as a very relevant and informative context for our analysis.Conscious capitalism's key premise is that capitalism's societal purpose has, historically, been defined too narrowly (i.e., maximizing shareholder wealth and optimizing consumers' market choices) and, accordingly, its society-enhancing potential remains greatly underutilized. Rather than grafting a social mission onto a traditional profit-maximization model, as per conventional corporate social responsibility approaches, proponents of conscious capitalism contend that businesses should place value-driven goals and social consciousness at the core of their institutional missions ([62]).By aiming to redefine the nature and function of capitalism, conscious capitalism can be analyzed as a market logic that transcends its iconic brands (e.g., Patagonia, Starbucks, TOMS, Whole Foods) or socially conscious businesses (e.g., a cooperatively owned, fair trade, local coffee shop). As discussed by [27], pp. 40–42), a market logic is an integrated network of meanings, values, and norms that provide ( 1) principles that can guide thoughts, actions, and preferences; ( 2) vocabularies of motivation and justification; and ( 3) material and symbolic resources for constructing an identity (such as being an ethical consumer or a purpose-driven business owner).Conscious capitalism organizes a constellation of ideologically aligned brands and an even larger network of businesses that have different scales of operation and serve different roles in the supply chain. Thus, consumers who support this array of brands and enterprises have access to a set of normative principles to guide their purchase choices (e.g., locally sourced materials are preferred over imported ones, plastic product packaging should be avoided); they learn an intricate system of terms and codes (e.g., ""postconsumer recycled content,"" third-party certification labels such as the Rainforest Alliance or Certified Carbon Neutral); and they can express their socially conscious sensibilities through an array of consumption practices—wearing a Patagonia fleece, driving an electric car, shopping at a farmers' market, brandishing a reusable Whole Foods' canvas tote bag, buying fair trade chocolate, or supporting a farm-to-table restaurant.In the general public discourse, however, the authenticity of conscious capitalist brands and businesses, and their consumer supporters, is frequently called into question. These authenticity challenges are sufficiently problematic that leading proponents of conscious capitalism feel compelled to address them:There is a growing network of people building their companies based on the idea that business is about more than making a profit. It's about higher purpose ... and the innate potential of business to make a positive impact on the world.... But one of the most predictable responses we get from people when we mention the idea of conscious capitalism is, ""That's an oxymoron!"" ([61])Conscious capitalism's authenticity challenges hinge on a cultural tension between the profit-maximizing ethos of capitalism and the ennobling idea that capitalist enterprises can serve higher societal and moral purposes that supersede commercial interests ([ 3]; [33]). In this vein, critics often suggest that conscious capitalism deploys the language of sustainability and other socially beneficial goals for the instrumental purpose of catering to higher-income consumers who will pay a premium to imbue their consumption practices with an aura of moral virtue:Thus, the rise of social enterprises [i.e., conscious capitalist enterprises] has been met with hostility, particularly toward its authenticity and its sustainable impact. If their goods and services continue to be priced as they are, is the sustainable movement only for the demographic that can afford it? ([14])[70], p. 73) similarly argue that conscious capitalism is more promotional hyperbole than a viable business reality and, further, add this reservation: ""It is important to note that the firms associated with the Conscious Capitalism movement are far from a random sample of American businesses: In fact, a great many sell relatively expensive products to relatively affluent, socially- or health-conscious consumers.""This incredulous and, at times, adversarial public response to conscious capitalism has not arisen ex nihilo. Rather, it draws from a cultural narrative that we characterize as the ""elitist critique."" As historian [34] elaborates, the political charge of elitism has evolved from its classic populist roots, which railed against the undue power wielded by the captains of industry and affluent political insiders, to an antipathy toward the intellectual class (who may not be unduly wealthy or politically powerful). Through this shift, the charge of elitism was distanced from its origins in economic conflicts between the working class and the owners of capital (and their management intermediaries) and became repositioned in a culture war rift whereby ""the 'elite' could be identified by its liberal ideas, coastal real estate, and highbrow consumer preferences"" ([34], emphasis added).We investigate how the specific authenticity challenges posed by the elitist critique of conscious capitalism are negotiated by consumers and producers in the context of the Slow Food movement ([89]). Slow Food encompasses an array of ideologically aligned brands, enterprises (farm-to-table restaurants, artisan producers, and organic and free-range farmers), consumption practices (e.g., shopping at a farmers' market or a local co-op), and goods and services (e.g., an heirloom tomato, grass-fed beef, a class in fermentation techniques). Slow Food's signature issues and social change goals are grounded in the market logic of conscious capitalism, including local sourcing, fair wages for workers, sustainable modes of production, environmental awareness and habitat protection, and a broader project of redressing societal ills through the coordinated actions of socially conscious businesses and consumers (see [73], [74]). The elitist critique has also become part and parcel of Slow Food's brand image, and it poses salient authenticity challenges for Slow Food's producers, entrepreneurs, and consumers.In the following sections, we first discuss the key analytic premises of the semiotic square. Next, we develop a semiotic conceptualization of authenticity that maps out its structural contradictions (and ambiguous classifications). We use this analytic framework to explicate the ways in which the elitist critique gives a particular cultural form to the authenticity contradictions plaguing the market logic of conscious capitalism. We then profile the authenticating strategies that Slow Food advocates (consumers, producers, and restauranteurs) use to counter these disauthenticating elitist associations. We conclude by discussing the implications of this analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist enterprises. The Semiotics of Authenticity The Semiotic Square as an Analytic ToolFrom a semiotic perspective ([40]), the meaning and categorical boundaries of a given concept are defined through relations to what it is not. For example, the cultural meanings of masculinity have been historically established through contrasts to those that have defined femininity and the related nexus of ever-changing ideals, values, and practices through which this binary contrast has been culturally articulated and transformed over time ([50]). These structural relations give rise to ambiguous categories whose associated cultural meanings can become points of contestation and debate, such as in the cases of ""metrosexuals"" ([78]), stay-at-home dads ([19]), or the ongoing controversies sparked by the category of transgender athletes ([12]).The binary opposition between authenticity and inauthenticity presents a similar arrangement of contradictions and ambiguous classifications. Consequently, we propose that authenticity is not a set of discrete properties that distinguish the genuine from the fake—but, rather, an ongoing process of managing a network of contingent relationships. In some markets, for some brands and enterprises, these contingencies may be more stable, whereas in others, they may become more culturally contested and, thus, unstable. We suggest that conscious capitalist brands and businesses, owing to the elitist critique, exemplify this latter and more managerially challenging case.Figure 1 presents a semiotic square representation of authenticity. In this article, we use the ""contradictions of authenticity"" as an integrative term that encompasses the structural relations among the semiotic 3Cs (contrariety, complementarity, and contradictory relations).Graph: Figure 1. A semiotic model of the authenticity–inauthenticity opposition.The horizontal arrows represent contrariety relations. These relations are roughly analogous to the standard binary oppositions that anchor semantic differential scales. However, relations of contrariety further indicate that the meaning of a term is defined through a relationship to its binary contrast (e.g., good is understood relative to evil). Accordingly, the meaning of authenticity is always contingent on the operative meaning of inauthenticity, and vice versa. We refer to the authentic ↔ inauthentic contrariety as the primary contrariety relation because it represents the dominant tension that, in turn, sets the complementary terms for the secondary contrariety relation (i.e., not inauthentic ↔ not authentic).The vertical arrows represent complementarity relations. Such conceptual pairings are compatible and noncontradictory (but are not necessarily synonymous or interchangeable). For example, ""not inauthentic"" is congruent with the dominant term, authentic. However, this classification also harbors other connotations and, thus, ambiguous meanings. For example, imagine a painting created by a famous artist, say Picasso, who at the time was a fledgling beginner, imitating the style of another painter. Because the painting does not evince Picasso's quintessential artistic motifs, its authenticity becomes ambiguous (and debatable)—that is, at what point in his career does a painting by Picasso truly become a ""Picasso""? The term ""not inauthentic"" conveys this type of ambiguity.The diagonal arrows represent contradictory relations. These relations indicate that any entity or action deemed to be authentic (or inauthentic) will harbor some qualities that can be judged as contradicting such an assessment. As an illustration, let us again consider the idea of artistic authenticity. From a conventional standpoint, the authenticity of an artist, even a renowned one, can always be challenged on the grounds that their creations exhibit properties that are derivative of other genres, styles, or artistic predecessors (authentic ↔ not authentic). Conversely, the art world's postmodern movement disavows the idea of artistic originality and, instead, celebrates that all artistic productions are, in some sense, a reworking of something prior. As exemplified by Andy Warhol's replications of iconic cultural images (Coca-Cola bottles, Campbell Soup cans, the face of Marilyn Monroe), postmodern art is also heralded for its capacity to surprise and inspire revelatory aesthetic experiences through its creative (and often ironic) uses of repetition, collage, assemblage, montage, and bricolage (inauthentic ↔ not inauthentic) ([43]).[ 4] A Semiotic Conceptualization of AuthenticityIn Figure 1, the cloud-like drawings represent the specific cultural meanings that give contextual form to the contradictions of authenticity. For purposes of our analysis, the relevant meaning systems are the market logic of conscious capitalism and the elitist critique. This system of semiotic relationships gives rise to four emergent (and ambiguous) classifications, each harboring latent contradictions. In discussing these ambiguous categories, we first illustrate them in more general terms and then address their manifestations in the context of conscious capitalism and the elitist critique. Authentic + not inauthenticThis complementarity relation corresponds to what [39] discuss as indexical authenticity. In this usage, an index refers to a given object or behavior—for example, the actions of a whitewater raft guide, handprints in front of Grauman's Chinese Theater, a painting, or a branded good. Indexes are classified as authentic when they are believed to possess a factual and spatiotemporal connection to some validating condition. For example, consumers will judge the actions of a whitewater raft guide as authentic if they are believed to reflect an inner passion for the outdoors (rather than being a calculated performance done for remunerative purposes; [ 2]). Similarly, consumers will typically deem a branded good to be authentic when they believe its design, production, and quality certification has proceeded under the auspices of those who own or manage the brand of note.As these examples suggest, perceptions of indexical authenticity can be more or less certain. As an example of higher certainty, Prada certifies the genuineness of its handbags by assigning each a unique and traceable serial number that is documented on an authenticity card. On the less certain side, customers have to infer the indexical authenticity of a whitewater raft guide's passion for the outdoors or a retail associate's expressions of friendliness and interpersonal concern. In these cases, consumers' judgements about the authenticity (or inauthenticity) of a marketer's actions (or the actions of other consumers) depend on their inferences about underlying motivations and intent.The elitist critique provides a constellation of culturally shared meanings and rationales that support disconfirming suppositions about the indexical authenticity of conscious capitalist enterprises and their consumer followers. These disauthenticating associations directly correspond to the ambiguous categories emerging from the primary contrariety (authentic ↔ inauthentic), the secondary contrariety (not inauthentic ↔ not authentic), and the complementarity relation of inauthentic ↔ not authentic relations. Authentic + inauthenticThis ambiguous classification corresponds to seemingly oxymoronic constructions such as authentic reproductions—or, in semiotic vernacular, ""iconic authenticity"" ([39]). In this usage, the icon is an object that is a known facsimile of an original referent and that is appreciated for its mimetic properties, as in the case of a comedian doing an uncanny impression of a celebrity. For the category of iconic authenticity, the ensuing goal is to present a compelling sense of verisimilitude through a meticulous recreation of the original referents' characteristics. Iconic authenticity is pursued by, among others, members of the cosplay community ([80]) and consumers who perform in historical recreations, such as Civil War reenactments ([17]). In a different market application, iconic authenticity would also be highly relevant to a budget-conscious consumer who wants to buy a convincing counterfeit version of an expensive designer brand.When situated in the context of the elitist critique, the ""authentic + inauthentic"" category assumes less favorable meanings of moral pretentiousness and hypocrisy. In this disauthenticating cultural frame, affluent (and typically left-leaning) consumers use conscious capitalist brands and goods to distinguish themselves from the price-conscious mainstream and their socioeconomic peers who display affluence through more ostentatious lifestyle choices ([24]). By claiming the mantle of moral virtue, such consumers can pursue social distinction in an otherwise orthodox manner—that is, through material displays of refined tastes ([ 9]; [44])—while appearing to disavow materialism and status consciousness.For example, during its heyday as a cultural icon, the Toyota Prius inspired oppositional brand communities (Muñiz and O'Guinn 2001) who referred to the vehicle (and its drivers) as ""the pious."" This epithet suggested that Prius drivers evinced a self-aggrandizing ""holier than thou"" stance that amplified the moral merits of their automotive preferences relative to those who made different choices ([59]). In a similar cultural vein, [13] note that ecofriendly consumers who purchase organic foods, drive electric luxury cars, and use natural cleaning products typically lead lifestyles that carry a much higher carbon footprint than lower-income consumers who live in smaller housing units, rely on public transport, and seldom fly. Seen in this critical light, such ecoconscious (affluent) consumers are virtue signaling ([24]; [42]; [90]), but their pretense of moral superiority is not warranted by these symbolic acts. Not inauthentic + not authenticThis ambiguous classification highlights that perceptions of genuineness (often taken as the sine qua non of authenticity) are a necessary but not insufficient condition for ascribing this honorific appellation to an object or action. That is, an entity or action may be deemed as genuine (i.e., not fake) but lack the perceived aesthetic or moral virtues needed to be classified as ""authentic,"" or, conversely, to have its authenticity challenged. Thus, we can have marketplace conditions where authenticity, in its full moral and aestheticized sense, is not a relevant cultural category.To illustrate, barring extenuating circumstances, consumers seldom venerate conventional mass-produced goods (e.g., a Big Mac, a Gillette disposable razor) for their authenticity because they lack potent associations with rarefied aesthetic ideals. Conversely, such items are not typically classified as inauthentic either (assuming that they are not knock-off products), with companies often promoting the standardized nature of their branded offerings—and the resulting performance consistency—as value-added benefits.In the context of the elitist critique, the ""not inauthentic + not authentic"" classification suggests that middle-class consumers who support conscious capitalist brands and enterprises are, owing to their class privileges, inherently ""not authentic."" This disauthenticating implication hinges not on conscious intent but on the systemic advantages afforded by their relatively privileged socioeconomic position. Rather than being hypocritical per se (i.e., authentic + inauthentic), the implication is that such consumers may genuinely believe that conscious capitalism is a viable means to create a more equitable and just society. However, their genuine belief is an ideological one, steeped in their internalized class interests. Middle-class consumers' ideological affinity for the market logic of conscious capitalism allows them to lead a materially privileged lifestyle in a guilt-free manner ([92]). By purchasing brands and goods that signify a heightened social consciousness (e.g., fair trade coffee; TOMS shoes; organic, locally sourced foods), they can feel symbolically absolved from culpability in the perpetuation of socioeconomic inequalities. Consequently, their habituated class predilections also create an ideological blind spot toward the exclusionary signals that conscious capitalist ideals and values convey to those who lack the economic and cultural resources needed to fully participate in a middle-class lifestyle ([47]). Inauthentic + not authenticThis ambiguous classification suggests that conscious capitalists' products, services, and brands are, to use[66] term, ""gimmicks"" that always promise more than they can deliver. [21], p. 668) further discuss issues relevant to this classification in their typology of transactions. Among their designations of deceitful transactions (i.e., scams), they list ""fraud"" and ""confidence games."" In the former condition, a disingenuous party misrepresents their intentions to an unsuspecting partner; in the latter condition, the scammer actively enrolls their target in the ruse, such as in catfishing and pyramid schemes.The inauthentic + not authentic classification implies a manipulative opportunism whereby an unethical agent feigns genuineness to extract ill-gotten gains from another. In the context of conscious capitalism, this disauthenticating association is most germane to the marketer side of the exchange. As one well-known example, the business ethics journalist Jon Entine accused the pioneering conscious capitalist brand The Body Shop, and its founder Anita Riddick, of fraudulent misrepresentation. According to [25], Riddick stole the brand concept from a local entrepreneur and fabricated an authenticating origin story about traveling the world searching for natural skin care and hair treatments. His exposé further contended that The Body Shop significantly overstated the percentage of profits that it donated to philanthropic causes. Though Riddick formally denied these charges, the authenticity challenges posed by these accusations, as well as others that subsequently followed, continued to plague the brand ([76]). After years of underperformance, relative to the brand's prescandal pinnacle, The Body Shop undertook a revitalizing strategy that its management characterized as an activist revamp ([77]).The ""inauthentic + not authentic"" classification can also cast more nuanced doubts on the authenticity of conscious capitalist entrepreneurs. Although such conscious capitalist entrepreneurs would not be committing overt acts of fraud (i.e., they are not lying about their business practices per se), the disauthenticating implication is that they are cynically espousing higher-order civic ideals to serve commercial ends, such as charging a premium to their consumers or driving higher stock valuations. This disauthenticating association can arise, for example, when the founder/chief executive of a privately owned conscious capitalist brand sells its rights to a larger corporate entity. Such a backlash arose when Gene Kahn—the founder of Cascadian Farms—sold his business to General Mills. Many leading voices in the organic food community lambasted Kahn's integrity, condemning him as a Boomer sellout and warning that the brand's corporate ownership would not stay true to the higher-order values that originally galvanized the organic food movement (see [75]). Research ProceduresTo investigate how Slow Food advocates negotiate the elitist critique of conscious capitalism and its disauthenticating connotations, we recruited informants from a Slow Food chapter located in a metropolitan area of the Midwestern United States using informational flyers, contacts made at local chapter meetings, and snowballing referrals. We conducted interviews at public locations such as coffee shops or at Slow Food–sponsored events, with exception of two that respectively occurred in these participants' domestic residence and private work office. Interviewees were paid $20 in appreciation for their time. Interviews were audiotaped and ranged from one to four hours in duration, yielding 830 double-spaced pages of verbatim text. All participant names are pseudonyms.Of our 19 interviews, 8 were conducted with chapter organizers, 5 with Slow Food producers and entrepreneurs, and 6 with Slow Food advocates who had volunteered their time to different outreach activities (for our participants' profiles, see Table 1). Most of our Slow Food organizers and consumer advocates are college graduates employed in professional occupations and hail from middle- and upper-middle-class families. Among the entrepreneurs, Dave, Leslie, Maggie, and Tom are also college graduates. This demographic profile matches the membership ranks of Slow Food USA, which skews toward middle-class professionals ([16]).GraphTable 1. Participant Profiles. PseudonymGenderAge (Years)EducationOccupationSlow Food RoleAaronM28Ph.D.Research scientistMiddle-class advocateAlexM28Some collegeArtisan cheese makerEntrepreneurAmandaF31MBASmall business ownerChapter organizerBenM61M.S.Pursuing Ph.D.Middle-class advocateBrendaF44B.A.Former organic farmer, at-home momMiddle-class advocateCarolineF56B.A.Government employeeChapter organizerChrisM47B.ASoftware designerChapter organizerChristinaF44M.S.Nurse practitionerChapter organizerDaveM30B.A.Organic farmerEntrepreneurErinF29M.S.E-commerce food entrepreneurChapter organizerHankM59M.S.Interior designer, former chefMiddle-class advocateHeidiF58B.A.Professional chefMiddle-class advocateJaneF68Associate's degreeHolistic medicine practitionerMiddle-class advocateKevinM42B.S.Software designerChapter organizerLeslieF23B.SOrganic farmerEntrepreneurMaggieF34B.S.Free range farmerEntrepreneurPaulaF49M.S.Retail buyer and jewelry makerChapter organizerRichardM34B.S.Computer systems analystChapter organizerTomM35M.A.Farm-to-table restauranteurEntrepreneur 1 Notes: M = male; F = female; B.A. = bachelor of arts; B.S. = bachelor of science; M.A. = master of arts; MBA = master of business administration; M.S. = master of science; Ph.D. = doctor of philosophy.Following the conventions of in-depth phenomenological interviewing ([85]), our participants largely determined the course of the dialogue. The interviewer relied on follow-up probes to elicit more detailed accounts of the informants' experiences and viewpoints and to ensure that various aspects of food production, distribution, and consumption were covered. Procedurally, our interpretation developed through an iterative process of creating, challenging, and reworking provisional understandings by tacking back and forth between individual transcripts and the broader data set ([84]). We then pivoted to another level of hermeneutic tacking that entailed iterations between these emic narratives and theoretical concepts, which led us to the application of the semiotic square and our resulting focus on the elitist critique and corresponding strategies for countering the authenticity challenges posed by the cultural contradictions manifest in this market system. Contextual Background The Slow Food MovementAs an institutional entity, Slow Food is a transnational organization encompassing 1,500 local chapters plus numerous subsidiary organizations. Beyond its formal institutional boundaries, Slow Food's culinary practices, values, and activist goals organize ideological and economic alliances among a globally diffused network of food writers (such as Mark Bittman and Michael Pollan), consumers, producers, merchants, and restauranteurs (including celebrity chefs Alice Waters and Jamie Oliver). As [18], p. 131) writes, ""The phrase 'slow food' strikes a chord among the public not because it is the name of an organization but because it reflects a series of desires, interests and concerns.""Slow Food discourses valorize meals that are traditionally prepared with fresh ingredients as unique sources of pleasurable experiences that can mobilize consumers to resist the industrialized system of food production. Over the years, the Slow Food movement has embraced a broader conscious capitalist agenda that advocates for sustainable production, environmental protection, and social justice (i.e., fighting hunger, advocating for living wages for agricultural workers; see [16]; [89]). The Elitist Critique of the Slow Food MovementLike other conscious capitalist exemplars, Slow Food has also been plagued by charges of elitism from its inception in 1986 when its founder, Carlo Petrini, organized a series of public protests over the opening of a McDonald's in the heart of Rome (see [89]). This ignominious view of Slow Food finds ready expression in both academic analyses (e.g., Guthman 2007; [56]) as well as journalistic accounts, such as [38], who states that ""none of the aggressive, judgmental pitches of the movement have ever been proven. The power of its association with the economic elite has.""From this skeptical standpoint, Slow Food's exalted rhetoric of sustainable diets, biodiversity, and socially conscious eating (see https://www.slowfood.com/about-us/our-philosophy/) is a guise for privileging upper-middle-class tastes over the dietary practices of less affluent (and lower-cultural-capital) consumer segments (see [53]; [56]). Even Slow Food's ardent proponents, such as food writer Annie Levy, concede that a tacit elitism has hindered the cultural diffusion of its core principles:—The revered Alice Waters once said, ""when we eat food that is fast, cheap, and easy, we digest those very values."" What are the judgments contained in this kind of statement? She intends, I believe, to critique the values of a food system that doesn't care about its conditions or effects on people and the environment. But the words suggest that if you eat fast, cheap, and easy you become fast, cheap, and easy—language many women might recognize as shaming. Isn't this how it really sounds to someone who enjoys such food, or is caught in situations in which it might seem the best available option? ([58])Slow Food encourages consumers to shift their culinary tastes away from fast food and industrialized fare (including the oft-demonized category of junk food) ([16]; [83]). Such admonitions can imply a moralistic hectoring and an invidious comparison with those whose food tastes and practices are more orthodox. These elitist associations often cross into other sociocultural spheres, such as the controversy sparked by former First Lady Michelle Obama's school lunch initiative, which was institutionalized through the Healthy, Hunger-Free Kids Act of 2010. By explicitly disavowing fast food and processed foods, the revised school lunch guidelines dovetailed with Slow Food's mobilizing agenda—an alliance that Slow Food USA was eager to promote (see Figure 2).Graph: Figure 2. Fodder for the elitist critique: Slow Food's controversial political alliances.Once these Slow Food–friendly standards went into effect, news (and social) media began to feature anecdotal reports of children refusing to eat these presumably unpalatable lunches and skyrocketing food waste ([28]), with some critics characterizing the program as ""gastro-fascism"" ([71]). The elitist charge became integral to this cultural (and political) backlash:Michelle Obama thinks she knows what your children should eat. She's adamant about promoting her nutrition policies for kids, even the new and disastrous school meal standards implementing the ""Healthy, Hunger-Free Kids Act.""... But attending Ivy-League schools doesn't magically make someone better parent material than an individual who attended a public university, or, dare it be said, someone who didn't attend college. ([ 4])Like other market exemplars of conscious capitalism, Slow Food's aesthetic and experiential arguments have become strongly associated with an unwarranted moralism (i.e., the authentic + inauthentic classification; primary relation of contrariety). Slow Food advocates frequently argue that fast food is a debased cuisine that deprives humanity of meaningful and rewarding experiences of eating and sociability ([73]; [83]). For the many consumers who have warm memories of enjoyable fast-food meals with friends and family (and maybe look forward to such treats), Slow Food's moralizing pronouncements seem to emanate from an elitist taste bubble that is disconnected from everyday pleasures and real-world practicality. Similarly, Slow Food's veneration of locally sourced ingredients, heirloom vegetables and grains, artisan-crafted foods, and seasonal cuisine also seems to assert an unwarranted claim to moral virtue. Rather than sacrificing for a greater societal good, such rarefied culinary objects seem more attuned to signaling that one possesses the discretionary resources of time and money to treat food and cooking as a self-actualizing identity practice. Accordingly, Slow Food is easily, via the elitist critique, decried as an aggrandized form of cultural snobbery (e.g., [53]). Authenticating Strategies in the Slow Food MarketFigure 3 represents the correspondences between Slow Food's contextualized authenticity contradictions, the disauthenticating association that ensues from each contradiction, and the strategies through which Slow Food advocates seek to negate these authenticity challenges. In this representation, indexical authenticity (authentic ↔ not inauthentic) is the contested ideal that our Slow Food advocates are seeking to defend.Graph: Figure 3. Authenticity contradictions and authenticating strategies in the Slow Food market.Our Slow Food consumers place the most emphasis on the reflexive strategy, which they use to counter the authentic + inauthentic contradiction (primary relation of contrariety) and its disauthenticating association of virtue signaling and moral pretentiousness. Rather than rejecting the elitist critique outright, Slow Food advocates interpret it as a warning sign that the Slow Food market has become a gentrified facsimile of the movement's origins in the everyday cuisines of rural Italians (i.e., a disparaging version of iconic authenticity). Accordingly, our participants revere practices that seem to resurrect Slow Food's agrarian values and democratizing goals.The humanistic rebel strategy redresses the not inauthentic + not authentic contradiction (secondary relation of contrariety) and its disauthenticating association of social exclusion. In the context of the elitist critique, this contradiction holds that individuals whose lives have been shaped by class privilege may be blithely unaware of their own internalized elitist predispositions. From this standpoint, Slow Food advocates may have a genuine interest in making the world a better place (i.e., they are not consciously ""faking it""; rather, they are being ""not inauthentic""). However, they are largely oblivious to how their viewpoint on these problems and solutions has been shaped by a life of class privilege and their habituated, middle-class (bourgeoisie) sensibilities. This disauthenticating association renders Slow Food consumers as being somewhat akin to the proverbial fish in water. Rather than not realizing they are wet, however, the analogical implication is that they cannot comprehend that other terrestrial animals lack the requisite resources to enjoy life in the water, as they do.To negate this authenticity challenge, our participants drew from humanistic rationales, such as the idea that certain kinds of experiences and social connections have magical and transformative qualities that transcend social differences ([ 2]). Importantly, this strategy combines a humanistic ethos with the idea of rebelling against a deleterious marketing and cultural status quo and, thereby, creates a distinction to the complicit, part-of-the-problem connotations of liberal elitism (see [92]).The perfective strategy corresponds to the inauthentic + not authentic contradiction (relation of complementarity). This strategy is most relevant to those positioned on the entrepreneurial/production side of this market system. It aims to negate the disauthenticating association of commercialism (i.e., conscious capitalist enterprises are profit-seeking marketing ploys). In response, our Slow Food producers and entrepreneurs draw from the bohemian ideal of the artist who refuses to compromise their artistic vision, despite market incentives to ""sell out"" (i.e., betraying one's artistic integrity in return for financial reward) ([10]; [87]). Accordingly, they present themselves as being intrinsically committed to perfecting their Slow Food craft and pursuing conscious capitalist values and ideals, rather than doing it for the money. The Reflexive StrategySlow Food advocates use the reflexive strategy to negate the authenticity challenge of moral pretentiousness. The implication is that Slow Food assigns an unwarranted degree of moral virtue to those who have the economic wherewithal to buy rarefied ingredients, spend time on complex meal preparations, and dine at expensive farm-to-table restaurants while casting those who lack such resources as less virtuous consumers. In response, our participants interpret Slow Food's cultural associations with affluent foodies and elite taste practices as a regrettable, but correctible, market distortion of the movement's authentic values and practices.While acknowledging that market upscaling has imbued Slow Food with an elitist aura, our participants reiterate that expensive, epicurean cuisine need not be and, indeed should not be, regarded as the quintessential expressions of Slow Food:I think one of the things is this perception that if you shop at farmers' markets or at the co-op, it's a lot more expensive. And there is a little bit of this Slow Food bent into cooking elaborate meals, and I think some people perceive that as being elitist because it's sort of this educated way of thinking about food. I don't think of it as being elitist because a lot of times, recipes can be super expensive to buy all the ingredients for, but they don't have to be. I don't think that enjoying your food should be something that is thought of as elitist.... Like, I buy what's not super expensive at the co-op and I cook pretty simply.... What I really like about Slow Food in particular is the aspect of enjoyment and that good food is for all—what good, fair, clean food means for the farm worker to the people who are consuming the food. (Erin)Erin does not summarily dismiss the elitist charge. Rather, she takes a more ambivalent stance by first conceding that Slow Food values and ideals are often enacted in ways that can be read as elitist, such as cooking elaborate meals using expensive ingredients. In her authenticating interpretation, she counters that Slow Food values are better expressed through fundamentally simple meals that do not require extensive preparation time or costly ingredients. Her emphasis on there being many affordable options at her food co-op (which is, of course, a relative judgment) and on ""cooking simply"" (another relative assessment) convey that she is staying true to Slow Food's core principles rather than trying to place superficial, foodie predilections on a higher moral plane.Erin further counters this aspect of the elitist critique by incorporating the economic interests of farmers into her inclusive interpretation of Slow Food stakeholders. This interpretation creates a rhetorical contrast between Slow Food's foundational discourse of economic populism (emphasizing fair wages for agricultural workers) ([73]) and the elitist condemnation that higher prices are merely a means for affluent consumers to mark status distinctions.Paula's narrative exhibits a similar authenticating logic to that expressed by Erin:Slow Food has often come under fire for being elitist. I don't actually think that's true.... The beginnings of Slow Food were about people eating good food, and those were not necessarily rich people. We are talking about people who might have had very little money.... When most people think about amazing Italian cuisine, they were eating very basic foods. So, the whole idea of eating good food to me doesn't seem elitist at all.... Slow Food in the United States, yes, we do certain things that might be seen as elitist—the restaurant dinners and stuff like that. But again, you are still educating people. You are still getting more people involved. And the more people who know about local farming, sustainable farming, eating seasonally, making sure that farm workers are protected and paid properly, that spreads out. And we do projects with a variety of different populations, and we are trying to do more of that.... Slow Food does a lot of work in all its chapters to help with urban gardens or school gardens.... In the long run, our goal is that all people have access to this kind of food.... We are working toward passing that power on to more people. So, I don't think wanting children and families in need to have high-quality food is elitist. (Paula)In this vignette, Paula first differentiates Slow Food practices from elitist pretentions by invoking its historical connections to rustic Italian foodways. She asserts that Slow Food enjoins a pleasurable, resourceful, and fundamental relationship to food that should be accessible to people from all walks of life, rather than being an exclusive province of affluent consumers. However, Paula also recognizes that her inclusive rendering of Slow Food is contradicted by the realities of socioeconomic stratification. From Paula's viewpoint, Slow Food's community outreach efforts can play a pivotal role in democratizing these forms of culinary cultural capital so that consumers from less privileged backgrounds can acquire the skills and knowledge needed to incorporate Slow Food practices and ideals into their culinary routines.When utilizing this reflexive strategy, Slow Food advocates routinely assert that cultural capital ([ 9]), rather than a lack of economic resources per se, is the primary barrier that keeps consumers from integrating Slow Food ideals and practices into their everyday lives. Christina echoes this rationale when discussing how low-income consumers could enact Slow Food practices if they had more knowledge about utilizing the fresh produce and bulk goods that often go to waste in the local food pantry where she volunteers:Some people who are in Slow Food are foodies. However, it does not cost a lot of money to eat right. There are food pantries who throw away produce because people who come to the pantry don't know what to do with it and they don't take it.... Fresh produce going to waste.... No one wants it because they don't know what to do with it. It's really unfortunate. So, people have more access than they think. There are bulk aisles at grocery stores that you can get food for less money. It is actually a lot less expensive to buy bulk rice or bulk oats or whatever else, than to buy the bagged, boxed stuff that's like creamy preprocessed. I think the real lack of resource is education, not so much money. (Christina)Slow Food advocates often draw an authenticating contrast between foodie-ism—which fetishizes highly aestheticized meals prepared with exotic (and typically expensive) ingredients (see [52])—and the Slow Food ethos of preserving traditional foodways and skills ([75]). This distinction is quite salient to Slow Food chapter leader Kevin. He posits that Slow Food's culinary values and ideals have become misconstrued in their translation to the consumerist, status-conscious culture of the United States. Kevin's goal is to reclaim Slow Food's original ethos from its commercial appropriation by high-end retailers and restaurants:To buy imported cheese, organic wine, and all these kinds of things, I don't think those are meant to be the most obvious expressions of Slow Food values. And I think this is where the cultural translation from Italy to the United States went wrong, is that it got tied up with those folks [i.e., affluent foodies]. In Italy, it's much more about cooking at home. It's much more about preserving grandma's recipes. It's much more about celebrating the seasons and the tradition and preserving home ways of life than it is about eating in restaurants that do everything right. And you know, like anything else, capitalism wants to subsume this revolution.... That's a schism that I am personally trying to address and maybe lead by example. I don't think we should be cooking like a Michelin-starred restaurant at home. I think we should be cooking like our grandparents and great-grandparents. And I think we can learn a lot from traditional cultures and indigenous people—to the extent that there still are any indigenous people—how to eat well, and you know, a lot of those foods have become an affectation in restaurants. They'll have poutine, but it's made with truffles, and confit duck and elaborate things. I have realized that we are all attracted to the comfort foods and the simple foods, like tacos, and they are easy and fun to make. (Kevin)For Kevin, Slow Food should be accessible, basic, fun, and easy—characteristics that diverge from associations with rarity, cosmopolitan sophistication, aesthetic refinement, and technical proficiency that mark elite tastes ([44]). If read in a more critical light, Kevin's narrative reiterates the nostalgic glossing of preindustrial foodways that critics of the Slow Food movement assail for ignoring the harsh realities of scarcity and subsistence endured by those who had to survive on ""traditional diets"" ([56]).While romanticizing images of a bucolic culinary past have considerable appeal to our Slow Food advocates, the idea of rekindling a premodern utopia is not that central to the reflexive strategy's authenticating function. Rather, these homages to a bygone era, when people lived close to the land and prepared food in traditional ways, symbolically link Slow Food practices to agrarian and/or rural lifestyles far removed from elite pretensions:I did an internship through Worldwide Working on Organic Farms.... I went to Italy and I milked sheep and goats for a couple of months. And it was very rural. It was very low-tech. We milked in buckets by hand, sheep and goats, and we kind of went out with big sticks and sheepdogs and herded them.... Slow Food originated in Bra, Italy, and that was only like an hour and a half away from the farm. So, I think that's kind of how Marco [the farmer] was involved in Slow Food. He made cheese that was very well regarded, and he went to cheese festivals and stuff. But I mean the whole day was slow. Like wake up kind of late, drink your espresso, milk leisurely, walk the mount, you know. Dinner took a really long time, but that was kind of okay. And just kind of do the same things over and over again. So, we all cooked together. They also did some kind of agro-tourism. They'd have people from the city come out and we would cook with them the food we either grew or found.... That was fun. (Leslie)Leslie's narrative validates a nexus of Slow Food ideals regarding small-scale production and the slower pace of agrarian life. While this rural setting has some trappings of a staged performance—most notably Marco's side business in agro-tourism—it places Slow Food in a symbolic sphere far removed from the Whole Foods brandscape, expensive farm-to-table restaurants, and other consumption domains invocative of foodie affectations (see [52]). For Leslie, her story of interning on a rustic Italian farm affirms that she has actually lived the conscious capitalist values she endorses through her Slow Food advocacy (and thus is not a hypocritical moralizer). More generally, consumer narratives that link Slow Food practices to traditional modes of food production, down-home family meals, and simpler ways of living—rather than exorbitantly priced gourmet dishes—express a rhetorical parry to the elitist charge of moral pretentiousness. The Humanistic Rebel StrategyOur Slow Food advocates use the humanistic rebel strategy to redress the authenticity challenge posed by the elitist critique's connotation of social exclusion and the related sociological argument that consumers' social class backgrounds structurally predetermine their taste affinities ([44]). From this critical viewpoint, Slow Food advocates may not consciously intend to be elitists, but their preferences for goods that convey meanings of sustainability, locavorism, and artisanship betray a host of class advantages that distinguish them from consumers whose lives are marked by conditions of necessity ([47]). While Slow Food advocates may be well intentioned (i.e., they are being ""not inauthentic""), they are also complicit in a system of institutionalized class-based inequities.To illustrate this tension, let us reassess Leslie's preceding vignette in relation to this association with social exclusion (rather than moral pretentiousness). On the one hand, working for room and board on a small, rural farm clearly diverges from conventional notions of status posturing. However, a sociological counterpoint is that Leslie—as a college-educated young adult engaging in an exploratory experience—is building a reservoir of life stories and cultural capital that can afford career and status advantages later in life (see [91]). Seen in this sociological light, Leslie is enacting her class privilege by having the economic and social latitude to intern on a rustic, Italian farm before transitioning into more conventional middle-class occupational pursuits, such as attending graduate school.As a chapter leader, Kevin has oriented his local chapter's activities toward the goal of making Slow Food a more inclusive organization that does not merely cater to the interests of middle-class consumers. When implementing these outreach projects, however, Kevin recognizes that many Slow Food practices are simply incompatible with the situational demands that lower-income consumers have to negotiate on a daily basis:I have amazing privilege,... like being an American to a middle-aged white guy who has very marketable skills.... But I realize that that is a privilege, and this is the biggest thing for us in Slow Food to grapple with—that a single mother who has three jobs and two kids doesn't have the luxury of deciding, ""I think I would like to work less and spend more time in my garden"".... So, you have to be very careful and sensitive about it. (Kevin)Kevin believes that this class chasm can be bridged if his team of middle-class volunteers presents Slow Food's approach to food provisioning, cooking, and eating in a manner that is ""sensitive"" (i.e., adapted) to the lifestyle constraints faced by less well-resourced consumers. His optimistic viewpoint is based on the authenticating assumption that Slow Food's experiential and social benefits have an inherent appeal to consumers, regardless of their class position, because they tap into fundamental human desires and needs.The humanistic rebel strategy takes this inclusive rationale further by suggesting that a confluence of technological and commercial forces have locked individuals into an accelerating pace of life. Consequently, experiences of social connection, spontaneity, and everyday small pleasures are sacrificed to demands for efficiency, convenience, and the seductive (and ultimately alienating) effects of social media and digital communications.Aaron echoes this humanistic rebel mantra in his commentary on the inherent importance of eating and cooking with others:You have the social component of Slow Food as cooking and eating together and taking time to reflect and to connect and to develop community. That's something that we lose when we have things like drive-through or microwave dinners, which aren't to be destroyed or demonized altogether. I take advantage of these services of society. But for them to be the baseline means that we are losing what ... enriches our social systems a lot more than people eating alone and interacting through screens.... So, I think food, when it's jointly cooked and eaten, serves as a very natural medium for connection and idea generation and creativity. (Aaron)Aaron characterizes Slow Food as a needed corrective to societal transformations in the practices of cooking and eating that have resulted in a loss of sociability, communal bonding, and creative interactions. By interpreting Slow Food as a medium for enjoining meaningful social interactions, Aaron expands its cultural province well beyond the predilections of affluent consumers. Aaron's caveat that he has partaken in the conveniences afforded by fast food and microwave meals is another rhetorical means for distancing his Slow Food preferences from elitist connotations. His narrative signals a conscious disavowal of snobbery by acknowledging that there is a legitimate time and place for ""fast food."" Aaron then specifies the problem as the cultural ubiquity of fast food, which, in turn, he links to dehumanizing consequences.When expressing the humanistic rebel strategy, our participants often couched the communal experiences of preparing or eating food as magical moments that affirm Slow Food's class-transcendent qualities:I ran a cheese-making class, and that was really fun because I've always been interested in cheese-making, just for the fun of it. There were seven or eight people there. And one of the members still is making cheese today. And it was really fun to be able to share that magic with people. That this is how this cheese actually comes into being, and it's totally doable by yourself at home. So, that's really exciting, that he got so inspired. It's fun to see somebody get really interested in something. (Amanda)Amanda emphasizes the magic of cheese-making and the inspiration and rewarding personal experiences of self-sufficiency it can enjoin (i.e., ""totally doable by yourself at home""). Her narrative expresses a cosmological view of nature as a magical, life-transforming force ([ 2]) whose authenticating properties are not inherently tied to class-shaped tastes. In keeping with this formulation, Amanda interprets the sharing of her Slow Food skills as a means to help others experience these inspiring connections to nature, which, in turn, implies a revelatory contrast to the alienated experiences of industrialized fast food.Maggie similarly interprets her self-taught Slow Food skills as a means to help people create a sense of communal togetherness and to experience new sensory pleasures and magical connections to the land:I think of Slow Food as taking your time to respect the ingredients and preparing them from scratch and enjoying food. And that, to me, resonates. And bringing back the social aspect of eating. Like, you take time to prepare this meal, you sit down, you share it with people who care about the same things that you do. And it's also, creating another community of people who value these things whether they're growing, or cooking, or eating; having that kind of common thread, I think is really satisfying. (Maggie)Maggie venerates Slow Food as being inherently conducive to experiences of sociability and community and as a way to recognize the meaningfulness of food (a normative orientation that reads as a general human value, rather than a class-interested practice). However, Maggie is a meat producer who, like other Slow Food entrepreneurs, faces an additional task of negating contradictions that derive from the ""inauthentic + not authentic"" category. The Perfective StrategyThe perfective strategy seeks to negate Slow Food's disauthenticating association with commercialism. This aspect of the elitist critique casts Slow Food producers and entrepreneurs as disingenuous actors who are enrolling consumers into an inauthentic market relationship (akin to a gimmick or a confidence game) to serve their own economic interests. In response, our Slow Food entrepreneurs strive to authenticate their actions by signaling that they would never compromise their Slow Food ideals for the sake of profit, such as by recounting the copious amounts of time and energy they invest into perfecting their Slow Food enterprisesIn this spirit, Tom, a farm-to-table restauranteur, views his business as a way to enact his passionate commitment to producing food in a more meaningful and socially beneficial way:When you go to a fast-food restaurant, you have no idea of who actually made that food and the process of where it came from is not known to you. The taste and flavor are mostly engineered to play off the cheap sensory sensations. So, it's fatty and salty and sweet and so, yeah, on a certain level, it might be gratifying, but it's a cheap way to do it that is less meaningful. Slow Food is like, ""We're going to do things in a way that is process oriented!"" I talk about process a lot.... We [Tom and his restaurant staff] were really structured around learning, and so it was a process where we feel like we've excelled and learned a lot and we'll keep pursuing that.... [With Slow Food,] you have this process where people are eating and making something and understanding where it came from and how it works. Eating is such an important part of our lives, and it can have a really important impact on our community and environment. So, the more you understand about it, hopefully you'll make better decisions. The basic motto of Slow Food is clean, fair, and good food. I can totally get behind those values. (Tom)During his interview, Tom extensively discussed ""the process"" aspects of his restaurant and how he views it less as a business than a means to cultivate and diffuse knowledge about the complex interrelationships among food, cooking, sensory enjoyment, ecology, and societal well-being. His quote also reiterates Slow Food's argument that the experiences of these simple culinary pleasures can mobilize consumers to resist the industrialization of the food system (thus echoing the humanistic rebel strategy). For Slow Food entrepreneurs, however, it serves the additional function of associating their enterprises with civic goals that stand distinct from conventional commercial aims.Returning to Maggie, she raises pasture-fed rabbits for sale to farm-to-table restaurants and consumers. In developing her production techniques, Maggie has constantly experimented with different procedures and equipment designs. Through this long trial-and-error process, Maggie believes she has developed an innovative method that better simulates the lives her rabbits would enjoy outside of captivity:Maggie: Daniel Salatin is the son of Joel Salatin, who is the owner of Polyface Farms, and he is the person who is raising rabbits in this system that he has devised and calls the Hare Pen system. So essentially, you still have your does in cages.... You put them in a glorified cage that you then put on grass.... I've copied their system exactly, and I was very unsatisfied with the results that I got. [Maggie then provides an extensive description of her alternative and labor-intensive system and how she developed it]... I don't know why I kept doing it. But I finally have a system that is really effective.... It just was a lot of observation of the rabbits on pasture, making so many mistakes and then incorporating what I had learned.Interviewer: Did you have any economic incentives?Maggie: No! It has to be a personal belief that there might be a better way to do things.... It's kind of like what makes an artist a good artist. If they all hold the brush the same way and they are using the same colors, but they create vastly different things, and one appeals to you, and one doesn't appeal to you. So, what makes that one piece of art recognized by the vast majority of people as superior?... I have this wonderful platform to invest energy and creativity, and it's nice. And so, I feel in some ways really lucky.Invoking the image of the passionate artist, Maggie distinguishes her efforts to perfect an ecologically appropriate system for raising rabbits from crass commercial and economic interests. Maggie's closing sentiment expresses her authenticating belief that such actions can make things better, rather than being driven by instrumental aims. Through storytelling, and by showing how her system works to customers who visit her farm, Maggie deploys narrative and material resources to negate disauthenticating concerns that her Slow Food affinities are merely an instrumental means to charge higher prices. Her personal investment in learning about rabbits' natural habitats/behaviors and inventing a complex ecosystem for raising them further signals that she is not likely to compromise her Slow Food principles in the interest of commercial expediency. Discussion Theoretical ImplicationsPrior research has treated authenticity as a perceptual value or quality that consumers attribute to a brand ([11]; [55]), person-brand ([87]), product ([57]), or performance ([ 5]; [39]). In contrast, we have reconceptualized authenticity as an ongoing process through which consumers and marketers negotiate a contextualized system of cultural contradictions and ambiguous classifications. We suggest that our semiotic framework can better analyze the authenticity contestations that arise in a given market or sociocultural context than conventional theories that assume authenticity perceptions operate on a continuum or selectively draw from an essential set of defining attributes.The conceptualization of authenticity as a relative point along an authentic-to-inauthentic continuum ([22]; [65]) can depict a zone of ambiguity where the authenticity or inauthenticity of a market actor is perceived as being uncertain and, thus, debatable. However, this conceptualization does not offer a means to specifically analyze the cultural meanings (and the interrelationships among them) that generate these ambiguous perceptions. Accordingly, it offers limited theoretical discrimination and managerial guidance.For example, [65] argue that quality commitment, heritage, and sincerity are the primary perceptual cues of authenticity. They then propose that brands should differentially leverage these cues depending on whether consumers perceive them as having low, moderate, or high levels of authenticity. In their normative framework, brands with low perceived authenticity should emphasize sincerity, brands with moderate perceived authenticity should emphasize quality and heritage, and brands with a high level of perceived authenticity should emphasize all three authenticity cues.Such recommendations presume that brands falling into the lower and middle sectors of this proposed continuum have a shortfall of perceived quality commitment, sincerity, or heritage that is rectifiable through compensatory signaling. However, such contested brands are often plagued by contradictory meanings that undermine their promoted claims to authenticity ([37]; [86]). Furthermore, more complex, disauthenticating narratives, such as the elitist critique, can cast doubt on the very credibility of such authenticating cues when used by a contested brand or actors in a market system.Turning to combinatory definitions, [67] have offered a comprehensive theorization of authenticity (as understood from the consumer's perspective) that warrants comparison to our approach. They identify six subdimensions of authenticity (accuracy, connectedness, integrity, legitimacy, originality, and proficiency) and then trace out the relative impact of those dimensions across different market categories and on consumers' behavioral intentions. Rather than a continuum, Nunes, Ordanini, and Giambastiani argue for a family resemblance explanation in which ""a concept (authenticity, in this case) may be qualified by different subsets of its dimensions across different contexts, and not always by all of them in the same way"" (p. 16).Like a continuum, [67] family resemblance logic is limited to the explanation that authenticity is a multidimensional construct whose subcomponents may be more or less important in a given market or consumption context. In contrast, a semiotic framework shifts attention from correlational premises (e.g., this authenticity subdimension seems more important for hedonic products than utilitarian ones) to the cultural meanings, and underlying structural contradictions, relevant to a particular judgment regarding the authenticity of a given product, brand, or market action. For example, the authentic ↔ inauthentic tension elevates the importance of authenticity's moral dimensions in ways that traverse product category distinctions, such as hedonic or utilitarian.To illustrate, a hamburger would typically be classified as a hedonic good. [67], pp. 3–4) find that judgments of ""legitimacy""—which they define as ""the extent to which a product or service adheres to shared norms, standards, rules, or traditions present in the market ... appear to matter for utilitarian but not hedonic products."" However, if we examine this consumer choice in the context of the Slow Food market, then legitimacy becomes a far more important issue. From this standpoint, an ""authentic burger"" would need to exhibit fidelity to various aesthetic and moral norms—grass-fed beef, local sourcing, traditional preparation techniques, and so on—and, its perceived authenticity would be understood and legitimated through a contrast to fast-food burgers. That authenticating contrast (the fast-food burger vs. a Slow Food burger) could then become subject to the elitist critique, which, in turn, would provide motivation for Slow Food advocates to negate these disauthenticating associations.In summary, we have argued that authenticity is culturally constructed (and contested) in a network of structural relations (rather than being a discrete set of essential properties attributed to a brand, person-brand, market performance, or market relationship). Consumers and marketers alike covet indexical authenticity (i.e., the abstract ideal of authenticity) because it can confer cultural legitimacy ([51]), moral authority ([59]), and identity validation ([ 7]; [86]) all of which, can be converted into micro-celebrity status ([79]) and a branding asset ([31]; [45]). However, this authenticity ideal is structurally linked to contradictory meanings and ambiguous classifications. When consumers' or marketers' authenticity claims are challenged by these cultural contradictions, they have pressing incentives to distinguish their actions and identities from the invoked disauthenticating associations. In the following subsection, we discuss how this authenticating goal can be enacted by negating associations that flow along the contradictory path of deception and promoting those that follow the contradictory path of redemption. Two Managerial Paths to Authenticating a BrandAs [49] have argued, marketing managers often find it difficult to redress brand image problems because they are unable to effectively decipher the cultural meanings contributing to those dilemmas. Our semiotic framework can help redress this managerial shortfall. It offers a tangible means for marketing managers to systematically analyze the cultural contradictions of authenticity that emerge in a given market and then to identify strategies for authenticating their brands in the face of these challenges.As a general heuristic, we propose that marketers can be successful in authenticating their brands and/or other strategic assets when they are able to accomplish two complementary goals. The first is to leverage cultural meanings that negate the disauthenticating associations that flow along the contradictory of deception path (authentic → not authentic; see Figure 1). When consumers follow this perceptual path, they experience a glaring contradiction between a prevailing ideal of authenticity and its market manifestation in a brand or marketing practice (authentic ↔ not authentic), which then leads to an association of inauthenticity via the complementary relation of not authentic → inauthentic. In response, marketers should try to provide consumers with compelling and emotionally resonant meanings and rationales that discount the credibility, relevance, or importance of the disauthenticating associations that have gained cultural currency in their respective market.As one illustration, Patagonia confronted a path of deception authenticity challenge soon after it began campaigning against the Trump administration's executive order to reduce the size of Utah's Bears Ears National Monument by two million acres. On December 4, 2017, Patagonia featured this message on the front page of its website: ""The President Stole Your Land: In an illegal move, the president just reduced the size of Bears Ears and Grand Staircase-Escalante National Monuments. This is the largest elimination of protected land in American history."" This web page then directed consumers to various information sources and encouraged consumers to contact their elected officials and to also take the protest to social media, using the hashtag #MonumentalMistakes (see [ 1]).However, defenders of the administration's policy change were quick to denigrate Patagonia's activism as a deceptive marketing ploy. Interior Secretary Ryan Zinke condemned Patagonia as a dishonest ""special interest"" and proclaimed it was ""shameful and appalling that they would blatantly lie in order to put money in their coffers."" Utah Representative Bob Bishop, then chairman of the House Natural Resources Committee, also evoked the elitist critique in his tweet proclaiming that ""Patagonia is Lying to You... A corporate giant hijacking our public lands debate to sell more products to wealthy elitist urban dwellers from New York to San Francisco"" (quoted in [35]).In terms of our model, the Trump administration's response challenged the authenticity of Patagonia's mobilizing campaign by impugning its motivations, thereby reframing an ostensibly authentic (conscious capitalist) action as a disingenuous public relations stunt designed to extract more profits from elite consumers (authentic → inauthentic), which, in turn, triggers the complementary association to inauthenticity. In response, Patagonia joined as a coplaintiff with five Native American tribes and several nonprofit groups in a lawsuit aiming to halt the policy change ([35]; see also https://www.patagonia.com/stories/hey-hows-that-lawsuit-against-the-president-going/story-72248.html). Patagonia also continued to be a vocal critic of the Trump administration's environmental policies and, in a politically and ideologically related vein, donated all its tax savings from the Trump-backed corporate tax cut to environmental groups while condemning the new corporate tax rates as being irresponsible ([63]). Through these responses, Patagonia signaled a deeper commitment to its conscious capitalist values and gave consumers reasons to doubt or dismiss the disauthenticating associations of greed and deception that were being cast on it. In response to Patagonia's uncompromising stance, Inc. offered the following commentary on its 2018 Company of the Year finalist:For Patagonia and its fans, that purpose is doing whatever they can to try to save the planet. In 2018, Patagonia proved that it will not only preach that mission, it will do so with a much louder voice than most other companies. And—so far, anyway—it's only further burnished the Patagonia brand. ([ 8])The second marketing goal is to create conditions in which consumers interpret a brand or business along the contradictory of redemption path (inauthentic → not inauthentic) (see Figure 1). This redemptive chain of associations begins with the widespread cultural view of marketers as inauthentic and, thus, untrustworthy actors ([45]; [66]; [67]). Accordingly, we can assume that consumers will typically harbor varying degrees of skepticism and suspicion toward the authenticity of marketing and branding claims. Redemptive meanings encourage consumers to believe that a given brand or business is operating in ways that favorably diverge from the marketing status quo (i.e., not inauthentic) which, in turn, leads to the complementary relation of not inauthentic → authentic.Volkswagen's (VW's) ""Hello Light"" advertisement, which launched its new line of electric vehicles (circa 2019), takes viewers on a journey that follows a path of redemption arc (see https://www.youtube.com/watch?v=qEvNL6oEr0U). The ad begins with a silhouetted figure entering a dark and seemingly abandoned production facility, while a news report about VW's emission scandal, or ""Dieselgate,"" blares in the background. In a seemingly counterproductive marketing communications move, the ad explicitly reminds its viewers of all the inauthentic associations (VW as liar, deceiver) that arose from those ""dark"" days. The protagonist is revealed to be a despondent engineer struggling to design a new VW model, against the musical backdrop of Simon and Garfunkel's 1960s anthem, ""The Sound of Silence."" Desperate for inspiration, our engineer scours the company archives and finds his creative muse—an image of the iconic VW Van (aka the ""Love Bus"").Through the choice of song and reference to this totem of the 1960s counterculture, the ad recalls VW's countercultural legacy as an authentic symbol of antimaterialist values and a rebuke to status consciousness and marketing hype ([46]). The ad's message is that VW, despite having lost its way, still possesses a latent essential ""goodness"" that is ""not inauthentic."" As ""The Sound of Silence"" reaches its crescendo, lights go on, puncturing the darkness. We observe the production facility come to life and give metaphorical rebirth to the VW brand in the form of an electric van (which also places ""The Sound of Silence"" on a different, ecofriendly cultural register). Thus, the ad's narrative follows the redemptive path of ""inauthentic"" (VW's Dieselgate) to ""not inauthentic"" (VW's 1960's countercultural heyday) to ""authentic"" (signifying that VW has rekindled its socially conscious roots).Our discussion of the three authenticating strategies used by Slow Food consumers and entrepreneurs has emphasized their function as a defensive means to disavow or negate the disauthenticating associations that flow along the contradictory of deception path. However, these same strategies also promote affirmative meanings and associations that operate along the contradictory of redemption path. For example, the perfective strategy does more than negate the disauthenticating association with commercialism. It also magnifies authenticating differences to fast food or industrialized food production and thereby encourages consumers to interpret Slow Food enterprises in a manner compatible with the contradictory of redemption path (even though they may be aware of some disauthenticating associations). This redemptive associative chain takes the following form: Slow Food entrepreneurs are suspected to be ""inauthentic"" due to their commercial motivations → Slow Food entrepreneurs are seen as being ""not inauthentic"" because their deep commitment to artisan ideals and noncommercial values differentiates them from conventional fast-food establishments and industrialized modes of commercial food production → the signification of ""not inauthentic"" supports a broader conclusion that the Slow Food entrepreneur is an authentic actor. Implications for Managers of Conscious Capitalist BrandsGiven their shared ideological affinities, the authenticating strategies used by Slow Food advocates should also have a high degree of applicability to brands espousing conscious capitalist goals and ideals. Of the three authenticating strategies, we find numerous examples of conscious capitalist brands that have enacted some version of the perfective strategy, which aims to negate associations with commercial opportunism and foster interpretations compatible with the contradictory of redemption path. While less commonplace, we can also find branding campaigns that align with the reflexive and humanistic rebel strategies. In the following discussion, we use these various exemplars to illustrate how these authenticating strategies can be implemented by conscious capitalist brands and the authenticity contradictions they potentially redress. Perfective strategyBrands using the perfective strategy engage in unconventional actions that demonstrate a deep commitment to activist causes that supersede profit motives. Over the years, Patagonia has made frequent use of this authenticating strategy to signal that proenvironmental values were central to its corporate mission, even when such acts could mean sacrificing sales, such as its iconic ""Don't Buy"" promotion ([49]) or their ""Give a Damn"" holiday messaging ([72]). REI has also enacted a perfective strategy in its #OptOutside campaign, whereby the retailer closes its stores on Black Friday and encourages consumers to engage in a range of proenvironmental, outdoor activities. Like Patagonia, REI's campaign builds on (and authenticates) the brand's history of supporting environmental causes and promoting a heightened concern for habitat protection and environmental conservation. Last but not least, Clif Bar illustrated a fairly novel implementation of the perfective strategy when its founder and chief executive officer, Gary Erickson, published an ""advertorial"" in the New York Times, offering to donate ten tons of organic ingredients to his main competitor Kind Bars. This advertorial further promised to share his company's knowledge about organic sourcing and production so that the two companies could collectively ""lay the foundation for a healthier, more just and sustainable food system"" ([26]).Owing to their status as commercial enterprises, whose existence depends on profitability, the perfective strategies of conscious capitalist brands can always be reframed as yet another kind of commercial deception. However, such brands can lessen the cultural viability of such recursive challenges by further signaling that their passionate commitment to the supported causes takes precedence over profit motives. Though addressing a different context, [20] offer evidence that supports this strategic approach. They find that customers attribute the quality of authenticity to third-place establishments (e.g., cafes, coffee shops, restaurants) when they believe the respective proprietors are aiming to create meaningful social connections rather than merely trying to make a profit. As they write, ""The authenticity perceived in treasured commercial places is based on exchanges that go beyond mere commercial aspects.... Although being business operators, proprietors invite the consumer to engage in activities that are not undertaken purely for profit"" ([20], p. 913).Accordingly, we propose that conscious capitalist brands are more likely to be perceived as authentic when they provide tangible means for consumers to participate in their social change mission but do so in ways that are not dependent on purchases. From this standpoint, The Body Shop's repositioning of its stores as activist hubs ([77])—where consumers can listen to speakers discuss environmental and social justice issues, sign petitions, and join activist organizations—is an enactment of the perfective strategy and a culturally viable means to reestablish the authenticity of its conscious capitalist branding claims. Reflexive strategyThis strategy aims to negate the charge that a market actor is evincing a ""holier than thou"" stance for actions that are either hypocritical (e.g., ""do as I say, not as I do"") or overstate the positive impact of the self-proclaimed act of conscious capitalist rectitude. For conscious capitalist brands, this authenticating logic most readily translates into a reformist agenda. As one prominent example, Chipotle's ""Back to the Start"" campaign (circa 2012) rallied a diverse assemblage of activist groups that shared a commitment to transforming the corporate-controlled system of food production and who saw the fast-food sector as exemplifying its presumed ills (see [48]). The two-minute short film, which ran across multiple media platforms, shows an increasingly disenchanted farmer witnessing the steady industrialization of his enterprise, replete with enclosed animals, the heavy use of antibiotics, and food being transformed into nondescript goo-like substances. Against the backdrop of Willie Nelson's plaintive version of Coldplay's ""The Scientist,"" the farmer triumphantly decides to go ""back to the start"" by raising free-range animals, using traditional farming techniques, and selling his preindustrial goods to Chipotle.Some relevant insights into this campaign and its authenticating effects can be gleaned from a Fast Company interview with Jesse Coulter, co–chief creative officer of Creative Artists Agency Marketing, which worked with Chipotle's management team in developing this campaign:We were tasked to find new ways to tell Chipotle's Food with Integrity story.... The first issue Chipotle wanted to address was industrial farming.... Chipotle shared many stories of family farmers who have turned their farms into factory farms and have subsequently grown to regret it.... It was provocative because it took a stab at Big Agriculture. Chipotle is a bold company, who has the courage to really stand up for what they believe in.... At the end of the film, a title card appears letting people know that they can download the song on iTunes, and the proceeds benefit the Chipotle Cultivate Foundation, which is dedicated to creating a sustainable, healthy, and equitable food future. People responded and the song reached number one on the iTunes Country chart. ([15])When interpreted through the lens of the reflexive strategy, Chipotle's ""bold"" move was to accept and amplify activist groups' criticisms of the fast-food industry's sourcing and production practices, rather than attempting to deny, rationalize, or obscure these problems through conventional images of consumers enjoying their fast-food meals. The campaign thereby draws an ideological distinction between Chipotle (as a reformist enterprise returning to more humane and sustainable agricultural practices) and the broader fast-food industry that is portrayed as having debased the time-honored practice of farming in the name of speed, efficiency, and cost reduction. In this way, Chipotle aligned itself not with the interests of the fast-food industry at large, but with activist groups who are seeking to reform the broader system of industrialized food production. Chipotle further reinforced the credibility of their reflexive strategy by investing additional resources to support the broader cause (i.e., creating a synergy between the reflexive and the perfective strategy). Humanistic rebel strategyThis strategy promotes the brand as a means for reconstituting meaningful social connections and breaking down societal boundaries that artificially separate people. To avoid being just another nostalgic marketing ode, this strategy should take a critical stance toward selected status quo consumption and marketing practices. The intended message is that the conscious capitalist brand is enabling consumers to resist or escape the dehumanizing and/or isolating influences of materialism, status consciousness, and upward-ratcheting lifestyle competitions.IKEA has run numerous campaigns that align with the humanistic rebel strategy. These campaigns embed its conscious capitalist commitments to sustainability and support of social justice issues, such as gender equity and LGBTQ rights, in a home-as-haven brand narrative. In these ads, the IKEA-furnished home represents a therapeutic space where people can, at least temporarily, unplug from the stresses and distractions of the ""networked life"" ([88], p. 17) and experience meaningful human connections and emotional fulfillment.More than just a haven, however, IKEA often portrays the home as an active force that keeps at bay the outside forces that would interfere with the pleasures of slow living. In an ad titled ""Home Is a Haven,"" we see a father and daughter running to their house during a rainstorm. As they approach the front door, the child's teddy bears spring to life as human-sized entities (whose muscular physiques resemble bouncers at a club). The bears rearrange the house into an open play area and protect the dad from intrusive calls and other outside distractions. We watch as father and daughter play dress-up and numerous other games, eventually falling asleep after their fully engaged bonding time (https://www.youtube.com/watch?v=wGgcYNlH02g).IKEA's ""Let's Relax"" commercial presents a pointedly critical take on the performative, competitive affectations of Instagram micro-influencers, for whom everyday social activities are treated as an instrumental means to garner likes and followers. In the ad, we first observe an eighteenth-century family about to begin formal dinner in a very well-appointed dining room. Suddenly, the father halts the proceedings so that an artist can paint a portrait of the meal, which is immediately transported across the town in a horse-drawn carriage so that affirmative thumbs up gestures from the populace can be tabulated. The scene then suddenly shifts to a modern-day kitchen table, where the same father meticulously photographs the family meal, while his wife and children begrudgingly wait for this documenting ritual to end. The dad sheepishly retires his camera, and the family begins their more enjoyable and authentic social interactions, all framed by the closing caption: ""Relax: It's a meal, not a competition"" (https://www.youtube.com/watch?v=2BXRGzjo1%5fQ).Drawing on our semiotic framework, we anticipate that conscious capitalist brands would gain the most authenticating benefit from the humanistic rebel strategy when they present their brands as ideological allies ([48]; [49]) of consumers who are sensitized to the psychological and social costs of careerism, exclusionary status hierarchies, and the calculated practices of social media self-presentations. In this way, the humanistic rebel strategy undercuts the elitist critique by suggesting that a conscious capitalist brand enables consumers to tap into more basic and rewarding emotional and sensory experiences. It further emphasizes that helping people, from all walks of life, feel genuinely connected to each other is an important and accessible way to make the world a better place. ConclusionDrawing from structural semiotics ([40]), we have developed a conceptual framework that can be used to analyze the cultural contradictions of authenticity, as they emerge in a given market context, and then to identify strategies for combatting their disauthenticating associations. Our analytic approach recognizes that perceptions of authenticity are constructed and contested in a dynamic cultural system. When negotiating such dynamism, marketing managers need to identify strategically significant patterns in the flux of cultural change and to adroitly react to cultural flash points, competitive shifts, and other exogenous shocks that could undermine the credibility of their existing authenticity claims. Whether undertaken in the context of conscious capitalist brands, status-marketing luxury goods, price-driven big-box retailers, or sharing-economy enterprises such as Uber or Airbnb, marketing managers can use our semiotic approach to more effectively negotiate the sociocultural complexity inherent to the process of authenticating their strategic assets.  "
4,"Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The ""Word-of-Machine"" Effect Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel ""word-of-machine"" effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person's unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).Keywords: algorithms; artificial intelligence; augmented intelligence; hedonic and utilitarian consumption; recommendations; technologyRecommendations driven by artificial intelligence (AI) are pervasive in today's marketplace. Ten years ago, Amazon introduced its innovative item-based collaborative filtering algorithm, which generates recommendations by scanning through a person's past purchased or rated items and pairing them to similar items. Since then, more and more companies are leveraging advances in AI, machine learning, and natural language processing capabilities to provide relevant and in-the-moment recommendations. For example, Netflix and Spotify use AI and deep learning to monitor a user's choices and provide recommendations of movies or music. Beauty brands such as Proven, Curology, and Function of Beauty use AI to make recommendations about skincare, haircare, and makeup. Real estate services such as OJO Labs, REX Real Estate, and Roof.ai have replaced human real estate agents with chatbots powered by AI. AI-driven recommendations are also pervading the public sector. For example, the New York City Department of Social Services uses AI to give citizens recommendations about disability benefits, food assistance, and health insurance.In response to the proliferation of AI-enabled recommendations and building on long-standing research on actuarial judgments ([12]; [17]; [30]), recent marketing research has focused on whether consumers will be receptive to algorithmic advice in various domains ([ 9]; [14]; [24]; [26]; [27]). However, no prior empirical investigation has systematically explored if hedonic/utilitarian trade-offs in decision making determine preference for, or resistance to, AI-based (vs. human-based) recommendations.We focus our investigation on hedonic/utilitarian attribute trade-offs because of their influence on both consumer choice and attitudes ([ 6]; [11]). Specifically, we examine when and why hedonic/utilitarian attribute trade-offs in decision making influence whether people prefer or resist AI recommenders. This question is of pivotal importance for managers operating in both the private and public sectors who are looking to harness the potential of AI-driven recommendations.Across nine studies and using a broad array of both attitudinal and behavioral measures, we provide evidence of a ""word-of-machine"" effect. We define ""word of machine"" as the phenomenon by which hedonic/utilitarian attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. We suggest that the word-of-machine effect stems from a lay belief about differential competence perceptions regarding AI and human recommenders. Specifically, we show that people believe AI recommenders are more competent than human recommenders to assess utilitarian attribute value and generate utilitarian-focused recommendations. By contrast, people believe that AI recommenders are less competent than human recommenders to assess hedonic attribute value and generate hedonic-focused recommendations. As a consequence, and as compared with human recommenders, individuals are more (less) likely to choose AI recommenders when utilitarian (hedonic) attributes are important or salient, such as when a utilitarian (hedonic) goal is activated.Our research is both theoretically novel and substantively impactful. A first set of theoretical contributions relates to research on the psychology of automation and on human–technology interaction ([12]; [17]; [30]). The pervasiveness of AI-driven recommendations has led to a burgeoning body of research examining whether consumers are receptive to the advice of algorithms, statistical models, and artificial intelligence ([14]; [24]; [27]). With respect to this literature, we make three novel contributions. First, we extend it by addressing the previously unexplored question of when and why hedonic/utilitarian trade-offs in decision making influence preference for or resistance to AI recommenders. Second, we show under what circumstances AI-driven recommendations are preferred to, and therefore more effective, than human ones: when utilitarian attributes are relatively more important or salient than hedonic ones. These results are especially noteworthy, as most research in this area has documented a robust and generalized resistance to algorithmic advice (for exceptions, see [ 9]; [15]; [26]). Third, we explore under what circumstances consumers will be amenable to AI recommenders in the context of human–AI partnerships: when AI supports rather than replaces a human. These results are also novel as researchers have just begun devising AI systems capable of deciding when to defer (vs. not defer) to a human ([18]), and empirical investigations are yet to examine if consumers will embrace such hybrid human–AI decision making.Our research makes a second theoretical contribution to the literature on hedonic and utilitarian consumption ([ 1]; [22]; [31]; [44]). Prior research in this area has examined how the evaluation of hedonic and utilitarian products depends on characteristics of the task, locus of choice, and justifiability of choice (e.g., [ 5]; [ 8]; [34]). However, research in this area has not addressed the question of whether shifts in hedonic/utilitarian trade-offs in decision making determine preference for the source of a recommendation (e.g., an AI vs. a human recommender). Recent developments of AI have brought this question to the fore, making it of critical importance for companies seeking to leverage the potential of AI-driven recommendations.From a managerial perspective, our results are useful for companies in both the private and public sectors that are looking to leverage AI recommenders to better reach their customers. As we investigate when consumers prefer AI over human recommenders, our findings are useful for companies debating if and how to effectively leverage AI-based recommendation systems. Our findings have implications for a host of marketing decisions. For instance, our results indicate that a shift away from hedonic attributes and toward utilitarian attributes leads to consumers preferring AI recommenders. Accordingly, AI recommenders may be more aligned with functional positioning strategies than experiential ones. In addition, emphasizing utilitarian benefits may be relatively more impactful with an AI-based system than emphasizing hedonic benefits. Taken together, our research and findings provide actionable insights for managers looking for ways to leverage AI to orchestrate consumer journeys so as to successfully move customers through the funnel, increase the likelihood of successful transactions, and, overall, optimize the customer experience at each phase of the journey. Theoretical Development Hedonic and Utilitarian ConsumptionAlthough consumption involves both hedonic and utilitarian considerations, consumers tend to view products as either predominantly hedonic or utilitarian (for a review, see [23]). Hedonic consumption is primarily affectively driven, based on sensory or experiential pleasure, reflects affective benefits, and is assessed on the basis of the degree to which a product is rewarding in itself ([ 8]; [11]; [21]). Utilitarian consumption is instead more cognitively driven, based on functional and instrumental goals, reflects functional benefits, and is assessed on the basis of the degree to which a product is a means to an end ([ 8]; [11]; [21]).Prior research on hedonic/utilitarian consumption has focused on the effect of characteristics of the task on product judgments. For instance, choice tasks tend to favor utilitarian options, whereas rating tasks tend to favor hedonic options ([ 5]; [34]), and forfeiture increases the relative salience of hedonic attributes compared to acquisition ([13]). Justifiability leads people to assign greater weight to utilitarian (vs. hedonic) options ([34]), and hedonic (vs. utilitarian) choices are associated with greater perceived personal causality ([ 8]).Although spanning over a decade, research on hedonic/utilitarian consumption has not yet addressed the question of whether hedonic and utilitarian trade-offs influence preference for the source of a recommendation (AI vs. human). This question has come to the fore given its importance for managers looking to leverage the potential of algorithmic recommendations. We discuss prior research on algorithmic recommendations in the next section. (Resistance to) Algorithmic RecommendationsEver since seminal work on statistical and actuarial predictive models was published ([12]; [17]; [30]), a large body of research has documented how statistical/actuarial models outperform clinical/human judgments in predicting a host of events, such as students' and employees' performance ([12]) and market demand ([37]). Despite the superior accuracy of algorithmic models, people tend to eschew them. With only a few exceptions ([ 9]; [15]; [26]), most of the extant literature has shown that people resist the advice of a statistical algorithm. For instance, recent research in the medical domain has shown that consumers may be more reluctant to utilize medical care delivered by AI providers than by comparable human providers ([27]; [28]). Corporate settings show similar patterns, with recruiters ([19]) and auditors ([ 7]) trusting their judgment and predictions more than algorithms.There are numerous reasons why people resist algorithmic recommendations. People (erroneously) believe that algorithms are unable to learn and improve ([12], [19]) and therefore lose confidence in algorithms when they see them err ([14]). People also believe that algorithms assume the world to be orderly, rigid, and stable and therefore cannot take into consideration uncertainty ([17]) and a person's uniqueness ([27]). Resistance to algorithmic advice may also be borne out of generalized concerns, such as people's fear of being reduced to ""mere numbers"" ([12]) and mistrust of algorithms' lack of empathy ([17]).We extend this literature and show circumstances under which people prefer (and not just resist) algorithmic recommendations. Specifically, we examine how and why hedonic/utilitarian trade-offs determine preference for, or resistance to, AI recommenders, as articulated in the next section. The Word-of-Machine Effect: Utilitarian/Hedonic Trade-offs Determine Preference for (or Resis...We hypothesize a word-of-machine effect, whereby hedonic and utilitarian trade-offs determine preference for or resistance to AI recommenders compared to human ones. We suggest that the word-of-machine effect stems from consumers' differing competence perceptions of AI and human recommenders in assessing attribute value and generating recommendations. Specifically, we suggest that people believe AI recommenders to be more (less) competent to assess utilitarian (hedonic) attribute value and generate utilitarian-focused (hedonic-focused) recommendations than human recommenders.These predictions rest on the assumption that people believe hedonic and utilitarian attribute value assessment to require different evaluation competences. Hedonic value assessment should map onto criteria on the basis of experiential, emotional, and sensory evaluative dimensions. By contrast, utilitarian value assessment should map onto criteria on the basis of factual, rational, and logical evaluative dimensions. This assumption is rooted in the very definition of hedonic and utilitarian value. Hedonic value is conceptualized as reflecting experiential affect associated with a product, sensory enjoyment, and emotions ([ 4]; [20]). Indeed, hedonic consumption tends to be affectively rich and emotionally driven ([ 8]). By contrast, utilitarian value is conceptualized as reflecting instrumentality, functionality, nonsensory attributes, and rationality ([ 4]; [20]). Overall, utilitarian consumption is cognitively driven ([ 8]).How do different types of recommenders (AI vs. human) then fare with respect to assessing hedonic and utilitarian attribute value? We suggest that people believe AI recommenders are more competent to assess utilitarian attribute value than human recommenders and less competent to assess hedonic attribute value than human recommenders. We attribute this lay belief to differing associations people have about how AI (vs. human) recommenders process and evaluate information. Lay beliefs are developed either directly through personal experience ([36]) or indirectly from the environment ([32]). Throughout childhood we learn firsthand that, as humans, we are able to perceive and connect with the outside world through our affective experiences. By contrast, we learn that AI, computers, and robots are rational and logical, and lack the ability to have affective, experiential interactions with the world. These associations are reflected in idioms such as ""thinking like a robot,"" which refers to thinking logically without taking into consideration more ""human"" aspects of a situation such as sensations and emotions. Thus, whereas AI and computers are associated with rationality and logic, humans are associated with emotions and experiential abilities. These associations are also echoed in books, songs, and movies. For example, in the Star Trek universe, the artificially intelligent form of life named Data has superior intellective abilities but is unable to experience emotions. Popular movies like Her, Ex Machina, and Terminator further reinforce these associations.Accordingly, we suggest that people believe AI recommenders are more competent than human recommenders when assessing information because they use criteria that rely relatively more on facts, rationality, logic, and, overall, cognitive evaluative dimensions. By contrast, we propose that people believe human recommenders are more competent than AI recommenders when assessing information because they use criteria that rely relatively more on sensory experience, emotions, intuition, and, overall, affective evaluative dimensions.Because people perceive AI and humans to have different competency levels when assessing information, and because assessment of utilitarian and hedonic attribute value underscore different evaluative foci, it follows that people perceive AI and humans to have different competency levels with respect to assessing utilitarian and hedonic attributes. This lay belief about competence perceptions forms the basis for the proposed word-of-machine effect. In summary, we predict that if utilitarian (hedonic) attributes are more important or salient, such as when a utilitarian (hedonic) goal is activated, people will be more (less) likely to choose AI recommenders than human recommenders.A final note warrants mention. As competence perceptions driving the word-of-machine effect are based on a lay belief, they are embedded in the cultural context. That is, humans are not necessarily less competent than AI at assessing and evaluating utilitarian attributes. Vice versa, AI is not necessarily less competent than humans at assessing and evaluating hedonic attributes. Indeed, AI selects flower arrangements for 1-800-Flowers and creates new flavors for food companies such as McCormick, Starbucks, and Coca-Cola ([41]). Overview of StudiesStudies 1a–b focus on product choice in field settings and show the main word-of-machine effect: that AI (human) recommenders lead to greater choice likelihood when a utilitarian (hedonic) goal is activated. Study 2 shows different perceptions that result from the two recommendation sources: AI (human) recommenders lead to higher evaluation of utilitarian (hedonic) attributes upon consumption. Study 3 shows that when a utilitarian (hedonic) attribute is considered important, consumers prefer AI (human) recommenders. Study 4 uses an analysis of mediation to corroborate the role of competence perceptions in explaining the word-of-machine effect while ruling out attribute complexity as alternative explanation. Studies 5–7 explore the scope of the word-of-machine effect by identifying boundary conditions. Study 5 shows that the effect is reversed for utilitarian goals when the recommendation needs to match to a person's unique preferences, a type of task people view AI as unfit to do. Study 6 shows that the effect is eliminated when AI is framed as ""augmented"" intelligence rather than artificial intelligence, that is, when AI enhances and supports a person rather than replacing them. Finally, Studies 7a–b test an intervention using the consider-the-opposite protocol to moderate the word-of-machine effect. Studies 1a–b: Preference for AI Recommenders When Utilitarian Goals Are ActivatedStudies 1a–b focus on the word-of-machine effect on actual product choice in field settings as a function of an activated utilitarian or hedonic goal. We first activated either a utilitarian or a hedonic goal and then, in an incentive-compatible setting, measured choice as a function of recommender. Study 1a: Hair Treatment Sample ProcedureTwo hundred passersby in a city in northeast United States participated in Study 1a on a voluntary basis. We handed willing passersby a leaflet explaining that we were conducting a blind test for products in the haircare industry and, specifically, for hair masks—a leave-in treatment for hair and scalp. Passersby read that for the purpose of the market test, we wanted them to select one of two hair mask samples solely on the basis of the instructions in the leaflet. These instructions activated, in a two-cell between-subjects design, either a hedonic or a utilitarian goal:[Hedonic] For the purpose of this blind test, it is very important that you set aside all thoughts you might already have about hair masks. Instead, we would like you to focus only on the following. Imagine that you have a ""hedonic"" goal. We would like you to imagine that the only things that you care about in a hair mask are hedonic characteristics, like how indulgent it is to use, its scent, and the spa-like vibe it gives you. When you make the next choice, imagine that there are no other things that are important for you in a hair mask.[Utilitarian] For the purpose of this blind test, it is very important that you set aside all thoughts you might already have about hair masks. Instead, we would like you to focus only on the following. Imagine that you have a ""utilitarian"" goal. We would like you to imagine that the only things that you care about in a hair mask are utilitarian characteristics, like how practical it is to use, its objective performance, and the chemical composition. When you make the next choice, imagine that there are no other things that are important for you in a hair mask.The leaflet further explained that there were two hair mask options from which they could choose. One option had been recommended by a person, and the other option had been recommended by an algorithm. The leaflet specified that the person and the algorithm had the same haircare expertise and that the pots of hair masks, available for pickup on a desk, all contained the same amount of fluid ounces. The pots were identical except for a marking of ""P"" if selected by a person or ""A"" if selected by an algorithm (stimuli in Web Appendix A). The key dependent variable was whether passersby chose the product selected by the person or by the algorithm. Results and discussionTo assess product choice, we compared the proportion of people who chose the product recommended by the algorithm with the proportion of people who chose the product recommended by the person depending on the activated goal (utilitarian vs. hedonic). The two proportions differed significantly (χ2( 1, N = 200) = 12.60, p =.001). As predicted, when a utilitarian goal was activated, more people chose the product recommended by the algorithm (67%) than by the person (33%; z = 4.81, p <.001). When a hedonic goal was activated, more people chose the product recommended by the person (58%) than by the algorithm (42%; z = 2.26, p =.024). Study 1b: Selection of House Properties ProcedureStudy 1b was a field study conducted over four consecutive days in Cortina, a resort town in northeast Italy. We selected this town because in 2026 it will host the Olympic games and is likely to experience a boom in its real estate market, which is the domain of the study. We secured the use of a centrally located bar and set up the study as follows. We placed an ad (translated to Italian) promoting a local real estate agency at the bar entrance. The ad headline reminded people of the opportunity to make fruitful real estate investments due to the upcoming Olympic games. In a two-cell, between-subjects design, we alternated the text in the ad to focus people on a hedonic or utilitarian goal:[Hedonic] With the Olympic games coming up, it is really important that you look for a real estate investment that is fun, enjoyable, and speaks to your emotions. You want a place that pleases your senses considering all the changes that will affect [name of town] in the next few years.[Utilitarian] With the Olympic games coming up, it is really important that you look for a real estate investment that is functional, useful, and speaks to your rationality. You want a place that is practical considering all the changes that will affect [name of town] in the next few years.At the bottom of the ad there were two envelopes described as containing a curated selection of available properties in Cortina that could fit with the opportunity in the ad (i.e., one of the activated goals). One property selection had been (ostensibly) curated by a person (the respective envelope read: ""one of [name of agency]'s agents has selected these properties"") and the other by an algorithm (the respective envelope read: ""[name of agency]'s proprietary algorithm has selected these properties""). The ad invited people to pick up only one of the two envelopes given the limited quantity of promotional materials (stimuli in Web Appendix B). The key dependent variable was whether people chose the selection made by the agent or by the algorithm. A waiter ensured that participants took only one of the two envelopes, and we excluded two participants who picked up two (final N = 229). Results and discussionWe compared the proportion of people who chose the selection made by the algorithm with the proportion of people who chose the selection made by the agent depending on the activated goal (utilitarian vs. hedonic). The two proportions differed significantly (χ2( 1, N = 229) = 29.33, p <.001). When the goal was utilitarian, more people chose the selection made by the algorithm (59.8%) than by the agent (40.2%; z = 3.07, p =.002), whereas when the goal was hedonic, more people chose the selection made by the agent (75.7%) than by the algorithm (24.3%; z = 7.52, p <.001).Together, Studies 1a–b show that when a utilitarian goal is activated, people are more likely to choose an AI recommender than a human recommender. When a hedonic goal is activated, people are less likely to choose an AI recommender than a human recommender. Study 2: AI Recommenders Shift Hedonic/Utilitarian Perceptions Upon ConsumptionStudy 2 examines the word-of-machine effect upon consumption. As conceptual information such as expectations affects food consumption experiences (e.g., [ 2]; [42]), we predicted that the type of recommender would affect perceptions of hedonic and utilitarian attributes upon actual consumption of a product (a chocolate cake). ProcedureOne hundred forty-four participants from a paid subject pool (open to students and nonstudents) at the University of Virginia completed this study (Mage = 27.5 years, SD = 9.5; 60.4% female). We told participants that we were testing chocolate cake recipes on behalf of a local bakery (stimuli in Web Appendix C). We told participants that the bakery had two options for chocolate cake recipes: one created using the ingredient selection of an AI chocolatier and one created using the ingredient selection of a human chocolatier. We specified that both the human and AI chocolatier had access to the same recipe database. We invited participants to look at the two chocolate cakes on top of a podium in a pop-up bakery/classroom desk. The two types of cake looked (and were) identical. We told participants that the two chocolate cakes, although based on different recipes, looked the same because the bakery did not want them to be influenced by the shape or the color. In a two-cell between-subjects design, we asked participants to consume either the chocolate cake whose recipe was selected by the human chocolatier or the one selected by the AI chocolatier. After consuming the cake, we measured hedonic/utilitarian attribute perceptions by asking participants to rate the cake on two hedonic items (indulgent taste and aromas; pleasantness to the senses [vision, touch, smell, etc.]) and two utilitarian items (beneficial chemical properties [antioxidants]; healthiness [micro/macro nutrients, etc.]) on seven-point scales anchored at 1 = ""very low"" and 7 = ""very high."" The order of hedonic and utilitarian items was randomized. Results and Discussion Hedonic attribute perceptionsA one-way analysis of variance (ANOVA) on the average of the two hedonic items (r =.87, p <.001) revealed that, upon consumption, participants rated the chocolate cake as having lower hedonic value when based on the recommendation of an AI chocolatier than a human one (MAI = 4.57, SD = 1.38; MH = 6.17, SD = 1.03; F( 1, 142) = 61.33, p <.001). Utilitarian attribute perceptionsA one-way ANOVA on the index of the two utilitarian items (r =.84, p <.001) revealed that, upon consumption, participants rated the chocolate cake as having higher utilitarian value when based on the recommendation of an AI chocolatier than a human one (MAI = 5.48, SD = 1.21; MH = 5.02, SD = 1.35; F( 1, 142) = 61.33, p =.034).Thus, Study 2 shows that the word-of-machine effect extends to actual consumption and that the type of recommender influences people's perceptions of hedonic/utilitarian trade-offs. AI recommenders led participants to perceive greater utilitarian attribute value and lower hedonic attribute value compared to human recommenders. Study 3: Preference for AI Recommenders When Utilitarian Attributes Are More ImportantStudy 3 further tests the word-of-machine effect. Instead of activating hedonic/utilitarian goals as in Studies 1a–b, we measured the importance given to hedonic/utilitarian attributes with respect to a specific product category (winter coats). Then, we assessed relative preference for a human or an AI recommender. We expected people to prefer AI to human recommenders when utilitarian attributes were more important to them, and to prefer human over AI recommenders when hedonic attributes were more important to them. We benchmarked these hypotheses with a condition in which people chose between two human recommenders, wherein we expected recommender preference to be uncorrelated with importance assigned to hedonic/utilitarian attributes. ProcedureThree hundred three respondents (Mage = 38.0 years, SD = 11.1; 49.5% female) recruited on Amazon Mechanical Turk participated in exchange for monetary compensation. Participants imagined that they were planning to purchase a new winter coat (as it was the winter season) and were looking for recommendations. Participants read that winter coats have functional/utilitarian aspects (""Winter coats have functional or utilitarian aspects, such as insulating power, breathability, and the degree to which the coat is rain and wind proof"") and sensory/hedonic aspects (""Winter coats have sensory or hedonic aspects, such as the color and other aesthetics, the way the fabric feels to the touch, and the degree to which the coat fits well""). Then, to measure the importance of hedonic/utilitarian attributes, participants rated the extent to which, in general, they cared about sensory/hedonic and functional/utilitarian aspects in winter coats (1 = ""mostly care about functional/utilitarian aspects,"" and 7 = ""mostly care about sensory/hedonic aspects"").Participants then read that to get recommendations about winter coats, they could rely on one of two shopping assistants, X or Y. We specified that both assistants had access to the same type and size of database, would charge the same fees, would generate recommendations autonomously, and were trained to serve users well and to the best of their capacity. To control for the possibility that different recommenders would be associated with different service quality perceptions, we also specified that the two shopping assistants had the same rating of 4.9/5.0 stars provided by 687 consumers that had used their services in the past. To manipulate choice set, half of the participants chose between two human shopping assistants (both X and Y were people and were described as two different sales associates at that particular retailer), and the other half chose between a human assistant, X, and an AI assistant, Y. Thus, whereas X was always human, Y was either human or AI depending on the condition. Finally, participants indicated their preference for one of the assistants (1 = ""definitely shopping assistant X,"" 4 = ""indifferent,"" and 7 = ""definitely shopping assistant Y""). Results and DiscussionWe regressed recommender preference on choice set (human–human vs. human–AI), hedonic/utilitarian attribute importance, and their interaction. This analysis revealed significant main effects of choice set (b =.85, t(299) = 5.49, p <.001) and hedonic/utilitarian attribute importance (b =.32, t(299) = 7.46, p <.001), as well as a significant two-way interaction (b = −.29, t(299) = −6.91, p <.001). As hedonic/utilitarian attribute importance was continuous, we explored the interaction using the Johnson–Neyman floodlight technique ([39]), which revealed a significant effect of recommender preference in human–AI choice set for levels of hedonic/utilitarian attribute importance lower than 2.35 (bJN =.15, SE =.08, p =.050) and higher than 3.36 (bJN = −.14, SE =.07, p =.050). That is, the more participants cared about utilitarian attributes (values lower than 2.35 on the seven-point scale), the more they preferred an AI assistant over a human one. Conversely, the more participants cared about hedonic attributes (values higher than 3.36 on the seven-point scale), the more they preferred a human assistant over an AI one. As predicted, in the human–human choice set, which served as the control condition, participants were indifferent between the two assistants (M = 3.98, SD =.34) and recommender preference was uncorrelated with hedonic/utilitarian attribute importance (r =.116, p =.162; see Figure 1).Graph: Figure 1. Results of Study 3: Preference for AI (human) recommenders when utilitarian (hedonic) attributes are more important.Notes: The y-axis represents preference for recommender measured on a seven-point scale anchored at 1 = ""definitely shopping assistant X,"" and 7 = ""definitely shopping assistant Y."" The x-axis represents importance of hedonic/utilitarian attributes, measured on a seven-point scale anchored at 1 = ""mostly care about functional/utilitarian aspects,"" and 7 = ""mostly care about sensory/hedonic aspects."" The shaded region represents area of significance.These results provided correlational evidence that hedonic/utilitarian attribute importance predicts preference between human and AI recommenders. The next study utilizes an analysis of mediation to test competence perceptions as drivers of the word-of-machine effect. Study 4: Mediation by Competence Perceptions: Ruling Out ComplexityStudy 4 uses an analysis of mediation to measure competence perceptions as lay beliefs underlying the word-of-machine effect. In addition, this study tests attribute complexity as an alternative explanation: a belief that AI recommenders are better capable to process more complex attribute information than human recommenders. One could argue that utilitarian attributes seem more complex to evaluate than hedonic attributes. If this argument is accurate, preference for AI recommenders when utilitarian attributes are more salient could be explained by a lay belief about the recommender's ability, higher for AI recommenders, to deal with complexity.[ 7] We tested this alternative explanation by manipulating attribute complexity orthogonally to recommender type (human, AI) and activated goal (hedonic, utilitarian). We manipulated attribute complexity by way of number of product attributes, which is consistent with prior research ([25]; [40]). ProcedureFour hundred two participants (Mage = 38.5 years, SD = 12.6; 46% female) from Amazon Mechanical Turk participated in exchange for monetary compensation in a 2 (complexity: low, high) × 2 (goal: hedonic, utilitarian) × 2 (recommender: human, AI) between-subjects design.Participants read about the beta testing of a new app created to give recommendations of chocolate varieties by relying on one of two sources: a human or an AI master chocolatier (i.e., a computer algorithm). We told participants that the human and AI recommenders relied on the same database of chocolate varieties and operated autonomously. The app had the same cost regardless of recommender. Participants saw screenshots of the app (Figure 2).Graph: Figure 2. Stimuli of Study 4.We specified that the ratings of the chocolate varieties in the data set were not based on personal experience but rather that they had been rated by consumers and manufacturers in terms of certain dimensions that varied by complexity condition. In the high complexity condition, we described the chocolate varieties as being rated on eight attributes, four of which were hedonic (sensory pleasure, taste, fun factor, and pairing combinations) and four of which were utilitarian (chemical profile, nutritional index, digestibility profile, and health factor). In the low complexity condition, we described the chocolate varieties as being rated on two attributes, one of which was hedonic (sensory pleasure) and one of which was utilitarian (chemical profile).We then activated either a hedonic or a utilitarian goal by asking participants to set aside all thoughts they might already have had about chocolate and instead imagine that they wanted a recommendation based only on ( 1) sensory pleasure, taste, fun factor, and pairing combinations (hedonic/high complexity); ( 2) sensory pleasure (hedonic/low complexity); ( 3) chemical profile, nutritional index, digestibility profile, and health factor (utilitarian/high complexity); or ( 4) chemical profile (utilitarian/low complexity). Finally, we manipulated recommender in a two-cell (recommender: human, AI) between-subjects design by telling participants that in the version of the app they were considering, it was either the human or the AI master chocolatier that would give them a recommendation.As a behavioral dependent variable, we asked participants if they wanted to download the chocolate recommendation at the end of the survey (yes, no), specifying that payment would not be conditional on electing to download the recommendation (which is consistent with previous research; see [10]). We then measured the hypothesized mediator (competence perceptions) by asking participants to rate the extent to which they thought the human (AI) recommender ( 1) was competent to recommend the type of chocolate they were looking for and ( 2) could do a good job recommending the type of chocolate they were looking for (1 = ""strongly disagree,"" and 7 = ""strongly agree""; r =.89, p <.001).[ 8] At the very end of the survey, participants who elected to download the recommendation were automatically directed to a downloadable PDF document with information about the chocolate (a relatively more indulgent hazelnut-based chocolate called ""gianduiotti"" in the hedonic condition or a relatively healthier chocolate toasted at low temperature called ""crudista"" in the utilitarian condition). Results and Discussion BehaviorWe assessed behavior (i.e., the proportion of participants who decided to download vs. not download the recommendation) by using a logistic regression with complexity, goal, recommender, and their two-way and three-way interactions as independent variables (all contrast coded) and download (1 = yes, 0 = no) as dependent variable. We found no significant main effect of complexity (B = −.04, Wald =.09, 1 d.f., p =.77) or goal (B =.03, Wald =.06, 1 d.f., p =.81), and we found a marginally significant main effect of recommender (B =.25, Wald = 3.75, 1 d.f., p =.053). The three-way goal × recommender × complexity interaction was not significant (B = −.11, Wald =.80, 1 d.f., p =.37), ruling out the role of complexity. In terms of two-way interactions, complexity did not interact with goal (B = −.13, Wald = 1.04, 1 d.f., p =.31) nor with recommender (B = −.18, Wald = 1.99, 1 d.f., p =.16). Replicating previous results, the two-way goal × recommender interaction was significant (B =.75, Wald = 34.60, 1 d.f., p <.001). The AI recommender led to more downloads than the human recommender when the goal was utilitarian (MAI = 82%, MH = 63%; z = 3.10, p =.002) and fewer downloads when the goal was hedonic (MAI = 52%, MH = 88%; z = −5.63, p <.001). Competence perceptionsA 2 × 2 × 2 ANOVA on competence perceptions revealed no significant main effect of complexity (F( 1, 394) = 1.24, p =.27) and significant main effects of goal (F( 1, 394) = 8.99, p =.003) and recommender (F( 1, 394) = 19.81, p <.001). The three-way complexity × goal × recommender interaction was not significant (F( 1, 394) =.64, p =.44), ruling out complexity. In terms of two-way interactions, complexity did not interact with goal (F( 1, 394) =.61, p =.44), nor with recommender (F( 1, 394) =.36, p =.55). Importantly, the two-way goal × recommender interaction was significant (F( 1, 394) = 57.63, p <.001). Planned contrasts revealed that participants perceived the AI recommender as more competent than the human recommender in the case of a utilitarian goal (MAI = 5.92, SDAI = 1.10; MH = 5.50, SDH = 1.38; F( 1, 394) = 4.90, p =.027) and less competent in the case of a hedonic goal (MAI = 4.51, SDAI = 1.77; MH = 6.13, SDH =.96; F( 1, 394) = 73.04, p <.001). Moderated mediationWe ran a moderated mediation model using PROCESS Model 8 ( 5,000 resamples; Hayes 2018). In this model, the moderating effect of goal takes place before the mediator (competence perceptions). The interaction between recommender and goal was significant (95% CI =.38 to.64) in the path between the independent variable and the mediator but not in the path between the independent variable and the dependent variable (95% CI = −.08 to.63). As predicted, the indirect effect recommender → competence perceptions → download was significant but in the opposite direction conditionally on the moderator (hedonic: 95% CI = 1.19 to 2.40; utilitarian: 95% CI = −.88 to −.06).These results provide evidence for the hypothesized role of competence perceptions as drivers of the word-of-machine effect. Participants rated AI recommenders as more (less) competent in the case of utilitarian (hedonic) goals. Differential competence perceptions explained higher choice likelihood for the AI's recommendation than the human's if a utilitarian goal had been activated and lower choice likelihood for the AI's recommendation than the human's if a hedonic goal had been activated. Furthermore, we did not find evidence that the word-of-machine effect was moderated by complexity. The next three studies tested the scope of the word-of-machine effect by identifying boundary conditions. Study 5: Testing Unique Preference Matching as a Boundary ConditionStudy 5 explores a circumstance under which the word-of-machine effect might reverse: when consumers want a recommendation that matches their unique needs and preferences.[ 9] Matching a recommendation to one's preferences is valued and might even be expected ([16]). In this study, we tested the hypothesis that consumers view the task of matching a recommendation to one's unique preferences as being better performed by a person than by AI.[10] This argument is in line with recent research in the medical domain showing that consumers perceive AI as less able than a human physician to tailor a medical recommendation to their unique characteristics and circumstances ([27]). Thus, we expected people to choose AI recommenders at a lower rate and, conversely, choose human recommenders at a higher rate if matching to unique preferences was salient, even in the case of an activated utilitarian goal. In other words, if matching to unique preferences was salient, we expected people to prefer a human recommender for both hedonic and utilitarian goals. We tested this possibility by manipulating whether participants' desire to have a recommendation matched to their unique needs and preferences was salient and then measuring their choice of recommender. ProcedureFive hundred forty-five respondents (Mage = 39.0 years, SD = 12.9; 46.6% female) from Amazon Mechanical Turk participated in exchange for monetary compensation in a 2 (goal: hedonic, utilitarian) x 2 (matching: unique preferences, control) between-subjects design. Participants read information about the beta testing of a new smartphone app offered by a real estate service. The app would allow users to chat with a Realtor to find properties to buy or rent. Participants further read that there were two versions of this app. In one version of the app, users would interact with a human Realtor, and in the other version, users would interact with an AI Realtor (i.e., a computer algorithm). Participants saw screenshots of the app (Figure 3) and read about how the app would work: the users would indicate what attributes they were looking for in a property (square footage, number of rooms, budget) and the [Realtor/AI Realtor] would use [their/its] training and knowledge to make apartment recommendations. We specified that both the human and AI Realtors had access to the same number and type of property listings. We then activated either a hedonic or a utilitarian goal by asking participants to set aside all thoughts they might already have had about apartments and instead imagine that they wanted a recommendation based only on: ( 1) how trendy the neighborhood is, the apartment views, aesthetics (hedonic goal condition) or ( 2) distance to their workplace, proximity to public transport, functionality (utilitarian goal condition; based on [ 6]). Finally, to make unique preference matching salient, we told half of the participants that it was very important for them to get a recommendation that would be matched to their unique needs and personal preferences. Participants in the control condition were not focused on unique preference matching. As a dependent variable, we measured choice of recommender by asking participants if, given the circumstances described, they wanted to chat with the human or the AI Realtor.Graph: Figure 3. Stimuli (top) and results (bottom) of Study 5: The word-of-machine effect is reversed for utilitarian goals if the recommendation needed to match participants' unique preferences.Notes: The y-axis represents the proportion of participants who chose to chat with the human versus AI realtor. Results and DiscussionWe assessed choice on the basis of the proportion of participants who decided to chat with the human versus AI Realtor by using a logistic regression with goal, matching, and their two-way interaction as independent variables (all contrast coded) and choice (0 = human, 1 = AI) as a dependent variable. We found significant effects of goal (B = 1.75, Wald = 95.70, 1 d.f., p <.000) and matching (B =.54, Wald = 24.30, 1 d.f., p <.000). More importantly, goal interacted with matching (B =.25, Wald = 5.33, 1 d.f., p =.021). Results in the control condition (when unique preference matching was not salient) replicated prior results: in the case of an activated utilitarian goal, a greater proportion of participants chose the AI Realtor (76.8%) over the human Realtor (23.2%; z = 8.91, p <.001), and when a hedonic goal was activated, a lower proportion of participants chose the AI (18.8%) over the human Realtor (81.2%; z = 10.35, p <.001). However, making unique preference matching salient reversed the word-of-machine effect in the case of an activated utilitarian goal: choice of the AI Realtor decreased to 40.3% (from 76.8% in the control; z = 6.17, p <.001). That is, making unique preference matching salient turned preference for the AI Realtor into resistance despite the activated utilitarian goal, with most participants choosing the human over the AI Realtor. In the case of an activated hedonic goal, making unique preference matching salient further strengthened participants' choice of the human Realtor, which increased to 88.5% from 81.2% in the control, although the effect was marginal, possibly due to a ceiling effect (z = 1.66, p =.097).Overall, whereas the word-of-machine effect replicated in the control condition when unique preference matching was salient, participants preferred the human Realtor over the AI recommender both in the hedonic goal conditions (human = 88.5%, AI = 11.5%; z = 12.40, p <.001) and in the utilitarian goal conditions (human = 59.7%, AI = 40.3%; z = 3.24, p =.001; Figure 3), corroborating the notion that people view AI as unfit to perform the task of matching a recommendation to one's unique preferences.These results show that preference matching is a boundary condition of the word-of-machine effect, which reversed in the case of a utilitarian goal when people had a salient goal to get recommendations matched to their unique preferences and needs. The next study tests another boundary condition. Study 6: Testing Augmented Intelligence as a Boundary ConditionStudy 6 explores under what circumstances the word-of-machine effect is eliminated, and it tests the role of AI as boundary condition. Studies 1–5 tested cases in which the role of AI was to replace human recommenders. Study 6 explores the case in which AI is leveraged to assist and augment human intelligence. ""Augmented intelligence"" involves AI's assistive role in enhancing and amplifying human intelligence instead of replacing it ([ 3]). So far, we have showed that consumers resist AI recommenders when a hedonic goal is activated. In Study 6, we tested the hypothesis that consumers will be more receptive to AI recommenders, even in the case of a hedonic goal, if the AI recommender assists and amplifies a human recommender who retains the role of ultimate decision maker. In this case, we expected people to believe that the human decision maker would compensate for the AI's relative perceived incompetence in the hedonic realm. We expected the reverse effect in the case of a utilitarian goal. In other words, we expected that augmented intelligence—a human–AI hybrid decision making model— would help bolster AI to the level of humans for hedonic decision making and help bolster humans to the level of AI for utilitarian decision making. In addition, we added a control condition in Study 6 in which neither recommender was mentioned to serve as a baseline measure of participants' perceptions of hedonic and utilitarian attributes. ProcedureFour hundred four respondents (Mage = 40.2 years, SD = 12.5; 48.9% female) from Amazon Mechanical Turk participated in exchange for monetary compensation in a three-cell (recommender: human, artificial intelligence, augmented intelligence) between-subjects design. A fourth control condition contained no recommender manipulation and served as the baseline.The stimuli and procedure were identical to those of Study 4. Participants read about the beta testing of a new app created to give recommendations of chocolate varieties by relying on one of two sources: a human or an AI master chocolatier. Participants read that human and AI recommenders relied on the same database, which comprised a large number of chocolate varieties that had been rated by consumers and manufacturers. Participants read that the app had the same cost regardless of the type of recommender it relied on. Finally, participants read that the app would suggest a curated selection of five chocolate bars.We then manipulated recommender by randomly assigning participants to ( 1) a human condition, in which a human chocolatier would curate the chocolate section; ( 2) an artificial intelligence condition, in which an AI chocolatier (i.e., a computer algorithm) would curate the chocolate section; or ( 3) an augmented intelligence condition, in which the AI chocolatier would assist the human chocolatier in the curation of the chocolate selection. Specifically, participants read:[Human condition] In the version of the app we are testing today, it is the human chocolatier that curates a selection of chocolate bars. This selection contains five chocolate bars selected by the human chocolatier. That is, it is a person who selects chocolate bars. This version of the app is technically called ""human intelligence,"" because it uses what human intelligence can do.[Artificial intelligence condition] In the version of the app we are testing today, it is the A.I. chocolatier that curates a selection of chocolate bars. This selection contains five chocolate bars selected by the A.I. chocolatier. That is, it is a computer algorithm that selects chocolate bars. This version of the app is technically called ""artificial intelligence,"" because it uses a computer algorithm to substitute and replace what human intelligence can do.[Augmented intelligence condition] In the version of the app we are testing today, it is the A.I. chocolatier that curates a selection of chocolate bars. This selection contains five chocolate bars selected by the A.I. chocolatier. That is, it is a computer algorithm that selects chocolate bars. The computer algorithm makes the initial selection and assists a human chocolatier, who will make the final decision about which chocolate bars to recommend. This version of the app is technically called ""augmented intelligence,"" because it uses a computer algorithm to enhance and augment what human intelligence can do.The control condition entailed no recommender manipulation; instead, it merely included a description of the app and no information about the source of the chocolate bar recommendation. As a dependent variable, we measured hedonic attribute perceptions with two items (indulgent taste and aromas; pleasantness to the senses [vision, touch, smell, etc.]) and utilitarian attribute perceptions with two items (beneficial chemical properties [antioxidants, etc.]; healthiness [micro/macro nutrients, etc.]), all on seven-point scales anchored at 1 = ""very low,"" and 7 = ""very high."" The order of items was randomized. Results and Discussion Hedonic attribute perceptionsThe one-way ANOVA on the average of the two items measuring hedonic attribute perceptions (r =.79, p <.001) was significant (F( 1, 436) = 48.92, p <.001). In line with previous results, and replicating the word-of-machine effect, participants reported higher hedonic attribute perceptions when the recommender was human (MH = 6.00; SD = 1.06) than when the recommender was AI (Martificial_intelligence = 4.15, SD = 1.64; F( 1, 436) = 125.55, p <.001). However, when the AI recommender was augmenting human intelligence, the word-of-machine effect was eliminated: participants reported the same hedonic perceptions (Maugmented_intelligence = 5.74, SD = 1.11) as they did when the recommender was human (F( 1, 436) = 2.31, p =.129) and higher hedonic perceptions than when the recommender was AI alone (F( 1, 436) = 84.73, p <.001). Participants in the control condition reported lower hedonic perceptions (Mcontrol = 5.62, SD = 1.09) than participants in the human condition (F( 1, 436) = 5.32, p =.022) and higher hedonic perceptions than participants in the AI condition (F( 1, 436) = 77.92, p <.001). Control condition and augmented intelligence condition did not differ (F( 1, 436) < 1, p =.49). Utilitarian attribute perceptionsThe one-way ANOVA on the average of the two items measuring utilitarian attribute perceptions (r =.75, p <.001) was significant (F( 1, 436) = 6.60, p <.001). In line with previous results, and replicating the word-of-machine effect, participants reported higher utilitarian attribute perceptions when the recommender was AI (Martificial_intelligence = 5.24; SD = 1.41) than when the recommender was human (MH = 4.75, SD = 1.57; F( 1, 436) = 6.40, p =.012). However, when the AI recommender was augmenting human intelligence, the word-of-machine effect was eliminated: participants reported the same utilitarian perceptions (Maugmented_intelligence = 5.44, SD = 1.32) as they did when the recommender was AI alone (F( 1, 436) =.99, p =.321) and higher utilitarian perceptions than when the recommender was human (F( 1, 436) = 11.87, p <.001). Participants in the control condition reported the same utilitarian perceptions (Mcontrol = 4.70, SD = 1.56) as participants in the human condition (F( 1, 436) =.05, p =.820) and lower utilitarian perceptions than participants in both the AI (F( 1, 436) = 7.47, p =.007) and augmented intelligence conditions (F( 1, 436) = 13.22, p <.001; Figure 4).Graph: Figure 4. Results of Study 6: The word-of-machine effect is eliminated in the case of augmented intelligence (human–AI hybrid decision making).Notes: The y-axis represents hedonic attribute perceptions and utilitarian attribute perceptions measured on seven-point scales anchored at 1 = ""very low,"" and 7 = ""very high."" Error bars represent standard errors. The solid-line pairwise comparisons represent the word-of-machine effect. The dashed-line pairwise comparisons represent moderation by augmented intelligence: A human–AI hybrid decision making model bolsters AI to the level of humans for hedonic decision making, and humans to the level of AI for utilitarian decision making. Details of all pairwise comparisons are reported subsequently.Hedonic Attribute PerceptionsWord-of-machine effect: Human versus AI: F( 1, 436) = 125.55, p =.000Moderation by augmented intelligence (H + AI hybrid decision making bolsters AI to the level of humans for hedonic decision making): Human versus H + AI: F( 1, 436) = 2.31, p =.129AI versus H + AI: F( 1, 436) = 84.73, p =.000Control versus H: F( 1, 436) = 5.32, p =.022Control versus AI: F( 1, 436) = 77.92, p =.000Control versus H + AI: F( 1, 436) =.49, p =.486Utilitarian Attribute PerceptionsWord-of-machine effect: Human versus AI: F( 1, 436) = 6.40, p =.012Moderation by augmented intelligence (H + AI hybrid decision making bolsters H to the level of AI for utilitarian decision making): AI versus H + AI: F( 1, 436) =.99, p =.321H versus H + AI: F( 1, 436) = 11.87, p =.001Control versus H: F( 1, 436) =.05, p =.820Control versus AI: F( 1, 436) = 7.47, p =.007Control versus H + AI: F( 1, 436) = 13.22, p =.000These results delineate the scope of the word-of-machine effect and show a circumstance under which the effect is eliminated. Even when a hedonic goal was activated, AI recommenders fared as well as human recommenders as long as they were in a hybrid decision-making model in partnership with a human. Studies 7a–7b: Attenuating the Lay Belief Underlying the Word-of-Machine EffectStudies 7a and 7b test an intervention to attenuate the lay belief underlying the word-of-machine effect—that AI recommenders are less (more) competent than human recommenders in assessing hedonic (utilitarian) value. We used a protocol called ""consider-the-opposite,"" in which people are prompted to consider the opposite of what they initially believe to be true and take into account evidence that is inconsistent with one's initial beliefs. This protocol has been effectively used to correct biased beliefs in judgment, such as the explanation bias ([29]), confirmatory hypothesis testing ([43]), anchoring ([33]) and halo effects in marketing claims ([35]). Study 7a tests this intervention following the original protocol (i.e., [33]), and Study 7b tests a protocol that is relatively easier to implement and scale by embedding the intervention in a real chatbot. Study 7a: Testing the Original Consider-the-Opposite Protocol ProcedureThree hundred sixty-eight respondents (Mage = 39.8 years, SD = 12.5; 49.2% female) from Amazon Mechanical Turk participated in exchange for monetary compensation in a 2 (recommender: human, AI) × 2 (intervention: consider the opposite, control) between-subjects design.The stimuli and procedure were identical to those of Studies 4 and 6: participants read about a new app created to give chocolate recommendations by relying on either a human or an AI master chocolatier. We manipulated recommender between subjects by telling participants that, in the version of the app they were considering, it was either the human or the AI chocolatier that would suggest a curated selection of five chocolate bars. We also implemented the intervention between subjects by prompting half of the participants to ""consider the opposite"": consider the ways in which they could be wrong about what they expected the [human/AI] recommender to be good at (based on [33]):Think for a moment about what you expect the [human/AI] chocolatier to be good at when selecting chocolate bars. Before you rate the chocolate selection, we would like you to consider the opposite. Can your expectations about what the human chocolatier is good at when selecting chocolates be wrong? Imagine that you were trying to be as unbiased as possible in evaluating this chocolate selection—consider yourself to be in the same role as a judge or juror. Could the [human/AI] chocolatier be good at the opposite of what you expect them to be good at? Please write down some ways in which you could be wrong in terms of your expectations about what the [human/AI] chocolatier is good at when selecting chocolates.This prompt was absent for participants in the control condition. As a dependent variable, participants reported their perceptions of hedonic/utilitarian attributes of the curated selection of chocolate bars, measured on a seven-point scale ranging from 1 = ""sensory pleasure (taste, aromas, etc.)"" to 7 = ""healthy chemical properties (antioxidants, micro/macro nutrients, etc.)."" Thus, lower numbers indicated higher hedonic value. Results and discussionA 2 × 2 ANOVA on hedonic/utilitarian attribute perceptions revealed no significant main effect of intervention (F( 1, 364) =.25, p =.62), a significant main effect of recommender (F( 1, 364) = 65.17, p <.001), and a significant two-way recommender × intervention interaction (F( 1, 364) = 12.11, p =.001). Planned contrasts revealed that the word-of-machine effect replicated both in the control and intervention conditions, with lower hedonic perceptions (or, conversely, higher utilitarian perceptions) for AI recommenders than human recommenders (control conditions: MAI_control = 4.51, SD = 1.84, MH_control = 2.49, SD = 1.48; F( 1, 364) = 81.48, p <.001; intervention conditions: MAI_intervention = 3.99, SD = 1.78, MH_intervention = 3.18, SD = 1.44; F( 1, 364) = 8.93, p =.003; higher numbers indicate higher utilitarian/lower hedonic perceptions). More importantly, the intervention attenuated the word-of-machine effect and led to participants perceiving the AI's recommendation as having higher hedonic value compared with the control condition (MAI_intervention = 3.99, SD = 1.78, MAI_control = 4.51, SD = 1.84; F( 1, 364) = 7.66, p =.006) and the human recommendation as having higher utilitarian value compared to the control condition (MH_intervention = 3.18, SD = 1.44; MH_control = 2.49, SD = 1.48, F( 1, 364) = 4.59, p =.033; higher numbers indicate higher utilitarian/lower hedonic perceptions; Figure 5).Graph: Figure 5. Results of Study 7a: Prompting people to consider the opposite attenuated the word-of-machine effect.Notes: The y-axis represents perceived hedonic/utilitarian attribute value measured on a seven-point scale anchored at 1 = ""sensory pleasure (taste, aromas, etc.), and 7 = ""healthy chemical properties (antioxidants, micro/macro nutrients, etc.)""; therefore, higher numbers indicate higher utilitarian value/lower hedonic value. Error bars represent standard errors.Thus, these results provide evidence for a potential intervention that alleviates initial beliefs about, and therefore resistance to, AI recommenders: prompting people to consider the opposite. Study 7b: Testing a Consider-the-Opposite Intervention That Is Easier to Implement and ScaleStudy 7b builds on the original consider-the-opposite protocol and the results of Study 7a to test an intervention better suited for implementation and scalability in a real-world setting. To do so, we created a real chatbot that participants could interact with and that delivered the intervention. ProcedureTwo hundred nighty-nine respondents (Mage = 40.4 years, SD = 12.6; 43.1% female) from Amazon Mechanical Turk participated in exchange for monetary compensation in a two-cell (intervention: consider the opposite, control) between-subjects design. Participants read about an app called ""Cucina"" that would rely on AI to give recipe recommendations. The app worked by giving users the chance to chat with the AI Chef and ask for recipe suggestions and recommendations. Participants further read that they could try out the AI Chef by chatting with it in a web browser window. We created a chatbot ad hoc for this experiment by embedding a JavaScript in the Qualtrics survey (Figure 6). The chatbot was programmed to first introduce itself: ""Hello I am an A.I. Chef at Cucina! Thank you for trying out our app! What is your name?"" Participants could then reply to the chatbot using a text box. We programmed the chatbot's next response to differ depending on the intervention condition:[Intervention: consider the opposite] ""Hi [participant's name]! I am here to suggest a recipe for you to try! Some people might think that an Artificial Intelligence Chef is not competent to give food suggestions...but this is a misjudgment. For a moment, set aside your expectations about me. When it comes to making food suggestions, could you consider the idea that I could be good at things you do not expect me to be good at? Okay, let's chat about food. How can I help you?""Graph: Figure 6.Stimuli of Study 7b.[Intervention: control] ""Hi [participant's name]! I am here to suggest a recipe for you to try! Okay, let's chat about food. How can I help you?""As a dependent variable, we measured hedonic/utilitarian attribute perceptions of the recipes suggested by the AI chatbot, as measured on a seven-point scale ranging from 1 = ""mostly based on sensory pleasure (taste, aromas, etc.)"" to 7 = ""mostly based on healthy chemical properties (antioxidants, micro/macro nutrients, etc.)."" Results and discussionA one-way ANOVA on hedonic/utilitarian attribute perceptions revealed that the intervention attenuated the word-of-machine effect and led to higher hedonic perceptions compared to the control condition (Mintervention = 3.75, SD = 1.46, Mcontrol = 4.25, SD = 1.37; F( 1, 297) = 9.15, p =.003; lower numbers indicate higher hedonic perceptions). These results corroborate those of Study 7a and provide evidence for a practical and relatively easier-to-implement intervention for managers looking to attenuate the lay belief underlying the word-of-machine effect. General DiscussionAs companies in the private and public sectors assess how to harness the potential of AI-driven recommendations, the question of how trade-offs in decision making influence preference for AI recommenders is of great importance. We address this question across nine studies and show a word-of-machine effect: the phenomenon by which hedonic and utilitarian trade-offs determine preference for (or resistance to) AI-driven recommendations. Studies 1a–1b show that a utilitarian (hedonic) goal makes people more (less) likely to choose AI recommenders than human ones. Study 2 shows that AI (human) recommenders lead to higher perceptions of utilitarian (hedonic) attributes upon consumption. Study 3 shows that people prefer AI (human) recommenders when utilitarian (hedonic) attributes are more important. Study 4 shows that differing competence perceptions underlie the word-of-machine effect and rule out complexity. Studies 5 and 6 identify boundary conditions: Study 5 shows that the word-of-machine effect is reversed for utilitarian goals if the recommendation needs to match a person's unique preferences, and Study 6 shows that the effect is eliminated when AI is framed as ""augmented"" rather than ""artificial"" intelligence, that is, in human–AI hybrid decision making. Finally, Studies 7a–7b tested an intervention to attenuate the word-of-machine effect. Theoretical ContributionsOur research makes several important theoretical contributions. A first set of contributions speaks to research on the psychology of automation and on human–technology interactions ([12]; [17]; [30]). First, we extend this literature by addressing the question of whether hedonic/utilitarian trade-offs in decision making drive preference for or resistance to AI recommenders. This question is novel, as prior research has not relied on differences inherent to hedonic/utilitarian consumption to predict people's reactions to receiving advice from automated systems.Second, we show under what circumstances AI-driven recommendations are preferred to, and therefore more effective, than human ones: when utilitarian attributes are relatively more important or salient than hedonic ones. Research in this area has largely focused on consumers' resistance to automated systems. For example, in the domain of performance forecasts, people are less likely to rely on the input of an algorithm than a person to make predictions about student performance, an effect that is due to the belief that algorithms, unlike people, cannot learn from their mistakes ([14]). In the domain of health care utilization, people are less likely to rely on an automated medical provider if a human provider is available, even when the two providers have the same accuracy ([27], [28]).Limited research has identified under what circumstances resistance to algorithmic advice is attenuated: if people have the opportunity to modify algorithms and thus exert control over them ([15]), if the human likeness of algorithms is increased ([ 9]), if the task entails a numeric estimate of a target ([26]), and if the algorithm is described as tailoring a recommendation to a person's unique case ([27], [28]). We extend this literature by showing circumstances in which consumers' resistance to AI may be reversed and by showing cases in which consumers even prefer automated systems: when they assign greater importance to utilitarian attributes or when a utilitarian goal is activated.Third, we explore under what circumstances consumers will be amenable to AI recommenders in the context of human–AI partnerships. We show that augmented intelligence helps bolster AI to the level of humans for hedonic decision making and helps bolster humans to the level of AI for utilitarian decision making. This contribution is important because it represents the first empirical test of augmented intelligence as an alternative conceptualization of artificial intelligence that focuses on AI's assistive role in advancing human capabilities. We hope that this contribution will prioritize new research focused on understanding the potential of AI in conjunction with humans rather than in contraposition, as this seems to be the advocated way forward by many practitioners ([ 3]; [18]).We also contribute to the literature on hedonic and utilitarian consumption ([ 1]; [22]; [31]; [44]). Literature in this area has identified the factors that influence evaluation of hedonic and utilitarian product dimensions. We extend this literature by investigating how hedonic/utilitarian attribute trade-offs influence the effectiveness of a source of a product recommendation (i.e., a human vs. an AI recommender; Studies 1a, 1b, 3–5) and how the source of a product recommendation influences hedonic/utilitarian perceptions (Studies 2, 6–7b). Managerial ImplicationsThe current speed of development and adoption of AI, machine learning, and natural language processing algorithms challenge managers to harness these transformative technologies to optimize the customer experience. Our findings are insightful for managers as they navigate the remarkable technology-enabled opportunities that are growing in today's marketplace. These new technologies are also experiencing a renewed prominence in public discourse. For instance, the U.S. government has established the National Artificial Intelligence Research and Development Strategy to address economic and social implications of AI.Our findings provide useful insights for both companies and public policy organizations debating if and how to effectively automate their recommendations systems. A company like Sephora relies both on human-based recommendations from sales associates and its customer base and AI-based recommendations through its Visual Artist app, a conversational bot that interacts with prospective shoppers. Our results suggest cases in which AI-based recommendations would be more effective (i.e., when utilitarian attributes are more salient or important, such as grooming products) and when they would be less effective (i.e., when hedonic attributes are more salient or important, such as fragrances).Our results are insightful for strategic and tactical marketing decisions. Marketers could prioritize functional positioning strategies over experiential ones in the case of AI-based recommendations for target segments for whom utilitarian attributes are more important. For instance, a company in the hospitality industry such as TripAdvisor should emphasize AI-based recommendations for business travel services and deemphasize AI-based recommendations for leisure travel services. Our results also apply to a host of tactical decisions such as marketing communications. Managers could communicate to their customers in a way that is aligned with a target segment's goal (i.e., hedonic vs. utilitarian) and emphasize the most effective points of parity/difference with competing brands or across different products in the portfolio. Companies like Netflix and YouTube could emphasize AI-based recommendations when utilitarian attributes are relatively more important (e.g., documentaries) and human-based recommendations (""similar users"") when hedonic attributes are relatively more important (e.g., horror movies).This research also highlights boundary conditions that may prove useful for practitioners. Study 5 indicated that when consumers want recommendations that are matched to their unique preferences, they resist AI recommenders and instead prefer human recommenders, regardless of hedonic or utilitarian goals. These results suggest that companies whose customers are known to be satisfied with ""one size fits all"" recommendations, or who are not in need of a high level of customization, may rely on AI systems. However, companies whose customers are known to desire personalized recommendations should rely on humans. Some companies, such as Amazon, seem to be implementing a similar strategy. Even though most of Amazon's recommendations are based on algorithms, the company has recently started offering an additional service for an added fee called ""personal shopper."" This service relies on human shopping assistants to give clothing recommendations rather than on algorithms. Our results indicate that more companies, especially those in markets that are relatively more hedonic, should follow Amazon's example.Study 6 provides another managerially relevant boundary condition: augmented intelligence. The results of this study indicate that consumers are more receptive to AI recommenders, even in the case of hedonic goals, if the AI recommender does not replace a human recommender but instead assists a human recommender who retains the role of ultimate decision maker. These results are important for practitioners managing relatively more hedonic products or services. For instance, in a personal conversation with the authors, a Walmart marketing manager noted how the top two most frequently ignored recommendations on the company's website are those for alcoholic beverages and food items—arguably products for which hedonic attributes tend to be more salient and important. In these circumstances, practitioners could leverage our results and utilize AI systems to generate an initial recommendation on which a human then ""signs off.""Finally, in Studies 7a–7b we tested an intervention that practitioners managing relatively more hedonic products and relying on AI systems may execute. Building on the consider-the-opposite protocol, we created a realistic chatbot that interacted with participants and nudged them to consider that the AI recommender could be good at things that participants did not expect it to be good at. The intervention was successful in both studies, suggesting that practitioners may utilize this technique if hedonic attributes are important. Limitations and Future ResearchDespite the robustness of the word-of-machine effect, our research has limitations that offer several opportunities for future research. First, there is the possibility that drawing attention to the source of a recommendation primed study participants. AI recommenders might have primed utilitarian attributes or made utilitarian goals more salient, and it was the associated increased activation of these concepts, rather than competence perceptions, that gave rise to the word-of-machine effect. Although possible, this alternative explanation based on priming is unlikely given the results of a study we report in Web Appendix D. In this study (N = 230), we first primed participants with either human or AI-related concepts by drawing their attention to either a human or an AI recommender, thus approximating the kind of priming that could have occurred in our studies. To assess whether the AI recommender primed utilitarian concepts, we then measured perceptions of utilitarian and hedonic attributes of a stimulus in a domain unrelated to one in which the priming manipulation occurred. This stimulus was pretested to be neutral (i.e., perceived to be equally utilitarian and hedonic). The results indicate that the stimulus was perceived to be equally utilitarian and hedonic regardless of the priming manipulation. Although these results offer preliminary evidence that priming does not account for the word-of-machine effect, the inferences one can draw from a null effect are limited. More broadly, the question of whether AI-based recommendations activate specific constructs that might be influential on decision making is a worthy avenue for future research.Second, even though we tested the word-of-machine effect across multiple domains, there remains the possibility that the effect is stronger or weaker in certain categories. For instance, the effect might be stronger in categories (e.g., a chocolate cake) in which discerning hedonic attributes (e.g., how tasty or how indulgent it is) is easier than discerning utilitarian attributes (e.g., how many macronutrients it contains, or how healthy it is). Future research could more systematically investigate what dimensions of different product categories strengthen versus weaken the word-of-machine effect.Third, the lay beliefs underlying the word-of-machine effect may be transitional. As competence perceptions driving the word-of-machine effect are based on a lay belief, they are embedded in a cultural view that may change over time. The lay belief about differential competence perceptions may already be inaccurate, as AI is already utilized in domains that are relatively more hedonic. For instance, AI curates flower arrangements on the basis of customers' past transactions and inferred preferences (1-800-Flowers) and creates new flavors for food companies such as McCormick, Starbucks, and Coca-Cola ([41]).Our research also suggests opportunities for future exploration of this area. First, the word-of-machine effect may have interesting downstream consequences on other responses. For instance, relying on an AI recommender may lead consumers to compensate by adjusting their own choices. Given the belief that AI-based recommendations excel on utilitarian attributes and are weaker on hedonic attributes, consumers may choose from a set of options by paying closer attention to the hedonic attributes of the options, assuming that the options are satisfactory in terms of utilitarian attributes. This ""second-step choice"" is an interesting question to consider in the future.Second, in Studies 7a–b we show preliminary evidence of how lay beliefs toward AI systems could be successfully alleviated through a protocol utilized in the decision making literature. Future research could identify other real-world variables that might have similar attenuating effects, such as domain expertise, involvement, time spent making decisions, or familiarity/repeated use of AI systems. A third fruitful research opportunity would be to explore whether consumers can be persuaded to trust AI systems, even more than humans, in the eventuality that AI systems are sufficiently sophisticated to pass the Turing test. In this vein, future research could identify conditions under which the word-of-machine effect reverses, with AI recommenders being more persuasive than humans for hedonic products.As research on the psychology of automation expands to include developments such as AI, we hope that our findings (especially those of Study 6) will spur further research prioritizing the understanding of the vast potential of AI operating in partnership with humans. More research is also necessary to map out the impact of AI systems across consumption settings. AI-powered technologies will be instrumental in optimizing the customer experience at each phase of the consumer journey by offering products of increasing personalization ([41]). New technologies like image, text, and voice recognition, together with large-scale A/B testing will provide managers with the data necessary for a complete, AI-driven customization of the journey ([41]) and will allow researchers to gather the consumer signals that are produced as a by-product of consumer activities ([38]). We hope that future research will focus on how to harness this great potential of AI for managers and researchers alike.Overall, understanding when consumers will be amenable to and when they will resist AI-driven recommendations is a pressing and complex endeavor for researchers and firms alike. We hope that our research will spur further exploration of this important topic. "
5,"Augmented Reality in Retail and Its Impact on Sales The rise of augmented reality (AR) technology presents marketers with promising opportunities to engage customers and transform their brand experience. Although firms are keen to invest in AR, research documenting its tangible impact in real-world contexts is sparse. In this article, the authors outline four broad uses of the technology in retail settings. They then focus specifically on the use of AR to facilitate product evaluation prior to purchase and empirically investigate its impact on sales in online retail. Using data obtained from an international cosmetics retailer, they find that AR usage on the retailer's mobile app is associated with higher sales for brands that are less popular, products with narrower appeal, and products that are more expensive. In addition, the effect of AR is stronger for customers who are new to the online channel or product category, suggesting that the sales increase is coming from online channel adoption and category expansion. These findings provide converging evidence that AR is most effective when product-related uncertainty is high, demonstrating the technology's potential to increase sales by reducing uncertainty and instilling purchase confidence. To encourage more impactful research in this area, the authors conclude with a research agenda for AR in marketing.Keywords: augmented reality; mobile app; online retail; product uncertainty; virtual product experience""At some point, we're going to look back and think, how didwe not have a digital layer on the physical world?"" – Greg Jones, Director of VR and AR at GoogleAugmented reality (AR) is a technology that superimposes virtual objects onto a live view of physical environments, helping users visualize how these objects would fit into their physical world. Even though AR is in its early stages of growth, leaders in the field such as Apple's CEO, Tim Cook, and Google's Director of Virtual Reality (VR) and AR, Greg Jones, have lauded its potential to transform the retail experience ([ 3]; [23]). With the launch of AR toolkits by technology giants Apple and Google, it is now easier for companies to develop their own AR-enabled mobile apps. Jumping on the bandwagon, Facebook recently introduced AR-enabled display advertisements for their News Feed ([11]), making the technology even more accessible to companies.Augmented reality (AR) is a technology that superimposes virtual objects onto a live view of physical environments, helping users visualize how these objects would fit into their physical world. Even though AR is in its early stages of growth, leaders in the field such as Apple's CEO, Tim Cook, and Google's Director of Virtual Reality (VR) and AR, Greg Jones, have lauded its potential to transform the retail experience ([ 3]; [23]). With the launch of AR toolkits by technology giants Apple and Google, it is now easier for companies to develop their own AR-enabled mobile apps. Jumping on the bandwagon, Facebook recently introduced AR-enabled display advertisements for their News Feed ([11]), making the technology even more accessible to companies.From a retail perspective, a promising application of AR is to facilitate product evaluation by letting customers experience products virtually prior to purchase. Although research has emphasized the importance of direct product experiences to help customers learn about product benefits and assess product fit ([ 6]; [12]), offering direct product experiences can be a logistical challenge, especially in online retail. The introduction of AR has made it possible for shoppers to experience products virtually in the absence of physical products, managing their expectations and instilling purchase confidence ([44]). For example, Amazon and IKEA are using this technology to help customers determine if products or furniture pieces offered online are compatible with their existing room décor, and L'Oréal and Sephora are using AR to show customers how different cosmetic products would alter their appearance. Some of these applications are illustrated in Web Appendix A.Despite the keen interest in AR, there has been limited research demonstrating its tangible impact in real-world contexts. Understanding the potential for AR to increase revenues is important for justifying investments in this new technology. However, the impact of AR on actual product sales is still ambiguous. By helping customers visualize products in their consumption contexts, AR could reduce product fit uncertainty, resulting in more sales. Conversely, AR may also discourage purchases if it leads to perceptions that the products may not fit well. As the technology is unable to convey experiential product attributes that could be important in purchase decisions (e.g., product texture or scent), the impact of AR on sales could also be insignificant. This uncertainty surrounding the impact of AR has been cited as one of the main reasons why companies are still hesitant to embrace the technology, even though most recognize the exciting opportunities it offers ([ 5]). Echoing this lack of clarity, a recent CNN article regarding applications of AR in the cosmetics industry expressed that ""virtual lipsticks and smokey eye shadows are popular in apps, but are they translating into more makeup sales? Hard data isn't easy to come by"" ([38]).Furthermore, whether and how the impact of AR varies across different products or customer segments is also unclear. Having a more nuanced understanding of how AR affects sales would help marketing managers determine when it would be most appropriate to deploy the technology. Conceivably, if AR increases sales by reducing uncertainty, its impact may depend on product and customer characteristics that influence uncertainty in purchase decisions, such as brand popularity, product appeal, and customers' familiarity with the retail channel or category. Accordingly, the present research adopts the retailers' perspective to examine the following questions: How does the use of AR to facilitate product evaluation impact product sales? How does the sales impact of AR usage differ across product characteristics, such as brand popularity, product appeal, rating, and price? How do customers' prior experiences with the online channel and product category influence the sales impact of AR usage?Given that AR is predominantly available on mobile apps ([43]), we focus on the mobile app platform for our analyses. We obtained data from an international cosmetics retailer that incorporated AR into its mobile app to help customers realistically visualize how they would look when they are using different cosmetic products (e.g., eyeshadows, lipsticks). The data contain sales records for 2,300 products, as well as browsing and purchase histories for 160,400 customers, allowing us to investigate how the sales impact of AR varies by product and customer characteristics. In addition, introduction of the AR feature for two product categories during the observation period provided us with a quasi-experimental setting to examine the impact of AR introduction on category sales.Findings from our research provide preliminary evidence that AR usage has a positive impact on product sales. The overall impact appears to be small, but certain products are more likely to benefit from the technology than others. In particular, the impact of AR is stronger for brands that are less popular and products with narrower appeal, suggesting that AR could level the playing field for niche brands or products (sometimes referred to as products in the ""long tail"" of the product sales distribution; e.g., [ 8]). The increase in sales is also greater for products that are more expensive, indicating that AR could increase overall revenues for retailers. In addition, customers who are new to the online channel or product category are more likely to purchase after using AR, suggesting that AR has the potential to promote online channel adoption and category expansion. These findings provide converging evidence that AR is most effective when product-related uncertainty is high, implying that uncertainty reduction could be a possible mechanism by which AR could improve sales.This article is one of the first to empirically demonstrate the impact of AR on sales and how it varies across product and customer characteristics using real-world data. In doing so, it extends prior studies on AR in the marketing field and represents an initial step in understanding what AR means for marketers and retailers. Beyond influencing sales, AR could transform the way brands reach out to and connect with customers at different stages of the customer journey. In the following section, we provide an overview of AR and elaborate on four ways the technology can be incorporated into brands' marketing strategies to reshape the customer retail experience. Then, we focus specifically on how the use of AR to facilitate product evaluation prior to purchase impacts sales in online retail. To encourage marketing academics to further engage in impactful and managerially relevant research in this area, we conclude with a research agenda that we developed in consultation with industry experts and marketing practitioners. Augmented Reality Augmented Reality TechnologyAugmented reality integrates virtual elements into real-world environments to create alternate perceptions of reality. Using sensors and object recognition capabilities from input devices such as cameras, AR technology scans the physical environment, identifies features in the environment, and superimposes virtual objects (e.g., two- or three-dimensional images or animations, text, sounds) on top of a live view of the real world. By blending virtual elements into physical environments in real time, AR enriches users' visual and auditory perceptions of reality. In most cases, the virtual elements are also responsive to movements or gestures, creating an interactive experience for users.Although AR is often classified together with VR, the two technologies are distinct, both in how they function and the way they are experienced. Unlike AR, which receives input from the real world and adds virtual elements to it, VR immerses users in a completely digital and artificial environment, shutting them out from their surroundings. Due to the disorienting experience of being entirely isolated from the real world and the expensive headsets required ([19]), the appeal of VR has largely been limited to industries with products high in simulated content, such as gaming and entertainment ([13]). In contrast, AR allows users to experience virtual elements without the vulnerability of being blind to the real world. In addition, AR can be experienced directly from handheld devices that users already own (e.g., tablets or smartphones). Thus, AR is rapidly gaining prominence, and close to 100 million U.S. consumers are expected to use the technology regularly by 2022 ([43]). Augmented Reality in RetailThe unique capabilities of AR present marketers with new opportunities to engage customers and transform the brand experience. Drawing on an extensive review of current applications of AR, we identified four broad uses of the technology in retail settings: to ( 1) entertain and ( 2) educate customers, help them ( 3) evaluate product fit, and ( 4) enhance the postpurchase consumption experience. These uses loosely correspond to customers' journey from awareness to interest, consideration, purchase, and consumption, and they may not be mutually exclusive. Next, we elaborate on these four uses and provide a summary with relevant examples in Table 1.[ 6]GraphTable 1. Uses of AR in Retail. Uses of ARRole of ARIllustrative Use CasesEntertain customersCreate novel and engaging experiences for customersBuild brand interestDrive foot traffic to physical storesWalmart collaborated with DC Comics and Marvel to bring exclusive superhero-themed AR experiences to selected outlets.Starbucks Reserve Roastery in Shanghai uses AR to offer customers a digital tour of their massive roasting facility.Educate customersDeliver content and information in an interactive and visually appealing mannerHelp customers understand complex mechanisms and better appreciate the value of productsWalgreen's and Lowe's use AR in their in-store navigation apps to guide users to product locations and notify them if there are special promotions along the way.Toyota and Hyundai use AR to demonstrate key features and innovative technologies in their new car models.Help customers evaluate product fitHelp customers visualize products in their actual consumption contextsIncrease customers' confidence in their purchase decisions in the absence of physical productsAccommodate wide product assortments and customization without the need for physical inventoryIKEA's Place app uses AR to help customers determine if products fit with their existing room décor.L'Oréal's Virtual Try-On feature and Sephora's Virtual Artist app use AR to show customers how different cosmetic products would look on them.Uniqlo and Topshop use AR to offer a more convenient way of trying on different outfits in their physical stores.BMW and Audi use AR to give customers a preview of cars based on customizable features such as paint color, wheel design, and interior aesthetics.Enhance customers' postpurchase consumption experienceOffer new ways of enjoying products after they are purchasedDeliver additional information while the products are being used or consumedLEGO's Hidden Side sets are specially designed to be played together with the companion AR app.McDonald's used AR to let customers discover the origins of ingredients in the food they purchased.Hyundai's Virtual Guide app uses AR to teach car owners how to perform basic maintenance. 1 Note: URL links to these examples are provided in Web Appendix B. EntertainAR's ability to transform static objects into interactive and animated three-dimensional objects offers new ways for marketers to create fresh experiences to captivate and entertain customers. Besides generating hype and interest, marketers have also used AR-enabled experiences to drive traffic to their physical locations. For example, Walmart collaborated with media companies such as DC Comics and Marvel to bring exclusive superhero-themed AR experiences to their stores by placing special thematic displays in selected outlets. In addition to creating novel and engaging experiences for customers, it also encouraged them to explore different areas within the stores. EducateDue to its interactive and immersive format, AR is also an effective medium for delivering content and information to customers. For instance, to help customers better appreciate their new car models, Toyota and Hyundai have utilized AR to demonstrate key features and innovative technologies in a vivid and visually appealing manner. Retailers can also use AR to help customers navigate stores or highlight relevant product information to influence their in-store purchase decisions. Companies such as Walgreen's and Lowe's have developed in-store navigation apps that overlay directional signals onto a live view of the path in front of users to guide them to product locations and notify them if there are special promotions along the way. EvaluateBy retaining the physical environment as a backdrop to virtual elements, AR also helps users visualize how products would appear in their actual consumption contexts, allowing them to more accurately assess product fit prior to purchase. For example, IKEA's Place app uses AR to give customers a preview of different furniture pieces in their homes by overlaying true-to-scale, three-dimensional models of products onto a live view of the room. Customers can easily determine if a product fits in a given space without the hassle of taking measurements. Fashion retailers Uniqlo and Topshop have also deployed the same technology in their physical stores, offering customers greater convenience by reducing the need for them to change in and out of different outfits. An added advantage of AR is its ability to accommodate a wide assortment of products. By replacing tangible product displays with lifelike virtual previews of products, retailers can overcome the constraints of physical space while still offering customers the opportunity to explore different product options. This capability is particularly useful for made-to-order or bulky products. Car manufacturers BMW and Audi have used AR to provide customers with true-to-scale, three-dimensional visual representations of car models based on customizable features such as paint color, wheel design, and interior aesthetics. These cases exemplify AR's huge potential to increase customers' confidence in their purchase decisions for a variety of products. EnhanceLastly, AR can be used to enhance and redefine the way products are experienced or consumed after they have been purchased. For example, LEGO recently launched several brick sets that are specially designed to combine physical and virtual gameplay. Through the companion AR app, animated LEGO characters spring to life and interact with the physical LEGO sets, creating a whole new playing experience. In a bid to address skepticism about the quality of its food ingredients, McDonald's has also used AR to let customers discover the origins of ingredients in the food they purchased via storytelling and three-dimensional animations.The present research focuses on the use of AR to help customers evaluate products prior to purchase. Specifically, we explore the possibility of leveraging AR to reduce product-related uncertainty in online purchase decisions. To extend prior research on AR in retail (summarized in Table 2), we use real-world data to examine how customers' use of AR to try products (for brevity, we refer to this as ""AR usage"" for the rest of the article) affects product and brand sales. In the following section, we present our conceptual framework and develop hypotheses for the impact of AR usage on sales.GraphTable 2. Selected Literature on AR in Retail. ArticleMethodologyContextKey Outcome VariablesKey FindingsHilken et al. (2017)ExperimentalUsing situated cognition theory to understand AR's potential to enhance online experiencesValue perceptions of online experiences; decision comfort; purchase and word-of-mouth intentionsThe combination of simulated physical control and environmental embedding offered by AR creates a feeling of spatial presence.As a result, AR enhances consumers' perceptions of online experiences, decision comfort, and behavioral intentions.Yim, Chu, and Sauer (2017)ExperimentalComparing AR versus web-based product presentationsAttitude toward AR and purchase intentionsCompared to web-based displays, AR is more immersive due to its interactive and vivid nature.As a result, AR is perceived to be more useful and enjoyable, leading to positive consumer attitudes and purchase intentions.Brengman, Willems, and Van Kerrebroeck (2019)ExperimentalImpact of AR on perceived ownershipPerceived ownership and purchase intentionsCompared to other touch and nontouch interfaces, mobile-enabled AR creates higher feelings of perceived ownership, positively affecting consumers' attitudes and purchase intentions.Heller et al. (2019a)ExperimentalUsing mental imagery theory to understand how AR influences word of mouthWord-of-mouth intentionsAR improves processing fluency by facilitating imagery generation and transformation, leading to higher consumer decision comfort and word-of-mouth intentions.Heller et al. (2019b)ExperimentalComparing touch versus voice control modalities in multisensory ARDecision comfort and willingness to payTouch control (vs. voice control) reduces mental intangibility, leading to higher consumer decision comfort and willingness to pay.Hilken et al. (2020)ExperimentalShared decision making using social ARDecision makers' product choice; spillover effects to recommendersAR empowers recommenders by allowing them to take the point of view of the decision makers.AR stimulates recommenders' desire for products, leading to positive behavioral intentions.Current articleInstrumental variable estimation and quasi-experiment using real-world dataExamining the impact of AR usage on sales and the moderating impact of product and customer characteristicsProduct and category salesAR has a positive impact on sales for brands that are less popular, products with narrower appeal, and products that are more expensive.AR has a stronger impact for customers who are new to the retailer's online channel or product category.  Conceptual Framework Product Uncertainty in Online RetailBecause customers cannot perfectly predict the consequences of their purchase decisions, uncertainty is inherent in market exchanges ([ 4]). However, it is especially pronounced in online environments due to the spatial separation between buyers and sellers as well as the temporal separation between payment and product fulfillment ([ 9]; [41]). Unlike in traditional retail settings, customers are unable to physically inspect or evaluate products before making a purchase, resulting in greater uncertainty that the products would be able to deliver the expected level of performance or benefits ([ 6]; [17]; [36]).Researchers have broadly distinguished between two types of product uncertainty in online markets: product performance uncertainty and product fit uncertainty. Product performance uncertainty occurs when customers are unable to evaluate or predict product performance due to imperfect knowledge ([17]). In contrast, product fit uncertainty occurs when customers are unable to determine if the product matches their needs ([ 6]; [32]). The latter form of uncertainty is typically higher for products with experience attributes (i.e., attributes that can only be evaluated after the product has been experienced; [32]), such as apparel or beauty products.Several mechanisms to reduce product performance uncertainty in online retail have been suggested. For example, retailers could lower information asymmetry by providing diagnostic product descriptions or by including credibility signals such as third-party product assurances, warranties, or customer reviews ([17]; [51]). In contrast, product fit uncertainty typically requires direct product experience to resolve, as it is idiosyncratic in nature and varies from individual to individual. Although some retailers have adopted try-before-you-buy programs (e.g., Warby Parker's home try-on program; [ 6]) or lenient product return policies ([24]; [53]) to provide opportunities for direct product experiences, these measures are notoriously costly for retailers due to the additional shipping and handling costs and risks of product damage ([22]). Furthermore, direct product experiences may not be viable or appropriate for certain products, such as products that are customized (e.g., engagement rings), products that require assembly (e.g., furniture), or personal care products (e.g., cosmetics). Augmented Reality and Product UncertaintyThe introduction of AR has made it possible to substitute direct product experiences with virtual product experiences to facilitate product evaluation and reduce product fit uncertainty. Using a situated cognition perspective, [29] propose that the value of AR lies in its ability to help customers visually integrate virtual products into the real-world environment (i.e., ""environmental embedding"") and use bodily movements and physical actions to control how products are presented (i.e., ""simulated physical control""). The unique combination of these two properties induces perceptions that the virtual products are physically present in the real world, creating realistic product experiences. Consequently, customers are able to evaluate products as if they are actually interacting with the real products, resulting in reduced product fit uncertainty. In line with this, prior research finds that vivid images and greater control over the presentation of information are effective ways to alleviate uncertainty in online environments ([51]). By helping customers visualize products in their consumption contexts and reducing product fit uncertainty, AR-enabled product experiences increase the level of ease customers feel in the decision-making process, translating to positive behavioral intentions ([26]; [29]).However, although AR communicates visual information about products, it is unable to convey other experiential product attributes (e.g., product texture, scent). For example, even though customers may use AR to visualize an IKEA sofa in a room, they are unable to assess how comfortable it is. Similarly, users trying on cosmetic products via AR are unable to evaluate other product attributes such as the texture and consistency of the product, which may affect ease of application and the way the product feels on the skin. According to [34], if customers do not perceive trial experiences as accurately representing actual consumption experiences, they may discount those trial experiences when they form judgments about the product. Thus, the extent to which virtual product experiences involving AR could influence online purchases is unclear. Nevertheless, as prior research has demonstrated the positive effects of providing fit information in online retail (e.g., [21]; [35]), we expect AR usage to have a positive impact on product sales because the technology could convey visual information that may reduce product fit uncertainty in online purchase decisions. Therefore, we predict the following: H1:  AR usage has a positive impact on sales.Building on the proposition that AR usage increases sales by reducing product fit uncertainty, we further hypothesize that AR would have a stronger impact when customers experience higher levels of uncertainty. In particular, the level of uncertainty experienced in a purchase decision could depend on product characteristics such as brand popularity, product appeal, and ratings. The level of uncertainty may also influence the price that customers are willing to pay for the product. Thus, the relationship between AR usage and sales may differ across these product characteristics. In addition, customers also vary in their need to reduce product fit uncertainty before making a purchase ([ 6]). This need to reduce uncertainty could depend on customers' familiarity with the online channel and product category. As a result, the impact of AR may also vary across these customer characteristics. Accordingly, we develop hypotheses for the moderating effects of product and customer characteristics in the following sections. Our conceptual framework is presented in Figure 1.Graph: Figure 1. Conceptual framework.Note: Signs in parentheses represent the hypothesized effects. Moderating Effects of Product Characteristics Brand popularityPrior research has shown that consumers are more cautious when they purchase from lesser-known brands, as they anticipate feeling more regret if the product turns out to be inferior ([45]). Consistent with this, [18] find that cultures high in uncertainty avoidance place greater emphasis on brand credibility. In online environments, brand signals are even more important because consumers are not able to inspect products before purchasing ([16]). However, [31] demonstrates that when additional information is available to facilitate decision making, consumers rely less on brand signals. As a result, less-established brands benefit more from the increased availability of information. In the same vein, by communicating visual information to help customers assess product fit, AR may reduce uncertainty in online purchase decisions. Consequently, AR may decrease customers' reliance on brand signals and inadvertently increase preference for brands that are less popular. We use the term ""popular"" in a general sense to refer to brands that are more widely adopted. Therefore, we hypothesize the following: H2a:  The impact of AR usage on sales is stronger for brands that are less popular. Product appealWithin the same category or brand, products may also have different levels of appeal due to the alignment between their inherent characteristics and general consumer preferences. For example, a red lipstick is more mainstream and has broader appeal than a blue lipstick. We draw a distinction between brand popularity and product appeal in that the latter depends on intrinsic properties of the product and could be independent of the brand. Thus, a red lipstick from an unknown brand could have broad appeal but low brand popularity, whereas a blue lipstick from a well-known brand could have limited appeal despite having high brand popularity. As products with broad appeal cater to the masses, they are more likely to match the needs of the general consumer. Conversely, because products with narrower appeal serve a niche segment, there is a higher probability that they will not match the preferences of the general consumer and will thus carry greater product fit uncertainty. Nevertheless, [ 8] demonstrate that in online contexts, search and discovery features such as search tools or recommendation engines can shift consumers' preferences to niche products by lowering the cost of acquiring product information. Consistent with this, [50] find that products with narrower appeal benefit more from greater information availability. By visually conveying product information to help customers assess product fit in an effortless and risk-free environment, AR could have a stronger impact for products with narrower appeal due to the higher product fit uncertainty associated with these products. Therefore, we hypothesize the following: H2b:  The impact of AR usage on sales is stronger for products with narrower appeal. RatingsCustomers often turn to online ratings or reviews as a source of information to resolve uncertainty about product quality and fit ([14]). In line with this, [37] find that consumers from countries that are high in uncertainty avoidance are more sensitive to both the valence and volume of product ratings. However, as consumers tend to overrate direct experiences with products ([30]), the ability to evaluate products and resolve uncertainty via firsthand experiences with those products on AR platforms may reduce customers' reliance on online ratings. Thus, by enabling customers to learn about product benefits and assess product fit through their own virtual experiences, AR could diminish the role of online ratings in purchase decisions. As a result, customers may be more amenable to purchasing products despite their lower ratings if they are able to try these products using AR. Therefore, we predict the followsing: H2c:  The impact of AR usage on sales is stronger for products with lower ratings. PriceWhen customers experience product uncertainty, they are not able to accurately assess the benefits the products offer. As a result, customers are more likely to undervalue the products and are less willing to pay a premium ([17]). Consistent with this, [36] find that customers who are familiar with online shopping are still hesitant to purchase expensive products through the internet when there is a high degree of product uncertainty because they could suffer greater financial losses if these products do not fit them well. By facilitating product evaluation prior to purchase, AR helps customers ascertain if products match their needs and preferences. Consequently, customers may experience less uncertainty and feel more comfortable purchasing products that are more expensive. In line with this, [27] find that AR usage improves decision comfort, leading to higher willingness to pay. Therefore, we predict the following: H2d:  The impact of AR usage on sales is stronger for more expensive products. Moderating Effects of Customer Characteristics Channel experienceAccording to [36], customers who are familiar with online shopping are more inclined to purchase products with a higher degree of uncertainty because their cumulative online shopping experiences help them develop the ability to assess products when limited information is available. Thus, customers who have purchased from a retailer's online channel in the past may feel more comfortable making subsequent online purchases despite experiencing product uncertainty, potentially making them less dependent on AR to make their purchase decisions. In contrast, customers who are new to the retailer's online channel (but have made prior purchases at the retailer's offline channel) are not accustomed to making purchases in the absence of actual products. As a result, they may experience greater product fit uncertainty and may be deterred from purchasing online due to the inability to assess product fit. Because AR simulates the in-store experience of trying products, it may help reduce product fit uncertainty for customers who are new to the online channel. These customers may derive greater value from the ability to evaluate products virtually, potentially making them more likely to purchase online after using AR. Therefore, we predict the following: H3a:  The impact of AR usage on sales is stronger for customers who are new to the retailer's online channel. Category experienceBesides channel experience, customers' familiarity with the product category also affects their level of product fit uncertainty ([32]). Customers who are familiar with a product category can draw on their prior experiences as an information source to form judgments about products ([46]). As a result, they may rely less on AR in their purchase decisions. Conversely, customers who are unfamiliar with a product category lack the necessary category knowledge to evaluate product attributes and, at the same time, may not be aware of their own preferences ([32]). Consequently, these customers will have more difficulty assessing whether a product's attributes match their preferences, resulting in greater product fit uncertainty. By helping customers visualize how products would appear in their actual consumption contexts, AR could reduce product fit uncertainty and increase purchase confidence for customers who are new to the product category. As a result, AR usage may have a stronger impact on the purchase decisions for these customers. Therefore, we predict the following: H3b:  The impact of AR usage on sales is stronger for customers who are new to the product category.To summarize, we propose that AR usage will positively impact sales by reducing product uncertainty. Following this line of reasoning, we developed several predictions about which products would be more likely to benefit from AR and which customers would be more likely to respond to AR. Empirical Analysis Empirical ContextAs AR is predominantly available through mobile apps ([43]), we focus our analyses on the mobile app platform. To test our hypotheses, we obtained data from an international cosmetics retailer with both an online and offline presence. Leveraging AR technology, the retailer integrated a new feature on its existing mobile app that allowed customers to virtually try on makeup products (e.g., eyeshadows, lipsticks). The AR technology detected customers' facial features via their smartphone cameras and superimposed the shade of chosen products onto a live view of their face in real time, giving them a realistic visual representation of their appearance when they are using the products. The brand, product name, and price were displayed at the top of the screen. Figure A3 in Web Appendix A provides a visual example of a customer trying on a lipstick using the AR feature. For comparison, the corresponding product detail page view (i.e., the conventional way of conveying product-related information on mobile retail apps) is also provided. Prior to the start of our observation period in December 2017, the AR feature was only available for lip categories (i.e., lipstick and lip gloss) and was subsequently introduced for eye categories (i.e., eyeshadow and eyeliner) in March 2018. Figure A4 in Web Appendix A provides a visual overview of AR availability for the different categories.We obtained two separate data sets from the retailer for one of its key markets in Asia Pacific. The first data set contained information about browsing activities on the mobile app, including specific products customers tried using the AR feature, and covered a 19-month period from December 2017 to June 2019. The second data set contained transaction records from June 2017 to June 2019 for all retail channels, including mobile app, website, and offline stores. We merged the two using customers' loyalty card number, which allowed us to match AR usage and product purchases at a disaggregate level.During the 19-month period, a total of 160,407 shoppers browsed products from the lip and eye categories across 806,029 sessions, 20.8% of which involved AR usage. Customers who used AR during the session spent 20.7% more time browsing (Mwith_AR = 16.6 minutes, Mwithout_AR = 13.8 minutes, p <.01) and browsed 1.28 times more products (Mwith_AR = 53.9, Mwithout_AR = 42.2, p <.01). The purchase rate for sessions with AR usage was 19.8% higher than for sessions without AR usage (3.15% with AR vs. 2.63% without AR, p <.01), providing preliminary indication of the positive impact of AR on sales.We divide our analyses into three sections. In the first section, we perform the analysis at the product level to examine the moderating effects of brand popularity, product appeal, rating, and price. To minimize selection bias arising from availability of the AR feature, we focus on lipsticks and lip glosses, as the feature was available for > 96% of products in each of these categories. In the second section, we take advantage of the introduction of AR for two eye categories (i.e., eyeshadow and eyeliner) to examine the effect of AR introduction on category sales using a quasi-experimental differences-in-differences-in-differences (DDD) approach. Finally, we investigate how the impact of AR varies at the customer level. As all customers had no knowledge that the AR feature would be introduced for the two eye categories prior to the introduction, the event provided us with a clean setting for examining how customers' channel and category experience (prior to the introduction) would moderate the impact of AR usage on purchase probability. Product-Level AnalysisAs product color is an important factor in cosmetic purchases, we considered each shade/color of retail merchandise as a unique product. In total, we had 2,334 products in the lipstick and lip gloss categories (1,984 products across 41 brands for lipstick; 350 products across 28 brands for lip gloss). Our empirical strategy was to relate the number of customers using AR to try each product during a particular time period with sales volume for that product during the same time period. We estimated the model at the monthly product level, giving us a total of 44,346 observations (2,334 products × 19 months from December 2017 to June 2019). As one of our objectives was to examine the moderating effect of product ratings, we included products with a rating in the main analysis and replicated the analysis for all products as a robustness check. Our final sample for the main analysis consisted of 29,345 observations. Model specificationFor each product i, we modeled how the volume of AR usage in month t, AR Usageit, influenced the number of products sold in month t, Product Salesit. As Product Salesit was a count variable with significant over-dispersion (M =.46, SD = 1.73) and over 80% of observations were 0, we used a zero-inflated negative binomial model for the estimation. The vector of covariates in the regression is given by the following equation: Xitβ =β0+ β1AR Usageit+ β2Brand Popularityit+ β3Appealit+β4Ratingit+β5Priceit+ β6AR Usageit×Brand Popularityit+ β7AR Usageit×Appealit+ β8AR Usageit×Ratingit+ β9AR Usageit×Priceit+ β10Categoryi+∑m = 1T − 1δmMontht+∊it. Graph1In Equation 1, we measured AR Usageit as the number of customers using AR to try product i during month t. As brands that are more widely adopted should have higher sales, and because the web and app channels are both online and carry identical products, we used total brand sales (within the category) from the web channel during the same period as a proxy for brand popularity, Brand Popularityit. Following prior research using product sales as an indicator of mass or niche appeal (e.g., [ 8]), we used total product sales from the web channel during the same period to reflect product i's breadth of appeal, Appealit. Ratingit and Priceit were the rating and price of product i at time t, respectively. To examine how the impact of AR was influenced by brand popularity, product appeal, rating, and price, we included the corresponding interactions in the model. In addition, we included Categoryi (1 = lipstick, 0 = lip gloss) and a series of dummy variables, Montht, (for t = 1,..., T months) to control for category and month effects. Table 3 provides a summary of how the variables were operationalized and their descriptive statistics, and we provide their correlations in Web Appendix C. All the correlations were low, and the variance inflation factors were below 1.62, indicating that multicollinearity was not an issue. To prevent overestimation of effects due to the panel structure of the data, we clustered standard errors at the product level (e.g., [49]).GraphTable 3. Variable Operationalization and Descriptive Statistics for Product Model. VariableOperationalizationMeanSDMinMedianMaxProduct SalesTotal product sales from mobile app (in units).461.73.00.0064.00AR UsageNumber of customers using AR to try the product in focal country13.9022.67.006.00611.00AR UsageAltNumber of times the product was tried using AR in focal country14.4523.92.007.00620.00Brand PopularityTotal brand sales from website (in thousands of units).04.07.00.02.51Brand PopularityAltNumber of customers buying the brand from website (in thousands).03.04.00.01.42AppealTotal product sales from website (in units).25.97.00.0032.00AppealAltNumber of customers buying the product from website.25.95.00.0031.00RatingProduct rating at time t (on a five-point scale)4.14.67.504.255.00PriceProduct price at time t31.8011.175.0032.0077.00AR UsageCountry_ANumber of customers using AR to try the product in Country A.591.43.00.0039.00AR UsageCountry_BNumber of customers using AR to try the product in Country B.28.71.00.0011.00  Identification strategyOur objective was to understand how the volume of AR usage for product i during month t, AR Usageit, influenced product sales, Product Salesit. However, AR Usageit could be endogenous, as customers may have been more inclined to use AR to try products they were already interested in purchasing. To account for this endogeneity, we used the two-stage residual inclusion method ([48]), which has been used in recent research when both the endogenous and dependent variables are nonlinear (e.g., [ 2]; [15]).Following the two-stage residual inclusion method, we first regressed the endogenous variable, AR Usageit, on all other covariates in Equation 1. Residuals from this first stage were then included to estimate Product Salesit. Similar to the control function approach ([42]), the included residuals controlled for the portion of the endogenous variable that would otherwise correlate with the error term in Equation 1. According to [48], we needed to include instruments in the first stage estimation to resolve the identification problem in Equation 1. These instruments should ( 1) be strongly related to the endogenous variable and ( 2) not be correlated with the error term in Equation 1. In other words, the instruments should only have an indirect relationship with the outcome variable, Product Salesit, through their association with the endogenous variable, AR Usageit. As realizations of the same variable from different markets can serve as suitable instruments ([40], p. 601), we used the volume of AR usage in two other countries for the same product during the same month as our instruments (i.e., AR UsageitCountry_A and AR UsageitCountry_B, respectively). Underlying this choice of instruments is the assumption that customer preferences are similar across markets and that product-specific factors affecting customers' interest in trying products using the AR feature should be constant in all markets, satisfying the first condition. However, the number of customers using the AR feature to try products in other markets should have no bearing on customers' purchase decisions in the focal market, satisfying the second requirement. We also used lagged values of AR Usageit as an alternative instrument (e.g., [15]) and discuss this further in the robustness analyses section.Because AR Usageit is a count variable with significant over-dispersion (M = 13.9, SD = 22.7), we used a negative binomial model for the first stage estimation. As predicted residuals from the first stage were used in the estimation of Equation 1, standard errors needed to be corrected to account for this additional source of variation ([42]). We implemented the cluster bootstrapping method ([10], p.327) to approximate the correct standard errors using 1000 bootstrap samples. ResultsFrom the first stage estimation (provided in Web Appendix D), coefficients for the instruments are positive and significant (.414 for AR UsageCountry_A and.301 for AR UsageCountry_B, p <.01 for both). Furthermore, the instruments are highly correlated with AR Usage (.75 for AR UsageCountry_A and.64 for AR UsageCountry_B, p <.01 for both), and the F-statistic of excluded instruments in the first stage regression is 5,520, which is well above the recommended cutoff of 10 ([ 1]). These results indicate that the instruments are strongly related with the endogenous variable. To assess validity of the instruments, we performed the Hansen J-test for over-identifying restrictions. Results from the test fail to reject the null hypothesis that the instruments are uncorrelated with the second stage error term (χ2 ( 1) =.699, p =.40), providing additional support for the choice of instruments.To examine the main effect of AR usage in H1, we estimated the second stage model without interaction terms. Results for this model are presented in Table 4, Column 1. The coefficient for AR Usage is significantly positive (.006, p <.01), suggesting a small but positive relationship between the number of customers using AR to try the product and sales for that product during the same month. Thus, H1 is supported. The coefficients for other variables are largely in line with common intuition. For example, brand popularity (.894, p <.05), breadth of product appeal (.385, p <.01), and product rating (.094, p <.05) are positively associated with product sales, whereas price (−.005, p <.10) has a negative relationship with product sales. The coefficient for the residual correction term, which is equivalent to the Hausman test for the presence of endogeneity ([40]), is significant (.071, p <.01), indicating that the endogeneity-corrected estimates are preferred. Thus, we focus on results from the two-stage model and provide results for the uncorrected model in Web Appendix D.GraphTable 4. Product Model: Impact of AR Usage on Product Sales and Moderating Effects of Product Characteristics. Column 1Second Stage(WithoutInteractions)Column 2Second Stage(Full Model)Column 3Alternative Instrument for AR UsageColumn 4Including Products Without RatingsAR Usage (H1).006 (.001) ***−.002 (.006)−.003 (.007).007 (.006)Brand Popularity.894 (.364) **1.482 (.356) ***1.796 (.396) ***1.675 (.333) ***Appeal.385 (.029) ***.416 (.023) ***.419 (.027) ***.473 (.023) ***Rating.094 (.042) **.052 (.054).062 (.056)–Price−.005 (.003) *−.009 (.004) **−.008 (.004) **−.010 (.003) ***AR Usage × Brand Popularity (H2a)–−.022 (.009) **−.025 (.008) ***−.028 (.010) ***AR Usage × Appeal (H2b)–−.001 (.000) ***−.001 (.000) **−.001 (.000) ***AR Usage × Rating (H2c)–.001 (.001).001 (.001)–AR Usage × Price (H2d)–.000 (.000) *.000 (.000) *.000 (.000) *Correction term.071 (.027) ***.050 (.027) *.102 (.053) *.063 (.026) **Constant−1.114 (.223) ***−.913 (.264) ***−1.185 (.280) ***−.687 (.162) ***Category dummyIncludedIncludedIncludedIncludedMonth dummiesIncludedIncludedIncludedIncludedObservations29,34529,34528,30544,346Log likelihood−18,573−18,517−17,630−25,310 2 *p ≤.10; **p ≤.05; ***p ≤.01.3 Note: Standard errors (clustered at product level) are in parentheses.Results for the full second stage model are presented in Table 4, Column 2. In support of H2a and H2b, the interactions between AR Usage and Brand Popularity (−.022, p <.05) and Appeal (−.001, p <.01) are significantly negative, indicating that the sales impact of AR usage is stronger for brands that are less popular and products with narrower appeal. The interaction between AR Usage and Price is significantly positive (.000, p <.10), suggesting that the sales impact of AR usage is stronger for products that are more expensive. Thus, H2d is also supported. However, the results do not provide support for H2c, as the interaction between AR Usage and Rating is not significant (.001, p >.10). Robustness analysesWe performed several analyses to ensure that our findings are robust to different assumptions and model specifications. First, following prior research, which has used lagged values of endogenous variables as instruments (e.g., [15]), we used the volume of AR usage for product i in the past one month as an alternative instrument. As app activity data prior to the first month (i.e., December 2017) was unavailable, we excluded observations for the first month. Results for this model are presented in Table 4, Column 3, and the findings are consistent. Because we were interested in the moderating effect of ratings, we focused on products that had a rating in the main analysis. Since the coefficient for Rating was not significant, we excluded it in the model specification and replicated the analysis for all products. Results for this model are also consistent with the main findings and are presented in Table 4, Column 4.We also explored alternative operationalizations for AR Usage, Brand Popularity, and Appeal. Instead of operationalizing AR Usage as the number of customers using AR to try product i, we used the number of times product i was tried using AR to account for repeated AR usage from the same customer. We also operationalized Brand Popularity and Appeal as the number of customers purchasing the brand and product, respectively. Results for these models are reported in Web Appendix E. Across all robustness analyses, results are generally consistent with the main model, providing further validation for our findings. Category-Level DDD AnalysisThe introduction of AR for two eye categories (i.e., eyeshadow and eyeliner) in mid-March 2018 presented a unique opportunity to examine the impact of AR introduction on sales. Using a quasi-experimental approach, we regarded AR introduction as a treatment and examined its impact by comparing differences in sales for products with and without the AR feature, before and after the feature was introduced. Because the AR feature was only available for eyeshadows and eyeliners, a potential comparison could be between these categories and other eye categories that did not have the feature (i.e., eyebrows, mascaras, and eye palettes). This between-category comparison relies on the crucial assumption that sales trends across different eye categories would be parallel in the absence of AR introduction. As cosmetic products are often used concurrently, sales for products targeting the same facial feature should generally move in the same direction. Since the AR feature was only available on the mobile app, an alternative comparison could be between the app and web channels. This approach avoids the assumption that trends across different eye categories are similar, but it requires a separate assumption that without AR introduction, sales trends in the two online channels would be parallel.A more robust approach that does not require either of these assumptions is the DDD approach ([ 1], p. 181; [54], p.150), which combines both comparisons. Specifically, the DDD analysis measures differences between app and web sales for eyeshadows and eyeliners before and after AR introduction, relative to the same differences for other eye categories that do not have the AR feature. Thus, the DDD approach controls for both channel and category trends that could potentially confound the effect, and it relies on the more relaxed assumption that in the absence of AR introduction, sales trends in the two online channels would be parallel for products in the same category. Following [33] and [20], we conducted two falsification tests using data from the pre–AR introduction period, and the results provide support that this assumption holds in our study. Details and results for these falsification tests are included in Web Appendix F.Accordingly, we examined changes in weekly sales for the five product categories (i.e., eyeshadow, eyeliner, eyebrows, mascara, and eye palettes) across two channels (i.e., app and web) before and after AR introduction. Our sample covers a duration of 108 weeks (i.e., 42 weeks for the pre–AR introduction period and 66 weeks for the post–AR introduction period), giving us a total of 1,080 observations (5 × 2 × 108 = 1,080). Model specificationThe outcome variable of interest was sales for category j on channel k during week t, Category Salesjkt. As Category Salesjkt was a count variable with significant overdispersion (M = 64.8, SD = 78.0), we used a negative binomial model for the estimation. Following [54], p.150), the vector of covariates in the regression is given by the following:  Xjktβ=β0+β1AR Introt+β2Appk+β3AR Featurej+ β4AR Introt×Appk×AR Featurej+ β5AR Introt×Appk+ β6AR Introt×AR Featurej+ β7Appk×AR Featurej+∑c = 1J − 2γcCategoryj+∑w = 1T − 2δwWeekt + ∊jkt. Graph2In Equation 2, AR Introt is a dummy variable with a value of 1 if week t was in the post–AR introduction period and 0 otherwise. Appk is a dummy variable with a value of 1 for the mobile app and 0 for the website, and AR Featurej is a dummy variable with a value of 1 for eye categories with the AR feature (i.e., eyeshadow and eyeliner) and 0 for other eye categories. The key coefficient of interest was β4, which captured the three-way interaction between AR introduction, retail channel, and categories that have the AR feature. Thus, β4 represents the additional change in mobile app sales post–AR introduction for eyeshadow and eyeliner, after accounting for channel and category-related changes over the same period (captured by β5 and β6 respectively). We included all lower-order interactions in the model, as well as a series of dummy variables, Categoryj (for j = 1,..., J categories) and Weekt, (for t = 1,..., T weeks) to control for category and week effects. Because Categoryj was perfectly collinear with AR Featurej and Weekt was perfectly collinear with AR Introt, we excluded dummy variables for an additional category and week. To account for the panel nature of the data, we clustered standard errors at the category-channel level, allowing errors for observations from the same category within each channel to correlate. ResultsBefore discussing results for the DDD analysis, we present the basic pre-post model in Table 5, Column 1. We regressed weekly mobile app sales for eyeshadow and eyeliner on AR Introt and the vector of dummies. The coefficient for AR Introt is significantly positive (.611, p <.05), providing preliminary evidence that sales increased after AR was introduced. Results for the DDD analysis are presented in Table 5, Column 2. The coefficient for the three-way interaction between AR introduction, app, and categories with the AR feature is marginally significant (.449, p <.10), providing some evidence that sales for eyeshadows and eyeliners increased on the app channel after AR was introduced.GraphTable 5. DDD Analysis: Impact of AR Introduction on Category Sales. Column 1Basic Pre-PostModelColumn 2DDD AnalysisColumn 3Including TrendsColumn 4Excluding Sale EventsAR Intro.611 (.245) **.265 (.214).771 (.445) *.720 (.429) *App–.125 (.058) **39.187 (9.72) ***31.953 (8.95) ***AR Feature–.068 (.146)5.794 (8.44)3.720 (7.57)AR Intro × App × AR Feature–.449 (.262) *.441 (.249) *.465 (.264) *AR Intro × App–−.601 (.237) **.243 (.154).036 (.169)AR Intro × AR Feature–−.155 (.177)−.066 (.209)−.113 (.200)App × AR Feature–−.034 (.088)−.027 (.056)−.036 (.048)Constant2.043 (.040) ***2.604 (.156) ***2.403 (.226) ***2.464 (.201) ***Category dummiesIncludedIncludedIncludedIncludedWeek dummiesIncludedIncludedIncludedIncludedCategory trendNot includedNot includedIncludedIncludedChannel trendNot includedNot includedIncludedIncludedObservations2161,0801,080940Log likelihood−897−5,143−5,095−4,179 4 *p ≤.10; **p ≤.05; ***p ≤.01.5 Note: Standard errors (clustered at category-channel level) are in parentheses. Robustness analysesTo check the DDD identification strategy, we included channel and category trends in Equation 2 ([ 1], p. 178). Results of this alternative model are presented in Table 5, Column 3, and the coefficient of the three-way interaction of interest is similar in direction, magnitude, and significance with the main model. Although the weekly fixed effects controlled for variations in overall sales between weeks, they did not account for time-varying confounding effects that were specific to the channel-category. Thus, if there were more app-exclusive sale events for the eyeshadow and eyeliner categories in the post–AR introduction period relative to the pre–AR introduction period, the effect of AR Introduction on app sales in these two categories would be overstated. As a robustness check, we removed weeks that coincided with sale events from the analysis and present the results in Table 5, Column 4. We also split AR Featurej into the two eye categories with the AR feature, Eyeshadowj and Eyelinerj, and the coefficients of both three-way interactions are marginally significant, providing convergent validity for our results. Furthermore, results from the Wald test for equality of coefficients fail to reject the null hypothesis that the coefficients are the same (p =.76), indicating that the effect of AR introduction on sales is not category-specific. Lastly, we estimated the same model using a Poisson regression. Results of these additional analyses are provided in Web Appendix G. Across all robustness analyses, the direction, magnitude, and significance of coefficients are similar to the main model.Overall, the results provide additional support for H1 and demonstrate that the positive impact of AR generalizes to other product categories. We note that because the retailer did not announce the introduction of AR for the eye categories, usage of the feature was low. On average, the weekly number of customers using AR to try products from the eye categories was 6.4 times lower than the number for lip categories (Meyes = 271.14 vs. Mlips = 1,737.00). Thus, our result is a conservative estimate of the impact of AR introduction, and we speculate that the effect could have been larger if the retailer had advertised the feature. To establish a direct relationship between AR usage and purchase, and to further examine the moderating effects of customers' channel and category experience, we next turn our attention to the customer level. Customer-Level AnalysisWe focused on the sample of active customers (i.e., those who made a purchase in the past one year) who browsed products in the eyeshadow or eyeliner categories during the 12-month period[ 7] after the retailer introduced AR for these two categories (i.e., mid-March 2018 to mid-March 2019). In total, our sample included 42,493 customers. At the time of AR introduction, 40.2% of these customers had never purchased online before (i.e., new to the online channel) and 43.4% had never purchased eyeshadow or eyeliner before (i.e., new to the categories). During the 12-month period after the retailer introduced the AR feature for the two categories, 13.9% of customers used the feature to try eyeshadows and eyeliners, and 15.0% purchased at least one product from these categories using the app. Accordingly, we modeled how AR usage influenced customers' probability of purchasing products from these two categories in the focal period. Model specificationThe dependent variable of interest, P(Purchaseieyes), was customer i's probability of purchasing at least one eyeshadow or eyeliner on the app within 12 months of AR introduction for the two categories. As the dependent variable was binary, we used a probit model with the following specification: P(Purchaseieyes= 1|Xiβ)= Φ (β0+ β1AR Usageieyes                           + β2New Channeli+ β3New Categoryi                           + β4AR Usageieyes× New Channeli                           + β5AR Usageieyes× New Categoryi                           + Browsingi'γ + PastPurchasei'δ + ∊i). Graph3In Equation 3, Φ denotes the standard probit link function. AR Usageieyes represents the focal independent variable and takes a value of 1 if customer i used the AR feature to try eyeshadows or eyeliners during the period and 0 otherwise. New Channeli and New Categoryi are both indicator variables representing customers' (lack of) prior experience with the channel and category. New Channeli takes a value of 1 if customer i is new to the online channel and 0 otherwise, and New Categoryi takes a value of 1 if customer i is new to the two eye categories and 0 otherwise. To examine how these two variables moderate the effect of AR usage on purchase, we included interactions between the variables and AR Usageieyes. We also included a vector, Browsingi, to control for customers' browsing behavior before and during the focal period to account for customer interest and engagement. Because the browsing activity data set starts at December 2017 (i.e., three months prior to the introduction of AR for the eye categories), we used a three-month window for past browsing behavior. Lastly, we included a vector, Past Purchasei, to control for customers' purchase history in the 12 months prior to AR introduction for the eye categories to account for customer loyalty. Table 6 provides a summary of how the variables are operationalized and their descriptive statistics. The correlations are provided in Web Appendix H, and the variance inflation factors are below 1.75, indicating that multicollinearity is not an issue.GraphTable 6. Variable Operationalization and Descriptive Statistics for Customer Model. VariableOperationalizationMeanSDMinMedianMax  Purchaseeyes (1/0)1 if customer bought eye products in the focal period, 0 otherwise.15.36.00.001.00  AR Usageeyes (1/0)1 if AR was used to try eye products in the focal period, 0 otherwise.14.35.00.001.00  New Channel (1/0)1 if customer had never purchased from the retailer's online channel before the focal period, 0 otherwise.40.49.00.001.00  New Category (1/0)1 if customer had never purchased eye products from the retailer before the focal period, 0 otherwise.43.50.00.001.00Browsing controls  Past DurationTotal browsing duration in the past three months (in minutes)11.5625.60.00.00150.18  Past PageseyesNumber of eye product pages viewed in the past three months.772.90.00.0023.00  Past AR Usagelips (1/0)1 if customer had used AR for lip products in the past three months, 0 otherwise.02.15.00.001.00  DurationTotal browsing duration in the focal period (in minutes)153.17148.322.82106.571054.25  PageseyesNumber of eye product pages viewed in the focal period19.2124.981.009.00128.00Purchase controls  Past Order NumberNumber of transactions in the past one year6.925.831.005.0037.00  Past Order ValueAverage value of transactions in the past one year81.8850.49.0070.68547.00  Past Eye PurchasesNumber of eye products purchased in the past one year.891.50.00.0011.00  Recent OrderNumber of months from the most recent transaction2.642.49.032.0312.13  First OrderNumber of months from the first transaction18.927.73.0322.3726.80 6 Notes: ""Eye products"" refers to eyeshadow and eyeliner, the two categories of interest. The focal period is 12 months after the retailer introduced AR for eye products (i.e., March 15, 2018, to March 15, 2019). Identification strategyAs customers who already intend to purchase products may be more likely to try them using the AR feature, we used the two-stage residual inclusion method to account for this self-selection bias. We used customers' past AR usage for lip products (prior to AR introduction for the eye categories) as the instrument. Customers who had used AR to try lip products in the past were already aware of the feature and could have been more likely to use it again to try eye products. Conversely, customers who had never used the feature to try lip products in the past may have been unaware of it. Because the retailer did not announce the AR introduction for the eye categories, these customers could still have been unaware of the feature. As a result, they would have been less likely to use it to try eye products. Furthermore, because lip and eye products target different areas of the face, past usage of the AR feature to try lip products should not have directly affected the probability of purchasing eye products during the focal period. Thus, we included Past AR Usageilips as an instrument in the first stage to estimate customer i's likelihood of using AR in the focal period. The variable was coded as 1 if customer i used the AR feature to try lip products in the three months before AR was introduced for the eye categories and 0 otherwise. Residuals from the first-stage estimation were then included in Equation 3 to estimate P(Purchaseieyes). Similar to the product model, we bootstrapped 1,000 samples to obtain the proper standard errors. To examine if the findings are robust to alternative identification strategies, we also adopted the propensity score weighting approach, which does not rely on instruments. We discuss this further in the robustness analyses section. ResultsThe coefficient of the instrument in the first stage estimation (provided in Web Appendix I) is positive and significant (.176, p <.01), and the F-statistic of excluded instrument in the first stage regression is 15.8, providing evidence for the strength of the instrument. However, the coefficient for the residual correction term, which is equivalent to the Hausman test for the presence of endogeneity ([40]), is not significant, suggesting that endogeneity may not be a concern. We also used the Heckman selection method ([25]) as an alternative identification strategy, and the inverse Mills ratio is similarly not significant. Therefore, we report estimates for the uncorrected model in the results section and provide the full result for both the two-stage residual inclusion and Heckman selection methods in Web Appendix I. We note that across all models, the substantive findings of interest remain consistent.Table 7, Column 1, displays the results for the model without interactions, representing factors influencing the purchase of eyeshadows or eyeliners during the 12 months after AR introduction for these categories. The coefficient of AR Usageeyes is positive and significant (.046, p <.05), providing further evidence for H1. The coefficients of other variables are largely in line with expectations. For example, the coefficient for New Channel (−.329, p <.01) and New Category (−.120, p <.01) are significantly negative, indicating that customers who are new to the online channel or product category are less likely to make a purchase. The number of orders (.007, p <.01), average order value (.002, p <.01), and number of eye products purchased in the past (.080, p <.01) are positively related to probability of purchasing eye products. Furthermore, total browsing duration (.000, p <.01) and number of eye product pages viewed (.007, p <.01) are also positively related to the purchase of eye products.GraphTable 7. Customer Model: Impact of AR Usage on Probability of Purchase and Moderating Effects of Customer Characteristics. Column 112-Month Model (Without Interactions)Column 212-Month Model(Full Model)Column 36-Month Model(Full Model)Column 43-MonthModel(Full Model)AR Usageeyes (H1).046 (.022) **−.015 (.030).004 (.047)−.036 (.084)New Channel−.329 (.018) ***−.344 (.019) ***−.323 (.029) ***−.296 (.040) ***New Category−.121 (.020) ***−.134 (.021) ***−.060 (.032) *−.122 (.042) ***AR Usageeyes × New Channel (H3a)–.091 (.046) **.144 (.070) **.091 (.138)AR Usageeyes × New Category (H3b)–.082 (.045) *.075 (.068)−.004 (.135)Past Duration−.002 (.000) ***−.002 (.000) ***−.002 (.000) ***−.002 (.000) ***Past Pageseyes−.003 (.003)−.003 (.003).000 (.002)−.004 (.002) **Duration.000 (.000) ***.000 (.000) ***.001 (.000) ***.001 (.000) ***Pageseyes.007 (.000) ***.007 (.000) ***.008 (.000) ***.017 (.001) ***Past Order Number.007 (.002) ***.007 (.002) ***.005 (.003).003 (.004)Past Order Value.002 (.000) ***.002 (.000) ***.002 (.000) ***.002 (.000) ***Past Eye Purchases.080 (.006) ***.081 (.006) ***.103 (.012) ***.062 (.011) ***Recent Order−.019 (.004) ***−.020 (.004) ***−.020 (.006) ***−.016 (.008) *First Order−.001 (.001)−.001 (.001)−.002 (.002).005 (.002) *Constant−1.285 (.035) ***−1.276 (.035) ***−1.567 (.054) ***−1.771 (.077) ***Observations42,49342,49324,14713,434Log likelihood−16,614−16,609−7,423−3,773 7 *p ≤.10; **p ≤.05; ***p ≤.018 Note: Standard errors are in parentheses.Table 7, Column 2, provides results for the 12-month model, including interactions. The interaction between AR Usageeyes and New Channel is positive and significant (.091, p <.05), suggesting that AR has a stronger effect among customers who had never purchased online in the past. The average marginal effect of AR usage for customers who are new to the online channel is significantly positive (.018, p <.01), but this effect is not significant for existing online customers (.004, p =.59). Thus, H3a is supported. While the interaction between AR Usageeyes and New Category is marginally significant (.082, p <.10), the average marginal effect of AR usage is significantly positive for customers who are new to the product category (.019, p <.01) and not significant for existing category customers (.003, p =.65), providing support for H3b as well.To understand how the impact of AR changes over time, we repeated the same analysis using a six-month and three-month window, presented in Columns 3 and 4 of Table 7. We find that the interactions between AR Usageeyes and both New Channel and New Category become stronger over time. Although both interactions (as well as the average marginal effects) are insignificant in the 3-month period, the interaction with New Channel becomes significant in the 6- and 12-month period, and the interaction with New Category becomes marginally significant in the 12-month period. Similar to the 12-month model, the average marginal effects in the 6-month model are significantly positive for customers who are new to the online channel (.022, p <.01 vs..006, p =.44 for existing online customers) and product category (.019, p <.05 vs..007, p =.31 for existing category customers). These results suggest that customers may require some time to become comfortable with the technology before using it to make purchase decisions. In addition, the results also imply that the impact of AR does not wear out over time, which rules out novelty effects as an alternative explanation. Robustness analysesResults for the two-stage residual inclusion and Heckman selection methods for all 3-, 6-, and 12-month periods are provided in Web Appendix I. To further examine if the findings are robust to alternative identification strategies, we applied the propensity score weighting approach. We used the first stage equation to calculate customers' propensity for using AR in the focal period and include this as weights in the estimation of Equation 3, following [ 6]. The results are consistent with the main model and are also reported in Web Appendix I.We also examined if the findings are robust to alternative variable operationalizations. First, instead of the probability of purchasing eye products, we used the number of eye products purchased during the focal period as an alternative dependent variable. Second, we replaced the binary AR Usage variable with the number of sessions involving AR usage during the focal period. Third, as alternative measures of channel and category experience, we used the number of online transactions and number of eye products purchased prior to AR introduction for the eye categories, respectively. Findings from these models are consistent, and the results are presented in Web Appendix J. DiscussionAlthough firms are keen to invest in AR, research demonstrating its impact in real-world contexts is limited. The present research provides some preliminary confirmation that both the availability and usage of AR have a small but positive impact on sales. Taken together, our findings provide converging evidence that AR is most effective when product-related uncertainty is high, indicating that uncertainty reduction could be a possible mechanism through which AR could improve sales. Nevertheless, we do not find a significant moderating effect for product ratings, suggesting that even though AR may reduce product fit uncertainty, it may still be unable to compensate for the higher performance uncertainty associated with products that have lower ratings.[ 8] Although we have adopted instrumental variable and quasi-experimental approaches to address endogeneity that is inherent in observational data, we acknowledge that these findings should be viewed as evidence based on correlations, with attempts to come close to causality. Research Implications Augmented reality and product preferenceComplementing past research that has explored how website features drive sales for niche products (e.g., [ 8]; [50]), we show that AR can increase preference for products or brands that are less popular. Thus, retailers carrying wide product assortments can use AR to stimulate demand for products in the long tail of the sales distribution. AR may also help level the playing field for less popular brands. With the launch of AR-enabled display ads on advertising platforms such as Facebook and YouTube, less-established brands could consider investing in this new ad format, as they stand to benefit most from this technology. Retailers selling premium products may also leverage AR to improve decision comfort and reduce customers' hesitation in the purchase process. Augmented reality and category salesWe find that the impact of AR is stronger for customers who are new to the product category, suggesting that AR could increase sales via category expansion. However, because AR seems to be most effective when the level of uncertainty is high, its impact may diminish over time as customers become more familiar with the product category and experience less uncertainty.[ 9] Nevertheless, the finding that AR has a stronger impact for products that are more expensive suggests that, beyond increasing unit sales, AR can also improve category revenues by encouraging customers to purchase products with wider margins. Thus, investments in deploying AR in retail could pay off in the long run. Augmented reality and channel choiceCompared with customers who are already familiar with purchasing online, we find that AR has a stronger effect for customers who are new to the online channel. As prior research has shown that multichannel customers are more profitable ([39]), omnichannel retailers can use AR to encourage their offline customers to adopt the online channel. Given that AR increases online sales among customers who are new to the channel, a potential concern is that AR could lead to cannibalization of sales from offline channels. To understand if the increase in app purchases that we observed was happening at the expense of other sales channels, we ran the same model in Equation 3 but replaced the dependent variable with the probability of purchasing eye products in the web and offline channels (results reported in Web Appendix K). We did not find evidence to indicate that offline customers who use AR (on the app) are more likely to purchase from the web, suggesting that the impact of AR is specific to the app platform. Interestingly, we find that offline customers who use AR are more likely to purchase from the offline channel in the three-month model but not in the six and 12-month model. Thus, contrary to our expectations, the results suggest that AR could have a positive spillover effect to the offline channel, at least in the short run. An Agenda for Future ResearchComplementing prior research, which has predominantly studied AR from a consumer perspective, our research extends the literature by examining what AR means for retailers. To encourage the academic community to produce more impactful research in this nascent field, we developed a research agenda for AR in marketing, with an emphasis on identifying research topics that have strong managerial relevance for industry practitioners. Drawing on a review of the academic literature (e.g., [52]) and recent advancements in AR technology, we generated a list of potential research topics and synthesized these topics into five themes. Next, we consulted two senior marketing practitioners and two academics with expertise in this area to review the research themes and associated topics, and we refined the list according to their feedback.To determine the practical importance of each research theme, we conducted an online survey with 36 marketing practitioners from companies that were using (or planning to use) AR in their marketing, advertising, or retailing activities. Survey respondents first independently rated each research theme in terms of importance to business performance (see [47]) before ranking the five research themes from most to least important. To avoid primacy and recency effects, the order of research themes was randomized across respondents. The mean rating (ranging from 5.1 to 5.8 on a seven-point scale) and ranking scores (from 1 to 5; lower number reflects higher importance) are inversely proportional, demonstrating internal consistency. Web Appendix L provides details for the survey, including survey design, respondent recruitment, and background of respondents.Table 8 presents the research agenda for AR in marketing, comprising the five research themes (ordered by practical importance) and potential topics that could be explored under each theme. Given the novelty of the technology, marketers were primarily concerned with how different design features could be configured to create more effective AR experiences for consumers. For example, greater clarity is needed regarding factors that affect AR experiences, such as fidelity (i.e., how closely virtual objects resemble real objects), motion (i.e., static vs. animated virtual objects), spatial presence (i.e., the feeling that virtual objects exist in a physical space), and embodiment (i.e., the ability to use bodily movements to control virtual objects), and how these can be delivered on AR interfaces. Beyond visual and auditory senses, how haptic feedback (e.g., emission of vibrations on devices to stimulate the sense of touch) influences AR experiences is also of interest.GraphTable 8. Research Agenda for AR in Marketing. Research ThemesPotential Research TopicsDesigning effective AR experiencesRating: 5.81Ranking: 2.14How factors such as fidelity (i.e., how closely virtual objects resemble real objects), motion (i.e., static vs. animated virtual objects), spatial presence (i.e., the feeling that virtual objects exist in a physical space), and embodiment (i.e., the ability to use bodily movements to control virtual objects) affect AR experiences.How the incorporation of senses such as haptic feedback (e.g., emission of vibrations) influences AR experiences.How content and other elements in the virtual environment could be personalized to enhance AR experiences and influence behavior.AR and marketing strategyRating: 5.50Ranking: 2.78How marketers could use AR more effectively at different stages of the customer journey to increase brand engagement and improve relationships with customers.Synergy between AR and other elements in the marketing communications mix (e.g., advertising, sales promotions).Effectiveness of product placements and pop-up stores in AR-enabled virtual environments, as well as their potential to complement or replace physical stores.How AR could be deployed in service industries (e.g., tourism and hospitality, food and beverage retail).AR and consumer behaviorRating: 5.22Ranking: 3.06How AR experiences affect sensory perceptions and cognitive functions (e.g., attention, information processing, learning, and memory).How AR experiences affect rational decision making (e.g., product selection strategies, relative importance of attributes) and irrational tendencies (e.g., psychological ownership).The role of AR experiences in attitude formation and brand perceptions.How AR experiences affect purchase behaviors and postpurchase product evaluations.Promoting AR adoptionRating: 5.22Ranking: 3.42Exploring barriers to consumers' use of AR technologies (e.g., awkwardness of using it in public, privacy and security concerns, lack of realism in virtual environments) and how marketers can overcome these barriers to encourage wider adoption.How delivery of the AR experience and advancements in high-tech devices (e.g., 3D depth camera technology, wearable AR glasses) influence consumers' acceptance and usage of the technology.How offline contextual factors (e.g., distance to physical stores, private vs. public space) affect AR usage.AR as a marketing intelligence toolRating: 5.11Ranking: 3.61How AR experiences could be used to generate insights for new product development, assortment planning, and store layout/design.Identifying new behavioral data (e.g., motion, interactions within virtual environments) that could be obtained from AR platforms, as well as how such data could be used to measure/predict consumers' responses or decision-making processes.Privacy and security concerns regarding behavioral data collected on AR platforms and what marketers can do to reduce these concerns. 9 Notes: Research themes are ordered by importance on the basis of surveys with 36 marketing practitioners. Details of the survey are provided in Web Appendix L. ""Rating"" refers to the mean importance rating score (on a seven-point scale). ""Ranking"" refers to the mean importance ranking (from 1 to 5; lower number reflects higher importance).Another important consideration is how AR fits into companies' overall marketing strategy. Specifically, marketers would like to know how they can better integrate AR at different stages of the customer journey to increase brand engagement, build emotional connections, and improve relationships with customers. There is also ambiguity regarding the synergy between AR and other elements of the marketing communications mix (e.g., advertising, sales promotions), as well as the effectiveness of product placements and pop-up stores in AR-enabled virtual environments. In particular, the potential for this new technology to complement or replace existing communication and retail channels is still uncertain. As most recent applications of AR have focused on consumer products, marketers also need more guidance on how AR can be appropriately deployed in service industries, such as the tourism and hospitality sector.Besides these two key areas, other worthwhile avenues to explore include the impact of AR on consumer behavior (e.g., cognitive functions, rational decision making, and brand perceptions), how marketers can promote wider adoption of AR, and how the technology can be used to generate valuable marketing insights. Although our research agenda focuses on AR, we note that the research themes could be broadened to encompass other extended realities (i.e., virtual reality and mixed reality).In conclusion, we believe that the marketing community would benefit from a deeper investigation of virtual experiences and their role in marketing. We are excited about where this field is heading, and we look forward to more insightful research to reinforce our understanding of the profound impacts of these new technologies in the marketing domain. "
6,"Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and Choices Consumers have grown increasingly skeptical of brands, leaving managers in a dire search for novel ways to connect. The authors suggest that focusing on one's relationships with competitors is a valuable, albeit unexpected, way for brands to do so. More specifically, the present research demonstrates that praising one's competitor—via ""brand-to-brand praise""—often heightens preference for the praiser more so than other common forms of communication, such as self-promotion or benevolent information. This is because brand-to-brand praise increases perceptions of brand warmth, which leads to enhanced brand evaluations and choice. The authors support this theory with seven studies conducted in the lab, online, and in the field that feature multiple managerially relevant outcomes, including brand attitudes, social media and advertising engagement, brand choice, and purchase behavior, in a variety of product and service contexts. The authors also identify key boundary conditions and rule out alternative explanations, further elucidating the underlying mechanism and important implementation insights. This work contributes to the understanding of brand perception and warmth, providing a novel way for brands to connect to consumers by connecting with each other.Keywords: praise; brand relationships; brand communication; brand evaluations; warmth and competenceIn 2017, a popular video gaming brand, Xbox, openly congratulated its competitor, Nintendo, on the launch of its new Switch gaming system ([53]). A few months later, The New York Times encouraged readers to read other news sources such as The Wall Street Journal ([20]). And, in responding to a playful challenge from Kit Kat, Oreo disarmed the brand by communicating how truly irresistible Kit Kat is ([62]). Conventional wisdom fervently advises that ""even mentioning your competition is a bad idea"" ([51]), so why have these brands not only mentioned their competitors but praised them?In a world where brands are trying hard to connect with consumers, many of whom have grown increasingly skeptical of marketers' intentions ([21]), it may be that praising the competition provides unexpected benefits. Our research explores the consequences of brand-to-brand praise—when a brand communicates positively and publicly about another brand. We argue that consumers who observe a brand praising a competitor will believe that the brand has positive intentions toward others, also known as brand warmth, which heightens consumer evaluations and interest in the brand giving the praise.As an introductory illustration of this idea, we scraped data from the Twitter pages of Nintendo and its fiercest competitors, Xbox and PlayStation, around the time of the Nintendo Switch launch in 2017. We found a greater number of likes and retweets (in fact, over ten times as many), as well as more positive sentiment among consumers' comments, when Xbox and PlayStation praised Nintendo for the launch compared with all other types of messages (Web Appendix A). Such preliminary field data motivate our exploration into whether, when, and why brand-to-brand praise affects consumer reactions to brands.Across seven studies, we examine the effects of brand-to-brand praise on consumer attitudes and behavior versus more common forms of brand messaging (e.g., self-promotion or providing helpful information to consumers) and identify important boundaries. In doing so, this research offers several contributions.First, we contribute to the brand perception literature by demonstrating how consumers' perceptions of brands are affected by a brand's interactions with other brands. Prior work has focused on how brand-to-consumer interactions affect consumer perceptions (e.g., [42]) but has not yet explored how observing brand-to-brand interactions do so. Furthermore, we contribute to research on the fundamental dimensions by which people judge other people and brands: warmth and competence (e.g., [ 1]). We identify brand-to-brand praise as a novel antecedent that often leads consumers to perceive brands that praise their competitors as warmer. We show that praising a competitor is viewed as a costly action that does not obviously benefit the praiser, thus making it a credible signal. In doing so, we demonstrate the role of costliness in signaling warmth that effectively combats consumer skepticism, a major barrier to warmth identified in prior research ([18]). We also introduce two moderators—organization type and individual differences in skepticism—to identify when costly displays of warmth are most important.Second, we further contribute to the literature on warmth and competence by introducing a context in which brand-to-brand praise increases warmth without damaging perceptions of competence. Prior work suggests that warmth and competence are often negatively correlated, particularly in contexts in which people are considering two or more entities ([32]). Leveraging consumers' lay theories about the characteristics of brands that would be willing to praise their competitors, brand-to-brand praise provides an opportunity for brands to communicate warmth while maintaining perceptions of competence.Third, while prior work has examined brand communication in which the competitive brand is ultimately shown to be inferior, such as in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]), our research identifies how directing attention away from one's own brand and toward the competition in a purely positive light affects brand evaluations and choice.Finally, we add to the current literature on praise by identifying brand-to-brand communication as a viable and distinct form of praise, noting that observers respond more favorably to praise in a brand-to-brand context than is typically observed in traditional person-to-person contexts. In doing so, we also highlight the understudied effects of praise in competitive relationships. In what follows, we review literature on brand relationships, praise, and brand warmth and present seven studies that test our hypotheses. Building Consumer–Brand RelationshipsA great deal of research has investigated how brands establish and build relationships with consumers, identifying influential factors such as the personalities, motives, and communication styles of both consumers and brands (for a review, see MacInnis, Park, and Priester [2009]). Surprisingly, researchers interested in brand relationships have not yet explored how brands' public communication with other brands affects their relationships with consumers. Of course, researchers have explored how strategic partnerships, such as brand collaborations and alliances, affect brand outcomes (e.g., [38]; [57]). However, these brand interactions occur largely behind the scenes and reflect formal business arrangements as opposed to informal, public communication that makes consumers privy to how a brand treats its competitors. We suggest that consumers will infer important information about a brand based on the way it communicates with other brands. Specifically, we posit that consumers who observe a brand praising its competition will perceive the praiser brand as having positive intentions toward others, known as brand warmth. Subsequently, consumers will develop more positive evaluations of and interest in the praiser brand. Brand-to-Brand Praise and Warmth PerceptionsPrior research has established that people judge others—individuals, groups, cultures, countries—using two fundamental dimensions, often referred to as warmth and competence ([25]; [32]).[ 5] Warmth is the degree to which one has positive intentions toward others and includes perceptions of thoughtfulness, kindness, honesty, and trustworthiness ([ 1]; [25]). Evolutionarily speaking, it allowed our ancestors to quickly distinguish friend from foe and prepare to fight or flee. Warmth judgments are therefore formed more quickly and generally have the greatest impact on attitudes toward individuals ([64]). Conversely, competence reflects the degree to which one is able to enact one's intentions.Given that people relate to brands similarly to how they relate to people in many ways ([26]), warmth and competence are important traits for firms to consider ([33]). Surprisingly, scant literature has explored the drivers of warmth and competence for brands ([ 1]). The research that does exist identifies factors such as a firm's profit focus ([ 2]), social responsibility ([ 8]), racial dynamics ([ 6]), and expression/communication style ([61]; [65]) as affecting consumers' perceptions of warmth and competence. Significantly more work has explored the consequences of warmth and competence. As examples, research has shown that warmth and competence perceptions affect consumer emotions ([ 1]), product evaluations and interest ([10]; [14]; [34]; [36]; [39]; [58]), and word of mouth ([ 7]; [54]). Importantly, while the precise contribution of warmth versus competence to different downstream consequences varies (e.g., [37]), brands generally aspire to be strong on both dimensions and occupy the coveted ""golden quadrant"" ([ 1]).In this research, we suggest that brand-to-brand praise affects consumers' reactions primarily through perceptions of warmth. Prior research has theorized that warmth is established through signals of cooperation (vs. competition) and actions that appear to serve others as opposed to the self (i.e., actions that are ""other-profitable""; [16]; [52]). We suggest that offering praise to a competitor provides a strong illustration of such cooperative, ""other-profitable"" activity. Consumers will therefore perceive a brand that praises competitive brands as warmer (relative to a brand that engages in other types of common brand messaging). This then leads to positive downstream consequences, such as increased purchases. Costliness as an Antidote to SkepticismIn hypothesizing the positive effect of brand-to-brand praise on warmth, it is important to note that high warmth is more difficult to establish and maintain than high competence, as people are often skeptical of others' motives. That is, warm behavior is often discounted, considered to be driven by ulterior motives and easier to fake than competence ([18]; [56]). Such skepticism has also been identified as an issue in research on praise more specifically. For example, in many brand-to-consumer exchanges, such as salesclerk-to-consumer interactions, praise generates suspicion from the consumer; consumers often suspect ulterior motives behind a salesclerk's compliment and perceive the salesclerk to be insincere when the praise occurs before a purchase ([11]; [44]). Further, observers who witness praise happening among others tend to be more skeptical of the praiser and do not react as positively as recipients of the praise ([13]; [29]; [60]). This then leads to the question: When and why might brand-to-brand praise surmount the skepticism associated with praise and other displays of warmth?We suggest that brand-to-brand praise operates uniquely from the aforementioned person-to-person and brand-to-consumer displays of warmth, particularly when the praise is directed toward competitors, due to its costliness. Consumers assume that complimenting a competitor is a costly action that does not directly benefit the complimenting brand. Research across disciplines suggests that the costliness of an act is the key component of whether the act is perceived as a meaningful signal of an underlying trait rather than an uninformative act motivated by devious or ulterior motives, also known as ""cheap talk"" (e.g., [59]; [67]). A common example from the natural world is the male peacock's tail. The costliness of this large and colorful tail, such as how it can handicap the bird's ability to escape from predators, is what makes it a credible signal of fitness to potential mates. Only those who truly possess the focal underlying trait would or could incur such costs. Linking this principle to the present context, only brands that are truly warm would incur the real or potential costs of praising the competition. Consumers should therefore not be as suspicious of brands that praise competitors as compared with brands that engage in less costly messaging. This costliness is why brand-to-brand praise, when directed toward competitors, is a strong signal of warmth, differentiating it from other common types of praise (e.g., salesclerk-to-consumer), surmounting consumer skepticism, and driving its positive influence on consumer attitudes and reactions. Effect on Competence PerceptionsAlthough the focus thus far has been on perceptions of warmth, one might wonder whether brands sacrifice competence when enhancing perceptions of warmth via brand-to-brand praise. This is a reasonable concern, as prior research suggests that in comparative contexts, for people and brands alike, when warmth (competence) is relatively high for a given entity, perceptions of that entity's competence (warmth) suffer (e.g., [ 2]; [28]; [36]; [54]). We suggest that praising a competitor offers brands unique advantages that allow them to maintain perceptions of competence in the face of increasing warmth. In particular, we suggest that consumers hold a lay intuition that a brand that is willing to praise its competitors must be fairly confident in its own abilities, which then allows consumers to maintain, if not increase, positive perceptions of its competence. This is reminiscent of the lay belief—confirmed by academic research—that people who are secure in their identity are the most willing to be kind and compliment others (e.g., [ 9]; [22]; [41]; [63]). Still, while competence may play a role in driving the effects of brand-to-brand praise, we do not expect it to be the primary driver of increased interest in the praiser brand, as perceptions of competence should be most strongly driven by cues of status versus the cues of cooperation that are focal in brand-to-brand praise ([24]). Summary of the Current ResearchIn summary, we argue that brand-to-brand praise often promotes positive brand evaluations and choice of the praiser. Specifically, we predict that consumers will evaluate a brand more favorably and show more interest in the brand when observing brand-to-brand praise compared with observing traditional self-focused messages or even other benevolent messages (H1). We theorize that this is because such praise increases perceptions of the praiser brand's warmth (H2). Moderators/Boundary Conditions CostlinessWe expect that this effect exists when the praise is costly (H3), such as when the brand is praising a competitor, so as to provide a meaningful signal of warmth to the consumer. We reason that the costly signal of warmth indicates to the consumer that the brand truly has positive intentions toward others, even when it is not in the best interest of the brand. As prior work suggests, such warmth leads consumers to be more interested in identifying with, using, and sharing the brand ([ 7]; [37]). For-profit versus nonprofit organizationsAs additional evidence for the role of warmth, we expect the effect of brand-to-brand praise to be stronger for brands that are typically associated with lower levels of warmth than those already endowed with high levels of warmth; brands with high levels of warmth, such as nonprofits, have little to gain from increasing warmth even further (and thus less to gain from brand-to-brand praise). This suggests that for-profit brands (which are lower in warmth than nonprofit brands; [ 2]) will benefit more from brand-to-brand praise (H4). Chronic consumer skepticismFinally, we posit that brand-to-brand praise allows brands to manage consumer skepticism often associated with displays of warmth. We introduce an important moderator to illustrate this point: individual differences in consumers' skepticism toward brand messaging. We predict that the effect of brand-to-brand communication will be strongest among individuals who are highly skeptical of brands. This is because the costliness associated with praising a competitor minimizes the extent to which persuasion knowledge concerns are activated among this group of consumers as compared with more traditional brand communication (H5). We summarize these predictions in the conceptual model in Figure 1 and discuss the boundary predictions further in the study introductions relevant to each.Graph: Figure 1. Conceptual model. Overview of StudiesWe first demonstrate the positive effects of brand-to-brand praise (vs. more traditional messages) across three field and lab studies involving real, consequential behaviors (Studies 1a, 1b, and 2; H1). Next, we show that this effect is specific to praise that is aimed toward a brand's competitor and thus deemed costly (Study 3; H3). We then explore warmth as the key mechanism, demonstrating that it mediates the effect of brand-to-brand praise (Study 4; H2) and that the effect is strongest for for-profit brands that have greater need to enhance perceptions of warmth (Study 5; H4). Finally, we demonstrate that brand-to-brand praise has the largest effect on individuals with high levels of skepticism toward traditional brand communication (Study 6; H5). Throughout these studies, we also test for alternative explanations, including that praise confers benefits due to novelty or authenticity, or that the effect is driven by negative reactions to self-promotion (i.e., bragging) rather than positive reactions to praise. Finally, we also examine the role of competence in several studies (Studies 4, 5, and 6). Evidence suggests that, while perceptions of brand warmth are the primary mechanism underlying this effect, brand-to-brand praise does not harm and can even boost perceptions of competence, which can influence desired outcomes.Together, these studies provide insight into when and why brand-to-brand praise is beneficial for brands. Notably, however, we demonstrate (in Studies 3, 5, and 6) and explain (in the ""General Discussion"" section) when and among whom brand-to-brand praise might not be as beneficial or might even be less beneficial. Study 1: The Effect of Brand-to-Brand PraiseStudies 1a and 1b provide initial causal evidence for our hypothesis by testing the effects of various types of brand messaging on advertisement click-through rate and brand choice in a field and lab study, respectively. We compare the effects of brand-to-brand praise with a traditional self-promotion message from the brand. In addition, in Study 1a, we utilize another type of control message in the form of an endorsement from another organization in the industry to show that praising a competitor enhances evaluations more than receiving an endorsement from others. Further, the endorsement condition serves as a separate point of comparison, ensuring that the hypothesized difference between the brand-to-brand praise condition and the self-promotion condition can be interpreted as a boost from brand-to-brand praise and not merely a negative reaction to self-promotion. Study 1a: Facebook Advertisement Click-Through Rate MethodParticipants and design. This preregistered study utilized a three-cell between-subjects design: Facebook users saw an advertisement on Facebook featuring self-promotion, an external endorsement, or brand-to-brand praise.[ 6]Procedure. We created a fictitious car wash brand called Precision Car Wash and launched three advertisements for the brand on Facebook. The self-promotion message stated, ""Precision Car Wash is proud to receive the Industry Best 2020 Award."" The external endorsement consisted of a message from a fictitious organization, The Industry Best 2020 Award Committee, announcing Precision Car Wash as the year's award recipient. Finally, the brand-to-brand praise ad consisted of a message from Precision Car Wash congratulating another fictitious car wash business, LikeNew Car Wash, on winning the Industry Best 2020 Award (for study stimuli, see Web Appendix B). Facebook users who saw the ad could click on the message to be taken to the Facebook page of Precision Car Wash for more information. We measured the number of impressions and clicks, which allowed us to compare click-through rates (clicks as a percent of impressions; CTRs), for each ad. We ran the ads over the course of five days and used Facebook's recommended advertising algorithm to reach 80% power in testing the ads for a final sample of 13,719 impressions.We also conducted two separate supplemental tests of these stimuli. First, we conducted a manipulation check for the ads, assigning participants to one of the three ad conditions and asking them to indicate the extent to which the brand was praising its competition (1 = ""definitely NOT praising the competition,"" and 7 = ""definitely praising the competition""; N = 105). Second, given variations in the text used to communicate the focal messages (e.g., number of words, fonts), we conducted a posttest to assess the ""graphic design"" (three items: quality, clarity, graphic appeal) of the assigned ad (1 = ""not done very well,"" and 7 = ""done very well""; α = .87; N = 105). These items allowed us to be sure that our focal praise condition was not (unintentionally) more aesthetically appealing and more likely to be clicked for that reason. ResultsManipulation check and aesthetic appeal. The separate manipulation check showed a significant difference across conditions (F( 2, 101) = 21.47, p < .001,  ηp2   = .30). Those who viewed the brand-to-brand praise message perceived it to praise the competition (M = 4.82) more than the self-promotion (M = 1.96; p < .001) and external endorsement (M = 2.43; p < .001) ads. The self-promotion and external endorsement conditions did not differ (p = .30). This manipulation check was conducted for and confirmed the manipulations in each of the remaining studies (see Web Appendix C).In addition, the separate test designed to assess the aesthetic appeal of the different messages indicated that the focal praise ad was not advantaged aesthetically. We find a significant difference across conditions (F( 2, 102) = 5.50, p = .005,  ηp2   = .10) such that those who viewed the brand-to-brand praise ad perceived it to be lower in aesthetic appeal (M = 3.83) than the self-promotion ad (M = 5.03; p = .001) and not statistically different from the external endorsement ad (M = 4.42; p = .11). The self-promotion and external endorsement conditions did not significantly differ (p = .10). However, because the praise ad was rated to be lower in aesthetic appeal, any positive benefits of the praise ad should not be a result of viewers liking the appearance of the praise ad more.CTR. A chi-squared analysis revealed that the CTR differed across conditions (χ2( 2, N = 13,719) = 91.59, p < .001). The percentage of those who clicked on the ad was greater for the brand-to-brand praise condition (5.4% of 4,392 impressions) compared with the self-promotion (3.3% of 4,075 impressions; χ2( 1, N = 8,467) = 21.42, p < .001) and external endorsement (1.8% of 5,252 impressions; χ2( 1, N = 9,644) = 90.45, p < .001) conditions. Study 1b: Lab Brand Choice MethodParticipants and design. One hundred fifty-four members of the local community (general public, students, and staff) were recruited through a business school's behavioral lab. Participants were randomly assigned to one of two conditions: self-promotion or praise.Procedure. First, participants learned that they would give their opinions about snack brands that the business school was considering for its café and vending machines. Next, they were informed that they would be able to choose a sample snack to take home after the study.Participants then saw two advertisements from real local popcorn shops in the Raleigh-Durham, North Carolina, area. The first ad was from the recipient brand, Carolina Popcorn Shoppe, and stated, ""Come check out our FIVE newest flavors! In-store or online."" Everyone saw this ad. The second ad was from the focal brand of the study, The Mad Popper, and differed by condition. The praise condition ad read, ""We love good popcorn. Big shout-out to Carolina Popcorn Shoppe on their FIVE new flavors!"" The self-promotion ad read, ""We love good popcorn. Come explore our FIVE brand new flavors! In-store or online"" (Web Appendix D).Next, participants were reminded that both brands were being considered for use at the business school. They were then asked to choose Carolina Popcorn Shoppe or The Mad Popper as the brand they would prefer to sample, which they received at the conclusion of the study.[ 7] Finally, participants responded to a stimuli believability measure, assessing their willingness to suspend disbelief and assume that the experimental stimuli were real (""How believable was this advertisement by The Mad Popper?""; 1 = ""not at all,"" and 7 = ""very""). We asked this believability question consistently across experiments (except Study 1a, where we could not) because of the low prominence of brand-to-brand praise currently in the market. Although brand-to-brand praise is not yet commonly practiced in the real world and thus may not yet be highly believable (i.e., seem real) for some consumers, we are examining what could happen if consumers witnessed brand-to-brand praise, meaning controlling for variation in whether the stimuli were believed to be real.[ 8] We use believability as a covariate across each subsequent experiment to enhance experimental power ([45]). Details on this measure appear in Web Appendix E, which includes a summary of key results with and without the covariate.[ 9] ResultsBrand choice. We conducted a binary logistic regression with condition (1 = praise, 0 = self-promotion) as the key predictor and believability as a continuous covariate on brand choice (1 = The Mad Popper, 0 = Carolina Popcorn Shoppe). Conceptually replicating the results of Study 1a, those in the praise condition were significantly more likely to choose the focal brand, The Mad Popper, compared with those in the self-promotion condition (β = .91, SE = .43, Wald χ2( 1, N = 154) = 4.47, p = .03). In percentages, 27.63% chose the focal brand in the self-promotion condition, and 34.62% did so in the praise condition.[10] We also find that believability does not act as a moderator (β = −.02, SE = .25, Wald χ2( 1, N = 154) = −.08, p = .93); thus, we use it as a covariate in our remaining studies (for moderation by believability results for the remaining studies, see Web Appendix E). DiscussionStudies 1a and 1b provide initial experimental evidence that brand-to-brand praise can have positive consequences for the praiser on real behavior, including advertisement CTR and brand choice. The comparison of brand-to-brand praise to an external endorsement in Study 1a suggests that the key effect is not because consumers dislike the other control condition—the self-promotion message—but is instead driven by a boost from observing brand-to-brand praise. Furthermore, by presenting participants with a forced choice between two brands in Study 1b, this data begins to suggest that praise benefits the praiser more than the praised. We further explore this in Study 2. Study 2: How Are Real Purchases Affected for the Praising and Praised Brands?In Study 2, we build on the findings of Studies 1a and 1b in assessing consumers' real, behavioral reactions to brand-to-brand praise, but we do so with some important changes. First, we assess a longer-term reaction to brand-to-brand praise by investigating purchase behavior for popular national brands (Kit Kat and Twix) 11 days after people were exposed to the brand messaging. Second, unlike Study 1b (but similar to Study 1a), we only expose participants to the competitor brand in the brand-to-brand praise condition. This enables us to further assess the potential costs of making competitive brands more top-of-mind (via praise) than they might otherwise be. This study also allows us to assess behavioral reactions to both the focal and competitive brands by gauging consumers' purchase behavior toward both, as opposed to forcing a choice between them (Study 1b) or tracking reactions to only the focal brand (Study 1a). Finally, this design offers an opportunity to see the proposed effect of praise on behaviors of greater financial consequence for the consumer and the brand: purchases. Methods Participants and designThis preregistered study had a two-cell between-subjects design (control, praise) and was conducted in two stages on Prolific.[11] First-stage procedureIn the first stage of this study, participants recruited from Prolific (N = 1,502; 49.6% female) viewed an image of Kit Kat's Twitter page. In the control condition, participants read a tweet that said, ""Start your day off with a tasty treat!"" In the praise condition, participants read a tweet that said, ""@twix, Competitor or not, congrats on your 54 years in business! Even we can admit—Twix are delicious"" (Web Appendix F). Unlike Study 1b (but similar to Study 1a), participants did not see a separate introduction to the nonfocal brand, Twix. Thus, Twix would presumably not be as top-of-mind unless they read the praise tweet. Afterward, we measured participants' attitudes toward both Kit Kat and Twix using the attitude measure from [46] (""negative/positive,"" ""dislikeable/likeable,"" and ""unfavorable/favorable"" on seven-point scales; α = .95; for details on this measure, see Web Appendix F). Second-stage procedureFrom the initial sample of 1,502 participants, we excluded those who indicated that they have not bought chocolate candy for themselves within the past six months, those with dietary restrictions that prevent them from buying chocolate candy, and those who indicated that they were not interested in completing a follow-up survey, leaving us with a sample of 1,298 potential participants for the second stage. Approximately 11 days after participants completed the first stage, we sent a follow-up survey to the 1,298 participants. Of these participants, 772 participants completed the second-stage survey. Attrition rates were similar across conditions (control = 40.64%, praise = 40.40%, χ2( 1, N = 1,298) = .008, p = .93).In the second-stage survey, participants indicated whether they had purchased any Kit Kats (yes/no) or Twix (yes/no) since they took the first portion of the survey. Finally, we asked participants in an open-ended question what they could recall about the tweet they saw in the first stage of the study, bringing the tweet to their attention so that we could measure the believability of the tweet using an adapted version of the measure in the prior study. ResultsWe conducted a binary logistic regression with condition (1 = praise, 0 = control) as the key predictor and believability as a continuous covariate on purchase behavior (1 = purchased Kit Kat, 0 = did not purchase Kit Kat). Those in the praise condition were significantly more likely to purchase Kit Kat compared with those in the control condition (β = .34, SE = .16, Wald χ2( 1, N = 772) = 4.17, p = .04). In raw percentages, 23.77% in the control condition purchased Kit Kat versus 31.95% in the praise condition. We conducted the same analysis for purchase behavior for Twix and find that there was no difference between the praise and control conditions (β = −.25, SE = .19, Wald χ2( 1, N = 772) = 1.68, p = .20) DiscussionStudy 2 extends our prior findings, demonstrating the effect of brand-to-brand praise on actual purchase behavior over a longer time horizon and relative to competitors' purchase outcomes. In doing so, we also show that even if the competitor brand becomes more salient than it would have normally been as a result of brand-to-brand praise, such praise still primarily provides positive benefits to the focal brand.Notably, in a supplemental study, we replicate the current findings using a paradigm with Subway and Jimmy John's (Web Appendix G). We find that when Subway brings Jimmy John's into the consideration set through brand-to-brand praise, Subway gains a boost in brand attitude compared with the self-promotion condition; Jimmy John's, however, does not gain a boost from receiving the praise.In the remaining studies, we identify when the beneficial effects of praise are mitigated and the underlying mechanism of the effect. In doing so, we offer additional understanding of the psychological processes driving consumer responses to praise as well as practical insights for choosing the appropriate recipients of and circumstances for praise. Study 3: The Effects of Brand-to-Brand Praise Toward Competitors Versus NoncompetitorsIn Study 3, we expand on the findings of the prior studies by examining the effects of brand-to-brand praise toward both a direct competitor and a noncompetitor. We expect that when a brand compliments a relevant direct competitor, which we define as a brand that competes in the same category as the focal brand, consumers view that compliment as relatively costly or risky because a brand has more to lose when bringing positive attention to such competition. It is such costliness that credibly signals that the brand must truly have warm intentions. We do not expect our effects to hold when a brand compliments an irrelevant noncompetitor, which we define as a brand that does not compete in the same category as the focal brand. In this scenario, the brand incurs lower cost by bringing attention to the irrelevant noncompetitor because there are less obvious repercussions, and thus the compliment is a less credible signal of a brand's warmth. This study also aims to demonstrate that the effects of brand-to-brand praise are not driven solely by the perceived novelty of the message or by negative reactions to self-promotional messages. Method Participants and designThree hundred ninety-nine participants (50.4% female) recruited from Amazon Mechanical Turk took part in this four-cell between-subjects design (helpful control, self-promotion, costly praise, noncostly praise). ProcedureWe created two eyeglasses brands for this study, Franklin's Frames and Lazlo's Lenses, and introduced them to participants as competitors. Participants then viewed an ostensibly recent series of three tweets from the focal brand, Franklin's Frames—one manipulated tweet that varied by condition and two filler tweets. The manipulated tweet in the praise condition read, ""@lazloslenses, Wow! Your new frames are looking good!"" with an image of a pair of glasses. In the self-promotion condition, the manipulated tweet read, ""Wow! Our new frames are looking good!"" with an image of a pair of glasses identical to the praise condition. In the helpful control condition, the manipulated tweet read, ""How to clean your frames:"" with a screen shot from a video showing how to clean eyeglasses. Participants in the noncostly praise condition saw a burger brand that is clearly not a direct competitor to Franklin's Frames, called Ben's Burgers. The focal tweet for those in the noncostly praise condition said, ""@bensburgers, Wow! Your new burgers are looking good!"" with an image of a burger attached (Web Appendix H).After reading the scenario, participants completed the measures of brand attitude (α = .95) noted in Study 2. In addition, we measured the perceived costliness of the tweets with four items on seven-point scales (α = .75; Web Appendix C). Finally, participants answered the same believability question as in prior studies as well as a measure of perceived novelty (""How novel was this tweet by Franklin's Frames?""; 1 = ""not novel,"" and 7 = ""very novel"") of the focal tweets. Results CostlinessA one-way analysis of covariance (ANCOVA) showed significant differences across conditions on perceived costliness of the focal tweet (F( 3, 394) = 37.66, p < .001,  ηp2   = .22).[12] As we expected, contrasts revealed that perceived costliness was significantly greater for the costly praise condition (M = 3.82) than the self-promotion (M = 2.21, p < .001), control (M = 2.37, p < .001), and noncostly praise (M = 2.52, p < .001) conditions. Perceived costliness for the noncostly praise condition was greater than the self-promotion condition (p = .05) but not significantly different from the control condition (p = .34). Lastly, the self-promotion and control conditions did not differ (p = .32). Notably, we measure perceived costliness of the stimuli in all of our studies and find the same pattern of results; praise toward a direct competitor is perceived as more costly than other types of messages (Web Appendix C). Brand attitudeA one-way ANCOVA revealed an effect of condition on brand attitude toward the focal brand, Franklin's Frames (F( 3, 394) = 4.83, p = .003,  ηp2   = .04). Contrasts revealed that brand attitude was significantly greater for the costly praise condition (M = 6.05) than the control (M = 5.43, p < .001) and self-promotion (M = 5.46, p < .001) conditions. Brand attitudes were also significantly greater for costly praise than noncostly praise (M = 5.65, p = .02). Brand attitude for those in the control, self-promotion, and noncostly praise conditions were not significantly different (all ps > .18), suggesting that the results are driven by a boost from the praise message and not a dislike of the self-promotion message. NoveltyUnsurprisingly, a one-way ANCOVA revealed significant differences across conditions on how novel the tweet seemed (F( 3, 394) = 8.80, p < .001,  ηp2   = .06). Contrasts revealed that both the noncostly praise (M = 4.30) and the costly praise (M = 4.56) conditions were perceived to be more novel than the control (M = 3.48) and self-promotion (M = 3.43, all ps ≤ .001) conditions. Crucially, however, there was no difference in perceived novelty between the noncostly praise and the costly praise conditions (p = .31), suggesting that the differences between these conditions on brand attitude were not driven purely by differences in novelty.[13] DiscussionIn Study 3, we replicate the findings of prior studies, demonstrating that brand-to-brand praise between competitors boosts brand evaluations compared with other messages, including self-promotion and a helpful control message. Furthermore, we find that noncostly praise (i.e., praise toward irrelevant noncompetitors) did not give the same boost. This result suggests that simply speaking positively about another brand is not enough; the consumer must view the compliment as costly to the brand for it to have positive effects. Although we find costliness to be necessary to show the benefits of praise, it is not the underlying driver of the effects (mediation comparing the self-promotion and costly praise conditions: ab = .06, 95% confidence interval [CI] = [−.045,.191]; PROCESS Model 4, 5,000 bootstrap samples; [31]). This study also begins to cast doubt on novelty as a primary driver of the effect, given that the noncostly praise was perceived to be as novel (but not evaluated as positively) as the costly praise. We further test the role of novelty in subsequent studies. In addition, we again find that the effects on brand attitude do not occur because consumers dislike self-promotion messages but are instead driven by the boost from seeing the costly praise message, replicating the findings of Studies 1a and 2. Study 4: Testing Warmth as the MechanismTo begin to understand why brand-to-brand praise increases brand evaluations and choice, we conducted a qualitative pretest gauging people's natural reactions (i.e., inferences, attributions) to observing a brand complimenting a competitor. Participants (N = 150) on Prolific saw a tweet from PlayStation congratulating Nintendo on its Nintendo Switch launch and then listed five adjectives to describe PlayStation. We find that observers more frequently attribute warmth-related words (e.g., friendly, supportive, kind; 55.73% of the adjectives), rather than competence-related words (e.g., confident, successful, intelligent; 17.07% of the adjectives), or any other kind of perception, to the brand, implicating warmth as the most top-of-mind inference following brand-to-brand praise. For additional insight, we asked participants to take the perspective of a manager and indicate why they would or would not post a message praising the competition. We find that most participants would be willing to praise a competitor (69.33%). Again, the majority of the responses (61.5%) indicated warmth to be the primary reason for the action (e.g., ""I want to show others that I act in selfless ways""). Moreover, some responses also positively noted competence-related reasons (e.g., ""have confidence in my own brand and its qualities""; ""we appear to be positive and not insecure about our place in the market"") either exclusively or in combination with warmth (25%), suggesting that while warmth-related reasons are more top-of-mind, perceptions of competence are unlikely to be harmed (for pretest details, see Web Appendix I).In light of these initial qualitative insights, we empirically test warmth as the driver of the benefits of brand-to-brand praise and compare it with competence in Study 4. We predict that brand-to-brand praise enhances perceptions of brand warmth, which predominantly drives the boost in brand evaluations, rather than competence. Ultimately, we expect the heightened brand evaluations to drive consumer action, which is measured in this study as their willingness to sacrifice their own time (for free) on behalf of the brand. Methods Participants and designTwo hundred participants (57% female) from Prolific took part in this two-cell between-subjects study (control, praise). ProcedureParticipants were introduced to a real tea brand, Treecup Tea, from Kickstarter. They were told that Treecup Tea focuses on making high-quality tea blends and competes with other tea companies on Kickstarter for funds. Participants read that Treecup Tea's competitors include Teafir, Shisso Tea, and Phat Tea. Next, participants were introduced to Treecup Tea's Twitter page. For the control condition, participants saw only the brand's Twitter header. We use this control condition, different from the self-promotion control in prior studies, to show again that the boost for the brand results from people feeling positive about the praise message instead of negative toward self-promotion. For the praise condition, participants saw a Twitter page that consisted of three praise messages toward the three competitors interspersed throughout other tweets on the page (e.g., ""@ShissoTea Your tea is fresh and sustainable! That's amazing!""; Web Appendix J).Participants then completed the same measure of brand attitude as in prior studies. Participants were also asked how much time (scale from 0 to 4 minutes, in 30-second increments) they were willing to volunteer to answer some market research questions for Treecup Tea after the main study, without additional pay. We chose this dependent variable as an action of consequence to the consumer, given that volunteering time is a costly consumer behavior. Finally, participants completed measures for warmth (""warm, friendly"" on a seven-point scale; r = .81) and competence (""competent, capable"" on a seven-point scale; r = .90; [ 1]). The order of the warmth and competence measures was randomized to ensure that one did not explain the effect more than the other simply due to order. We again measured believability of the stimuli as a covariate in our analyses. Results Brand attitudeReplicating prior results, a one-way ANCOVA revealed a significant effect of message (F( 1, 197) = 17.61, p < .001,  ηp2   = .08), whereby praise (M = 5.86) led to greater brand attitude compared with the control (M = 5.33). Volunteer timeThe data were skewed because 34.1% of the sample indicated that they would not volunteer any time,[14] so we conducted a binary split on volunteer time (1 = those who would volunteer any amount of time, 0 = those who would not volunteer). A binary logistic regression with condition (1 = praise, 0 = self-promotion) and believability as the predictors on willingness to volunteer revealed that those in the praise condition were significantly more likely to volunteer time compared with those in the self-promotion condition (β = .59, SE = .30, Wald χ2( 1, N = 200) = 3.76, p = .05).[15] Warmth and competenceA one-way ANCOVA revealed a significant effect of message on warmth (F( 1, 197) = 22.54, p < .001,  ηp2   = .10): praise (M = 5.92) led to greater perceptions of warmth compared with the control (M = 5.20). A one-way ANCOVA also revealed a smaller but significant effect of message on competence (F( 1, 197) = 5.59, p = .02,  ηp2   = .03): praise (M = 5.49) led to greater perceptions of competence compared with the control (M = 5.17). MediationNext, we tested the effect of praise on willingness to volunteer through warmth and brand attitude as serial mediators. We find that the praise message leads to increased warmth perceptions, which leads to improved brand attitude and, subsequently, greater willingness to volunteer (indirect = .08, 95% CI = [.008,.228]). However, a serial mediation model replacing warmth with competence is not significant (indirect = .05, 95% CI = [−.005,.169]). DiscussionIn Study 4, we replicate prior findings and also demonstrate a novel downstream consequence of praise whereby observers are more likely to give up some of their time, without additional compensation, to help the praiser brand after viewing a praise message. We find warmth to be the most direct driver of the effects of brand-to-brand praise. However, given that competence perceptions also benefit from a praise message, it is worth considering how brand-to-brand praise may usher brands into the ""golden quadrant"" ([32]) of warmth and competence dimensions. Lastly, the stimuli used for the praise message in this study consisted of three compliments to three different competitors on a single Twitter page. The fact that we still see a boost in brand attitude suggests that giving praise repeatedly, at least to some extent, may not harm the brand in this context (an idea we revisit in the ""General Discussion"" section). Study 5: Moderated Mediation by Organization TypeIn Study 5, we again test for warmth as the mechanism underlying the effect of brand-to-brand praise on evaluations, and we do so with process by moderation, directly manipulating (and again also measuring) brand warmth. Compared with brands that are lower in warmth, we predict that brand-to-brand praise will not increase brand evaluations as much for brands that are higher in warmth. We theorize that this is because brands that are already high in perceived warmth will not have as much need or space to grow in that aspect, thus rendering praise less influential. In contrast, brands with lower perceived warmth at baseline will benefit more from the boost given by brand-to-brand praise. Based on prior research demonstrating that nonprofit organizations are high in perceived warmth ([ 2]), we compare the effects of praise on nonprofit and for-profit organizations. Beyond its relation to our theory, comparing the effect of praise in for-profit versus nonprofit organizations is of practical importance, as it will help managers from these clearly identifiable sectors better assess how effective brand-to-brand praise may be for their brands. Lastly, we measure perceptions of the brand's arrogance as an alternative explanation, given that self-promotional messages may be perceived as a form of bragging. We also measure perceptions of authenticity and novelty of the message as other potential explanations for the effect. Method Participants and designSix hundred one participants (62.2% female) from Prolific took part in this 2 (message: control, praise) × 2 (organization type: nonprofit, for-profit) between-subjects experimental study that was preregistered.[16] ProcedureParticipants were introduced to a fictitious internet service organization, Tech Dev. In the nonprofit condition, participants were told that Tech Dev was a nonprofit organization that had a goal of helping the community. In the for-profit condition, participants read that Tech Dev was a for-profit organization with a goal of increasing profits. Participants were also told that Tech Dev competed with another organization, Networks.org or Networks.com, for either donations or sales (depending on its nonprofit or for-profit status, respectively). Next, participants were shown the Twitter page of Tech Dev in which they read either a control message (""Want to know more about the quality of our services? Click here: techdev[.com/.org]/internet"") or a message praising Networks (""Networks[.com/.org], we are impressed by the quality of your services—competitor or not!""; Web Appendix K).Participants then indicated their interest in Tech Dev using two measures (""How likely are you to seek out more information about Tech Dev?"" and ""How willing are you to talk to a customer representative to learn more about Tech Dev?""; r = .79).[17] We also measured warmth, competence, and believability of the stimuli. As in the prior study, the order of warmth and competence was randomized. In addition, we measured the extent to which participants' assigned condition was perceived as arrogant (braggy, conceited, arrogant; α = .94), authentic (authentic, self-aware; r = .65), and novel (single-item measure) as alternative explanations,. Results Brand interestUsing a two-way ANCOVA, we find a main effect of message (F( 1, 596) = 24.51, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 35.82, p < .001,  ηp2   = .06), and a significant interaction (F( 1, 596) = 4.79, p = .03,  ηp2   = .008). As predicted, in the for-profit conditions, we replicate prior results in that praise (M = 3.74) led to greater brand attitude than the control did (M = 2.78; p < .001). In the nonprofit conditions, praise (M = 4.25) also led to greater brand evaluations than the control did (M = 3.86; p = .04), but to a diminished extent. WarmthWe find a similar pattern for warmth. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 98.85, p < .001,  ηp2   = .14), a main effect of organization type (F( 1, 596) = 124.43, p < .001,  ηp2   = .17), and a significant interaction (F( 1, 596) = 8.44, p = .004,  ηp2   = .01). In the for-profit conditions, we again replicate prior results where praise (M = 4.78) led to significantly greater warmth compared with the control (M = 3.37; p < .001). In the nonprofit conditions, we find that praise (M = 5.67) led to weaker, though still significant, effects compared with the control (M = 4.88; p < .001). CompetenceWe also find a similar pattern for competence. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 26.90, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 23.33, p < .001,  ηp2   = .04), and a significant interaction (F( 1, 596) = 4.49, p = .03,  ηp2   = .007). In the for-profit conditions, we find that praise (M = 5.22) led to greater competence compared with the control (M = 4.53; p < .001). In the nonprofit conditions, we find that praise (M = 5.48) led to weaker, though still significant, effects compared with the control (M = 5.18; p = .02). AlternativesWe do not find the same pattern of results for bragging. A two-way ANCOVA revealed only a main effect of organization (F( 1, 596) = 55.77, p < .001,  ηp2   = .09) such that the nonprofit (M = 2.08) was perceived as less arrogant than the for-profit (M = 2.93).For authenticity, we find a similar pattern to that of warmth in which there was a significant interaction (F( 1, 596) = 16.33, p < .001,  ηp2   = .03) where the boost from praise was stronger in the for-profit conditions. We find similar patterns for novelty (interaction F( 1, 596) = 6.58, p = .01,  ηp2   = .01). Moderated mediationWe first tested the predicted model. Specifically, we tested for moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by organization type, controlling for believability. As expected, we find that organization type significantly moderated the mediation through warmth (index of moderated mediation = .36, 95% CI = [.119,.609]), and the mediation effect is stronger in the for-profit conditions (ab = .82, 95% CI = [.624, 1.039]) compared with the nonprofit conditions (ab = .46, 95% CI = [.299,.632]).Then, to explore the role of alternative explanations, we entered all the potential mediators (warmth, competence, bragging, authenticity, and novelty) in parallel into the moderated mediation model. We find that warmth remains a significant mediator in the model (index of moderated mediation = .23, 95% CI = [.079,.408]), suggesting that none of these alternatives ""swamp"" warmth in explaining this effect. Next, we find that the indices of moderated mediation for competence (index of moderated mediation = .08, 95% CI = [.005,.180]), authenticity (index of moderated mediation = .13, 95% CI = [.022,.271]), and novelty (index of moderated mediation = .12, 95% CI = [.025,.231]) were also significant. However, the index for bragging was not significant (index of moderated mediation = −.02, 95% CI = [−.083,.038]). Although competence, authenticity, and novelty were also significant mediators in the model in addition to warmth, warmth remains significant with the largest index of moderated mediation.Because warmth is most frequently compared with competence, we conducted a final analysis statistically comparing the sizes of their indirect effects in a parallel mediation model. We do this within the for-profit conditions in which the effects were more prominent (and because we are unable to statistically compare the full moderated mediation models). We find that the indirect effect for warmth as a mediator was significantly greater than that of competence (p < .001, 95% CI = [.205,.718]). Thus, though other factors may play a role, our results point to warmth as the primary driver of the effects. DiscussionStudy 5 sheds further light on warmth as the primary mechanism underlying brand-to-brand praise. We demonstrate that brand-to-brand praise increases perceived warmth, which enhances brand interest, but this effect is attenuated when the brand is already high in perceived warmth (e.g., nonprofits), as there is less room and need for growth in warmth. Thus, the benefits of praise can be most clearly seen among brands that are perceived as less warm (e.g., for-profits) at baseline. In addition, while perceptions of competence, authenticity, and novelty may play a role in driving brand interest, we show warmth to be the more consistent, primary driver of the effect. Finally, we rule out bragging as an alternative explanation. Study 6: Moderation by SkepticismIn Study 6, we look to another context in which the effects of brand-to-brand praise may be attenuated. Here, we examine skepticism toward advertising as an individual difference that may moderate the effects of praise. Advertising skepticism is an individual difference that has been defined as consumers' chronic doubt or mistrust in a marketer's message ([47]). Individuals who are more skeptical are generally less persuaded by marketing messages and are less trusting of brands. As brands are confronting an ""age of cynicism"" where skepticism is at an all-time high ([21]; [49]) and 71% of consumers report having little faith in brands ([30]), it is very important to understand its effects.As seen in the previous study, the effects of brand-to-brand praise become more prominent when there is room for increasing perceptions of brand warmth. Consumers who are more skeptical of advertising distrust that brands have positive intentions, or warmth, and thus inherently provide brands with greater room to improve in warmth than their nonskeptical counterparts who are already trusting. Thus, brand-to-brand communication may be most effective among skeptics, as it can bypass their cynicism and boost perceptions of warmth. Moreover, such skeptics are generally more persuaded by nonadvertising sources of information and emotional appeals ([48]), which allow marketers to circumvent consumer resistance by decreasing the activation of persuasion knowledge ([27]). Because brand-to-brand praise does not explicitly aim to promote one's brand and instead relies on the more emotional signal that the brand is high in warmth, it is less likely to activate persuasion knowledge and more likely to be accepted. Thus, we predict that consumers high in skepticism will be the most affected by brand-to-brand praise. Study 6 tests this idea utilizing two well-known competitors, Lyft and Uber. Method Participants and designSix hundred participants were recruited from Prolific for this 2 (message: self-promotion, praise) × measured (advertising skepticism) preregistered study.[18] ProcedureParticipants first completed the skepticism toward advertising scale ([47]; 1 = ""strongly disagree,"" and 5 = ""strongly agree"") and then completed filler items. Next, participants saw Lyft's Twitter page, where they read either a self-promotion tweet (""Congratulations to us on all our achievements this past year!"") or a praise tweet toward Uber (""@Uber Congratulations on all your achievements this past year!""; Web Appendix L). Then, participants completed the same measure of brand attitude, warmth, and competence as in previous studies. We also measured perceptions of bragging, authenticity, and novelty using the same measures as in Study 5 as potential alternative explanations, as well as believability of the stimuli as a covariate. Results Brand attitudeReplicating prior results, an ANCOVA controlling for believability revealed a main effect of message on brand attitude (F( 1, 597) = 22.12, p < .001,  ηp2   = .04), where praise (M = 5.22) outperformed self-promotion (M = 4.74). Next, we find a significant interaction between message and skepticism (skepticism scale reverse coded; β = .21, SE = .10, p = .03; Figure 2) on brand attitude, such that when skepticism toward advertising was higher, the praise message led to a greater increase in brand attitude (Johnson–Neyman point = 1.48, 75.67% of participants; β = .23, SE = .12, p = .05). The effect of message was attenuated at lower levels of skepticism below the Johnson–Neyman point.Graph: Figure 2. Study 6: Moderation by skepticism. MediationWe find that praise significantly boosted perceptions of warmth (Mpraise = 5.36 vs. Mself = 4.99; F( 1, 597) = 13.76, p < .001,  ηp2   = .02), competence (Mpraise = 5.52 vs. Mself = 5.32; F( 1, 597) = 4.88, p = .03,  ηp2   = .008), authenticity (Mpraise = 4.90 vs. Mself = 4.56; F( 1, 597) = 9.46, p = .002,  ηp2   = .02), and novelty (Mpraise = 4.58 vs. Mself = 4.28; F( 1, 597) = 6.59, p = .01,  ηp2   = .01) compared with self-promotion, controlling for believability. Self-promotion was seen as more arrogant than praise (Mpraise = 3.18 vs. Mself = 4.16; F( 1, 597) = 54.23, p < .001,  ηp2   = .08).We then tested for the predicted moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by skepticism, controlling for believability. While the index of moderated mediation was not significant (index = .10, 95% CI = [−.046,.247]), we find that the indirect effect patterns were in line with our predictions, such that the indirect effect is stronger and significant for those higher in skepticism (+1 SD = 3.17, ab = .31, 95% CI = [.084,.550]) and nonsignificant for those lower in skepticism (−1 SD = 1.30, ab = .12, 95% CI = [−.026,.274]).Next, we test for mediation with all of the potential alternative explanations in parallel (PROCESS Model 4, 5,000 bootstrap samples, [31]). Warmth (ab = .14, 95% CI = [.064,.230]), competence (ab = .04, 95% CI = [.004,.091]), bragging (ab = .09, 95% CI = [.049,.146]), authenticity (ab = .06, 95% CI = [.021,.116]), and novelty (ab = .03, 95% CI = [.004,.054]) mediate the effect of message on brand attitude. However, we again find that the indirect effect for warmth was the largest, pointing to its primary role in causing this effect.Finally, we statistically compared the indirect effects of warmth and competence in the model and find the indirect effect of warmth to be significantly greater than that of competence (p = .001, 95% CI = [.085,.349]), again suggesting that warmth plays the primary role in driving the effect of brand-to-brand praise on brand attitudes. DiscussionIn Study 6 (and in a behavioral replication in Web Appendix M),[19] we show that brand-to-brand praise can actually operate more effectively for people who are generally more skeptical of advertising, as brand-to-brand praise bypasses their suspicions and creates more favorable consequences for the brand. Further, while we find that competence, bragging, authenticity, and novelty play a role in driving the effects of brand-to-brand praise on brand evaluations, we still identify warmth as the primary, more consistent underlying driver. General DiscussionWe investigate how observing brand-to-brand praise affects consumers' brand evaluations and choices. Across a variety of different modes of communication (social media, print advertising, digital advertising), study methods (web scraping; field, lab, and online studies), contexts (consumer products and services), outcomes (brand attitudes, social media and advertising engagement, brand choice, purchase behavior), and praise content, we show that consumers who witness brand-to-brand praise between competitors form more favorable evaluations of the praiser brand than consumers who witness other forms of communication, including typical self-promotion messages, helpful messages, basic brand information, and even outside-industry praise (for a summary of contexts, conditions, and findings across studies, see Web Appendix N). In addition to showing robustness to a variety of praise messages in the presented studies, a preregistered supplemental study demonstrates that the effect is further robust to both general and specific praise (Web Appendix O). Furthermore, the varied study stimuli suggest that this effect is robust to brands in a wide range of industries—including car care, snacks, candy, eyewear, beverages, technology, and transportation—with competitive relationships varying in intensity. In other words, brand-to-brand praise seems to benefit the praiser in less competitively intense relationships (e.g., mom-and-pop popcorn brands) as well as more competitively intense relationships (e.g., PlayStation and Nintendo, Uber and Lyft) in a variety of industries. We trace this effect primarily to the notion that brand-to-brand praise signals a brand's warmth, which leads to improved brand evaluations and affects consumer choices.In addition, we show that this effect only exists when the praise is deemed to be associated with significant cost or risk (Study 3). Importantly, we find that the effects of brand-to-brand praise are diminished in some situations or among some consumers, such as when the brand is already high in warmth (Study 5) or among consumers who are already trusting of brand intent (Study 6). These boundaries provide further evidence for the crucial role that perceptions of warmth play in driving the benefits of praise. While other mechanisms may also play a role, as consumer behaviors are generally multiply determined ([35]; [55]), we find warmth to be the most consistent, primary driver of the effects of brand-to-brand praise. Theoretical ContributionsOur research makes several theoretical contributions to literature streams on brand perceptions and relationships, brand communication, and praise. First, we contribute to the brand perception literature by showing that consumers' perceptions of brands can be affected by viewing a brand's interactions with other brands. We demonstrate that observing brand-to-brand praise can positively affect perceptions of a brand's warmth and influence subsequent brand evaluations. Second, we add to the warmth and competence literature by introducing a novel context in which brand-to-brand praise increases warmth without harming perceptions of competence. Third, our research demonstrates that directing positive attention toward the competition instead of toward one's own brand brings about benefits for the praiser brand, unlike what brands would typically do in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]). Finally, we contribute to the literature on praise by identifying brand-to-brand communication between competitors as a feasible form of praise that is less likely to induce suspicion compared with praise in traditional person-to-person contexts. Marketing Implications and Future ResearchAs we have noted, our findings may be surprising to practitioners who have been regularly and reasonably advised to avoid bringing positive attention to their competitors. However, our studies show that in some circumstances, praise is a method of brand-to-brand interaction that can result in beneficial consequences for the praising brand. Managers might consider offering compliments to competitors to boost their own brand evaluations. In other words, brands can expand from solely focusing on brand-to-consumer relationships to also focusing on their brand-to-brand relationships. This is akin to the positive reactions that politicians sometimes receive when positively acknowledging their opponent ([12]). We suggest a new context in which positive acknowledgment can benefit brands. With the rise of the digital age, brands can easily ""speak"" with each other and be observed by consumers. While it is not uncommon for brands to speak via ""feuds"" on social media, such as when Wendy's teases McDonald's for using frozen beef ([19]), we show in a supplemental study (Web Appendix P) that positive communication provides unique advantages. Negative or snarky communication, directed at a competitor or even directed at the self (as in the case of two-sided messaging), does not provide the same increase in brand evaluations. Although there will likely be variation depending on the exact content and cleverness of snarky communication (a ripe area for future research), marketers would be wise to consider opportunities for brand-to-brand praise instead, perhaps utilizing social media as a platform, to foster a warmer image.Brand-to-brand praise may also be a valuable way to respond to competitors' actions. While the norm for companies responding to a competitor's new product release is to avoid saying anything that would bring attention to that competitor, our studies suggest that responding positively can increase purchases for the praiser brand without boosting the competitor to the same degree. Importantly, while prior work has shown that a brand's positioning as an underdog or market leader has important implications ([50]), brand-to-brand praise may be appropriate regardless of the competitor's market status. In a preregistered supplemental study (Web Appendix Q), we find that the favorable effects of brand-to-brand praise on brand evaluations are robust to the market leadership status of the brand. Future research could further explore how characteristics about the firm such as market leadership or firm size may factor into the effect of brand-to-brand praise.Similarly, while the variety of brands leveraged in our studies suggest that brand-to-brand praise is likely beneficial when directed toward both less intense (e.g., local popcorn brands) and more intense (e.g., PlayStation and Nintendo, Uber and Lyft) competitors, future research should more systematically explore the role of competitive intensity. Such research might explore brand-to-brand praise with indirect versus direct competitors, which would likely vary in levels of perceived costliness. Indirect competitors may include other brands that are in a similar industry but do not compete directly in the same product category, such as in the case of soda and water in the drinks industry. In Study 3, we find that praise toward a direct competitor is deemed to be costly and results in better brand evaluations compared with praise toward noncompetitors. However, would praise toward an indirect competitor be perceived to be costly enough to increase evaluations? Outside of direct competition, what else determines whether praise is deemed to be costly or not? How would consumers react if complementary brands, such as soda and popcorn brands, praised each other? While we suggest that praise should benefit the praiser as long as it is perceived as costly, understanding what kind of brand-to-brand praise is considered costly may be beneficial to marketers.In addition, future research could also explore how brand-to-brand praise compares to other types of messages that convey warmth. For example, brands often communicate prosocial messages, such as a food brand showing active support for a local food bank. Brands might also post self-deprecating messages that recognize their own room for improvement. While these types of messages may signal warmth, they may not benefit the messenger brand in the same way as brand-to-brand praise does because of the unique aspect of costliness associated with praising a direct competitor. Future research could compare these various types of brand messaging to find ones that benefit brands the most.Future work might also examine whether certain brand personalities benefit more from praise than others. Our for-profit versus nonprofit results in Study 5 hint that brands with personalities that are inherently associated with warmth (e.g., sincere brands) may benefit less. Relatedly, are there circumstances under which snarky interactions or backhanded compliments are better suited for some brands, such as Wendy's ([15])? We noted that negativity did not fare as well as positivity (Web Appendix P), but under what conditions might this be different? Cultural differences, such as collectivism and individualism, may also play an important role. Collectivists, known for valuing community and kinship, may be particularly likely to value the warmth associated with brand-to-brand praise ([ 3]). Furthermore, future work could also explore when the effects of brand-to-brand praise are affected by gender. While women may value warmth more in some circumstances ([66]), we do not find consistent moderation by gender in our studies (Web Appendix R), which is consistent with prior work showing that gender does not always moderate brand warmth perceptions ([ 5]). Further research is needed to better understand the possible role that factors such as brand personality, consumer culture, and gender play in affecting brand-to-brand praise responses.Another direction for future research involves investigating the optimal frequency of brand-to-brand praise. Although we demonstrate that some repetition may be acceptable (Study 4), one limitation of this research is the focus on a single instance of praise in a short span of time. Praise when repeated over time may become less effective or even backfire. Excessive praise may trigger suspicion in observers (e.g., [23]), and previous research has shown that suspicious praise can lead to the praiser being perceived as less sincere ([11]; [44]). Thus, future research might explore the optimal levels or repetition of praise and the consequences of excessive praise to prevent such a backfiring effect.To further prevent backfiring effects, future research might also explore what happens if the praised competitor leverages the praise ""against"" the praiser? For instance, what if a praised brand publicly suggests that they are so great that even their competition praises them? It is possible that, if used by the competitor in this manner, the praise could damage the praiser brand. However, it is also possible that this kind of act could be perceived as arrogant or manipulative, damaging the praised brand and eliciting sympathy for the praiser.Overall, this work raises the question: Are brands missing an opportunity to build positive relationships with consumers by not (publicly) building positive relationships with competitors? We suggest that marketers would be wise to explore the intriguing benefits of brand-to-brand praise. "
7,"Blame the Bot: Anthropomorphism and Anger in Customer–Chatbot Interactions Chatbots have become common in digital customer service contexts across many industries. While many companies choose to humanize their customer service chatbots (e.g., giving them names and avatars), little is known about how anthropomorphism influences customer responses to chatbots in service settings. Across five studies, including an analysis of a large real-world data set from an international telecommunications company and four experiments, the authors find that when customers enter a chatbot-led service interaction in an angry emotional state, chatbot anthropomorphism has a negative effect on customer satisfaction, overall firm evaluation, and subsequent purchase intentions. However, this is not the case for customers in nonangry emotional states. The authors uncover the underlying mechanism driving this negative effect (expectancy violations caused by inflated pre-encounter expectations of chatbot efficacy) and offer practical implications for managers. These findings suggest that it is important to both carefully design chatbots and consider the emotional context in which they are used, particularly in customer service interactions that involve resolving problems or handling complaints.Keywords: customer service; artificial intelligence; conversational agents; chatbots; anthropomorphism; anger; expectancy violationsThe use of artificial intelligence (AI) in marketing is on the rise, as managers experiment with the use of AI-driven tools to augment customer experiences. One relatively early use of AI in marketing has been the deployment of digital conversational agents, commonly called chatbots. Chatbots ""converse"" with customers, through either voice or text, to address a variety of customer needs. Chatbots are increasingly replacing human service agents on websites, social media, and messaging services. In fact, the market for chatbots and related technologies is forecasted to exceed $1.34 billion by 2024 ([71]).While some industry commentators suggest that chatbots will improve customer service while simultaneously reducing costs ([16]), others believe they will undermine customer service and negatively impact firms ([34]). Thus, while customer service chatbots have the potential to deliver greater efficiency for firms, whether—and how—to best design and deploy chatbots remains an open question. The current research begins to address this issue by exploring conditions under which customer service chatbots negatively impact key marketing outcomes. While many factors may influence customers' interactions with chatbots, we focus on the interplay between two common features of the customer service chatbot experience.The first feature relates to the design of the chatbot itself: chatbot anthropomorphism. This is the extent to which the chatbot is endowed with humanlike qualities such as a name or avatar. Currently, the prevailing logic in practice is to make chatbots appear more humanlike ([ 9]) and for them to mimic the nature of human-to-human conversations ([44]). However, anthropomorphic design in other contexts (e.g., branding, product design) does not always produce beneficial outcomes (e.g., [36]; [38]). Accordingly, we examine circumstances under which anthropomorphism of customer service chatbots may be harmful for firms.The second dimension explored in this research is a commonly occurring feature in customer service interactions, irrespective of the modality: customer anger. Anger is one of the most prevalent specific emotions occurring in customer service contexts; estimates suggest that as many as 20% of call center interactions involve hostile, angry, complaining customers ([24]). Furthermore, the prevalence of anger increased during the COVID-19 pandemic ([57]; [60]), so a higher proportion of interactions are likely to be with angry customers. Thus, it is both practically relevant to consider how customer anger interacts with chatbot anthropomorphism and theoretically relevant due to the specific responses (e.g., aggression, holding others accountable) evoked by anger that impact the efficacy of more humanlike technology.Across five studies including the analysis of a large real-world data set from an international telecommunications company and four experiments, we find that when customers in an angry emotional state encounter a chatbot-led service interaction, chatbot anthropomorphism has a negative effect on customers' satisfaction with the service encounter, their overall evaluation of the firm, and their subsequent purchase intentions. However, this is not the case for customers in nonangry emotional states. The negative effect is driven by an expectancy violation; specifically, anthropomorphism inflates preinteraction expectations of chatbot efficacy, and those expectations are disconfirmed. Our findings suggest that it is important to both carefully design chatbots and consider the emotional context in which they are used, particularly in common types of customer service interactions that involve handling problems or complaints. This research contributes to the nascent literature on chatbots in customer service and has managerial implications both for how chatbots should be designed and for context-related deployment considerations. Conceptual Framework Anthropomorphism in MarketingDeliberate marketing efforts have made anthropomorphism, or the attribution of humanlike properties, characteristics, or mental states to nonhuman agents and objects ([20]; [68]), especially pervasive in the modern marketplace. Product designers and brand managers often encourage customers to view their products and brands as humanlike, through a product's visual features (e.g., face-like car grilles; [40]) or brand mascots (e.g., the Pillsbury Doughboy; [66]). In digital settings, advances in machine learning and AI have ushered in a new wave of highly anthropomorphic devices, from humanlike self-driving cars ([70]) to voice-activated virtual assistants with human names and speech patterns (e.g., Amazon's Alexa; [32]).Extant research generally suggests that inducing anthropomorphic thought is linked to improved outcomes. ""Humanized"" products and brands are more likely to achieve long-term business success because they encourage a more personal consumer–brand relationship ([ 1]; [66]). Anthropomorphic product features can make products more valuable ([28]) and can boost overall product evaluations in categories, including automobiles, mobile phones, and beverages ([ 1]; [39]; [40]). [67] found that anthropomorphized products increased consumers' preference and subsequent choice of those products.Anthropomorphism of technology has also been shown to improve marketing outcomes. Humanlike interfaces can increase customer trust in technology by increasing perceived competence ([ 5]; [70]) and are more resistant to breakdowns in trust ([18]). Avatars (anthropomorphic virtual characters) can make online shopping experiences more enjoyable, and both avatars and anthropomorphic chatbots can increase purchase intentions ([27]; [31]; [72]). Anthropomorphic digital messengers can even be more persuasive than human spokespeople in some contexts ([63]) and can increase advertising effectiveness ([13]). Anthropomorphized digital devices can even become friends with their users ([56]), such that the consumer resists being disloyal by replacing the product ([12]), leading to greater customer brand loyalty.Although most evidence points to beneficial effects of anthropomorphism, there are drawbacks. For example, anthropomorphic helpers in video games reduce enjoyment of the gaming experience by undermining a players' sense of autonomy ([36]). Other research shows that for agency-oriented customers, brand anthropomorphism exaggerates the perceived unfairness of price increases ([38]) and hurts brand performance amid negative publicity ([54]). Low-power customers perceive risk-bearing entities (e.g., slot machines) as riskier when the entities are anthropomorphized ([37]). Further, research suggests that when customers are in crowded environments and want to socially withdraw, brand anthropomorphism harms customer responses ([53]). Thus, it would be overly simplistic to assume that anthropomorphism positively impacts customers' encounters with brands, products, or companies. The consequences are more nuanced, with outcomes depending on both customer characteristics and the context ([64]).While customers' downstream responses to anthropomorphism are mixed, one consistent consequence of anthropomorphism is that customers attribute more agency to anthropomorphic entities ([20]). ""Agency"" refers to the capacity to plan and act ([25]). Because anthropomorphism leads customers to perceive a mental state in another entity, it increases individuals' perception that the entity is capable of acting in a deliberate manner ([69]). This increases expectations that the agent has abilities such as emotion recognition, planning, and communication ([25]). These heightened expectations lead individuals to ascribe moral responsibilities to anthropomorphic entities ([69]), to believe that the entity should be held accountable for its actions ([18]), and to think the entity deserves punishment in the case of wrongdoing ([25]).Of course, anthropomorphic entities do not always perform in a manner consistent with the high levels of agency customers expect. In fact, some researchers suggest that one reason behind the ""uncanny valley"" (i.e., the tendency for a robot to elicit negative emotional reactions when it closely resembles a human; [48]) is because robots do not perform in the agentic manner that their human resemblance would imply ([69]). In other words, the robots' behavior violates the expectations elicited by their highly anthropomorphic facade. These violations arguably apply to current chatbots, given that their performance is not expected to reach believable levels of human intelligence before 2029 ([58]). Thus, expectancy violations play an important role in chatbot-driven customer service settings. Expectancy Violations and Customer AngerBefore using a product or service, customers form expectations regarding how they anticipate the target product, brand, or company will perform. Postusage, customers evaluate the target's performance and compare that to their preusage expectations ([10]). When performance fails to meet expectations, the negative disconfirmation is known as an expectancy violation ([62]), which arises because ( 1) preusage expectations are high or ( 2) postusage performance is poor ([10]). Expectancy violations not only harm customer satisfaction ([50]; [51]) but also negatively impact other consequential downstream outcomes, including attitude toward the company ([10]) and purchase intentions ([11]; [50]). Importantly, customer responses to expectancy violations are highly influenced by their emotional states, particularly anger ([ 4]).Two theories help explain why anger increases customers' negative responses to expectancy violations. The functionalist theory of emotion suggests that anger is an activating, high-intensity emotion with an evolutionary purpose: it evokes quick decision making and heuristic use to react quickly to immediate threat ([ 8]). Anger is often used as a strategy to respond to obstacles ([43]; [46]) or retaliate against an offending party ([14]) because of its tendency to increase action and aggression, compared with other emotions that are deactivating (e.g., sadness; [15]; [41]) or nonaggressive ([43]).This retaliation is also predicted by appraisal theorists, who suggest that even in situations of incidental anger, anger increases the tendency to hold others responsible for negative outcomes ([35]) and to respond punitively toward them ([23]; [42]; [43]). This is markedly distinct from emotions such as frustration or regret, which are more likely to manifest when people hold the situation or themselves responsible for negative outcomes, respectively ([21]; [55]). Thus, angry (vs. nonangry) customers are more likely to blame others and retaliate when another's performance falls short of expectations. This is particularly the case if their goals are obstructed ([46]), as angry customers especially feel the need to achieve a desirable outcome ([55]). Linking Chatbot Anthropomorphism, Expectancy Violation, and Customer AngerDrawing from the extant theories and research, we hypothesize that anthropomorphism heightens customers' preperformance expectations about a chatbot's level of agency and performance capabilities, resulting in expectancy violations. Further, angry customers are more likely to suffer from expectancy violations due to their need to overcome obstacles, to blame and hold others accountable, and to respond punitively to such expectancy violations due to their action orientation (i.e., giving lower satisfaction ratings, poor reviews, or withholding future business from the offending party). This logic would suggest that angry customers might be better served by nonanthropomorphic agents. Recent research supports this notion by demonstrating that in unpleasant service situations, reducing human contact (e.g., through technological barriers; [ 6]) can help attenuate customer dissatisfaction and limit negative service evaluations ([22]).Building on these arguments, we predict that individuals who enter a chatbot service interaction in an angry emotional state will respond negatively to chatbot anthropomorphism, whereas individuals in nonangry emotional states will not. While the most immediate negative reaction is likely to manifest in reduced customer satisfaction ratings of the service encounter with the chatbot, this can also carry over to harm more general firm evaluations and result in lower future purchase intentions, which are known consequences of dissatisfaction ([ 3]). Formally, we hypothesize the following: H1:  For angry customers, chatbot anthropomorphism has a negative effect on (a) customer satisfaction, (b) company evaluation, and (c) purchase intention. This negative effect does not manifest for customers in nonangry emotional states. H2:  Chatbot anthropomorphism leads to inflated expectations of chatbot efficacy, which, for angry customers, results in the negative effect described in H1.Our proposed conceptual framework is illustrated in Figure 1. Across five studies, using a combination of real-world and experimental data, we test the different parts of our theorizing to collectively support our proposed framework. In Study 1, we analyze a large data set from an international mobile telecommunications company that captures customers' interactions with a customer service chatbot. We use natural language processing (NLP) on chat transcripts and find that for customers exhibiting an angry emotional state during a chatbot-led service encounter, anthropomorphic treatment of the bot has a negative effect on their satisfaction with the service encounter (consistent with H1a). In Study 2, the first of four experiments, we manipulate chatbot anthropomorphism and customer anger and find that angry customers display lower customer satisfaction when the chatbot is anthropomorphic versus when it is not (consistent with Study 1 and H1a). Study 3 shows that the negative effect extends to company evaluations (H1b) but not when the chatbot effectively resolves the problem. Study 4 shows that the negative effect of chatbot anthropomorphism for angry customers extends further to reduce customers' purchase intentions (H1c) and provides evidence that this effect is driven by inflated preinteraction expectations of chatbot efficacy (H2). Finally, Study 5 manipulates preinteraction expectations and demonstrates that the negative effect dissipates when people have lower expectations of anthropomorphic chatbots (further supporting H2).Graph: Figure 1. Illustration of proposed model. Study 1Study 1 analyzes a real-world data set from an international mobile telecommunications company capturing customers' interactions with a customer service chatbot. The chatbot was available via the company's website and mobile app and was a text-only bot driven by machine learning, specifically, advanced NLP. The chatbot was highly anthropomorphic; the avatar was a cartoon illustration of a young female avatar with long hair, makeup, and modern casual clothing. Her name appeared in the chat, and customers could visit a profile webpage with her bio describing her personality and listing some of her likes and dislikes.The main purpose of the study was to examine how treating a chatbot as more or less human (i.e., higher or lower anthropomorphic treatment) impacted customer satisfaction with the encounter and, critically, whether this effect was moderated by customer anger (i.e., H1a). Because the chatbot was anthropomorphic and this could not be varied experimentally, we focused on the anthropomorphic treatment of the chatbot. If a customer treats a chatbot in a more human-consistent way, then we assume that is a consequence of a customer having more anthropomorphic thoughts resulting from perceiving the chatbot as more anthropomorphic. Specifically, we operationalized anthropomorphic treatment as the extent to which customers used the chatbot's name in their text-based conversation. As a name makes an object more anthropomorphic ([68]), the use of the chatbot's name indicates treating it as more human and serves as a reasonable proxy for anthropomorphic treatment. Data and MeasuresData were provided by a major international mobile telecommunications company. The data set covers 1,645,098 lines of customer text entries from 461,689 unique customer chatbot sessions that took place between September 2016 and August 2017 in one European country served by this company. At the end of each session, customers were given the option to rate their satisfaction with the chatbot encounter from one to five stars. Approximately 7.5% of sessions were rated (34,639 out of 461,689). In addition, for each line of customer text entered, there were metadata from the underlying chatbot NLP system that indicated the system's confidence in it having correctly ""understood"" each line of customer input, which was expressed as a percentage and termed the ""bot recognition rate."" We used the 1–5 satisfaction rating as the dependent variable. The distribution of this variable is shown in Figure 2, and the mean (SD) satisfaction rating was 2.16 (.79). We controlled for quality of the chatbot experience using the bot recognition rate, drawing on the assumption that for a given chatbot session, a higher average and lower variance in recognition rate indicated that the chatbot consistently understood more of a customer's inputs, which likely meant that the customer had an overall better communication experience.Graph: Figure 2. Distribution of user satisfaction ratings following interaction with anthropomorphic service chatbot.We processed chat transcript data (i.e., unstructured text) using the dictionary-based Linguistic Inquiry and Word Count (LIWC) package[ 4] ([52]) to classify each consumer text entry with respect to anger and to build our measure of the extent to which each customer treated the chatbot anthropomorphically. AngerIn line with our theorizing, anger was the key emotion in customers' inputs to the chatbot. Our measure of anger was the corresponding LIWC item (""anger"") that indicates the proportion of words in the input that are classified as being associated with anger. To arrive at the session-level measure, we averaged the LIWC anger value of each customer input within a given session. Unfortunately, this implicitly assumes that anger is exogenous by ignoring the initial emotional state of the customer and the dynamics of the consumer–bot exchange. We subsequently examine the robustness of our results when relaxing this assumption. Anthropomorphic treatmentThe chatbot was anthropomorphic because it had been endowed with extensive humanlike features (e.g., name, avatar, likes/dislikes). As we had no control over this, we instead aimed to measure anthropomorphic treatment, or the extent to which customers treated the chatbot in a humanlike manner.There are several possible measures to derive for anthropomorphic treatment. Our approach was to use a simple measure based on the frequency of use (or nonuse) of the chatbot's name, assuming that if a customer used the chatbot's name, then they were treating it in a more humanlike manner than if they did not use it. Thus, our measure of anthropomorphic treatment was the total number of times in a customer's chatbot session that they used the chatbot's name. Repetition of the chatbot's name may also be an implicit acknowledgement by a customer of the chatbot's agency, which is another key indicator of humanlike treatment. Examples of this are customer inputs such as, ""Hello [Bot Name], my name is [Customer Name], can you please help me with my bill?"" and finishing a conversation with ""Thank you [Bot Name]."" The mean (SD) of this count was.032 (.178), ranging from zero to six times the bot's name was used per user session. Bot recognition rateAs noted previously, the chatbot's NLP system produced a confidence value, expressed as a percentage, of the likelihood that it correctly understood customer text input. The average and standard deviation of these values within a user session provide control variables for the quality and consistency of that customer experience. The mean (SD) of this value was 73.09 (23.6). Number of interactionsAs a control, we captured the number of times the customer interacted with the chatbot in each session (i.e., the number of customer inputs per session). The mean (SD) was 3.56 (4.21), with a range of 1 to 491. Chatbot interaction typeFinally, we controlled for the type of interaction the customer had with the service chatbot. This categorization was used by the chatbot itself, retained as metadata, connecting the requests received with a broad categorization of different types of service encounters: General Dialog, Questions and Answers, Providing Links, Frequently Asked Questions, and Feedback. Examples of individual dialogs include ""Invoices,"" ""SIM Card Activation,"" and ""PIN Recovery."" AnalysisOur goal was to estimate the extent to which anthropomorphic treatment, anger, and their interaction affected customer satisfaction. Considering that satisfaction was measured on a 1–5 scale (with five being the highest level of satisfaction), but with a great deal of mass in the distribution at the scale midpoint and both endpoints, we treated this as an ordinal variable and analyzed it using an ordinal logistic regression. Thus, we accounted for the potential for heterogeneity in distances between scale points. Vitally, we econometrically handled the obvious potential for a selection bias because only 7.5% of all customer chat sessions in our data set included a satisfaction rating. Thus, our analysis is based on the 34,639 chat sessions for which we had a satisfaction measure, but we make use of all 461,689 chat sessions in our treatment of endogenous sample selection.To account for the ordinal nature of our data and the sample selection, we used an extended ordinal probit model, which estimates the probit selection and the ordinal satisfaction ratings equations simultaneously and with correlated errors ([26]).[ 5] The first equation was a binary probit model for leaving a satisfaction rating ( 1) or not (0), as described in Equation 1 (with i denoting the chat session and error esi). The second equation was an ordered probit model, as shown in Equation 2 (with i denoting the chat session and error ei). The error terms in Equations 1 and 2 were correlated. Note that in Equation 1 we used bot interaction type as an exclusion restriction because it impacts the likelihood of leaving a rating, P(Feedback = 1)i, but not the satisfaction rating, Ratingi. P(Feedback=1)i=α0+α1(AnthropomorphicTreatment)i+α2(Anger)i+α3(BotLanguageRecognitionRate)i+α4(BotLanguageRecognitionVariance)i+α5(NumberofInteractionswithBot)i+α6(BotInteractionType)i+esi, Graph( 1) Ratingi=β0+β1(AnthropomorphicTreatment)i+β2(Anger)i+β3(AnthropomorphicTreatment×Anger)i+β4(BotLanguageRecognitionRate)i+β5(BotLanguageRecognitionVariance)i+β6(NumberofInteractionswithBot)i+ei. Graph( 2) ResultsTable 1 reports descriptive statistics and correlations. Table 2 reports results from the model described previously. First, considering the selection model in Table 2, we see that the type of customer–bot interaction (which we use as an exclusion restriction) is a significant predictor of the likelihood of the customer providing feedback to the firm, especially when, compared with general conversation (the base case in the model), the bot is focused on eliciting feedback (α6-feedback = 2.973, p <.001). Anger had a negative and significant effect on the likelihood of providing feedback (α2 = −.008, p <.05), whereas the number of exchanges with the bot during the session had a positive and significant effect (α5 = .033, p <.001).GraphTable 1. Descriptive Statistics in Study 1. MeanSD1234561. Rating2.165.799——————2. Anthropomorphic treatment.031.177.05—————3. Anger.0641.706.06.05————4. Bot language recognition rate73.0923.60.00.07.01———5. Bot language recognition variance21.1619.39.04.09.02.01——6. Number of interactions with bot3.5634.218.01−.03−.01.02−.02— 1 Notes: Boldface indicates p < .05.GraphTable 2. Probit Selection Model—Likelihood of Customer Providing a Rating After Interaction and Ordinal Probit—Customer Rating (Study 1). VariableCoefficientz-ValueIntercepts1|2−.0215−.152|31.0421***6.043|41.7599***9.244|51.8442***9.56Rating: Second-Stage ModelAnthropomorphic treatment−.0554−1.00Anger−.0015−.07Anthropomorphic treatment × Anger−.1665*−1.96Bot language recognition rate.0126***12.81Bot language recognition variance.0132***17.30Number of interactions with bot.0064†1.71P(Feedback): First-Stage ModelAnthropomorphic treatment.0152.65Anger−.0080*−2.16Bot language recognition rate.0002.97Bot language recognition variance−.0051***−17.05Number of interactions with bot.0334***18.27Interaction type FAQ.7680***19.77 Feedback2.9731***6.84 Link−.2184−1.46 Question and answer.0881***3.13Constant−1.6001***−48.03ρ−.4921***−10.86Number of observations461,689Akaike information criterion74,082 2 †p < .10.3 *p < .05.4 **p < .01.5 ***p < .001.Next, considering the main model for satisfaction ratings in Table 2, after accounting for the likelihood of providing feedback, both main effects of anthropomorphic treatment and anger were nonsignificant (β1 = −.055, n.s.; β2 = −.002, n.s.). However, their interaction was significant and negative (β3 = −.167, p = .05). This was after controlling for the technical performance of the bot during that session (recognition rate mean and variance) and the number of customer interactions in a session. Probing this interaction revealed the hypothesized effects across the distribution of consumer anger scores. When anger is higher (1 SD above the mean), the marginal effect of anthropomorphic treatment on satisfaction rating is significant and negative (β1 = −.350, p = .02), consistent with H1a. Interestingly, we also found that when anger was lower, but still present (1 SD below the mean), the marginal effect of anthropomorphic treatment on satisfaction rating remained negative, albeit with a smaller effect size than in the higher anger case (β1 = −.329, p = .02). Thus, it appears that the mere presence of anger can result in a negative relationship between anthropomorphic treatment and satisfaction. Further probing this interaction, we found that when anger was zero (i.e., not at all present), the marginal effect of anthropomorphic treatment on satisfaction rating was nonsignificant (β1 = .011, p = .32). Robustness ChecksWe were restricted in our analysis of Study 1 given that the data were provided as an outcome of the firm's operations and the conditions around each customer could not be assigned or manipulated. We could not control whether a customer provided feedback, the level of anthropomorphic treatment, or the customers' level of anger upon entering the chatbot interaction. While the first two limitations were addressed with a selection model and by exploiting variance in exhibited behavior, the levels of anger were taken as a given.As a robustness check, we instead considered anger as a binary treatment effect and estimated an additional augmented model that accounted for the ordinal nature of ratings, the selection bias in providing feedback, and the anger of customers as a treatment condition. The inherent weakness of this model comes from a loss of information by dichotomizing anger into a binary (angry/not angry) condition, thus losing the nuance of levels of anger interacting with the level of anthropomorphic treatment. However, as a check, it shows the robustness of results to the specification of anger as an endogenous component of the customer experience, and for the estimation of drivers of anger, again with correlated errors (for the outcome, selection, and anger).The model and results are presented in Web Appendix B. The endogenous anger treatment was positively but weakly correlated with the decision to provide feedback (r = .047) and negatively correlated with satisfaction rating (r = −.449). In the endogenous anger condition model, anthropomorphic treatment was positive and significant (γ1 = .289, p < .001). Critically, in the model for satisfaction rating, anthropomorphic treatment was nonsignificant for customers in the nonangry treatment (β1a = −.059, n.s.) and negative and significant for customers in the angry treatment (β1b = −.573, p = .04). We confirmed the significant difference between those two groups using a Wald test (χ2( 2) = 6.62, p = .04), which highlights that even if we model anger as a binary outcome of an endogenous process, our conclusions remain essentially the same.In addition, we test whether alternative negative emotions (anxiety and sadness) or a positive mood valence meaningfully explain our customer satisfaction ratings or interact with the degree of chatbot anthropomorphic treatment. A reestimated extended ordinal probit model containing additional emotions is presented in Web Appendix C. While sadness is a significant predictor in our first stage model of feedback selection, no other negative emotions were significant. However, positive valence interacts meaningfully with anthropomorphic treatment (β = .0508, p = .001) in explaining satisfaction, consistent with prior research showing positive consequence of anthropomorphism. But, importantly, the hypothesized effect between anger and anthropomorphic treatment remains unchanged (β = −.1617, p = .05). DiscussionBy leveraging real-world data from customers actively engaging with a chatbot across numerous chat sessions, we find initial evidence in support of H1a. An increase in the average level of anger exhibited by the consumer during their session resulted in a lower level of satisfaction with the service encounter, but only when the chatbot was treated anthropomorphically. In situations where the bot was not treated anthropomorphically, higher levels of anger did not meaningfully affect consumer satisfaction. Of course, this study has limitations. First, all customers were presented with the same highly anthropomorphic chatbot, so we had to rely on the variance in customers' anthropomorphic treatment of the bot, as opposed to variation in chatbot anthropomorphism, per se. Second, we initially assumed that customers entered the chat angry, independent from their exchange with the chatbot; however, our robustness checks confirm that anger is not strictly exogenous but also arose out of characteristics of the exchange with the bot (e.g., the number of exchanges, variance in language recognition). Finally, both anthropomorphic treatment and anger were measured from customer behaviors, rather than manipulated. These limitations motivated the four follow-up experiments. Study 2The purpose of Study 2 was to test our theory under a controlled experimental setting and further show that, for angry customers, chatbot anthropomorphism has a negative effect on customer satisfaction. Accordingly, this study manipulated both chatbot anthropomorphism (via the presence/absence of anthropomorphic traits in the chatbot) and customer anger, allowing us to infer a causal relationship on satisfaction. In addition, careful chatbot selection enabled us to rule out idiosyncratic features of the Study 1 chatbot. Specifically, two of its specific features pose potential confounds in trying to generalize the results. First, it was clearly female, and previous research suggests that female service employees are more often targets of expressed frustration and anger from customers than are male service employees ([59]). In addition, she had a smiling expression, which is incongruent with the emotional state of participants who were angry, and such affective incongruity may cause a negative reaction and lower satisfaction. Pretests AvatarWe pretested avatars to select one that was both gender and affectively neutral. Twenty-five participants from Amazon Mechanical Turk (MTurk) evaluated a series of avatars on bipolar scales assessing both gender (""definitely male–definitely female"") and warmth (""extremely cold–extremely warm"") and indicated their agreement with one seven-point Likert item: ""This avatar has a neutral expression."" Drawing on the results of this pretest, we used the avatar pictured in Web Appendix D for the anthropomorphic chatbot condition. Specifically, our analysis confirmed that this avatar was neutral in both gender and warmth, with scores that did not significantly differ from the corresponding scale midpoints (Mgender = 3.64, t(24) = −1.23, p = .23; Mwarmth = 3.80, t(24) = −1.16, p = .26) and agreement with the neutral expression item was significantly above the midpoint (M = 5.76, t(24) = 7.80, p < .001).We also wanted to choose a gender-neutral name for the anthropomorphic chatbot. Twenty-seven participants from MTurk evaluated a series of names on a seven-point bipolar scale assessing gender (""definitely male–definitely female""). From the results, we chose the name ""Jamie,"" which was not significantly different from the midpoint (M = 3.89, t(26) = −.68, p = .50). ScenarioWe created two customer service scenarios (neutral vs. anger) to use in this study (for the full scenarios used in both conditions, see Web Appendix E). In the neutral condition, the scenario described how the participant had purchased a camera for an upcoming trip, but upon receipt, the camera was broken. After searching the website, they diagnosed the issue as a problem with the lens and read about how to exchange the camera. It must be mailed back to the company before receiving a new camera, which is expected to arrive after they depart for a trip, the reason they wanted it originally.In the anger condition, the scenario contained additional details designed to evoke anger. The original camera shipping was delayed, diagnosing the issue was time consuming, and they already tried to contact customer service and were placed on hold and passed from one representative to another. To ensure this scenario was successful in invoking anger compared with the neutral condition but did not differ in realism, we conducted a pretest on MTurk. Fifty participants were randomly assigned to read one of the two scenarios and then indicated how angry the scenario would make them feel (two items: ""This situation would leave me feeling angry [frustrated]""; r = .65) and how realistic they found the scenario (two items: ""How realistic [true-to-life] is this scenario?""; r = .61) on seven-point Likert scales (1 = ""not at all,"" 7 = ""extremely""). Our analysis confirmed that those in the angry condition reported significantly greater feelings of anger than those in the neutral condition (Mneutral = 5.46 vs. Manger = 6.06; F( 1, 48) = 4.31, p = .04). However, there was no significant difference in scenario realism (Mneutral = 5.48 vs. Manger = 5.69; F( 1, 48) = 2.09, p = .16). AnthropomorphismWe created two versions of the customer service chatbot (control vs. anthropomorphic). In the control condition, participants were told they would interact with ""the Automated Customer Service Center,"" and in the anthropomorphic condition, they were told they would interact with ""Jamie, the Customer Service Assistant."" Furthermore, in the anthropomorphic condition, the chatbot featured the avatar selected from the pretest, and the chat text consistently used a singular first-person pronoun (i.e., ""I"") and appeared in quotation marks.To ensure that this manipulation was successful, we conducted a pretest on MTurk. One hundred one participants were randomly assigned to one of the two chatbots and then indicated how anthropomorphic the chatbot was on nine seven-point Likert scales (adapted from [19]] and [38]]: ""Please rate the extent to which [the Automated Customer Service Center/Jamie]: came alive (like a person) in your mind; has some humanlike qualities; seems like a person; felt human; seemed to have a personality; seemed to have a mind of his/her own; seemed to have his/her own intentions; seemed to have free will; and seemed to have consciousness""; α = .98). Analysis confirmed that those in the anthropomorphic condition reported significantly greater anthropomorphic thought (Mcontrol = 3.37 vs. Manthro = 4.82; F( 1, 99) = 16.64, p < .001). Main Study Design and ProcedureTwo hundred one participants (48% female; Mage = 37.29 years) from MTurk participated in this study in exchange for monetary compensation. The study consisted of a 2 (chatbot: control vs. anthropomorphic) × 2 (scenario emotion: neutral vs. anger) between-subjects design. Participants were randomly assigned to read one of the aforementioned scenarios (neutral or anger). Then, participants entered a simulated chat with either ""the Automated Customer Service Center"" in the control chatbot condition or ""Jamie, the Customer Service Assistant"" in the anthropomorphic condition.In the simulated chat, participants were first asked to open-endedly explain why they were contacting the company. In addition to serving as an initial chatbot interaction, this question also functioned as an attention check, allowing us to filter out any participants who entered nonsensical (e.g., ""GOOD"") or non-English answers ([17]). Subsequently, participants encountered a series of inquiries and corresponding drop-down menus regarding the specific product they were inquiring about (camera) and issue they were having (broken and/or damaged lens). They were then given return instructions and indicated they needed more help. Using free response, they described their second issue and answered follow-up questions from the chatbot about the specific delivery issue (delivery time is too long) and reason for needing faster delivery (product will not come in time for a special event). Finally, participants were told that a service representative would contact them to discuss the issue further. The interaction outcome was designed to be ambiguous (representing neither a successful nor failed service outcome; however, we manipulate this outcome in Study 3). The full chatbot scripts for both conditions and images of the chat interface are presented in Web Appendices F and G.Upon completing the chatbot interaction, participants indicated their satisfaction with the chatbot by providing a star rating (a common method of assessing customer satisfaction; e.g., [61]) between one and five stars, on five dimensions (α = .95): overall satisfaction, customer service, problem resolution, speed of service, and helpfulness. Lastly, participants indicated their age and gender and were thanked for their participation. Results and DiscussionFour participants failed the attention check (entering a nonsensical response for the open-ended question), leaving 197 observations for analysis. Analysis of variance (ANOVA) results revealed a significant main effect of scenario emotion on satisfaction (i.e., averaged star rating on the five dimensions), in that those in the anger scenario condition were less satisfied than those in the neutral scenario condition (F( 1, 193) = 33.45, p < .001). Importantly, we found a significant chatbot × scenario emotion interaction on customer satisfaction (F( 1, 193) = 5.26, p = .02). Consistent with Study 1, a simple effects test revealed that participants in the anger scenario condition were less satisfied when the chatbot was anthropomorphic (M = 2.09) versus when it was not (M = 2.58; F( 1, 193) = 4.13, p = .04). For those in the neutral scenario, chatbot anthropomorphism had no significant influence on satisfaction, but satisfaction was directionally higher in the anthropomorphic condition (Mcontrol = 3.16 vs. Manthro = 3.44; F( 1, 193) = 1.46, p = .23). Figure 3 presents an illustration of means.Graph: Figure 3. The effect of chatbot anthropomorphism and anger on customer satisfaction (Study 2).Whereas Study 1 provides initial support for the interactive effect of anthropomorphism and anger on customer satisfaction, Study 2 tests our theorizing in a controlled experimental design. This allowed us to more definitively conclude that when customers are angry, anthropomorphic traits in a chatbot lower customer satisfaction with the chatbot (consistent with H1a)[ 6] and rule out alternative explanations based on the chatbot's gender or expression. While not central to our main theorizing, we ran an identical study manipulating sadness instead of anger. Both anger and sadness are negative emotions, but anger represents an activating emotion, whereas sadness is a deactivating emotion ([15]; [41]). We predicted that only angry customers are activated to respond negatively to anthropomorphic chatbots due to their need to overcome obstacles, blame others, and respond punitively to expectancy violations ([23]; [43]; [42]). Interestingly, participants in the sad condition were more satisfied when the chatbot was anthropomorphic versus when it was not (Mcontrol = 1.90 vs. Manthro = 2.53; F( 1, 188) = 8.49, p < .01), which is consistent with prior literature ([27]; [72]) that demonstrates the positive effect of anthropomorphic chatbots in other situations. Full details are available in Web Appendix I. Study 3There were three main goals of Study 3. First, while our previous study induced anthropomorphism via a simultaneous combination of visual and verbal cues (with an avatar and first-person language, respectively), the current study aimed to show that the effect diminishes with the reduction of anthropomorphic traits. Thus, we remove the visual trait of anthropomorphism (i.e., the avatar) and anticipate the negative effect of anger to attenuate, providing further support that the degree of humanlikeness is responsible for driving the effect. Second, we wanted to test H1b by exploring whether the negative effect of anthropomorphism for angry customers would extend to influence their evaluations of the company itself. Finally, we wanted to provide initial evidence that expectancy violations are responsible for these observed negative effects. To do so, we examined whether the outcome of the chat interaction—namely, if the chatbot was able to indubitably resolve the customer's concerns—could serve as a boundary condition. We predicted that if the chatbot could meet the high expectations of efficacy, the negative effect of anthropomorphism should dissipate. Pretests AvatarWe selected a new avatar in this study to increase the robustness of our examination and generalizability of our findings. Twenty-six participants from MTurk evaluated a series of avatars as in the Study 2 pretest. Our analysis confirmed that the avatar (pictured in Web Appendix D) was considered neutral in both gender and warmth (Mgender = 4.04, t(25) = .12, p = .90; Mwarmth = 4.27, t(25) = 1.32, p = .20) and had a neutral expression (M = 5.12, t(25) = 4.35, p < .001). ScenarioAs in Study 2, we created two customer service scenarios (neutral vs. anger; for the full scenarios, see Web Appendix J). In the neutral condition, the scenario described a situation where the participant was interested in buying a camera from the company, ""Optus Tech,"" with a specific feature (advanced video stabilization). After searching the website, it was difficult to tell whether Optus Tech's camera had this feature. In addition, the delivery window was wide, which meant that the expected delivery may or may not occur after they depart for a trip, the whole reason they wanted the camera.In the anger emotion condition, there were additional details designed to evoke anger: researching the feature was time consuming, they already tried to contact customer service and were placed on hold and passed from one representative to another, the representative could not answer their question, and they had to call a second time to ask about shipping. To ensure that this scenario was successful in invoking anger compared with the neutral condition, we pretested 48 MTurk participants who were randomly assigned to read one of the two scenarios and then indicated both their anticipated feelings of anger and how realistic they found the scenario (as measured in the Study 2 pretest; r = .77 and r = .63, respectively). Indeed, those in the anger condition reported significantly greater anticipated feelings of anger than those in the neutral condition (Mneutral = 3.63 vs. Manger = 5.46, F( 1, 46) = 18.00, p < .001). However, there was no significant difference in how realistic participants found the two scenarios (Mneutral = 5.50 vs. Manger = 4.92, F( 1, 46) = 2.54, p = .12). We only used the anger scenario in this study, but an upcoming study used both scenarios. AnthropomorphismWe created three versions of the customer service chatbot: control, verbal anthropomorphic, and verbal + visual anthropomorphic. The first and last chatbots were similar to Study 2 except using the new avatar. The additional chatbot used the verbal anthropomorphic traits (i.e., the bot introduced itself as Jamie and used first-person language in quotations) but not the visual trait (i.e., the avatar). One hundred twenty-one MTurk participants were randomly assigned to one of the three chatbots: the Automated Customer Service Center (control chatbot condition), Jamie without an avatar (verbal anthropomorphic condition), or Jamie with an avatar (verbal + visual anthropomorphic condition) and then indicated how anthropomorphic the chatbot was on nine seven-point Likert scales (as in Study 2; α = .97). We coded the control, verbal anthropomorphic, and verbal + visual anthropomorphic conditions as 0, 1, and 2, respectively, to represent the strength of the anthropomorphic manipulation. Results demonstrated that the linear trend was significant (Mcontrol = 2.38 vs. Mverbal = 4.14 vs. Mverbal + visual = 4.39; F( 1, 118) = 39.16, p < .001). Main Design and ProcedureFour hundred nineteen participants (61% female; Mage = 38.50 years) from MTurk participated in this study in exchange for monetary compensation. This study consisted of a 3 (chatbot: control vs. verbal anthropomorphic vs. verbal + visual anthropomorphic) × 2 (scenario outcome: ambiguous vs. resolved) between-subjects design. First, all participants read the anger scenario and then entered a simulated chat with the chatbot. In the ambiguous outcome condition, participants encountered a series of questions and corresponding drop-down menus regarding the specific product (camera) and feature (advanced video stabilization) they were inquiring about. They were then given basic product information about the feature that was purposefully ambiguous. Then, participants indicated they needed more help. Using free response, they described their second issue and answered follow-up questions from the chatbot about the specific delivery issue (delivery window/timing) and reason for needing faster delivery (product will not come in time for a special event). Participants were told that a service representative would contact them to discuss the issue further. In the resolved outcome condition, there were two critical differences: participants were directly given the product feature information that resolved their query and explicitly informed of the specific delivery time information, which confirmed they would receive their delivery in time for their special event. The entire chatbot scripts for both conditions and images of the interface are presented in Web Appendices K and L.Upon completing the interaction, participants evaluated the company, Optus Tech, on four seven-point bipolar items (α = .95): ""unfavorable–favorable,"" ""negative–positive,"" ""bad–good,"" and ""unprofessional–professional."" As a manipulation check for the scenario outcome, participants responded to three items: ""My question was sufficiently answered,"" ""My problem was appropriately resolved,"" and ""I got the help I needed"" (1 = ""strongly disagree,"" and 7 = ""strongly agree""; α = .97). To assess whether participants knew they were interacting with a chatbot (vs. a human), we asked participants to indicate the extent to which they felt they interacted with a human versus an automated chatbot (1 = ""definitely a real live human,"" 7 = ""definitely an automated chatbot"".[ 7] Participants indicated demographics and were thanked for participating. Results and DiscussionFifty-two participants failed the attention check (entering a nonsensical response for the open-ended question), leaving 365 observations for analysis. Scenario outcome manipulation checkParticipants in the resolved condition indicated that their problem was more appropriately resolved (M = 6.36) than participants in the ambiguous condition (M = 4.32; t(363) = 12.40, p < .001), indicating a successful manipulation. Main analysisUnsurprisingly, ANOVA results revealed a significant main effect of scenario outcome on company evaluation, such that participants reported lower evaluations of the company when the outcome was ambiguous (M = 4.68) versus when it was resolved (M = 5.36; F( 1, 359) = 18.44, p < .001). There was no main effect of chatbot anthropomorphism (F( 1, 359) = 1.38, p = .25). Importantly, there was a marginally significant chatbot anthropomorphism × anger scenario interaction on company evaluation (F( 2, 359) = 2.64, p = .07). A simple effects test revealed that there was no significant difference between the chatbot conditions when the outcome was resolved (F < 1). This provides some evidence that effectively meeting expectations eliminates the negative effect, which is conceptually consistent with H2, because if high preinteraction expectations of efficacy are met, there should be no resultant expectancy violations. However, there was a significant difference when the outcome was ambiguous (F( 2, 359) = 3.78, p = .02). Planned contrasts revealed when the outcome was ambiguous, participants reported lower company evaluations when the chatbot was verbally and visually anthropomorphic (M = 4.28) versus the control (M = 5.06; t(359) = 2.75, p < .01), providing evidence in support of H1b. However, the verbal anthropomorphic condition (without an avatar) did not significantly differ from the control (Mverbal = 4.69 vs. Mcontrol = 5.06; t(359) = 1.36, p = .17) or the verbally and visually anthropomorphic condition (vs. Mverbal + visual = 4.28; t(359) = 1.48, p = .14). Because the verbal anthropomorphic condition both theoretically and empirically fell between the two other conditions, we tested whether our anthropomorphism manipulation demonstrated a linear trend. We coded the control, verbal anthropomorphic, and verbal + visual anthropomorphic conditions as 0, 1, and 2, respectively, to represent the strength of the anthropomorphic manipulation. Results demonstrated that the linear trend was not significant when the outcome was resolved (F < 1) but was significant when the outcome was ambiguous (F( 1, 359) = 7.55, p < .01). These findings suggest that visual and verbal anthropomorphic traits likely produce an additive effect, where multiple traits lead to greater anthropomorphic thought, and accordingly results in lower company evaluations (at least in the case of angry consumers, which we exclusively examined in this study). Figure 4 presents an illustration of means.Graph: Figure 4. The effect of chatbot anthropomorphism and anger on company evaluation (Study 3). Study 4Study 4 serves two key purposes. First, we extend our investigation to an even further downstream negative outcome by examining purchase intentions (H1c). Second, we build on the findings of Study 3 and directly test our proposed underlying process: expectancy violations driven by preperformance expectations (H2). Specifically, we predict anthropomorphism increases preperformance expectations that a chatbot would display greater agency and performance. While people in a neutral state will perceive the expectancy violation, they are less motivated to retaliate or respond punitively. Angry people, in contrast, punish the company by lowering their purchase intentions. Design and ProcedureOne hundred ninety-two participants (55% female; Mage = 37.31 years) from MTurk participated in exchange for payment. This study consisted of a 2 (chatbot: control vs. anthropomorphic) × 2 (scenario emotion: neutral vs. anger) between-subjects design. Participants were randomly assigned to read one of the neutral or anger information search scenarios pretested in the prior study. Then participants were told they were about to enter a chat with either the Automated Customer Service Center (control condition) or Jamie (anthropomorphic condition). At this point, all participants saw the brand logo for Optus Tech, but those in the anthropomorphic chatbot condition also saw the avatar.Next, participants indicated their preinteraction efficacy expectations regarding the chatbot's upcoming performance on four seven-point Likert items (""I expect the Automated Service Center/Jamie to: do something for me; take action; be proactive in resolving my issues; say things to calm me down""; α = .89). Participants completed the same interaction as in the ambiguous condition from Study 3 and indicated their purchase intentions for the camera on two seven-point Likert items: ""I would buy the camera from Optus Tech,"" and ""I would try to find a different company to buy the camera from"" (the latter was reverse-coded; r = .65). Afterward, participants rated their postinteraction assessment of the chatbot's efficacy, on four seven-point Likert items that corresponded to the preinteraction items (""I felt the Automated Service Center/Jamie: did a lot for me; took action; was proactive in resolving my issues; said things to calm me down""; α = .92). Lastly, participants indicated their age and gender and were thanked for their participation. Results and Discussion Purchase intentionTwenty-one participants failed the attention check used in prior studies, leaving 171 observations for analysis. ANOVA results revealed a significant main effect of anger on purchase intentions, where participants in the anger scenario condition reported lower purchase intentions than those in the neutral scenario condition (F( 1, 167) = 20.04, p < .001). Consistent with the pattern of results predicted in H1c, there was a significant chatbot anthropomorphism × anger scenario interaction on purchase intentions (F( 1, 167) = 4.29, p = .04). A simple effects test revealed that participants in the anger scenario condition reported lower purchase intentions when the chatbot was anthropomorphic (M = 2.73) versus when it was not (M = 3.57; F( 1, 167) = 5.79, p = .02). For those in the neutral scenario, the chatbot had no significant influence on purchase intentions. Figure 5 presents an illustration of means.Graph: Figure 5. The effect of chatbot anthropomorphism and anger on purchase intentions (Study 4). Expectancy violationWe predicted that encountering an anthropomorphic chatbot at the start of the service experience would increase participants' preinteraction expectations about the efficacy of the chatbot, relative to the control chatbot. However, the postinteraction efficacy assessments of the chatbots should not differ because they performed equally, resulting in greater expectancy violations for anthropomorphic chatbots (H2).To assess this hypothesis, we ran a repeated-measures ANOVA with chatbot anthropomorphism as the between-subjects variable and time (preinteraction expectations at Time 1 and postinteraction assessments at Time 2) as the within-subjects factor. We did not find a significant overall main effect of chatbot anthropomorphism on efficacy (F( 1, 169) = .91, p = .34). Importantly, there was a significant interaction of chatbot anthropomorphism and time (F( 1, 169) = 7.31, p = .01). Probing this interaction, as we expected, preinteraction expectations of the chatbot's efficacy at Time 1 were significantly higher in the anthropomorphism condition than in the control condition (Mcontrol = 4.94 vs. Manthro = 5.50; F( 1, 169) = 6.91, p = .01), but there was no difference in the postinteraction assessments at Time 2 (Mcontrol = 4.09 vs. Manthro = 3.88; F < 1). These results are consistent with the logic that a greater expectancy violation is more likely in the anthropomorphism condition than in the control because of inflated preinteraction expectations of chatbot efficacy stemming from more humanized traits.We also calculated an expectancy violation score for each participant by subtracting their postinteraction assessment score at Time 2 from their preinteraction expectation score at Time 1 ([45]). As we expected, an ANOVA with chatbot anthropomorphism and anger scenario as predictors and expectancy violation as the dependent variable produced only a significant main effect of chatbot anthropomorphism on expectancy violation (Mcontrol = .85 vs. Manthro = 1.62; F( 1, 169) = 7.31, p = .01). MediationImportantly, our theorizing suggests that anthropomorphism inflates preinteraction expectations of chatbot efficacy for all customers. Yet, angry customers are more motivated than nonangry customers to respond punitively by lowering their purchase intent. Accordingly, we performed a moderated mediation analysis based on 10,000 bootstrapped samples ([29], Model 15). While the index of moderated mediation did not reach significance (indirect effect = .0279; 95% confidence interval [CI]: [−.0778,.1562]), we examined the separate indirect effects at each emotion condition based on our a priori predictions ([ 2]; [30]). In other words, while we did not have predictions for what might drive purchase intention for those in the neutral condition, we did predict that for angry customers, lowered preinteraction expectations would explain the decreased purchase intention. As per our theorizing, results demonstrated that for individuals in the anger condition, preinteraction expectations mediated the effect of chatbot anthropomorphism on purchase intention (indirect effect = .0675; 95% CI: [.0012,.1707]). However, for participants in the neutral condition, the indirect effect was not significant (indirect effect = .0396; 95% CI: [−.0408,.1468]). These results suggest that, as we predicted, the inflated preinteraction expectation of efficacy caused by the anthropomorphic chatbot is the underlying mechanism lowering purchase intentions for angry participants. Study 5Study 4 demonstrated that anthropomorphic chatbots result in lower purchase intentions when customers are angry by elevating preinteraction expectations of efficacy. Yet, it is theoretically and managerially important to understand how this effect can be remedied. Some companies attempt to explicitly temper customer expectations of their chatbots. For example, Slack's chatbot introduces itself by explaining that, ""I try to be helpful (But I'm still just a bot. Sorry!)"" ([65]). Study 5 explores whether explicitly lowering customer expectations of anthropomorphic chatbots prior to the interaction effectively reduces negative customer responses. Avatar PretestFor the Study 5 pretest, 31 participants from MTurk evaluated a series of avatars as in the prior pretests. Our analysis confirmed that the avatar (see Web Appendix D) was considered neutral in both gender and warmth (Mgender = 5.55, t(30) = 1.52, p = .14; Mwarmth = 4.97, t(30) = −.19, p = .85) and had a neutral expression (M = 5.94, t(30) = 8.91, p < .001). Design and ProcedureThree hundred two participants (52% female; Mage = 40.78 years) from MTurk participated in exchange for monetary compensation. The study consisted of a 2 (chatbot: control vs. anthropomorphic) × 2 (expectation: baseline vs. lowered) between-subjects design. All participants read the anger information search scenario. Afterward, participants saw they would chat with either ""the Automated Customer Service Center"" in the control or ""Jamie, the Customer Service Assistant"" in the anthropomorphic chatbot condition. In the lowered expectation condition, they also read, ""The Automated Customer Service Center/Jamie, the Customer Service Assistant will do the best that it/I can to take action but sometimes the situation is too complex for it/me (it's/I'm just a bot) so please don't get your hopes too high.""Participants then indicated their preinteraction efficacy expectations (as in Study 4; α = .89), completed the product information interaction and evaluated the company (as in Study 3; α = .97), rated their postinteraction assessment of the chatbot's efficacy (as in Study 4; α = .92), answered demographic questions, and were thanked for their participation. Results and Discussion Expectancy violation manipulation checkConsistent with the manipulation intention, there was a main effect of anthropomorphism on preinteraction expectations (F( 1, 298) = 4.36, p = .04), where participants had higher expectations when the chatbot was anthropomorphic (M = 4.53) compared with the control (M = 4.18). There was also a main effect of expectations on preinteraction expectations (F( 1, 298) = 45.61, p < .001), where, as we intended, lowering expectations resulted in lower preinteraction expectations (M = 3.78) than in the baseline expectation condition (M = 4.93). There was a significant chatbot × expectation interaction on preinteraction expectations (F( 1, 298) = 7.00, p < .01). Simple effects tests revealed that in the baseline expectation conditions, the people in the anthropomorphic condition had higher expectations of chatbot efficacy than in the control condition (Mcontrol = 4.53 vs. Manthro = 5.34; F( 1, 298) = 11.35, p = .001). Yet, in the low-expectation conditions, there was no difference between the preinteraction expectations of efficacy for the anthropomorphic and control chatbot (Mcontrol = 3.83 vs. Manthro = 3.73; F < 1). For postinteraction evaluations, consistent with predictions, there were no significant differences (i.e., no main effect of anthropomorphism, no main effect of expectation, and no interaction between anthropomorphism and expectation). This indicates preinteraction expectations are responsible for changes to expectancy violations.Expectancy violations were calculated by subtracting postinteraction evaluations from preinteraction expectations, with higher numbers indicating greater violations. There is a main effect of anthropomorphism on expectancy violations (F( 1, 298) = 7.38, p < .01), where participants indicated greater expectancy violations when the chatbot was anthropomorphic (M = .65) compared with the control (M = .10). There was also a main effect of the expectation manipulation on expectancy violations (F( 1, 298) = 36.73, p < .001), where there were greater expectancy violations in the baseline expectation condition (M = .99) compared with the lowered-expectation condition (M = −.24). Importantly, there was also a significant chatbot × expectation interaction on expectancy violations (F( 1, 298) = 13.26, p < .001). As we expected, when participants had the baseline expectation (i.e., no information given), they experienced greater expectancy violations driven by preinteraction expectations when the chatbot was anthropomorphic compared with the control (Mcontrol = .35 vs. Manthro = 1.63; F( 1, 298) = 20.47, p < .001). Yet when participants were told to have lower expectations, there was no difference between the expectancy violations for the anthropomorphic chatbot and the control (Mcontrol = −.14 vs. Manthro = −.33; F < 1). Company evaluationThe ANOVA results revealed a marginal main effect of anthropomorphism on company evaluation; participants reported marginally lower evaluations of the anthropomorphic chatbot (M = 4.11) versus control (M = 4.44; F( 1, 298) = 2.86, p = .09). There was no main effect of expectation (F < 1). There was a significant chatbot × expectation interaction on company evaluation (F( 1, 298) = 4.35, p = .04). Consistent with prior studies, a simple effects test revealed that participants in the baseline expectation condition rated the company lower when the chatbot was anthropomorphic (M = 3.90) versus when it was not (M = 4.63; F( 1, 298) = 4.13, p = .04). For those in the lowered-expectation condition, chatbot anthropomorphism had no significant influence on company evaluations (Mcontrol = 4.25 vs. Manthro = 4.32; F < 1), indicating that lowering customer expectations of anthropomorphic chatbots effectively mitigated the negative effect of anger on company evaluations. Figure 6 presents an illustration of means, and Web Appendix M provides additional analysis.Graph: Figure 6. The effect of chatbot anthropomorphism and expectations on company evaluation (Study 5). General DiscussionThe deployment of chatbots as digital customer service agents continues to accelerate as the underlying machine learning technologies improve and as the practice becomes more common across industries. Customers are increasingly interacting with firms through chatbots, and there has been a significant push for more humanlike versions of such bots. Prior research has begun to demonstrate some negative implications of anthropomorphism in specific situations, including video games ([36]), gambling ([37]), and overcrowded environments ([53]), as well as for some types of people (agency-oriented customers; [38]). Yet, our research is the first to demonstrate the negative effect of anthropomorphism in the wider context of customer service and connect the use of these humanlike chatbots to negative firm outcomes.We find, using a large data set of real-world customer interactions and four experiments, that anthropomorphic chatbots can harm firms. An angry customer encountering an anthropomorphic (s. nonanthropomorphic) chatbot is more likely to report lower customer satisfaction, lower overall evaluation of the firm, and lower future purchase intentions. This negative effect is driven by expectancy violations due to inflated preinteraction expectations of efficacy caused by the anthropomorphic chatbot. Angry customers respond more punitively to these expectancy violations compared with nonangry customers.The decision to anthropomorphize a chatbot is a deliberate and strategic choice made by the firm. The current research shows that this choice has a significant impact on key marketing outcomes for a substantial (and increasing due to the pandemic; [57]; [60]) group of customers: specifically, those who are angry during the service encounter. As such, firms should attempt to gauge whether a customer is angry either before or early in the conversation (e.g., using NLP) and deploy a chatbot with an appropriate level of anthropomorphism or lack thereof. A less precise solution would be to assign nonanthropomorphic chatbots to customer service roles that tend to involve angry customers (e.g., customer complaint centers) while continuing to employ anthropomorphic agents in more neutral or promotion-oriented settings (e.g., searches for product information) due to their previously documented beneficial effects ([27]; [72]) and the current empirical evidence (i.e., Study 1, Web Appendix I). This strategic deployment of chatbots should help firms deliver better chatbot-mediated service experiences. Moreover, appropriate chatbot deployment can improve immediate customer satisfaction, company evaluations, and future purchase intentions (e.g., customer retention).Given our finding that the negative effect of anthropomorphism for angry customers is driven by an expectancy violation due to inflated preinteraction efficacy expectations, another practical implication is that marketers should consider how to frame their customer service chatbots to customers. As our final study shows, if an anthropomorphic chatbot is deployed to angry customers, it is best to downplay its capabilities. Some companies seem to have intuited this, as illustrated by the aforementioned Slack bot example. Similarly, the Poncho weather app told people, ""I'm good at talking about the weather. Other stuff, not so good"" ([65]). Explicitly informing customers that they are conversing with an imperfect chatbot lowers preinteraction efficacy expectations that were inflated by anthropomorphic traits. Yet, this is not obvious to all companies; there are plenty of examples of chatbots that inadvertently increase preinteraction efficacy expectations. For example, Madi, Madison Reed's chatbot, is labeled a ""genius"" ([49]) and Tinka, T-Mobile's chatbot, is given a 18,456 IQ ([47]). Of course, Study 3 shows that meeting the high expectations for service can also reduce the negative impact of anthropomorphism. Thus, by utilizing these strategies, all customers can be handled well via AI technology.Alternatively, firms could transfer angry customers directly to a real live person to assist them, thus avoiding an anthropomorphic chatbot–based expectancy violation entirely. Yet this option incurs additional costs and assumes that the human agent has greater agency and efficacy. While it is plausible that human agents will deliver higher quality service, in actuality, human agents suffer from constraints that limit their effectiveness. Thus, future research might address how people respond to chatbots compared with humans. It would be interesting to explore whether higher expectations of quality and agency would be compensated for by social norms of polite interactions and compassion for others.In addition, anger may not be the only relevant emotion to consider managerially or theoretically. While our data indicate that anger is of primary importance and is the most commonly identified emotion in service contexts, it is possible that with more sophisticated language processing tools, other emotions, different sources of those emotions, and social conventions could become relevant. For example, it could be that anger remains relevant in customer service contexts but that the source of the anger, such as whether it arises from a lack of procedural or interactional fairness, also impacts the success of anthropomorphic bots ([ 7]). Thus, it is important for future research to continue to investigate how the complexities of emotion, sources of emotion, and social norms interact to influence the effectiveness of anthropomorphic digital customer service agents.It is worth noting that there might be a point in the future when the conversational performance of AI becomes sufficiently advanced and its implementation so commonplace that expectancy violations simply cease to be a concern. In this future, chatbots might be capable of greater freedom of action, in addition to performing intuitive and empathetic tasks ([33]). In approaching such a point, the difference between the reactions of angry and nonangry customers would likely diminish until the groups are nondistinct, and anthropomorphism might cease to conditionally influence customer outcomes. However, this future does not appear to be imminent ([58]).In the short and medium term, therefore, as firms experiment with conversational agents in a variety of customer-facing roles, it remains important to consider the anthropomorphic traits of the chatbot, including simple features such as naming (i.e., ""Alexa""), language style, and the degree of embodiment, along with the specific customer contexts in which the interactions are likely to occur. Specific contexts can vary from traditional corporations to government (e.g., the Australian government chatbot), law (e.g., the ""DoNotPay"" chatbot), and psychotherapy (e.g., the ""Woebot"" chatbot). It is worthwhile to decide, and important for future research to explore, which chatbot is most appropriate for any given interaction, according to the chatbot's characteristics and the specific context.Altogether, chatbots deliver a multitude of benefits to the business (e.g., scalability, cost reductions, control over the quality of interactions, additional customer data). As such, they will continue to be a valuable tool for marketers as the technology matures. Here, we have shown that the unconditional deployment of humanized chatbots leads to negative marketing outcomes, from dissatisfaction to lowered purchase intentions. However, with careful and conscientious implementation, considering the customer's emotional state (e.g., anger), firms can reap the benefit of this burgeoning technology. "
8,"Can Encroachment Benefit Hotel Franchisees? Franchise encroachment, or the addition of an outlet in the vicinity of existing franchisees, is largely viewed as resulting in revenue cannibalization of incumbent locations. Against this backdrop, the authors consider the possibility that the addition of same brand outlets can, in fact, also create positive effects via customer utility and ultimately benefit franchisees, due to a range of mechanisms such as quality signaling, learning, or brand awareness, resulting in a positive pathway on franchisee performance. The authors unpack this possibility using an experiment and detailed proprietary and publicly available data sets from the hotel industry over a five-year period. Their results show evidence of positive effects on customer utility for same-brand outlets and stronger effects for newer brands, cross brands, and online travel agency channel bookings. Counterfactual simulations indicate that although encroachment hurts franchisees on average, it can modestly benefit same-brand franchisees in low-brand-density markets. Together, the findings illustrate the potential ""sunny side"" of encroachment, underscoring the need to update our view of encroachment as context-dependent. The novel emphasis on customers versus the dominant firm view suggests customer and incumbent responses to encroachment should be accounted for in the development of franchise strategy and public policy decisions.Keywords: brand management; demand estimation; franchise encroachment; franchise sales; franchise systems; structural modelFranchise encroachment (""encroachment"" hereinafter) is a key phenomenon in franchise management. Encroachment occurs when a franchisor places a new outlet in proximity to an existing franchisee(s). Incumbent franchisees view these actions negatively, as the new outlet increases competition and cannibalizes their revenues. This view is reflected in substantial literature in marketing, which primarily focuses on documenting this negative impact ([21]; [33]) and managing the resulting conflict ([23]). The literature also examines ways to safeguard franchisee interests through litigation ([ 4]), contracting ([22]), and a range of governance mechanisms ([ 3]). In broad strokes, this literature has painted a picture of encroachment as being predominantly negative for incumbent franchisees and has focused on firm roles.We explore the possibility that encroachment might in fact be positive for incumbent franchisees by incorporating both a customer view and brand differences into this picture. Specifically, we introduce the possibility that customers may gain utility from more outlets of the same brand, subsequently increasing demand and, ultimately, franchisee performance. This utility might come about via a range of mechanisms such as quality signaling, learning, or brand awareness. While researchers have acknowledged conceptually that encroachment may improve franchisee performance ([ 8]; [21]), little empirical evidence exists to date. The closest finding is limited: [13] show that multiple exposures of a trademark hotel brand name on a third-party intermediary's website generated an overall net increase in consumer clicks. However, no studies thus far have provided evidence of encroachment activities directly improving performance via customer utility and demand.While research has examined encroachment effects on customer demand in a static setting with endogenous franchisee pricing ([35]; [39]), these studies do not make the distinction between same versus different brand effects on customer utility, and they focus on a single franchise brand. In contrast, we consider a range of brand characteristics within a franchise chain to inform a broader view of encroachment, performance outcomes, and a more explicit incorporation of the customer's viewpoint. For example, an entry of a Residence Inn hotel will have same brand effects on other Residence Inn hotels and different brand effects on Westin hotels, Element hotels, and so on in the market. We examine these differing effects of brand encroachment on consumer choices.Our context is a single franchisor, incumbent franchisee(s) of multiple brands, and a range of encroachment types across various markets. We model the net impact using publicly available data, a proprietary data set from one of the largest hotel groups in the United States over a five-year period, and an experiment. We find that encroachment effects are not only statistically significant but also economically relevant. Although the effects are predominantly negative, we nevertheless find positive effects on customer utility that are stronger for same brands, newer brands, cross brand locations (i.e., brands with overlapping monikers such as Hyatt Place and Hyatt House), and bookings made through online travel agencies such as Expedia and Kayak.Counterfactual simulations reveal both positive and negative economic outcomes. Overall, encroachment reduces incumbent revenues and profits by 5.3% ($43,810 in revenue and $26,090 in profit per month), consistent with the historical bent of the literature (i.e., increased competition). However, this impact is not statistically significant for same-brand franchisees, suggesting the negative impact is reduced. Further analysis reveals significant positive effects in low-brand-density markets: same-brand franchisees improve performance by approximately 2% per month (i.e., $4,300 in revenue and $2,900 in profit), with 70% (30%) of these outlets realizing higher (lower) revenues as a result.To the best of our knowledge, ours is the first study to directly model encroachment effects on customer preference and identify the conditions under which it might benefit franchisees. We find that while encroachment hurts franchisees on average, the prevailing assumption that it always results in revenue losses for all incumbent franchisees is inaccurate; we find a distinct difference in its impact on same versus different brand outlets.The stakes for better understanding franchising phenomena are substantial. Franchising is one of the most pervasive organizational forms in the U.S. economy, accounting for as much as $890 billion, or 50% of all retail sales across 75 industries. This amounts to approximately 3% of the 2016 U.S. gross domestic product in nominal dollars.[ 6] Our research context—the hotel industry—accounted for approximately $660 billion in 2019.[ 7]In the following sections, we provide a motivating example, overview the relevant literature, and develop a customer utility model that estimates the net effect of encroachment, franchisee competition, endogenous pricing, and heterogeneous customer preferences. We next describe data, and the estimation section then applies a logit and a full demand model. We explore heterogeneous brand characteristics to gain further insights and present counterfactual simulations to consider market composition differences. A general discussion and implications for management conclude. Background and Related LiteratureIn this section, we review the related literature on franchise encroachment. Table 1 summarizes the range of terms that occur throughout the article and their definitions.GraphTable 1. Terms and Definitions. TermDefinitionPresent studyFranchisorThe organization that owns the franchise brand(s) and oversees and coordinates the franchise systemExamines data from one hotel franchisor and its franchise system.ChainThe totality of retail outlets in the franchise systemExamines a chain that comprises multiple distinct hotel brands, most of which share no common brand-related terms, although two brands have a key brand-related term in common (e.g., brands ""AB"" and ""ABC"").FranchiseeEntity that owns and operates outlet(s) in the franchise chainExamines franchisees that have one hotel with one of the chain's brands.EncroachmentThe addition of a new outlet of the same franchise chain in proximity to an existing franchisee.Examines three distinct types of encroachment: Same brand (the new hotel franchisee has the same brand as an incumbent)Different brand (the new hotel has a different brand within the hotel chain)Cross brand (the new hotel franchisee shares a component of the incumbent's brand name; e.g., Hyatt and Park Hyatt)  Sources of Conflict in Franchise SystemsEncroachment arises because franchise business models are marked by a fundamental incentive misalignment: franchisors earn a royalty rate on franchisee sales and an upfront fee ([27]), but franchisees are profit maximizing in their pursuits ([23]). The fixed fee averages 8% of all payments from franchisees ([28]), leaving 90% of the franchisor's income from royalties. Given that franchisors cannot extract all rents via a fixed fee (see [24]), they have an incentive to add more outlets as long as revenues are positive, but this creates a downward pressure on franchisee profits. This behavior is widely viewed as anticompetitive ([ 8]), and the incentive misalignment has led to an active regulatory context and lobbying efforts aimed to minimize the resulting conflict.Several states have implemented measures to safeguard franchisees' interests through disclosure requirements and relationship laws, and research has shown that such laws do in fact lower litigation incidences ([ 4]). [22] show how the franchisor's use of ex ante contracts and extra-contractual incentives influence subsequent monitoring and enforcement, while [ 3] examine the relationship between governance mechanisms and franchisees' motivation and bankruptcy. Adding a New Franchisee OutletIn a related vein, other research considers the factors that drive a franchisor's decision to add a new location to a market. The academic literature mainly focuses on the trade-offs between increased franchisee competition and firm-side benefits such as preemption and firm size spillovers related to entry of hamburger chains ([10]; [17]) or new entrant sales at the expense of incumbents ([20], [21]; [30]). In contrast, practitioners predominantly talk about ""demand generators,"" or businesses that bring in out-of-town visitors, which seem to be less studied. Our analyses corroborate this and show the number of businesses as a strong predictor of hotel entry (Web Appendix W1). However, we find that local population characteristics are less relevant in predicting hotel entry after controlling for the number of businesses. Encroachment ImpactEncroachment can have positive and negative effects on incumbents. Herein, we consider potential theoretical mechanisms that drive positive and negative pathways and their performance impact, although we are not able to empirically tease them apart due to data or modeling limitations. Figure 1 displays an overview of the pathways.Graph: Figure 1. Positive and negative pathways of encroachment on franchisees' financial performance. Negative PathwaysA long-standing prediction of industrial organization is that an incumbent's prices and revenues will decrease if new firms enter the market through business stealing ([ 2]; [31]; [36]; [40]). This prediction is easily extended to a franchising context: the entry of additional franchisees will lead to greater revenue losses for incumbent franchisees.Ample evidence supports the resulting negative pathway from competition. For example, [21] finds that encroachment leads to significant decreases in hotel franchisee revenues per room ($51–$66.81 per quarter). [18] show that a hotel in Manhattan was less likely to survive as the number of chain units in the area increased. [33] shows the presence of competing convenience stores in the same market, regardless of their chain affiliation, reduces store-level revenues significantly. In fast food franchise settings, evidence for negative pathways approximates on average an 18 percentage point decrease in revenues ([45]). [35] show that incumbents' revenues are decreased by 1%–2%, but distance can help mitigate this negative pathway; for every one mile increase between stores, revenue loss is stemmed by as much as 28.1%. Collectively, these results show that encroachment generally hurts incumbent performance via negative pathways by increasing competition. Positive PathwaysAn alternative point of view is that a cluster of same-brand locations can be useful for customers, while simultaneously benefiting incumbents ([ 8]; [21]). This possibility has been understudied. We next consider three possible drivers of positive pathways that can arise from customers. Brand awarenessMore same-brand outlets can increase awareness of the brand or simply make options more noticeable. Franchisee locations can act as a ""living billboard to build awareness and positive brand association"" ([ 5], p. 97). Although most hotel customers are not local, this effect might be operative in online choice settings in which a consumer sees multiple same-brand locations available in a specific area (cf. [13]). One extension of brand awareness impacting positive pathways is via cross-brand effects. In our context, an example of cross-brands is hotel locations with overlapping monikers, such as Hyatt Place and Hyatt House. Given the prominence of the Hyatt chain name, its use in multiple locations might boost demand for these outlets. This may increase brand awareness via multiple exposures and market prevalence. Brand quality signalingMultiple locations can also act as a quality signal, much as advertising communicates quality ([ 1]; [25]). When multiple products share a common brand name, this acts as a nontrivial ""performance bond,"" a credible indicator assuring consumers that products are of high quality, thereby lifting sales of all products under certain conditions ([43]). Costly brand and infrastructure investments communicate a pledge to a prespecified quality level for both existing and new hotels.This logic might be moderated by brand type and booking channel. For example, a quality signal might be more diagnostic for newer than for older, more established brands because consumers may perceive greater purchase uncertainty associated with new brands, and the quality signal acts as an assurance in the consumer's choice process. Similarly, the signal might be more pronounced in channels in which customers do not have strong brand loyalty or product experience. As an example, when a consumer sees multiple Marriott locations in a purchase channel that overviews multiple brands, the cluster of outlets may be a more diagnostic quality signal (i.e., the brand's commitment to a market area) than for a customer using Marriott's booking channel. A prominent avenue for quality signaling is online travel agency (OTA) channels: platforms such as Expedia and Kayak rely heavily on sponsor advertising, and these efforts are another means by which firms can signal brand quality to customers. Consumer learningConsumer learning across similar brands occurs ( 1) at a point in time, such as when a new brand enters a market, and ( 2) over time via consumption and exposure to firm communications ([19]). Thus, the first scenario can be purely cognitive: consumers form a quality perception, and existing brand quality inferences spill over to the new, similar brand entrant. In this context, online reviews from other guests may enable a customer to pool their experiences to infer hotel quality, potentially improving utility for specific brands. In contrast, the second scenario may involve a gradual process over time owing to consumers' experiences. Learning can be particularly valuable with experience goods such as a hotel stay.The net effect of positive and negative pathways on incumbent franchisee performance is not obvious. The new outlet may increase customer preference for franchisees and benefit their performance but, at the same time, heighten competition, which can put pressure on prices and decrease revenues. Thus, a model is needed to quantify the net effect on incumbent performance. While we attempt to rule out as many alternative explanations as possible, completely distinguishing among the positive pathway mechanisms—quality signaling, consumer learning, and brand awareness—remains a fertile ground for future research. ModelWe employ a structural model to examine customer response to encroachment in the presence of franchisees' endogenous prices as well as subsequent outcomes, such as whether their sales increase or are cannibalized. [ 7] provide a useful starting point in their model of an individual customer choosing a product with the greatest indirect utility out of a discrete set of differentiated goods that compete on price; this is a natural framework of static competition. We build on their model and account for a positive pathway effect on customer utility, customer heterogeneity, and price competition as a modification to the standard model.Past research models the addition of a new franchise outlet via reduced form analyses using revenue data and estimates costs related to the firm's decisions (e.g., [10]; [16]; [17]; [38]). In contrast, we micro-model customers' heterogeneous demand response to encroachment while abstracting away from modeling the firm's dynamic entry. Further, we control for the firms' entry decision using fixed effects and uncertainty in the exact timing of hotel entries. A more comprehensive approach would be to micro-model both heterogeneous demand response and a firm's dynamic entry decision with endogenous prices accounting for the demand response to the number of same brand outlets, but we leave this to future research. Customer PreferenceConsider a geographic market m in time t, in which H (or  Hmt  to be precise) hotels compete. In each market, customer i either decides to stay at one of the H hotels or chooses the outside option. Our data are from one of the largest hotel chains in the world with a wide range of brands; H hotels are the focal chain's franchisees, and the outside options are to stay at hotels that are not part of this chain or not stay at any hotel. Customer i gets the following utility if she stays at hotel h in time t. Otherwise, she gets the normalized mean zero utility (i.e., the outside option): uihmt=Xhmtβi+ξhmt+εihmt. Graph( 1) Xhmt  includes price, and seasonality dummies for summer (June–August) and winter (November–January). We operationalize a positive pathway effect by the number of same-brand hotels (  SBhmt  ) in  Xhmt  as hotel h in market m at time t, representing consumer response to same brand encroachment. For example, with three Hilton hotels in a market, there would be two same brand hotels for each Hilton hotel. Trials of other functional forms of the number of same brand hotels, such as a nonlinear function, produce similar results (for robustness checks, see Web Appendix W2.1). ξhmt  captures the product characteristics observed by customers but not the researcher. It is possible that both price and  SBhmt  are correlated with  ξhmt  , which raises endogeneity concerns that can bias parameter estimates. In the estimation section, we address this issue using instrumental variables, generous fixed effects, and institutional knowledge in the timing of hotel entries.  εihm  is an idiosyncratic random term, assumed to be i.i.d. Type I extreme value.We further model  ξhmt  as follows: ξhmt=ξh+ξms+Δξhmt. Graph( 2) ξh  captures individual hotel characteristics such as amenities, size, capacity, and location characteristics.  ξms  is a set of market × six-month dummies to control for time-changing market-specific unobserved heterogeneity. We include these fixed effects to address potential endogeneity issues with identifying the parameter for  SBhmt  .  Δξhmt  is then considered a residual common demand shock.For the kth observed product characteristic  Xkhmt  , we specify the following random coefficient: βik=β¯k+Aiβko+vikβku. Graph( 3) β¯k  is the mean sensitivity parameter that is common across customers for product characteristic k.  Ai  denotes a vector of customer  i  's observed attributes, and  βko  represents a taste coefficient for product characteristic k that varies with observed attributes.  βku  is a coefficient for the standard deviation of unobserved customer attributes on  vik  , assumed to be independently distributed standard normal. This parametrization allows covariance between product characteristics and observed customer attributes; if customers with higher  Ai  have a higher preference for X, the model estimates a positive  βko  .By grouping demand parameters into  θ=(θ1,θ2)  , where  θ1  are macro parameters that are common across customers, and  θ2  are micro parameters that are individual taste dependent (i.e.,  βo  ), the customer utility is rewritten as follows: uihmt=δhmt(Xhmt,ξhmt;θ1)+μihmt(Xhmt,Ai,vi;θ2)+εihmt,δhmt=Xhmtθ1+ξhmt,μihmt=Xihmtθ2. Graph( 4) δhmt  is the mean utility that is common across individuals, and  μihmt  accounts for customer heterogeneity in preferences for different product characteristics, where  Xihmt  denotes the interaction between  Xhmt  and customer heterogeneity  Ai  and  vi  . The predicted market share is then calculated by integrating the probability of each customer staying at hotel h in market m at time t over all customers: shmt(μ⋅mt,δ⋅mt;θ2)=∫exp(δhmt+μihmt(Ai,vi;θ2))1+Σh′exp(δh′mt+μih′mt(Ai,vi;θ2))×dP(v)dP(A). Graph( 5) The Supply SideThe franchisee sets its own hotel price (per night) without restrictions from the franchisor. Thus, we assume that hotel h in market m at time t decides on the room price to maximize the following profit function: maxphmt[phmt−mchnt]Mhmtshmt(X;θ). Graph phmt  ,  mchmt  ,  shmt(⋅)  , and  Mhmt  are price, marginal cost, predicted market share, and the total market potential, respectively. The optimal price is determined via the first-order conditions specified next. Note that hotel  h  's market share  shmt  depends on other hotels' optimal prices  pkmt(k≠h)  , which is part of X. In a given market m at time t, the J first-order conditions with respect to price competition are then given by shmt(X;θ)+∑h′[ph′mt−mch′mt]∂sh′mt(X;θ)∂phmt=0∀h=1,…,H. Graph( 7)Marginal cost  mchmt  is not observed but estimated by inverting the first-order conditions to the following equation: mchmt=phmt−Δ(X;θ)−1shmt(X;θ), Graph( 7)where  mc  , p, and  s(⋅)  denote the vectors of marginal cost, price, and predicted market shares, respectively.  Δ(X;θ)  is own-price share derivative  ∂shmt(⋅)∂phmt  . We use [ 7] supply-side model by adding observable margin cost components  Whmt  at hotel  h  : mchmt=Whmtγ+ωhmt, Graph( 8)where  γ  is the vector of marginal cost parameters, and  ωhmt  is the unobserved component.  Whmt  includes the number of same-brand hotels and various fixed effects to study the effect of potential economies of scale (e.g., cost savings via volume discounts on supplies such as shampoo, soap, towels, etc.). Franchisees pay a portion of revenue to the franchisor as a royalty fee. Royalty fees are hotel specific and are time consistent in a typical 15- to 20-year franchise contract; therefore, we include individual hotel dummies to absorb the fees as part of marginal costs, along with other hotel-specific characteristics. Note that hotel dummies are more granular than market dummies; therefore, any market-specific cost variables are absorbed into hotel dummies. We also include market × six-month fixed effects for unobserved factors that affect marginal cost differently across markets over time. Data Data OverviewWe obtained a unique data set from one of the largest hotel franchisors in the world with multiple distinct brands, which provides substantial variation for investigating encroachment. More than 98% of the hotels are run by independent franchisees that set their own prices and pay a royalty fee to the chain. The royalty rate is assumed to be 10.61% of the gross franchisee revenue, which is the average for the focal chain in HVS Global Hospitality Services studies without much variation across brands.[ 8] Due to confidentiality concerns, we cannot disclose the number of brands in the focal chain. Suffice it to say that the individual brands are distinctive with unique logos, brand names, and service quality. The data include each hotel's brand, price, monthly demand data, and so on. Price and demandWe have monthly revenue and room demand of the franchisor's hotels operated between September 2007 and March 2012 (55 months), from which we derive price (revenue per room night), consistent with previous work such as [14], [21], and [38]. The data include information on the percentage of business stays at each hotel in a given month, providing some insight into customer heterogeneity. Publicly available data on industry cost shifters, such as the median hourly wages of hotel employees from the U.S. Bureau of Labor Statistics, serve as instrumental variables for price. Market share and potentialMarket share is defined as the demand (nights × rooms sold) divided by the monthly market potential. Market potential is defined as the maximum of the sum of a franchisee's demand and the demand of its competitors (including other hotels in the chain and hotels that are not part of the focal chain) in a particular market over the data period; this value is constant across time periods. The competitor demand information was collected by the chain to ensure that a relevant, competitive set determines each franchisee's market; this value is verified via mutual discussion with the franchisee. For example, suppose two franchisees are in a given market: franchisee 1 and franchisee 2. Also suppose that the demand for month t is 10 and 20 for the two hotels, respectively. Additionally, assume that the surveyed competitors' demand for the period is 30 and 35, respectively. Market potential in month t is then max(franchisee 1 market potential, franchisee 2 market potential) = max(10 + 30 = 40, 20 + 35 = 55) = 55 hotel nights. We then again take the maximum of such market potential across time periods for the market. This information is likely more accurate than an industry association such as Smith Travel Research might provide.The outside market share is defined as staying at hotels that are not part of the focal chain or not staying at any hotel. Web Appendix W3 provides further justification of the market potential being constant over time and discusses the outside option related to time trends. Defining and cleaning the geographical market dataTo properly assess encroachment and its impacts, our definition of a market needs to correctly capture the true local competition faced by each franchisee. One constraint is that the data do not include the address or location of each hotel. The location information provided is a geographic tract, which is defined by the combination of a metropolitan statistical area designation and location type (e.g., near an airport, highway or resort; suburban). For example, in the San Francisco-Oakland-Fremont metropolitan statistical area, the Oakland airport and San Francisco downtown area are provided as two separate tracts.One problem is that some tracts may encompass multiple local markets. For example, three major highways crossing the state lie within the ""Wyoming Interstate"" tract. It would be erroneous to assume that all the hotels along these highways compete against one another (for a map of this area, see Web Appendix W4.1). Their inclusion could result in unreasonable parameter estimates, and more importantly, they do not reflect local franchisee competition, which is crucial to correctly specify Equation 5. We exclude rural metro/towns and suburban areas citing a similar argument. Therefore, we exclude such tracts (i.e., suburban, rural metro/town, and interstate locations) from estimation and retain tracts that are more likely to reflect a single market: airports, resorts, and urban areas. Absent more granularity and location information, we are unable to accord all the excluded tract data to the necessary unit of analysis. We also exclude markets with a single hotel of the focal chain during the entire period, as they may not be representative markets where our counterfactual simulations are relevant.Web Appendix W4 details the process of tract exclusion and considers various empirical tests to justify their exclusion. Logit model results suggest that our main model results are robust to the inclusion of single hotel tracts[ 9] and tracts with missing variable information such as wages. Importantly, including suitably matched excluded suburban and rural metro locations that are likely small, well-defined markets yields similar results (see Web Appendix W4.4), which suggests that our findings are not limited to urban, airport, and resort locations but can generalize to suburban and rural metro/town locations.The final estimation sample consists of 21,644 hotel-market-month observations of 447 unique hotels in 128 geographic markets across 34 states. Although the data set spans 55 months, we do not necessarily observe 55 months for every tract, depending on the chain's presence in a market. Out of 447 franchisees, 127 entered the market during the data period. Forty-three percent of the entries (54 hotels) involved one or more instances of same brand encroachment, whereas the remainder did not. The data include 6,924 market-month observations. Descriptive StatisticsThe focal chain consists of a mix of multiple economy and luxury hotel brands; we are prohibited from revealing each brand's market share and price point for reasons of confidentiality. Instead, we provide summary statistics of monthly demand, market share, mean hotel size and more in Table 2. The total market share of the chain is approximately 28%, which is generally consistent with its known overall market share in the industry. The average room price is $107.45 (SD = $39.05) per night. The large standard deviation is driven by the fact that the chain has multiple brands: the highest-end brand has a 9.19% market share with the average price of $192.79, whereas the lowest-end brand has a 6.84% market share with an average price of $73.68 per room per night.GraphTable 2. Hotel Summary Statistics. StatisticMeanSDp25p50Average monthly demand (room nights)4,028.593,061.872,001.003,021.00Total monthly market potential (room nights)61,214.9342,647.2430,628.0047,111.00Average market share at a focal hotel.077.046.043.069Average room price per night ($)107.4539.0583.9799.55Average hotel size (room nights)200.49132.66104.00150.00 1 Notes: Summary statistics are calculated across 21,644 hotel-month observations.Based on the 6,924 market-month observations, the average number of hotels is 3.13 per market in a month (SD = 2.13). All brands have at least one market with same brand encroachment (.99 hotels on average per market). Figure 2 overviews the distribution of this chain's hotels across market-month observations. The top graph shows that approximately 90% of market-month observations have multiple hotels, and the bottom graph indicates that approximately 38% of the market-month observations with multiple hotels have at least one brand with same brand encroachment. Some market-month observations even have two or more such brands.Graph: Figure 2. Distribution of market-month observations. Customer Heterogeneity, Moderators, and Supply-Side DataBusiness stays account for 52% of hotel visits, and we use this proportion to estimate different demand response types. We employ multiple moderators to understand brand heterogeneity (e.g., brand type and age) and the number of cross brands (two brands in the chain that share a common moniker in the market). Overall, 21.8% (SD = 41.2%) of the 21,644 hotel-month observations represent luxury brand hotel observations, and 16.2% (SD = 36.9%) represent newer brands (introduced after 1997). The average number of cross brands is.76 hotel (SD = 1.14) across hotel-month observations. We use wage data from the U.S. Bureau of Labor Statistics: the mean of median wages of desk clerks (maids) was $10.20/hour ($9.66/hour) with a standard deviation of $1.38/hour ($1.52/hour) during the data period. Estimation Berry, Levinsohn, and Pakes (1995) and IdentificationThe first component of the estimation matches the predicted market shares  sj(⋅)  to the observed market shares in the data  sjN  : sj(δ(θ),θ)−sjN=0, Graph( 9)where  θ  is the set of parameters to be estimated. [ 6] shows that a unique value of  δ  matches these two market shares through a contraction mapping (i.e., inner loop), as in [ 7].We then construct moment conditions by assuming that demand shock  Δξ  is uncorrelated with Z, a set of instrumental variables including exogenous variables as well as a constant: G1(θ)≡E[Δξj(θ)|Zj]=0. Graph(10)Note that price can be potentially correlated with the demand shock  Δξ  , which may create an endogeneity problem that could potentially bias the price coefficient estimate ([41]). To deal with this issue, we use [ 7] instruments, which are the averages of other hotel characteristics. Specifically, the first instrument is the average hotel size (i.e., number of hotel rooms) of all other hotels in the chain, and the second instrument is all hotels of differing brands of the focal chain in the market. The number of rooms in each hotel is determined by the combination of the availability of land for commercial use and the local government's zoning restrictions on the land use (e.g., the total number of floors due to height restrictions or the shape of the building) that are deemed to be exogenous with respect to the demand shock  Δξ  .[10] Web Appendix W5 provides the test results of various instrumental variables. In the estimation, we also include functions of these instrumental variables and cost shifters such as the median wages of desk clerks and room service personnel.Another endogeneity concern involves the positive pathway variable as defined by the number of same-brand hotels in a market. Part of  ξhmt  may contain unobserved heterogeneity that affects the number of hotels of the encroached brand in a given market. One way to deal with this concern is to include market dummies. However, market dummies may not be sufficient to control for unobserved heterogeneity. For instance, a hotel within walking distance to a popular tourist attraction (e.g., Disney World) would draw higher customer preference over a hotel that is far from it, despite competing in the same geographic market. We therefore include individual hotel dummies (  ξh  ) as part of product characteristics.While individual hotel dummies control for endogeneity from time-consistent demand heterogeneity, concerns remain about market-specific time-varying unobservables. Because the positive pathway is proxied by the number of same-brand hotels and its change in a market, if such unobserved heterogeneity is correlated with different hotel entry patterns, it can ultimately bias the estimate ([29]; [35]). We address this issue in several ways. First, we include fixed effects specified at the market × six-month level to flexibly control for the unobservables that may influence hotel entry and, thus, the change in the number of same-brand hotels. The fixed effects also account for time-changing outside options to some degree by allowing them to have different intercepts.Second, we exploit the institutional knowledge of the industry regarding the timing of hotel entry. This entry decision may be decided at large time intervals (e.g., hotel entry planned in the second half of 2015); interviews with chain executives reveal that the exact timing of entry is difficult to predict due to the nature of the commercial real estate development process. This timing varies within a six-month window, depending on many unpredictable external forces, such as financing commercial real estate mortgages, passing inspections, and obtaining permits from local government. Therefore, we assume that the variation in the number of same hotel brands is exogenous within a six-month window.This approach is consistent with the precedent of [12], who similarly assumes exogenous entry timing of generic drugs due to unpredictable FDA approval processes. We first checked the validity of this assumption by showing that the month in the six-month window cannot predict entry (see Web Appendix W2.2). We checked robustness of the results with additional control variables (see Web Appendix W2.3). Additionally, we examine incumbents' demand and price surrounding the entries of same and different brands and find that both increase only for same brands post entry (Web Appendices W6 and W7, respectively). These results corroborate our modeling approach that customer utility or willingness to pay increases only in the case of same brand encroachment.Since we only observe hotels from one chain, it is difficult to completely rule out the possibility that the entry timing in this chain may coincide with entry/exit of unobserved nonchain hotels. Two factors mitigate this concern. First, the uncertainty in the commercial real estate development process equally applies to unobserved hotels. Thus, it is unlikely that their entry/exit patterns will systematically coincide with our chain's entry timing within the six-month window and result in an overestimate of our results.[11] Second, we do not observe the full range of competitive options, which can lead to an attenuation bias via measurement error (i.e., our estimates may be conservative; for a similar setting in the airline industry, see [34]). Finally, we consider other endogeneity concerns (e.g., the granularity of our fixed effects, independent hotels rebranding as new hotels in our data) in Web Appendix W8.1. Other Details of EstimationThe estimation strategy is based on the generalized method of moments estimation combined with micro moments (e.g., Berry, Levinsohn, and Pakes 1995 2004) to estimate customers' heterogeneous demand response in terms of the percentage of business stays. Supply moments are estimated to evaluate the degree to which an economies of scale explanation might be operative after controlling for hotel and time-varying market factors. Technical details of the estimation are included in Web Appendix W8. Results Logit ResultsWe estimate a homogeneous logit demand model (  β=β¯  in Equation 3) to assess data variations and address endogeneity issues using instrumental variables and fixed effects before estimating the full random coefficient demand model. The logit model regresses  log(sj)−log(s0)  on price and the number of same brand hotels in the market (SB), as well as hotel seasons (June–August for summer and November–January for winter).  s0  is the market share of the outside option. This is Model 1, and the results are displayed in Table 3. For Model 2, we add hotel fixed effects to control for unobserved hotel-specific characteristics (e.g., amenities, size, capacity, location). Model 3 adds market × six-month fixed effects to control for time-varying market-specific unobservables that may result in hotel entry, which in turn can bias the SB estimate. Finally, we run an instrumental variable regression to address the endogeneity of price (Model 4).GraphTable 3. Logit Model Results. Model(1)(2)(3)(4)RegressionOLS1OLS2OLS3IVHotel FEsNYYYMarket × six-month FEsNNYYPrice ($00) [β¯price].121***.552***.549***−1.297***(.012)(.013)(.015)(.367)# Same Brand Hotels (SB) [β¯SB]−.105***.199***.090***.059***(.004)(.008)(.008)(.015)Constant−2.363***———(.014)Observations21,64421,64421,64421,644R2.079.841.875- 2 *** p < .01.3 Notes: The dependent variable represents a transformation of customer utility in a logit model, expressed as a logarithmic function of market share minus a logarithmic function of the outside option market share. Hotel season dummies for summer and winter are included for all specifications. Robust standard errors are reported in parentheses. FE = fixed effects. OLS = ordinary least squares regression estimation. IV = instrumental variables regression estimation.The results of the logit regression using price and SB in Model 1 imply that customers prefer higher prices and dislike having more of the same-brand hotels in a market, as SB is negative and significant. When we include hotel fixed effects in Model 2, a positive pathway emerges, and the number of same-brand hotels increases customer utility, suggesting that demand is positively associated with same brand encroachment. The price coefficient remains positive and statistically significant, an unconventional finding. Including additional market × six-month fixed effects in Model 3 substantially reduces the SB estimate (from.199 to.09, although still significant at 1%), underscoring the value of controlling for time-varying market-specific unobservables.However, after instrumenting price with the average size of the focal chain's other hotels and that of different brand hotels in the market, the price coefficient in Model 4 is now negative (−1.297, p < .01); this suggests that an instrumental variable approach results in more reasonable results (i.e., hotel demand decreases as price increases). The first-stage regression results show that the instruments have enough explanatory power with the first-stage F-statistics of 112.9. Collectively, the logit results suggest consumers respond positively to same brand encroachment—a positive pathway that can create revenue benefits among same brand franchisees if the negative pathway or market competition is not severe.The results are robust to different functional forms of SB, such as a log function of the same-brand hotels (see Web Appendix W2.1). In comparison to the log specification, the linear function of same brand hotels results in a better fit (  R2  =.714 versus.694 under the log function) and is therefore used throughout. Full Model Parameter Estimates Demand-side parametersWe now estimate the full model with heterogeneous customer preferences (  βik=β¯k+Aiβko+vikβku  in Equation 3). Table 4 provides macro parameter estimates of common customer preference on all observed hotel and market characteristics explaining consumer utility (  β¯  in Equation 3).GraphTable 4. Demand Macro Parameter Estimates in Equation 3. Coef.SEPrice ($00) [β¯price]−1.544***.302Number of Same Brand Hotels (SB) [β¯SB].097***.037Summer (June–August).092***.008Winter (November–January)–.426***.023Luxury.759***.013 4 *** p < .01.5 Notes: A full set of individual hotel dummies and market × six-month dummies are included in the estimation. We obtained estimates for the luxury brand dummy using a minimum-distance procedure.The mean price parameter estimate is negative and significant (−1.544, p < .01), implying that customers have a preference for lower prices. Combined with the price heterogeneity estimates in Table 5, this results in an average own-price elasticity of −1.33. Seasonality has a significantly negative (positive) effect for winter (summer), which suggests that demand peaks in summer. We draw additional insights on customer taste by running a generalized least squares regression ([11]; [32]) in which we regress hotel dummy estimates on a luxury brand dummy (=0 if economy brand). As expected, customers prefer luxury over economy hotels (.759, p < .01). Parameter estimates for all dummy variables (  ξh  and  ξms  in Equation 2) are summarized in Web Appendix W9.GraphTable 5. Customer Heterogeneity Parameter Estimates in Equation 3. CoefficientSEPrice − Business Stays [βpriceo].009***.003Price − Unobserved Heterogeneity [βpriceu].516***.010# Same Brand Hotels (SB) − Business Stays [βSBo]−.037***.012# Same Brand Hotels(SB) − Unobserved Heterogeneity [βSBu].042.279 6 Notes: *** p < .01.Customer heterogeneity coefficients (  βo  for observed characteristics and  βu  for unobserved heterogeneity in Equation 3) are presented in Table 5. A positive price estimate for business stays implies that business stays exhibit lower price sensitivity, in line with the conventional wisdom in the industry. The estimate of unobserved heterogeneity (or the standard deviation of the normally distributed random coefficient) is statistically significant for price, capturing a large variance in customer taste. The SB estimates suggest that business stays are less sensitive to the number of same brand hotels than nonbusiness stays (.097 –.037 = .060 for business stays versus.097 for nonbusiness stays). The estimate of unobserved heterogeneity for SB is nonsignificant. Supply-side parametersThe estimation of the supply side (Equation 8) shows that the effect of SB on marginal costs after controlling for hotel and market-time fixed effects is slightly negative but not significant (–.008, p = .233), suggesting a lack of economies of scale (for detailed estimation results, see Web Appendix W9). An alternative covariate is the number of same-brand hotels in the nation in a given time period if the level of economies of scale is at the national as opposed to local markets. Our analysis shows that the economic significance of such an effect is very close to zero. The estimated marginal cost is $27.11 per room night, which approximates the marginal cost found in the hotel industry (e.g., [16]). Economic Value of a Positive Pathway EffectWe can estimate the economic value of this effect by comparing the observed data with a scenario in which the SB parameter is set to zero; conceptually, this approximates a scenario in which consumers are not responsive to an increase in the number of same brand hotel outlets (i.e., no positive pathway effect). Note that in this scenario, franchisees may set different optimal prices. The first-order conditions defined in Equation 6 allows us to recalculate the new optimal price. We then compare the scenario's outcome against our data in Table 6. The results suggest that a positive pathway contributes to a slight increase in average price and accounts for approximately 9.4% (8.3%) of the average hotel franchisee monthly revenue (profit) in the focal market. Also, 4% of the $707.5 million gross royalty income is estimated to result from the positive pathway effect. Therefore, the positive pathway is not only statistically significant, but also economically relevant regarding the franchisee's economic welfare.GraphTable 6. Positive Pathway Economic Value. VariableModel with SB = 0 (Simulation)Model with SB (Data)% ChangeAvg. Price ($)116.11117.09.85Avg. Market Share (%)6.476.632.58Avg. Franchisee Revenue ($000)577.39631.539.38Avg. Franchisee Profit ($000)358.73388.588.32Total Royalties ($M)707.50734.563.82 7 Notes: Franchisee revenue and profit are monthly figures, and royalties are summed up across the data period. The Impact of Brand Characteristics on Encroachment OutcomesIn this section, we investigate whether the positive pathway varies across a range of brand dimensions such as quality (luxury vs. economy brands), brand age (more vs. less established brands), and cross-brands (brands with an overlapping moniker). Brand qualityIn Model 1 of Table 7, we reestimate the logit model (Model 4 in Table 3) with an interaction term between  SBhmt  and  Luxuryh  (an indicator function equals 1 if hotel h is a luxury brand and 0 otherwise). The parameter estimate of the interaction term is not statistically significant, implying that the positive pathway for luxury brands is similar to economy brands.GraphTable 7. Heterogeneous Effects of the Number of Same Brand Hotels. Model(1)(2)(3)(4)HeterogeneityBrand QualityBrand AgeCross-BrandCombinedPrice ($00)−1.326***−1.264***−.936***−.888**(.370)(.362)(.358)(.351)Number of Same Brand Hotels (SB).056***.057***.056***.053***(.015)(.014)(.013)(.013)SB⋅Luxury.033.010(.049)(.043)SB⋅Newer.237***.246***(.063)(.065)CrossBrand.048***.051***(.013)(.013)Observations21,64421,64421,64421,644R2.709.720.771.778 8 ** p < .05.9 *** p < .01.10 Notes: The dependent variable represents a transformation of customer utility in a logit model, expressed as a logarithmic function of market share minus a logarithmic function of the outside option market share. Hotel season dummies for summer and winter, hotel fixed effects, and market × six-month fixed effects are included in all specifications. Robust standard errors are reported in parentheses. SB = same brand. Brand agePrior to 1992, the chain was composed of 57% of the chain's brands relative to the data period. Three or more brands were added to the chain's portfolio after 1997, and then the full set of brands developed together. We define a dummy variable  Newerh  equal to 1 for these newer, less established brands and 0 for older brands. In Model 2, the interaction of  SBhmt  with  Newerh  is positive and significant (.237, p < .01), implying that newer brands are associated with even higher positive pathways relative to older brands, which we surmise could be because well-established, older brands face ceiling effects that limit the positive pathway. This result suggests that encroachment may help newer brands enter the market, leading to more hotel choices that can potentially increase consumer welfare. Cross-brandsWe consider the possibility of additional positive pathways through cross-branding, in which two outlets share a common aspect of the brand name. For example, a ""Hyatt Place"" hotel may benefit whenever another hotel outlet incorporating ""Hyatt"" (e.g., ""Hyatt House"") is added to the market. Among the chain's brands, two have similar names (e.g., brands A and AB): brand A's moniker is one word, and brand AB has two words starting with brand A. The second word (B) does not directly signal superior overall quality but refers instead to the speed of service. If positive pathways are generated via the number of outlets with same brand monikers, we may observe a positive pathway not only between two outlets of the A brand, but also between A and AB brands. We assess this possibility by adding  CrossBrandhmt  , a count of the number of cross-brands in the market. For instance, if there are two A hotels and one AB hotel,  CrossBrandhmt  for A (AB) hotels would be 1 ( 2). For brands other than A and AB hotels,  CrossBrandhmt  equals 0. Model 3 results indicate that, in addition to the same-brand encroachment effect, there exists a smaller positive cross-brand effect on customer utility between A and AB hotels (.048, p < .01), corroborating the existence of a positive pathway. Model 4 reflects the combined model with robust results. Collectively, these analyses unpack how a positive pathway effect might be heterogeneous in brand characteristics. While we do not observe significant additional positive pathways with regard to brand quality, we do observe additional positive pathway effects for newer and cross-brands. Placebo testTo further assess robustness, we conduct a placebo test to see if a cross-brand effect is no longer observed for the count of the number of hotel brands with dissimilar names. For instance, in place of brand A's  CrossBrandhmt  , we count the number of brand E outlets in the market, where brands A and E are distinct names without moniker overlap. As expected, the coefficient for the placebo brand term is not statistically significant. We also conduct additional placebo tests using the number of future same brand entries and do not find significance. See Web Appendix W10 for further details. Booking Channel AnalysesBooking channels offer varying forms of product comparison and purchase convenience in the customer's purchase process. We consider positive pathway effects across booking channels that support varying degrees of purchase intent and brand loyalty. One advantage of OTA channels such as Expedia or Orbitz is that they allow customers to compare across brands and options in terms of price, dates, and locations, which may be particularly valuable to customers who are seeking a specific type of deal and are open to a range of brands. OTA customers may be more likely to make occasional purchases and may not be brand loyal. In contrast, customers who book through a chain's official website are likely to have a preference for the brand and may not need an array of brand options for their purchase; they may be brand loyal or repeat customers. A positive pathway for demand may be less likely for this group due to saturation or ceiling effects, relative to OTA channel customers. Put differently, OTA consumers are more likely to benefit from a positive pathway, and this impact should be observable via heightened demand as encroachment increases.We investigate this possibility by estimating a series of ordinary least squares (OLS) regressions for different online channel demand over brief windows (±3 months) around hotel entries for incumbent hotels, excluding the entered hotels. Details and results of the estimation are included in Web Appendix W11. We find that the incumbent's demand (9.946, p < .10) and the proportion of demand (.013, p < .05) through OTAs increase relative to other brands when a same-brand hotel enters the market. However, there is no significant corresponding effect through the chain's website channel. These significant interaction terms support our contention that same brand encroachment spurs a positive pathway effect and increases customer preference in OTA channels. Thus, incumbents benefit more from purchases in the OTA channel than the chain's channels. We evaluated the price response to determine whether a demand increase in the OTA channel decreases the overall price (hotel revenue per room). We find an increase in price for same brand incumbents post-entry, suggesting higher willingness to pay in customer utility (see Web Appendix W7 for further details).We conduct similar analyses with offline booking channels associated with the chain, namely, traditional travel agents and call centers, but did not find any significant results. We conjecture that because travel agents are experienced in hotel bookings, they are less influenced by the addition of same brands. In addition, customers who book through the chain's call centers, like those who book through the chain's website, may have brand loyalty or preference that would make them less sensitive (i.e., a ceiling effect) to same brand encroachment. Explanations of Observed Positive PathwaysWe next consider various explanations for the observed positive pathways. Although we are not able to unequivocally isolate their effects, a careful consideration of the empirical evidence can suggest which explanations might be operative. Potential Explanations Brand awareness and quality signalingOur results in prior sections suggest that brand quality signaling and awareness mechanisms are likely operative. Evidence of an additional positive pathway on customer utility for cross-brands and a moderating effect of newer brand outlets further corroborates this possibility. Additionally, the demand patterns show that consumers who purchase through the chain's booking channels (as opposed to OTA channels) are less likely to reflect a positive pathway effect, possibly due to brand loyalty and familiarity. Online advertising in OTA channels may also result in greater exposure of same brand hotels, which would increase brand awareness and quality signaling. This is consistent with our finding in Table W22. Consumer learningWe have suggested that a possible positive pathway might arise if consumers learn from the presence of multiple brand outlets. Although our hotel-level data prohibit us from isolating this effect at the customer level, we explore this potential mechanism via an online experiment. If learning improves as the number of hotels in a market (i.e., brand exposure) increases, consumers may learn about hotel quality more accurately (i.e., correct quality association of the brand with more outlets), absent more information such as budget constraints, pricing, location, and so on. Better learning would thus be evidenced by greater accuracy in markets with more hotels regardless of the hotel quality level.We test this possibility using a design in which we manipulate hotel quality (high vs. low) and number of hotels in a market (two vs. five) in a 2 × 2 between-subjects design. The dependent variable is the recall accuracy of the focal hotel brand quality. If the consumer learning mechanism holds, one would expect that recall inaccuracy would be higher in the two-location condition than the five-location condition. Web Appendix W12 contains more details regarding data collection, stimuli, and analyses.The results are supportive: inaccuracy is significantly higher in the two-location than the five-location conditions across both high- and low-quality hotels. When there are fewer hotel outlets, the inaccuracy is mainly driven by the participants' reported ratings gravitating toward the condition mean, consistent with an explanation of not learning well. As a result, for higher-than-average hotel quality (i.e., as in our data), more same-brand hotels are associated with higher perceived quality. Collectively, these results provide evidence for the possibility that more outlets can increase preference via consumer learning, supporting a positive pathway effect.More broadly, our results suggest that customer utility and market demand can be positively impacted by the addition of a new outlet when the addition is of the same brand, a newer brand, a cross brand, or purchased in an OTA channel. Unlikely Explanations ScarcityIf consumers prefer scarce boutique hotels (e.g., Hotel Indigo) over more standard hotel offerings with wide market coverage (e.g., Hilton Garden Inn), then the existence of multiple establishments of the same brand would decrease utility for the encroached brands. Research on luxury goods consumption is consistent in this regard, showing that scarcity and exclusivity creates value for customers ([37]; [44]). However, our results do not support differing pathway effects for luxury hotel brands compared with economy brands, suggesting that scarcity is an unlikely explanation. Television advertisingTelevision advertising is difficult to target or tie to a specific outlet opening, given the difficulty of predicting a hotel opening even within a six-month window. Unlike fast food customers, hotel customers are not necessarily local, which can complicate advertising targeting. Therefore, we consider television advertising unlikely in our context. Other supply-side explanationsAlthough our focus has been on demand-side mechanisms for positive pathways, several supply-side mechanisms have often been discussed in the franchise literature as well, such as co-ownership, cost agglomeration, economies of scale, and capacity constraints. While these mechanisms are useful, they cannot adequately explain the positive pathway effects on customer utility or are irrelevant in our research context, as explained in Web Appendix W13. Counterfactual SimulationsA key advantage of a structural modeling approach, relative to regression analyses, is the ability to conduct counterfactual simulations. In this section, we explore two counterfactual simulations. The first simulates the impact of encroachment reduction, which has implications for shaping policy and represents a possible action for conflict reduction. It illustrates how removal of an encroaching outlet impacts the remaining hotels' optimal pricing and customers' subsequent hotel choice. The second counterfactual explores a key boundary condition of intrabrand competition: markets with varying same brand density. These counterfactuals have both practical and theoretical implications, and they allow us to explicitly illustrate the role of positive and negative pathways on incumbent performance. Counterfactual 1: Encroachment Reduction and Incumbent PerformanceWe simulate encroachment reduction by removing one same brand franchisee and allowing the remaining franchisees to reoptimize their prices through Equations 6 and 8. We compare this simulated market outcome to the current model estimates and quantify the net aggregate effect on the remaining franchisees' performance. Because we do not have data on hotels outside the chain, our results assume that such hotels would not respond to the simulated policy changes.We consider all combinations of the removal of a single same brand hotel using the following process: first, in each market, one same brand hotel is removed. If there are three brand A hotels, then one is randomly selected (A1) for removal. Second, the remaining hotels (A2 and A3) reoptimize their prices, according to Equations 6 and 8. Third, market outcomes—including franchisee price, demand, revenues, and profits, as well as franchisor revenues and profits—are then recalculated. Finally, the foregoing three steps are repeated for each remaining hotel (A2 and A3) and across all markets.We further decompose the impact of encroachment reduction by considering its effects on same and different brand outlets. Table 8, Panel A, reflects the performance impact on revenues and profits with one fewer same brand outlet. The first column reflects the simulation results (i.e., less encroachment), the second column reflects our model estimates (i.e., more encroachment), the third column reflects the mean percentage change between the foregoing, and the fourth column reports 95% confidence intervals. The overall impact of more encroachment results in an average revenue and profit loss of 5.3% (i.e., −$43,810 in revenue and −$26,090 in profit per month) for all incumbents. This result is consistent with the negative pathway findings in the empirical literature. This value is meaningful, as the ""impact threshold,"" or the minimum drop in revenue that must be observed for a franchisee to bring suit for encroachment in Iowa, is 5% (Iowa Code 523 H, 1995).Decomposing this further for same- and different-brand incumbents, we find that the effect is even greater on different-brand franchisees in the franchise system, with revenues and profits declining significantly by 7%–8% (i.e., −$59,834 in revenue and −$35,586 in profit per month). In contrast, more encroachment has little to no impact on same brand franchisees: although revenues decrease significantly (i.e., −$5,914 per month or −.58%, 95% CI [−.68%, −.48%]), the impact on profits is not statistically significant (−$3,659 per month or −.07%, 95% CI [−.18%,.03%]). Given that franchisors earn a royalty rate on revenues and franchisees are profit maximizing, these effects are economically meaningful. The results suggest that while a negative pathway dominates for different brands, a positive pathway seems to mostly offset a negative pathway for same brands (i.e., mitigation). This represents a critical contribution to our understanding of encroachment. Counterfactual 2: Brand Density Market ConditionsAnother dimension to consider is the number of existing brands, or a brand's ""density,"" in a market. The fewer the number of same brand outlets, the lower the brand's density, and potentially, the weaker the negative pathway from adding yet another outlet. Positive pathways might then dominate negative pathways in the case of same brands, potentially improving franchisee performance. In contrast, in markets in which brand density is high, firms have more competition for customers; thus, we are likely to observe that negative pathway effects prevail.Suppose there are two markets that vary in terms of the number of A hotels: Low-density market (""Market L"") consists of two A hotels, one B hotel, one C hotel, and one D hotel. In this market, we can calculate a brand density ratio for A hotels to be 40%. High-density market (""Market H"") contains three A hotels, and one B hotel, and one C hotel. In this market, the same-brand density ratio for A hotels would be 60%.If one A hotel is removed, we would expect its impact on the remaining A hotels to be smaller in Market H because the marginal positive pathway is likely lower. We divide the data set into low (≤50%) and high (>50%) same brand density ratio markets.[12] Note that the brand density ratio reflects the number of same-brand hotels over the franchisor's total number of hotels in the market. We provide further evidence that the brand density ratio is a good proxy for the same brand concentration with respect to all hotels, including hotels that are not part of the chain, in Web Appendix W14.1.Table 8, Panel B, displays the performance impact. Consistent with a negative pathway mechanism, encroachment generally hurts franchisees, reducing average revenue and profits by 4%–6% regardless of the market's brand density. This performance decrement increases for the remaining different brand franchisees, resulting in a reduction in revenue and profits of 7%–8% and replicating results from the previous counterfactual. In contrast, the performance decrement is minimized and can even be positive for same brand franchisees. In high-density markets, they experience only a 3% reduction in performance (−$18,954 in revenue and −$12,011 in profit per franchisee per month). In low-density markets, incumbent performance improves by approximately 2% ($4,349 in revenue and $2,914 in profit per franchisee per month).GraphTable 8. Counterfactual Simulations of More Encroachment on Franchisee Monthly Performance. A: More Encroachment Impact Across Franchisee Brand TypesImpact on remaining franchisees OutcomeLess Encroachment ($000)More Encroachment ($000)Mean % Change, [95% CI]Same-brand franchisees onlyRevenue567.41561.50−.58, [−.68, −.48]Profit342.00338.34–.07, [−.18,.03]Different-brand franchisees onlyRevenue87.8281.99−7.23, [−7.28, −7.18]Profit475.30439.72−7.52, [−7.57, −7.47]All franchiseesRevenue78.63736.82−5.25, [−5.30, −5.20]Profit435.67409.58−5.31, [−5.36, −5.25]B: More Encroachment Impact Across High and Low Brand Density MarketsMean % change, [95% CI]OutcomeLow-brand-density markets (≤50%)High-brand-density markets (>50%)Same-brand franchisees onlyRevenue1.65%, [1.55%, 1.75%]−3.41%, [−3.57%, −3.26%]Profit2.27%, [2.17%, 2.37%]−3.05%, [−3.22%, −2.89%]Different-brand franchisees onlyRevenue−7.19%, [−7.24%, −7.14%]−7.90%, [−8.09%, −7.71%]Profit−7.48%, [−7.53%, −7.42%]−8.34%, [−8.54%, −8.14%]All franchiseesRevenue−5.43%, [−5.49%, −5.37%]−4.35%, [−4.48%, −4.22%]Profit−5.53%, [−5.60%, −5.47%]−4.15%, [−4.30%, −4.01%] 11 Notes: In the simulation, we iteratively remove same brand encroaching hotels in the market one at a time and average the outcomes. Same brand franchisees have the same brand as the removed hotel in each iteration. Less encroachment means one less encroaching chain outlet in simulation. More encroachment reflects the main model estimates.Figure 3 sheds further light on these point estimates by illustrating the distributions of franchisees' monthly revenue change: the midpoint of the graphs reflect no change, while areas to the right reflect the net outcome of a positive pathway dominating a negative pathway, reflected in higher franchisee revenues. Panel A of Figure 3 reflects the relative distributions for the first two cells of the last rows in Panel B. It is worth noting that the brand-density markets' plot is bimodal. We explore this further in Figure 3, Panels B and C.Graph: Figure 3. Distribution of incumbent revenue change across high- and low-brand-density markets.Figure 3, Panel B, shows revenue change from the perspective of same brand outlets (i.e., the first two rows in Table 8, Panel B). In low-brand-density markets, a clear positive pathway impact is evident for same-brand franchisees; this plot is similar to the positive pathway impact for low-brand-density markets in Figure 3, Panel A. This distribution shift suggests that on average, 70% of franchisees benefit from having a same brand hotel added in their vicinity. With high brand density markets, revenues are generally lower, consistent with a negative pathway dominating; however, even under these conditions there is a sizable portion of revenue for which positive pathways dominate.Figure 3, Panel C, illustrates franchisee revenues decrease from the perspective of different brand outlets (i.e., the middle rows in Table 8, Panel B). This plot suggests that the addition of a different brand hotel hurts franchisee revenues regardless of market brand density. It also suggests that there are no positive pathway gains for outlets whose brand is different from the new outlet—only a negative pathway.Collectively, the counterfactual analyses and associated plots provide a more nuanced understanding of the impact and market conditions under which positive and negative pathways to franchisee performance occur. Web Appendix W14.2 presents histograms of selected market revenues pre- and post-simulation. Conclusions and Implications Substantive ImplicationsA chief advantage of franchising organizations is rapid growth in the number of outlets and service availability. This growth may inevitably result in more same brand franchisees being located close to one another. Prior to our research, encroachment was only considered in terms of negative pathways. However, our work provides important insights into the circumstances under which this might not be the case. Adding a same brand outlet can in fact lead to positive pathways via customer utility enhancement, which can subsequently mitigate negative pathways and even improve franchisee revenue and profits for same brand outlets in low-brand-density markets.The reduced negative performance impact of encroachment on same brand franchisees suggests that encroachment may not be as universally harmful as once thought. Moreover, the prevailing positive pathway increasing both revenues and profits in low-brand-density areas suggests that firms should seek to encroach more in those areas (i.e., strength in numbers). To this end, we find that newer brands benefit more, holding promise for market entry strategies, as do cross brand locations and OTA bookings. Our work calls for managers and researchers to rework their view of encroachment to recognize that it may not be unilaterally negative.Beyond the hotel industry, if encroachment involves mostly negative pathways dominating positive pathways, one may not observe any increase in performance related to same brands. Alternatively, if products are well differentiated such that negative pathways are relatively small, newer brand franchisees may achieve improved performance with same brand encroachment. On the other hand, different brand encroachment does not involve positive pathways, so we expect it to be seen as an adverse event to incumbents in other industries as well. Marketing implicationsEncroachment has been a contentious issue in the marketplace, but our research is the first to identify potential benefits and positive pathways from the addition of a same brand via a customer-centric viewpoint. Our ability to trace these pathways to improved customer utility underscores the power of the customer in a research area that has mostly focused on the franchisor's view of the market. Encroachment can be positive through customer utility, and we demonstrate where and when these benefits occur.Being the first to distinguish the performance impact for same- versus different-brand outlets in a franchise system adds nuance to our understanding of how franchise systems should go to market and how brand characteristics play into it. Specifically, franchisors might prioritize markets with low same brand density for more locations. In this regard, we observe that a positive pathway effect can increase franchisee revenues by 1.7%, reflecting a 2.3% improvement in profits. This illustrates the potential ""sunny side"" of this controversial phenomenon. Importantly, franchisors can be strategic about when and where to reduce or expand locations, particularly as it pertains to franchisee conflict or relationship improvement.Regarding channel conflict, franchisors face a fundamental trade-off in achieving their expansion goals; prevailing in a particular conflict, such as encroachment, can come at the cost of less location expansion. Our work identifies an alternate path, suggesting that additional same brand locations can result in a win-win outcome in low-brand-density markets and with various brand characteristics.It is also worth noting that franchise contract specifics may make it difficult to generalize our results. For instance, encroachment is irrelevant in franchise chains that guarantee exclusive territories. Also, the type of franchising may affect generalizability. While many new franchises are business format franchising (i.e., franchisees pay running royalty payments to the franchisor as a percentage of sales), the traditional franchising model is still prevalent in sectors such as automobile dealerships and gasoline stations ([ 9]). In traditional formats, the franchisor sells the product to franchisees with the right to serve a certain geographic area instead of collecting ongoing royalties. Regardless of the format differences or other unique aspects of the hotel industry such as capacity constraints, we may yet observe positive pathways if brand awareness and quality signaling are relevant. We leave these issues for future research to verify.From a consumer welfare perspective, our research sheds a new light on how to consider hotel choice offerings across a range of markets. Our findings of additional positive pathways (e.g., cross brands, newer brands, purchase channel types) to hotel performance suggest that such offerings might enhance consumer welfare in new markets and in the creation of valued product offerings. These insights are important for policies such as those described in The Network Expansion Handbook, which is developed by the International Franchise Association's Franchise Relations Committee to assist in establishing codes of conduct and setting expectations and compliance for encroachment practices. Policy implicationsOur counterfactual simulations help inform the fractious efforts around franchise regulation. Although franchisors may allow exclusive territories to individual franchisees, established franchisors often maintain legal rights in the final encroachment decision (for more detailed discussion, see [21]). However, the threat of litigation motivates franchisors to maintain goodwill. Our findings of prevailing positive pathways in low-brand-density markets and the substantially reduced negative pathway impact on same-brand incumbents indicate customer utility gains from having more same brand hotels. Future research might consider how a ban on same-brand encroachment, such as the Iowa Franchise Act of 1992, would affect customers. Theoretical ImplicationsOur finding that encroachment can be economically beneficial for same brand franchisees contributes to the literature, which has historically shown the opposite. Incorporating a consumer perspective represents a substantial departure from the dominant approach, which has historically focused on the franchisor's revenue and market share. Moreover, the customer's perspective enables us to illuminate the value of encroachment in terms of increased customer utility and demand for same brand franchisees. Although we find customer heterogeneity differences for hotel industry-specific factors such as business stays versus leisure stays, generalizability to other industries remains an area for future investigation. As an example, considering that hotel franchisees rely on out-of-town customers more than a fast food franchisee might, pathways for these two industries may differ.We expand the scope of research in the marketing literature on franchise management, which has mostly focused on issues of governance and enforcement ([ 3]; [22]), or organizational forms such as the proportion of a chain that is franchisor versus franchisee owned ([ 4]). Our research highlights the phenomenon of encroachment as a substantive direction for future research in its own right.Our work contributes to the growing body of work on firm entry. Similar to [15] and [42], we allow both negative and positive outcomes from entry, albeit in a franchising context. Their results support the positive effects that arise from economy-of-scale gains on the supply side or a concentration of product category brand locations. We, in contrast, focus on the effect of multiple same brand outlets on customer demand as opposed to the use of multiple-store formats or category effects.Generalizing our findings beyond the hotel industry depends on industry-specific factors that may affect pathways.[13] For example, the magnitude of negative pathways may be different in other industries. One measure of this negative pathway is own-price elasticity. For example, [35] find an elasticity of −2.09 among fast food franchisees, and [26] finds −2.82 among auto radiator distribution franchisees. Our price elasticity of −1.33 suggests weaker negative pathways relative to these industries.The findings also suggest that one would expect stronger positive pathways with newer brands. If an industry mostly consists of long-established brands with a high degree of familiarity, positive pathways may be small or difficult to measure.Future inquiries could consider these possibilities as well as identify additional moderating factors in which positive pathways might be operative. Historical or other secondary data analyses would assist in documenting the real financial and relational impact of encroachment for same brand outlets in low-brand-density markets. More broadly, such results could be investigated across other franchised service industries. Limitations and Future ResearchWe acknowledge several considerations in the summation of our results. First, the data limitation on location information of hotels led us to mainly focus on well-defined markets such as downtown areas, airports, and resort towns to isolate encroachment effects. Can we observe the effect of encroachment in other markets? Including suburban and rural metro locations that are likely small, well-defined market yields similar results (Web Appendix W4.5), suggesting that our findings are not limited to downtown, airport, and resort locations. Further information on hotel location data in the excluded markets would allow us to break them into local markets for inclusion in our analyses.Second, while consumer learning remains a possible explanation for our findings, our static model is unable to fully capture potential dynamics. A dynamic model based on individual choice data and experience would be required to determine that learning has occurred. We leave this as an avenue for future research.Finally, from a modeling perspective, while our results are robust with more granular fixed effects, our market × time fixed effects may not fully capture all unobserved demand shocks, which can bias results. Individual-level customer panel data would have been useful for determining which demand side explanations—consumer learning, brand awareness, or quality signaling—dominate. Although our study focuses on the presence of same brand hotels, the role of online advertising and consumer search behavior would further shed light on this topic. These topics remain fruitful directions for the future.  "
9,"Carbon Footprinting and Pricing Under Climate Concerns This article studies how organizations should design a product by choosing the carbon footprint and price in a market with climate concerns. The authors develop a model and first show how the cost and demand effects of reducing the product carbon footprint determine the profit-maximizing product design. They find that stronger climate concerns reduce the product carbon footprint, demand, the overall corporate carbon footprint and profit, but have an ambiguous impact on price. Next, the authors establish that offsetting carbon emissions can create a win-win outcome for the firm and the climate if the cost of compensation is sufficiently low. Going net zero leads to a win for society if the cost of offsetting is sufficiently low compared to the social cost of pollution created by the corporate carbon footprint. Third, the authors show how regulation in the form of a cap-and-trade scheme or a carbon tax affects product design, firm profitability, and green technology adoption. Finally, the authors extend the analysis to a competitive scenario and show that going net zero creates a win-win-win outcome for the firm, the climate, and society if the offset technology is sufficiently effective.Keywords: carbon footprint; carbon offsetting; climate impact; net-zero emissions; pricingThe consequences of climate change have become apparent and touch every corner of our society. Public opinion has reached a point where ""business as usual"" is hard to justify, and many organizations are pressed to find solutions. For instance, ""flight-shaming"" and the European Green Deal ([22]) pose a threat to the business model of airlines ([ 7]). Amid much fanfare, most major carriers are studying or already adopting approaches that are broadly in line with the three-step process ""measure, reduce, compensate"" outlined in the United Nations Climate Neutral Now initiative ([57]), essentially pledging to operate ""net-zero"" flights in the short run. Similarly, the automotive industry must urgently find ways to replace combustion engines to meet the increasing demand for low-emission vehicles and more stringent emission targets ([27]). Car makers around the world are rushing to bring electric vehicles to the market at reasonable prices.These recent developments, which generalize to organizations in logistics, fashion, retailing, and other sectors in the economy, underscore the importance of understanding climate concerns and making the necessary adjustments to one's offerings and prices. Marketing professionals play a critical role here because they are often tasked with sensing changes in consumer preferences and channeling them within an organization. According to a recent article, ""chief marketing officers should be involved in the development of the sustainability strategy based on what they can bring to the table: customer data, market analysis and audience insights"" ([ 8]). At the same time, however, marketing officials may lack the confidence to contribute to the debate and provide meaningful guidance to internal stakeholders ([43]).This article develops a model that helps marketers address climate concerns by optimizing carbon footprinting and pricing. It is well documented that consumers have climate concerns ([63]; [64]) and that media coverage of climate change motivates consumers to make more sustainable consumption decisions ([11]; [30]). The starting point of our analysis is a monopoly setting in which the firm designs a product by choosing its product carbon footprint and price, hereinafter referred to simply as product design. Calculating product carbon footprints—the climate impact per unit of product in carbon dioxide equivalent (CO2 eq) emissions—is now common practice ([42]; [60]), and these footprints are routinely certified based on international accounting standards ([26]; [34]). Using terminology from the [26], we define the product carbon footprint as ""cradle-to-gate emissions,"" which include production emissions (Scope 1) and emissions from purchased energy (Scope 2).[ 5] Importantly, reducing the product carbon footprint has both a cost effect due to the change in the unit cost to produce a greener product and a demand effect due to consumers' climate concerns.The key difference from a standard model where the firm chooses price and (environmental) quality is that the total number of purchases made by consumers determines not only the firm's profit but also its corporate carbon footprint ([28])—the aggregate climate impact of the firm across all units sold. The corporate carbon footprint causes a market externality that depends on the strength of the climate concerns and that a regulator may want to control. Figure 1 illustrates how the interplay between firm and consumers drives the market outcome (comprising product design, firm performance, and climate impact) and climate externality, and the role played by the regulator intervening to limit corporate carbon footprints.Graph: Figure 1. The interplay between firm, consumers, and regulator and the resulting market outcome under climate concerns.We derive three key results from this initial framework. First, we show how the profit-maximizing product carbon footprint depends on the relative size of the cost and demand effects of reducing the product carbon footprint. This insight reflects the familiar return-on-quality logic in the marketing literature ([50]; [51]; [52]) but accounts for consumers' climate concerns.Next, we show the impact of stronger climate concerns on the profit-maximizing product design. We demonstrate that it is optimal for a firm to decrease the product carbon footprint, and analyze the impact on price. In addition, we show that stronger climate concerns reduce firm profit.Finally, we show that stronger climate concerns reduce the overall corporate carbon footprint. This result occurs because stronger climate concerns reduce both the product carbon footprint and demand, which leads to a reduction in overall emissions. In other words, the greener product design further contributes to the reduction in overall emissions that results from the lower sales volume. Listening to the voice of consumers who demand greener products therefore helps organizations to reduce their corporate carbon footprint.We then extend the analysis in several directions. First, we consider the profitability of carbon offsetting, which compensates for a carbon footprint by reducing, avoiding, or sequestering carbon emissions elsewhere on the planet ([25]). Carbon offsetting is feasible because carbon emissions are a global (rather than local) environmental problem. Projects that result in carbon offsets tend to focus on renewable energy (such as building wind farms that replace coal-fired power plants) or carbon sequestration in soils or forests (such as agroforestry and tree-planting activities). Specifically, we allow the firm to purchase carbon offsets to attain a net-zero corporate carbon footprint. As a result, a firm may be able to offer a climate-neutral product even if its carbon footprint prior to offsetting is positive. We show that it is optimal for firms to go net zero if the compensation cost is sufficiently low relative to the demand-enhancing effect of reducing the product carbon footprint to net zero. In this case, going net zero is a win-win strategy for the firm and the climate.Second, we examine the profit-maximizing product design from a welfare perspective, effectively complementing the profit motive of the firm with respect for the environment and social justice ([ 6]; [32]; [35])—often referred to as the triple bottom line of profit, planet, and people ([21]). We show that, in the absence of carbon offsetting, the profit-maximizing corporate carbon footprint generally deviates from the socially optimal level. A net-zero corporate carbon footprint, in turn, is economically efficient if the cost of offsetting is sufficiently low compared with the social cost of the corporate carbon footprint.Third, we analyze how carbon regulation affects product design and the corporate carbon footprint. We study three common market interventions ([66]): carbon caps, cap-and-trade systems, and a carbon tax. We find that these interventions typically reduce firm profit. In addition, we show that these instruments are generally effective in curbing both the product carbon footprint and the corporate carbon footprint when taking the profit-maximizing price response into account. We also show how carbon regulation can accelerate green technology adoption.Finally, we extend our analysis to competition and illustrate how competitive carbon offsetting emerges in equilibrium if the offset technology is sufficiently effective. From a policy perspective, this suggests that providing efficient carbon removal technologies can accelerate the transition to a low-carbon economy. Table 1 provides an overview of the key findings and highlights the insights for marketers.GraphTable 1. Key Results and Insights for Marketers. TopicInsightProduct design (Propositions 1 and 2)Climate concerns affect the product carbon footprint and price, and thereby profit. Stronger climate concerns reduce the product carbon footprint and profit, but have an ambiguous impact on price.Climate impact (Proposition 3)Stronger climate concerns reduce demand and the corporate carbon footprint.Carbon offsetting (Proposition 4)Offsetting carbon emissions can create a win-win outcome for the firm and climate if the demand effect of reducing the corporate carbon footprint to net zero is sufficiently large compared with the cost of carbon removal.Corporate social responsibility (Proposition 5)Offsetting carbon emissions can create a win-win-win outcome for the firm, climate, and society if the cost of carbon removal is sufficiently low compared with the social cost created by the corporate carbon footprint.Regulation (Propositions 6–9)Carbon regulation in the form of binding carbon caps, cap-and-trade systems, and carbon taxation reduces firm profitability, stimulates green technology adoption, and typically leads to the design of greener products.Competitive strategy (Proposition 10)Stronger climate concerns reduce the product carbon footprint and the corporate carbon footprint of each firm. If the offset technology is sufficiently effective, going net zero creates a win-win-win outcome for the firm, the climate, and society. Taken as a whole, our results contribute to research on green product development ([10]) by showing how carbon footprinting and pricing are determined by the interplay of consumers' climate concerns ([37]), firm technology, and market regulation ([49]). By endogenizing product design, this article also adds to the return-on-quality literature ([50]; [50]; [52]). Importantly, we provide a welfare analysis to understand the implications of product-design decisions for corporate social responsibility and thereby add to the sustainability literature in marketing ([ 9]; [14]; [32]; [39]; [47]). Finally, we extend [10] and related literature in supply chain management and engineering ([12]; [17]; [29]; [67]) by accounting for the climate externality and providing the first analysis of carbon offsetting.Our results also contribute to the literature on regulation in economics ([ 4]) by showing how carbon caps and carbon taxes ([13]) affect product design. In addition, we show that climate regulation can trigger investments in green technologies, thereby adding to the insights of [49] on the dynamic impact of regulation and the economics of climate science more broadly ([31]; [46]; [54]). The ModelConsider a firm that designs a product (or service) by choosing the price  p≥0  and product carbon footprint  κ∈[0,κ¯]  . The set  [0,κ¯]  indicates the technologically feasible product carbon footprints, where the firm offers a green product with zero emissions if  κ=0  and a maximally polluting brown product if  κ=κ¯  . The technology of the firm results in the unit cost function  c(κ)  defined on  [0,κ¯]  , where  c′(κ)≠0  is the change in unit cost in response to a change in the product carbon footprint κ.[ 6] If  c′(κ)<0  , reducing the product carbon footprint increases unit cost. The opposite is true if  c′(κ)>0  .We consider a market with consumers who have climate concerns and evaluate the product based on not only its intrinsic features and price p but also its carbon footprint κ. Without loss of generality, the mass of consumers is normalized to unity. A buyer derives utility u(κ,p;λ)=v−p−z(κ;λ)−E, Graph1where  v∈[0,∞)  is the valuation of the intrinsic features;  z(κ;λ)≥0  measures the disutility from purchasing a product with carbon footprint κ, with  λ≥0  capturing the strength of climate concerns; and  E≥0  is the disutility from the climate externality caused by other buyers. Because a single buyer has no impact on the climate externality, E is the same irrespective of whether or not the consumer purchases the product. By normalizing the (intrinsic) utility of the outside option to zero, a consumer purchases the product if v exceeds the perceived price  p+z(κ;λ)  .The unobserved valuation v is distributed independently across consumers according to the cumulative distribution function  F(v)  . The disutility  z(κ;λ)  is assumed to increase at an increasing rate in the product carbon footprint κ, reflecting the increasing guilt or ""cold prickle"" ([ 3]) of consumers from purchasing a product that affects the climate. Formally, letting subscripts denote first and second partial derivatives, the convexity assumption  z(κ;λ)  can be restated as  zκ(κ;λ)>0  and  zκκ(κ;λ)≥0  . We set the disutility to zero if consumers do not have climate concerns or if the product is green, that is,  z(κ;0)=z(0;λ)=0  .[ 7] The other boundary case occurs if consumers have strong climate concerns, in which case we assume that  limλ→∞z(κ;λ)=κ  . We further assume that stronger climate concerns increase the disutility from a given carbon footprint, that is,  zλ(κ;λ)>0  . Finally, we assume that stronger climate concerns increase the marginal disutility of increasing κ, that is, zκλ(κ; λ) > 0.Consumers purchase if the utility from the product exceeds the utility from the outside option. Therefore, the demand for the product is derived as D(κ,p;λ)=1−F[p+z(κ;λ)]. Graph2Demand is decreasing in the product carbon footprint and price. Interpreting the product carbon footprint as an inverse measure of product quality, a lower κ means higher quality and therefore higher demand. Lowering the product carbon footprint implies demand neutrality when consumers do not care about the climate impact of the product  (Dκ=0)  and demand expansion when consumers have climate concerns  (Dκ<0)  . The novel aspect of our modeling approach is that ""product quality"" affects not only demand but also the corporate carbon footprint (i.e., the overall climate impact of the firm).The corporate carbon footprint results from multiplying the product carbon footprint by demand and is therefore given by  Φ=κ D(κ,p;λ)  . Note that if buyers do not fully account for their carbon emissions, they create a climate externality—""the biggest market failure the world has seen"" ([54], p. 1). The climate externality results from adding up the noninternalized carbon emissions across buyers: E(κ,p;λ)=[κ−z(κ;λ)]D(κ,p;λ). Graph3This climate externality is reduced to zero when consumers have strong climate concerns (  z(κ;λ)=κ  ) and equals the corporate carbon footprint if consumers do not care about purchasing a product that affects the climate (  z(κ;0)=0  ). Therefore, the corporate carbon footprint has an impact on all consumers if buyers do not fully account for the product carbon footprint when making their purchase decision. Product DesignThis section first derives the profit-maximizing product carbon footprint and price of a product. We then study the impact of stronger climate concerns on these variables. Finally, we consider the impact of product design on the corporate carbon footprint. We assume throughout that the profit function is strictly concave in κ and p and thus has a unique constrained global maximum. Product Carbon Footprint and PriceThe firm chooses the product carbon footprint κ and the price p of the product to maximize profit. More formally, the firm solves maxκ,p π(κ,p;λ)=[p−c(κ)]D(κ,p;λ) Graph4 s.t. 0≤κ≤κ¯. GraphThe profit function shows that the product carbon footprint and price have a dual impact on markup and demand. Proposition 1 characterizes the profit-maximizing product design with product carbon footprint  κ*  and price  p*(κ*)  . To facilitate exposition, all proofs are relegated to the Appendix. Proposition 1:  If reducing the product carbon footprint lowers unit cost, the firm should offer a green product with  κ*=0  at price  p*(0)  , irrespective of the demand effect. If reducing the product carbon footprint increases unit cost but not demand, then it is optimal to offer a brown product with  κ*=κ¯  at price  p*(κ¯)  . Finally, if the demand effect is sufficiently strong compared with the cost effect, then it is optimal to offer a product with  κ*∈(0,κ¯)  at price  p*(κ*)  .Proposition 1 mirrors the familiar return-on-quality logic in the marketing literature ([50]; [50]; [52]) and has two important implications. First, if lowering the product carbon footprint reduces unit cost, then it is optimal to increase efficiency and thereby increase ""process quality"" ([15]; [16]). Green cost cutting is more attractive when lowering the product carbon footprint not only reduces cost but also increases demand ([48]). This result helps explain why many sustainability efforts increase firm profit ([65]).Second, if lowering the product carbon footprint increases unit cost, there may be a trade-off between the cost effect and the demand effect. Absent a demand effect, reducing the product carbon footprint below that of the brown product only results in higher unit cost and is therefore suboptimal under profit maximization. However, when the increase in demand outweighs the impact of higher unit cost, firms should reduce the product carbon footprint relative to the brown product. In contrast to cost-cutting sustainability, cost-increasing sustainability reflects the idea that ""major pressure for changing marketing practices may come from consumers themselves"" ([37], p. 133) and can be viewed as one of the ""sustainability programs worthy of the name"" ([19]). Figure 2 summarizes the product design strategies derived in Proposition 1.Graph: Figure 2. Profit-maximizing product design as a function of the cost effect (due to the change in unit cost) and the demand effect (due to consumers' climate concerns). Impact of Climate Concerns on Product DesignStronger climate concerns affect demand and thereby product design and firm profitability. The next result summarizes the implications. Proposition 2:  If reducing the product carbon footprint lowers unit cost, stronger climate concerns do not affect the profit-maximizing product carbon footprint and price, and leave profit unchanged. Instead, if reducing the product carbon footprint increases unit cost, stronger climate concerns decrease the product carbon footprint, have an ambiguous impact on price, and reduce profit.Proposition 2 has two important implications. First, it shows how climate concerns affect the profit-maximizing product design. Stronger climate concerns increase the consumers' marginal disutility of raising κ, which motivates the firm to reduce the product carbon footprint to make the product more attractive to consumers. The impact on the price is ambiguous because stronger climate concerns not only increase the unit cost due to the lower product carbon footprint, but also compress the price-cost margin.[ 8] Note that if we interpret the product carbon footprint as an inverse measure of product quality, then Proposition 2 implies an ambiguous relationship between product quality and price, which contributes to the literature on price-quality relationships ([24]; [48]).Second, Proposition 2 implies that a monopoly firm has a motive to downplay climate concerns due to their negative impact on profit. This suggests an intuitive explanation for ""dither and denial"" ([62]) by polluting firms in the face of climate change ([38]; [40]). This result also points to a potential tension between product managers who tend to focus on profit and managers who are in charge of corporate social responsibility. As we will show next, one way firms can resolve this tension is by broadening the scope of performance measurement beyond profit to include climate and societal impact. Corporate Carbon FootprintThe first two propositions extend the logic of profit-maximizing product design to a setting where consumers have climate concerns. The goal of this subsection is to provide new insights on how changes in climate concerns affect the climate impact of the firm. Proposition 3:  Stronger climate concerns reduce demand and the corporate carbon footprint  Φ*=κ* D(κ*,p*;λ)  .Proposition 3 shows that stronger climate concerns necessarily reduce the corporate carbon footprint. The reason is that stronger climate concerns reduce both the product carbon footprint and demand, which leads to a reduction in overall emissions. This is a strong result, because the positive demand effect of offering a greener product could be expected to compensate for the lower product carbon footprint—similar to the rebound effect from technological progress (Alcott 2005), which suggests that higher efficiency leads to an initial reduction in demand for a resource that is outweighed by a corresponding increase in demand due to relatively lower resource cost (""Jevons paradox""). In our setting, the rebound effect cannot occur because stronger climate concerns increase the disutility from a given carbon footprint (zλ (κ; λ) > 0), which translates into an overall reduction in demand. In contrast, in a setting where zλ (κ; λ) < 0, stronger climate concerns may lead to a rebound effect because of the resulting increase in demand, an outcome that is conceivable if consumers use the brown product (rather than the green product) as a reference point.[ 9] In sum, whenever listening to the voice of consumers leads to greener product design and lower demand, the consumer pressure for greener products motivates profit-maximizing organizations to become greener, even though the impact on profit is negative. Proposition 3 thus suggests that consumers play an important role in making both products and organizations greener. Carbon OffsettingWhile producing a green product is perhaps the most obvious means for a firm to achieve climate neutrality, an increasingly popular alternative is to adopt an offset strategy whereby the corporate carbon footprint is fully compensated for (by funding projects that achieve an equivalent level of carbon dioxide saving), thereby creating a net-zero corporate carbon footprint. While carbon offsetting is arguably not the solution to climate change, it allows firms to achieve climate neutrality even if the available production technology does not yet allow it. In principle, any company can go net zero by buying offset services (that promote the planting of trees, renewable energy, etc.) from providers such as Carbon Footprint Ltd or Gold Standard.Accordingly, the purpose of this section is to study under what conditions firms can benefit from adopting an offset strategy. Suppose that an offset provider charges a fixed price  ω≥0  per unit of carbon offset. The firm then chooses the product carbon footprint κ and the price p to maxκ,p π(κ,p;ω)=[p−c(κ)−ωκ]D(0,p) Graph5 s.t. 0≤κ≤κ¯, Graphwhere  ωκD(0,p)  is the total offsetting cost of reaching a net-zero corporate carbon footprint if the product carbon footprint prior to offsetting is κ. That is, with carbon offsetting purchase decisions and demand depend on the net-zero product carbon footprint rather than the product carbon footprint prior to offsetting. The next result points to the possibility of a win-win outcome for the firm and the climate, where the benchmark is provided by the no-offset strategy. Proposition 4:  Adopting an offset strategy is optimal for a firm if the compensation cost is sufficiently low compared with the additional profit from the demand-enhancing effect of reducing the product carbon footprint to net zero. Stronger climate concerns make the adoption of an offset strategy more attractive. The downside of an offset strategy is that it motivates a firm to increase the product carbon footprint before offsetting if the price per unit of carbon offset is sufficiently low.Proposition 4 shows that offsetting carbon emissions can boost profit and fight climate change. The key driver of this result is that relieving consumers from disutility resulting from consuming a product with a positive carbon footprint has a demand-enhancing effect that directly translates into higher profit. A firm is more likely to adopt an offset strategy if the price per unit of carbon offset is low. This suggests that providing low-cost carbon offset options to firms might curb their corporate carbon footprints even when the standard tools of carbon regulation have no bite. Corporate Social ResponsibilitySustainability is an umbrella term generally viewed as comprising economic profitability, respect for the environment, and social justice ([ 6]; [32]; [35]). To integrate these three ingredients into the analysis, we say that a firm behaves in a manner that is consistent with corporate social responsibility if it maximizes welfare. To do so, it must consider the triple bottom line of profit (firm and offset provider), planet (climate impact), and people (consumer surplus). Our next result shows that the adoption of an offset strategy can create a win-win-win outcome. Proposition 5:  Without offsetting, the corporate carbon footprint is generally nonzero and different from the socially optimal level. Adopting an offset strategy that leads to net-zero carbon emissions improves welfare if the cost of carbon offsetting is sufficiently low compared with the social cost created by the corporate carbon footprint.Proposition 5 confirms the notion that focusing exclusively on profit leads firms to make decisions that are generally inconsistent with corporate social responsibility. Intuitively, a firm has an incentive to strategically distort the product carbon footprint to exploit pricing power, which leads to an economically inefficient product carbon footprint ([53]). Interestingly, under an offset strategy, profit-maximization may result in a net-zero corporate carbon footprint even if it is socially undesirable to fully compensate for the emissions because the firm does not factor in the social cost of carbon removal. However, if the carbon removal technology is sufficiently cost effective, the win-win outcome for the firm and the climate under an offset strategy translates into a win-win-win outcome and therefore produces benefits for society at large.In addition, Proposition 5 sheds light on the controversial debate about carbon offsets that ""have been used by polluters as a free pass for inaction"" ([58]). The cost efficiency of carbon offsetting stems from the fact that emissions are compensated for in places where the cost of offsetting is low, typically in developing countries. While this makes sense from an economic perspective, managers have to bear in mind ""whose mess this is"" and that ""some of these places would welcome investment in reforestation and afforestation, but they would also need to be able to integrate such endeavours into development plans which reflect their people's needs"" ([20]). Carbon RegulationRegulators increasingly try to limit carbon emissions of firms to meet climate targets and address climate change. The most recent examples include the Green New Deal in the United States and the European Green Deal, which address climate change by introducing various regulatory interventions. We show how a firm should respond to carbon caps, cap-and-trade systems, and carbon taxes, which are by far the most common regulatory market interventions today ([66]), and study their impact on expected firm profitability. While the institutional details of these interventions vary across industries and legislations, we focus on their key characteristics and show that the risk of regulation accelerates investments in green technology. Carbon CapsThe most direct approach to limit the corporate carbon footprint is to impose a binding carbon cap  R≥0  . An example is the European Union's fleet-wide binding emissions target for new cars imposed on manufacturers ([23]). In the face of such regulation, the firm solves the following problem: maxκ,p π(κ,p;λ)=[p−c(κ)]D(κ,p;λ) Graph6 s.t. 0≤κ≤κ¯ and Φ(κ,p;λ)≤R, Graphwhere  Φ(κ,p;λ)  is the corporate carbon footprint. The next result summarizes the impact of a binding carbon cap. Proposition 6:  A binding carbon cap reduces the corporate carbon footprint and profit, and translates into a lower product carbon footprint if the sales expansion that results from offering a greener product is sufficiently small.A binding carbon cap has the obvious effect of reducing the corporate carbon footprint and profit. More interestingly, restricting overall emissions forces the firm to adjust the profit-maximizing product design by lowering the product carbon footprint because this helps to relax the carbon constraint if the sales expansion from offering a greener product is sufficiently small compared with the direct reduction in overall emissions. In real-world markets, however, carbon caps are often coupled with a carbon market, where firms can sell or purchase carbon allowances, which gives rise to cap-and-trade systems. Cap-and-Trade SystemsThe leading examples of cap-and-trade systems are California's Cap-and-Trade Program, the Chinese National Carbon Trading Scheme, and the European Union Emissions Trading System. Cap-and-trade systems have an important advantage over carbon caps: firms with low compliance costs can sell carbon allowances in the emissions market and turn them into a source of revenue. For example, Tesla generates significant revenues by selling zero-emission vehicle credits in the United States ([41]).The society's need to tackle climate change creates considerable uncertainty for businesses regarding their regulatory environment. To address how a firm can proactively deal with the possible introduction of regulation, we assume that a regulator is expected to implement a cap-and-trade system with probability  ρ∈[0,1]  , with a given carbon cap  R≥0  . Under regulation, the firm can choose between two options: ( 1) adjust the product design to meet the potential regulatory constraint at the firm level (profit  πr  ) or ( 2) stick to the current product design and purchase carbon allowances at a market price  ϖ≥0  . The following result summarizes the impact of a binding carbon cap coupled with the possibility to buy carbon allowances in the emissions market. Proposition 7:  The expected cost of a cap-and-trade system to the firm is given by  ρmin{π*−πr,ϖ(Φ*−R)}  , where  ρ(π*−πr)  is the expected reduction in profit if the firm complies with the carbon cap by adjusting product design, and  ρϖ(Φ*−R)  is the expected reduction in profit if the firm purchases carbon allowances to offset the emissions. The expected cost increases when the implementation probability ρ is higher, when the carbon cap R is more severe, and when the carbon price ϖ is higher.Proposition 7 confirms the intuition that uncertain cap-and-trade regulation reduces the expected profit of the firm. Furthermore, the cost of regulation to the firm is increasing in the probability of regulation and the market price for emissions. This is important because companies should anticipate changes in the regulatory environment and thus want to invest in the adoption of a greener technology to comply with expected regulation. Carbon TaxIn December 2019, the International Monetary Fund issued a report suggesting that a global average carbon price of $70 a ton would be sufficient for many (but not all) countries to meet their Paris accord mitigation targets ([33]). While a carbon cap directly limits the climate impact of the firm, such a price alters the cost structure of the firm with the goal of reducing the corporate carbon footprint to a socially desirable level. To reflect this, assume that  t≥0  is the fixed (Pigouvian-style) tax rate on carbon emissions. Under such a proportional carbon tax, the firm solves maxκ,p π(κ,p;λ,t)=[p−c(κ)−tκ]D(κ,p;λ) Graph7 s.t. 0≤κ≤κ¯. GraphThe next result summarizes the impact on the product design and firm profit, where the optimized profit under a carbon tax is denoted by  π*(t)  . Proposition 8:  A proportional carbon tax reduces not only the profit-maximizing product carbon footprint but also the corporate carbon footprint. The expected cost of taxation is given by  ρ{π*(0)−π*(t)}>0  , which increases in the probability ρ that a tax will be implemented and in the tax rate t.Proposition 8 shows that a higher carbon tax effectively reduces the corporate carbon footprint via its impact on the profit-maximizing product carbon footprint and price in response to higher cost, which leads to an overall reduction in sales. The result also shows that the uncertain introduction of a carbon tax reduces expected profit: The increase in the carbon tax increases unit cost, but only part of this can be passed on to consumers in the form of higher prices—a result akin to the imperfect pass-through of trade deals documented in the channels literature ([44]; [45]). The adverse impact on profit helps explain why firms lobby against regulation ([61]).That said, the result also shows why an offsetting strategy is an interesting option: a net-zero corporate carbon footprint makes regulation unnecessary and has an immediate positive impact on the climate. A carbon tax, in turn, affects the product carbon footprint and raises revenue for the government without offsetting the emissions. However, carbon offsets do not provide an incentive for firms to invest in green technologies and are therefore often considered an interim measure until new technologies become available. Green Technology AdoptionThe need to comply with carbon regulation may trigger investments in green technologies. To demonstrate this, we consider the case of a carbon cap and assume that an existing brown technology  c0(κ)  can be replaced with a green technology  c1(κ)  at a fixed cost  f1>0  . This green technology enables the firm to reach any product carbon footprint at a lower unit cost, that is,  c0(κ)≥c1(κ)  for all κ. Letting ρ denote the probability that carbon regulation becomes effective, we derive the following result. Proposition 9:  The threat of carbon regulation stimulates green technology adoption if the anticipated carbon regulation reinforces the profit advantage of adopting the green technology.Proposition 9 shows that regulatory risk provides an incentive for the firm to adopt the green technology. In other words, the mere threat of carbon regulation can prompt a reduction of the corporate carbon footprint and lead to process innovation ([49]). More broadly, from a policy perspective, the threat of effective regulation allows the government to put some of the burden of technology adoption on the shoulders of the firm. Competitive StrategyIn this section, we extend the baseline case to include competition. We first describe the interaction between two firms and consumers and then study conditions under which adopting an offset strategy is consistent with pursuing a triple bottom line. SetupWe consider a market with two single-product firms  i=1,2  that simultaneously choose the product carbon footprint  κi  and price  pi  . The technology of firm i is represented by the unit cost function  ci(κi)=ci0(1−κi)2  , where  ci0>0  is a firm-specific cost parameter. Carbon offsetting to achieve a net-zero product carbon footprint is provided by an independent provider at cost  ω≥0  per unit of carbon emissions. The carbon removal technology of the provider is represented by the unit cost  ϕ≥0  and fixed cost  F>0  . Each firm can choose between two strategies: a no-offset strategy where the product is marketed with carbon footprint  κi  , or an offset strategy where the product is marketed with a net-zero carbon footprint.The products are differentiated horizontally and vertically. Horizontal differentiation is à la Hotelling and reflects consumer heterogeneity with respect to intrinsic product features. We assume that the firms are located at the extremes of the characteristics space  [0,1]  , that is,  x1=0  and  x2=1  . Vertical differentiation on the carbon footprint reflects the notion that a lower product carbon footprint enhances the worth of the product in the minds of consumers. Category demand is fixed, and the market consists of a unit mass of consumers. We assume that individual preferences are described by the conditional indirect utility function ui(κi,pi;λ)=v−pi−zi(κi;λ)−12|x−xi|−E, Graph8where v is the valuation of the intrinsic product features,  zi(κi;λ)=λκi  is the disutility from purchasing a product with carbon footprint  κi  , and  E≥0  is the disutility from the climate externality caused by other buyers in the market. Following convention, we let  x∈[0,1]  denote the consumer's preferred product characteristic and  |x−xi|  denote the horizontal distance to the product of firm i ([ 2]). The preferred product characteristics are drawn independently across consumers from a uniform distribution over the interval  [0,1]  . Demand for the product of firm i as a function of the product carbon footprints  κ=(κ1,κ2)  and prices  p=(p1,p2)  can be derived as Di(κ,p;λ)=12−λ(κi−κj)−(pi−pj). Graph9Each firm can therefore obtain a competitive advantage over its rival by offering a product with a lower carbon footprint, by charging a lower price, or both. Competitive Carbon OffsettingIn a setting with two firms and two strategic options per firm, there are a total of four possible outcomes: both firms adopt a no-offset strategy, both firms adopt an offset strategy, or one firm adopts an offset strategy while the other firm adopts a no-offset strategy. To illustrate the emergence of competitive carbon offsetting, we consider symmetric firms with  c10=c20=1  and let λ = 1 represent strong climate concerns. The following result holds. Proposition 10:  Suppose that firms are symmetric and consumers have strong climate concerns. Then, if the offset technology is sufficiently effective, each firm benefits from adopting an offset strategy irrespective of the rival's choice of strategy. This leads to a net-zero industry carbon footprint and improves welfare.To understand the intuition for this result, consider the profits that each firm can earn in the four possible outcomes represented in Figure 3. From firm 1's point of view, it is always more profitable to adopt an offset strategy than a no-offset strategy, no matter whether firm 2 chooses a no-offset strategy (which yields profit  A>14  ) or an offset strategy (which yields profit  14>B  ). Because the firms are symmetric, the same logic applies to firm 2, no matter whether firm 1 chooses a no-offset strategy or an offset strategy. In other words, each firm can create a win-win for itself and the climate by adopting the offset strategy—irrespective of the rival's choice of strategy. Therefore, the offset strategy is strictly dominant for each firm, and competitive carbon offsetting emerges in equilibrium.Graph: Figure 3. Possible outcomes and corresponding profits for c10=c20=1 and λ=1, where the top left number is the profit of Firm 1 and the bottom right number the profit of Firm 2.Interestingly, Proposition 10 further shows that if the offset technology is sufficiently cost effective, competitive forces can create a win-win-win outcome for each firm, the climate, and society. Therefore, choosing an offset strategy is consistent with pursuing corporate social responsibility. This has an important implication for policy makers: Providing efficient carbon removal technologies can accelerate the transition to a zero-carbon economy by providing incentives for firms to offer products and services with a net-zero product carbon footprint. DiscussionThis article explored how organizations should design a product by choosing the carbon footprint and price in a market with climate concerns. We also analyzed how changes in product design affect profitability and the organization's overall climate impact—the corporate carbon footprint. Furthermore, we analyzed how offset strategies and carbon regulation can be used to limit the corporate carbon footprint, and how they affect green technology adoption. Finally, we examined the role of competition for product-design decisions and carbon offsetting.Throughout, the underlying objective was to help marketing professionals understand how climate concerns translate into optimal carbon footprinting and pricing decisions. With this in mind, the current section elaborates on the implications of our results for organizations, policy makers, and consumers. We end by discussing some of the limitations of our work and avenues for future research. Implications for OrganizationsWhen confronted with climate concerns, the first response of an organization should be to assess the carbon footprint of its product and understand the impact on cost. If it is possible to reduce the product carbon footprint and reduce cost, eliminating waste (e.g., improving energy management) and adjusting price is the obvious consequence. However, if reducing the product carbon footprint is costly, it is imperative for marketers to understand the trade-off with demand (via, e.g., market research). They can then advise their organizations on how to adjust the product design in response to stronger climate concerns.Another key consideration is how changes in product design affect the organization's overall climate impact. Because marketers lower the product carbon footprint and adjust price in response to stronger climate concerns, the changes in product design lead to a lower corporate carbon footprint as both the product carbon footprint and demand are reduced. Therefore, greener products lead to greener organizations. At the same time, the consumer pressure for greener products reduces profit. To save the cost of making the product greener, organizations may decide to go net zero by offsetting the carbon emissions. For example, UPS offers a carbon-neutral shipping service with net-zero carbon emissions ([59]), while Kering ""will become carbon neutral within its own operations and across the entire supply chain"" ([36]). EasyJet announced its decision to go net zero and claims to be ""the first major airline to offset the carbon emissions from the fuel used for every single flight"" ([18]). In theory, net-zero carbon footprints are consistent with corporate social responsibility if the social cost of carbon compensation is sufficiently low. However, in practice, carbon compensation remains an imperfect solution that falls short of green product development.A third consideration is whether competition forces otherwise brown organizations to offset their carbon emissions. We showed that competition has the potential to prevent an industry from a race to the bottom where firms offer brown products. For instance, several European airlines offset their carbon emissions on domestic flights to compete for climate-concerned consumers. Implications for Policy MakersWhile carbon regulation effectively limits the organization's overall climate impact, it imposes a cost of regulation on organizations. This is arguably the reason why policy makers hesitate to implement effective carbon regulation. Our analysis shows that the mere threat of regulation negatively affects firm profitability. On the positive side, a well-designed market intervention benefits consumers and society at large, and stimulates green technology adoption.More generally, our work suggests that society should put a price on carbon emissions. Carbon offsets and carbon taxes achieve this goal. However, carbon offsets do not provide an incentive for firms to invest in greener technologies and, therefore, should be considered an interim measure until new technologies become available. The recent call by the United Nations Global Compact to set an internal price at a minimum of $100 per metric ton by 2020 is an attempt to price carbon emissions and put climate change at the heart of corporate strategy ([56]). Implications for ConsumersOur research suggests that organizations should act on consumers' climate concerns even if it reduces their profit, because not doing so would result in even lower profit. Voicing stronger climate concerns in our model necessarily reduces the corporate carbon footprint. However, in pressuring organizations into offering greener products, consumers may end up paying higher prices.In addition, our research shows that consumers with stronger climate concerns cause a smaller climate externality and thereby reduce the burden they impose on society. Stronger climate concerns also increase the profitability of carbon offsetting, which may stimulate the transition to a net-zero carbon economy. More broadly, our analysis suggests that ""green consumerism"" has a real impact on market outcomes. Limitations and Future ResearchFuture research could study how climate concerns are shaped and how they affect the consumers' utility function. Exploring preferences is key to understanding the impact of stronger climate concerns on product design and the overall corporate carbon footprint. One approach is to assume that climate concerns are influenced by opinion leaders. Another option is to assume that the organization can influence climate concerns via persuasive advertising. Yet another issue for future research is whether changes in climate concerns monotonically affect purchase decisions.Second, future research could consider emissions that occur during the consumption stage (Scope 3). This would allow marketers to understand what drives the life-cycle carbon footprint of a product (a cradle-to-grave approach). The interesting aspect of such an extension is that the emissions in the consumption phase are driven by consumer behavior that cannot be easily influenced by the firm.Third, it would be interesting to study the role of competition in a more nuanced way. A limitation of our approach is that it ignores the possibility of market expansion. Researchers could also study the impact of carbon regulation and taxation on industry dynamics and their potential to accelerate the transition to a zero carbon economy.Overall, this article highlights some of the complexities and consequences of climate concerns on product design and corporate carbon emissions. Hopefully, this will spur further research into understanding the impact of climate-dependent preferences and exploring the system-wide effects of government actions, including the determination of offset prices. We also hope to see multiple approaches brought to bear in the area, including agent-based simulations, data-based empirical analyses, and natural experiments. Ideally, this will lead to creative regulations and behaviors that result in win-win-win outcomes for consumers, organizations, and the environment or at least, absent that, better understanding of the trade-offs being made among the three parties. AppendixProof of Proposition 1. Assuming that the profit function  π(κ,p;λ)  is strictly concave in  (κ,p)  , the profit-maximizing product carbon footprint  κ*  and price  p*  must satisfy the following necessary and sufficient Kuhn–Tucker conditions (the multipliers  μ1≥0  and  μ2≥0  are associated with the inequality constraints): −c′(κ*)D(κ*,p*;λ)+[p*−c(κ*)]Dκ(κ*,p*;λ)+μ1−μ2=0, GraphA1 D(κ*,p*;λ)+[p*−c(κ*)]Dp(κ*,p*;λ)=0, GraphA2 μ1κ*=0 and μ2(κ*−κ¯)=0. GraphDepending on the slope of the unit cost function, we distinguish two cases. First, we consider the case where  c′(κ)>0  . Suppose that  Dκ=0  and  κ*>0  . Then, Equation A1 leads to a contradiction as  μ1=0  , so that  κ*=0  . This result holds a fortiori if  Dκ<0  . Second, we assume that  c′(κ)<0  . If  Dκ=0  , then a solution that involves  κ*<κ¯  leads to a contradiction in Equation A1, so that  κ*=κ¯  . Next, if  Dκ<0  , then the choice of the product carbon footprint is governed by the relative strength of the cost effect and the demand effect of increasing κ: If  −c′(0)D+[p*−c(0)]Dκ≤0  , then  κ*=0  , whereas if  −c′(κ¯)D+[p*−c(κ¯)]Dκ≥0  , then  κ*=κ¯  ; otherwise, there is an interior solution with  κ*∈(0,κ¯)  . □Proof of Proposition 2. First, from Proposition 1,  κ*=0  and  p*(0)  if  c′(κ)>0  . Because  D(0,p*[0];0)=D(0,p*[0];λ)  for  λ>0  , stronger climate concerns leave product design and profit unchanged.Second, if  c′(κ)<0  , there are two subcases: the emergence and the reinforcement of climate concerns. In the absence of climate concerns (  λ=0  ), profit at  κ¯  is given by  π(κ¯,p;0)=[p−c(κ¯)]D(κ¯,p;0)  . Instead, when consumers have climate concerns (  λ>0)  , profit at  κ*≤κ¯  is given by  π(κ∗,p;λ)=[p−c(κ∗)]D(κ∗,p;λ)  . Because the emergence of climate concerns reduces demand and (weakly) increases unit cost, this implies that π(κ∗,p∗;λ)<π(κ¯,p0;0), GraphA3where  p∗=argmaxpπ(κ∗,p;λ)  and  p0=argmaxpπ(κ¯,p;0)  , which means that the emergence of climate concerns reduces profit. Instead, when climate concerns are reinforced, applying the envelope theorem yields π(κ∗,p∗;λ)Dλ=[p∗−c(κ∗)]Dλ(κ∗,p∗;λ)<0, GraphA4where  Dλ=−F(p+z(κ;λ))zλ(κ;λ)  from Equation 2 and  zλ(κ;λ)>0  by assumption, which means that the reinforcement of climate concerns reduces profit.To understand the impact of reinforced climate concerns on product design, suppose that  κ*∈(0,κ¯)  , so that the multipliers  μ1  and  μ2  are zero in Equations A1 and A2. Substituting Equation A2 into Equation A1 yields  −cʹ(κ*)−Dκ/Dp=0  , which can be expressed in model primitives as −cʹ(κ*)=zκ(κ*;λ) GraphA5by using that  Dκ=−F(p+z(κ;λ))zκ(κ;λ)  and  Dp=−F(p+z(κ;λ))  . Applying the implicit function theorem to Equation A5 yields dκ∗dλ=−zκλ(κ∗;λ)cʺ(κ∗)+zκκ(κ∗;λ)<0, GraphA6because the denominator is positive by the concavity assumption and  zκλ(κ*;λ)>0  by assumption. To see the impact of stronger climate concerns on price, note that Equation A2 can be expressed in model primitives as p*=c(κ*)+1−F(p*+z(κ*;λ))F(p*+z(κ*;λ)). GraphA7From the implicit function theorem, dp∗dλ=−[f(⋅)+(p∗−c(κ∗))fʹ(⋅)]zλ(κ∗;λ)2f(⋅)+(p∗−c(κ∗))fʹ(⋅)+cʹ(κ∗)dκ∗dλ. GraphA8The concavity assumption and  zλ(κ*;λ)>0  imply that the first term on the right-hand side of Equation A8 is negative, whereas the second term is positive by Equation A6 and the assumption that  cʹ(κ)<0  . Therefore, the impact of stronger climate concerns on the profit-maximizing price is ambiguous.Proof of Proposition 3. The corporate carbon footprint results from multiplying the product carbon footprint by demand and can therefore be written as Φ*(λ)=κ*(λ)[1−F(p*+z(κ*;λ))]. GraphA9Differentiating Equation A9 with respect to λ yields dΦ*(λ)dλ=dκ*dλ[1−F(⋅)]−κ*f(⋅)[dp*dλ+zκdκ*dλ+zλ]  =dκ*dλ[1−F(⋅)]−κ*[f(⋅)]2zλ(κ*;λ)2f(⋅)+(p*−c(κ*))fʹ(⋅),  GraphA10where the second equality follows from substituting the expressions for  dκ* /dλ  and  dp* /dλ  in Equations A6 and A8, respectively, and using that  −cʹ(κ*)=zκ(κ*;λ)  from Equation A5. Because  zλ(κ*;λ)>0  and  2F(⋅)+(p*−c(κ*))fʹ(⋅)>0  by the concavity assumption, stronger climate concerns reduce demand and therefore the corporate carbon footprint because  Dκ*Dλ<0  by Proposition 2.Proof of Proposition 4. Suppose that  c′(κ)<0  . Under carbon offsetting, the product has a net-zero carbon footprint, which implies that z(0,λ) = 0 and thus that demand D(0,p) is independent of λ. An offset strategy yields the profit π(κo,po;ω)=[po−c(κo)−ωκo]D(0,po), Graphwhere  κo  is the product carbon footprint prior to offsetting and  po  is the corresponding price. Noting that  π(κ¯,po;ω=0)=π(κ¯,p0;λ=0)  , it follows from Equation A3 that  π(κ¯,po;ω=0)>π(κ*,p*;λ)  . Applying the envelope theorem to the optimal profit under offsetting yields  dπ(κo,po;ω)/dω=−κoD(0,po)<0  . This implies there exists  ω¯  such that  π(κo,po;ω)>π(κ*,p*;λ)  for  ω∈(0,ω¯)  , which means that the firm can benefit from adopting a climate neutral strategy when ω is sufficiently low. Next, stronger climate concerns reduce profit in the benchmark case absent carbon offsets by Equation A4 (  dπ(κ*,p*;λ)/dλ<0  ), whereas they leave profit unaffected under an offset strategy (  dπ(κo,po;ω)/dλ=0  ) because demand is independent of λ. Consequently, stronger climate concerns make the adoption of an offset strategy more attractive to the firm.Absent carbon offsets, the optimal product carbon footprint is determined by the condition  −cʹ(κ*)=zκ(κ*;λ)  from Equation A5. Under an offset strategy, the firm chooses  κo  to maximize the markup  p−c(κ)−ωκ  because demand does not depend on the choice of κ. Therefore, at an interior solution,  κo  is determined by the condition  −cʹ(κo)=ω  . Clearly,  κo>κ*  if  ω<zκ(κ*;λ)  . Note that this result holds a fortiori at a corner solution where  ω<−c′(κ¯)  . Thus, an offset strategy motivates a firm to increase the product carbon footprint before offsetting the emissions if ω is sufficiently low.Proof of Proposition 5. Following convention, we define welfare as the sum of consumer surplus and profit. Consumer surplus is obtained by adding up the utilities from buyers and nonbuyers: S(κ,p;λ)=∫p+z(κ;λ)∞[v−p−z(κ;λ)−E]dF(v)+∫0p+z(κ;λ)[−E]dF(v) Graph =∫p+z(κ;λ)∞[v−p−z(κ;λ)]dF(v)−E Graph =∫p+z(κ;λ)∞vdF(v)−pD(κ,p;λ)−Φ(κ,p;λ), GraphA11where the third equality uses the definition of demand in Equation 2, the definition of the market externality in Equation 3, and where  Φ(κ,p;λ)  denotes the corporate carbon footprint. Summing consumer surplus in Equation A11 and profit in Equation 4 yields welfare: W(κ,p;λ)=∫p+z(κ;λ)∞vdF(v)−c(κ)D(κ,p;λ)−Φ(κ,p;λ). GraphA12Drawing on [53], let  π¯(κ)≡maxp π(κ,p;λ)  and  W¯(κ)≡maxp W(κ,p;λ)  . The ratio of maximized profit to maximized welfare is defined as  β(κ)≡π¯(κ)/W¯(κ).  Taking logs and differentiating, it follows that  βʹ(κ)/β(κ)=π¯ʹ(κ)/π¯(κ)−W¯'(κ)/W¯(κ).  Now let  κ*  denote the profit-maximizing product carbon footprint. Because  π¯ʹ(κ*)=0  by definition, it follows that  βʹ(κ*)/β(κ*)=−W¯ʹ(κ*)/W¯(κ*).  Thus, the product carbon footprint  κ*  exceeds the socially optimal level if  βʹ(κ*)>0  and conversely, which implies that the firm's choice of the product carbon footprint is not necessarily consistent with corporate social responsibility.Adopting an offset strategy is consistent with corporate social responsibility if it increases welfare compared with the no-offset strategy. To this end, consider an offset market in which an offset provider compensates emissions at variable cost  ϕωκoD(po)  , where  ϕ∈[0,1]  is an efficiency parameter, and fixed cost  F>0  . In this scenario, welfare is obtained by adding up consumer surplus and the profits from the firm and the offset provider: W(κo,po;ω)=∫po∞vdF(v)−c(κo)D(0,po)−ϕωκoD(0,po)−F. GraphA13Because the offset cost  ωκoD(0,po)  is a transfer from the firm to the offset provider, it cancels out in the welfare calculation. Using Equations A12 and A13, adopting a climate neutral strategy is economically efficient if  W(κo,po;ω)≥W(κ*,p*;λ)  . This condition is satisfied if the social cost of carbon offsetting  ϕωκoD(0,po)+F  is sufficiently low compared with the climate damage that results from the corporate carbon footprint under a no-offset strategy, given by  Φ(κ*,p*;λ)  .Proof of Proposition 6. The profit-maximizing product carbon footprint and price satisfy the following Kuhn–Tucker conditions: −c′(κr)D(κr,pr)+(pr−c(κr))Dκ(κr,pr)+μ1−μ2−μ3Φκ(κr,pr)=0  GraphA14 D(κr,pr)+(pr−c(κr))Dp(κr,pr)−μ3Φp(κr,pr)=0, GraphA15 μ1κr=0, μ2(κr−κ¯), and μ3(Φ(κr,pr)−R)=0. GraphWe denote the unique constrained profit-maximizing product design by  (κr,pr)  and assume that the carbon constraint is binding so that  0<κr<κ¯  , which implies that  μ1=μ2=0  . Substituting Equation A15 into Equation A14 and rearranging yields −cʹ(κr)−zκ(κr;λ)+μ31−F(⋅)[Φp(κr,pr)zκ(κr;λ)−Φκ(κr,pr)]=0. GraphA16The third term on the left-hand side of Equation A16 is negative if  Φκ(κ,p)>0  , as  Φp<0  and  zκ(κr;λ)>0  . Consequently,  κr<κ*  if  Φκ(κ,p)>0  , that is, if the sales expansion from offering a greener product is sufficiently small to not compensate for the direct reduction in overall emissions.Proof of Proposition 7. Suppose that a firm with product design (  κ*,p*  ) and profit  π*  faces regulation that is implemented with probability ρ. If the firm decides to meet the carbon cap by adjusting the product design, the expected reduction in profit is given by  π*−[ρπr+(1−ρ)π*]=ρ(π*−πr)  , where  πr  is the constrained optimal profit. Instead, if the firm decides to leave the product design unchanged and to purchase carbon allowances, the expected reduction in profit is given by  π*−[ρ(π*−ϖ(Φ(p*,κ*)−R))+(1−ρ)π*]=ρϖ(Φ(p*,κ*)−R)  . Clearly, the firm chooses the option that minimizes the negative profit impact. Therefore, the expected cost of a cap-and-trade regulation to the firm is given by  ρmin{π*−πr,ϖ(Φ(p*,κ*)−R)}  .Proof of Proposition 8. To analyze the impact of a carbon tax, we use the same approach as in Proposition 2. Letting  c(κ)≡c(κ)+tκ  denote the effective unit cost, the problem in Equation 7 is structurally equivalent to the problem in Equation 4. Therefore, using Equation A5, the profit-maximizing carbon footprint at an interior solution satisfies  −cʹ(κ*)=zκ(κ*;λ)  , which can be rewritten as −c′(κ*)−t=zκ(κ*;λ). GraphA17Applying the implicit function theorem to Equation A17 yields dκ∗dt=−1c′′(κ∗)+zκκ(κ∗;λ)<0, GraphA18where the inequality follows from the second-order condition. From Equation A7, the first-order condition for the profit-maximizing price reads  p*=c(κ*)+[1−F(⋅)] /f(⋅)  . Implicit differentiation yields dp∗dt=κ∗f(⋅)2f(⋅)+(p−c(κ∗)−tκ∗)fʹ(⋅)−dκ∗dtzκ(κ∗;λ). GraphA19The corporate carbon footprint can be written as Φ*(t)=κ*(t)[1−F(p*(t)+z(κ*(t);λ))]. GraphDifferentiating  Φ*(t)  with respect to the tax rate t and substituting for  dκ* /dt  and  dp* /dt  given in Equations A18 and A19, respectively, yields Φʹ(t)=dκ*dt[1−F(⋅)]−κ*F(⋅)[dp*dt+zκdκ*dt]  =dκ*dt[1−F(⋅)]−[κ*F(⋅)]22F(⋅)+(p−c(κ*)−tκ*)fʹ(⋅)<0. GraphA20Therefore, a proportional carbon reduces not only the profit-maximizing carbon footprint but also the corporate carbon footprint.The expected cost of carbon taxation is the difference between the actual profit and the expected profit under uncertain taxation, which can be expressed as  π*(0)−[ρπ*(t)+(1−ρ)π*(0)]=ρ{π*(0)−π*(t)}  , where  π*(0)≡π*  is the profit under a zero tax rate. Because  Dπ*(t) /dt=−κ*D(κ*,p*;λ)=−Φ*(t)  from the envelope theorem and using the definition of the corporate carbon footprint, we have that π*(0)−π*(t) =−∫0tdπ*(y)dydy=∫0tΦ*(y)dy, Graphwhich implies that uncertain taxation reduces profit.Proof of Proposition 9. In the absence of carbon regulation, the firm adopts the green technology if  π1*−π0*≥f1  . With regulation, the firm adopts the green technology if  ρ(π1r−π0r)+(1−ρ)(π1*−π0*)≥f1  . Therefore, if  π1r−π0r>π1*−π0*  , the threat of carbon regulation captured by  ρ>0  relaxes the standard adoption constraint.Proof of Proposition 10. Demand for each firm i,  i=1,2  , can be derived from the location of the consumer who is indifferent between buying from firm 1 and firm 2, denoted  x^  . From the indirect utility function in Equation 8, this location solves the indifference condition  v1(x^)=v2(x^)  . With linear mismatch, the consumer located at  x^  segments the market, that is, consumers located to the left of  x^  purchase from firm  1  , while consumers located to the right of  x^  purchase from firm  2  . Demand of firm i can therefore be derived as Di(κ,p;λ)=12−λ(κi−κj)−(pi−pj). GraphA21To illustrate the emergence of competitive carbon offsetting, we focus on the case where  c10=c20≡1  and  ω < 1  . Firm i then solves maxκi,pi πi(κi,pi)=[pi−(1−κi)2]Di(κ,p). GraphA22The (necessary and sufficient) first-order conditions are given by ∂πi∂κi=2(1−κi*)Di(κ*,p*)+[pi*−(1−κi*)2]∂Di(κ*,p*)∂κi=0 GraphA23 ∂πi∂pi=Di(κ*,p*)+[pi*−(1−κi*)2]∂Di(κ*,p*)∂pi=0. GraphA24Simultaneously solving the first-order conditions of firm i by substituting Equation A24 into Equation A23 and using the properties of demand in Equation A21 (with  λ=1  ) yields  κi*=12  . The optimal prices  pi*=34  are then determined by solving for equilibrium in a standard Hotelling model with given unit costs  ci(κi*)  and demand. The solution is indeed the profit maximum because  πi  is concave in both  κi  and  pi  and the determinant of the Hessian matrix  Hi  evaluated at (  κi*,pi*  ) is strictly positive (specifically,  det(Hi)=2  ). By substitution,  x^=12  ,  πi*=14  (the upper-left cell in Figure 3), and  Φi*=14  . Consumer surplus for buyers of firm 1 is obtained as S1(κ1,p1;1)=∫0 (v−p1−κ1−x2−E)dx. GraphA25Because consumers fully internalize their climate externality (  λ=1  ), it follows that  E=0  . By substitution, Equation A25 reduces to  S1*=8v−1116  , and symmetry implies that  S1*=S2*  . Welfare is obtained by aggregating consumer surplus and profit net of the climate impact across firms: W*=∑i=12(Si*+πi*−Φi*)=v−118. GraphA26Second, we analyze the setting in which firm 1 uses an offset strategy and firm 2 uses a no-offset strategy. Firm 1 therefore solves maxκ1,p1 π1(κ1,p1)=[p1−(1−κ1)2−ωκ1](12+κ2−(p1−p2)), GraphA27where ω denotes the offset cost per unit of carbon emissions. Instead, firm 2 solves maxκ2,p2 π2(κ2,p2)=[p2−(1−κ2)2](12−κ2−(p2−p1)). GraphA28Using a similar approach as in the case where both firms adopt a no-offset strategy, simultaneously solving the first-order conditions and substituting the unique solutions back into the profit functions yields the optimal profits  π^1=1144(ω(ω−4)+9)2≡A  and  π^2=1144(ω(ω−4)−3)2≡B  (the lower-left cell in Figure 3). Note that these profits are reversed in a setting in which firm 1 uses a no-offset strategy and firm 2 uses an offset strategy (the upper-right cell in Figure 3).Third, we analyze the setting in which both firms adopt an offset strategy. Therefore, firm i solves maxκi,pi πi(κi,pi)=[pi−(1−κi)2−ωκi](12−(pi−pj)). GraphA29Using a similar approach as in the previous cases, simultaneously solving the first-order conditions and substituting the unique solutions back into the profit functions yields the optimal profits  π¯i=14  (the lower-right cell in Figure 3).Inspection of Figure 3 shows that adopting an offset strategy is a strictly dominant strategy for each firm. The reason is that the offset strategy is more profitable than the no-offset strategy, no matter what the competitor may choose because  A>14  and  B<14  for all  ω<1  .These equilibrium strategy choices are consistent with corporate social responsibility if welfare is improved over the benchmark case where both firm use a no offset strategy. Welfare under offset strategies can be derived as W¯*=∑i=12(Si*+πi*)+(ω−ϕ)∑i=12Φi*−F      =v−ω24−ϕ2(2−ω)−18−F,   GraphA30where  (ω−ϕ)∑i=12Φi*−F  is the profit of the offset provider.Carbon offsets improve welfare over the case absent offsets if  W¯*>W*  . Clearly, this holds if the marginal cost ϕ and the fixed cost F are sufficiently small, that is, as long as the offset technology is sufficiently cost effective.  "
10,"Communication in the Gig Economy: Buying and Selling in Online Freelance Marketplaces The proliferating gig economy relies on online freelance marketplaces, which support relatively anonymous interactions through text-based messages. Informational asymmetries thus arise that can lead to exchange uncertainties between buyers and freelancers. Conventional marketing thought recommends reducing such uncertainty. However, uncertainty reduction and uncertainty management theories indicate that buyers and freelancers might benefit more from balancing—rather than reducing—uncertainty, such as by strategically adhering to or deviating from common communication principles. With dyadic analyses of calls for bids and bids from a leading online freelance marketplace, this study reveals that buyers attract more bids from freelancers when they provide moderate degrees of task information and concreteness, avoid sharing personal information, and limit the affective intensity of their communication. Freelancers' bid success and price premiums increase when they mimic the degree of task information and affective intensity exhibited by buyers. However, mimicking a lack of personal information and concreteness reduces freelancers' success, so freelancers should always be more concrete and offer more personal information than buyers. These contingent perspectives offer insights into buyer–seller communication in two-sided online marketplaces. They clarify that despite, or sometimes due to, communication uncertainty, both sides can achieve success in the online gig economy.Keywords: business-to-business exchange; gig economy; multisided platforms; online freelance marketplaces; text analysis; uncertainty managementOnline freelance marketplaces, such as Upwork, Fiverr, and PeoplePerHour, have prompted massive transformations in business-to-business (B2B) markets ([13]; [82]). In particular, they allow buyers to post gigs, or short-term service projects, which initiate reverse auctions whereby interested freelance workers submit bids to offer their services ([39]). In these digital environments, buyers and freelancers often devote rather limited time and attention to detailed assessments and instead make choices on the basis of rational value expectations or prices ([ 2]). In addition, online freelance marketplaces suffer from information asymmetries because they rely on text-based messages, which can create uncertainty and hinder the exchange ([34]; [77]). Imagine a buyer wants to hire a freelancer to optimize their pet website's search rankings, so they post a call for bids, requesting ""someone for an SEO job."" In response, Freelancer A might vaguely promise, ""I have plenty of experience writing content that users find interesting to improve the quality and quantity of your traffic,"" whereas Freelancer B more concretely states, ""I have four years of experience writing articles and blogs that engage users and are SEO-friendly. For example, I could focus on interest pieces like the everyday lives of pets."" The communication of both the buyer and freelancer create different degrees of uncertainty, likely impacting who applies and who gets hired.Uncertainty regarding communication can lead to various negative outcomes on both sides, including high rates (more than 50%) of service gigs that go unfulfilled ([36]), diminished bid success, and less-than-optimal pricing for freelancers ([ 2]). However, parties in B2B exchanges can also strategically leverage uncertainty in their communication to achieve more effective outcomes, such as when negotiators conceal information ([69]) or when ambiguous contracts help reduce litigation concerns and increase cooperation ([81]). Buyers and freelancers on online freelance marketplaces engage in a form of B2B exchange, so we propose that they similarly might balance their communication efforts by strategically reducing and increasing uncertainty to maximize their exchange success. In our previous example, by staying vague and without any specific direction from the buyer, Freelancer A might be trying to keep multiple options open and avoid overpromising outcomes.In addition to fundamental questions regarding how to manage uncertainty in B2B exchanges ([50]; [63]), we seek to address the role of communication in such exchanges ([ 7]; Rajdeep et al. 2015; see also Web Appendices A and B). We integrate uncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) to predict that, in online freelance marketplaces, various strategies for reducing and increasing the ability of message recipients to anticipate message senders' meaning and actions can benefit the exchange ([ 8]). Using Grice's ([27]) communication principles, we argue that greater provision of task and personal information might reduce uncertainty in service exchanges ([54]) but could also lead to information overload or disagreements ([18]; [40]). Presenting information in a more concrete (cf. abstract) manner or with greater affective intensity also can reduce uncertainty ([27]; [28]; [61]). But again, too much concreteness or affective intensity might lead to restrictive communication that hinders exchanges ([18]; [37]).We apply this theoretical reasoning to exchanges in online freelance marketplaces, in which buyers post calls for bids to attract as many freelancers as possible to apply ([36]). These buyers face a trade-off between reducing uncertainty for freelancers (e.g., providing more information, using less ambiguity) and still efficiently granting them sufficient interpretative freedom. Theorists concur that principles for using relevant information or less ambiguity often get deliberately flouted in conversation, such as when an individual is attempting to save face ([23]) or please a counterpart ([44]). If different communication strategies might entice more freelancers to bid, buyers could establish optimal designs for calls for bids.In response to those calls for bids, freelancers write and submit their bids. In doing so, these freelancers also must manage uncertainty. Thus, they might benefit from matching or mimicking the communication approach adopted by the prospective buyer that issued the call ([78]). Communicative mimicry can evoke similarity perceptions, which tend to increase receivers' sense of rapport and reduce their uncertainty ([75]). Research on adaptive selling recommends matching the buyer's communication (e.g., [57]; [72]). However, in some situations, deviations also may be beneficial ([ 1]), so we consider a more nuanced distinction related to the level at which the similarity occurs. Furthermore, if freelancers compete on price, they may become enmeshed in a self-defeating value trap ([34]; [76]) in which they win more bids but earn less revenue. Strategically mimicking or deviating from a buyer's communication might provide a viable means to winning more gigs without being trapped. We accordingly suggest how freelancers should calibrate their bid formulations to improve their bid success and achieve a price premium.Using a unique, large-scale data set of calls for bids and bids, obtained from a leading online freelance marketplace, along with a series of multilevel models that account for endogeneity, we establish three main contributions. First, we determine the effects of buyers' strategic communications in two-sided online marketplaces ([ 7]). Rather than uncritically recommending that communication should always be informative and unambiguous, we specify the diminishing, even adverse consequences that can result if buyers relay too much task or personal information in a very concrete, intense manner. Second, in an extension of research into adaptive selling ([57]; [78]), we reveal how freelancers' dyadic communicative mimicry affects bid success. Mimicry effects are contingent on the communicative aspect and the buyer's relative uses of each aspect. As we show, mimicking buyers in terms of the provision of task information and use of affective intensity increases bid success. In contrast, we find that freelancers should always offer more personal information and be more concrete in their bid formulations than buyers' calls for bids. Third, we offer insights into how freelancers can avoid predatory pricing ([13]) and escape a value trap ([76]). By strategically managing uncertainty according to the information communicated, and by managing the manner in which they do so, freelancers can earn price premiums. Online Freelance Marketplace ExchangesOnline freelance marketplaces that feature reverse auctions rely on a three-stage process ([34]; [39]). First, in seeking a suitable freelancer, a buyer describes a gig or short-term service project in a call for bids. Second, multiple freelancers apply by formulating and submitting bids that describe themselves, the service offering, and the price requested. Third, the buyer compares the bids and selects a freelancer to complete the project. The outcome of each stage defines exchange success. That is, buyers' success results from a large pool of viable freelance offers. A higher number of bids increases the chances of finding a suitable freelancer for the gig ([35], [36]). Freelancers' success depends on whether their bids are chosen, preferably at a price premium ([13]; [32]). In this context, a price premium is the monetary amount in excess of the buyer's original payment offer (i.e., expected price; [26]; [73]). Buyers might pay a premium beyond their original payment offer for various reasons, including their willingness or ""need to compensate the seller for reducing transaction risks"" ([ 2], p. 248). In competitive online marketplaces, freelancers also might encounter value traps in which they wind up selling more of their services at a lower price ([ 2]; [76]). In this sense, freelancers' success depends on winning the bid but also earning price premiums (or avoiding discounts). Unlike traditional B2B exchanges, buyers' and freelancers' success hinges on textual communication ([13]; [36]). Comparing theories on uncertainty and the role of communication in producing or reducing it, we delineate how both buyers and freelancers may best strike a balance between providing more information and reducing ambiguity versus preserving some uncertainty and maintaining interpretative flexibility. Conceptual BackgroundUncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) draw on a central tenet of information theory ([71])—namely, that communication, information, and uncertainty are inextricably linked. Thus, uncertainty is inherent to any interaction. [22] suggests uncertainty depends on the ability to draw inferences from provided information content and the manner in which it is provided. Whereas uncertainty reduction theory predicts how communication can reduce uncertainty, uncertainty management theory examines how people cope with uncertainty, which may include efforts to increase uncertainty to attain beneficial outcomes ([ 8]). Our conceptual development relies on these fundamental principles. Communication Principles in Online Freelance MarketplacesIn online freelance marketplaces, buyers and freelancers depend on one another; all else being equal, they want their mutual exchange to succeed. In such interactions, [27] suggests that four generalized cooperative communication principles (or maxims) apply. Three principles refer to what should be said: the quantity of information (""give as much information as is required and no more than is required""), its quality (""do not say what is false or that for which you lack adequate evidence""), and its relevance. The fourth principle, manner (be clear and avoid ambiguity), pertains to ""how what is said is to be said"" ([27], p. 46). In our study context, neither a buyer nor a freelancer can know upfront whether the other party might be lying, so truthfulness would have to be assumed prior to the exchange. We also highlight that information does not have to be ""correct"" to influence uncertainty perceptions ([ 9]). Therefore, among the four maxims, we focus on the quantity of relevant information that buyers and freelancers offer and the manner in which they present it. Uncertainty Implications of Communication PrinciplesCommunication outcomes are fundamentally uncertain ([ 6]). When people vary their use of communication principles ([27]), they create conversational implications such that message recipients must infer what speakers are trying to imply with their wording. Accordingly, the (un)certainty that buyers and freelancers encounter while making inferences should depend on the degree to which calls for bids and bids provide relevant information in an unambiguous manner, though the meaning of relevant information varies by context. In line with prior research (e.g., [ 7]), we define this degree as the proportion of specific lexical terms used relative to the total number of words in a message.More information reduces uncertainty ([ 6]) and increases receivers' perceptions of the information's value ([79]). In service exchanges, the parties seek information about the task and the person who will complete it ([54]). A greater degree of task information should reduce uncertainty about functional service aspects ([54]). By self-disclosing greater degrees of personal information, a sender also provides a receiver with more information about the self ([15]). In line with the quantity principle ([27]), sparse provision of relevant task and personal information would make it difficult for the receiver to anticipate outcomes or distinguish among options, thus creating uncertainty ([19]).Regarding the principle of manner ([27]), greater degrees of concreteness and affective intensity should reduce ambiguity and enhance clarity. Concrete terms describe something in a perceptible, precise, specific, or clear manner ([11]; [49]; [61]). A greater degree of concreteness reduces ambiguity because it makes it easier for receivers to perceive or recognize what the message sender is implying ([11]; [28]; [61]). Affective intensity reflects the proportion of affective terms included in a message. More affective terms as a proportion of the total word count produce a greater degree of affective intensity, which increases receivers' ability to make evaluative judgments ([28]; [37]).[ 5] We provide examples of these principles in Table 1.GraphTable 1. Communication Elements, Links to Uncertainty, and Examples. Communication ElementDefinitionLink to UncertaintyExampleTask informationA content element of communication. In service exchanges, it is conveyed through functional, duty terms (Ma and Dubé 2011). The proportion of task terms to the total number of words in a message defines the degree of task information.Greater (lesser) degrees of task information decrease (increase) uncertainty.Sparse degree of task information: ""I saw your project description and I would like to work for you. I have plenty experience in different settings where I have written content which users find interesting.""Dense degree of task information: ""I saw your project description and would like to write the content for your website. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Personal informationA content element of communication that is conveyed through self-disclosing terms (Derlega, Harris, and Chaikin 1973). The proportion of self-disclosing terms to the total number of words in a message defines the degree of personal information.Greater (lesser) degrees of personal information decrease (increase) uncertainty.Sparse degree of personal information: ""Saw your project description and would like to write the content for your site. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of personal information: ""I saw your project description and I would like to write the content for your site. I have 12 years of work experience in copy writing for articles, blogs & E-books. I have a Master's in Journalism and have worked fulltime for companies like Adobe.""ConcretenessA manner element of communication conveyed by terms that are perceptible, precise, or specific (Brysbaert, Warriner, and Kuperman 2014; Packard and Berger 2020). The proportion of concrete terms to the total number of words in a message defines the degree of concreteness.Greater (lesser) degrees of concreteness decrease (increase) uncertainty.Sparse degree of concreteness: ""I noticed your project description and I would like to do work on it. I have plenty of experience in scripting text, which is engaging, compelling, and SEO friendly.""Dense degree of concreteness: ""I saw your posted project description on Upwork, and I would like to write the contents for your website. I have a lot of experience in article and weblog writing in an SEO friendly fashion.""Affective intensityA manner element of communication that is conveyed through affective terms (Hamilton and Hunter 1998). The proportion of affective terms to the total number of words in a message defines the degree of affective intensity.Greater (lesser) degrees of intensity decrease (increase) uncertainty.Sparse degree of affective intensity: ""I saw your project description and I can write the required content for your site. I have plenty of experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of affective intensity: ""I liked your project description and would be happy to write the content for your site. I have great experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""  Reducing and Maintaining Uncertainty in Communication ExchangesCross-disciplinary research provides ample evidence that conversational partners generally prefer to reduce uncertainty ([ 5]). In B2B relationships, reducing uncertainty increases exchange effectiveness ([29]; [50]; [63]). In Web Appendix A, we offer an overview of some key empirical marketing studies on B2B communication aspects. Specifically in online freelance marketplaces, which are relatively anonymous, the required coordination and dependence between rational buyers and freelancers may increase their need for information and clarity ([13]; [34]). Thus, for example, reputation cues commonly appear in online freelance marketplaces as a way to reduce uncertainty and facilitate exchanges ([34]). More broadly, reducing uncertainty by adhering to [27] principles in dyadic buyer–freelancer communications may boost exchange success.However, people experience uncertainty differently and do not always prefer to reduce it ([ 8]). Instead, according to uncertainty management theory ([ 9]), strategic communication choices that might not minimize uncertainty, and even cultivate it, can be effective and lead to better outcomes for consumers ([38]), organizations ([18]; [31]), and interorganizational governance ([81]). For example, [38] find that a lack of concreteness aids consumers' initial online searches because such vague queries return a greater variety of search results. In collective bargaining settings, seasoned negotiators use concealment and ambiguity to enhance the likelihood of agreement ([69]). In B2B exchanges, parties can use less information and more ambiguity strategically to accomplish specific goals ([ 3]; [81]). Even if such efforts are not universally favored, uncertainty-cultivating communication provides benefits by allowing different receivers to perceive multiple different meanings simultaneously ([18]). Moreover, communication theorists concur that people sometimes deliberately flout or violate [27] conversation principles, such as when they intentionally maintain uncertainty to save face ([23]) or please a counterpart ([44]). Subverting the principles is not necessarily less cooperative, and furthermore, the purpose of communication is not always to be as informative and clear as possible. Arguably, cooperative principles encourage reasonable adherence, not compulsion. Thus, strategically allowing recipients to develop a broader range of possible interpretations by maintaining some level of uncertainty might facilitate buyer–freelancer exchanges. Research Propositions Managing Freelancers' Uncertainty in Calls for BidsFreelancers choose whether to offer their services in response to a buyer's call for bids. The number of freelancers who choose to do so is consequential for the buyer, as more bids implies a greater likelihood of finding a suitable service provider ([36]). Managing freelancers' uncertainty through relevant information provision and the manner of communication in the calls for bids should influence freelancers' decisions to apply. Relevant informationIn calls for bids, buyers can vary the degree of task and personal information included in the description of the gig. If freelancers evaluate this information favorably, they develop more positive dispositions and are more likely to apply ([72]). As prior research establishes, more information enhances communication outcomes in business settings by reducing uncertainty. For example, studying web forums, [79] indicate that the breadth of information provided by a sender affects receivers' objective judgments of the value of that information. [49] find that greater degrees of monetary information increase peer-to-peer lending, and [41] shows that more task information increases the time and commitment sellers allocate to a buyer. Greater degrees of personal information also reduce uncertainty, increase trust ([55]), and enhance performance on crowdsourcing platforms ([68]). Such self-disclosure can strengthen ongoing buyer–seller relations as well ([14]). In contrast, a greater proportion of nonrelevant information (i.e., a lesser degree of relevant information) increases uncertainty ([ 9]). Because greater degrees of task and personal information in calls for bids help reduce freelancers' uncertainty, freelancers who believe they qualify for the gig should be more willing to submit bids.However, excessive relevant information may be ineffective, even if it reduces freelancers' uncertainty. That is, if buyers provide excessive details about the task, the gig may appear too restrictive or prescriptive ([18]), which might not appeal to freelancers. For example, leaving detailed information out of contracts ([21]) or negotiations ([69]) represents a tactic for improving exchange performance. In a downsizing context, a greater degree of information provision can increase uncertainty and negative reactions ([31]). For freelancers, excessive information can feel overwhelming and can limit their motivation, opportunity, or ability to process the information and submit bids ([40]). A buyer that self-discloses a high amount of personal information might also appear less attractive as a prospective business contact ([12]). Because extensive self-disclosures are unusual in initial B2B online exchanges ([46]), such disclosures might be perceived as inappropriate ([59]).In summary, we argue that moderate degrees of task and personal information in calls for bids relate to more freelancer bids. Buyers who provide greater degrees of task and personal information should attract more bids, but beyond a moderate degree (i.e., a very dense provision of relevant information), providing still greater degrees of task and personal information may decrease the number of bids. We thus propose a curvilinear relationship: P1:  Extremely sparse and extremely dense degrees of (a) task and (b) personal information in calls for bids yield fewer freelance bids than moderate degrees. Communication mannerIn calls for bids, buyers can vary the concreteness and affective intensity with which they describe the gig. Researchers disagree about whether more or less ambiguous communication leads to more efficacious speech ([ 8]; [18]; [37]), but in an online freelance marketplace, we posit that buyers must reduce ambiguity to at least some extent by being more concrete and intense. Greater concreteness and affective intensity can be more efficient because recipients can process the information with less time and effort ([37]; [61]). These approaches also tend to result in communication that is more persuasive, memorable, and accessible than communication that uses predominantly abstract or unemotional wording ([28]; [37]). In other settings, greater concreteness increases consumer satisfaction with employee interactions and purchase likelihood ([61]). Greater degrees of intensity achieved through proportionally more affective words provide accessible, diagnostic signals to customers ([52]). They can also sway business partners' decisions when used as inspirational appeals ([72]). Finally, greater concreteness and affective intensity provide heuristic cues that allow freelancers to take mental shortcuts, which makes them more likely to bid ([37]).However, if the calls for bids appear too concrete or too intense, the task might appear narrow, which reduces the appeal of performing the gig ([37]). [80] finds that greater vagueness (i.e., less concreteness) can enhance judgments of a speaker's character, message acceptance, and recall. Moreover, some research asserts that reducing uncertainty with more concrete formulations is ineffective ([ 9]; [18]), so managers instead should embrace strategic ambiguity to allow for interpretative freedom ([43]). In contracts, unexpected specificity even increases ex ante costs ([58]). Contrastingly, greater task ambiguity can lower costs as well as reduce the risk of litigation and enhance cooperation in B2B exchanges ([81]). Greater degrees of concrete terms in communications with investors also can have adverse effects ([64]), and excessive degrees of positive affective words diminish the impact of customer reviews ([52]). Thus, we predict a stylistic trade-off: Overly ambiguous calls for bids, lacking any concreteness or affective intensity, may undercut buyers' success in attracting freelancers, but some degree of ambiguity (i.e., avoiding overly concrete, affectively intense communication) can allow for divergent interpretations to coexist. Thus, moderate degrees of concreteness and affective intensity may be most effective in encouraging freelancers to bid. P2:  Extremely sparse and extremely dense degrees of (a) concreteness and (b) affective intensity in calls for bids yield fewer freelance bids than moderate degrees. Managing Buyers' Uncertainty in BidsBuyers also face uncertainty when deciding whom to hire and how much to pay ([ 2]; [13]). By managing these uncertainties through their bids, freelancers can affect their chances of winning bids and their price premiums. To establish relevant predictions, we integrate [27] communication principles with uncertainty research such that we anticipate a greater provision of relevant information communicated with greater concreteness and affective intensity allows buyers to draw inferences from freelancers' bids with more certainty. Beyond these communication principles, [ 6] suggest that perceived similarity to a message sender reduces receivers' uncertainty. Thus, both purchase likelihood and buyers' willingness to pay a price premium might be influenced by freelancers' adherence to certain communication principles, as well as by their communicative similarity to the buyer. Winning bidsIn other exchange contexts, research has established that when service employees relay greater degrees of service or personal information ([51] 2015; [62]), it improves customers' intentions to purchase. Willingness to purchase also increases if employees use greater concreteness in online service chats ([61]) or greater degrees of affective words in their emails ([72]).However, the dense provision of relevant information in a bid risks information overload ([40]), and a freelancer being overly concrete or intense might signal a restrictive, narrow approach to the gig ([37]). Our reasoning here parallels that for buyers' formulations of calls for bids. We thus similarly predict that moderate degrees of task and personal information provided in a moderately unambiguous manner (i.e., moderate degrees of concreteness and affective intensity) enhance freelancers' chances of winning the gig.Yet preferences for uncertainty also might be situational and dispositional ([ 9]), as reflected in buyers' own communicative choices ([30]). Specifically, calls for bids can reveal buyers' expectations and preferences for communication behaviors. For example, buyers might like to get to know freelancers, or they may prefer to keep their business relationships impersonal. The extent to which they disclose their own personal information in calls for bids should signal these preferences. An ambiguous bid offered in response to an ambiguous call for bids might lead the buyer to conclude that the freelancer is tactful, sensitive, and noncoercive ([10]). Adaptive communications also raise perceptions of credibility, common social identity, approval, and trust ([52]; [75]), as well as similarity perceptions, all of which in turn reduce uncertainty ([ 6]). Crafting responses that mimic the buyer's communication is a common personal selling recommendation ([78]). As [72] show, when sellers mimic buyers' communicative manner, it increases buyers' attention. Accordingly, freelancers who mimic a buyer's communication content and manner might improve their exchange success.In some situations, though, deviating from buyers' communications may be more beneficial ([ 1]). Even in studies that note the performance benefits of adaption, researchers highlight the importance of the degree of adaptivity (e.g., the degree to which salesperson behaviors adjust for each customer during interactions; [78]). Similarly, studies of communication accommodation investigate the degree of accommodation used ([75]). Extending these insights, the outcomes of adaptation likely depend on communication levels (e.g., very informative vs. not informative). In keeping with uncertainty reduction theory, we expect that buyers are less likely to hire freelancers whose bids offer sparse information and are very ambiguous, even if the call for bids has these characteristics. P3:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of bid success by mimicking buyers' communications. Achieving price premiumsBuyers' uncertainty about a freelancer should influence their willingness to pay a price premium ([ 2]). Although there are many reasons for price variations ([26]) in online freelance marketplaces, buyers compensate (penalize) freelancers for reducing (increasing) their transaction uncertainty by deciding to accept a price above (below) their original payment offer ([ 2]). In line with [70], freelancers' greater provision of relevant task and personal information in a more concrete and intense manner in bids likely reduces buyers' information asymmetry and exchange-specific risks. Therefore, buyers who want to transact with high certainty may render a price premium for such bids ([51] 2015).The degree to which freelancers mimic buyers' communication also may influence the price premium. For example, [60] find that adaptive approaches for different customers help salespeople increase those customers' willingness to pay a price premium. However, in line with our arguments regarding bid success, we expect that the positive influence of a freelancer's communicative mimicry depends on the specific degree to which the buyer uses a specific communicative element. This reasoning aligns conceptually with the communication principles ([27]), the recommendation that uncertainty should be carefully managed ([ 8]), and the benefits of mimicry identified in studies of communication accommodation ([75]) and adaptive selling ([78]). However, we know of no studies that consider price premium implications of communicative trade-offs between reducing buyers' uncertainty and adapting to buyers' communication. In addition, we are not aware of any research that considers the possible negative effects when sellers mimic buyers who provide less task and personal information, are less concrete, or sparsely use affective intensity.Buyers who want to transact with high certainty might render a price premium to freelancers who reduce uncertainty by providing greater degrees of relevant information in a more concrete and intense manner. But if buyers perceive that the provision of relevant information, degree of concreteness, and level of intensity surpasses their own reasonable level, they might feel overloaded or restricted and thus unwilling to pay a premium. We therefore predict that buyers offer a price premium to freelancers who provide degrees of relevant information, concreteness, and affective intensity at a level similar (but never too sparse) to their own communication, as only these bids help reduce buyers' exchange risks. P4:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of earning a price premium by mimicking the communication of the buyer.Graph: Figure 1. Effect of buyers' communication on call for bids success. Field Study of an Online Freelance Marketplace Setting and SampleWe conducted a large-scale field study with a proprietary data set of calls for bids and corresponding bids posted on a leading, global online freelance marketplace. The marketplace hosts seven freelance service submarkets: ( 1) design; ( 2) writing and translation; ( 3) video, photo, and audio; ( 4) business support; ( 5) social media, sales, and marketing; ( 6) software and mobile development; and ( 7) web development. The bidding process follows a sequential, sealed-bid reverse auction format, and it concludes when the buyer chooses one winning bid ([34]; [39]). As with recent marketing research that investigates large scales of communication (see Web Appendix B for an illustrative overview), this process depends on and is captured in text data. We used ( 1) text data from 343,796 calls for bids issued by 49,081 buyers (restricted to those who posted at least two gigs) to predict buyers' call for bids success, ( 2) 2,327,216 bids submitted by 34,851 freelancers (restricted to those who submitted at least two bids) to predict freelancers' bid success, and ( 3) 148,158 bids submitted by 30,851 freelancers (restricted to those who won and for which the payment was disclosed) to predict freelancers' price premium. Our multilevel approach required more than one observation (call for bid or bid) in each Level 2 unit (buyer or freelancer); otherwise, Level 2 and Level 1 variance might have been confounded ([74]). Web Appendix C summarizes the definitions and operationalizations, and Web Appendix J provides the descriptive statistics and correlations. Measurement of ConstructsThe number of freelancers who submit bids to offer their services provided the measure of success of buyers' call for bids. More submitted bids increases the probability that buyers can find an appropriate freelancer, whereas failing to find a suitable match is time consuming and costly because it requires further searches and delays the project ([35], [36]). We measured freelancers' bid success as a binary indicator of whether ( 1) or not (0) the freelancer was chosen by the buyer and won the bid ([32]). For freelancers' price premium, we gauged the percentage by which the accepted bid price for the project exceeded (or fell short of) the buyer's original payment offered (i.e., benchmark price; [20]). This operationalization accounted for the difference between the final price a buyer paid and the original price they offered (i.e., what the buyer expected to pay) ([73]).To capture the independent communication variables, we mined the text of each call for bids and each bid. For the preprocessing and extraction steps, we used the R package Quanteda ([ 4]), as well as a combination of newly developed and prevalidated text mining dictionaries. For the degree of task information in each text, we inductively sourced a list of context-specific task descriptor words. To start, we acquired all 34,851 freelancers' service skill tags ([ 7]; for an illustration, see Web Appendix D), which freelancers list in their profiles to describe the service tasks they offer (e.g., ""developer,"" ""illustrator""). After removing stop words and duplicates, two coders reviewed the remaining word list, deleted any misspelled words, and removed terms that did not describe a service (e.g., ""great,"" ""reliable""). Using Quanteda ([ 4]), we stemmed the remaining words, leaving 1,912 unique word stems that describe service tasks. We mined each call for bids and bid, then summed word occurrences reflecting the new task dictionary. By dividing this sum by total words, we obtained a measure of the degree (ratio) of task information in each text. When people self-disclose personal information, they use singular, first-person pronouns. In line with previous research (e.g., [67]), we measured the degree of personal information as the ratio of first-person singular pronoun words (e.g., ""I,"" ""me"") to the total words in each text. To determine the degree of communication concreteness, we mined each text for [11] list of generally known English lemmas that indicate whether a concept denoted by a term refers to a perceptible entity. Following their operationalization, we included all terms that received a rating of 3 or greater on their bipolar, five-point abstract-to-concrete rating scale.[ 6] That is, terms that score 3 or higher refer to relatively more specific objects, materials, people, processes, or relationships. We again divided the sum of the concrete terms by the total words in each text. Finally, the ratio of emotion-laden words (e.g., ""problematic,"" ""easy""; [28]; [37]) determined affective intensity. Using the Linguistic Inquiry and Word Count (LIWC) affect dictionary, we obtained a list of affect words, which we then summed for each text ([66]) and divided each by the corresponding total word count to obtain the degree of affective intensity. Pilot Studies Validity of text-mined measuresTo ensure the validity of our text-mined communication measures, we asked two coders to classify the texts of a random subsample of 100 calls for bids (Mlength = 129 words) and 100 bids (Mlength = 102 words). The coders indicated whether considerable task information, personal information, concreteness, and affective intensity were present in each text ( 1) or not (0). Comparing the coders' classifications with our text-mined classification revealed substantial agreement for both calls for bids (.73 to.94) and bids (.66 to.88) ([47]). The average F1 measure was sufficiently high for both bids (.79 to.95) and calls for bids (.80 to.95), as we detail in Web Appendix F. Experimental evidence of uncertainty reductionTo establish the internal validity of the chosen communication aspects on receivers' uncertainty perceptions, we conducted a series of experimental pilot studies. We used single-factor, within-subject designs for ( 1) task information, ( 2) personal information, ( 3) concreteness, and ( 4) affective intensity. For each pilot study, we recruited between 50 and 53 U.S. consumers with a mean age of 37.6 years (50% women) from Amazon Mechanical Turk (for details, see Web Appendix G). In line with previous research (e.g., [28]; [49]; [55]; [61]), we find that greater use of all four communication aspects in bids significantly reduces buyers' uncertainty perceptions and affects their hiring intentions. Predicting the Success of Buyers' Calls for Bids Model-free evidenceIn Web Appendix H, we summarize the model-free findings. The mean-level comparison indicates that calls for bids with significantly greater degrees of task information and concreteness, as well as significantly lower degrees of personal information and affective intensity, receive more freelance bids than an average call for bids (M = 5). Econometric model and identificationThe success of calls for bids reflects a count variable. Noting the overdispersion in the data (p < .001), we used a negative binomial model instead of a Poisson model. Furthermore, calls for bids are nested within buyers, and thus, the call for bids and number of freelancers who offer their service might be interdependent. The significant between-group variance (p < .001) and ICC( 1) of.27 suggests a multilevel structure. We therefore specified a multilevel model with a random intercept to control for time-invariant unobserved differences between buyers (e.g., education, country, gender) that could relate to differences in their success, using the following base equation: CALSUCij=y00+y01BTASKij+y02BPERSij+y03BCONCij+y04BINTEij+y05BTASK_SQij+y06BPERS_SQij+y07BCONC_SQij+y08BINTE_SQij+μ0j+εij, Graph( 1)where  CALSUCij  is the success of a call for bids i (i = 1, ..., 343,796) issued by buyer j (j = 1, ..., 49,081),  BTASKij  is buyer task information,  BPERSij  indicates buyer personal information,  BCONCij  is buyer concreteness, and  BINTEij  refers to buyer affective intensity in the call for bids. In turn,  BTASK_SQij  is buyer task information squared,  BPERS_SQij  is buyer personal information squared,  BCONC_SQij  is buyer concreteness squared, and  BINTE_SQij  is buyer affective intensity squared. Finally,  μ0j  is the random intercept and  εij  is the error term.Some empirical challenges inhibited a robust model identification, which we addressed in several ways. To account for observed heterogeneity, we incorporated covariates that might influence how many freelancers respond to a particular call for bids. First, in line with extant text mining studies ([ 7]), we controlled for the word count in each call for bids. Second, as a reputation cue, we measured buyer experience as the number of projects a buyer had commissioned previously on the platform prior to posting the focal call for bids ([32]). Third, a higher payment offer may attract more freelancers ([36]), so we determined the payment offered by the buyer in U.S. dollars, multiplied by an undisclosed index for anonymity. We used a dummy for nondisclosed payments, but we replaced missing values with a grand mean to retain the observations. Fourth, we measured project duration, as longer projects attract more freelancers ([36]). A dummy variable indicated whether the project was slated to last more ( 1) or less than a month (0). Fifth, more buyers demanding freelance services at the same time creates a relative shortage of freelancers ([36]). To account for an excess supply of freelancers, we calculated the sum of all active freelancers in the specific submarket of the call for bids and divided by the sum of all calls for bids posted around the same time (±31 days) in the same submarket. Sixth, the marketplace grew over time, so we included fixed effects for the year of the call for bids. Seventh, we included fixed effects for the seven submarkets, since submarkets that feature more complex projects have fewer qualified freelancers.Beyond these observed covariates, buyers' bid formulations might have varied by project characteristics unobservable to us. To the extent that these unobserved project characteristics influenced both the buyers' communication strategies and buyer outcomes, the estimated parameters might be biased. Therefore, we concatenated all service skill tags from the service profile of each freelancer who submitted a bid in response to a specific call. Then, to uncover the latent mixture of project types, we applied a latent Dirichlet allocation model to the project-specific skill tags (e.g., [ 7]; see Web Appendix I). We included the resulting 12 latent project characteristics as fixed effects to account for unobserved heterogeneity.Buyers also strategically make their communication decisions in learned anticipation of a larger number of bids or other factors, which were potentially unobservable to us. This strategic behavior could make communication approaches endogenous ([42]). Because our data did not contain valid, strong instruments for buyers' communications, we adopted [65] approach and used Gaussian copulas to model the correlation between each buyer communication  BCOMij1-4  and the error term. We added regressors to Equation 1, such that BCOMij1−4~=Φ−1[H(BCOMij1−4)], Graph( 2)where  Φ−1  is the inverse of the normal cumulative distribution function and  [H(BCOMij1−4)]  represents the empirical distribution functions of the four buyer communication approaches. The endogenous regressors must be nonnormally distributed for identification ([65]), and we confirmed this was true using Shapiro–Wilks tests (all p < .001). The updated equation to predict buyers' call for bids success, after correcting for endogeneity, was thus CALSUCij=y00+y01BTASKij+y02BPERSijqquad+y03BCONCij+y04BINTEijqquad+y05BTASK_SQij+y06BPERS_SQijqquad+y07BCONC_SQij+y08BINTE_SQijqquad+y09−14CONij1−6+y15−20YEARij1−6qquad+y21−26SUBMij1−6+y27−37PROJij1−11qquad+y38−41BCOM~ij1−4+μ0j+εij, Graph( 3)where  CONij1−5  is the vector of control variables,  YEARij1−6  are year effects,  SUBMij1−6  are submarket effects,  PROJij1−11  are latent project clusters, and  BCOM~ij1−4  are Gaussian copulas. We used a robust estimator to account for correlated and clustered standard errors. Results and discussionThe maximum variance inflation factor is 2.11, indicating no potential threat of multicollinearity. Table 2 contains the results of a main effects model and the full model, and Figure 1, Panels A–D, display the curvilinear effects from the full model. We have proposed that extremely sparse and extremely dense degrees of relevant information, concreteness, and affective intensity in calls for bids yield fewer freelance bids than moderate degrees of these communication elements. In line with our expectations, we find a positive linear effect (.152, p < .01) and negative squared effect for task information (−.026, p < .01), as displayed in Figure 1, Panel A. Moderate levels of the use of task information (50%:.222, p < .01) yield better results than sparse (10%: −.426, p < .01) and dense (90%: −.495, p < .01) uses. Furthermore, we find a positive linear effect (.052, p < .01) and negative squared effect for concreteness (−.080, p < .01) (Figure 1, Panel C). Moderate use (50%:.078, p < .01) yields better results than sparse use (10%: −.092, p < .01) or dense use (90%: −.251, p < .01) of concreteness. Contrary to our expectations, we find a negative linear effect (−.190, p < .01) and a positive squared effect (.032, p < .01) of personal information (Figure 1, Panel B). We also find a negative linear effect (−.084, p < .01) and a nonsignificant squared effect (.001, ns) of affective intensity (Figure 1, Panel D). Thus, it appears that any provision of personal information or greater use of affective intensity by the buyer is always ineffective. As a possible explanation, we note that in B2B online conversations, self-disclosure and emotions may be valued only after business relations have been established, not at the moment they form ([46]). Most of the exchanges in our data were between strangers, rather than being repeat exchanges, so it may be more appropriate for buyers to avoid personal details and appear rational rather than emotive.Graph: Figure 2. Response surfaces for bid success and price premium.GraphTable 2. Predicting the Success of Buyers' Calls for Bids. Model 1:Main EffectsModel 2:Full ModelβSE95% CIβSE95% CIBuyer Communication Task information.123**.003.117,.128.152**.003.146,.159 Personal information−.149**.004−.157, −.141−.190**.004−.199, −.181 Concreteness.040**.003.035,.045.052**.003.046,.057 Affective intensity−.098**.007−.112, −.085−.084**.008−.100, −.068Buyer Communication Squared Task information squared−.026**.001−.028, −.024 Personal information squared.032**.002.029,.035 Concreteness squared−.008**.001−.011, −.007 Affective intensity squared.001.001−.001,.004Controls Word count−.025**.003−.031, −.019−.027**.003−.033, −.021 Buyer experience−.033**.007−.046, −.020−.033**.007−.046, −.020 Project payment.170**.005.159,.181.168**.005.158,.179 Payment not disclosed.073**.005.063,.083.071**.005.061,.081 Project duration.116**.003.110,.122.117**.003.111,.123 Excess supply of freelancers.614**.004.606,.622.606**.004.598,.613Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedBuyers49,081Call for bids343,796 1 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is the count of all bids received. The sample included all projects listed by buyers with at least two projects to which at least one freelancer submitted a bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.2 *p < .05.3 **p < .01.To entice more freelancers to bid, buyers should keep their calls for bids brief (−.027, p < .01 for word count), which emphasizes the need for careful formulations. Higher payment offers (.168, p < .01), longer project durations (.117, p < .01), and an excess supply of freelancers (.606, p < .01) all increase the number of bids. Notably, the number of projects a buyer has previously commissioned relates negatively to the number of freelancers who bid (−.033, p < .01). These experienced buyers might have established relationships with specific freelancers, which reduces other freelancers' chances and causes them to refrain from bidding ([48]). Predicting Freelancers' Bid Success Model-free evidenceBids that offer less personal information and greater task information, concreteness, and affective intensity are more successful in winning projects. Among bids that won, the mean-level comparisons indicate nonlinear effects of mimicry. That is, successful freelancers mimic buyers' use of task information, personal information, and concreteness closely. If a buyer uses very sparse or very dense degrees of these communication aspects, the winning freelancers deviate more, indicating a nonlinear impact of mimicry. We do not find evidence of this mimicry relationship for affective intensity (see Web Appendix H). Measurement of similarityPrevious studies often operationalize communication similarity as the absolute difference between two measures (e.g., [52]; [75]), but this approach suffers some implicit constraints ([17]). In particular, difference scores suggest that one party's communication increases at the same magnitude as the other's decreases. They also ignore the degree at which the relative mimicry occurs. As a preferable alternative, we use polynomial regression, which allows for simultaneous testing of similarity and dissimilarity effects on bid success, at different levels of freelancers' and buyers' uses of the four communication aspects. In their study of positive and negative emotional tone convergence, [24] also use polynomial regression to explore the nuanced effects of convergence in leader–follower relationships on leader–member exchange quality. A simple regression model that captures absolute deviation cannot simultaneously assess the degree of task information by the buyer and the potential nonlinear effects of task information mimicry by the freelancer. So, we performed a polynomial regression with response surface analyses for each communication aspect to capture the extent to which freelancers mimicked a prospective buyer's provision of relevant information and communication manner. We detail this polynomial modeling approach that led to Equation 4 and the calculation of all polynomial terms, using task information as an example, in Web Appendix E. Econometric model and identificationWe tested freelancers' trade-off between adding more uncertainty-reducing communication versus mimicking the buyer's communication in a polynomial regression model that included linear terms, quadratic terms, and interactions. In the multilevel base equation to predict freelancers' bid success (ICC( 1) = .09, p < .001), BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+μ0l+εkl, Graph( 4) BIDSUCkl  is the success of bid k (k = 1, ..., 2,327,216) by freelancer l (l = 1, ..., 34,851),  FCOMkl1−4  are the four freelancer communication aspects,  BCOMkl1−4  indicate the four buyer communication aspects,  FCOM_SQkl1−4  are freelancer communication aspects squared,  (FCOMkl1−4×BCOMkl1−4)  are interactions of freelancer and buyer communication aspects,  BCOM_SQkl1−4  are buyer communication squared,  μ0l  is the random intercept, and  εkl  is the error term.We incorporated several covariates that might influence freelancers' bid success. As in the buyer model, we controlled for word count, project payment, project duration, and excess supply of active freelancers. We also included fixed effects for years, submarkets, and latent project characteristics. We accounted for the number of projects the freelancer completed prior to submitting the focal bid as a reputation cue that might determine bid success ([32]). Freelancer rating is an average five-point satisfaction rating that a freelancer has received for all completed projects. To retain observations of unrated freelancers, we included a dummy for observations without star ratings and replaced the missing values with a grand mean rating.Several additional controls relate to whether a bid is successful. First, following prior research, we assessed linguistic style matching, or the similarity between each bid and the respective call for bids, across nine function word categories ([52]). Second, we accounted for any previous relationship in which the freelancer had completed at least one project for the same buyer prior to the specific call for bids ([32]). Third, freelancers submit a bid price that may differ from the payment offered by the buyer, and a higher bid price may reduce the likelihood of bid success ([32]). In light of this, we measured each bid price as a ratio between the asking price and the average indexed bid price requested by all competing freelancers for the same call for bids. Fourth, the longer it takes freelancers to submit a bid, the lower their chances of success ([34]). So, we measured time-to-bid as the number of days between the posting of the call for bids and the bid submission. A dummy variable also indicates whether the bid was submitted late ( 1) or on time (0). Fifth, competition for a specific call for bid should impact each bid's success chances, so we controlled for the number of bids for the same call ([34]).Similar to buyers, freelancers make communication decisions strategically in anticipation of higher bid success or other, unobservable factors. Thus, freelancer communication is potentially endogenous, so we again used Gaussian copulas (Shapiro–Wilk tests: all p < .001). The updated equation to predict freelancers' bid success is as follows: BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−33CONkl1−13+y34−39YEARkl1−6+y40−45SUBMkl1−6+y46−56PROJkl1−11+y57−60FCOM~kl1−4+y61−64BCOM~kl1−4+μ0l+εkl, Graph( 5)where  CONkl1−14  is the vector of control variables,  YEARkl1−6  are year effects,  SUBMkl1−6  are submarket effects,  PROJkl1−11  are latent project clusters,  FCOM~kl1−4  are Gaussian copulas for bid text, and  BCOM~kl1−4  are Gaussian copulas for calls for bids text. Results and discussionThe maximum variance inflation factor is 3.86, indicating no threat of multicollinearity. Table 3 contains the results of the freelancer bid success models, Web Appendix K summarizes the response surface coefficients, and Figure 2 displays these coefficients on three-dimensional surfaces, reflecting relationships among freelancer communication, buyer communication, and bid success. We also highlight the misfit line used to explore the trade-off between exceeding and falling short of buyers' communication levels.GraphTable 3. Predicting Freelancers' Bid Success. Model 3:Freelancer CommunicationModel 4:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.014**.001.013,.015.015**.001.014,.016 y02: Personal information.018**.001.016,.019.017**.001.016,.017 y03: Concreteness.030**.001.029,.031.031**.001.030,.032 y04: Affective intensity.001.001−.001,.003.000.001−.002,.001Buyer Communication y05: Task information−.009**.000−.009, −.008 y06: Personal information−.017**.000−.018, −.017 y07: Concreteness−.008**.000−.009, −.008 y08: Affective intensity.001**.000.001,.002Freelancer Communication Squared y09: Task information squared−.006**.000−.006, −.005−.006**.000−.007, −.006 y10: Personal information squared−.005**.000−.005, −.004−.005**.000−.005, −.004 y11: Concreteness squared−.006**.000−.007, −.006−.007**.000−.007, −.007 y12: Affective intensity squared.000**.000.000,.001.000**.000.000,.001Freelancer–Buyer Interactions y13: Task information interaction.015**.000.015,.016 y14: Personal information interaction−.002**.000−.002, −.001 y15: Concreteness interaction.005**.000.004,.005 y16: Affective intensity interaction.020**.000.020,.021Buyer Communication Squared y17: Task information squared.001**.000.001,.002 y18: Personal information squared−.004**.000−.005, −.004 y19: Concreteness squared.001**.000.001,.001 y20: Affective intensity squared.000*.000.000,.000Controls Word count−.022**.001−.024, −.021−.021**.001−.022, −.020 Linguistic style matching.051**.001.048,.053.051**.001.048,.053 Freelancer experience.002**.001.001,.003.002**.001.001,.003 Freelancer rating.010**.001.009,.010.010**.001.009,.010 Project payment−.001**.000−.001, −.001−.001**.000−.001, −.001 Payment not disclosed−.028**.000−.029, −.028−.029**.000−.030, −.028 Previous relationship.078**.001.076,.081.078**.001.075,.080 Bid price−.006**.000−.006, −.006−.006**.000−.007, −.006 Time-to-bid.001.000.000,.001.001.000.000,.001 Late submission−.005**.000−.005, −.004−.004**.000−.005, −.004 Competition−.251**.007−.265, −.238−.251**.007−.264, −.238 Excess supply of freelancers−.044**.000−.045, −.043−.042**.000−.043, −.042Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedFreelancers34,851Bids2,327,216 4 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is whether the freelancer was chosen and won the bidding process. The sample included all bids by freelancers with at least one winning and at least one losing bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.5 *p < .05.6 **p < .01.We have proposed that when the degree of relevant information, concreteness, and affective intensity provided by the buyer is at least moderately dense (sparse), freelancers can increase (decrease) their chances of bid success by mimicking the buyer's communication. The surface-level tests along the plotted misfit line (Web Appendix K) display negative curvatures for task information (−.020, p < .01), personal information (−.007, p < .01), concreteness (−.011, p < .01), and affective intensity (−.020, p < .01). These results indicate that mimicking the buyer's communication increases bid success (see Web Appendix L for further clarification).In line with our proposition, we qualify this effect for sparse degrees of task and personal information, concreteness, and affective intensity provided by the buyer in Web Appendix M. If we were to find positive slope coefficients at lower levels, it would suggest that freelancers can increase their chances of bid success by exceeding, rather than mimicking, the buyer's communication. This prediction holds for personal information (.020, p < .01) and concreteness (.024, p < .01), according to the slopes at low levels of buyer communication. However, contrary to our expectations, we find negative effects for the slopes of task information (−.008, p < .01) and affective intensity (−.030, p < .01) at low levels of buyer communication. Therefore, freelancers should always mimic the degree of task information and affective intensity provided by the buyer. For these two communication aspects, the tenets of communication accommodation theory ([75]) and adaptive selling ([78]) hold: mimicking the buyer is always better. To increase their chances of bid success further, freelancers also must keep their bids concise (−.021, p < .01 for word count). Reputation cues (experience:.002, p < .01; rating:.010, p<.01) increase freelancers' chances of bid success, as do linguistic style matching (.051, p < .01), previous business relations with the buyer (.078, p < .01), lower bid prices (−.006, p < .01), timely (cf. late) bid submissions (−.004, p<.01), lack of competition (−.251, p < .01), and reduced supply of freelancers (−.042, p < .01). Predicting Freelancers' Price Premium Model-free evidenceBids with significantly less affective intensity and significantly more task information, personal information, and concreteness achieve greater price premiums than an average bid (M = 14% discount). Moreover, 96% of freelancers completed projects without any price premium, indicating the prevalence of value traps. The bids that achieved price premiums mimicked those buyers that made moderate use of task information, concreteness, and affective intensity closely, yet they deviated from buyers that made very sparse or very dense use of them. For personal information, we find a distinctive, positive, linear relationship for mimicry. Successful freelancers mimicked buyers that supplied a lot of personal details but deviated if buyers supplied very little or moderate degrees of personal information (Web Appendix H). Econometric model and identificationThe price premium analysis is restricted to bids that win and buyers that disclose their payment offer upfront. Thus, our estimates may be biased by buyers' self-selection, in terms of which bid they chose and whether they disclosed payments. Therefore, we employed a two-stage selection model. In the first stage, we estimated a choice model, with the availability of the necessary data as a binary dependent variable (i.e., bid was won and payment was disclosed). Using this model, we computed the inverse Mills ratio to account for the potential selection bias (probit model in Web Appendix N) and included this correction term in the final model estimation. To identify second-stage parameters, there needed to be one term in the first-stage equation that was unrelated to the error term in the freelance price premium equation. We thus included the dummy that indicates if the bid was submitted late only in the first-stage equation because this term explained buyers' choice of the bid, but we did not expect it to be conceptually related with the eventual price premium. Thus, this term satisfied both relevance and exogeneity requirements. The updated equation of our multilevel model (ICC( 1) = .13, p < .001) is as follows: PREMIUMkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−31CONkl1−11+y32−37YEARkl1−6+y39−43SUBMkl1−6+y44−54PROJkl1−11+y55−58FCOM~kl1−4+y59−62BCOM~kl1−4+y63IMRkl+μ0l+εkl, Graph( 6)where  PREMIUMkl  is the price premium of bid k (k = 1, ..., 148,158) offered by freelancer l (l = 1, ..., 30,851), and  IMRkl  is the correction term. Results and discussionThe maximum variance inflation factor is 2.74, indicating no threat of multicollinearity. Table 4 contains the results of the freelancer price premium models, Web Appendix K details the response surface coefficients, and Figure 2 displays the surfaces.GraphTable 4. Predicting Freelancers' Price Premium. Model 5:Freelancer CommunicationModel 6:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.023**.002.020,.026.022**.002.019,.025 y02: Personal information.021**.002.017,.025.021**.002.017,.026 y03: Concreteness.006**.001.004,.007.005**.001.003,.007 y04: Affective intensity.004.003−.002,.010.003.003−.003,.009Buyer Communication y05: Task information.003*.001.001,.005 y06: Personal information−.016**.002−.019, −.012 y07: Concreteness−.004**.001−.006, −.002 y08: Affective intensity−.001.002−.005,.003Freelancer Communication Squared y09: Task information squared−.001.001−.002,.000−.001.001−.002,.001 y10: Personal information squared−.004**.001−.006, −.002−.004**.001−.006, −.002 y11: Concreteness squared−.003**.001−.004, −.002−.003**.001−.005, −.002 y12: Affective intensity squared.000.000−.001,.000.000.000−.001,.000Freelancer–Buyer Interactions y13: Task information interaction.025**.003.024,.027 y14: Personal information interaction−.004**.001−.005, −.002 y15: Concreteness interaction.002**.000.001,.002 y16: Affective intensity interaction.010**.003.008,.012Buyer Communication Squared, y17: Task information squared.003**.001.001,.004 y18: Personal information squared.003**.001.002,.005 y19: Concreteness squared−.002**.001−.004, −.001 y20: Affective intensity squared.002**.001.000,.003Controls Word count−.014**.002−.018, −.011−.014**.002−.018, −.011 Linguistic style matching.022**.004.013,.030.023**.004.015,.032 Freelancer experience−.001.001−.004,.001−.001.001−.004,.001 Freelancer rating−.001.001−.003,.001−.001.001−.003,.001 Project payment−.029**.010−.049, −.009−.029**.010−.049, −.009 Previous relationship.057**.001.054,.059.056**.001.054,.059 Time-to-bid.009**.001.007,.012.009**.001.006,.011 Competition−.015**.002−.019, −.011−.015**.002−.019, −.011 Excess supply of freelancers−.001.001−.002,.001−.001.001−.002,.001Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedSample-Selection Correction Inverse Mills ratio−.016**.002−.020, −.012−.016**.002−.020, −.012Freelancers30,851Bids148,158 7 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is price premium for the chosen bid. The sample includes all winning bids for which the payment was disclosed. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.8 *p < .05.9 **p < .01.We proposed that when the degree of relevant information and communication manner provided by the buyer is at least moderately high (low), freelancers increase (decrease) their chances of earning a price premium by mimicking this communication. Web Appendix O displays the misfit lines on two-dimensional planes. In line with our expectations, the surface-level tests along the plotted misfit line show a negative curvature for task information (−.023, p < .01), concreteness (−.007, p < .01), and affective intensity (−.008, p < .01), such that mimicking the buyer's communication increases bid success. However, for personal information, we find a positive curvature (.003, p < .05), which implies freelancers should always offer more personal information than the buyer. For these B2B services, the provider and the service are inseparable, which may lead buyers to place more value on personal information about freelancers, even if their own provision of personal details in the calls for bids is sparse.If a buyer provides little relevant information or is less concrete (Web Appendix P), a positive slope would suggest that freelancers can increase their chances of earning a price premium by exceeding rather than mimicking the buyer. We find support for this prediction in the slope of personal information (.027, p < .01) at low levels of buyer personal information. However, negative effects emerge from the slopes of task information (−.016, p < .01) and affective intensity (−.012, p < .01), and we find a nonsignificant effect for concreteness (.002, n.s.). Mimicking the buyer's task information and affective intensity is always better, which is in line with accommodation theory and adaptive selling ([75]; [78]).Freelancers also increase their price premiums by avoiding lengthy bids (−.014, p < .01). Although platform reputation cues (experience and rating) can boost freelancers' chances of bid success, they do not determine the final price buyers pay. The skew in the ratings toward very high scores may limit their ability to help prospective buyers determine an appropriate price ([45]. Linguistic style matching (.023, p < .01), a previous relationship with the prospective buyer (.056, p < .01), submitting early in the bid process (.009, p < .01), and reduced competition (−.015, p < .01) all increase buyers' acceptance of a price premium. General DiscussionAcross disciplines, substantial research has identified various success determinants in online freelance marketplaces (e.g., [36]; [77]). For example, studies of B2B exchanges and two-sided marketplaces emphasize communication (see Web Appendix A). But at the specific word level, we lack insights into the optimal information or manner of communication ([ 7]). With this initial investigation of how buyers' and freelancers' success might be enhanced by appropriately managing the other party's uncertainty, we postulate, in line with uncertainty reduction ([ 6]) and uncertainty management ([ 9]) theories, that communication that is not completely informative and clear may still be effective. Accordingly, we investigate how buyers' communication can attract freelance bids and how freelancers' communication can determine their bid success and price most effectively, and the results offer both theoretical and practical implications. Theoretical ContributionsFirst, we advance research on how buyers' communication determines their ability to attract freelancers. Drawing on prior communication research, we identify communication principles that critically relate to receivers' uncertainty, such as relevant task and personal information and the relative concreteness and affective intensity with which this information is communicated ([ 8]; [27]). To entice more freelancers to bid, buyers should carefully formulate their calls for bids to keep them brief. Freelancers' information processing motivation, time, skills, and proficiency likely are limited, so buyers must choose their wording carefully and select from various effective communicative aspects. They can attract a larger pool of bids if they provide moderate degrees of task information in a moderately concrete manner. Offering too little of these features leaves freelancers with too much uncertainty, and dense information provision or being very concrete is too restrictive. If buyers provide greater degrees of personal information or express greater affective intensity in their calls for bids, it reduces the number of service offers they receive. This finding contrasts with uncertainty reduction theory ([ 6]) and B2B research that suggests self-disclosure strengthens buyer–seller cooperativeness ([41]). However, instead of ongoing B2B relationships, our study refers mostly to initial interactions between strangers (in 98% of cases, the freelancer had never worked for the prospective buyer). Evidence obtained from buyer–seller online chats similarly suggests that self-disclosure and emotive expressions are valued only in existing B2B relationships, not in new ones ([46]). Overall, we offer empirical support for communication theorists' suggestions that common communication principles can be purposefully flouted to achieve better conversation outcomes ([23]).Second, freelancers must keep their bids concise. They too face a trade-off between reducing the buyer's uncertainty and offering overly dense information. In line with research on communication accommodation ([75]) and adaptive selling ([78]), we show that freelancers can improve their bid success by mimicking the prospective buyer's communication. Adding to these research streams, we introduce a contingency perspective that reveals the efficacy of mimicry depends on the degree to which buyers use specific communication elements. In line with accommodation theory and adaptive selling, bid success always improves when freelancers mimic buyers' provision of task information and use of affective intensity. However, in line with uncertainty reduction theory ([ 6]) and expectancy violations ([ 1]) when buyers supply little personal information and are less concrete, freelancers can increase their chances of bid success by diverging and providing more personal information and concreteness.Third, freelancers often struggle to avoid value traps in which they sell more of their services for less ([76]). Rational buyer expectations should allow high-quality freelancers to charge price premiums ([70]), but the quality of freelance services is unobservable prior to purchase, and rational buyers might refuse to pay any price premium if they feel uncertain and suspect the freelancer may be hiding information ([16]). Therefore, to achieve premiums, freelancers should offer short, appropriately formulated bids. Buyers are more willing to pay a premium to freelancers who mimic their provision of task information, concreteness, and affective intensity, which is in line with communication accommodation theory ([75]) and adaptive selling research ([78]). However, similar to the findings for bid success, freelancers should offer more personal information than buyers, rather than mimicking buyers' provision of such information. In most service settings, a ""bad"" seller might provide a great product by chance; however, almost by definition, a bad freelancer produces bad service ([36]). This tight coupling between the freelancer and service quality represents a conceptual distinction in our study, which accordingly shows that buyers' willingness to pay a premium increases with more personal information issued by the freelancer. Practical ImplicationsOur findings offer actionable insights for the millions of buyers and freelancers utilizing online freelance marketplaces, the collective value of which is predicted to reach $2.7 trillion by 2025 ([56]). In detail, being informative and unambiguous may be a common assumption, but it is not an imperative, nor does it always lead to success. Implications for buyersAlthough 59% of U.S. companies use a flexible workforce to some degree, more than one-third of contracted projects are never completed ([33]). To attract freelancers, buyers should keep their calls for bids succinct. Beyond that recommendation, we offer several tips for formulating calls for bids in Table 5. In particular, a task description with a moderate amount of information helps freelancers anticipate the task without overloading them with details. Due to the relative anonymity of online freelance marketplaces, buyers might assume that freelancers will need to know who they are, but instead, we find that the less buyers describe themselves (to focus on describing the task), the better the outcomes. Relatable and imaginable (rather than abstract) descriptions of the project help freelancers grasp the requirements. However, being excessively concrete becomes prescriptive, which deters freelancers. Using emotion words makes the content of a call for bids relatively more intense. Such intensity can remove ambiguity and make opinions quickly accessible, but we find that calls for bids are more effective if they are formulated relatively impassively. Enthusiastic project descriptions seemingly might raise freelancers' suspicion that the project is too good to be true. Also, offering higher payment might attract a larger pool of freelance bids, as do long- rather than short-term gigs. Finally, more freelancers bid when there are fewer calls for bids in the subsector.GraphTable 5. Buying and Selling Services in Online Freelance Marketplaces. How to Formulate Calls for Bids to Attract FreelancersBad Practice ExcerptGood Practice ExcerptLift in BidsSpecify tasks and skills""I need a website to showcase the full range of my fitness workouts.""""I need a website designer who can design a WordPress website using a WordPress premium theme.""An increase in task terms from 18% to 29%, resulting in 5% more bids.Avoid personal information""I have been creating my own classes for almost 10 years now...clients tend to especially love my classes on strength and flexibility. Now I need help setting up my website.""""I am a Fitness Trainer and need help with building my website to showcase my mixed services and home workouts.""A decrease in personal terms from 9% to 4%, resulting in 4% more bids.Be moderately concrete""I require a professional who is savvy in configuring a stylish website employing a premium theme.""""You should have got very good creative skills but know how to design for web and also know how to include calls to actions within a good design.""An increase in concrete terms from 21% to 26%, resulting in 1% more bids.Avoid being affectively intense""I have created a fantastic theme but you should be confident and eager about WordPress and help optimize.""""The theme and examples will be provided, but you should also know about WordPress and optimize.""A decrease in affective terms from 11% to 4%, resulting in 4% more bids.How to Formulate Successful Bids and Achieve Price Premiums Bad Practice ExcerptGood Practice ExcerptLift in Bid SuccessMimic task description""Dear Sir, would love to work for you...""""Hi Gary, I am happy to help you with your fitness website development and design...""An increase in task terms from 16% to 25%, resulting in 7% higher bid success and 8% higher price premium.Exceed buyers who supply little personal information""I am an enthusiastic designer and expert in Web development...""""I am a WordPress Freelancer with 15 years of work experience...""An increase in personal terms from 6% to 8%, resulting in 3% higher bid success and 4% higher price premium.Exceed buyers who are not concrete""I have great skills and plenty of fantastic experience in creating relevant websites...""""I have worked on several similar projects, designing websites, also using WordPress, including premium themes and I can deliver to a tight schedule...""An increase in concrete terms from 24% to 30%, resulting in 7% higher bid success (but no effect on price premium).Mimic the buyer's affective intensity""The content will be creative and fun, attractive, and thoughtful...""""Website content that I produce will be creative and include original designs...""A decrease in affective terms from 18% to 6%, resulting in 11% higher bid success and 7% higher price premium. 10 Notes: Web Appendix S provides the full call for bids and bid examples we used for calculating the degrees of each communicative principle and the corresponding expected lift success. We used the ""good practice"" call for bids example to devise the bad and good examples for the corresponding freelance bid. Implications for freelancersFreelancers are not necessarily natural marketers, but their bid formulations determine their marketability. Existing online reputation systems provide some assistance, but they also create entry barriers to new freelancers who first must earn good overall ratings ([13]). Fortunately, winning gigs and achieving price premiums also depends on freelancers' communication. Table 5 includes advice to help freelancers formulate more successful bids and avoid the value trap. In line with the mantra of adaptive selling, the call for bids provides a starting point in which mimicking the buyer's task information and affective intensity increases freelancers' success—even if they provide few task details or seem very impassive. But freelancers should always offer personal information and be concrete. Even if a buyer does not provide personal information or the call is relatively abstract, freelancers' chances of success and obtaining price premiums increase if their bids contain more personal information and are at least somewhat concrete. The strongest predictor of bid success is a preexisting buyer relationship, so more broadly, freelancers should grow their buyer relations. Limitations and Directions for Further ResearchIn examining theoretically grounded communicative aspects, we offer novel insights into how to manage uncertainty in buyer–freelancer exchanges. Intriguingly, we find that communication approaches that do not aim to minimize uncertainty can be effective. Continued research should investigate this notion further and develop additional insights into the exchange implications of linguistic choices in B2B but also B2C and C2C communication on multisided platforms ([53]). For example, affiliative ([66]) or collaborative terms might affect uncertainty and influence exchanges as well. Arguably, the personal characteristics of buyers and freelancers (e.g., gender, education, experience), channel choices ([50]), different sources of uncertainty ([29]), perceived risks ([25]), and spatial distances between buyers and freelancers also might moderate the efficacy of communication aspects, so additional research should specify their influences. For example, if buyers lack the expertise to specify what they want, they might benefit from more ambiguous calls for bids ([38]). Perhaps buyers' communication or alternative factors that we cannot account for (e.g., underestimation of the amount of work required to fulfill the task) influence the final price they pay, too. Efforts to specify these additional effects also might address some of our more controversial findings, such as the evidence that the number of previously commissioned projects by a buyer relates negatively to the number of freelancers who bid. We posit that experienced buyers might prefer freelancers whom they have hired in the past ([48]). Buyers also might have incurred switching costs or supplier dependencies ([29]). Methodologically, we estimated all the models sequentially, as buyers' calls for bids and their success occur prior to freelancers' bids and their success. But an equilibrium approach that estimates these models simultaneously at the bid level could reflect an alternative way to think about the data structure. The concreteness word list we used ([11]) may also require further refinement to differentiate specific concreteness levels among the word set. Finally, the anonymity and speed of exchanges in online freelance marketplaces may make communication particularly important in this context. A comparative analysis of the influence of uncertainty management efforts across different B2B contexts beyond these marketplaces could offer interesting insights, especially if uncertainty avoidance is a central goal.  "
11,"Conducting Research in Marketing with Quasi-Experiments This article aims to broaden the understanding of quasi-experimental methods among marketing scholars and those who read their work by describing the underlying logic and set of actions that make their work convincing. The purpose of quasi-experimental methods is, in the absence of experimental variation, to determine the presence of a causal relationship. First, the authors explore how to identify settings and data where it is interesting to understand whether an action causally affects a marketing outcome. Second, they outline how to structure an empirical strategy to identify a causal empirical relationship. The article details the application of various methods to identify how an action affects an outcome in marketing, including difference-in-differences, regression discontinuity, instrumental variables, propensity score matching, synthetic control, and selection bias correction. The authors emphasize the importance of clearly communicating the identifying assumptions underlying the assertion of causality. Last, they explain how exploring the behavioral mechanism—whether individual, organizational, or market level—can actually reinforce arguments of causality.Keywords: quasi-experiments; marketing methods; econometricsQuasi-experimental methods have been widely applied in marketing to explain changes in consumer behavior, firm behavior, and market-level outcomes. ""Quasi-experiment"" refers to the use of an experimental mode of analysis and interpretation to data sets where the data-generating process is not itself intentionally experimental ([23]). Instead, quasi-experimental research uses variation that occurs without experimental intervention but is nonetheless exogenous to the particular research setting. Work using quasi-experiments in marketing settings has used events such as weather, geographic boundaries, contract changes, shifts in firm policy, individual-level life changes, and regulatory changes to approximate a real experiment. In each case, an external shock creates a source of exogenous variation that the researcher uses to establish a causal relationship between the variation and the outcome of interest.Companies also use quasi-experimental methods to understand the consequences of key business actions. For example, [17] analyzed a quasi-experiment where eBay shut all the paid search advertising on Bing during a dispute with Microsoft but lost little traffic. These quasi-experimental results inspired a follow-up field experiment where eBay randomized suspension of its branded paid search advertising and found results consistent with the quasi-experiment. Reflecting the importance of such methods at firms, some companies provide causal inference training for their data scientists ([32]; [81]). The ability to make causal claims is highly valuable in academia and in practice. This article aims to help both marketing scholars and practitioners conduct and evaluate the credibility of quasi-experimental studies.Quasi-experimental research, as in much work in applied statistics, begins with the equation  y=f(X,ε;β)  . The focus is then on whether a change in a single covariate x in the vector of  X  can be demonstrated to cause a change in  y  . This focus often enables the exploration of foundational questions in marketing, because marketers often have data representing the actions of many individual consumers or clients and need to understand the causal relationship between a particular x and y to make decisions about whether and how much x to use.A marketing article that successfully uses the quasi-experimental econometric approach considers the following nine topics, which are echoed in the structure of this article: Research Question: Do We Care Whether x Causes y? Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x? Identification Strategy: Does x Cause y to Change? Empirical Analysis: How Can Researchers Estimate the Effect of x on y? Challenges to Research Design: What if Variation in x Is Not Exogenous? Robustness: How Robust Is the Effect of x on y? Mechanism: Why Does x Cause y to Change? External Validity: How Generalizable Is the Effect of x on y? Apologies: What Remains Unproven and What Are the Caveats?We start by explaining why quasi-experimental scholars may appear obsessed with identification, and how this influences the choice of research question and data setting. Quasi-experiments come in different shades, ranging from an almost completely random exogenous shock to where the treatment assignment is only partly random. We suggest different frameworks to accommodate various levels of evidence depending on the strength of the underlying identification argument. We then turn to the importance of understanding the underlying mechanism behind the causal result. Typically, this means showing that the effect is largest where theory would predict and is smallest where theory would predict a negligible effect. We also emphasize that researchers need to be clear about the external validity of their study and apologize for what remains unconvincing. Why the Focus on Identification?Why are quasi-experimental scholars seemingly obsessed with identification? Identification is defined by the challenge that ""many different theoretical models and hence many different causal interpretations may be consistent with the same data"" ([58], p. 47). However, effective decision making requires an understanding whether a measured relationship is indeed causal.One way to describe this issue is through the ""potential outcomes approach"" developed by Jerzy Neyman, Donald Rubin, and others ([86]).[ 5] This approach starts with the insight that for any discrete treatment—which could be an event or explicit policy (  D  )—each individual  i  has two possible outcomes: yi1  if the individual i experiences the treatment  Di=1  , and yi0  if the individual i does not experience the treatment  Di=0 The difference between the two is the causal effect. The identification problem occurs because a single individual i cannot both receive the treatment and not receive the treatment at the same time. Therefore, only one outcome is observed for each individual at any point in time. The unobserved outcome is called the ""counterfactual."" The unobservability of the counterfactual means that assumptions are required. The identification problem means that those who experience D, and those who do not, are different in unobserved ways.Random assignment solves the inference problem, as the ""unobserved ways"" should not matter ex ante ([31]). [88], p. 13) explain that ""if implemented correctly, random assignment creates two or more groups of units that are probabilistically similar to each other on the average."" With enough people assigned randomly to one group or another, the only meaningful difference between the groups will be a result of the treatment.Therefore, random assignment is often called the gold standard of identification ([70], p. 8). [12], p. 11) emphasize that ""the most credible and influential research designs use random assignment."" That said, we should be clear that field experiments are merely a gold standard for being able to plausibly claim causality, not the gold standard for empirical work ([34]). Indeed, in many marketing situations, experiments are not feasible, appropriate, or affordable ([44]).Quasi-experimental work, by contrast, is aimed to identify exogenous shocks or events that can approximate random assignment. Given that assignment is not random, a researcher's goal is to make the unobserved ways in which the treatment and control groups differ as untroubling as possible to the researcher and the reader and thereby mimic random assignment as closely as possible. Research Question: Do We Care Whether x Causes y?The first and hardest stage in this process is identifying a question in which marketing scholars, managers, or policy makers actually care whether x causes y. This is difficult because many of the  y  s and  x  s for which we can measure a causal relationship are (unfortunately) uninteresting. Therefore, researchers who do quasi-experimental research do best if they start not with the data or an exogenous shock but instead start by asking themselves, ""Suppose I convincingly showed that an increase in x increases  y  —who would care about this substantive issue?This means that the first stage requires the identification of a causal relationship that would be of interest to marketers or policy makers because their decisions will be usefully informed by a clear understanding of the consequences of a particular action. As marketing technology and practices change, the number of measurable, interesting, and unanswered questions grows. A variety of editorials in this journal and elsewhere focus on how researchers can identify important issues. For example, the January 2021 special issue of the Journal of Marketing was dedicated to finding important marketing research questions, as highlighted in the editorial ([36]. Other editorials that discuss ways to identify important research questions are [59]) and [26] in the Journal of Marketing, [87]) in the Journal of Consumer Research, [96] in Marketing Science, and [50]) in the Journal of Marketing Research. Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x?[12], p. 7) explain that an identification strategy ""describe[s] the manner in which a researcher uses observational data or data that is not generated as part of an intentional experiment, to approximate a real experiment."" They suggest first thinking of an ideal randomized experiment that can address the research question. This helps the researcher see clearly why an effect may not be identified causally in a nonexperimental setting.As [75], p. 151) discusses, ""Good natural experiments are studies in which there is a transparent exogenous source of variation in the explanatory variables that determine treatment assignment."" Unfortunately, there is no universally accepted interpretation of what it means to have a transparent exogenous source of variation. Therefore, [75] (p. 151) emphasizes the importance of clarifying identification assumptions and understanding the institutional setting, stating, ""If one cannot experimentally control the variation one is using, one should understand its source."" In the marketing context, [84] discusses the dangers of using methods in which the source of the exogenous variation is either poorly understood or only weakly related to the correlation of interest.Much of the work using quasi-experimental variation in marketing settings uses mundane but easily understood events such as contract changes, regulation, individual-level life changes, or shifts in firm policy that did not occur because of an anticipated effect on the outcome of interest. In some sense, some of the best sources of exogenous variation are mundane: nonmundane sources of variation such as global pandemics or earthquakes tend to be associated with other things happening that make it difficult to establish a clean causal relationship.Table 1 lists several example quasi-experimental papers published in 2018, 2019, and 2020 in the Journal of Marketing, the Journal of Marketing Research, and Marketing Science. This table also summarizes the source of variation these articles use, spanning contractual changes; ecological variation (e.g., weather); geography; and macroeconomic, individual, organizational, and regulatory changes. It is useful to consider in turn why each of these sources of variation can approximate random assignment.GraphTable 1. Examples of Quasi-Experiment Studies in Journal of Marketing, Journal of Marketing Research, and Marketing Science in 2018–2020. Quasi-Experimental VariationGeneral CategorySourceArticleResearch QuestionContractualTiming of American–Orbitz disputes to evaluate the absence of a major airline from a popular aggregator on consumer searchAkca and Rao (2020)Who has more market power in the airline-aggregator relationships?Timing of the introduction of the New York Times paywallPattabhiramaiah, Sriram, and Manchanda (2019)How does a paywall affect readership and site traffic?EcologicalVariation in the forecast error of the pollen levelsThomas (2020)How much does advertising affect purchases of allergy products?GeographicalDiscontinuities in the level of advertising at the borders of DMAsShapiro (2020)Does advertising affect consumer choice of health insurance?Discontinuities in the level of political ads at the borders of DMAsWang, Lewis, and Schweidel (2018)How does political advertising source and message tone affect vote shares and turnout rates in 2010 and 2012 Senatorial elections?IndividualTiming of users' adoption of a music streaming serviceDatta, Knox, and Bronnenberg (2018)How does a streaming service affect total music consumption?Variation in national ad exposures due to the local game outcomesHartmann and Klapper (2018)How do Super Bowl ads affect brand purchases?MacroeconomicVariation in income and wealth due to the recession between 2006 and 2009Dube, Hitsch, and Rossi (2018)Do income and wealth affect demand for private label products?OrganizationalDiscontinuity in the rounding rule that TripAdvisor uses to convert average ratings into displayed ratingsHollenbeck, Moorthy, and Proserpio (2019)How do online reviews affect advertising spending in the hotel industry?Timing of data breach and variation whether customer information was breached in a data breach eventJanakiraman, Lim, and Rishika (2018)How does a data breach announcement affect customer spending and channel migration?Variation in timing of adoption of front-of-package nutritional labels across categoriesLim et al. (2020)Do front-of-package nutritional labels affect nutritional quality for other brands in a category?RegulatoryTiming of the Massachusetts open payment lawGuo, Sriram, and Manchanda (2020)Do payment disclosure laws affect physician prescription behavior?Enforcement of minimum advertisement price policiesIsraeli (2018)What is the effect on violations if firms improve digital monitoring and enforcement of minimum advertised price policies?Timing of India's foreign direct investment liberalization reform in 1991Ramani and Srinivasan (2019)How do firms respond to foreign direct investment liberalization? 1 Notes: DMA = designated market area. ContractualTo find plausibly exogenous variation in timing, it often depends on an argument that the exact timing of a measure is plausibly exogenous. [28] argued that the timing of a dispute between the Associated Press and Google was essentially random as it was influenced by a contract negotiated many years previously, and so the timing could be used to study the effect of the removal of content from news aggregators on downstream news websites. EcologicalGenerally, within-season variation in weather is plausibly exogenous. For example, [95] uses quasi-experimental variation in actual and expected pollen counts. Key to the identification strategy is the focus on deviations from what was expected by firms. GeographicalWork using geographical boundaries often exploits the fact that people who live on either side of a demarcated geographic border are similar enough to be thought of as being randomized across them. For example, by looking at a remote border of Maryland that was geographically isolated from the rest of the state, [ 8] were able to argue that the imposition of sales tax for those who lived on one side of the border was random, relative to those people who lived nearby but just happened to be over the state border. MacroeconomicIt is also possible to take leverage of macroeconomic shocks. For example, [40] use the Great Recession as a key source of the variation on household incomes over time. They exploit the within-household variation in private label shares associated with within-household changes in income and wealth. The identifying assumption is that, conditional on all other factors, including an overall trend, within-household changes in income and wealth are as good as randomly assigned or exogenous changes. IndividualPlausibly exogenous variation can also be argued to occur at the individual level. For example, [20] use consumer migration to new locations as a quasi-experiment to study the causal impact of past experiences on current purchases. They argue that while migration is not necessarily random, the precise direction of migration can be, at least with respect to local brand market shares. OrganizationalShifts in firm policy and organizational events can also be leveraged as a source of variation. For example, [64] assess the change in customer behaviors between those whose information is breached and those whose information is not. The identification assumption is that the assignment of customers into the data breach group is likely to be random. RegulatoryMany papers also use the timing of regulatory changes as a source of variation. The argument here is typically that though the imposition of regulation may not be random, the timing of the regulation is. For example, [97] use a change in Massachusetts regulation of home sale listings to identify the effect of information about time on the market on house prices, and [76] use a change in the standardized nutrition labels on food products required by the Nutrition Labeling and Education Act and investigate how the Act changes brand nutritional quality.This discussion emphasizes that there are many potential sources of exogenous variation that can approximate a randomized experiment. We emphasize that typically the best papers focus on the research question first, and then imagine what the idealized experiment would look like to identify an actual quasi-experiment. Identification Strategy: Does x Really Cause y to Change?To convince a reader that an identification strategy is valid requires two steps. First, the researcher must explain where the variation they are calling exogenous comes from. This requires institutional knowledge and careful research into the setting. Second, the researcher needs to demonstrate that the relationship between the variation and the outcome of interest is very likely driven by the relationship between x and y and not by some other factor.To achieve the second requirement, it is useful to think about defending the experiment in terms of the exclusion restriction. Although the term ""exclusion restriction"" is often used specifically for instrumental variables, it is also a useful concept for other quasi-experiments. The exclusion restriction states that the quasi-experiment only affects y because it affects x.There are a variety of ways in which the exclusion restriction can fail, and so researchers look for exogenous variation in x that will have no direct effect on y. For example, [91] use wind speed as a quasi-experiment to provide an exogenous driver of posting to a user-generated content site about windsurfing. This allows them to understand the relationship between content creation and the creation of social ties. The argument for the exclusion restriction is that there is no other plausible way that wind could affect the creation of social ties except through content creation. As they mention in the paper, plausible challenges to this exclusion restriction are that windy days could affect friendship formation directly because users meet future online friends at windier surf locations. To address such challenges, the researchers present empirical data to suggest that the social ties that are being formed do not seem to reflect geography.Another example is [66], which examines the effect of delays in the early part of a banking technology adoption process on ultimate usage. Through a quasi-experiment that provides a source of exogenous variation in delays, they exploit the fact that Germany has a highly regulated system of public holidays and vacations that vary at the state level to prevent freeways from becoming overly congested. This leads to delays in technology adoption in that particular period to customers in one state, and not in others. The exclusion restriction is that there is no other reason that vacations or public holidays in the few days surrounding adoption would affect ultimate usage except through delaying the ability to navigate the security protocols required to sign up for the online banking service. One challenge for the exclusion restriction could be that individuals who sign up for a banking service around public holidays are somehow systematically different from others in terms of their laziness or motivation. To counter this challenge, the researchers present evidence that users are not different along any observable dimension.The exclusion restriction can also fail because of spillovers between groups that receive the exogenous shock or treatment and those that do not. The assumption that treatment of unit i affects only the outcome of unit i is called the stable unit treatment value assumption (SUTVA) in the treatment literature ([10]; [61]). This is not a trivial assumption. For example, [ 5] use the 2011 Orbitz–American Airlines disputes as an exogeneous event that led to a five-month period in which American fares were not displayed on Orbitz. The authors use this dispute to identify which company was hurt the most in terms of site visits and purchases. The SUTVA requires a valid control group such that the Orbitz–American Airlines disputes have no spillover on that group. As a result, the authors chose not to use airfare- or hotel-booking websites as a control due to the possible spillovers from Orbitz to other websites where customers can purchase. Instead, the authors used consumers' search of Lonely Planet as the control, because Lonely Planet is a travel website that is rarely used for bookings. The underlying idea is that an exclusion restriction cannot hold if the fact that one group was treated may also affect the control group's behavior. The SUTVA is therefore part of an argument that researchers make about an appropriate exclusion restriction.Importantly, there is no formula for a convincing explanation and defense of the empirical identification strategy in quasi-experiments. Except in cases of random assignment, it is not possible to prove that the identifying assumption is right. Instead, the objective for the authors is to pursue projects only when they can convince themselves (and their readers) that the causal interpretation is more plausible than other possible explanations. It is impossible to prove the validity of a quasi-experiment, such as whether one set of U.S. states serves as a legitimate control group for another or whether the exclusion restriction holds in instrumental variables. The credibility of any quasi-experimental work therefore relies on the plausibility of the argument for causality rather than on any formal statistical test. Empirical Analysis: How Can Researchers Estimate the Effect of x on y ?After establishing the identification assumption through the underlying framework of an exclusion restriction, the next step is to explore the data and conduct analysis that allows measurement of the effect of interest. This measured causal relationship is what has the potential to inform decision making. We discuss three different regression analysis frameworks using quasi-experiments: difference-in-differences (DID), regression discontinuity, and instrumental variables (IV). At the heart of all these strategies is a similar argument about the validity of the quasi-experiment.Table 2 outlines eight key steps in the three regression analysis frameworks. As pointed out by [54]) and others, the techniques are very similar in terms of the underlying econometric theory. However, though similar in the conceptual ideas, in terms of practical implementation, presentation, and how the researcher should best reassure their audience about the validity of the technique, there are some differences, which we expand on. The three frameworks differ in the first four implementation steps. We discuss the first four steps for each of the three regression analysis frameworks and highlight the issues in common across the three analysis frameworks in the last four steps. We also emphasize that many excellent papers do not implement each step, and this description is not intended to lead to unproductive dogmatism.GraphTable 2. Quasi-Experimental Regression Analysis Frameworks. Difference-in-DifferencesRegression DiscontinuityInstrumental VariablesIdentificationClarify the source of the shock, provide evidence why the shock can be seen as quasi-experimental, be clear on the identifying assumptions, and be transparent on the potential confoundedness.Justify the source of the fixed threshold, and whether the assignment to the treatment is determined, either completely or partly, by the value of the predictor on either side of a fixed threshold.Justify why the IV moves the endogenous covariate as if they are an experiment; explain the exclusion restriction.Raw dataTest whether those who receive the treatment are similar to those who do not; whether the parallel assumptions are satisfied; illustrate the trajectory.Provide evidence that the threshold is arbitrarily determined and not linked to underlying discontinuities in effects.Regress the outcome directly on the instrument and show that the instrument has the expected direct effect.Data analysisApply difference-in-differences regression framework in Equations 1 and 2 and adapt accordingly for other variations.Apply regression continuity framework in Equation 3.Report the first stage and determine whether the instruments are strong. Apply 2SLS in Equations 4 and 5 and conduct relevant tests.Standard errorsCluster at the level of treatment to account for within-unit correlation of the error term over time.Use robust standard errors, do not cluster on a discrete variableCluster at the level of treatment to account for within-unit correlation of the error term over time.Robustness checksConduct multiple robustness checks.Mechanism checksMeasure mediator variables or show moderation analysis.External validityDiscuss the assumptions required to capture the ATE.Apologies and caveatsApologize for all that is still unproven and give caveats. All of these methods implicitly rely on throwing out variation in the data that is not exogenous. In other words, they involve losing power to support the exogeneity assumption. This means that quasi-experimental work cannot use the R-squared as a useful summary of the appropriateness of the model. [41]) provide some useful evidence. While R-squared or a comparison of log-likelihoods is very useful in many other contexts (e.g., forecasts), benchmarking quasi-experimental analyses against other methods by using the R-squared will be misleading. Difference-in-Differences AnalysisA standard DID analysis compares a treatment group and a (quasi-) control group before and after the time of the treatment. The ""treatment"" is not truly a random experiment but, rather, some ""shock."" Unlike a simple comparison (or single-difference) analysis, DID methods generate a baseline for comparison between the treatment and the control group. By highlighting the change in the treatment group relative to the control group, DID enables the researcher to control for many of the most obvious sources of heterogeneity across groups.[47] is an example of a DID paper. The authors examine the impact of privacy regulation on the effectiveness of online advertising. In late 2003 and early 2004, many European countries implemented new restrictions on how firms could collect and use online data. The paper uses data on the success of nearly 10,000 online display advertising campaigns in Europe, the United States, and elsewhere between 2001 and 2008. The authors compare the change in effectiveness of the ad campaigns inside and outside Europe. Therefore, the first difference is the change in the campaign effectiveness, and the second difference is the change in Europe relative to elsewhere. Compared with before the regulation, ad campaigns became 2.8% less effective in Europe after the regulation. In contrast, compared with before the European regulation, ad campaigns became.1% more effective outside of Europe after the European regulation was implemented. Identification of Difference-in-DifferencesThe first step is to clearly lay out the identifying assumptions. [47], p. 63) state that ""the identification is based on the assumption that coinciding with the enactment of privacy laws, there was no systematic change in advertising effectiveness independent of the law"" and that ""the European campaigns and the European respondents do not systematically change over time for reasons other than the regulations."" A substantial portion of the paper is devoted to providing empirical evidence regarding whether ( 1) European ad agencies invest less in their ad creatives relative to non-European ad agencies after the laws, ( 2) the demographic profile of the respondents is representative of the general population of internet users, and ( 3) there may have been a change in European consumer attitudes and responsiveness to online advertising separate from the Privacy Directive.The analysis of consumer attitudes and ad responsiveness is based on a concern about unobservables, specifically whether there are alternative explanations for the measured changes in the attitudes of survey participants toward online advertising that were separate but contemporaneous with the change in European privacy laws. To check for such unobserved heterogeneity, [47]) examine the behavior of Europeans on non-European websites that are not covered by the European Privacy Directive to see if a similar shift in behavior can be observed, and they find evidence that changes in behavior are connected with the websites covered by the law, rather than the people taking the survey. The identification exclusion criterion is further validated by a mirror image of the falsification test by looking at residents of non–European Union (EU) countries who visited EU websites. When residents of non-EU countries visit EU websites, the ads are less effective in the postperiod. In contrast, when residents of these non-EU countries visit non-EU websites, there is no change in effectiveness before and after the EU regulation. Therefore, the results appear to be driven by what happens at EU websites rather than by a difference in how Europeans behave relative to non-Europeans. Raw Data Exploration of Difference-in-DifferencesThe second step is to explore the raw data. Before applying the DID framework, it is important to explore the raw data to assess whether the quasi-experiment appeared to have an effect. For example, when a treatment occurs in the middle of a time series, many papers use a graph that shows that before the treatment occurred, the treatment and control groups were on a similar trend and had similar values; then, after the treatment occurred, the trajectory of the treatment group diverged from the control group.Researchers should also assess whether their quasi-experimental setting meets the parallel trend assumption while exploring their raw data. This involves demonstrating that behaviors were similar in the period prior to the policy change across the treatment and control groups. Depending on the length of the time period, this can be done by conducting two-sample mean comparisons for each pretreatment period or by running a linear regression and looking at the time trend differences between the control and treatment groups. It is also often ideal to simply plot the raw data to support this point.Though it is desirable and convincing if the main effect of interest can be seen through descriptive statistics or visualization, we caution that this is not always possible. This may happen because effect sizes are small—as they often are in advertising—or because there is variation in the data that is best addressed using a regression framework. Analysis of Difference-in-DifferencesAlthough a DID regression can be represented in a 2 × 2 table, it is usually analyzed with regression analysis to allow researchers to control for factors that may change over time and across individuals. The simplest version of this regression is as follows: yit=α1TreatmentGroupi+α2AfterTreatmentt+βTreatmentGroupi×AfterTreatmentt+γXit+εit, Graph( 1)where y is the outcome of interest; i represents the individual, firm, or other cross-sectional unit of interest; t represents the time period; and  εit  represents the error. The key focus of the DID specification is on  β  , which captures the explanatory power of the crucial interaction term. Usually, researchers add controls  Xit  to address additional omitted variables concerns, such as an observed covariate that may not affect the treatment and control groups in the same way.When researchers have access to a panel, it is possible to address this concern directly by observing the same individuals, or the same campaigns, both before and after the timing of the treatment. It is then possible to add fixed effects to control for all individual-level (time-invariant) heterogeneity. Furthermore, if the data set includes more than two time periods, then adding time-specific fixed effects controls for all time-period-specific heterogeneity (across all individuals). With individual and time fixed effects, the DID regression is yit=βTreatmentGroupi×AfterTreatmentt+γXit+μi+τt+εit, Graph( 2)where  μi  is the individual-level fixed effect and  τt  is the time-period fixed effect. The fixed effects mean that the main effect of  TreatmentGroupi  and  AfterTreatmentt  drop out because they are collinear with the fixed effects. If possible, it is often desirable to difference out, rather than estimate, the fixed effects to avoid bias due to the incidental parameters problem (e.g., [67]). Most standard statistical packages automatically condition out the individual fixed effects from fixed effects panel data models where possible.[ 6]Though changes over time are common, DID methods do not require a time-series component. For example, [48] examine the impact of offline advertising restrictions on prices for keyword advertisements. The first difference is the keyword ad prices in states that have restrictions compared with states that do not. The second difference is the keywords that are affected by the restrictions compared with the keywords that are not.For quasi-experimental analyses that do examine changes over time, another tweak is that quasi-experimental treatment can occur at different times, meaning that individuals are treated at different times and that the  AfterTreatment  variable can change with subscripts i and t. For example, [27] study how a book review posted on Amazon affects sales of that book on Amazon, compared with sales of that book at barnesandnoble.com. Different books are reviewed at different times. Therefore, the treatment here is the review a book receives, and the  AfterTreatment  period occurs at different times for different books. [14], [19], [21], [35]), [49], and [85] explore the effects of variation in treatment timing. The issue is that because a fixed-effects DID estimator is a weighted sum of the treatment effect in each group and at each period, even though the weights sum to one, negative weights may arise when there is a substantial amount of heterogeneity in the treatment effects over time. A related concern has been highlighted by [45], who emphasize the problems that occur when both the treatment effect and treatment variance vary across groups.This means that researchers should be cautious in summarizing time-varying treatment effects with a homogeneous treatment effect as in the two-way DID framework if there is a substantial timing dimension. To address these issues, researchers have proposed a variety of estimators that allow for a cleaner comparison between the treated group and the control group. Both [21] and [35]) propose new estimands to estimate treatment effects in the presence of heterogeneity across groups and over time.[ 7] Another approach is taken by [93], who discuss corrections that should be applicable in a situation where leads or lags might be expected.Overall, DID is a powerful tool for helping identify the causal relationships that managers need for effective decision making. It can enable researchers to control for time-invariant individual-level heterogeneity, relying on the assumption that differences in the changes that the treatment and control groups experience over time are driven by the impact of the treatment. Regression Discontinuity AnalysisRegression discontinuity is a quasi-experimental technique in which the ""experiment"" relies on an exogenous arbitrary threshold. As [60], p. 616) put it, ""The basic idea behind the RD [regression discontinuity] design is that assignment to the treatment is determined, either completely or partly, by the value of the predictor being on either side of a fixed threshold."" Identification in regression discontinuityRegression discontinuity may be particularly useful to marketing scholars. [56] argue that many marketing interventions are based on thresholds of real or expected consumer or firm behavior. For example, direct mail companies use the scoring policies for recency, frequency, monetary models. Consumers just above and just below the cutoff should be similar in many dimensions, and their outcomes can be compared to assess the impact of the different mailings.Similarly, government policies based on firm size can provide a useful identification strategy for marketing scholars. For example, requirements for firms to post calories, undertake layoffs, and provide benefits often depend on the number of employees or other measures of firm size. By comparing firms just above and just below the threshold, it is possible to assess the effect of the policies on firm behavior.A regression discontinuity design implies that treatment is assigned depending on whether a continuous score  zi  crosses a cutoff  z¯  . The analysis then focuses on whether there is a change in the outcome of interest y in the neighborhood of  z¯  ([56]). In general, if a threshold is used as the source of the quasi-experiment, particular attention should be devoted to the source of the threshold and providing evidence that the threshold is essentially arbitrary and not likely to be linked to underlying discontinuities in behavior. Any discontinuity in the effect is assumed to be due to the treatment.This assumption is not always innocuous. Consider a $50 cutoff for receiving a marketing incentive. If the firm promotes the threshold and consumers try to achieve it, then there might be a substantial difference between people who spend $49 and people who spend $51. Those who spend $49 are likely to be unresponsive to the incentive because they did not try to cross the threshold to get the incentive. In contrast, those with exactly $50 in spending might have selectively chosen to spend exactly enough to get an incentive that they planned to use. It is important to address the potential for such concerns directly.This is reflected in a debate in economics about the effect of thresholds for low birth weight on medical outcomes. In an initial study, [ 6] used the fact that birth weight threshold of 1.5 kg is used to determine whether the newborn receives intensive medical treatment. In a critique of this work, [15] show that the children placed just at the cutoff seem to have significantly worse outcomes than babies on either side of the cutoff. This is evidence against use of this discontinuity for identification. [15] state, ""This may be a signal that poor-quality hospitals have relatively high propensities to round birth weights but is also consistent with manipulation of recorded birth weights by doctors, nurses, or parents to obtain favorable treatment for their children"" (p. 2119). Raw data exploration of regression discontinuityOnce the researcher has found a regression discontinuity setting, the first step is to explore whether the discontinuity is arbitrary and linked to discontinuities in any other variables. For example, [59] examine the relationship between online reviews and advertising spending in the hotel industry. They exploit the regression discontinuity design of the rounding rule that TripAdvisor uses to convert the average ratings of reviewers into the nearest half or full star (i.e., a rating of 3.74 is shown as 3.5 stars while a rating of 3.75 shown as 4 stars), building on work by [71]. The key identification argument is that the rounding mechanism creates discrete, random variations in perceived quality around the rounding threshold and is independent of a hotel's true quality.A threat to the arbitrary discontinuity threshold would be that hotels manipulate their average ratings around the rounding thresholds. [59] argue that if there is upward manipulation of ratings, there would be relatively few firms with average ratings just below the thresholds and a clump of firms with average ratings just above the thresholds. They show instead that the density of average ratings is uniform, with neither bumps nor dips above or below the round thresholds. They provide additional empirical evidence that characteristics of the hotels do not differ systematically above or below the threshold. Neither do they observe discontinuities in other key variables such as hotel prices and the number of five-star reviews. Analysis of regression discontinuityThe equation used for regression discontinuity can be written for panel data as yit=βI(zit≥z¯)+γXit+μi+τt+εit. Graph( 3)Here  β  is the treatment effect, the parameter of interest.  Xi  represents covariates.  I(zit≥z¯)  is an indicator function that equals one when  zit≥z¯  and zero otherwise. One final consideration is how to select the appropriate bandwidth for a regression discontinuity design, which is the question of how one decides on the sample to analyze, in terms of how far away the people in the sample are from the threshold where the discontinuity occurs. In general, such decisions have often been rather ad hoc, but there is an emerging literature that can help guide the researcher into thinking about how to take a more conservative approach to selecting bandwidth given the data at hand ([25]). The researcher should also ensure that their results are not sensitive to the choice of bandwidth. As with other quasi-experimental methods, the validity of the method cannot be statistically proven. Therefore, substantial emphasis must be placed on the explanation and defense of the quasi-experiment using raw data. Instrumental Variables AnalysisThe quasi-experimental perspective on IVs is somewhat different from the standard treatment in econometrics textbooks, which focuses on simultaneous equations and a more structural approach. The differences relate to justification and interpretation. The quasi-experimental approach emphasizes that the shocks that move the instrument should behave as if they are an experiment. The quasi-experimental approach gives a sense of the sign, significance, and magnitude of the causal effects. The structural approach emphasizes that the shocks should be motivated by an economic model that explains the exclusion restriction. The IV approach used in structural models gives elasticities that can be used to generate counterfactuals outside of the sample. Despite these differences in interpretation, it is important to remember that the underlying mathematics is identical. Identification of instrumental variablesThe basic idea behind using IVs is that the covariate of interest x contains both useful variation (to identify the causal effect of interest) and less useful variation (that confounds the effect). A good instrument z is strongly correlated with the useful variation but uncorrelated with the confounding variation. In other words, the researcher only uses the variation in x that can be explained by the exogenous shifter z.The standard two-stage model involves two steps. In the first-stage regression, a fitted value of  xi^  can be obtained by regressing x on instrument z and covariates  W  : xi=γzi+ϑWi+ηi. Graph( 4)In the second-stage regression, the IV estimator  β^  is obtained by regressing the outcome y on the fitted value of  x^  and covariates  W  : yi=βxi^+φWi+εi. Graph( 5)The identification of the effect of x on y relies on the following ""reduced form."" Inserting the predicted x to the y equation will give Equation 6. Here,  φ^  is used to highlight that when regressing y directly on instrument z and covariates W, the estimated covariate coefficient is rescaled as  φ^=βϑ+φ  . yi=βγzi+φ^Wi+εi. Graph( 6)Therefore, from the quasi-experimental point of view, an instrumental variable can be seen as a treatment that affects the endogenous covariate directly. This means that directly regressing the outcome of interest on the instrument (in one stage) will get the causal effect of interest, but it will not be properly scaled. The purpose of implementing two stages is to scale the treatment effect properly. There are many ways of operationalizing instrumental variables, and this can be a place for highly technical tools. We emphasize the simplest two-stage least squares (2SLS) approach, but the intuition behind the role of instrumental variables as an identification strategy remains regardless of functional form assumptions. Using two stages enables the researcher to disentangle  β  and  γ  . In other words, two stages are needed to get the elasticity right, but the experiment happens at the level of the instrument and so, even though the focus is on the relationship between x and y, the intuition on causality happens at the level of the relationship between z and y.Returning to [91], while the paper adds some additional necessary nuance to the estimation to fit the particular situation, the intuition on causality measures the impact of wind (the instrument  z  ) on social ties (the outcome of interest  y  ). This will be  βγ  . The relationship of interest, however, is the impact of posts (  x  ) on social ties (  y  ), which is measured as  β  .IV can be a less transparent solution to identifying causal effects compared with the other two analysis frameworks discussed previously (for a detailed discussion, see [84]). The distinction between the relationship of interest (  β  ) and the direct estimate from the quasi-experiment (  βγ  ) means that it is sometimes harder to visualize how the quasi-experimental variation works in IVs.Transparent communication of IV analysis is difficult for three reasons. First, in contrast to the binary nature of the exogenous variation in DID and regression discontinuity, instruments are often continuous. This makes it more difficult to communicate the intuition for why the variation is exogenous to the potential for omitted variables or simultaneity. The ability to use continuous instruments (and multiple instruments) can also be seen as a strength of IV techniques. They enable a more flexible set of counterfactuals because there are more treatments observed and used in the analysis. For example, while a discrete quasi-experiment on retailer discounts would allow the researcher to compare the impact of a small set of retailer discounts on sales, a continuous instrument for the discounts might allow the researcher to compare a variety of smaller and larger discounts.Second, weak instruments are a challenge. Instrumental variables techniques are consistent but biased, and this bias can matter even in seemingly large samples ([92]). Weak instruments can lead to incorrect inference in which the bias of the weak instrument dominates the potential bias of the omitted variables.Knowing the context and the institutional setting can be invaluable in identifying strong IVs. For example, [76] derived their instruments for brand taste and price from the authors' intimate knowledge of the regulation and food industry. There are also recent advances in econometric methods that allow for more accurate presentation of statistical significance when instruments are weak ([68]). As [11] point out, many of the challenges of weak instruments are magnified when authors use multiple instruments to deal with multiple sources of endogeneity. By contrast, a focus on a single endogenous variable with a single source of endogenous variation has attractive statistical properties as well as being more transparent to the reader.Third, many researchers present IV results with different tests and with different norms. This makes it difficult to read and assess the validity of papers with instruments. Raw data exploration and analysis of Instrumental Variables[12], pp. 212–13) provide a sequence of steps to follow in an attempt to standardize practice. In presenting this list, we hope that it does not lead to unproductive dogmatism, and we emphasize that this is just one possible way to communicate the rationale behind a causal interpretation of the results. Still, we hope that in following these steps to the extent possible, marketing scholars can avoid being subject to many of the criticisms highlighted by [84]. The steps are as follows: Regress the outcome directly on the instrument. When using IV techniques, it is also desirable to show the reduced form result of regressing the outcome directly on the instrument. Because this is an ordinary least squares regression, it is unbiased. At the very least, the researcher should be confident that the instrument (  z  ) has the expected direct effect on the outcome (  y  ). Report the first stage. Assess whether the signs and magnitudes of the coefficients make sense. Report the F-statistic on the excluded instruments. This helps determine whether the instruments are weak. [92] advise that F-statistics below 10 in case of only one instrument suggest weak instruments, though, as [12], p. 213) note, ""Obviously this cannot be a theorem."" Similarly, [84] suggests reporting the first stage with and without the instruments to document the incremental impact of the instruments on the R-squared. If there are multiple instruments, report the first- and second-stage results for each instrument separately (at least in the appendix) because bias is less likely if there is only one instrument. Presenting the results separately also helps the reader understand the intuition behind the quasi-experiment underlying each instrument—whether the multiple instruments use different variation in increasing the exogenous shift in x. If there are multiple instruments, an overidentification test such as the Sargan–Hansen J can be performed to test whether all instruments are uncorrelated with the 2SLS residuals.[ 8] However, given the difficulty of identifying a robust instrument, it is unusual for researchers to have convincing cases for multiple instruments in a way that leads their regression to be overidentified. In other words, increasingly, standard practice is to focus on one instrument rather than many ([11]). Conduct a Hausman test comparing ordinary least squares and instrumental variables. If the results change, reflect on whether they change in a direction that makes sense given the power of the instrument. Do not interpret the results of the Hausman test to prove that the endogeneity problem is irrelevant. As noted by [84], the instrument may not be valid and therefore the test would be uninformative. Assess whether there is a weak instrument problem. For example, in a linear model, compare the 2SLS results with the limited information maximum likelihood results. When there is a weak instrument, the two-stage least square estimators are biased in small sample. Limited information maximum likelihood estimators have better small sample properties than 2SLS with weak instruments. If the two estimates are different, there may be a weak instrument problem. Any inconsistency from a small violation of the exclusion restriction gets magnified by weak instruments. Presentation of Results and Clustering of ErrorsRegardless of which regression analysis framework to employ, presentation of baseline estimates and standard errors, along with a set of robustness checks ([59]) is standard. This typically appears in the form of a regression table with several different specifications. For example, the first column might not include any controls beyond the fixed effects, and the next set of columns might add controls. The economic magnitude of the coefficients should be discussed, both with respect to changes in the covariate of interest and relative to the range and standard deviation of the covariate and dependent variable.A key issue in quasi-experimental analysis is correlated errors in observations, because the outcome is often observed at a finer level than the treatment. For example, the researcher might observe treatment and control groups for several advertising campaigns over a long time period. For each campaign, the researcher might have data on many individuals per campaign and many time periods per individual; however, the choices of the same individual in many time periods are likely to be correlated. [16] emphasized that failure to control for the correlation between these choices will lead to an overstatement of the effective degrees of freedom in the data, and therefore, standard errors will be biased downward. They suggest clustering standard errors by individual over time to address this issue and provide Monte Carlo evidence that clustering is likely to lead to robust inference.Similarly, [38] emphasize that if individual responses to the same treatment are likely to be correlated, for example, because of close physical or social proximity, clustering standard errors by groups of individuals is a conservative and useful way to estimate standard errors. Researchers often need to decide on the size of the clusters. For example, in studying ready-to-eat breakfast cereals, is the correct unit the company such as General Mills, the brand such as Cheerios, or the sub-brand such as Honey Nut Cheerios? The answer depends on the data and research question. If the data are at a lower unit level (e.g., individuals) than a treatment that takes place at the firm level, cluster the standard errors at the level of the treatment. A useful perspective on this is provided by [ 2], who remind researchers that the major driver for clustering should be the experimental design rather than simple expectations of correlation. More recently, there has been evidence suggesting that it is undesirable to cluster on the variable that determines whether that observation is subject to the regression discontinuity design (e.g., age). The answer is often instead simply to reduce the bandwidth across which the regression discontinuity is studied ([65]).Clustered standard errors rely on consistency arguments and large samples. With a small number of clusters, alternative methods are needed, such as those developed by [22], [30], and [53]. For example, [43] investigate consumers' dynamic responses to price promotions in a retail setting that involved randomly assigning ten supermarkets into varying promotion depths. Given that treatment takes place at the store level while the observation is at the consumer level, each consumer's effective contribution to reducing standard error estimates is likely to be lower than in a setting where there is no correlation across observations. However, given the relatively small number of stores/clusters available in this setting, the authors implement the wild bootstrap procedure, as proposed by [22], to correct for downward bias potentially induced in small samples. However, [24] show that even this approach requires rather large assumptions. Challenges to Research Design: What if Variation in x is not Exogenous?A more general point is that quasi-experiments range in how plausible the exogenous variation underlying the paper is, ranging from cases where the allocation is almost completely random to less clear cases where a firm or consumer assignment to treatment or control is partly random and partly an endogenous choice. Perhaps the ideal thought experiment here is [101], whose treatment and control were a pair of kidneys from the same person. [101] finds that in the United States, even identical kidneys from the same donor are received differently depending on the observed number of rejections preceding the recipient in the queue. Most research settings are less favorable. In such settings, it is often useful to combine different approaches in the same paper. For example, [79] combines a DID strategy with counterfeit entry as the treatment with a convincing and high-powered instrument on government regulation.Still, there will be situations where a compelling exclusion restriction is lacking or the treatment–control allocation appears far from random. If the treatment and control groups are substantially different in the pretreatment or if the treatment appears to be applied based on selected characteristics, the control group is unlikely to be a good proxy for the counterfactual, and the quasi-experiment may be less likely to be valid.We provide a discussion of three methods that are further steps researchers can take when comparability between the control and treatment groups is violated. They vary in terms of the observed and potentially unobserved differences between the control and treatment groups. Table 3 provides a summary of the frameworks and when to apply them. The table emphasizes that researchers should be cautious about applying matching methods or correction for selection bias on the grounds that there are no plausible exclusion restrictions, because these methods still require the researcher to make an argument about an exclusion restriction. The technical details of matching methods or selection bias correction are different from the three methods described previously, but the idea is similar in nature. The main goal is to bring in additional data to create control and treatment groups that are like those in quasi-experiment studies.GraphTable 3. Steps if Researchers Are Worried They Do Not Meet the Exclusion Restriction. Propensity Score MatchingSynthetic ControlSelection Bias CorrectionAssumptionsObservable control variables are capable of identifying the selection into treatment and control conditionsThe counterfactual outcome of the treatment units can be imputed in a linear combination of control units in the absence of treatment.The unobservables that enter the treatment selection and the outcome are jointly distributed as bivariate normal.IdentificationThe exclusion restriction can be met conditional on the variables in the match.The exclusion restriction can be met conditional on the pretreatment outcomes.There is at least one variable for which a compelling argument can be made for the exclusion restriction in the selection equation.SettingsWhen matching is done to control the treatment and control pretreatment outcomes on a number of cross-sectional covariates.When the focus is on the evolution of the outcome and the pretreatment time period has rich data on treatment and control groups.When the allocation to the treatment condition is not fully random.CaveatsAssess the degree of overlap after matching, and assess sensitivity to potential selection on unobservables. Still need to justify the exclusion restriction.Harder to interpret the weights used to create the ""synthetic control."" Still need to justify the exclusion restriction.Justification of why certain observables only affect treatment selection but not the outcome variable. Still need to justify the exclusion restriction.  Propensity Score MatchingMatching methods, pioneered by [83], have been developed such that the outcomes of the treated are contrasted only against the outcomes of comparable untreated units. Many published articles in marketing have used propensity score matching when comparability between the control and treatment groups is violated. An assumption of propensity score matching is that there are observable control variables capable of identifying the selection into treatment and control conditions. This is not a trivial assumption. It suggests that propensity score matching is only good if the exclusion restriction is met conditional on the variables in the match. Any matching procedure to make the control and treatment more similar in the observables can be seen as a flexible functional form with adding ""control variables"" to an analysis framework. Propensity score matching requires subject-matter knowledge regarding the role of covariates in the treatment assignment decision and whether the exclusion restriction is satisfied conditional on the covariates. Therefore, we caution against applying matching methods without convincing justification of exclusion restriction.It is difficult to identify a standard procedure for propensity score matching. We refer to [61] as a good starting point. The general objective of propensity score matching is to estimate a score such that the distribution of all the observed variables and behaviors among the treated units is similar to that among the control units. In this discussion, we consider the set of treated units to be fixed a priori. Four steps are involved in the propensity-score-matching procedure.First, choose a functional form of the propensity score. The basic strategy uses logistic regression to model the probability of receiving the treatment given a set of observables. Second, measure the distance and apply a matching algorithm. Several possible matching methods are available including, for example, nearest-neighbor matching based on the distance in the estimated propensity score or multiple matching using all controls within some distance from the treated unit. Third, assess the degree of overlap in the distribution of the linearized propensity score after matching. Researchers typically plot and compare the histogram-based estimate of the distribution of the linearized propensity score (logarithm odds ratio) for the treatment and control groups. To inspect the match quality, it is useful to show tables on the distribution of the estimated propensity scores and the mean values of some key variables for the treated and untreated over different propensity score intervals.[ 9] Fourth and finally, calculate the average treatment effect (ATE) with the matched sample using, for example, the DID regression analysis framework discussed previously.There are at least two caveats regarding propensity score matching. First, the model for the propensity score may be misspecified. In that case, the balance in covariates conditional on the estimated propensity score may not hold, and the credibility of subsequent inferences may be compromised. This calls for a careful discussion on the role of covariates in the treatment assignment decision. Specifically, it is important to provide a discussion of whether the covariates can be considered exogenous to the treatment. Second, regardless of the number of observed covariates used, propensity score matching does not account for the potential selection on unobservables in treatment assignment. It is important to explain why controlling for observables will address concerns with the exclusion restriction or why unobservables are not an issue in treatment assignment. Synthetic Control MethodsIn some cases, even the closest match may not be close enough. This is particularly relevant when researchers are interested in how an event, regulatory intervention, or firm policy change affects the evolution of the outcome of interest, in contexts where only a modest number of treated units (possibly only a single one) and control units are observed for a large number of periods before and after the event. Two aspects make this setting different from the typical use of the propensity-score-matching method. First, matching is done over the pretreatment outcomes in each period rather than a number of covariates. Second, the number of control units and the number of pretreatment periods can be of similar magnitude. Synthetic controls use a different convex combination of the available control units ([ 3]; [ 4]; [39]). The intuition behind this method is that the created synthetic control unit closely represents the treated unit in all the pretreatment periods and affords time-varying causal inference on the trajectory of the outcome of interest.Synthetic control has been used in multiple recent studies with quasi-experimental design ([ 1]). For example, [51] analyze the causal effect of industry payment disclosure on physician prescription behavior, [99] assess the impact of mobile hailing technology adoption on drivers' hourly earnings, and [78] study the causal effect of online paywalls on the sales revenues of newspapers.Like propensity score matching, synthetic control methods are statistically rich, but they do not replace a carefully thought-out exclusion restriction and identification argument. Put differently, if propensity scores or synthetic controls appear to work when the treatment and control group are not similar, it is important to explain why controlling for observables will address issues with the exclusion restriction. In many cases, such explanations are weak and the exclusion restriction is unlikely to hold. Recent work in economics emphasizes this by showing the benefits of combining a synthetic control method with a strong exclusion restriction ([13]). Selection Bias Correction MethodMany papers written in marketing involve a comparison of potentially different groups that reflect endogenous choices by companies or consumers where the allocation to the treatment condition is not fully random. For example, [46] assess if the introduction of the free mobile app in a business-to-business context increases sales revenues from buyers who adopted the app. In an ideal setting, the company could randomize the treatment, then observe sales from buyers who did not get the app and sales from buyers who did get it. However, this company's app was available to all buyers. Therefore, the buyers' app adoption is not random, and self-selection into the treatment (adoption) group needs to be addressed. Omitted variables that drive strategic app adoption could correlate with the sales from these buyers.When this happens, it is sometimes useful to estimate a Heckman selection model ([57]), which explicitly models selection into the treatment as a two-step process. As [100], p. 564) pointed out, the exclusion criterion is still key to the identification of the treatment effect of interest in the two-step estimation procedure. Without the exclusion criterion, the effect of the treatment is identified only due to the nonlinearity in the functional form (specifically through the inverse Mills ratio). This may lead to severe collinearity and imprecision in the standard errors. More importantly, without a strong and credible exclusion restriction, identification in this setting is driven by the assumed functional form.In other words, although the Heckman correction will provide an estimate without an exclusion restriction, that estimate depends entirely on the assumption that the error structure is bivariate normal. When there is an argument for the exclusion restriction, a selection model is helpful. In the absence of the exclusion restriction, even if combined with other techniques such as propensity score matching, the results would be identified off the functional form assumption alone. Put differently, if one of the covariates in the correction equation satisfies the exclusion restriction, then it is the variation in that variable that identifies the control for selection. In contrast, if the covariates in the first step are all also in the second step, then it is only the assumed error structure that identifies the control for selection.There are both similarities and differences between selection bias correction and instrumental variable approaches. There are also similarities with the control function approach in terms of the importance of functional form assumptions on the errors in the absence of an exclusion restriction. Control functions are not part of the standard quasi-experimental toolkit, so we do not provide a detailed discussion. The selection bias correction approach uses the instrument to control for the effect of unobservables, while the instrumental variable approach attempts to eliminate the threat of endogeneity by only leveraging the useful variation created by the instrument. Yet, the two approaches share the basic idea of using an exclusion criterion (or instrument). Ultimately, both rely on the ability to find an exclusion restriction that creates useful and exogenous variation. This is why we emphasize the importance of identification in quasi-experiments and caution against blindly applying a correction for selection bias without carefully thinking about the identification assumption and providing a justification for why the exclusion restriction holds. Selection bias correction approaches are therefore only useful for causal inference in the presence of a strong credible exclusion restriction. Robustness: How Robust is the Effect of x on y ?The specific robustness checks chosen will depend on the exact context. With electronic appendices and increasingly cheap computation, it is possible to show robustness to a large number of alternative specifications. Here, empirical work with quasi-experimental methods differs substantially from research using forecasting models. The aim is not to show one specification (or model) and defend it. Instead, the idea is to show that the sign, significance, and magnitude of the estimate of  β  remain broadly consistent across a vast range of possible models ([59]). Often these robustness checks are dropped from the published version of the article, though they are very useful in the referee process and can end up as part of an online appendix. The following subsections describe some examples of useful robustness checks. Different ControlsCompare the coefficient of interest in the models with and without controls. For example, if the coefficient changes from 2.5 to 3.5, then this change (+1.0 in this example) is informative about how big the impact of the omitted variables has to be relative to the observed controls for the omitted variables to drive the result. [ 7] provide a method to examine how much the effect of interest changes as controls are added, and then to assess how important the omitted variables would have to be for the treatment effect to disappear. The method is based on Rosenbaum bounds ([37]; [82]). It has been applied in the marketing literature by [73] and extended by [90]. Although the formal method is useful, as discussed in [77], many researchers ([ 9]; [74]) use the more basic insight that there is information in the impact of the controls on the measured effect of interest. This does not mean that results are invalid if the controls do change the estimated effect substantially, but documenting that adding seemingly relevant controls does not change the results can provide further support for the causal interpretation. Different Functional FormsResults should not depend on arbitrary choices of functional form. For example, if using a linear probability model, show robustness to logit and probit. The choice between linear probability models and nonlinear models such as logit is widely debated. [12] argue for linear probability models because they are simple to interpret and consistent under a basic set of assumptions. Others argue against them because they are inefficient (and inconsistent if the assumptions are violated). In cases like this, where the literature does not give clear guidance on the choice of model, showing robustness to different choices is optimal. Different Choices of the Time Period Under StudyResearchers often can choose when to start and end the sample. For example, for a treatment that occurs in 2004, researchers should be comfortable that the results are robust to the arbitrary choice of whether the period studied is 2002 to 2006, 2000 to 2008, 1995 to 2015, and so on. Different Dependent VariablesThere might be several different dependent variables that relate to the outcome of interest. Showing robustness to these related outcomes increases confidence in results. Different Choices of the Size of the Control GroupResearchers choose whether all the data should be used in the control group, or only a subset of the data that is ""close"" to the treatment group (e.g., as measured by a propensity score). Researchers can also choose how to define the treatment group. Placebo TestsThe idea of a placebo test is to repeat your analysis using a different part of the data set where no intervention occurred. For example, if the quasi-experimental shock happens this year, instead of comparing the difference in the outcome between last year and this year between the control and treatment groups, you can conduct a placebo test by redoing the analysis and compare the difference in the outcome between the control and treatment groups using periods with no intervention shocks. Alternatively, analysis can be conducted on an outcome that should be unrelated to the intervention being studied. The goal is to establish a null effect when there is not supposed to be one.It is unlikely that every robustness check will yield the same level of significance or the same-sized point estimate as the initial specification. Researchers (and reviewers) should therefore not expect every specification to yield the exact same results. The key is to communicate when the results hold up. This will consequently help inform the reader what drives the statistical power behind the results.Broadly, quasi-experimental research aspires to identify effects that do not rely on the underlying assumptions outside of the experimental variation. There are many places where that can break down, including functional form assumptions, external validity, and various confounding effects. The focus is on a robust single causal relationship. Mechanism: Why Does x Cause y to Change?The most effective papers typically do not stop with identifying a causal effect and its magnitude. After identifying a likely causal relationship, it is important to assess why x causes y to shift. Understanding mechanisms is often a key goal of social science. There are at least three benefits of establishing mechanisms. First, it provides a rationale for why the effect should exist in the first place. It requires the authors to think about the theoretical contribution of their research more carefully and helps make the argument for causal identification more convincing. Second, identifying mechanisms can help evaluate the benefits and negative consequences of the intervention and identify avenues for course correction, if needed. Third, understanding mechanisms allows for the possibility to extrapolate the findings to other contexts. Research needs to provide guidance on when and why the causal relationship is relevant. Assessing the Mechanism Through Mediation AnalysisWhen the data afford a direct measure of mediator variables, mechanisms can be inferred by mediator analysis. To illustrate how quasi-experiments can show process through mediation, we use [52] as an example. They investigate whether a variable compensation scheme increases salespeople's stress, resulting in emotional exhaustion and more sick days, and counteracts the sales benefits companies might expect from variable compensation schemes. In one of their empirical analyses, they use a natural experiment where a company dropped the variable compensation share from 80% to 20% in one of its business units. To test the health state as a possible mediator variable, they were able to measure sick days both before and after the change in the variable compensation share. In the country of study, sick days are strictly regulated by law and require certification by a physician (at the latest on the third day of the leave). Those who take more than three sick days in a given month are more likely to have substantial health problems. They measure the sick days counting after the third sick day in a month.Combining the DID analysis with mediator analysis, [52] show that the direct effect of the treatment (drop in variable compensation share) on sales performance is significant and negative, and that the indirect effect of the treatment on sales performance via sick days is positive and significant. The mediator analysis suggests that a higher variable compensation share is associated with enhanced sales performance but also with more sick days, which, in turn, reduce the gains to sales performance. Assessing the Mechanism Through Moderation AnalysisHeterogeneous treatment effects can be used to test behavioral mechanisms. In a quasi-experimental setting, mechanism checks via heterogeneous treatment effects, sometimes referred to as falsification checks, are not simply equal to identifying moderators. They involve identifying which groups would be affected by a certain mechanism that would display the causal effect of interest, and which other groups would not display the causal effect of interest by the proposed mechanism.Moderation analysis therefore serves a broader purpose by providing an opportunity to help explore the behavioral mechanism. If the effect goes away when theory suggests it should, then this helps identify why it happens. If the effect is larger when theory suggests it should be, then this also helps identify the mechanism. A simple approach is to estimate the effect separately by whether an individual is a member of a group that theory suggests should experience a bigger effect. Formal testing of whether the difference is statistically significant requires a three-way interaction between x, the source of variation, and group membership.There are many relevant examples in marketing of the use of moderation analyses to demonstrate a mechanism if there is a reason to believe the boundary of underlying process exists or the magnitude of the treatment effect varies by some observables. For example, after showing the European privacy regulation hurt online advertising, [47] ran a falsification check demonstrating that European consumers behaved like Americans when visiting American websites and that American consumers behaved like Europeans when visiting European websites. The paper then explored the mechanism and showed that the regulation especially hurt unobtrusive advertising and advertising on general interest websites, two situations where using data to target advertising is particularly valuable.Overall, mechanism checks through mediator or moderation analyses are important because they distinguish the goal of the marketing scholar from the marketing practitioner. Marketing practitioners run experiments and analyze data to understand what they should do in the particular situation they are facing. Marketing scholars need to have a broader sense of applicability beyond the specific setting being studied. Mediation and moderation analyses provide an understanding of when a marketing action will and will not lead to the desired behavior. For this reason, marketing papers are more likely to be remembered for the evidence that is shown in support of a theory explaining why the result holds. External Validity: How Generalizable is the Effect of x on y ?The external validity discussion in a paper should recognize the assumptions required for the analysis to capture the ATE across the population of interest, rather than a more local effect that is an artifact of the data sample or the source of quasi-experimental variation. A key concept is the ATE across the entire population. This is the difference in outcomes that would occur by moving the entire population from the control group to the treatment group. However, in some cases, the ATE may not be particularly relevant, because it averages across the entire population and includes units that would never be eligible for treatment ([100], p. 604). For example, we would not want to include millionaires in computing the ATE of a job training program. To address this, the researcher could use the average treatment effect on the treated, which measures the expected effect of treatment for those who actually were in the treatment condition.One reason why a research setting may fail to be externally valid is if the treated population is unrepresentative ([72]). A concern that will drive whether the treated population is unrepresentative is whether those affected could self-select into and out of the treatment. For example, [28] study a rule change by Google that allowed non–trademark holders to use trademarks in search advertising copy. They study the rule change's effect on user click behavior. In this case, many advertisers did not alter their advertising copy strategy, for a variety of reasons. These advertisers may be systematically different from the advertisers that did change their strategy. Because these advertisers were not forced to change their strategy, we will never know what would have happened if they did. When faced with such issues, it is best to spell out the potential for self-selection and discuss whether it makes the paper more or less relevant. In this case, it would be accurate to say that the researchers captured the effect of a loosening of trademark restrictions, because it is unlikely that a search engine would force its advertisers into using other advertisers' trademarks. However, it would not be accurate to claim that the researchers capture the broader effect of all advertisers using other advertisers' trademarks in their copy.The treated population may also be unrepresentative if the treatment impacts a subpopulation to change behavior, but not the main population of interest. This means that the measured effect is localized to that subpopulation, and it is referred to in the literature as the local average treatment effect (LATE). For example, in the context of regression discontinuity, the LATE is the average of the treatment effect over the individuals who would have been in the counterfactual condition if the discontinuity threshold were changed. A limitation of regression discontinuity is that the results directly apply only to populations around the threshold. For example, comparing the $49 spend with the $51 spend may be informative about the impact of the marketing incentive on consumers who spend around $50; however, consumers who typically spend a lot more or a lot less might be different. The idea of LATE also has implications for the interpretation of instrumental variables estimates, as any IV estimate is the LATE for the observations in the regression who experienced the kind of variation exploited by the instrument.[10]More broadly, as with other aspects of quasi-experimental research, the best practice regarding the external validity of results is to clearly lay out the assumptions and limitations. For example, [94] use a quasi-experiment and DID to examine the impact of advertising revenue on the type of content posted on Chinese blogs. While it might be tempting to interpret the results as suggestive of a broader impact of commercial interests on media, they are careful to emphasize the many differences between blogs and other media, between China and the rest of the world, and between the way the bloggers were compensated and other online advertising models. In this way, Sun and Zhu's article explicitly limits the temptation of the reader to extrapolate too much.An internally valid quasi-experimental estimate can have broader external validity when used to identify relationships such as elasticities and then to use a structural model to identify the counterfactual of interest. In these cases, under the assumption that the model is a useful representation of reality, quasi-experimental methods serve as a complement for, rather than a substitute to, structure. For example, [ 9] use quasi-experimental methods to identify the impact of the automotive brand preferences of parents on the brand preferences of their children. They then use structural methods to estimate the implications for firm strategy. [42] use quasi-experimental variation in health insurance prices to identify price elasticity and then combine this measure with a structural model to estimate the welfare implications of adverse selection. [29] use quasi-experimental variation around set quotas to identify the relationship between commissions and sales, and then use this variation in a structural model to determine optimal compensation schemes.Overall, effective quasi-experimental research requires an understanding of the underlying assumptions behind any broad interpretation of quasi-experimental results. Quasi-experiments often require a focus on a narrow slice of the data, and therefore, it is important to consider the degree to which the results apply to a broader population. Apologies: What Remains Unproven and What Are the Caveats?Any identification strategy relies on a set of assumptions. These assumptions need to be explicit throughout the paper. There are always some tests that cannot be run, for example, due to lack of data. There are always some robustness checks that are weaker than others. There are always some steps from data to interpretation. While apologies do not mean all is forgiven, the objective should be to clarify the boundaries of the claims. Obfuscation is much worse than a clear summary of the identifying assumptions.As an example, [51] employ a DID research design to study the effect of the payment disclosure law introduced in Massachusetts in June 2009. The research design uses the setting that physicians located in the border counties of Massachusetts and its neighboring states did not have disclosure laws during this period. They lay out the assumptions underlying their estimation:Our identification of the effect of disclosure legislation relies on the change in new prescriptions by physicians located in Massachusetts (MA) after the policy intervention, relative to their counterparts from ""control"" states in which no such law existed in the same period.... To assess potential threats to the validity of our research design, we verify if the result was driven by changes in physician payments as a result of the MA disclosure law. If such payment changes were primarily driven by local pharmaceutical reps reallocating their marketing budgets across physicians operating on either side of state borders, this would render the border identification strategy problematic.([51], p. 517)This example communicates three distinct points. First, it explains the identification strategy. Second, it details the main threats to the validity of this identification strategy. Third, it describes what they do to address it. These points suggest that effective apologies focus on demonstrating what interpretations are reasonable, and what might be a stretch of the results. The goal is not to show that in all circumstances and every conceivable way the identification is perfect. That is not possible. Instead, the goal is to provide clear bounds on the interpretation. The paper's contribution is then a function of whether it provides new knowledge under this bounded interpretation. ConclusionQuasi-experimental techniques are an important tool for marketers. First, marketing scholars need to be able to inform marketing practitioners—both managers and policy makers—about the causal effect to allow practitioners to make superior decisions. Second, the best quasi-experimental papers do not simply prove a causal effect but delve into the underlying mechanism, which is key to marketing scholarship's goal of generalizability. Third, such techniques become more important as the scope and span of marketing practice expands and there are new settings and more varied sources of data that allow their application.The objective of a quasi-experimental research paper is to answer an interesting and important research question about a causal relationship and provide evidence suggesting the mechanism behind the relationship. The choice of method (DID, regression discontinuity, or instrumental variables) depends on the nature of the quasi-experiment. The framework we present focuses on understanding how exogenous variation helps uncover causal relationships and why specific actions affect behavior. Of course the details of the methods will evolve over time as new research appears. Because marketing scholars are often interested in providing generalizable insights about how marketing actions change the behavior of individual consumers, the quasi-experimental framework is particularly useful. Similarly, firms that want to use those insights benefit. As the availability of detailed data grows and marketing technology changes, these methods will enable marketing scholars to provide assessments of a wide variety of situations in which a particular marketing action is likely to change consumer behavior or market dynamics.  "
12,"Connecting to Place, People, and Past: How Products Make Us Feel Grounded Consumption can provide a feeling of groundedness or being emotionally rooted. This can occur when products connect consumers to their physical (place), social (people), and historic (past) environment. The authors introduce the concept of groundedness to the literature and show that it increases consumer choice; happiness; and feelings of safety, strength, and stability. Following these consequential outcomes, the authors demonstrate how marketers can provide consumers with a feeling of groundedness through product designs, distribution channels, and marketing communications. They also show how marketers might segment the market using observable proxies for consumers' need for groundedness, such as high computer use, high socioeconomic status, or life changes brought on by the COVID-19 pandemic. Taken together, the findings show that groundedness is a powerful concept providing a comprehensive explanation for a variety of consumer trends, including the popularity of local, artisanal, and nostalgic products. It seems that in times of digitization, urbanization, and global challenges, the need to feel grounded has become particularly acute.Keywords: connectedness; alienation; need to belong; groundedness; local; rootedness; terroir; traditionalTo be rooted is perhaps the most important and least recognized need of the human soul.—[47], p. 43).Dual forces of digitization and globalization have made our social and work lives become increasingly virtual, fast-paced, and mobile, leaving many consumers feeling like trees with weak roots, at risk of being torn from the earth. In response, we observe consumers trying to (re)connect to place, people, and past—to get anchored. Against this backdrop, we propose and test an important driver of consumer behavior that has largely been overlooked in marketing literature: the feeling of groundedness.We believe that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. This feeling emanates from connections to one's physical, social, and historic environment and provides a sense of strength, safety, and stability. Although the concept has received scant attention in prior marketing, consumer behavior, and social psychology research, the feeling of groundedness appears to be a familiar one among lay consumers. For example, we might feel grounded when returning to our birthplace, sitting at our grandparents' kitchen table while enjoying a pie made with apples from their backyard tree and according to a recipe passed down for generations. Similarly, we may have experienced feeling grounded when shopping at the local farmers market or foraging a basket of mushrooms from a nearby forest.We argue that there are at least three conceptually separable (but in practice often intertwined) sources of feelings of groundedness: connectedness to place, people, and/or past. Collectively, connections to place, people, and past engender feelings of groundedness by ""rooting"" us in our physical, social, and historic sphere. These connections may be established through many different objects, activities, and types of interactions. In this article, we focus on the role of products in providing customers with a connection to place, people, and past.Indeed, numerous marketplace examples illustrate increasing consumer demand for products that presumably make them feel more connected and thus grounded: Spearheading a renaissance of artisan, indie, and craft production, for example, locally rooted (micro)breweries have gained substantial market share in recent years. In 2019, craft beer accounted for 13.6% of total beer volume sales—a number that had increased by 4% even as overall U.S. beer volume sales had decreased by 2% ([ 6]). Similarly, sales estimates of local food increased from US$6.1 billion in 2012 to US$8.7 billion in 2015 ([24]; [45]) and farmers markets—which afford a connection to the land and to the people behind the food—are on the rise. In 2014, there were 8,268 farmers markets across the United States: a growth of 180% since 2006 ([24]). Beyond the food industry, online marketplaces such as Etsy connect consumers to handcrafted products and to the craftspeople that sell them. Impressively, Etsy reported 81.9 million users and US$10.3 billion gross merchandise sales worldwide in 2020 ([10]).This trend in demand for local, personal, and traditional products is surprising when considered against the backdrop of globalization, digitization, and modern society's penchant for technology and innovation. Marketers have begun to capitalize on these shifts in demand—for example, by stocking and promoting local products, encouraging contact with the people who make the products, and highlighting traditional ingredients or production methods. We have recently also observed marketers referring to the concept of groundedness. The Austrian grocery chain BILLA ran a national advertising campaign in fall 2020 referring to the farmers behind their products as ""The people who make us grounded"" (""Wer uns erdet"").In light of these trends, we contend that products can metaphorically connect us to place, people, and past, and thereby make us feel grounded. For brevity, we hereinafter refer to products that can make consumers feel grounded as ""grounding products."" We argue that the ability of products to provide a feeling of groundedness will make them more attractive to consumers. We further propose that feeling grounded may contribute to consumer well-being. Groundedness—understood as a feeling of deep-rootedness, having a strong foundation, and being securely anchored—gives consumers feelings of safety, strength, and stability as well as confidence that they can withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their overall well-being.This work makes several contributions. First, it introduces the feeling of groundedness as a driver of consumer behavior and consumer welfare. Second, it provides an overarching theoretical explanation for a variety of major consumer trends, such as the desire for local, craft, and traditional products. Third, it highlights that consumers experience a feeling of groundedness when products connect consumers to their physical (place), social (people), and historic (past) environment. Fourth, the studies offer various actionable marketing implications for products aimed at helping consumers connect to place, people, and past. Groundedness Groundedness in the LiteratureAs a personal characteristic, to be ""grounded"" is a common concept in everyday parlance, easily found in any dictionary. In contrast to everyday parlance, we found groundedness to be a fairly novel and underresearched construct in the literature. There are few direct references to groundedness in the marketing, consumer behavior, or social psychology literature streams. The mentions we did find in other literature (e.g., psychotherapy, environmental or educational psychology) are relatively obscure, only loosely related, or speculative (for an overview of relevant research, see Web Appendix A). For example, educational psychologist [29] writes about ""rootedness"" and develops a measure of rootedness for college students. However, McAndrew's explanation of rootedness is limited to location. Similarly, environmental psychologists (e.g., [27]; [33]) have studied connectedness to nature, which is also a more limited construct. We found a more closely related conception of groundedness in a psychotherapy doctoral dissertation, where [31], pp. 82–83) describes rootedness in terms of ""the personal, social, environmental, and economic anchoring that sees us through tough times. Within rootedness, there is a sense of togetherness, a combination of personal identity and group identity, past and present, and people and places.""In philosophy, [47], p. 43) points to the importance of being rooted. She notes:A human being has roots by virtue of his real, active, and natural participation in the life of a community, which preserves in living shape certain particular treasures of the past and certain particular expectations of the future. This participation is a natural one, in the sense that it is automatically brought about by place, conditions of birth, profession, and social surroundings.[12] likewise writes about rootedness in terms of the need to establish roots and feel at home in the world, while [41] refers to a connection to the land as a source of well-being that is undermined by technological forces that separate people from their roots in nature.In marketing, [42] examine rootedness in the context of community-supported agriculture (CSA), arguing that by connecting consumers to the land and producers, CSA membership may help consumers reconnect to their ""material, historical, and spiritual roots"" (p. 141). [ 2] also touch on some of the elements, antecedents, and consequences of groundedness, such as community and traditions.In summary, we believe the idea of groundedness has not been formally developed as a concept, nor have the full scope of the construct and its implications for consumer behavior and marketing been identified. We aim to fill this gap in the literature. The Construct of GroundednessWe argue that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. The feeling of groundedness results from being metaphorically embedded in one's physical, social, and historical environment. Like the roots of a tree or the foundation of a house, a feeling of groundedness connects a person to their ""terroir"" (where the French word terroir not only refers to the land per se but also includes its cultural history and human capital [[35]]). Consistent with relevant dictionary definitions—which include being mentally and emotionally stable or firmly established[ 5]—we argue that the feeling of groundedness provides a solid foundation that imparts a sense of strength, safety, stability, and confidence that one can withstand adversity. Connection to placeConsistent with the idea of ""spreading one's roots into the ground,"" and the literal translation of terroir as ""land"" or ""soil"" ([35]), the feeling of groundedness can be obtained from a connection to a physical environment or place. This connection can be physical in the literal sense, as when working with actual, tangible objects that originate in the local environment, or when immersing in the natural environment itself. We find examples of such immersion in, and connection to, the natural sphere in the East Asian tradition of shinrin yoku, or forest bathing ([19]), and the Nordic cultures' idea of outdoor life (Friluftsliv), which, according to [15], p. 3), provides ""a biological, social, aesthetic, spiritual and philosophical experience of closeness to a place, the landscape, and the more-than-human world; an experience most urban people today lack."" In the same vein, connection to place may be experienced when directly drawing from the earth, as popularly pursued in urban gardening and farming. Indeed, one of [42], pp. 140–41) informants states, ""That's what farming actually is [a connection to the earth].... You are working with the living world. It's the connection you give people to the farm."" In addition to a physical connection, consumers can also connect to place in a more symbolic sense. They may do so, for example, by consuming locally produced goods, such as a beer from a nearby brewery. Establishing a connection to one's place to feel grounded may have become especially important as a consequence of migration and mobility. For example, a consumer who has recently been relocated to a certain town may particularly desire to consume products local to that town, thus enabling them to build a connection to that place. Connection to peopleFeelings of groundedness can also arise from a connection to one's social environment. Just as the meaning of terroir also includes its human capital ([35]), the idea of a ""place"" that provides groundedness, such as home, is often strongly shaped by the people and community associated with that place.In the social psychology literature, the human need for connectedness or belongingness to other people ([ 4]) has been well established. Running counter to that need is the phenomenon of modern-day alienation ([26]). The concept has been revived by marketing scholars to describe alienation of the consumer from the marketplace ([ 1]), and from a product's producer ([46]). Along the same lines, [ 2] observe postmodern consumers' feelings of personal meaninglessness and loss of moorings brought on by globalization and technology, while stressing the importance of identity, home, and community as antidotes to these feelings.Although the strongest route to groundedness via people might be connecting to one's closest social surroundings (e.g., one's family), we also see customers trying to reestablish a connection to people by means of certain product choices. Both online and offline, consumers may obtain groundedness by buying directly from the producer. At a farmers market, consumers may buy eggs directly from the person who fed the chickens and collected their eggs. On Etsy, online shoppers can order a breakfast mug from the very person who designed and shaped the piece with their own hands; the shopper might even be able to communicate directly with that person and learn how they developed their passion for handicraft. Either way, this enables the customer to get ""closer to the creator"" ([39]). On the business side, many firms, big and small, try to facilitate connections between customers and the people behind their products: for example, featuring individual producers on the packaging, indicating the name and address of food suppliers, or communicating via the company's founder or chief executive officer ([14]). Connection to pastThe human environment, or terroir, also includes a historical dimension ([35]). We suggest that feelings of groundedness can also be experienced based on a connection to the past. The past provides a foundation of memories, traditions, and cultural values for individuals to be grounded in.Examples from the marketing literature illustrate how consumption behavior establishes a connection to the past and begets feelings of groundedness. In [42], some respondent quotes suggest that community-supported farms provide not merely a connection with their local physical environment and the people around them but also a symbolic connection to past generations within one's own family (e.g., a connection to ancestors who were farmers). [44], who investigated Nordic consumers' food consumption motives, state that ""in the end, it is the caring food-producer who can bring the ubiquitous brand consumption back to where we were before industrialism"" (p. 230). Similarly, [ 3] find that visits to local farmers markets allow consumers to ""reconnect with their agrarian roots"" (p. 567), searching for ""food that is embedded in their personal and shared social histories"" (p. 564). In the consumer product domain, we see a resurgence of historic brands such as Converse ([23]) and observe companies helping consumers get connected to, or grounded in, the past. For example, firms may purposefully manufacture according to traditional and artisanal methods, such as making things by hand ([13]), or return to using older, often more ""natural"" materials and ingredients.Building on this conceptualization, our first prediction is as follows: H1:  Products that connect consumers to place, people, and past provide consumers with the feeling of groundedness. How Groundedness Is Distinct from Related ConstructsProducts that connect consumers to place, people, and past frequently differ from other products in more aspects than their affordance of feelings of groundedness. For example, a local, traditional product is probably also more authentic ([32]; [34]). Likewise, products that connect to place, people, and past could be deemed higher quality or costlier to produce. They may be more unique ([25]), or perceived as made with love ([13]). Consumers may feel a stronger brand attachment to such products ([43]). These products may also provide a greater sense of human contact ([38]), brand experience ([ 5]), brand community (e.g., [28]), and sense of nostalgia ([ 9]). Products that provide a feeling of groundedness may also evoke a feeling of being true to oneself (i.e., self-authenticity or existential authenticity; e.g., [ 2]; [16]), a feeling of knowing who one is (self-identity), a general sense of belonging ([ 4]) that is not about feeling grounded and deep-rooted, or a general sense of meaning in life ([21]; [37]; [40])—all of which could increase one's well-being.While these related constructs are relevant, we argue that they play different conceptual roles than groundedness. First, some constructs—such as product authenticity, product quality, or product uniqueness—are characteristics of products. They logically cannot cast doubt about the existence of groundedness, which is a feeling about the self.Second, other alternative constructs could be classified not as characteristics of brands but as feelings about brands. For example, brand attachment is a feeling of connection to a brand. In some situations, feeling connected to a brand might be a consequence of a brand's relationship to a place, people, or the past that a consumer longs to feel a connection with. For example, a consumer may be more likely to feel attached to a wine brand from their own region (or to their favorite laptop brand, which may have nothing to do with feeling connected to place, people, or past). However, this feeling of brand attachment is not a feeling about the self. Thus, it cannot be the same as the feeling of groundedness.A third category of constructs relates to connectedness but is focused on only one of the three sources. For example, nostalgia, as ""a sentimental longing or wistful affection for a period in the past,""[ 6] is related to the past but not necessarily people or place. Likewise, these constructs might be alternative explanations for one of the antecedents of groundedness but not groundedness itself. In addition, nostalgia describes a state of longing or affection, but it does not stipulate that this longing has been satisfied by an actual connection to the past. Thus, nostalgia is conceptually more closely related to the need for groundedness than to actually feeling grounded.Finally, there are some constructs involving feelings about the self that might be driven by similar antecedents or generate similar consequences as the feeling of groundedness; these include feeling true to oneself (i.e., self-authenticity), a sense of belonging that does not involve a feeling of deep-rootedness, a sense of self-identity, and a general sense of meaning in life. Our studies will assess these alternative constructs to groundedness. FrameworkFigure 1 depicts our conceptual framework. At the core of this framework and as summarized in H1 is that there are at least three immediate sources of groundedness: connection to the physical environment, or to place; connection to the social environment, or to people; and connection to the historical environment, or to the past.Figure 1 further depicts our hypotheses about the consequences of the feeling of groundedness; in particular, we consider product attractiveness (H2) and consumer well-being (H3) as important outcome variables. We then examine ways in which marketers can leverage groundedness on the basis of marketing-mix elements (H4) and consumer characteristics (H5).Graph: Figure 1. Conceptual framework. How Groundedness Affects Consumer ChoiceIn our predictions about downstream effects of groundedness, we hypothesize that groundedness increases product attractiveness and, thus, affects consumer choice. In particular, we suggest that products providing a connection to place, people, and past beget feelings of groundedness for the customer and may therefore be more attractive than their competitors that do not. We thus predict that customers will prefer these products and have stronger intent to purchase and higher willingness to pay (WTP). More formally, H2:  Products' ability to provide consumers with the feeling of groundedness makes those products more attractive to consumers. How Groundedness Affects Consumer Well-BeingBeyond marketplace outcomes, we hypothesize in our predictions that groundedness increases consumer well-being. In particular, we suggest that feeling grounded provides consumers with a sense of strength, stability, safety, and confidence in one's ability to withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their well-being. We find conceptual support for these predictions in the descriptions of [31] and [42]. [31], p. 82) refers to rootedness as providing ""a sense of balance, belonging, and fitting to one's place."" Further specifying the elements of well-being afforded by groundedness, Ndi (p. 59) says that rootedness is ""the ultimate feeling that provides stability, harmony, and happiness among people and their community,"" whereas a lack of rootedness leaves a person with a sense of meaninglessness, disconnectedness, emptiness, vulnerability, and unhappiness. Building on [41] work in biodynamics, [42], p. 140) also suggest that emotional connections to one's environment ""are a primordial source of spiritual sustenance and a foundation of social and personal well-being and, conversely, that psychological and societal unrest are precipitated by technological forces that separate humanity from its roots in nature."" Research on constructs related to groundedness also provides indirect, suggestive evidence for our proposition that groundedness increases consumers' well-being. [27], for example, find that connectedness to nature is positively correlated with subjective well-being. We predict the following: H3:  The feeling of groundedness increases consumers' subjective well-being. How Marketers Can Leverage Groundedness Marketing-mix strategiesMarketers can use several marketing-mix variables that help connect consumers to place, people, and past and thus make them feel grounded. Marketers can promote the location where the product is made or ingredients are sourced, engage in storytelling about the history of the brand, or introduce the people who produce the products ([14]; [46]). Marketers can design products in a local or traditional style; use local, ethnic, or traditional ingredients; or employ traditional production processes (e.g., in ""indie"" products). Marketers can also adjust their channels of distribution to help customers connect to place, people, and past. For example, farms and small producers can use farmers markets (vs. supermarkets) that connect consumers with place, people, and past. Retailers can employ traditional store designs or focus their assortments on more traditional products. We propose the following: H4:  Marketing-mix variables such as communication, product design, and channels of distribution can be designed to increase the feeling of groundedness. Consumer segmentation strategiesWe expect that consumers differ in how important feelings of groundedness are to them. That is, the level of need for connection with place, people, and past, and thus, for groundedness, varies across consumers. We examine three reasons why the need for groundedness might be heightened in certain consumer segments. First, the need for groundedness should be particularly strong when consumers' life and work make it difficult to establish and maintain strong connections with place, people, and past. We suggest that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. With regard to work, we expect that performing mostly computerized work, confined to the limits of one's desktop, puts a distance between individuals and other people as well as the physical environment. We consequently argue that computerized work is associated with a stronger need for groundedness.Second, we propose that the need for groundedness is stronger when consumers' foundations are shaken or connections with place, people, and past are severed or under pressure. We expect this to have been the case, for example, during the COVID-19 pandemic, a global event that indeed disrupted many people's lives. Accordingly, those who the pandemic had more strongly put in a state of flux should have experienced a higher need for groundedness.Third, we suggest that the need for groundedness will be more prominent for consumers whose more basic needs are satisfied. Respective proxies such as consumers' socioeconomic status (SES) should thus be correlated with their felt need for groundedness. We predict the following: H5:  The feeling of groundedness is more important to consumers when their work and life do not provide a strong connection to place, people, and past; when life events shake their foundation; or when their basic needs are already sufficiently met. Overview of StudiesWith a view to robustness and generalizability, we test our predictions in eight experiments and one consumer survey, based on a variety of samples and data collection techniques (students in behavioral labs at universities, online platforms, and professional market research panels, both in the United States and in Europe). For managerial usability, our study paradigms include both consequential outcome measures as well as marketing-relevant factors that can be manipulated or measured. Study 1 provides evidence that groundedness increases product attractiveness in real economic terms using an incentive-compatible measure of WTP. Studies 2a–c show that groundedness has explanatory value above and beyond alternative constructs. These studies also explore how a product's affordance of groundedness depends on the closeness of the consumer's connection to the provenance of the product or the producer of the product. Studies 3 and 4 provide concrete implications for marketing practice by manipulating product design and assortment, showing how demand for traditional versus innovative products is affected by consumers' current need for groundedness, and exploring proxies that might allow managers to assess said need. In Studies 5a and 5b we focus on psychological effects on consumers. Study 5a shows that groundedness has a positive effect on consumer happiness, whereas Study 5b examines the effect of a grounding product on one's feelings of strength, stability, and safety. Study 1: Groundedness and Product AttractivenessStudy 1 tests the effect of groundedness on product attractiveness (H2). We do so in a study paradigm that aims to showcase the managerial relevance of the focal effect. Specifically, we exposed participants of a consumer panel to a more grounding ""indie"" brand of soap versus a less grounding industrial brand and took an incentive-compatible measure of participants' WTP for each product. We separately tested the extent to which the two brands provide a connection to place, people, and past (see Web Appendix B). We also measured a moderator—importance of the product category to the consumer—to provide further insight into the process and strengthen internal validity (e.g., to alleviate any concerns about demand effects). We reasoned that the self-related benefit of groundedness afforded by indie (vs. industrial) brands should be more pronounced when the product category is more central to the self (i.e., more important to the consumer). MethodAn age- and gender-representative sample of 311 Austrian consumers from a professional market research panel participated for monetary compensation (Mage = 41.8 years; 50.2% female; for instructions and stimuli of this and all following studies, see Web Appendices B–F). All participants were exposed to a color picture and verbal description for two bars of soap. An almond-scented soap made by Firm A was always presented on the left. An olive-scented soap from Firm B was always presented on the right. We manipulated which firm was described as indie (""makes high-quality products that are produced in a small and independent craft business"") versus industrial (""makes high-quality products that are industrially produced at scale in a large factory"").[ 7] Participants indicated their WTP for a bar of soap from both companies separately using an incentive-compatible elicitation method (dual-lottery Becker–DeGroot–Marschak procedure; e.g., [13]). This method provides an incentive-compatible measure of what the product is worth to participants.Next, participants indicated which soap provided relatively stronger feelings of groundedness by rating agreement with the following two statements (translated from the original German): ""When I think of this firm's soap ... I feel deep-rooted and firmly anchored ('grounded')"" and ""I can firmly feel my feet on the ground."" Participants also indicated how well a graphic depicting a human form with branches for arms and a deep, wide root system instead of legs (see Figure 1) represented their emotional state. The three items were measured on a seven-point scale (1 = ""true for Firm B,"" and 7 = ""true for Firm A"") and were averaged to create a groundedness index (α = .87).[ 8] We captured the importance of the underlying product category to the consumer with a three-item measure (e.g., ""The product category 'soap' is very important to me""). All measurement items used in this and subsequent studies, as well as their reliability statistics, are listed in Web Appendices B–F. Unless indicated differently, items are measured on seven-point scales (where ""strongly disagree/does not describe my feelings at all/not true of me at all/true for Brand B,"" etc. is coded as 1, and ""strongly agree/describes my feelings very well/very true of me/true for Brand A,"" etc. is coded as 7). Results and DiscussionWe ran a repeated-measures analysis of variance (ANOVA) with consumers' WTP in euros as the repeated-measures factor and our indie versus industrial counterbalancing manipulation as the between-subjects factor (for complete results, see Web Appendix B). We find the expected interaction effect (F( 1, 309) = 174.51, p < .001). Follow-up contrast analyses show that participants are willing to pay more for the soap of Firm A if that product is portrayed as an indie (Mindie = €3.29) versus as an industrial (Mindustrial = €1.91; F( 1, 309) = 37.47, p < .001) brand. Likewise, the soap of Firm B is valued more when Firm B is described as an indie (vs. industrial) company (Mindie = €3.12, Mindustrial = €2.11; F( 1, 309) = 20.67, p < .001)—a notable 60% increase in value. For moderation and mediation analyses, we calculated the intraindividual delta WTP (WTPFirm A − WTPFirm B: MFirm A indie = €1.18, MFirm A industrial = −€1.21; F( 1, 309) = 174.51, p < .001).An ANOVA on the groundedness measure indicates a significant effect: when Firm A is described as indie, participants more strongly declare that Firm A makes them feel grounded (MFirm A indie = 5.15) compared with when Firm A is described as industrial (MFirm A industrial = 2.92; F( 1, 309) = 269.58, p < .001). Mediation analysis ([20], Model 4, 10,000 bootstrap samples) shows that the WTP effect is mediated by feelings of groundedness (indirect effect = 1.24, 95% confidence interval [CI95%]: [.87, 1.67]). A moderation analysis ([20], Model 1) with the delta WTP measure as dependent variable confirms the hypothesis that the indie premium increases as the category importance increases (p < .001; for details, see Web Appendix B). Finally, a moderated mediation analysis ([20], Model 8) shows that this interaction effect is mediated by groundedness: the indirect effect of indie versus industrial on delta WTP through feelings of groundedness is always significant but stronger at high versus low levels of category importance (indirect effect16th percentile = .79, CI95%: [.51, 1.12]; indirect effect50th percentile = 1.13, CI95%: [.77, 1.55]; indirect effect84th percentile = 1.54, CI95%: [1.04, 2.16]; index of moderated mediation = .21, CI95%: [.11,.34]).Study 1 finds that products making a connection to the past, to people, and to a place make consumers feel more grounded, which increases their WTP. Thus, the result in Study 1 supports H2. The effect is managerially relevant: the more grounding product yielded a notable 60% increase in WTP. In addition, Study 1 shows that the effect is moderated by the importance of the product category. The pattern of moderated mediation, where the indie versus industrial nature of the brand is less important to feelings of groundedness when the product is less important to the consumer's identity, provides further evidence for our process.A limitation of Study 1 is that indie versus industrial products may differ in more aspects than their ability to provide a feeling of groundedness. For example, an indie brand might provide higher value to consumers by being perceived as more authentic ([32]) and more unique ([25]) than an industrial brand. Further, the description of the indie brand and its production method might give consumers a greater sense of love ([13]), human contact ([38]), attachment ([43]), brand experience ([ 5]), and brand community (e.g., [28]). Or the indie brand might simply be higher quality and costlier to produce. Our mediation and moderated mediation provide initial evidence for the proposed groundedness process, suggesting that these alternative processes are not the only drivers of the effects on WTP. We explicitly address these alternative explanations in Study 2. Study 2: Connectedness to Place or People, Groundedness, and Product AttractivenessOne major element of our theory is that the feeling of groundedness afforded by a product results from the connection that product provides to place, people, and past (H1). If products are indeed connectors between customers and their place, people, and past, we should be able to affect groundedness—and product attractiveness (H2)—not just by manipulating the place, people, or past of the product as we did in Study 1 but also by manipulating the place, people, or past of the customer. Thus, in Study 2a, we keep brands and products constant and manipulate how much groundedness a brand is able to provide as a function of a customer characteristic (i.e., customer location), rather than a product characteristic. Study 2a MethodWe asked 172 students (Mage = 21.9 years; 79.7% female) at a Northeastern U.S. university (n = 89, for a gift voucher and cookies) and an Austrian university (n = 83, for course credit) to imagine that they had just moved to either Karlstad or Umeå in Sweden. We then asked them to choose (using a three-item measure, e.g., ""Which of the two craft beers do you choose?"") which of two real Swedish craft beer brands, Good Guys Brew from Karlstad and Beer Studio from Umeå, they would purchase on their first night out. Next, participants reported which of the two brands they perceived would make them feel more grounded (""In the situation described, this brand would make me feel deep-rooted,"" ""This brand would make me feel well-grounded,"" and ""In a metaphorical sense: Which of the two craft beers would rather make you feel as illustrated by the following picture?"" [showing the picture of a human/tree form with deep roots]; α = .90). All items in this study were captured on seven-point scales where one anchor was the beer from Karlstad and the other anchor the beer from Umeå. We counterbalanced which beer was shown on the left- versus right-hand side. Before the participant location manipulation, we also asked participants to rate the two brands on a relative scale regarding nine product characteristics that might make either product more attractive. Because these were product characteristics that should not have been influenced by the participant's location, and because they were measured before the location manipulation, they did not—and could not—explain our results (for results regarding the control variables in this and all subsequent studies, see Web Appendices C–F). At the end of the study, we captured some information about the participants' relation to beer and to Sweden (e.g., ""Have you ever been to Sweden?,"" ""How much do you like beer in general?"").[ 9] Results and discussionA one-way ANOVA shows that participants who moved to Karlstad prefer the Karlstad-based beer significantly more than those who moved to Umeå (MKarlstad = 4.80, MUmeå = 4.14; F( 1, 170) = 6.70, p = .010). Similarly, the Karlstad-based beer provides relatively more groundedness to participants who moved to Karlstad versus Umeå (MKarlstad = 4.29, MUmeå = 3.79; F( 1, 170) = 5.77, p = .017). Groundedness mediates the effect of residence location on preference (indirect effect = .40, CI95%: [.07,.74]; [20], Model 4). For each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Study 2a shows that groundedness drives product attractiveness (H2) when we keep products constant but manipulate the place of the customer. This study highlights that the groundedness effect depends not only on the features of the product but also on the situation of the customer. Managerially, the study shows that local brands are particularly grounding and thus attractive to local consumers. Study 2a manipulated how participants relate to a place that is connected to a focal product, and thus how much groundedness it affords them. Unlike Study 2a, Study 2b capitalizes on participants' existing relationship to a place. Study 2b further addresses alternative constructs to groundedness by measuring them after the focal manipulation. Study 2b MethodThe week before Christmas, we asked 1,306 Austrian students from a university in Vienna (Mage = 22.8 years; 55.4% female; compensated by a lottery for an iPhone 11 and five €10 gift vouchers, prescreened for having grown up in Austria but outside Vienna and for celebrating Christmas) to imagine they were celebrating Christmas in Vienna this year and looking to buy a Christmas tree at a local market. We then varied between-subjects whether the market's Christmas trees originated from the state the participant grew up in or from a randomly selected other Austrian state. The trees were thus not connected to participants' current place (i.e., where they were studying and buying the tree) but to either the place where they grew up or a third location in Austria. Then, we assessed purchase intent for the Christmas tree using four items (e.g., ""I would very much like to buy a Christmas tree at this market""). We next captured feelings of groundedness from purchasing a Christmas tree at that market, using the same three items as in Study 2a. Finally, participants completed two-item measures of alternative constructs (the product's authenticity, uniqueness, quality, love, production costs, sense of human contact, brand experience, feeling of belonging to a brand community, and attachment). In addition, we measured participants' desire to support the producer as a possible alternative explanation. Due to this study's use of multiple items for each construct, we were able to ascertain that groundedness is empirically distinct from the other constructs captured (purchase intent and alternative constructs) using the [11] criterion. We performed the same tests in all subsequent studies with multi-item measures of our dependent variables (see Web Appendices C–F). Results and discussionParticipants are more intent on buying a Christmas tree from the focal market if it is from their own state (Mown place = 5.35) versus another state in the same country (Mother place = 4.95; F( 1, 1,304) = 24.27, p < .001). Further, when the trees originate from participants' own state, participants experience stronger feelings of groundedness than when the trees are from another state (Mown place = 3.39, Mother place = 3.15; F( 1, 1,304) = 8.43, p = .004), which is in line with H1. We do not find significant differences between conditions with regard to the alternative explanations captured (ps >.087). Differences in perceived production costs (Mown place = 4.17, Mother place = 4.30; F( 1, 1,304) = 2.92, p = .088) are marginally significant but run in the opposite direction of the dependent variable. Thus, they are unable to explain our results. Consistent with H2, a mediation model ([20], Model 4) shows that groundedness mediates the treatment effect on purchase intent (indirect effect = .11, CI95% [.03,.18]). For each of the ten alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Studies 2a and 2b show that a product that connects a consumer to a place they relate to (a city they move to, the state they are from) makes them feel more grounded and is more attractive than a product originating from a specified place they do not relate to (another city or state in the same country). One pertinent question is how much that feeling of groundedness depends on the closeness of the connection to place, people, and past. While the more grounding option in Studies 2a and 2b connects customers to their own (""my"") current or past place, the indie brand utilized in Study 1 merely provided a connection to ""a"" place (and ""the"" people who made it and ""the"" past, respectively). Our view is that, ceteris paribus, the depth of groundedness gradually increases with the closeness of the connection. The closer the personal relationship of the customer to the place, people, and past represented by the product, the stronger the connection and thus feelings of groundedness established via the product. We test this prediction in the context of a customer connecting to the people dimension next. Study 2cStudy 2c addresses whether differences in closeness indeed matter—that is, whether they afford different levels of feelings of groundedness when compared directly. Beyond that, the study isolates connection to people as a potential driver of groundedness (H1). MethodTwo hundred U.K. crowd workers on Prolific (Mage = 33.8 years; 55.0% female; for monetary compensation) were asked to indicate their feelings of groundedness associated with the use of a coffee mug (using the same measure as in Studies 2a and 2b). To sample different levels of personal closeness along the proposed continuum, the producer of the mug was manipulated to be either ""an artisan that is personally close to you (e.g., a close friend, relative, partner, etc.)"" or ""an artisan that is a distant acquaintance of yours (e.g., a colleague from work, a neighbor, a friend of a friend, etc.)."" We measured perceived connection to people through the mug using three items (e.g., ""Drinking from this mug, I somehow feel a connection to 'my people'""). We used the same control measures as in Study 2b (except for the motivation to provide financial support by purchasing a product, given that there was no purchase in this study). Results and discussionFirst, the pattern of results for groundedness and connection to people supports our theorizing about a continuum of closeness and, thus, groundedness: perceived connection to people is significantly higher when the artisan producer is a close other versus when they are merely an acquaintance (Mclose = 4.34, Mdistant = 3.75; F( 1, 198) = 6.63, p = .011). The same is true for feelings of groundedness: participants experience stronger feelings of groundedness when considering the coffee mug produced by an artisan that is a close other versus one that is merely a distant acquaintance (Mclose = 4.14, Mdistant = 3.29; F( 1, 198) = 14.78, p < .001). Further, a mediation model ([20], Model 4) shows that producer closeness mediates the effect on groundedness (indirect effect = .42, CI95%: [.09,.75]). Importantly, for each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Thus, Study 2c shows that being personally closer to one of the sources of groundedness enables consumers to experience stronger feelings of groundedness. More precisely, groundedness is a function of how close the consumer's relationship is to the product's place, people (e.g., the product's producer), or past. As for different routes to groundedness, the study shows that a product's people dimension alone (e.g., its producer) can boost groundedness via a stronger perceived connection to people established by the product. Managerially, the findings are important because marketers can choose the extent to which they highlight the closeness or similarity between customers and producers. In addition, the study highlights that managers may need to search for personally relevant and close sources of groundedness from the perspective of a given target customer.The next set of studies investigates how the groundedness effect can be leveraged via marketing-mix elements (Studies 3a and 3b) and which types of customers have a particularly high need for groundedness (Studies 3a and 4). Study 3: Marketing Mix, Connectedness to Past, Groundedness, and Product AttractivenessStudy 3a focuses on connections to past as a source of groundedness (H1) by manipulating product design (H4). We also examine how the effect of groundedness on product attractiveness (H2) varies across consumers by capturing their chronic need to connect to the past (the higher this need, the stronger the groundedness effect should become). Study 3b manipulates consumers' state need for groundedness and addresses category management considerations by testing how consumers' need for groundedness impacts the preference for traditional versus innovative products. Study 3a MethodWe showed 223 students in the behavioral laboratory of a large European university (Mage = 23.9 years; 65.5% female; for monetary compensation or course credit) two sets of cutlery (from Brand A and Brand B) side by side, stipulating that they were of comparable price and quality. We manipulated product design to provide more versus less connection to the past by using a more traditional versus modern product design. We manipulated which set of cutlery was presented on the left- versus right-hand side (i.e., as Brand A vs. B). Using adapted versions of the measures in Studies 1 and 2, we asked participants to indicate which of the two brands they would rather purchase, which would make them feel more grounded, and which evoked a stronger connection to the past. Need to connect to the past as a chronic consumer trait—our moderator—was measured in terms of agreement with three items (e.g., ""I generally try to see if I can somehow satisfy my desire to [metaphorically] 'connect to the past'"").[10] Results and discussionOur manipulation proved effective: Participants more strongly associate Brand A ( = 7, Brand B = 1) with a connection to the past when Brand A cutlery had a traditional design (MBrand_A_traditional = 5.49, MBrand_A_modern = 1.97; F( 1, 221) = 405.25, p < .001). As expected (H4), we find a significant effect on groundedness—Brand A is perceived to provide more groundedness (relative to Brand B) when Brand A features traditional design (MBrand_A_traditional = 4.39, MBrand_A_modern = 3.65; F( 1, 221) = 18.50, p < .001). For product preference, we find an overall preference for the modern cutlery (MBrand_A_traditional = 3.69, MBrand_A_modern = 4.48; F( 1, 221) = 9.01, p = .003; of course, the fact that traditional products provide a stronger sense of groundedness does not preclude that many people might still prefer a specific set of modern cutlery over a specific set of traditional cutlery, or modern designs over traditional ones in general). More importantly, and as expected (H2), we find a positive effect of groundedness on product preference (b = .61, p < .001), and a positive indirect effect ([20], Model 4) of traditional (vs. modern) design on preference through groundedness (indirect effect = .55, CI95%: [.27,.89]). As one would expect, preference becomes even stronger for the modern cutlery when the groundedness path is controlled for (estimated MBrand_A_traditional = 3.40, estimated MBrand_A_Modern = 4.74).As anticipated, we find that one's general need to connect to the past significantly moderates purchase preference (p < .001; [20], Model 1). Thus, participants with a low need to connect to the past have a more pronounced preference for the modern cutlery; conversely, participants with a high need to connect to the past show a preference for the traditional cutlery (e.g., at need to connect to past = 1, conditional effect = −2.53, CI95%: [−3.58, −1.48]; at need to connect to the past = 7, conditional effect = 1.29, CI95%: [.002, 2.57]). A moderated mediation analysis ([20], Model 58; see Web Appendix D) shows that traditional design affords a stronger feeling of groundedness, and that groundedness becomes a more important driver of preference as general need to connect to the past increases. In fact, at very low levels of general need to connect to the past, a product's ability to provide feelings of groundedness no longer significantly impacts product preference (e.g., at need to connect to the past = 1, conditional effect = .33, CI95%: [−.05,.71]).In summary, Study 3a shows that by varying a marketing-mix element (product design) to be more traditional (vs. modern), marketers can affect customer preference via feelings of groundedness. This is because the marketing-mix element directly caters to a source of groundedness (H4). Study 3bStudy 3b investigates preference for traditional versus innovative products as a direct function of consumers' current need for groundedness and manipulates this need. We also perform a test of how the relative interest in different product categories—traditional versus innovative—is affected by different levels of need for groundedness, pointing to potential boundary conditions of the groundedness effect. MethodTwo hundred crowd workers on Prolific (Mage = 33.4 years; 54.0% female) from the United Kingdom took part in this study for monetary compensation. Participants filled out two ostensibly unrelated surveys. The first manipulated participants' current need for groundedness. Participants in the high-need condition read, ""Research has shown that feelings of groundedness can be positive or negative depending on the context and situation we are in."" They were then asked to describe a recent situation where feeling grounded was desirable to them because ""you metaphorically felt your roots were too loose and weak with respect to your connection to a place, to people, and the past."" Conversely, participants in the low-need condition read, ""Research has shown that feelings of groundedness can be negative or positive,"" and were asked to describe a situation where groundedness was undesirable to them because ""you metaphorically felt your roots were too dense and strong."" After completing the writing task and reporting their current need for groundedness on a version of our three-item groundedness scale, participants were thanked and told they would be forwarded to another study. Here, participants were introduced to two different online stores, presented side by side: one specializing in ""the best traditional products"" and one specializing in ""the best innovative products."" We then asked participants to indicate which of the stores they would prefer to shop at on a seven-point scale, with Store A and Store B as anchors. We alternated which of the stores (A vs. B) was presented as traditional versus innovative in our stimuli. We subsequently reversed the Store A versus B preference scores for half the data set, so that the innovative store preference was always anchored at 1 and the traditional store preference was always anchored at 7. Results and discussionOur manipulation was effective: participants who wrote about a situation where their need for groundedness was high reported experiencing a higher need for groundedness (M = 5.25) than those who wrote about a situation where need for groundedness was low (M = 4.11; F( 1, 198) = 41.19, p < .001). In terms of shopping preferences, participants in the high-need-for-groundedness condition showed a stronger preference for the online store with traditional (vs. innovative) products (M = 4.00) than those in the low-need-for-groundedness condition (M = 3.47; F( 1, 198) = 4.17, p = .043).Thus, and in line with H4, Study 3b shows that relative interest in purchasing traditional products is higher in situations and contexts where consumers' need for groundedness is high. In situations and contexts where groundedness is less sought after, innovative products become relatively more interesting. Study 4: Consumer Characteristics and Need for GroundednessStudies 3a and 3b suggest that groundedness is not equally attractive and relevant to all consumers in all situations. For segmentation purposes, it is important to know which consumers are more likely to have a strong enduring need for groundedness. As predicted in H5, we argue that the feeling of groundedness is more important to consumers when their work and life (e.g., computerized desktop work, living in a large city) do not provide a strong connection to place, people, and past; when certain life events (e.g., the COVID-19 crisis) shake their foundation; or when their basic needs are already sufficiently met (e.g., when they have higher SES). In Study 4, we use a survey to measure these consumer characteristics, along with need for groundedness and preference for products that connect to place, people, and past. The study was conducted in spring 2020, at the beginning of the COVID-19 pandemic and first lockdown. This enabled us to assess the impact of a disruptive life event on the need for groundedness. MethodAn age- and gender-representative sample from a U.S. consumer panel completed this survey for monetary compensation (N = 325; Mage = 45.5 years; 51.1% female). We first measured product preference and need for groundedness: preference for products connected to one's place, people, and past were measured (in random order) using three items each (e.g., ""I like to purchase products that connect me to 'my place' ['my people'/'my past'], i.e., my physical [social/historic] environment""). We merged these into one global index of purchase interest. Need for groundedness was measured using a version of our three-item scale, adapted to measure general need for groundedness (e.g., ""In general, I want to feel deep-rooted""). We next captured a series of demographic and lifestyle variables.To assess a potential lack of connection to people, place, and past in consumers' work and social lives, we captured three variables. First, we asked respondents about the type of area they live in (1 = ""in the countryside,"" and 7 = ""in a big city""). We hypothesized that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. Second, we assessed participants' desktop work using two items (e.g., ""During the week [e.g., when being at work] ... I primarily work at the computer""). We expected a positive relationship between desktop work and need for groundedness, because a disproportionate amount of computerized work (while confined to one's desktop) separates individuals from other people as well as the physical environment. A similar logic might apply to people whose job is characterized as ""work of the head"" (i.e., work that contains many abstract tasks), as opposed to people who perform manual labor (""work of the hands"") or work in social jobs (""work of the heart""; [17]). Respondents accordingly indicated which of these three categories their current or most recent job fell into.Next, to assess a potential link between need for groundedness and a disruptive major life event, we examined perceived impact of the COVID-19 crisis on the consumer's life. We assessed this with a single item (""Due to the current Corona [COVID-19] crisis, I feel that my life is in a state of major change""). Last, we theorized that the need for groundedness should become more prominent when basic needs such as food and shelter are not a concern. Therefore, we tested whether higher SES (measured on a three-item scale [e.g., ""I have enough money to buy things I want""]) might be an effective proxy for one's need for groundedness. No other measures were taken. Results and DiscussionFirst, and as expected, we find a significant and positive correlation between one's need for groundedness and purchase intent for products connecting to place, people, and past (r = .57, p < .001). Second, we analyzed the correlations of all proposed indicators with the need for groundedness. In particular, need for groundedness correlates positively with desktop work (r = .26, p < .001), SES (r = .30, p < .001), change experienced as a result of COVID-19 (r = .12, p = .030), and living in a big city rather than the countryside (r = .10, p = .079), but correlates negatively with performing work of the hands (r = −.11, p = .040; for a complete correlation table of this study; see Web Appendix E).Third, we ran multivariate ordinary least squares (OLS) regressions with all predictor variables on both need for groundedness and purchase intent. For those variables that emerged as significant predictors for both the need for groundedness and purchase intent, we examined whether the need for groundedness mediates the respective effects on purchase intent while entering all other variables as covariates. For conciseness, we report only significant results hereinafter (see Table 1 for details).GraphTable 1. Multivariate OLS Regression Models (Study 4). Need for GroundednessPurchase Interest in Products Connected to Place, People, and PastUnstandardized Coef.Standardized Coef.Unstandardized Coef.Standardized Coef.bSEβtp-valuebSEβtp-value(Constant)2.983***.4177.157<.001.682.461.481.140Living environment.038.034.0611.11.270.08*.038.1012.105.036Desktop work.143***.037.2483.892<.001.24***.041.3245.905<.001Work of the hands−.17.19−.061−.9.370−.228.21−.064−1.091.276(1 = hands,0 = otherwise)Work of the head−.281.172−.117−1.64.102−.699***.19−.226−3.686<.001(1 = head,0 = otherwise)Change through COVID-19.091*.04.1232.302.022.234***.044.2475.356<.001SES.185***.037.2725.008<.001.287***.041.337.038<.001Age.01*.004.1362.357.019.000.004.005.104.917Gender (1 = male, 0 = female).004.13.002.034.973.465**.144.1513.228.001R2 = .162, d.f. = 8, 316R2 = .379, d.f. = 8, 316 1 *p < .05.2 **p < .01.3 ***p < .001.4 Notes: Mediation models ([20], Model 4): Mediator = need for groundedness, DV = purchase interest; ( 1) IV = SES: indirect effect = .10, CI95%: [.05,.15]; ( 2) IV = desktop work: indirect effect = .07, CI95%: [.03,.12]; ( 3) IV = change through COVID-19: indirect effect = .05, CI95%: [.002,.10].The multivariate OLS models showed that three predictors remain significant for both the need for groundedness (NG) and purchase intent (PI) when simultaneously including all variables in the model: ( 1) desktop work (NG: b = .14, SE = .04, t(316) = 3.89, p < .001; PI: b = .24, SE = .04, t(316) = 5.91, p < .001), ( 2) SES (NG: b = .19, SE = .04, t(316) = 5.01, p < .001; PI: b = .29, SE = .04, t(316) = 7.04, p < .001), and ( 3) change related to COVID-19 (NG: b = .09, SE = .04, t(316) = 2.30, p = .022; PI: b = .23, SE = .04, t(316) = 5.36, p < .001). Need for groundedness mediates the effect of all three variables on purchase intent (in line with H2; see Table 1 and Web Appendix E).Our ""work of the head"" dummy was not significant in the multivariate OLS model. We conclude that the ""work of the head/heart/hands"" measure was probably too rough and thus unable to adequately detect the important nuances in job characteristics that affect the need for groundedness. We were also surprised that one's living environment did not emerge as a significant predictor for need for groundedness in the multivariate OLS model. A closer look at the data reveals, however, that a disproportionately large number (29.2%) of respondents in our sample indicated living in big cities (i.e., chose the endpoint of the scale). When dichotomizing the measure (i.e., living in big city vs. not), we find the predicted positive effect: people living in a big city have a heightened need for groundedness (see Web Appendix E).In summary, Study 4 finds that a higher need for groundedness is apparent in consumer profiles characterized by larger societal trends: living in big cities (urbanization), doing desktop work at the computer (digitization), and undergoing major change (such as during the COVID-19 pandemic). Further, groundedness seems to be more relevant for high-SES consumers.Thus far, we have provided a cohesive picture of groundedness in terms of both triggers (H1) and market-relevant outcomes (H2), as well as ways for marketers to leverage groundedness (H4, H5). In the final two studies, we examine the implications of groundedness for consumers' psychological well-being (H3). Study 5: Connectedness, Groundedness, and Consumer Well-BeingTo test our hypothesis that feeling grounded increases consumers' subjective well-being (H3), Study 5a measures happiness as a consequence of attaining groundedness. We also test another managerial manipulation: channel type (H4). Study 5b expands into a broader range of psychological outcomes; as outlined in our conceptual framework, the feeling of groundedness should provide consumers with a sense of strength, stability, safety, and self-confidence. We test these outcomes in the context of using locally grown ingredients and also investigate alternative constructs to groundedness, such as self-authenticity, meaning in life, or sense of identity. Study 5a MethodWe randomly assigned 190 Austrian students (Mage = 22.5 years; 50.5% female; lab-based, for monetary compensation) to think about shopping at a supermarket or local farmers market. We then asked about their feelings of groundedness; happiness; and being connected to place, people, and past. Happiness was measured using three items (e.g., ""In the situation just described, how happy would you feel?""). Feelings of groundedness were measured using our three-item measure. Connection to place, people, and past were captured separately using three items each (e.g., ""Having been in the supermarket [to the farmers market] makes me feel connected to my physical/social/historic environment""). The order of the dependent measures (happiness, groundedness), as well as the order of the item blocks capturing connection to place, people, and past, were counterbalanced. Perceived quality and price were measured as control variables. Results and discussionChannel type has a significant effect on groundedness and happiness. Participants who thought about shopping at the farmers market reported feeling significantly more grounded (Mfarmersmarket = 4.66 vs. Msupermarket = 3.80; F( 1, 188) = 18.19, p��< .001) and happier (Mfarmersmarket = 5.32 vs. Msupermarket = 4.87; F( 1, 188) = 7.94, p = .005). Consistent with our theorizing (H4), shopping at the farmers market leads to significantly higher perceived connection to place (Mfarmersmarket = 4.96 vs. Msupermarket = 4.03; F( 1,188) = 15.74, p < .001), people (Mfarmersmarket = 4.58 vs. Msupermarket = 3.45; F( 1, 188) = 24.60, p < .001), and past (Mfarmersmarket = 3.73 vs. Msupermarket = 2.52; F( 1, 188) = 25.47, p < .001). We also find support for serial mediation such that the effect of channel on happiness is mediated, in series, by connection to place, people, and past, and groundedness (for mediation results, see Web Appendix F). All effects remain robust when we enter quality and price as covariates.Study 5a thus supports our prediction that the feeling of groundedness increases consumers' subjective well-being (H3) while providing converging evidence for H1. Finally, the manipulation of distribution channel (H4) offers an actionable strategy for marketers to leverage groundedness.In our last study, we employ the context of locally grown ingredients to test a broader range of psychological outcomes of groundedness. We also test the explanatory value of groundedness against alternative constructs that are self-related, such as feelings of self-authenticity or meaning in life. Study 5b MethodThree hundred four students from a major European university completed Study 5b's online study for course credit. We excluded 12 participants for failing our reading check, leaving us with a final data set of 292 participants (Mage = 22.3 years; 69.5% female). Participants were asked to think about making apple pie on a Saturday; specifically, a pie with Boskoop apples—their favorite pie-making variety. In addition, they were told that these apples were from either an orchard only 12 kilometers from their home or an orchard 1,200 kilometers from their home. Participants then completed a short survey that measured five downstream psychological outcomes of groundedness using a five-item scale: ""I feel truly safe as a person,"" ""I experience a feeling of inner strength,"" ""I feel truly stable,"" ""I have a strong feeling of basic trust and confidence in myself,"" and ""I feel that nothing can stir me up"" (α = .89). Afterward, we measured feelings of groundedness using our three-item measure. Finally, participants completed four multi-item measures intended to capture alternative explanations (self-authenticity [e.g., ""I feel out of touch with the 'real me'""], meaning in life [e.g., ""I have a good sense of what makes my life meaningful""], self-identity [e.g., ""I have the feeling that I know who I am""], feeling of belonging [e.g., ""I have a feeling of belonging""]). Results and discussionParticipants who considered making apple pie with apples grown close to home scored significantly higher in terms of experiencing the related psychological downstream consequences than those using apples grown far away (Mlocal = 5.08, Mnonlocal = 4.61; F( 1, 290) = 12.22, p = .001). Thus, the apple pie made with local products boosted participants' personal feelings of strength, safety, and stability (for effects on the individual dependent variable items, see Web Appendix F). They also reported significantly stronger feelings of groundedness (Mlocal = 4.65, Mnonlocal = 4.06; F( 1, 290) = 15.20, p < .001). A mediation model ([20], Model 4) shows that the downstream consequences are mediated by feelings of groundedness (indirect effect = .27, CI95%: [.13,.44]). Importantly, the indirect effect via feelings of groundedness on the downstream consequences holds when we add, one at a time, each of the four alternative explanations as a rival mediator.Study 5b thus confirms positive psychological downstream effects of groundedness (H3) tested in the realm of local products. Products grown closer to the consumer—that is, products that are more strongly connected to one's place—make consumers feel not only more grounded but also stronger, safer, and more stable. General DiscussionIn this research, we have provided systematic evidence that products can provide consumers with feelings of groundedness by giving them a sense of connection to place, people, and past. We do so across nine studies (eight experiments and one survey), both online and in the lab, using different populations (business students, crowd workers on Amazon Mechanical Turk and Prolific, and members of commercial, representative panels) across two continents (total N > 3,000). We have tested our theory for robustness across a variety of product domains, including both disposable and durable consumer goods (food, care products, seasonal products, and tableware), using real brands to strengthen external validity as well as highly controlled stimuli for internal validity. We have provided process evidence via mediation, moderation, and moderated mediation. Theoretical ImplicationsThis work introduces feelings of groundedness to the marketing literature by identifying these feelings as an important construct for marketing research and systematically examining it as a driver of consumer behavior. While references to groundedness and related constructs can be found in philosophy (e.g., [47]), different domains of psychology (e.g., [29]), and psychotherapy ([31]), the concept of groundedness is new to experimental research in marketing, consumer behavior, and mainstream psychology. Existing research in consumer culture theory has given passing treatment to concepts such as ""rooted connections"" ([42]) and has definitely been inspirational to this work. However, it has neither discussed nor empirically explored the full concept of groundedness with its antecedents, proxies, boundary conditions, and consequences, which we have aimed to do here.We also contribute to the growing literature on consumer well-being. [47], p. 43) proposes that ""every human being needs to have multiple roots. It is necessary for him to draw well-nigh the whole of his moral, intellectual, and spiritual life by way of the environment of which he forms a natural part."" Our work indeed shows that groundedness is related to happiness and a sense of strength, stability, and safety; thus, we propose groundedness as a novel antecedent of these outcomes.We also theorized about three sources of feelings of groundedness: connections to place, people, and past. Although the three sources are often empirically intertwined, we show that they are theoretically distinct and powerful in fueling consumers' feelings of groundedness. Our analysis further provides rich insight on the nature of these connections by showing that the extent to which products provide feelings of groundedness is a graded function of closeness. That is, a product provides stronger feelings of groundedness when the product's place, people, or past is closer to the consumer. Finally, by identifying the role of groundedness and its sources, we offer an overarching theoretical explanation for major current consumer trends, such as buying local products (connected to place), produced by people we relate to (connected to people), and according to traditional production methods (connected to the past). Marketing ImplicationsFeelings of groundedness are worthy of managers' attention because these feelings have important downstream consequences as shown across our studies. In particular, feelings of groundedness impact consumers' brand preference and WTP. In Study 1, for example, consumers were willing to pay a price premium of about 60% for the product that provided more groundedness.Our work also provides actionable implications for product and brand management: we give concrete approaches regarding how firms can elicit groundedness by showing consumers their product's connection to place, people, and past. For example, our results in Studies 1, 2a, and 2b show how presenting a product as artisanal or highlighting the local origin of a product can provide feelings of groundedness. In Studies 3a and 3b, we have shown that managers can utilize other marketing-mix elements such as product design or retail assortment and configure them (e.g., as more traditional instead of modern) to provide a stronger connection to the past. Similarly, Study 5a shows that a marketer's choice of distribution channel (e.g., farmers market) has an impact on feelings of groundedness.In terms of customer targeting, we have pointed out when and for whom groundedness is more important. In particular, we have shown that traditional (vs. innovative) products benefit from situational differences in the need for groundedness (Study 3b). On the level of individual differences, in Study 3a, only consumers with a high chronic need to connect to the past preferred the more traditional cutlery design. Our representative survey (Study 4) further showed a higher need for groundedness among consumers who are particularly affected by large global trends or major disruptive events. These global trends (e.g., digitization, urbanization) and major life events (e.g., the COVID-19 pandemic) make it harder for consumers to feel connected to people, place, and past. From a groundedness perspective, it is not surprising that during the safety- and stability-threatening COVID-19 pandemic, customers returned to the familiar grocery brands consumed with their families while growing up ([ 7]). There are probably multiple drivers for this behavior, but it is likely that consumers chose these products, at least in part, because of the connections to place, people, and past—and thus feeling of groundedness—they provide. Limitations and Future ResearchThis is the first series of experimental studies investigating feelings of groundedness. As such, many questions remain for future research. With regard to antecedents, for example, we have focused on products as means for consumers to experience feelings of groundedness. However, anecdotal evidence suggests that there are other ways for consumers to feel more connected to place, people, and past and, consequently, more grounded: for example, through services such as genealogy websites, cooking classes, lectures on local history, or yoga and meditation classes providing ""grounding"" exercises.The scope of Study 4 has allowed us to identify an initial set of indicators for who has a higher need for groundedness and why, but it is clear there will be additional consumer characteristics and lifestyle variables helpful to marketers in identifying relevant customer segments. For example, people who travel frequently for work and have little chance to connect to their current physical environment may seize opportunities to (re)-connect to place—such as through a local craft beer—to feel more grounded. Likewise, pandemics such as COVID-19 are not the only type of events that can shake a person's foundation. Stressful life events such as separation or loss, starting a new job, or moving homes may cause a higher need to feel grounded. Similarly, the need for groundedness may be subject to seasonal variations. Preliminary insights from our own qualitative explorations suggest that individuals' need for groundedness may be particularly high during the holiday season and other festive occasions, such as Christmas, Thanksgiving, Ramadan, and one's own birthday. Apart from that, interestingly, the need for groundedness appears to be higher during the colder seasons. We believe a more thorough testing of these hypotheses seems promising and would likely have important implications. If the initial signals are correct, for example, studies of scanner or panel data should reveal variations in the demand for products that connect to place, people, and past across the year.Finally, we have only begun to examine boundary conditions. For example, it seems possible that in some situations strong roots not only provide strength and stability but could also constrain movement, thus giving consumers the feeling of being ""stuck"" and unable to escape their roots. Imagine growing up on a farm, surrounded by one's family, and doing things day after day in the same way they have traditionally been done by previous generations. A person in this situation will likely feel grounded but might also feel more motivated to break free, move away, or challenge the status quo. If such is the case, too much groundedness might even backfire. Future research might thus enrich the present investigation by focusing on potential downsides of groundedness. ConclusionThis research introduced feelings of groundedness as a relevant construct for marketing research and consumer behavior. We have demonstrated its importance to marketers by documenting that it increases product attractiveness and that it can be manipulated through a variety of marketing-mix strategies and used for targeting consumer segments prone to a lack of groundedness. We also have shown that groundedness is important to consumer well-being, pointing to important consumer welfare and policy implications. We expect that the importance of this topic to consumers and marketers will only increase as digitization, urbanization, and global migration continue to challenge consumers' connections to place, people, and past.  "
13,"Despite Efficiencies, Mergers and Acquisitions Reduce Firm Value by Hurting Customer Satisfaction Most researchers focus on the effect of mergers and acquisitions (M&As) on investor returns and overlook customer reactions, despite the fact that customers are directly impacted by these corporate transformations. Others suggest that in M&A contexts, a dual emphasis of customer satisfaction and firm efficiency is both likely and beneficial. In contrast, the authors demonstrate that M&As not only do not yield a dual emphasis but also cause a decline in customer satisfaction to the extent that they eclipse any gain in firm value from an increase in firm efficiency. A quasiexperimental difference-in-differences analysis and an instrumental variable panel regression provide robust evidence for the dark side of M&As for customers. The authors use the attention-based view of the firm to demonstrate that post-M&A customer dissatisfaction occurs because of a shift in executive attention away from customers and toward financial issues. In line with the related upper echelons theory, they find that marketing representation on a firm's board of directors helps maintain executive attention on customers, which mitigates the dysfunctional effect of M&As on customer satisfaction. This research identifies a negative M&A–customer satisfaction relationship and highlights executive attention to customer issues and marketing leadership as factors that mitigate this negative relationship.Keywords: American Customer Satisfaction Index; attention-based view; board of directors; customer satisfaction; difference-in-differences; marketing in C-suite; mergers and acquisitions (M&As); upper echelons theoryFirms engage in mergers and acquisitions (M&As) to obtain assets, grow, reduce costs, and stave off competition ([ 6]; [63]). Yet, many M&As fail to generate positive results ([57]; [61]). Although prior research has explained the underperformance of M&As with deal- and firm-related factors, the role of customer reactions has largely been neglected. This is alarming given that customer growth is a key motivation for M&As ([18]) and customers are directly impacted by M&A-based changes to product lines, brands, prices, innovation, and frontline employees.The sheer enormity of M&A activity (e.g., more than 48,000 deals with a value of $3.7 trillion were transacted globally in 2019, and despite the COVID-19 pandemic, M&A activity declined by only 3% in 2020) suggests that M&As must be rewarding; otherwise, firms would not engage in them. M&As allow firms to reduce prices ([20]) and innovate ([53]), both of which should satisfy customers. Further, M&As enable firms to become more efficient through improvements in scale, scope, and cost savings ([16]; [37]). As a result, M&As are posited to enable a ""dual emphasis"" in which firms achieve both customer satisfaction and firm efficiency ([62]).While the link between M&As and firm efficiency is more straightforward, research has not systematically examined the effect of M&As on customer satisfaction. Experimental ([64]) and anecdotal ([65]) evidence suggests that M&As may in fact harm customers. Thus, we question whether M&As actually enable a dual emphasis of firm efficiency and customer satisfaction. Instead, we argue that although M&As might generate firm efficiency, they upset customers considerably, which, in turn, will lower firm value to an extent that any gain in efficiency will be outweighed. We theorize that this is because M&As cause executives to pay more attention to financial issues than to customer ones, which will dissatisfy customers. We contend that marketing representation on the board (MROB) of directors will direct executive attention toward customers during an M&A, which will then lessen a decline in customer satisfaction.To test our expectation that there is a tension between M&A activity and firm value via competing processes of lower customer satisfaction and higher firm efficiency, we collected data on a panel of firms from 1995 to 2017 from the American Customer Satisfaction Index (ACSI) database. First, we estimated a system of equations to demonstrate that ( 1) M&A activity is associated with a decrease in customer satisfaction, ( 2) M&A activity is associated with an increase firm efficiency, and ( 3) the net effect of a decrease in customer satisfaction and an increase in firm efficiency on firm value is negative. Thus, M&As lower customer satisfaction to the extent that it overshadows any gain in firm value from firm efficiency. Second, to solidly establish a negative effect of M&As on customer satisfaction, we conducted ( 1) a quasiexperimental differences-in-differences (DID) analysis of a treatment group of firms that engaged in M&As and several control groups of firms that did not, ( 2) a conventional panel regression analysis, and ( 3) a long-term analysis. We find strong evidence for a negative M&A–customer satisfaction relationship, which persists for two years post-M&A. Finally, we content-analyzed letters to shareholders to measure executive attention and collected data on MROB. Our instrumental variable panel moderated-mediation analysis provides support for a mediating role of executive attention to customers (vs. finance) and a positive moderating role of MROB.We contribute to the literature in multiple ways. First, previous research has focused on the effect of M&As on investor returns (e.g., [23]; [44]) and has largely overlooked customer reactions. In fact, a meta-analytic review of 25 years of customer satisfaction research does not report a single result with M&A activity as a driver ([50]). The few studies that have focused on customers (e.g., [62]) have suggested that in M&A contexts, a dual emphasis of customer satisfaction and firm efficiency is both likely and beneficial. In contrast, we demonstrate that M&As not only do not yield a dual emphasis but also cause a decline in customer satisfaction to the extent that they surpass any gain in firm value from an increase in firm efficiency. Although researchers in finance have highlighted the negative ramifications of M&As for acquirers (e.g., [ 2]; [36]), we are the first to empirically establish the negative ramifications of M&As for customers, which we show lowers firm performance.Second, we verify a negative M&A–customer satisfaction relationship with a DID analysis with multiple control groups. As a result, we add to emerging research (e.g., [25]) on the use of observational inference to document the causal effects of strategic decisions. We also confirm this negative relationship with an instrument variable panel regression with a larger sample and a long-term analysis. Our multimethod approach offers future research a template with which to improve the reliability and validity of findings from secondary research.Third, to study M&A outcomes, work in finance has relied on the efficient market theory, and work in marketing has relied on the resource-based view (RBV) of the firm. Rather, in a novel direction, we draw on the attention-based view (ABV) of the firm to argue the impact of M&A activity. Thus, we add to recent work on the marketing–finance interface (e.g., [19]) by showing that when faced with boundary-altering strategic decisions, executives tend to focus more on financial issues than on customer issues, which then indirectly lowers performance.Finally, marketing researchers have typically overlooked board of director composition, despite the fact that marketers on the board help shape a firm's strategic direction ([70]). We address this gap by complementing the ABV of the firm with the upper echelons theory to demonstrate that firms with (vs. without) marketers on their board of directors help channel executive attention to customers (vs. financial issues). This, in turn, helps minimize customers' post-M&A dissatisfaction. Here, we identify marketing leadership's important role in the marketing–finance interface during disruptive strategic transformations such as M&As. As a result, we are the first to incorporate marketing's role on the board into theories about M&As and customer satisfaction.In terms of our practical contributions, we caution executives against pursuing M&As to gain efficiencies without considering how customers may be harmed. This is because the negative effect of M&As on customer satisfaction lasts for at least two years. In particular, we show that during an M&A, firms that pay greater attention to their customers relative to financial issues experience a 45% reduction in loss of firm value. As a solution, we recommend that firms have at least one marketer on their boards of directors to retain executive attention on customers, which translates into a gain in firm value of 4.28%. Theoretical Background and HypothesesAs we show in our literature review in Table 1, we distinguish our research from prior work in four important ways. First, although prior work has investigated the effect of M&As on firm efficiency, we are the first to also consider the effect of M&As on customer satisfaction to determine their overall effect on firm value. Second, we examine the effect of M&As on customer satisfaction with multiple data structures and models across multiple industries and years to make causal inferences. Third, while previous research in finance has overwhelmingly relied on the efficient market hypothesis and those in marketing have relied on the RBV of the firm, we introduce the ABV of the firm to an M&A context. Finally, those who have used upper echelons theory have overlooked the role of marketing leadership in managing M&As and driving customer satisfaction. We address these gaps by proposing and demonstrating that MROB weakens the negative impact of M&As on executive attention to customer (vs. financial) issues. We depict our conceptual framework in Figure 1.Graph: Figure 1. Conceptual framework.GraphTable 1. A Review of the Literature on M&As. ResearchM&A → Firm EfficiencyM&A → Customer SatisfactionNet Effect of M&A on PerformanceBoard RepresentationMultiple IndustriesSample SizeTime PeriodAnalysisUnderlying TheoryVennet (1996)YesNoNoNoNo4921988–1993Univariate comparison of pre- and post-M&AManagerial efficiency theoryAgrawal, Jaffe, and Mandelker (1992)YesNoNoNoYes9371955–1987Event studyEfficient market theoryLoughran and Vijh (1997)YesNoNoNoYes9471970–1989Event studyEfficient market theoryAvkiran (1999)YesNoNoNo41985–1995Case studyTraditional merger theoryCapron and Hulland (1999)NoNoNoNoNo253Survey in 1994OLSRBV of the firmMaksimovic and Phillips (2001)YesNoNoNoNoNA1972–1992OLSNeoclassical model of firm organizationFuller, Netter, and Stegemoller (2002)YesNoNoNoYes3,1351990–2000Event studyEfficient market theoryMoeller, Schlingemann, and Stulz (2004)YesNoNoNoYes12,0231980–2001Event studyEfficient market theoryHomburg and Bucerius (2005)YesNoYesNoYes232Survey in 2002LISRELRBV of the firmPrabhu, Chandy, and Ellis (2005)NoNoNoNoNo1571988–1997Error-component regressionKnowledge-based view of the firmSorescu, Chandy, and Prabhu (2007)YesNoNoNoNo2381992–2002OLSRBV of the firmBahadir, Bharadwaj, and Srivastava (2008)NoNoNoNoYes1332001–2005Heckman two-stepRBV of the firmCummins and Xie (2008)YesNoNoNoNo1501994–2003Malmquist analysisCorporate control theorySwaminathan, Murshed, and Hulland (2008)YesNoNoNoYes2061990 to 2001OLSRBV of the firmThorbjørnsen and Dahlén (2011)NoNoNoNoYesaround 1,000Hypothetical experimentsOLSPerceived reactanceWiles, Morgan, and Rego (2012)YesNoNoNoYes5721994–2008Event studyRBV of the firmSwaminathan et al. (2014)Yes, only as a moderatorYes, only as a moderatorNoNoYes4291995–2003Random-effects GLSRBV of the firmRao, Yu, and Umashankar (2016)NoNoNoNoNo1,9791992–2008Matching modelRBV of the firmSaboo et al. (2017)YesNoNoCEO backgroundNo3191995–2013Event studyRBV of the firmBommaraju et al. (2018)NoNoNoNoNo2,512Longitudinal surveyDIDInternal marketing theoryThis articleYesYesYesYesYes2,1521995–2017SUR, Instrument variable panel regression, DIDABV of the firm, upper echelons theory 1 Notes: RBV = resource-based view; ABV = attention-based view; OLS = ordinary least squares; GLS = generalized linear model; DID = difference-in-differences; SUR = seemingly unrelated regression. M&A Activity and a Dual Emphasis of Customer Satisfaction and Firm EfficiencyAlthough there is sparse formal research on M&As and customer satisfaction, some work suggests a positive relationship. M&As can expand firms' product portfolios to provide customers with a larger set of choices ([12]) and higher-quality products ([33]). This supports Swaminathan et al.'s (2014) assertion that M&As are associated with higher customer satisfaction. In contrast, other research suggests that M&As may dissatisfy customers. In particular, M&As can result in price increases ([32]) and poor customer service ([60]). For example, the recent sale of DirecTV by AT&T to the private equity firm TPG for a third of the acquired price in 2015 was largely attributed to the loss of dissatisfied customers postacquisition ([34]). Moreover, anecdotes from the ACSI reveal that even two years after M&As, customers are less satisfied than they had been before ([ 1]). In particular, M&As may cause customers to lose access to their favorite brands. A recent survey by PwC shows that as firms become larger after an M&A, they tend to lose grip of their customers' feelings, and, as a result, customer experience suffers ([52]). This is detrimental because customer dissatisfaction lowers firm value and increases firm risk ([21]; [39]; [43]; [50]; [66]). Thus, we expect that M&As will dissatisfy customers.The strategy literature suggests that a primary motivation for firms to engage in M&As is to gain efficiencies ([37]). M&As increase firm efficiency by spreading ﬁxed costs over more output and eliminating redundancies ([12]). Specifically, M&As result in economies of scale and scope, asset and employee rationalization, a reduction in transaction costs ([14]), and a reallocation of intangible assets ([45]). These extra resources allow firms to reallocate their savings to other valuable projects, which, in turn, increases firm value ([43]). Thus, consistent with prior research, we expect that M&As will increase firm efficiency. This brings us to two competing outcomes of M&A activity: H1:  M&As are associated with (a) a decrease in customer satisfaction but (b) an increase in firm efficiency.A logical follow-up question is, what is the total effect of M&As on firm value given our opposing expectations of a decline in customer satisfaction but an increase firm efficiency? We expect that M&As will cause a steeper drop in customer satisfaction than an increase in firm efficiency for the following reasons. First, M&As often result in layoffs to reduce redundancies, which—while beneficial from an efficiency perspective—harms customer experience. The remaining employees that are not laid off are likely to be stressed ([11]), and stressed employees and their dissatisfaction with a major corporate shake-up negatively affect customers and the service they experience ([52]). Second, firms may either change or consolidate procedures such as credit policies, payment terms, and loyalty programs during an M&A to minimize the complexity of managing two separate systems. While these actions may be efficient, customers are likely to see their hard-earned privileges curtailed or, in the extreme, taken away ([64]), which results in relationship uncertainty ([29]). In fact, customers defect even before they know exactly how an M&A will affect them ([42]). Thus, customers who face poorer service and a loss of privileges will feel negatively about their relationship with a post-M&A firm. Third, customer dissatisfaction attracts short sellers, whose trading hurts firm value ([39]). Thus, we expect that a decline in customer satisfaction will be larger than an increase in efficiency, and as a result, firm value will decline. We test this notion in our estimation. M&A Activity, Executive Attention, and Customer SatisfactionSo far, we have argued that although M&As generate efficiencies, their negative impact on customer satisfaction is significant and noteworthy, yet underresearched. Next, we focus on the negative M&A–customer satisfaction relationship and aim to uncover a mechanism that drives this relationship. The marketing literature has often adopted the RBV of the firm view to examine M&A activity (Table 1). This research stream argues that a firm's ability to acquire and deploy marketing resources during an M&A can strengthen performance. Although the RBV provides a valuable strategic lens with which to examine M&A activity, another theoretical process may also be at play. We use the ABV to argue why M&As lower customer satisfaction.The ABV highlights the importance of executives' information-processing capacity and their distribution of attention. ""Attention"" refers to as a focus of time and effort with making sense of a firm's environment and how to respond to it ([48]). A key premise is that executives' attention is finite, so they are selective in what they notice and interpret. Further, how they respond to stimuli depends on what they notice and interpret in the first place. Thus, what executives pay attention to affects their resource allocation ([10]), which suggests that executives will invest resources in what they pay attention to at the expense of what they ignore. Further, attention drives executives to match their firms' resources with opportunities in their environment (Vadakeppatt et al. [67]; [72]). We draw on the ABV to argue that M&A activity directs executives' attention away from customers and toward financial issues, which, in turn, reduces the extent of resources allocated toward satisfying customers.M&As are incredibly expensive, complex, and heavily scrutinized by investors. As a result, executive attention is likely to be diverted to the price of the deals, capital requirements, paying back debt providers, and appeasing investors. In the process, customer experience might be underinvested in or even overlooked. In fact, managers know that there is a trade-off between serving customers and serving shareholders/debtholders such that creating value for one can detract from the other, and vice versa ([58]). For example, H.J. Heinz purchased Kraft Foods for nearly $36 billion in 2015. At the behest of investors, the merged company slashed $1.8 billion in overhead, which included a purge of nearly 2,500 jobs. Then, after Kraft Heinz's post-M&A sales slowed,[ 6] investors pressured it to acquire a large consumer products company to gain market share ([55]). Firms clearly face considerable financial pressure after an M&A, which can cause executives to focus on appeasing investors at the expense of customers.Further, M&As are often paid for by corporate debt. Recent examples of extensive borrowing for M&As include established companies such as CVS, IBM, Campbell's, Bayer, and Sherwin-Williams ([15]). Debt can turn executives' attention toward loan-servicing obligations, conserving cash rather than investing ([ 3]), and cost cutting ([38]). Debt also limits investments in advertising ([27]) and product quality ([40]). Thus, executives at indebted M&A firms may focus on satisfying debtholders over customers. Therefore, we hypothesize, H2:  M&As are associated with less executive attention to customers (vs. financial issues), which is associated with lower customer satisfaction. The Moderating Role of Marketing Representation on the Board of Directors (MROB)A key premise that the upper echelons theory ([28]) and the ABV ([48]) share is that the focus of executives' attention drives firm strategy and resource allocation. We use these complementary theories to examine how MROB influences executive attention toward customer-related issues during M&As. If executives pay more attention to, for example, innovation, then they allocate more resources toward innovation-related activities to drive success ([72]; [75]). Similarly, we argue that MROB will direct executive attention toward building organizational resources and processes, directing capabilities, and mobilizing employees to meet customers' needs during M&As.A firm's board of directors is a key body of leadership at its apex. It is both a governance body and a strategic body that sets a firm's goals and advises executives on how to pursue these goals ([ 9]). While executives are responsible for formulating strategies given a set of objectives, they do not determine these objectives ([24]). Rather, such objectives, which include growth or cost cutting, are usually made at the board level. As a result, a board of directors is heavily involved in the M&A process due to its transformative corporate consequences ([30]). A less researched, but critically important, type of board member is one who has a marketing title. Given their expertise in customer orientation, they provide marketing-related advice to other members on the board and the executive team, which ensures that firm strategies are customer-centric ([70]). We examine how MROB influences the relationship between M&As and customer satisfaction through a shift in executive attention.The upper echelons theory states that the characteristics of a ﬁrm's top leaders inﬂuence its strategic decisions and outcomes ([28]). Leaders' backgrounds create a lens through which they view business challenges and determine the strategies needed to address them ([17]). In particular, executive attention is ( 1) channeled toward issues of greater value or legitimacy for the firm, ( 2) evaluated through the lens of an executive's functional role, and ( 3) influenced by the environment ([22]). Given that financial issues dominate executives' attention during M&As, we contend that marketers on the board will serve as ""customer attention custodians"" to channel resources toward addressing any challenges faced by customers. They will do so by diffusing a customer-focus throughout the organization to mobilize employees to proactively attend to customers that face a disruptive context. Given that firms perform poorly in areas in which their board members have limited expertise ([41]), if there is no MROB, then customer-related issues are likely to be ignored or possibly mismanaged by others ([ 9]; [70]). Thus, we expect that marketers on the board will make customers a part of the conversation M&As largely because they are trained to do so.Scholars have typically relied on a resource-based perspective when they examine the board of directors' impact on firm strategy (e.g., [ 9]). We contend that the functional role of a board member influences not only whether role-related resources are conferred to the rest of the board and the firm but also what the board member interprets in the environment and encourages others to pay attention to. In other words, we expect that during an M&A, MROB will minimize a depletion of executive attention on customers and marketing-related issues. Therefore, H3:  The negative effect of M&As on executive attention to customers (vs. financial issues) is smaller when there is (vs. is not) MROB, which is associated with less customer dissatisfaction. Data and Method SampleWe drew our estimation sample from the ACSI database, which is a credible source for our primary outcome, customer satisfaction ([21]; [43]; [66]). We based our main analysis on a cross-sectional time series data set of 1,359 firm-year observations for 141 firms from 1995 to 2017. To identify the impact of M&As on customer satisfaction, we transformed this panel to a clean four-year rolling-window data structure, which we detail subsequently. Similar to prior research with multimethod studies (e.g., [51]), our sample sizes differ across different data structures and model specifications. MeasuresWe summarize our variables and data sources in Table 2.GraphTable 2. Operationalization of Variables. VariableMeasureData SourceFirm ValueNumber of shares outstanding × Share priceCenter for Research in Security PricesFirm EfficiencySales/Number of employeesCompustatCustomer Satisfaction (CSAT)ACSI scores, which range from 0 to 100ACSI DatabaseM&A Activity1 = firm engaged in an M&A in year t, 0 = firm did notSDC PlatinumM&A CountNumber of M&As that a firm engaged in in year tSDC PlatinumExecutive Attention to Customers (vs. Finance)Number of customer-related words/Number of finance-related words in letters to shareholdersAnnual Reports (EDGAR)Marketing Representation on the Board (MROB)Number of board members with a marketing title/Board size.Variables for selection model for MROB: Peer Firm Mean MROB: average number of MROB members for all firms in the focal firm's industryMean Board Age: average age of board membersCMO on TMT: CMO listed among TMT = 1, 0 otherwiseMean Board Tenure: average years board members have served on the focal boardBoard Size: Total number of board membersCEO Duality: CEO holds title of board chair = 1, CEO and board chair are separate = 0Female Percentage: percent of female board membersS&P Capital IQ Professional DatabaseMarket Share(t)Firm sales(t)/(Total industry sales at the four-digit SIC-level)(t)CompustatAdvertising/Sales(t)Advertising expenditures(t)/Sales(t)CompustatR&D/Sales(t)R&D Expenditures(t)/Sales(t)CompustatFirm Size(t)Natural logarithm of total assets(t) in CSAT models and natural logarithm of employees(t) in other modelsCompustatSegments(t)Natural logarithm of number of different four-digit SIC industries in which the firm operatesCompustatROA(t)Operating income before depreciation(t)/Total assets(t − 1)CompustatMarket Growth(t)Average of four-digit SIC industry year-over-year sales growth over four years preceding year tCompustatCompetitive Intensity(t)Reciprocal of the Herfindahl–Hirschman indexCompustatIndustry ROA(t)Four-digit SIC-level operating income before depreciation(t)/ Total assets(t − 1)CompustatRestructuring ChargesThe sum of restructuring charges in years t and t − 1 scaled by market capitalization of a firm in year tCompustatFirm ScopeThe number of distinct four-digit SIC business segments that a firm operates inCompustat/Segment Database 2 Notes: R&D = research and development; ROA = return on assets; SIC = Standard Industrial Classification. Dependent variablesSimilar to prior research (e.g., [43]), when multiple brands were represented in the ACSI database, we averaged their scores to create a firm-level annual score of customer satisfaction, or CSAT. We measured firm efficiency by dividing a firm's annual sales by its number of employees ([ 4]). We measured firm value with market value, or a firm's number of outstanding shares multiplied by its share price, which represents investors' expectations of a firm's profit potential ([19]). Independent variableConsistent with previous research (e.g., [54]; [73]), if a firm-year was present in the SDC Platinum database, then we designated that firm-year as having M&A activity (i.e., 100% ownership). If a particular firm-year was not present, then we assumed that this firm did not engage in M&A activity that year, and we used this information to create a group of non-M&A firms. Thus, we coded M&A activity as 1 if a firm engaged in M&A activity that year and 0 if it did not (for our list of M&A firms, see Web Appendix A). MediatorWe followed prior research (e.g., [51]) to assess executives' attention directed at theoretically relevant issues with a count of specific types of words from their letters to shareholders. To compile a dictionary of customer-related words, we began with [72]) dictionary of external focus and expanded their list based on a review of popular press announcements of M&As. To compile a dictionary of finance-related words, we reviewed popular press articles, finance M&A papers, and finance textbooks (we present our dictionary in Web Appendix B). We counted the number of words from these two dictionaries and created a ratio, attention to customers (vs. finance), by dividing the total number of customer words by the total number of finance words. ModeratorTo measure MROB, we created a list of marketing titles in top management based on research by [47] and [70]. We then counted the total number of people with marketing titles on the board and divided this by the total size of the board for each firm-year. Control variablesIn the CSAT model, we included market share, profitability, advertising intensity, R&D intensity, firm size, number of segments, and market growth ([38]; [56]). In the firm efficiency and firm value models, we included restructuring charges, firm scope, competitive intensity, industry profitability, firm size, market share, firm size, and market growth ([35]). Estimation MethodWe used three steps to test our hypotheses. First, we estimated a seemingly unrelated regression (SUR) model to test the effect of M&A activity on customer satisfaction (H1a) and firm efficiency (H1b) and the overall effect of M&A activity on firm value via customer satisfaction and firm efficiency. Second, we tested the negative M&A–customer satisfaction relationship (H1a) with ( 1) a quasiexperimental DID approach ([26]) with multiple control groups, ( 2) an instrumental variable panel regression, and ( 3) a long-term analysis. Third, we implemented a moderated-mediation SUR model to test whether the negative M&A–customer satisfaction relationship is mediated by executive attention to customers (vs. financial issues) (H2) and whether MROB shifts executive attention back toward customers (H3). A Test of H 1a–bWe created clean four-year rolling windows to include firms that had no M&A activity two years before an M&A and no M&A activity one year after. This enabled us to isolate the effect of M&A activity without confounding it with previous activity, because the effect of M&As tends to spill over to subsequent years ([68]). For example, in our first window for the M&A group, we included firms that engaged in M&As in 1997 but did not engage in M&As in 1995, 1996, and 1998. In the next window, for the M&A-year of 1998, we included firms that engaged in M&A activity in 1998 but not in 1996, 1997, and 1999. If a firm did not engage in any M&A activity during the four years (e.g., 1995–1998), then we included this firm in a non-M&A group. Overall, we had 119 firms in this sample.We estimated the following three models: ( 1) the effect of M&As on customer satisfaction (H1a), ( 2) the effect of M&As on firm efficiency (H1b), and ( 3) the effects of customer satisfaction and firm efficiency on firm value. We used the natural logarithmic values of our continuous variables of customer satisfaction, firm efficiency, and firm value to produce elasticities, which enabled us to compare the relative effects of customer satisfaction and firm efficiency on firm value. We included control variables that have been shown to influence CSAT ([38]; [56]), firm efficiency, and firm value ([35]). We winsorized the continuous variables before estimating the model to remove the potential effect of outliers and included fixed effects to account for unobservable firm characteristics. Given that M&A activity may simultaneously affect both customer satisfaction and firm efficiency, we estimated these relationships as a system of equations with SUR. We estimated the following system of equations for firm i at time t: CSATi(t)=φ0+φ1M&AGroupj+φ2Post-M&At+φ3(M&AGroupj×Post-M&At)+φ4MarketSharei(t−1)+φ5ROAi(t−1)+φ6FirmSizei(t−1)+φ7Advertising/Salesi(t−1)+φ8R&DSalesi(t−1)+φ9Segmentsi(t−1)+φ10MarketGrowthit+φ(t=2,3)TimeEffects+φ(firmcount)FirmFixedEffects+εit, Graph(1.1) FirmEfficiencyi(t)=β0+β1M&AGroupj+β2Post-M&At+β3(M&AGroupj×Post-M&At)+β4MarketSharei(t−1)+β5ROAi(t−1)+β6FirmSizei(t−1)+β7CompetitiveIntensityi(t−1)+β8IndustryROA(t−1)+β9Restructurei(t−1)+β10FirmScopei(t−1)+β11MarketGrowthit+β(t=2,3)TimeEffects+β(firmcount)FirmFixedEffects+εit, Graph(1.2) FirmValuei(t)=Θ0+Θ1CSATit+Θ2FirmEfficiencyit+Θ3MarketSharei(t−1)+Θ4ROAi(t−1)+Θ5FirmSizei(t−1)+Θ6CompetitiveIntensityi(t−1)+Θ7IndustryROA(t−1)+Θ8Restructurei(t−1)+Θ9FirmScopei(t−1)+Θ10MarketGrowthit+Θ(t=2,3,4)TimeEffects+Θ(firmcount)FirmFixedEffects+εit, Graph(1.3)where the M&A Group variable, j, has a value of 1 for the M&A group and 0 for the non-M&A group, and the Post-M&At variable has a value of 1 in the fourth year of each window (i.e., the post-M&A year). The interaction between M&A Group and Post-M&A has a value of 1 for the M&A firms and a value of 0 for the non-M&A firms in the post-M&A year. Therefore, φ3 (β3) represents the statistical effect of M&As on CSAT (firm efficiency). Finally, Θ1 (Θ2) is the effect of CSAT (firm efficiency) on firm value. H 1a–b ResultsWe first present model-free evidence of the effect of M&As on customer satisfaction and firm efficiency. For the M&A firms, customer satisfaction decreases a year after the M&A, whereas for the non-M&A firms, it increases (Figure 2, Panel A). In contrast, for the M&A firms, firm efficiency increases a year after the M&A, whereas for non-M&A firms, it remains steady (Figure 2, Panel B). Further, the average change in CSAT (ΔNon-M&A CSAT(t + 1, t − 1) = .43; ΔM&A CSAT(t + 1, t − 1) = −.14, p < .05) and firm efficiency (ΔNon-M&A Firm Efficiency(t + 1, t − 1) = 13.39; ΔM&A Firm Efficiency(t + 1, t − 1) = 71.83, p < .05) between the two groups are different.Graph: Figure 2. M&A customer satisfaction and firm efficiency trends.We present the descriptive statistics and correlations of our variables in Web Appendix C. Our SUR estimation results (Table 3) of Equations 1.1–1.3 demonstrate that M&As are associated with a decrease in customer satisfaction (φ3 = −.010, p < .05; H1a is supported) and an increase in firm efficiency (β3 = .070, p < .01; H1b is supported).GraphTable 3. Effect of M&As on CSAT, Firm Efficiency, and Firm Value. Dependent VariableCSATtFirm EfficiencytFirm ValuetFocal VariablesConstant3.929***.599***−1.194(.024)(.205)(1.375)CSATt2.214***(.325)Firm Efficiencyt.838***(.062)M&A Group−.001−.067***(.004)(.017)Post-M&A Year.005**.045***(.002)(.015)M&A Group × Post-M&A Year (H1a–b)−.010**.070**(.004)(.030)CovariatesRestructuring Charges(t − 1)−.263***–3.564***(.095)(.806)Firm Scope(t − 1).082***.009(.020)(.048)Competitive Intensity(t − 1).256***−.043(.033)(.049)Industry Profitability(t − 1)−.002**−.004(.001)(.005)Market Share(t − 1)−.073***.350*−.313(.012)(.192)(.347)Firm Profitability(t − 1).169***−.451***1.837***(.019)(.160)(.394)Firm Size(t − 1).025***−.200***.493***(.003)(.038)(.077)Market Growth(t)−.063***−.526***−.147(.015)(.075)(.211)Advertising/Sales(t − 1).021**(.010)R&D/Sales(t − 1)−.091***(.016)Segments(t − 1)−.010***(.003)Firm fixed effectsIncludedIncludedIncludedTime effectsIncludedIncludedIncludedModel Informationχ2χ2(130) = 7,918.48***χ2(131) = 61,482.97***χ2(131) = 15,867.32***R2.761.961.865Number of firms119119119Observations2,4682,4682,468 3 *p < .10.4 **p < .05.5 ***p < .01.6 Notes: We report parameter estimates with bootstrapped standard errors in parentheses. The Net Effect of M&As on Firm ValueGiven the asymmetric findings of a decline in customer satisfaction but an increase in firm efficiency from M&A activity, we next focus on the net effect of M&As on firm value through customer satisfaction and firm efficiency. From Table 3, we see that the positive association between customer satisfaction and firm value (Θ1 = 2.214, p < .01) is greater than the positive association between firm efficiency and firm value (Θ2 = .838, p < .01). To calculate the net effect of M&A activity on firm value via customer satisfaction and firm efficiency, we used the results from Equations 1.1 and 1.2. On average, the customer satisfaction of the M&A firms was 1.14% lower than the non-M&A firms (φM&A = φ1 + φ3 = −.0114 = −.0012 + −.0102) and the firm efficiency of the M&A firms was.29% higher than the non-M&A firms (βM&A = β1 + β3 = .0029 = −.0668 + .0697). We multiplied the M&A firms' customer satisfaction and firm efficiency elasticities for firm value from Equation 1.3 with the differences between the M&A and non-M&A firms in the post-M&A year from Equations 1.1 and 1.2. Then, we summed the products and found a net effect of −.0243. Therefore, compared with non-M&A firms, M&A firms' value decreased by 2.43% one year after an M&A, and as a result, the net-negative effect of M&As on firm value is due to a decrease in customer satisfaction. An In-Depth Analysis of the Negative Effect of M&As on Customer SatisfactionBecause we found that, despite gains in firm efficiency, M&As decrease firm value due to a decline in customer satisfaction, we aimed to validate the latter effect more systematically with several approaches. First, we estimated a quasiexperimental DID model with alternate non-M&A firm groups. Second, we created a panel of firms without imposing restrictions on which firms to include (i.e., we included all of the firms from the ACSI database). Third, we tested for the long-term negative effect of M&As on customer satisfaction. DID approachWe used with the same four-year rolling window data structure that we previously described. We assigned firms to an M&A treatment group if they engaged in M&A activity in the third year of a four year window and assigned all firms that did not engage in any M&As during those four years to a non-M&A control group (control group 1). For greater reliability, we created two alternative control groups by ( 1) matching the M&A and non-M&A firms on similar predictors of customer satisfaction (control group 2) and ( 2) matching the M&A and non-M&A firms on their propensity to engage in an M&A (control group 3) (for more information, see Tables D.1–D.3 in Web Appendix D). We specified the following model with a fixed-effects error component ([ 8]): CSATit=β0+γM&AGroupj+β1Post-M&At+β2(M&AGroupj×Post-M&At)+β3MarketSharei(t−1)+β4ROAi(t−1)+β5FirmSizei(t−1)+β6(Advertising/Sales)i(t−1)+β7(R&D/Sales)i(t−1)+β8Segmentsi(t−1)+β9MarketGrowthit+υi+εit, Graph( 2)where υi captures unobserved time-invariant firm characteristics. The M&A Group variable, j, has a value of 1 for the treatment group and a 0 for the control group. The Post-M&At variable has a value of 1 in the fourth year of each window (i.e., one year post-M&A). We used a fixed-effects within estimator to eliminate all time-invariant variables, such as υi and M&A Groupj. The interaction between M&A Group and Post-M&A has a value of 1 for the M&A firms and a 0 for the non-M&A firms the year after the M&A. Therefore, β2 is the statistical effect of M&As on CSAT. Conventional panel data structureWe created a conventional panel data setup to test the effect of M&As on customer satisfaction without a four-year rolling-window data restriction; as a result, our sample increased to 2,152 observations for 204 firms, of which 153 engaged in M&A activity. Because this sample includes firms for which there are several years of data (e.g., more than ten years), we used a one-year change in CSAT as our dependent variable. We created two versions of M&A activity: ( 1) a dummy variable that had a value of 1 if a firm engaged in M&A activity in year t and 0 if it did not and ( 2) the natural logarithm of the number of M&As a firm engaged in in year t.We estimated a selection equation in which our dependent variable was a firm's decision to engage in an M&A (0/1) and our predictors were factors that relate to M&A decisions (e.g., debt-to-equity ratio, competitors' M&A activity) but not to customer satisfaction (see Equation D.3 and Table D.5 in Web Appendix D). Thus, we achieved identification in Equation 3 and our subsequent Equation 4 based on our separation of factors that drive M&A decisions versus those that drive customer satisfaction. Based on this selection model, we included an inverse Mills ratio (IMR) in Equations 3 and 4 and estimated the following model with a fixed-effects within estimator to account for unobservable firm characteristics for firm i in year t: CSATi(t+1)−CSATit=α0+α1M&AActivityit+α2MarketShareit+α3ROAit+α4FirmSizeit+α5(Advertising/Sales)it+α6(R&D/Sales)it+α7Segmentsit+α8MarketGrowthi(t+1)+α9M&AIMRit+υi+εi(t+1). Graph( 3) Long-term effect of M&As on customer satisfactionWe investigated the long-term effect of M&As on customer satisfaction with a conventional panel data structure. We computed a change in CSAT from calendar year t + 4 to t as our dependent variable and included firms' M&A activity at t + 1, t + 2, and t + 3 as our independent variables. We included an IMR for each year in the model to control for selection bias. We estimated this panel data model with a fixed-effect within estimator: CSATi(t+4)−CSATit=γ0+γ1M&Ai(t+3)+γ2M&Ai(t+2)+γ3M&Ai(t+1)+γ4MarketSharei(t+3)+γ5ROAi(t+3)+γ6FirmSizei(t+3)+γ7(Advertising/Sales)i(t+3)+γ8(R&D/Sales)i(t+3)+γ9Segmentsi(t+3)+γ10MarketGrowthi(t+4)+γ11M&AIMRi(t+3)+γ12M&AIMRi(t+2)+γ13M&AIMRi(t+1)+υi+εi(t+4). Graph( 4) ResultsIn line with DID requirements ([26]), we compared the observable drivers of customer satisfaction between the M&A treatment group and the non-M&A control group two years before the M&A and found that for five of the seven drivers of M&As, the two groups were statistically similar (Table 4, Panel A; we present this graphically in Figures D.1–D.3 in Web Appendix D). We also tested the equality of changes in the drivers of customer satisfaction two years before the M&A through the M&A year and did not find any differences between the two groups. Thus, any distinction in post-M&A customer satisfaction between the two groups was not likely to be caused by firm-level differences, and the parallelness assumption was satisfied for the observable drivers of customer satisfaction. Next, we compared the two-year average customer satisfaction of the M&A and non-M&A groups before the M&A (t = .54, p > .10) and the equality of changes in customer satisfaction two years before the M&A through the M&A year to satisfy the parallelness assumption (t = 1.31, p > .10) and did not find any significant differences (Table 4, Panel B). Finally, the M&A firms' customer satisfaction was lower than the non-M&A firms' customer satisfaction a year after the M&A (t = 2.24, p < .05). We replicated these results for our alternate control groups (Table D.4 in Web Appendix D).GraphTable 4. A Comparison Between M&A and Non-M&A Groups. A: Comparison of Drivers of Customer Satisfaction Between M&A and Non-M&A GroupsM&A Group(Treatment)(n = 91)Non-M&A Group(Control)(n = 619)t-Test of Equality of MeansComparison of Pre-M&A Averages of Drivers of CSATMarket Share.15.16t = .96, p > .10ROA.14.14t = .52, p > .10Firm Size ($ million)44,598.2047,568.90t = .29, p > .10Advertising/Sales.03.04t = 2.23, p < .05R&D/Sales.04.03t = −1.14, p > .10Segments1.391.37t = −.24, p > .10Market Growth.08.05t = −2.80, p < .01Comparison of Pre-M&A Changes in Drivers of Customer SatisfactionPre-M&A Change in Market Share(t, t − 2)−.001.001t = .28, p > .10Pre-M&A Change in ROA(t, t − 2)−.009−.002t = 1.29, p > .10Pre-M&A Change in Firm Size(t, t − 2)1,154.271,344.63t = .12, p > .10Pre-M&A Change in Advertising/Sales(t, t − 2)−.004−.005t = −.29, p > .10Pre-M&A Change in R&D/Sales(t, t − 2)−.011.007t = 1.25, p > .10Pre-M&A Change in Segments(t, t − 2).008.004t = −.11, p > .10Pre-M&A Change in Market Growth(t, t − 2)−.024−.008t = 1.49, p > .10B: Comparison of Customer Satisfaction Between M&A and Non-M&A GroupsM&A Group(Treatment)(n = 91)Non-M&A Group(Control)(n = 619)t-Test of Equality of MeansPre-M&A PeriodsCustomer Satisfaction(t − 2)76.176.5t = .39, p > .10Customer Satisfaction(t − 1)76.076.4t = 1.00, p > .10Average Customer Satisfaction(t − 1, t − 2)76.176.5t = .71, p > .10Change in Customer Satisfaction(t, t − 2)−.28.19t = 1.31, p > .10Post-M&A PeriodCustomer Satisfaction(t + 1)75.877t = 2.24, p < .05 7 Notes: t denotes the year of the M&A activity. DID model estimation resultsWe present the estimation results of Equation 2 in Table 5. When we estimated Equation 2 with only the post-M&A variable and its interaction with M&A Group, we find that M&As caused a decline in customer satisfaction (β2 = −.635, p < .05; Model 1), which remained consistent with the inclusion of our control variables (β2 = −.754, p < .01; Model 3). We also estimated a model with only control variables (Model 2).GraphTable 5. DID Results of M&As and Customer Satisfaction with Multiple Control Groups. Dependent Variable: CSATtModels(1) Control Group 1:Only M&ATreatment(2) Control Group 1: Only covariates(3) Control Group 1:Full Model(4) Control Group 2(5) Control Group 3Focal VariablesConstant76.428***76.622***76.411***76.565***77.915***(.095)(.605)(.565)(.996)(1.234)Post-M&A Year.467***.466***.356*.576**(.114)(.104)(.207)(.262)M&A Group × Post-M&A Year (H1a)−.635**−.754***−.622**−.688**(.316)(.290)(.312)(.344)CovariatesMarket Share(t − 1)–2.315–1.991–1.285–2.561(1.599)(1.493)(2.218)(1.952)ROA(t − 1)3.682*3.762**1.362–4.577**(1.932)(1.748)(2.904)(1.948)Firm Size(t − 1)−.000**−.000*−.000−.000(.000)(.000)(.000)(.000)Advertising/Sales(t − 1).351.3712.458.834(.779)(.636)(5.847)(1.138)R&D/Sales(t − 1)–3.571***–3.671***–4.424**–1.157(1.175)(1.082)(1.854)(2.513)Segments(t − 1).163.200−.001−.188(.311)(.284)(.528)(.649)Market Growtht–1.197−.893.259.597(1.149)(1.084)(2.141)(1.442)Model InformationWald χ2χ2(2) =  17.09***χ2(7) =  24.62***χ2(9) = 57.42***χ2(9) = 19.90**χ2(9) = 15.62*R2.01.02.03.03.03Number M&A Firms6767676767Total Number of Firms14114114110698Observations2,8402,8402,840932728 8 *p < .10.9 **p < .05.10 ***p < .01.11 Notes: CSAT = customer satisfaction; ROA = return on assets; R&D = research and development. We report parameter estimates with bootstrapped standard errors in parentheses.While we accounted for time-invariant unobservable factors with a firm fixed-effects estimator, we tested whether time-varying unobservable factors altered our inference about the M&A treatment effect. We followed a procedure by [49] and used the PSACALC program in STATA. The result of this procedure suggests that our main analysis performed well because when we matched on time-varying unobservable variables, the coefficient estimate of β2 in Equation 2 only changed from −.75 to −.76. Alternate control group resultsWe analyzed Equation 2 with two alternative control groups and report their results in Table 5. When we only included firms that were similar to the focal firm in terms of predictors of customer satisfaction (control group 2), we find that M&As lowered customer satisfaction (β2 = −.622, p < .05; Model 4). When we used the propensity to engage in M&As scores (control group 3), we still find that M&As caused a decline in customer satisfaction (β2 = −.688, p < .05; Model 5). Thus, we find consistent support for a negative effect of M&As on customer satisfaction (H1a) with two alternative control groups. Conventional panel data resultsWe present our estimation results of Equation 3 in Models 1a (with a dummy variable for M&A activity) and 2a (number of M&As) in Table 6. We find that M&A activity lowers customer satisfaction (α1M&A Dummy = −2.438, p < .01; α1M&A Number = −.589, p < .01). Thus, we provide additional support for H1a and show that the negative M&A–customer satisfaction relationship is not sensitive to sampling and modeling approaches. The IMR coefficient is significant (α8 = 1.361, p < .01), which suggests that it is necessary to account for firms' propensity to engage in M&As.GraphTable 6. Effect of M&As on Customer Satisfaction Over Time. M&A DummyM&A NumberDependent Variables:Model 1aCSAT(t + 1) − CSAT(t)Model 1bCSAT(t + 4) − CSAT(t)Model 2aCSAT(t + 1) − CSAT(t)Model 2bCSAT(t + 4) − CSAT(t)Focal VariablesConstant.634*1.515.203.876(.363)(1.077)(.324)(.966)M&A(t)/(t + 3) (H1a)−2.438***−2.032**−.589***−.727*(.551)(.807)(.203)(.384)M&A(t + 2) (H1a)−2.743***−1.092***(.755)(.329)M&A(t + 1) (H1a).100−.426(.686)(.310)CovariatesMarket Share(t)/(t + 3)−.202−1.981−.500−1.996(.756)(2.346)(.767)(2.138)ROA(t)/(t + 3).9472.194.8392.052(1.594)(2.808)(1.603)(2.815)Firm Size(t)/(t + 3)−.000***−.000***−.000***−.000***(.000)(.000)(.000)(.000)Advertising/Sales(t)/(t + 3)2.337**3.252**2.414**3.227**(1.175)(1.645)(1.173)(1.608)R&D/Sales(t)/(t + 3)−1.327−5.431***−1.367−5.765***(1.146)(1.954)(1.139)(1.928)Segments(t)/(t + 3).123.686.083.659(.212)(.618)(.210)(.620)Market Growth(t + 1)/(t + 4)−.177−1.641−.713−2.455(1.005)(1.934)(1.006)(1.993)Decision to M&A IMR(t)/(t + 3)1.361***1.172**.271*.387*(.333)(.490)(.151)(.232)Decision to M&A IMR(t + 2)1.632***.634***(.437)(.197)Decision to M&A IMR(t + 1)−.015.297(.416)(.206)Model InformationF-statisticF(9, 203) = 4.83***F(13, 174) = 3.47***F(9, 203) = 3.28***F(13, 174) = 3.49***R2.013.046.007.044Number of firms204175204175Observations2,1521,7222,1521,722 12 *p < .10.13 **p < .05.14 ***p < .01.15 Notes: CSAT = customer satisfaction; IMR = inverse Mills ratio; ROA = return on assets; R&D = research and development. We report parameter estimates with cluster robust standard errors in parentheses. In Models 1a and 2a, the M&A variable, the IMR, and the control variables have the subscript (t), except for the market growth variable, which has the subscript (t + 1). In Models 1b and 2b, the M&A variables and their corresponding IMRs have the subscripts (t + 3), (t + 2), and (t + 1). In these models, the control variables have the subscript (t + 3), except for the market growth variable, which has the subscript (t + 4). Long-term resultsWe present the estimation results of Equation 4 in Models 1b (M&A dummy variable) and 2b (number of M&As) in Table 6. The negative impact of M&A activity on customer satisfaction persists for two years (γ1M&A Dummy = −2.032, p < .05; γ2M&A Dummy = −2.743, p < .01; γ1M&A Number = −.727, p < .10; γ2M&A Number = −1.092, p < .01). Thus, we find support for H1a even two years after an M&A. A Test of H 2 and H 3Given that we have established that M&As lower customer satisfaction with multiple methods, we aimed to test whether this decline is due to a shift in executive attention away from customers and toward financial issues (H2) and whether MROB moderates the M&A–executive attention relationship (H3). We used a four-year rolling window data structure and a SUR modeling approach to test these hypotheses. We present the descriptive statistics for this sample in Table 7, Panels A and B.GraphTable 7. Summary Statistics and Correlations. A: Summary StatisticsVariableObservationsMeanSDCSAT(t)2,46875.935.26Executive Attention(t)2,468.28.18MROB(t)2,468.02.04M&A Group2,468.12.33Market Share(t − 1)2,468.16.20ROA(t − 1)2,468.14.106Total Assets(t − 1)2,46845,271.44213,000Advertising Intensity(t − 1)2,468.04.08R&D Intensity(t − 1)2,468.04.09Ln(Segments)(t − 1)2,4681.36.47Market Growth(t − 1)2,468.04.08B: CorrelationsVariables(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)1. CSAT(t)2. Executive Attention(t)−.10*3. MROB(t).13*.034. M&A Group−.08*.03.035. Market Share(t − 1).23*−.11*.09*−.036. ROA(t − 1).08*−.10*−.07*−.00.10*7. Total Assets(t − 1)−.14*−.02−.01−.01−.04*−.15*8. Advertising Intensity(t − 1).05*.05*.07*−.06*−.03.05*−.049. R&D Intensity(t − 1)−.17*.20*−.03−.00−.13*−.15*−.04*.08*10. Ln(Segments)(t − 1).14*.15*.14*.01.19*−.01−.25*.06*.13*11. Market Growth(t − 1)−.07*.014−.14*.09*−.09*.10*−.05*−.04.13*.02 16 *p < .10.17 Notes: CSAT = customer satisfaction; MROB = marketing representation on the board; ROA = return on assets; R&D = research and development. Identification strategy for executive attention and MROBAfter collecting data on executive attention and MROB, we had a sample of 122 firms. Arguably, executive attention to customers (vs. finance) is endogenous because executives may strategically pay attention to issues that result in better outcomes, such as customer satisfaction. To address this, we used a latent instrumental variable approach ([31]; [35]). Specifically, we used a binary unobserved instrument to separate the observed endogenous predictor into correlated versus uncorrelated components with an error term in Equation 6.2 (for further details, see Web Appendix E).It is also plausible that MROB is endogenous such that there are systematic differences between firms that appoint a marketer to their boards and those that do not. We estimated a firm's decision to have MROB ([70]) and used board-related variables (peer firm mean MROB, mean board age, chief marketing officer [CMO] on the top management team [TMT], mean board tenure, board size, chief executive officer [CEO] duality, and female percentage) as our exclusion restrictions. We estimated the following random-effects probit model to produce an MROB IMR to include in our main estimation (for the results of Equation 5, see Table E.1 in Web Appendix E): PresenceofMROBit=δ0+δ1PeerFirmMeanMROBit+δ2MeanBoardAgeit+δ3CMOonTMTit+δ4MeanBoardTenureit+δ5BoardSizeit+δ6CEODualityit+δ7FemalePercentageit+δ8Advertising/Salesit+δ9R&D/Salesit+δ10FirmSizeit+δ11IndustryGrowthit+δ12MarketShareGrowthit+δ13–34YearDummiest+υi+εit. Graph( 5)To test H2 and H3, we estimated a SUR model with moderated mediation by estimating the effect of an M&A on attention to customers (vs. finance) (Equation 6.1) and the effect of the latent instrumental variable,  Attention toCustomers vs.Finance⌢  , on CSAT (Equation 6.2). We included time dummies and firm fixed effects to account for unobservable characteristics and an MROB IMR to control for selection bias. We winsorized our continuous variables and estimated the following models: AttentiontoCustomers(vs.Finance)it=β0+β1M&AGroupj+β2Post-M&At+β3(M&AGroupj×Post-M&At)+β4MROBit+β5(MROBit×M&AGroupj×Post-M&At)+β6MROBIMRit+β7(MROBit×M&AGroupj)+β8(MROBit×Post-M&At)+β9MarketSharei(t−1)+β10ROAi(t−1)+β11FirmSizei(t−1)+β12(Advertising/Sales)i(t−1)+β13(R&D/Sales)i(t−1)+β14Segmentsi(t−1)+β15MarketGrowthit+β(firmcount)FirmFixedEffects+β(t=2,3)TimeEffects(6.1)+εit, Graph(6.1) CSATit=π0+π1M&AGroupj+π2Post-M&At+π3(M&!AGroupj×Post-M&!At)+π4Attention toCustomers vs.Finance⌢it+π5MROBit+π6(MROBit×M&!AGroupj×Post-M&!At)+π7AttentiontoCustomers(vs.Finance)Residualit+π8MROBIMRit+π9(MROBit×M&!AGroupj)+π10(MROBit×Post-M&At)+π11MarketShare(t−1)+π12ROAi(t−1)+π13FirmSizei(t−1)+π14(Advertising/Sales)i(t−1)+π15(R&DSales)i(t−1)+π16Segmentsi(t−1)+π17MarketGrowthit+π(firmcount)FirmFixedEffects+π(t=2,3)TimeEffects+εit, Graph(6.2)where β3 captures the impact of M&As on executive attention to customers (vs. finance) a year after the M&A and π4 captures the impact of executive attention to customers (vs. finance) on CSAT, which allows us to test H2. β5 captures the moderating impact of MROB on the relationship between the M&A activity and executive attention to customers (vs. finance), which allows us to test H3. ResultsWe present model-free evidence of the relationship between M&A activity and executive attention to customers (vs. finance) in Figure 3. M&A firms experience a decline in executive attention to customers relative to financial issues. For example, from our sample we see that for United Airlines, executive attention to customers (vs. finance) declined 38% because of its acquisition of Continental Airlines and its customer satisfaction declined 8.2%.Graph: Figure 3. Executive attention to customer versus finance trends.When we compared a change in executive attention to customers (vs. finance) from two years before an M&A with the year before, the difference between the M&A and non-M&A groups was not significant (t = 1.54, p > .10). In contrast, when we compared a change from a year before the M&A with the year after, the M&A firms experienced a decline in executive attention to customers (vs. finance), whereas the non-M&A firms experienced a slight increase (ΔM&A Attention to Customers [vs. Finance][t + 1, t − 1] = −.03; ΔNon-M&A Attention to Customers [vs. Finance][t + 1, t − 1] = .01, p < .01). For the M&A firms with MROB, they experienced an increase in executive attention to customers (vs. finance) from the year before an M&A to the year after, whereas for those without MROB, they experienced a decrease (ΔM&A with MROB Attention to Customers [vs. Finance][t + 1, t − 1] = .01; ΔM&A without MROB Attention to Customers [vs. Finance][t + 1, t − 1] = −.04, p < .05). Consistent with this trend is the fact that for one of our sample firms, Macy's, it engaged in M&As in 2015 while having MROB. Macy's executive attention to customers (vs. financial issues) increased by 5.86% from the year before the M&As to the year after. Not surprisingly, Macy's did not experience any decline in customer satisfaction during this period.We present the estimation results for executive attention to customers (vs. finance) (Model 1a) and customer satisfaction (CSAT; Model 1b) in Table 8. We find that M&A activity is associated with lower executive attention to customers (vs. finance) (β3 = −.032, p < .05, Model 1a) and executive attention to customers (vs. finance) is associated with higher customer satisfaction (π4 = 1.423, p < .01, Model 1b). Still, the effect of M&As on customer satisfaction persists with the inclusion of the mediator, executive attention to customers (vs. finance) (π3 = −.764, p < .05, Model 1b), which suggests partial mediation. Its mediating impact persists when we incorporate the MROB interaction terms (Model 2b). Based on Model 2b, the indirect effect of M&As on customer satisfaction through executive attention to customers (vs. finance) is negative and significant (β3[Post-M&A Year and M&A group] × π4[Executive attention to customers (vs. finance)] = −.046, confidence interval = [−.144, −.000]). Thus, in support of H2, customer dissatisfaction from M&As is due, in part, to a shift in executive attention away from customers and toward financial issues.GraphTable 8. Executive Attention to Customers (vs. Finance) and MROB. Dependent Variables(1a) Executive Attention to Customers (vs. Finance)t(1b) CSATt(2a) Executive Attention to Customers (vs. Finance)t(2b) CSATtFocal VariablesConstant.301***67.024***.300***67.218***(.033)(.929)(.032)(.899)M&A Group.007−.310.012−.581**(.007)(.223)(.009)(.263)Post-M&A Year.004.452***.006.461**(.004)(.157)(.005)(.199)M&A Group × Post-M&A Year−.032** (H2)−.764**−.043***−.716**(.014)(.342)(.016)(.311)Executive Attention to Customers (vs. Finance)1.423***1.447***(.541)(.554)Marketing Representation on the Board (MROB).756***48.292***.838***43.301***(.094)(8.202)(.118)(8.300)MROB × M&A Group × Post-M&A Year.623*** (H3)−2.617(.241)(12.285)CovariatesExecutive Attention to Customers (vs. Finance) Residuals1.430***1.486***(.434)(.452)MROB IMR−.005**−.396***−.006**−.399***(.003)(.048)(.003)(.048)MROB × M&A Group−.31224.956***(.257)(7.398)MROB × Post-M&A Year−.130−.655(.181)(3.430)Market Share(t − 1)−.449***–3.134***−.449***−3.229***(.055)(1.040)(.056)(1.016)ROA(t − 1).322***12.470***.328***12.105***(.073)(1.461)(.074)(1.517)Firm Size(t − 1).000.000***.000.000***(.000)(.000)(.000)(.000)Advertising/Sales(t − 1).032−.124.032−.129(.020)(.573)(.020)(.573)R&D/Sales(t − 1).024−3.888***.022−3.837***(.042)(1.131)(.043)(1.129)Segments(t − 1).035***−.699**.035***−.710**(.013)(.300)(.013)(.302)Market Growtht−.065**−2.502***−.062**–2.493***(.032)(.731)(.031)(.736)Firm fixed effectsIncludedIncludedIncludedIncludedTime effectsIncludedIncludedIncludedIncludedModel Informationχ2χ2(135) = 9,330.54***χ2(137) = 8,150.45***χ2(138) = 9,351.80***χ2(140) = 8,195.91***R2.791.768.791.769Number of firms122122122122Total observations2,4682,4682,4682,468 18 *p < .10.19 **p < .05.20 ***p < .01.21 Notes: CSAT = customer satisfaction; ROA = return on assets; R&D = research and development. We report parameter estimates with bootstrapped standard errors in parentheses.MROB is associated with more executive attention to customers (vs. finance) (β4 = .756, p < .01; Model 1a) and higher customer satisfaction (π5 = 48.292, p < .01; Model 1b). Further, in support of H3, MROB reduces the negative impact of M&As on executive attention to customers (vs. finance) (β5 = .623, p < .01, Model 2a) and executive attention to customers (vs. finance) is associated with an increase in customer satisfaction (π4 = 1.447, p < .01, Model 2b).[ 7] Robustness tests (Web Appendix F)We reestimated Equations 6.1 and 6.2 by adding an industry-level control variable to capture business-to-business versus business-to-customer membership, and our results do not change (Tables F.1 and F.2). We estimated firm value, firm efficiency, CSAT, and executive attention to customers (vs. finance) as a system of equations (Table F.3). Our effects of interest stay consistent when we use a four-equation model. We find robust support for our hypotheses. DiscussionWhile there has been significant research on customer satisfaction and a stream of research on M&As and financial performance, prior studies have not connected these two streams. We situate our research on this intersection and draw on the two complementary theories of the ABV of the firm and the upper echelons theory to examine the influence of M&A activity on a key, but often overlooked, stakeholder: customers. Theoretical ContributionsPrior marketing strategy research has largely overlooked how disruptive corporate transformations can be for customers. Further, it has overlooked a key pathway between M&A activity and firm value: customer satisfaction. Some empirical work (e.g., [62]) has examined the interplay between M&A activity and customer satisfaction by treating customer satisfaction as a moderator and speculated (but not formally tested) that M&As enable a dual emphasis of firm efficiency and customer satisfaction. In contrast, we show that M&As not only do not enable a dual emphasis but also cause a decline in customer satisfaction to the extent that they outweigh any gain in firm value from firm efficiency. Thus, we add to previous work on firms' dual emphasis (e.g., [43]) but show that M&A activity works against a dual emphasis of firm efficiency and customer satisfaction.We examine heterogeneity in the decline in customer satisfaction with novel conceptual additions to the M&A and customer satisfaction literature streams: executive attention to customers versus finance and MROB. We address ongoing calls to increase marketing's profile in the C-suite and higher (e.g., [24]; [46]; [70]) by examining how marketing leadership at the top of a firm redirects executive attention to customer issues, which explains differences in customer outcomes of M&As. In doing so, we add to the limited research on marketing presence in the upper echelons (e.g., [ 9]; [70]) by examining its role in channeling executive attention during M&As.Existing research in marketing has overwhelmingly used the RBV of the firm to examine outcomes of M&As. This view, which emphasizes capabilities, fails to consider executive attention ([75]); however, executive attention is a precursor to resource investments. Further, although the ABV considers executive attention, it has primarily focused on the effect of supply-side (vs. demand-side) factors that influence managerial attention (e.g., [48]; [75]). In contrast, we extend the ABV to study marketing strategy phenomena in general, and a crucial demand-side stakeholder—customers—in particular. This aligns with newer research (e.g., Vadakkepatt et al. 2021) that aligns this theory with customer outcomes.We contribute work on the marketing–finance interface. We introduce executive attention to customers versus financial issues as a mediator of the relationship between firm strategy (M&As) and a market-based asset (customer satisfaction). We find that during M&As, executives focus on financial issues at the cost of customer issues but that MROB can help mitigate this. Thus, we add a nuanced understanding the role of top leadership in navigating the marketing–finance interface.We contribute to the literature on firm-level drivers of customer satisfaction (e.g., [50]; [56]) by examining a previously ignored antecedent: M&A activity. By showing that M&As negatively impact customer satisfaction, we shed light on how higher-level strategic actions that are often motivated by shareholder motives can risk the marketing function's most prized asset, its customer relationships. Finally, we add to growing research in marketing on the use of observational inference to document the causal effects of strategic decisions. Managerial ContributionsM&As have, on average, been shown to produce adverse financial effects. This has been attributed to overpayments as a result of optimism regarding synergies and cost efficiencies. However, we suggest that firms pay a price for dissatisfying customers during the M&A process, and in fact, this effect persists two years post-M&A. This finding is critical given that a recent survey of managers suggests that expanding a firm's customer base is a primary motivation for M&As. Thus, ignoring the dysfunctional effect of M&As on customers has serious long-term financial consequences and is inconsistent with firms' M&A objectives. To demonstrate the financial impact of the M&As due to a decline in customer satisfaction, we compared the firm value of M&A and non-M&A firms due to differences in customer satisfaction and firm efficiency with the estimation results from Table 3. Compared with that of non-M&A firms, the customer satisfaction of M&A firms was 1.14% lower a year after the M&A. In contrast, compared with that of non-M&A firms, the efficiency of M&A firms was.29% higher in the same period. When we incorporated these estimates in the firm value model (Equation 1.3), we found that the value of M&A firms was 2.43% lower than the non-M&A firms a year after the M&A. To calculate a change in firm value, we multiplied the percentage difference in value between M&A and non-M&A firms by the average firm value of the firms one year after an M&A. We find M&A firms' market value was worth $481 million less than that of the non-M&A firms. Although firms may be motivated to pursue an M&A to exploit scale-related synergies that provide cost-benefits, we show that efficiency gains fail to compensate for customer dissatisfaction-related financial losses. Thus, it is critical for managers responsible for M&As and industry consultants to include a consumer impact assessment in their M&A checklists.Although there are several competing needs that require executives' attention during an M&A process, it is essential for them to allocate some of their attention to customer-related issues. The financial payoff of such attention is meaningful. To demonstrate the impact of executives of M&A firms paying attention to customers despite their tendency to focus on financial issues, we computed the percentage difference in customer satisfaction between M&A firms whose executives pay more attention to customers (vs. finance) (1 SD above the mean) and M&A firms whose executives pay less attention to customers (1 SD below the mean) with the results from Table 8 (Column 2b). Then, we used this percentage difference in customer satisfaction (.46%) to calculate a difference in firm value with the estimates from Table 3. We find that M&A firms that pay more attention to customers relative to financial issues experience 45% reduction in loss in firm value from the M&A (−1.34% vs. −2.43%). Thus, executive attention to customers can help firms significantly reduce M&As' damaging effects on customer satisfaction and firm value.Moreover, MROB can attenuate a decline in customer satisfaction and, thus, increase firm value. In our data, 27.34% of the firms had MROB at some point during the 1995–2017 sample period. To illustrate, we calculated the firm value impact of adding one marketing title to the board with the estimation results from the moderated-mediation analysis that we report in Table 8. First, we computed the percentage difference in customer satisfaction between M&A firms with no MROB and M&A firms with just one person with a marketing title on the board in the post-M&A year with the results from Column 2b of Table 8, which is 2.85%. We then used this increase in customer satisfaction from MROB to calculate firm value with the estimates from Table 3 and find that the value of a firm with just one person with a marketing title on the board in the post-M&A year was 4.28% higher compared with firms that did not have any MROB. Adding these board positions is not trivial, especially during an M&A process. However, the financial consequence of not having MROB during M&As is severe. Thus, we make the case for marketing's voice in the C-suite, which is an important MSI Tier 1 Research Priority for 2020–2022. Limitations and Directions for Future ResearchLimiting dissatisfaction from M&As is a complex task, and multiple antecedents, including deal and integration-related factors and firm-level variables that speak to other functions of the firm, should be considered. Recent research has also found that customer satisfaction has a direct positive effect on firm efficiency ([ 7]), and future research could explore this pathway in the context of M&As. Further, our sample size was limited to what the ACSI database of customer satisfaction could provide. Future studies could identify alternative data sets ([39]) to enlarge their sample to extend the time frame of the panel and data frequency to examine changes in satisfaction several years after the M&As. In addition, we study customer satisfaction with the ACSI scores of acquirer firms and not target firms. This seems reasonable given that target firms are subsumed in acquiring firms, so any post-M&A ACSI score should reflect customers of both firms. Still, future research might benefit from assessing changes in satisfaction for the target firm. The challenge is that most Compustat and ACSI data are unavailable for the target firm after it has been acquired. Alternative data sources, which include primary data on customer satisfaction at the business unit level could be a solution. Finally, we empirically show that the ABV of the firm is a viable theoretical mechanism to explain the effect of M&As on customer satisfaction and how MROB moderates this relationship. Still, a change in executive attention is one of many potential pathways from M&A activity to customer satisfaction. Future research could consider how the RBV compares with the ABV in explaining these effects.  "
14,"Do Backer Affiliations Help or Hurt Crowdfunding Success? Crowdfunding has emerged as a mechanism to raise funds for entrepreneurial ideas. On crowdfunding platforms, backers (i.e., individuals who fund ideas) jointly fund the same idea, leading to affiliations, or overlaps, within the community. The authors find that while an increase in the total number of backers may positively affect funding behavior, the resulting affiliations affect funding negatively. They reason that when affiliated others fund a new idea, individuals may feel less of a need to fund, a process known as ""vicarious moral licensing."" Drawing on data collected from 2,021 ideas on a prominent crowdfunding platform, the authors show that prior affiliation among backers negatively affects an idea's funding amount and eventual funding success. Creator engagement (i.e., idea description and updates) and backer engagement (i.e., Facebook shares) moderate this negative effect. The effect of affiliation is robust across several instrumental variables, model specifications, measures of affiliation, and multiple crowdfunding outcomes. Results from three experiments, a survey, and interviews with backers support the negative effect of affiliation and show that it can be explained by vicarious moral licensing. The authors develop actionable insights for creators to mitigate the negative effects of affiliation with the language used in idea descriptions and updates.Keywords: backer affiliation; crowdfunding; prosocial; social structure; vicarious moral licensingCrowdfunding has emerged as a dominant mechanism to harness the power of crowds in raising funds for innovative ideas. Interest in crowdfunding has surged in recent years. Facebook acquired Oculus 3D visualization device, a crowdfunded idea on Kickstarter, for US$3 billion ([14]). Peloton, the highly successful exercise bike, started as a Kickstarter project. The global crowdfunding market is expected to be well over US$40 billion by 2026 ([54]). Brands such as GE ([11]) and Unilever use crowdfunding to spur innovation ([53]), and academic research on the phenomenon and its role in the digital economy is emerging ([ 2]; [12]). Crowdfunding is a form of crowdsourcing in which participants, hereinafter referred to as ""backers,"" are recruited to raise funds for ideas (e.g., [16]; [62]). As some backers fund the same ideas (i.e., ""cobacking""), overlaps develop between these backers. These overlaps, called ""affiliations,"" are key building blocks of the community's network structure and have been studied in other crowdsourcing communities (e.g., [48]). In this research, we explore how affiliation, defined as the number of cobacking relationships between potential backers and those who have previously funded the focal idea, might affect the idea's crowdfunding success. We illustrate affiliation in crowdfunding using a stylized example in Figure 1.Graph: Figure 1. Illustration of affiliation in crowdfunding.We know that crowd size affects outcomes positively as participants look to anonymous others for cues to decide which ideas to fund, a phenomenon referred to as ""herding"" (e.g., [66]). Previous research shows that attracting more backers positively impacts crowdfunding outcomes ([24]), an insight that many creators seem to grasp. However, crowd size does not represent the social structure (i.e., the pattern of connections in the community). In crowdfunding, as in other contexts in which shared communal goals exist (e.g., Wikipedia), social structure plays a more prominent role (e.g., [48]; [62]).Our primary contribution is in showing that while the total number of backers (i.e., crowd size) may positively affect funding behavior and idea success (e.g., [66]), adding backers may not be unilaterally beneficial as the ensuing affiliation between backers negatively affects funding. Our analysis reveals that the negative effect of backer affiliation is above and beyond the positive effect of number of backers (i.e., the herding effect), highlighting the tension between the benefits of adding more backers and the adverse effects of backer affiliation. In other words, while adding a new backer (e.g., the focal backer in Figure 1) may positively affect the focal idea's success, adding this focal backer may not be equally beneficial across the three scenarios in Figure 1 as the degree of affiliation differs. We propose that the affiliation between the focal backer and other backers will influence the amount that the focal backer puts toward the focal idea and, thus, the idea's funding success.Affiliation is a powerful force because it makes affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]). In some contexts, affiliation positively affects behavior as individuals desire to belong and therefore conform to affiliated others' behavior (e.g., [32]). However, in crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that such affiliation can negatively affect behavior. When individual actions benefit a social cause, seeing affiliated others participate may make individuals feel less of a need to do so, a process referred to as ""vicarious moral licensing"" (e.g., [13]; [22]; [37]). Thus, we propose that when backers decide whether to fund an idea, they are less likely to do so or more likely to fund a lower amount if affiliated others have already done so.While affiliations develop in the community through cobacking, creators and backers also engage through nonmonetary actions, thereby driving social interaction. Therefore, to develop further substantive implications about the effect of affiliation, we examine the moderating role of both creator and backer engagement (e.g., [ 4]; [35]). For example, creators communicate with backers through the description of the idea on its homepage, perceived to be an important determinant of an idea's success ([38]; [64]), and by posting updates about progress. Backers engage with the community by sharing ideas on social media. We aim to understand how the effect of affiliation varies due to creator and backer engagement, as they help shed light on the underlying mechanism that drives the effect of affiliation.We use multiple methods and data sets, including secondary data and experiments, to provide convergent validity to our findings. We also conduct interviews with 6 backers, survey 100 backers, and analyze 572 posts on backer forums to develop insights about the mechanism driving the effect of affiliation on funding outcomes. First, we assemble a comprehensive data set of daily funding for 2,021 new crowdfunded ideas listed on Kickstarter. We study two crowdfunding outcomes: ( 1) the monetary amount of funding received by an idea on any given day and ( 2) whether the idea raises sufficient funds during the funding window to meet or exceed its funding goal. We measure affiliation of an idea on the focal day as the number of cobacking relationships of backers who back on the focal day, with backers who funded until the day before the focal day (e.g., [35]; [41]). We estimate an instrumental variables regression model with fixed effects to assess the impact of affiliation among an idea's backers on the daily funding amount and report results from several robustness analyses. Second, we present results from controlled experiments, where we exogenously manipulate affiliation, and across three experiments, we replicate the negative effect of affiliation on funding, examine the underlying mechanism, and uncover the role of a key moderator. We find that the negative effect is stronger when creators use more communal words—both in the initial description of the idea and in subsequent updates—and when more backers share the idea on social media. Thus, creator and backer engagement may moderate prosocial motives to fund, further validating the proposed licensing mechanism.We make several contributions. We are the first to show that affiliation among backers affects crowdfunding success in statistically and economically significant ways after controlling for herding and accounting for several alternative explanations. A 10% daily increase in number of backers would lead to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation would lead to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by about 43%. Thus, adding backers is good, but if the additional backers increase affiliation, the positive effect of adding these backers is smaller in the scenario where affiliation is high. We isolate vicarious moral licensing as a theoretical mechanism that drives the negative effect of affiliation through experiments. We explore the role of factors related to the idea, the creator, and the backers, all of which interact with affiliation. Theoretical Background Social Influences in CrowdfundingAlthough crowdfunding has emerged as a dominant force for funding new ideas, research on crowdfunding is limited. Most early research focuses on microlending ([34]; [66]) or on crowdfunding platforms for music and journalism (e.g., [ 1]; [ 8]). Topics such as proximity to the deadline ([12]) and the text of content (e.g., [42]) have also garnered attention. Researchers have studied a variety of social factors that influence crowdfunding, in particular, the relationship between creators and individual backers, including the role of offline friendship ([34]), geographic proximity ([ 1]), and social interactions ([28]). We present a summary of representative research in Table 1.GraphTable 1. Comparison with Relevant Empirical Research. References (Published)Dependent VariablesExplanatory VariablesEmpirical Model FeaturesData ContextExperimentsFindingsOur researchFundingAffiliationMediator: vicarious moral licensingModerators: creator and backer engagementFixed-effects log-linear regressionEndogeneity (instrument)Robustness checks: probit, logit, and TobitCrowdfunding(Kickstarter, experiments)Three experimentsPrior backer affiliation decreases funding; the negative effect is due to vicarious moral licensing. This effect is stronger for ideas with communal descriptions, more communal updates, and more backer sharing on social media.Wei, Hong, and Tellis (2021)Success of fundingSimilarity between ideasNetwork similarityBinary regressionCrowdfunding(Kickstarter)NoPrior success of similar ideas affects success. Funding performance increases as an idea's novel and imitative characteristics are balanced. The optimal funding level is closer to the level of similar ideas.Netzer, Lemaire, and Herzenstein (2019)Loan paybackLoan descriptionText analyticsBinary regressionCrowdlending(Prosper)NoBorrowers who use certain types of words are more likely to default.Dai and Zhang (2019)Funding time elapsedGoing past deadlineProsocial motivationCreator is individualRegression continuityCrowdfunding(Kickstarter)NoBackers might be driven by prosocial motives around deadline following goal pursuit.Kim et al. (2020)Goal completionForward lookingSocial interactionsBayesian IJC methodTwo-stepCounterfactualsCrowdfunding (music)(Sellaband)NoForward-looking investment behavior as well as contemporaneous and forward-looking social interactions impact share purchases and goal completion.Burtch et al. (2013)ContributionsConcealmentSocial normsTobitEndogeneity (IV)Crowdfunding(Data set undisclosed)NoConcealment hurts the likelihood of contribution and contribution. Social norms drive concealment.Agrawal, Catalini, and Goldfarb (2015)Decision to investGeographyLinear regressionFixed effectsCrowdfunding (music)(Sellaband)NoLocal backers are not influenced by artist. The effect does not persist past the first investment, indicating the role of search but not monitoring.Burtch et al. (2013)Contribution frequencyCrowdingFunding windowDegree of exposureLog-linear regressionEndogeneity (GMM)Crowdfunding (journalism)(Data set undisclosed)NoPartial crowding-out effect. Backers experience lower marginal utility of giving as the funds become less relevant to the recipient. The funding window and degree of exposure have a positive effect, after publication of the story.Lin, Prabhala, and Viswanathan (2013)Interest rate, default rateFriendshipsProbit regressionCrowdlending(Prosper)NoOnline friendships act as signals of credit quality, increase the probability of funding, lower interest rates, and result in lower ex post default rates—gradation in effects based on roles and identities of friends.Zhang and Liu (2012)Loan amountsCrowdingHazard modelFixed effectsFirst differencesCrowdlending(Prosper)NoWell-funded borrowers attract more funding. Lenders learn from peer decisions and do not mimic. 1 Notes: IV = instrumental variable, GMM = generalized method of moments; IJC = [26].In addition to the relationship between creators and backers, there are several ways in which others' actions might inform backers' funding decisions. For example, [66] report that potential lenders assess borrowers' creditworthiness by observing other lenders. They attribute the positive effect of the number of other lenders to herding, wherein crowd size becomes a beacon for others to decide which ideas to fund. This finding might suggest that the mere addition of more supporters unilaterally benefits crowdfunding outcomes as potential backers simply follow other backers. What are some factors that might limit the positive impact of the crowd's behavior on crowdfunding? To answer this question, we note that most research has considered the presence of the anonymous crowd as the cause for a social effect that is generally positive. However, crowd size does not account for an important aspect of networks (i.e., the structure of connections among the community's participants).Thus, what is missing in extant research is an explicit acknowledgment of social structure beyond crowd size and an exploration of how it impacts crowdfunding outcomes. Social structure arises due to coparticipation in events, in our case, cobacking across ideas, a phenomenon referred to as affiliation (e.g., [17]; [60]). Affiliation, identified as an important phenomenon in the new digital economy dominated by crowdsharing ([15]), is the central focus of our research. Affiliation in CrowdfundingCommunities evolve through repeated interactions between members, which give rise to affiliations or overlaps. As affiliations grow, the interconnectivity among backers leads to scaffolding structures that hold the community together through both first- and second-order ties. Affiliations have been studied in interfirm relationships ([58]), board interlocks ([52]), product development (e.g., [35]), and wiki contributions ([48]). Regardless of the context, research suggests that ( 1) individuals notice affiliated others' behavior, ( 2) individuals feel a sense of connectedness and shared identity with affiliated others, and as such, ( 3) affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]).To establish that participants notice affiliated others' behavior when visiting crowdfunding platforms, we ran a pilot study with actual backers prescreened on the basis of their prior crowdfunding behavior. Participants were shown a screenshot of a crowdfunding page created by a web designer. To assess which information captured participants' attention, we used a standard heat-mapping approach for measuring visual attention ([ 5]). Invisible boxes around various pieces of information (e.g., idea title, backer information, idea description) coded visual attention as participants read and clicked on information, as per instructions. We found that many participants read and clicked on backer information, more so than other potentially relevant information such as the number of shares and creator information. Further, of the available backer information, affiliation ranked as highly important (for details, see Web Appendix A). Discussions on crowdfunding message boards and websites, as well as results from a survey that we conducted (discussed in the following sections), further support this idea, suggesting that among all available information, backers do consider affiliated others' behavior as they make funding decisions. Next, to confirm that affiliation affects perceptions of connectedness and shared identity in crowdfunding communities, we ran a pilot study with 150 Amazon Mechanical Turk (MTurk) participants. We find that affiliation significantly increased perceptions of connectedness and shared identity with other backers (see Web Appendix B).If potential backers notice affiliated others' behaviors and feel a sense of connectedness with these affiliated others, how might affiliated others' behavior influence their own funding decisions? To answer this important question, we next examine crowdfunding platforms and how they differ from noncommunal (i.e., transactional) contexts. Vicarious Moral Licensing in Crowdfunding CommunitiesCrowdfunding is a communal endeavor in which individuals collaborate to achieve shared goals, and platforms grow due to members' participation and interactions ([50]). Individuals behave differently in communal contexts than in noncommunal (i.e., transactional) contexts (e.g., [10]; [18]). For example, in communal contexts, individuals are more likely to request help from others, keep track of others' needs, respond to them, and report more positive emotions while doing so ([18]). In crowdfunding, these prosocial goals are reflected in the desire to help others, achieve funding goals, and be part of a community ([19]).In crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that seeing affiliated others fund may make individuals feel less of a need to do so, a process referred to as vicarious moral licensing (e.g., [13]; [22]; [37]). Vicarious moral licensing occurs when individuals see affiliated others' actions as satisfying their own goals, which changes their perceived moral imperative and subsequent behavior. For example, learning that affiliated others demonstrate environmentally friendly behavior makes individuals less likely to do so ([37]). It is important to recognize that this effect is not merely akin to strangers' behavior in a crowd (i.e., the bystander effect; e.g., [31]), but that it is those with whom an individual perceives a social connection (i.e., affiliation) that drives the focal effect.To confirm the importance of affiliated others' behavior and further validate the proposed mechanism, we conducted in-depth interviews of 6 backers, surveyed 100 backers, and coded 572 posts from KickstarterForum.org, the dominant crowdfunding discussion forum (see Web Appendix C). The findings confirmed the prominence of prosocial (i.e., communal) motives on crowdfunding decisions, the importance of affiliated others' behavior on backers' own funding behavior, and the role of vicarious moral licensing. For example, as one interview participant explained, ""I look at other funders only to further discover related projects. It's an interesting way to discover—because some people are more involved than you are.... It's interesting to follow that rabbit trail and see, 'Oh, this person supported this, and look at what else they fund.'"" Another stated, ""You are dealing with finite resources in terms of what you are willing to spend. If you support one thing, I don't know, for me, if I see someone supporting something else, I think, well yeah, they supported that. I'm sure I could find a bunch of other people that support a bunch of other things. I just gave X amount of dollars, whatever amount I have, and I'm not going to be giving any more than that right now.""Although we propose vicarious moral licensing as the mechanism underlying the focal effect of affiliation and initial evidence indicates this to be the case, we acknowledge the complexity of social interactions in crowdfunding. Because these social interactions are likely to be subject to several factors, we consider uniqueness as an alternative explanation for the negative effect of affiliation on funding. Backers may try to identify ideas that have received less funding from affiliated others. By doing so, backers can distinguish themselves from these affiliated others, fulfilling a need for uniqueness (e.g., [59]). In our analysis, we report results from an experiment where we test vicarious moral licensing and uniqueness as potential explanations for the negative effect of affiliation on funding. Moderators of the Effect of Affiliation on Crowdfunding SuccessCrowdfunding platforms are characterized by contributions from both creators and backers (e.g., [ 4]; [48]) as these interactions create and sustain the community's viability. Therefore, we explore the role of creator and backer engagement in moderating the impact of affiliation on crowdfunding.Previous research has found that while prosocial goals may be common in crowdfunding platforms (e.g., [50]), an idea's description can further induce prosocial motivation and behavior when it emphasizes communal language ([23]). We propose that the vicarious moral licensing effect (i.e., the negative effect of affiliation on funding) is driven by the communal context and the prosocial behavior it prompts and that this behavior is further heightened by creators describing their ideas with communal words like ""together"" and asking backers to ""partner"" with them by providing financial ""support"" ([47]). As such, ideas described with more (vs. less) communal words will exhibit a stronger negative effect of affiliation on funding outcomes.Creators can also engage with the backer community by posting updates to highlight their strategic goals and the idea's progress. Updates provide diagnostic information concerning an idea's success (e.g., [ 4]; [35]). Updates might draw backers' attention to the idea's characteristics and evolution, and lessen attention toward cobackers and affiliation. Consistent with the vicarious moral licensing mechanism, we expect updates that use more (vs. less) communal words to strengthen affiliation's negative effect. As such, we estimate the moderating effects of communal words in the creator's updates.We also explore how backers' engagement might moderate the affiliation effect by exploring the role of social media sharing of the focal idea by backers. While sharing behavior on social media could have several motivations, altruism is perceived as a primary motivator, and others seeing the shares likely view them as such ([29]; [33]). We expect that such sharing heightens funders' prosocial motives and vicarious moral licensing, further strengthening the negative effect of affiliation. Next, we describe our data and methodology. Data and MethodologyWe employed a multimethod approach to investigate the phenomenon. We collected and analyzed two types of data: observational data from a crowdfunding platform and experimental data from lab settings. We begin by describing the observational data, the empirical model and identification strategy, the results, and robustness checks. Then, we describe three experiments in which we identify the primary effect in a controlled setting and shed light on the mechanism underlying the primary effect and its moderator. The first experiment demonstrates the negative effect of affiliation on funding behavior. The second experiment validates vicarious moral licensing as an underlying mechanism and rules out uniqueness as one potential alternative explanation. The third experiment examines how the idea's description moderates the effect of affiliation. Collection and Analysis of Observational DataWe collected data on Kickstarter, the world's largest and most prominent crowdfunding platform. We utilized a web crawler to visit the new ideas page listed on Kickstarter beginning December 18, 2013. From that day and every subsequent day of data collection, the crawler visited the pages of the ideas that were started on the first day of the crawl, in addition to all the ideas that were started on the subsequent days. We stopped the crawler after 37 days, giving us data on 2,021 new ideas. We acknowledge that our research's funding constraints affected the number of days, but we went one week past the most common deadline of 30 days. We note that while some ideas in our sample received funding after data collection stopped, our results are robust to this truncation.[ 7]For the data collection, the crawler began with ideas that started receiving funds on the day of the crawl, and it identified every backer who funded the focal idea, the funded amount, and the calendar date. The crawler then visited every backer's history and collected information on all the other ideas that the backer had funded in the past. At the time of data collection, Kickstarter made all backers visible to all prospective backers. The list of backers on Kickstarter was available by clicking the ""community"" link that prominently appears on the focal idea's web page.[ 8] This process allowed us to construct the network, giving us the structure of relationships to calculate affiliation. The crawler also collected other relevant information from the page, including the idea's description, number and text of updates, and the number of Facebook shares of the idea to measure backer engagement.Our unit of analysis for the daily amount funded is an idea-day, and our final sample had 32,438 observations at the idea-day level. This specification makes the most sense because, for a data set with idea-day-backer as the unit of analysis, the funded amount (for an idea on a day) takes zero values for over 99.9% of observations, making such a specification noninformative. Next, we describe the key measures. Daily amount fundedConsistent with prior literature ([ 1]; [ 8]), our funding success measure is the amount of funding received by an idea on any given day. Across all crowdfunding platforms, this measure is always easily and prominently visible on the idea's webpage. Subsequently, we show that our results are robust to other measures of success. AffiliationConsistent with prior literature (e.g., [35]; [41]), we posit that two backers are affiliated if they have funded at least one common idea on the platform and are not affiliated if all the ideas that they have funded are mutually exclusive. Thus, the backer affiliation for a focal idea on a focal day is the number of cobacking relationships between those backers who fund the focal idea on the focal day and all backers who have funded the focal idea at any time before the focal day.Consider a backer of a focal idea who funds the focal idea on the focal day. Consider another backer of the focal idea who funds the focal idea any time before the focal day. A cobacking relationship exists between these two backers if they have both funded one idea (other than the focal idea) any time before the focal day. One cobacking relationship represents one unit of affiliation. Affiliation increases both with the number of backers who coback and with the number of cobacked ideas.To elaborate, consider the following examples. In each example, idea i is launched on day t = 1, say December 13. Further, Jack funds idea i on December 17 (t = 5), and the goal is to calculate affiliation as of December 17 (t = 5).Example 1: Tom funds idea i on December 13. Also, before December 17, Jack and Tom both fund another idea j. As there is one cobacking relationship (that between Jack and Tom for cobacking idea j), Affiliationi, t = 5 = 1.Example 2: Tom funds idea i on December 13. Jack and Jill both fund idea i on December 17. Before December 17, Jack and Tom both fund another idea j. Furthermore, before December 17, Jill and Tom both fund another idea k. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jill and Tom for cobacking idea k), Affiliationi, t = 5 = 2.Example 3: Tom funds idea i on December 13. Jane funds idea i on December 14. Before December 17, Tom, Jack, and Jane funded another idea j. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jack and Jane for cobacking idea j), Affiliationi, t = 5 = 2.We present summary statistics for Kickstarter in Table 2. The median number of backers who fund an idea in a day is 1, and most ideas have only a few backers. When a backer funds an idea, the median number of past backers of that idea is 10 backers (i.e., the median of the variable cumulative number of backers funding idea i before day t is 10). In the six months preceding data collection, 82% of backers in our data had not funded any idea on Kickstarter. Thus, the odds of having to remember multiple cobacking relationships are relatively low. Most importantly, the median value of affiliation is zero, and the mean is 3.3. In other words, a large majority of backers in our data must process a very small amount of information to infer affiliation. Our measure of affiliation reflects a more nuanced and disaggregated conceptualization of affiliations than the number of ""cobacked ideas"" or the number of ""common backers."" Other measures are likely sparser than our measure. Subsequently, we show that our results are robust to alternate measures of affiliation.GraphTable 2. Summary Statistics for Kickstarter. VariableMeanSDMinMedianMaxAmount of funding of idea i (in $) on day t409.344,764.7200593,731Backer affiliation of idea i by day t  −  1 (Affilit  −  1)3.3310.6600477Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1)53.85357.8701017,018Number of backers funding idea i on day t − 1 (Backersit − 1)6.71142.990117,010Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)8.8223.6400524Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).492.140.13132.63Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).31.240.27.97Closeness centrality of idea i as of day t − 15.67 × 10 − 95.01 × 10 − 83.49 × 10 − 112.50 × 10 − 102.2 × 10 − 6Betweenness centrality of idea i as of day t − 11,258.078,698.4900335,213Eigenvector centrality of idea i as of day t − 1.002.0302.50 × 10 − 91Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).08.28001Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)20.171,696.2700230,232  UpdatesWe measure the creator's engagement using the number of creator's updates on the idea page and separately measure the level of communal content in each update. To code communal words, we created a dictionary to capture words that reflect the use of communal language. For this, we asked two graduate research assistants to read descriptions of a random sample of 100 ideas (from our data) and identify words that reflected a ""communal"" idea while coding each description on whether it was communal. We provided the Merriam-Webster definition of ""communal"" (""of or relating to a community"") to the two coders along with synonyms from a thesaurus. Then, we cross-verified these words with LIWC's category for ""affiliation,"" comprising 248 words ([46]). Communal words that appear at least once in our corpus are member, team, group, groups, family, friends, affiliation, affiliate, relation, connection, alliance, relationship, partner, partners, partnership, link, merge, cooperate, cooperation, together, join, thanks, thank you, appreciate, our, and we. We measure backer engagement as the number of Facebook shares of the idea by backers, which we collected when the web crawler visited an idea's webpage. Model-free evidenceTo explore model-free evidence, we present summary statistics about three regimes of the distribution of the amount of daily funding achieved for Kickstarter in Table 3: ( 1) idea-day-specific observations when there is no funding, ( 2) when the daily funding is positive but does not exceed the mean level in the data ($409.34), and ( 3) when the daily funding exceeds the mean level. Backer affiliation is highest when ideas do not receive any funding and lowest when ideas achieve the highest funding. The measure of backer affiliation for an idea on a given day is based on cobacking relationships of backers that fund the idea on that specific day with backers who funded before that day. If no backer funds on a specific day, the affiliation measure for that day is zero. The measure is not cumulative, and it does not increase over time. Thus, there is model-free evidence for the negative effect of affiliation on funding outcomes. We collected similar data from another crowdfunding platform, Indiegogo, which we use in the robustness analysis. Additional details about the Kickstarter data and summary statistics for the Indiegogo data appear in Web Appendix D. We illustrate affiliation in Figures W1–W5 and the sample's network structure and growth in Figures W6–W8 in Web Appendix E. We estimate the primary empirical model on Kickstarter data.GraphTable 3. Means of Backer Affiliation and Other Time-Varying Covariates at Different Levels of Daily Funding (Kickstarter). VariableAmount Funded (yit) = 0Amount Funded 0 < (yit) ≤ $409.34Amount Funded (yit) > $409.34Number of observations17,50511,0713,863Proportion of all observations53.96%34.13%11.91%Amount of funding of idea i (in $) in day t0109.293,214.89Backer affiliation of idea i by day t − 1 (Affilit − 1)4.222.261.79Cumulative number of backers funding idea i by day t − 1(CumBackersit − 1)22.6544.23222.89Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)6.2110.5415.76Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).26.531.69Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).34.29.27Closeness centrality of idea i as of day t − 15.68 × 10−94.56 × 10−99.49 × 10−9Betweenness centrality of idea i as of day t − 1473.991,178.215,974.58Eigenvector centrality of idea i as of day t − 1.001.000.012Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).10.07.06Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1).691.93199.38  Empirical modelFollowing [ 8] and [66], our primary dependent variable (yit) is the monetary funding received by an idea i (i = 1,...N) on day t (t = 1,...Ti). As a starting point, we incorporate backer affiliation and several controls in a fixed-effects regression model as follows: log(yit)=αi+αt+β1log(Affilit–1)+β2log(CumBackersit–1)+β3log(CumUpdatesit–1)+β4log(yit–1)+β5PropGoalit–1+β6PropDurationit–1+β7LastWeekit–1+β8Networkit–1+β9log(CommunalUpdatesit–1)+β10log(Affilit–1)×CommunalUpdatesit–1+β11log(Affilit–1)×FBSharesi+eit. Graph( 1)To account for nonnegativity, we log-transform all variables that are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. Replacing this constant with other constants does not affect our results. Estimating the model without taking logarithms of any variable gave us consistent results.To control idea-specific confounding factors such as inherent differences in idea quality, the novelty of idea description, creator expertise, and so on, we employ idea-specific fixed effects αi, a vector of 2,021 elements for the Kickstarter data set. We incorporate fixed effects for each day in the idea's funding window to control temporal patterns in funding and changes in the Kickstarter environment over time. These are denoted by the vector αt. Error terms are assumed normally distributed and clustered at the idea level.Our key independent variable is Affilit − 1. This is the number of cobacking relationships between those backers who fund idea i on day t − 1 and all backers who have funded this idea before day t − 1. Subsequently, we report robustness checks to alternate measures of backer affiliation. Although our fixed-effects specification controls for confounds at the idea level and the day level, we need to control idea-specific factors that are time varying. Chief among these is the amount of funding received by the focal idea on day t − 1 ([ 8]), enabling us to control those time-varying idea-specific unobservables, which may be serially correlated (e.g., word of mouth about the idea) and to attenuate serial correlation among the residuals. This also accounts for the alternate explanation that affiliation on day t − 1 affects funding on day t − 1, but not on day t. By incorporating the lagged measure of funding, we can account for all factors that affect funding until the day t − 1.We next discuss other time-varying idea-specific controls. First, the number of affiliations among backers is correlated with the number of backers. There can be no backer affiliations without backers; more backers could result in more possibilities for affiliation. To control for the possibility that the number of backers drives the effect of affiliation on funding, we include CumBackersit − 1, the cumulative number of backers funding idea i by day t − 1, as a control variable. Also, to the extent that ideas with more backers attract more funding ([66]), this serves as a measure for herding behavior. Second, creators communicate with backers via updates, a means to elevate idea visibility and signal effort ([12]). To understand how creator actions might drive funding, we include CumUpdatesit − 1, the cumulative number of updates by the creator of the idea i by day t − 1. In addition, CommunalUpdatesit − 1 is the number of communal words contained in the updates.Third, the funding window of an idea influences its funding outcomes. Ideas receive more funding in the later stages of the funding window as the funding deadline nears (e.g., [12]; [31]). To account for this, we include the duration of the funding window completed for the idea (PropDurationit − 1) as a proportion of the total funding window (typically 30 days). Furthermore, ideas receive greater funding as they get closer to meeting their funding goals ([12]). Although daily fixed effects account for temporal variations in funding, they might not capture the effect of proximity to the funding goal. Therefore, we include PropGoalit − 1, the proportion of the funding goal of the idea that has been achieved until day t − 1, and LastWeekit − 1, a dummy variable for whether the observation belongs to the last week of the funding window.Finally, structural measures of network centrality might affect the outcome. Because these measures capture the extent of social capital that accrues to ideas due to being associated with certain backers, we want to control for the effects of these measures. We compute and include three of the most widely used network measures in marketing (e.g., [35]; [48]; [58]), (Networkit − 1): closeness centrality, betweenness centrality, and eigenvector centrality of idea i on day t. Closeness centrality in our context is how close the focal idea is from all the backers (connected and not connected) in the network, betweenness centrality is the extent to which the focal idea lies on the common paths between all pairs of backers in the network, and eigenvector centrality is the extent to which the focal idea's backers are prolific in backing other ideas. We computed both bipartite and single-mode network variants of each of these measures.[ 9] Given the high correlation across the bipartite and single-mode versions of each measure, we included in the model the version of each measure that leads to a more significant improvement in R2. As shown in the correlation matrix of all variables (Table 4), these variables are not highly correlated with our measure of affiliation, suggesting that affiliation captures the network's unique structural properties based on counts of overlaps. To assess interaction effects, we interact affiliation with CommunalUpdatesit − 1[10] and with the number of Facebook shares of the idea by backers (FBSharesi).GraphTable 4. Pairwise Correlation Coefficients of All Variables (Kickstarter). Variable12345678910111. Amount of funding of (in $) in t12. Backer affiliation i by t − 1−.0713. Cum. number of backers funding by t − 1.13.1414. Cum. number of updates by creator i by t − 1.13.15.1815. Amount of funding (in $) in t − 1.49−.11.17.1516. Proportion of funding goal achieved by t − 1.15.00.12.20.1917. Proportion of funding duration completed by t − 1−.11.37.05.30−.19.0518. Closeness centrality as of t − 1.01−.07−.00−.04.14−.01−.1219. Betweenness centrality i as of t − 1.09.37.23.29.07.07.32−.07110. Eigenvector centrality as of t − 1.05.04.33.09.07.04.00.07.09111. Last week (1 if day t is in the last week of funding)−.05.14.02.15−.05.05.61.03.01.00112. Cum. no. of communal words in updates by t − 1.02.02.26.01.03.00−.00.01.00.09−.00 2 Notes: We take logarithms of all variables, which are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. All variables pertain to the focal idea. Coefficients with p < .05 are in boldface.We use lags of all covariates because information about the focal day is not updated in real time and is unavailable until the following day.[11] We present the correlation matrix of all variables in Table 4; most correlations are less than.3, allaying multicollinearity's ill effects. Empirical strategyWe first discuss how our work is different from the peer effects literature and then explain our identification strategy. The prototypical problem in the marketing literature on the identification of peer effects (e.g., [40]) is to estimate the likelihood of agent A adopting a product (e.g., buying an online game) under the knowledge that agent B (a self-identified ""friend"" or influencer) has already adopted that same product. The herding literature has conclusively documented positive peer effects across various consumer contexts (e.g., [57]; [66]). If there are positive effects due to the size of the crowd or the number of peers, that would be equivalent to herding, not our study's primary focus. In other words, our main objective is not to estimate how backer A will fund the focal idea if another backer B has previously funded it. We account for herding in our model by incorporating the prior number of backers of the focal idea as a control. Instead, our objective is to study the effect of affiliation, which is formed when two backers back an idea that is not the focal idea. Affiliation arises in collaborative contexts (e.g., board interlocks, product development teams) rather than common product purchases. We note that this is a key difference of our article from other contexts. In addition, our interest is less in modeling agent behavior (e.g., an individual's rating of a product in [57]) than in modeling product success (i.e., funding success of an idea). We next address three main issues that could confound identifying the causal effect of affiliation on the focal idea's funding. Correlated unobservablesIdea-specific characteristics that are not observable to the researcher could be correlated with our affiliation measure and affect the focal idea's funding. Perhaps highly affiliated backers are attracted to ideas with high (or low) unobserved quality. The inability to control for quality dimensions might induce an upward (or downward) bias in our estimate of the effect of affiliation. Following [40] and [57], we incorporate idea-specific fixed effects. These effectively control for all idea-specific factors that might be correlated with affiliation. Next, there could be time-varying factors across the funding window that might be correlated with affiliation and funding. For example, affiliation and funding are both likely to be low in the first few days of funding. We control for all day-specific trends by incorporating day fixed effects. Finally, the presence of idea-specific time-varying factors cannot be ruled out. We control for funding received by the focal idea on day t − 1. As mentioned previously, this approach enables us to control those time-varying idea-specific unobservables, which may be serially correlated, and to attenuate serial correlation among the residuals. SimultaneityIn the context of peer influence, simultaneity implies that not only can the influencer influence the focal individual but the focal individual could also affect the influencer's actions, leading to an upward bias in the estimate of peer effects. In our context, affiliations formed on a focal day may affect the focal idea's funding. Simultaneously, the focal idea's funding on a focal day also affects affiliation formation on that day. Following recent literature (e.g., [45]), we use the lagged measures of affiliation in the model. While affiliation before the focal day can affect funding on the focal day, the reverse is not possible. Endogenous group formation (or homophily)Backers with similar preferences may be more likely to behave similarly. In such a scenario, the effect of prior affiliation on subsequent funding of the focal idea might manifest these common preferences. The literature on consumer peer effects has used consumer-specific fixed effects to deal with this. However, crowdfunding is different from consumer contexts in that while consumers buy (and evaluate) several products, backers typically fund very few ideas on a platform.Moreover, unlike crowdfunding, consumer contexts generally focus on the individual more than collective action ([50]). So, backer-specific fixed effects are econometrically infeasible to estimate for both the researcher and the platform. Instead, we first include the cumulative number of backers and several other network measures as controls. Next, we note that controlling for lagged funding of the idea also controls backer characteristics that have affected funding before the focal day.Finally, we include an instrument for affiliation. If our measure of affiliation is correlated with the error term in Equation 1, its coefficient could be biased. In our primary analysis, we use an observed instrument to estimate a two-stage least-squares instrumental variable regression model. As [49], p. 4) mentions, the ideal solution for endogeneity is to conduct an experiment where the endogenous variable is uncorrelated with the construction's dependent variable. Therefore, we ran controlled experiments, which we explain subsequently, where participants were randomly assigned to different affiliation levels, creating exogenous variation.For the primary instrumental variable approach, we follow recent research (e.g., [20]; [51]) that uses instruments based on agent behavior in categories (or firms) different from the focal category (or a firm). Following this approach, we use the mean (across ideas) of affiliations on day t − 1 of all ideas in our Kickstarter data, which are in a category different from that of the focal idea as the primary instrument for Affilit − 1 in Kickstarter. For example, for an observation about an idea on movies on December 22, this instrument is the mean of affiliations on December 22 of all ideas in our data that are not in the movies category. This instrument is correlated with Affilit − 1 (correlation = .16).Conceptually, this instrument is appealing because of the interdependencies across different parts of the global affiliation network on Kickstarter (i.e., the affiliation network across all ideas seeking funding concurrently), thus satisfying the relevance criterion. However, because most backers only back one idea (i.e., affiliation is sparse), the mean affiliation across ideas in other categories is very unlikely to be related to the unobserved component of the focal idea's funding outcome in Equation 1 providing the basis for identification. Further, a category-level measure of affiliation should remain unaffected by idea-level factors, especially if the idea is from a different category. A category-level measure should not correlate strongly with idea-day-level idiosyncratic shocks from another category, thus meeting the exclusion criterion. The first-stage equation is specified as log(Affilit–1)=λ0+λ1IVit–1+λ2log(CumBackersit–1)+λ3log(CumUpdatesit–1)+λ4log(yit–1)+λ5PropGoalit–1+λ6PropDurationit–1+λ7LastWeekit–1+λ8Networkit–1+λ9log(CommunalUpdatesit–1)+δit–1. Graph( 2)The R2 for the first stage regression without the instrument (i.e., assuming that λ1 = 0) is.365 and with the instrument is.385, showing that the instrument's addition improves the in-sample model fit. The estimate of λ1 is.42 (p < .01). The corresponding F-statistic for the F-test of excluded instruments is 879.83, far exceeding the threshold value of 10 ([55], p. 522). The large value of the Anderson–Rubin statistic (F( 1, 28,300) = 298.41) rejects the null hypothesis that the instrument is weak. We show in robustness analyses that the estimates are consistent across the use of alternative instruments. We also instrument for the interaction of affiliation and the number of communal words contained in the updates (CommunalUpdatesit − 1). Following [44], the instrument for this interaction variable is the interaction of the instrument for affiliation and CommunalUpdatesit − 1. We do not instrument for the interaction of affiliation and the number of Facebook shares, because the Durbin–Wu–Hausman test of the hypothesis that this regressor is exogenous could not be rejected (χ2 = .055, p > .1). Furthermore, the sharing activity of a specific idea on a social media platform other than Kickstarter is conceptually independent of its funding outcome on Kickstarter. ResultsFirst, we present the parameter estimates of the instrumental variable regression models estimated on the Kickstarter data and then discuss robustness checks. We present estimates of five models, with and without instruments, and the sequential addition of interactions in Table 5. M1–M4 do not have interaction effects, and while M1 ignores endogeneity, M2, M3, and M4 correct for it and show that the results are robust to different instruments. The results from the full model specified in Equation 1 are reported in M5, which we discuss next.GraphTable 5. Coefficient Estimates of the Fixed-Effects Regression Model of Daily Funding of Ideas on Kickstarter. VariableM1M2M3M4M5 (Final Model)Backer affiliation of idea i by day t − 1 (Affilit − 1)−.04***(.01)−.86***(.06)−1.88***(.41)−.80***(.06)−.87***(.06)Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1).15**(.06)1.92***(.15)4.13***(.90)1.79***(.15)2.02***(.16)Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit− 1)−.05**(.02)−.06**(.03)−.07*(.04)−.06**(.03)−.05*(.03)Amount of funding of idea i (in $) on day t − 1−.03(.02).01(.02).07**(.03).01(.02).01(.02)Proportion of funding goal of idea i achieved by day t − 1−.09(.06)−.23***(.07)−.39**(.10)−.22**(.07)−.17**(.07)Proportion of funding duration of idea i completed by day t − 1−.27(.99)1.12(1.10)2.84*(1.65)1.02(1.09).63(1.10)Closeness centrality of idea i as of day t − 12.54(3.12)2.43(3.54)8.46(5.30)1.99(3.48)2.71(3.54)Betweenness centrality of idea i as of day t − 1−.02**(.01)−.02**(.01)−.03*(.02)−.02**(.01)−.02*(.01)Eigenvector centrality of idea i as of day t − 1−.74(.82)−4.39*(2.33)−8.95*(4.66)−4.13*(2.21)−4.40*(2.67)Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).25(.21).13(.24).03(.35).14(.24).09(.25)Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)−.04(.03)−.02(.03)−.01(.04)−.02(.03)−.29*(.15)Interactions of Backer AffiliationsAffilit − 1 × CommunalUpdatesit − 1−7.68**(3.80)Affilit − 1 × Number of Facebook shares of idea i−.006***(.001)Fixed effects for each idea iYesYesYesYesYesFixed effects for each day tYesYesYesYesYesInstrument for Affilit − 1NoYesConstraOtherYes 3 *p < .10.4 **p < .05.5 ***p < .01.6 Notes:""Constra"" refers to [ 7] measure of constraint of the focal idea. ""Other"" instrument refers to the instrument constructed from Indiegogo data. Full model resultsWe find that affiliation among backers has a consistent negative effect on the funding of ideas on Kickstarter (β = −.87, p < .01). This effect persists despite the inclusion of idea-specific fixed effects, daily fixed effects, controlling for lagged funding, and the prior number of backers of the idea. We corroborate extant findings on herding (e.g., [66]) and additionally show that affiliation plays a key role and that its effect is negative.Concerning the moderators, the creator's engagement measured as using communal words in updates further strengthens the negative effect of affiliation, perhaps because of a heightened licensing effect (β = −7.68, p < .01). For backer engagement, we find the negative effect of affiliation is stronger as backer engagement, measured as the number of Facebook shares of the idea by backers, increases (β = −.006, p < .01). One explanation of this is that while individuals share on Facebook for various motives, the primary motivation is prosocial, and others seeing the shares likely see them as such, strengthening the vicarious moral licensing effect ([29]).Concerning control variables, the greater the number of backers of an idea before the focal day, a measure of herding, the more funding the idea will attract on the focal day (β = 2.02, p < .01). This indicates that the total number of backers for an idea may act as a signal of its quality or potential worthiness, a finding that is consistent with prior research (e.g., [34]). The current research replicates this effect and demonstrates that social structure influences behavior beyond the herding effect. Moreover, this theory supports our contention that affiliation, measured by cobacking, drives the negative effect, not herding. We also find that the total number of updates posted by the creator has a negative effect on crowdfunding success (β = −.05, p < .10), although this effect is not significant across all model specifications. The effect of the proportion of the funding goal which was achieved on the previous day is negative (β = −.17, p < .05), perhaps suggesting a preference to fund underfunded ideas. For network centrality measures, we find that betweenness (β = −.02, p < .05) and eigenvector centrality (β = −4.40, p < .05) have a negative effect on funding. The negative effects of these second-order network measures, compared with the positive effect of number of backers (proxy for first-order network effect), highlight the complexity in flow of information on the network and are consistent with findings from prior studies (e.g., [35]). This is perhaps because these measures indicate the backers' ability to identify and fund salient opportunities, or access to information from their overall networks about idea quality based on indirect ties across the whole network, not just direct ties. Thus, the effects also highlight the importance of distinguishing direct and indirect aspects of how networks operate in community contexts.To ensure that outliers are not driving our results, we estimate the main model (M5) after dropping the top 10th percentile of observations (which have affiliation values greater than 7), yielding a significant and negative estimate of the affiliation coefficient (β = −.87, p < .01). We find a similar negative effect in models estimated on various subsets of the data. To investigate if specific categories of ideas drive our results, we estimate the model separately for each category's ideas. We find a negative effect of affiliation for 11 out of 12 categories, with the most negative effect of affiliation in the ideas from the photography and technology categories. Our estimate of affiliation's effect is negative but not statistically significant for the ""dance"" category, which accounts for just 21 out of 2,021 ideas in our data. Robustness analysesWe conducted several robustness analyses. First, we estimated the model on Indiegogo data; the results are quite consistent (see Web Appendix F). Second, we estimated the model on Kickstarter data using three alternative sets of instruments: discrete latent instrumental variables, an instrument constructed using affiliation from another platform, and a network-based instrument (see Web Appendix G). Third, we estimated probit, logit, and Tobit models of funding success and checked the robustness of our results to two alternative measures of affiliation (Web Appendix H). All analyses show that our results are robust. Next, we report three experiments in which we probe the effect of affiliation, the underlying process, and a moderating factor to further validate our empirical model. Collection and Analysis of Experimental DataIn the first experiment, we demonstrate the negative effect of affiliation in a controlled experimental setting. In the second experiment, we validate vicarious moral licensing as an underlying mechanism and rule out uniqueness as one potential alternative explanation. In the third experiment, we show how the idea's description might moderate the effect of affiliation. Experiment 1We conducted Experiment 1 on MTurk with 200 North American residents[12] (Mage = 35.26 years; 49.8% women; 42.6% have previously funded a crowdfunding idea). We presented participants with two ideas seeking funding (both real ideas from Kickstarter; see Web Appendix I). First, participants saw a screenshot of a website created by a graphic designer to look like an idea page on a real crowdfunding platform (e.g., [ 8]; [65]).Consistent with prior research, participants were given money beyond study payment, creating an incentive-compatible dependent measure ([21]; [39]). Participants were told, ""As part of this study, you will receive a $2 bonus. You can use some or all of this money to fund this project."" They were then asked how much they would give toward the idea on a nine-point scale with dollar amounts in $.25 intervals, ranging from $0 to $2.00. If participants chose ""$0"" and opted to keep the full bonus, they were then forwarded to the end of the survey and were paid the original MTurk fee as well as the $2 bonus. If participants used any of their bonus to fund the first idea, they were included in our primary analyses. Ninety-three participants opted not to fund the first idea, leaving us with 107 participants. Four participants were removed who indicated that they had a child affected by autism, the focus of one of the two ideas, and were inclined toward funding but would opt to put the money toward helping their child. All participants completed the dependent measures. Two participants were removed for spending less than a second on the manipulation, leaving us with 101 participants. Participants then saw a screenshot of a second website designed to look like an idea on a crowdfunding platform (for details, see Web Appendix I).The screenshot included idea information and a list of recent backers shown on the screen's right side. Participants in the high-affiliation condition saw a high overlap in the number of backers across the two ideas. Participants in the control-affiliation condition saw the same number of backers, but the names on the two lists did not overlap. A manipulation check confirmed the effectiveness of the manipulation. All participants who funded the first idea were told that they would receive an additional $2 bonus to keep or use to fund the second idea. Their decision on a nine-point scale ranging from $0 to $2.00 in $.25 intervals served as the outcome. At the end of the study, participants were given the money that they chose to keep as a bonus, and the remainder (i.e., what they chose to fund each of the ideas) was put toward each crowdfunding idea. Finally, participants responded to a set of demographic measures (e.g., age, gender, whether they had previously funded an idea on a crowdfunding platform). A one-way analysis of variance showed a significant effect of affiliation on the funding of the second idea (F( 1, 99) = 4.05, p < .05). As we expected, those in the high-affiliation condition funded less than those in the control-affiliation condition (Mhigh = 4.27, SD = 2.27 vs. Mcontrol = 5.27, SD = 2.70). Of the $2 bonus, those in the high-affiliation condition chose to fund $.82 toward the focal idea, while those in the control-affiliation condition chose to fund $1.07, on average.The first experiment confirmed the negative effect of affiliation in the lab setting, validating our primary empirical finding that affiliation negatively affects crowdfunding success. Experiment 2In the second experiment, we measured two potential mediators in an attempt to document ""a"" mediating process (i.e., the mediating process given our stimuli and procedures) as opposed to ""the"" mediating process (i.e., a single mediating process that is operative across all crowdfunding contexts; e.g., [ 6]). We propose vicarious moral licensing as a mechanism for the negative impact of affiliation on funding and test need for uniqueness as an alternative mechanism ([59]).We conducted the study on MTurk with 228 North American residents (Mage = 39.57 years; 54.4% women; 38.2% had previously funded an idea on an online crowdfunding platform). All participants spent adequate time on the manipulation. Three participants did not complete the dependent measures, resulting in an effective sample of 225 participants. Participants were told to imagine that they had $50 and were asked to choose one idea to fund from a set of four real ideas seeking funding on Kickstarter and across categories (e.g., technology, nonprofits, arts/film); details appear in Web Appendix I. After this decision, they read about a second idea that they were told is seeking funding. Those in the high-affiliation condition were told that many of the backers who funded the first idea they chose also funded the focal idea. Those in the control affiliation condition were provided no information about other backers' funding decisions. A pretest confirmed the effectiveness of the manipulation (see Web Appendix I). Next, participants responded to two items to capture vicarious moral licensing (""Based on the funding behavior of cobackers, I do not feel the need to fund [focal idea]"" and ""Based on the funding behavior of cobackers, I do not feel obligated to fund [focal idea]""; M = 4.10, SD = 1.48; r = .72) and two items to capture uniqueness (""If I funded [focal idea], my decision to fund would say a lot about me as a unique individual"" and ""If I funded [focal idea], it would help me stand out from the crowd""; M = 3.68, SD = 1.45; r = .81).Next, we asked participants how much money they would pledge toward funding the subsequent focal idea (range: $0–$5,000, the total needed to hit the focal idea's funding goal). Consistent with prior research and our empirical model, we log-transformed funding ([36]). Finally, participants completed demographic questions.As expected, we found a negative effect of affiliation on funding (F( 1, 223) = 4.29, p < .04) such that those in the high-affiliation condition reported a lower funding amount than those in the control condition (Mhigh = 3.17, SD = 2.49 vs. Mcontrol = 3.82, SD = 2.24) or in raw numbers (Mhigh = $256.96, SD = $674.40 vs. Mcontrol = $339.88, SD = $875.90). A one-way analysis of variance showed a significant effect of affiliation on the licensing measure (F( 1, 223) = 3.89, p = .05). As we expected, those in the high-affiliation condition agreed more with the licensing measure, indicating less need to fund than those in the control condition (Mhigh = 4.29, SD = 1.57 vs. Mcontrol = 3.90, SD = 1.37). However, there was no significant effect of affiliation on uniqueness (Mhigh = 3.56, SD = 1.52 vs. Mcontrol = 3.80, SD = 1.36; F( 1, 223) = 1.53, p = .22). We then assessed the indirect effects of the two mediators on funding. The results indicate that licensing was a significant mediator (95% confidence interval does not include 0: [−.4423, −.0003]), but uniqueness was not (95% confidence interval: [−.5479,.1142]).In this experiment, we replicated the negative effect of affiliation and uncovered vicarious moral licensing as an underlying mechanism. Although we did not find an effect of affiliation on uniqueness in this study, we note that uniqueness may operate more strongly for some ideas and some individuals, providing an interesting avenue for future research on crowdfunding ([59]). Experiment 3In Experiment 3, we explored the role of a moderator: how the creator describes the idea. We theorized that the negative effect of affiliation occurs in a crowdfunding context, at least partly due to its communal nature and how the ideas are presented to potential backers. We conducted the third experiment on MTurk with 206 North American residents (Mage = 38.81 years; 46.1% women; 42.2% have previously funded an idea on an online crowdfunding platform). All participants completed the dependent measures. Three participants who spent less than one second reading the manipulation were removed, resulting in N = 203. We manipulated two factors between participants: ( 1) affiliation (high vs. control) and ( 2) idea description (more vs. less communal).As in Experiment 2, participants read about an idea currently seeking funding on Kickstarter and were told to imagine that they had funded this idea (see Web Appendix I). We used the same manipulation of affiliation as in Experiment 2. Those in the high-affiliation condition were told that many backers who funded the first idea they chose also funded the focal idea. Those in the control-affiliation condition were not provided any information about other backers' funding decisions. Participants then read about diveLIVE, a technology that allows divers to talk underwater while streaming live video to the internet. diveLIVE, the focal idea, was described as more or less communal with small changes (e.g., ""Let's learn about the oceans"" vs. ""This product uses technology to take videos of the oceans"").Next, participants indicated how much money they would pledge toward diveLIVE, the focal idea (range: $0–$20,000, the total needed to hit the focal idea's funding goal). Consistent with prior research, our empirical model, and Experiment 2, we log-transformed funding ([36]) for analysis but provide results in raw numbers for ease of interpretation. Finally, participants completed demographic questions.We found evidence for a main effect of idea description (F( 1, 199) = 13.86, p < .01) consistent with prior research, which finds that ideas described as more communal tend to be more successful than those described as an investment opportunity ([ 3]). More importantly, we found an interaction between the two manipulated factors (F( 1, 199) = 5.84, p < .02). As we expected, when the idea was described as more communal, those in the high-affiliation condition reported lower funding than those in the control-affiliation condition (Mhigh = $2,155.32, SD = $3,998.08 vs. Mcontrol = $4,073.04, SD = $5,316.08; t(199) = 2.09, p < .04). When the idea was described as less communal, there was no effect of affiliation on funding (Mhigh = $2,572.33, SD = $4,933.01 vs. Mcontrol = $1,868.68, SD = $4,039.03; t(199) = −1.34, p = .18; see Figure W9 in Web Appendix I). The third experiment established that the negative effect of affiliation is stronger when creator's use more communal words in the description of the idea. Validating Moderation with Observational DataAs discussed previously, we find a negative moderating effect of the number of communal words in updates posted by creators. To validate the third experiment with converging evidence, we returned to our secondary data to examine how the number of communal words in the idea description influenced the relationship between affiliation and funding behavior across thousands of crowdfunding ideas (e.g., [42]). This would establish how the use of communal words in creator's updates as well as in the idea's description would influence the effect of backer affiliation and highlight the importance of the communal mechanism. We used the same text dictionary that we created for coding communal words in updates and coded the description of every idea in our sample. The median number of communal words in an idea description is 3 (M = 6.1). We then created two subsets of our data based on a median split of the number of communal words used in describing the idea. We estimated the model separately on each subset and find that the coefficient of affiliation is less negative for ideas described using three or fewer communal words (M = −.92, SE = .09) than for ideas described using four or more communal words (M = −1.25, SE = .15). Replacing the number of communal words in this analysis with the ratio of the number of communal words to the total number of words does not affect this result, nor does splitting the data on the basis of the average number of communal words instead of the median. Finally, the effect of affiliation is less negative for ideas with no communal words than for ideas with at least one communal word. This provides real-world evidence for the role of idea description on the relationship between affiliation and funding behavior, validating our theory and experimental evidence.In summary, these findings further support our reasoning that the negative effect of affiliation is driven, at least in part, by the communal nature of crowdfunding and the prosocial mindset that it prompts ([50]). When an idea is described as more communal, these prosocial goals are exacerbated, leading potential backers to feel that they do not need to fund the idea because these affiliated others are funding it (e.g., [37]). However, when an idea is described as less communal, this effect is mitigated. Next, we discuss our results and develop implications for theory and practice. DiscussionWe establish a negative effect of affiliation on the crowdfunding success of ideas using a large empirical study and then validating the effect through experiments. We provide preliminary insights into the role of vicarious moral licensing as the underlying mechanism for this effect and investigate the moderating role of creator and backer engagement. The licensing effect and its role in reducing backers' perceived obligation to fund ideas could make backers less likely to fund or fund with less money if they opt to fund, both of which could explain the negative effect at the idea level. We begin with a focus on the novel contribution of our finding concerning affiliation, discuss the economic implications of our results, and identify the primary contributions of our research and how it paves the way for future research.The negative effect of affiliation among backers in crowdfunding is distinct from and in addition to the positive effect of herding due to the crowd's size shown in prior research (e.g., [66]). We establish an inherent tension between the positive effect of crowd size and the negative effect of backer affiliation in crowdfunding. Thus, we show that, in addition to relying on crowd size, backers make inferences based on the behavior of affiliated others in a crowdfunding context. A 10% daily increase in number of backers leads to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation leads to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by 43%. Our results concerning affiliation are both statistically and economically meaningful and highlight the need to recognize the tension between increasing the number of backers and limiting the ill effects of affiliation.Interestingly, Kickstarter stopped disclosing the prior backers' list on an idea's page as of the time of writing this article. This policy change is consistent with our results. If backer identities remain unknown, potential backers cannot infer affiliation, and therefore ideas cannot be negatively impacted by backer affiliation. Other crowdfunding platforms should reevaluate disclosure policies about past backers of an idea or perhaps reconsider whom they show at the top of their backer lists.So how might creators mitigate the negative effects of affiliation? The moderation effects from our results provide actionable insights for creators seeking crowdfunding from potential backers and considering what platforms to pursue. Our results concerning the interaction between affiliation and creator engagement show that creators can subdue the negative effects of affiliation by carefully crafting the idea description and updates, avoiding communal language.Further, while it appears that encouraging backers to share the idea on social media might be counterproductive because it strengthens affiliation's negative effect, the impact is small and should not be a major concern. The change in the marginal effect of affiliation as sharing by backers increases is small, indicating that change in backers' engagement, while statistically significant, does not have a meaningful effect on crowdfunding. Doubling the number of Facebook shares from its mean of 79 to 148 strengthens the negative effect of affiliation by.42% and translates to a decline of US$1.72/day.We developed recommendations for creators and examples of best practices from our data set (see Table 6). For example, creators should focus on the idea's inherent purpose and objective value in its description and avoid using too much communal language (e.g., cooperate, partner, support) in the idea description and updates. Overall, we recommend that platforms educate creators on how best to structure communication with backers and guide creators in meeting their goals. Backers could perhaps learn to interpret such updates better and use the information provided by the backer to qualify what they infer from the community.GraphTable 6. Actionable Outcomes for Managers Recommendations for Idea Descriptions and Updates. FindingRecommendations for CreatorsExamples from Kickstarter DataInteraction between affiliation and communal words in idea descriptionFocus on the idea's inherent purpose as opposed to a focus on community.The Drone PocketIdea Description: ""The world's first multicopter that'spowerful enough to carry a high-quality action camera and folds up smaller than a 7 in tablet.""Key technology features outlined prominently on idea's home page.Total Amount Raised: $929,212Pegasus Touch Laser SLA 3D PrinterIdea page includes recent press articles with links that highlight idea's featuresTotal Amount Raised: $819,535Use noncommunal words (e.g., ""you"" vs. ""we"") in idea description.The Floyd Leg""The Floyd Leg gives you the framework to take ownership of your furniture by allowing you to create a table from any flat surface"" (emphasis added)Total Amount Raised: $256,273Avoid thanking backers too much in idea description, as it can make the project appear needy.""First off, I want to say thanks for checking out of project. Every single person that takes the time to look at our project means the world to us.""Do not describe idea with overemphasis on communal language (e.g., ""support,"" ""team"").""As we approach Thanksgiving, I continue to be thankful for the patience and support that the unsung backers have shown with our team.""Interaction between affiliation and communal words in updatesDo not show too much appreciation via updates for funding as it is progressing.""Thanks to all of you who pledged for this campaign. We really appreciate your continued support.""Minimize communal language (e.g., partner) in updates.""Your first duty as partners with us on this project; should you choose to accept..."" Our results about the mechanism provide insights on how platforms and creators should engage with backers. Research has shown that licensing is a nonconscious effect and can be mitigated by making individuals aware of their behavior ([27]). Particularly in this type of vicarious moral licensing, highlighting individuals' uniqueness and independent identity may also mitigate the negative effect of affiliation on funding ([30]; [37]; [43]). If creators expect high overlap among backers, they could describe their ideas using less communal language, thereby lowering the licensing effect. Our results suggest that vicarious licensing might overwhelm other relevant idea information, potentially leading to suboptimal backer decisions. In line with our findings, backers might, in some cases, pay more attention to signals from affiliated others rather than from the whole crowd.For crowdfunding platforms, our findings provide a rationale for why there might be room for new crowdfunding platforms to thrive and grow. Although several crowdfunding platforms have flourished in the past decade, Kickstarter, Indiegogo, and GoFundMe have arguably dominated the market. Other once-popular platforms, such as Sellaband and PledgeMusic, have failed. Large platforms with millions of backers might pose high entry barriers to new entrants. However, our findings point to one source of competitive advantage for newer platforms: negative affiliation effects are more likely to occur in well-established platforms with large backer communities. Strategically building diverse and unaffiliated communities of backers might confer a competitive advantage to new platforms. Our results show that this can be achieved by expanding the number of categories of ideas, as affiliation's negative effect may be mitigated as backers of ideas across different categories may be less likely to coback ideas. The failure of category-specific platforms such as Sellaband (music), and the relative success of platforms hosting diverse ideas, such as Kickstarter, provides support for this reasoning. Second, platforms allocating marketing resources across existing and new backers (e.g., allocating social media spending across established markets such as Los Angeles and new markets such as Lima) could perhaps view our results as a reason to divert resources away from backer-dense markets. Third, platforms that provide backer information may also want to use algorithms that promote unaffiliated (vs. affiliated) backers, for example, by highlighting first-time backers. Finally, drawing on our results about creator engagement, we recommend that platforms educate creators on how to design better backer communication.Insights from our study are relevant to other types of crowdsourcing platforms as well. For example, participants on LEGO's Ideas, which focuses on ideation, and SeedInvest, which helps raise equity, could mitigate the negative effects of affiliation, for example, by describing initiatives as less communal and by posting updates with less communal language. Our findings are also applicable to crowdfunding contests (e.g., [ 9]; [25]), where participants could be encouraged to vote across categories to reduce coparticipation and help them break away from the adverse effects of groupthink.We highlight several areas of inquiry for future research. Reward structures could impact the role of affiliation in crowdfunding and thus merit attention (e.g., [56]). Fake reviews have been investigated in the online context (e.g., [67]), and it would be interesting to explore the veracity of idea descriptions and creator updates. In addition to affiliation, which we study, other network characteristics such as clans and core–periphery structures ([60]) could explain the nature of information flow across affiliation structures.As interest in crowdfunding increases, interesting research questions continue to emerge. We believe that our research explores important questions concerning crowdfunding that involve backer affiliation and community structure, and we hope to lay the foundation for future studies in the domain. "
15,"Examining Why and When Market Share Drives Firm Profit Many firms use market share to set marketing goals and monitor performance. Recent meta-analytic research reveals the average economic impact of market share performance and identifies some factors affecting its value. However, empirical understanding of why any market share–profit relationship exists and varies is limited. The authors simultaneously examine the three primary theoretical mechanisms linking firm market share with profit. On average, they find that most of the variance in market share's positive effect on firm profit is explained by market power and quality signaling, with little support for operating efficiency as a mechanism. They find a similar explanatory role of the three mechanisms in conditions where market share negatively predicts profit (for niche firms and those ""buying"" market share). Using these mechanism insights, the authors show that the value of market share differs in predictable ways between firms and across industries, providing new understanding of when managers may usefully set market share goals. The authors also provide new insights into how market share should be measured for goal setting and performance monitoring. They show that revenue market share is a predictor of firm profit while unit market share is not, and that relative measures of revenue market share can provide greater predictive power.Keywords: market share; quality; efficiency; market power; niche; firm profit; revenue share; unit shareMany firms use market share to set goals and monitor marketing performance, and market share is also widely used in research examining marketing's performance impact ([24]; [40]). [20] recent meta-analytic study (hereinafter, E-H 2018) reports a significant positive relationship between a firm's market share and its economic performance and identifies contingencies affecting this relationship. However, while the literature suggests several reasons market share may drive firm performance, few empirical studies have directly examined any (and none more than one) of these mechanisms. Thus, little is known about the underlying ""why"" of mechanism(s) linking firms' market share and economic performance and how they may both explain previously identified moderators and facilitate identification of additional moderators of this important relationship. In addition, when understanding of the mechanisms linking market share with firm performance suggests that it is economically valuable to measure market share for goal setting and performance monitoring purposes, managers currently have no empirical insights into how to do so.These knowledge gaps are important because understanding why market share is linked to firms' future profit can provide new insights into when and where market share is most likely to be valuable. While many firms use market share as a marketing performance metric, our research identifies new ways for managers to assess when this is most appropriate—and when it may not be. Because market share is such a common marketing goal, this is also important in delineating the role that marketing plays in determining firm performance and in understanding contingencies that may affect this role. Exploring the predictive value of alternative measures of market share, we also provide important new insights into how market share goals should be set and performance assessed via different market share measurement options in terms of unit versus revenue market share and absolute versus relative market share.In addressing these key questions, this study offers several contributions. First, we provide the first direct empirical assessment of the three primary causal mechanisms that have been theorized to link market share with firm profit: market power, operating efficiency, and quality signaling. Using direct measures, we examine each of these three mechanisms simultaneously and show that both market power and quality signaling are key mechanisms linking market share with firm profit. On average, we find little evidence of theorized economies of scale and learning benefits of market share, but we identify conditions under which such efficiency benefits do exist. We find no support for a fourth theorized mechanism linking market share negatively with profit as a result of a strong competitor orientation. However, we do find support for the same three mechanisms in conditions under which the market share–firm profit relationship is negative—for niche firms and when a firm ""buys"" market share. Overall, these findings provide important new empirical insights into market share's value-creating role.Second, using these new causal mechanism insights, we explore the consistency of the market share–profit relationship across different types of marketplaces and firms where the relative value of market share via the three mechanisms may be expected to vary. We show that the market share–profit relationship varies across industries and firms, and that the different causal mechanisms identified provide high explanatory power for such variations; thus, all three theories from which the hypothesized mechanisms arise can be ""correct."" In addition, this insight provides an empirically supported way for managers to identify when setting market share goals and monitoring market share performance may be more or less valuable. In contrast, we find that using indirect contingencies to try to infer the mechanisms linking market share with performance relationship often does not align with the directly observed mechanism effects, further indicating the value of direct measures in understanding the ""why"" mechanisms involved.Third, we extend recent meta-analytic insights regarding the nature of the relationship between market share and firms' economic performance by using direct measures of the three most widely cited mechanisms: measures of both revenue and unit market share and different market share benchmarks, firm size controls to isolate the benefits of market share versus firm scale, and different econometric approaches to address panel data and endogeneity estimation concerns. These aspects of our study enable us to provide several new insights. For example, we show that for most firms, economies of scale arise from firm size and not firm market share. They also allow us to identify which market share metrics are most predictive of profit for different types of firms and the economic value of increasing market share on these metrics. This is useful new knowledge for managers because it provides new insights into how market share should be measured in goal setting and performance monitoring as well as the scale of profit benefits that may be expected from any gain in a firm's market share.The article is organized as follows. First, we develop a conceptual framework and hypothesize relationships involving the three key mechanisms by which market share may be linked with firms' future profit. Next, we use the three mechanisms to identify three conditions under which the market share–profit relationship may be expected to be stronger versus weaker. We then describe the data set assembled and analysis approaches used to test the hypotheses and discuss the results. Having shown that the three mechanisms collectively mediate the market share–profit relationship, we then assess whether this remains true even under conditions when the market share–profit relationship is negative. Next, having shown that managers can use knowledge of the three mechanisms to identify when market share is likely to be economically valuable for their firm, we assess how managers may best measure market share. Finally, we assess the implications of our study for theory and practice and identify new questions for future research suggested by our findings. Conceptual Framework and HypothesesMuch of the theorizing regarding market share and firm performance in economics and management concerns related but distinct phenomena such as firm size and market concentration. We focus only on relationships that directly pertain to firm market share and the mechanisms underlying its economic value. As a result, we center our market share conceptualization on revenue market share—units sold × realized price (i.e., sales revenue) divided by total market sales revenue. In doing so, we conceptualize and measure the ""market"" as comprising firms selling similar product/service offerings. However, we also examine unit market share—units sold divided by total market unit sales—as well as several different operationalizations of revenue market share in robustness checks and post hoc analyses. Market Share and Firm Economic PerformanceThe marketing literature generally views market share as an indicator of the success of a firm's efforts to compete in a product-marketplace (e.g., [13]; [63]). From this perspective, market share is an outcome of a firm's marketing efforts including its advertising and promotion, product/service offering quality and price, channel and customer relationships, and selling activities ([24]). All of these are evaluated relative to those of other suppliers by customers (channel members and end users) when they consider and select offerings, which is what conceptually distinguishes a firm's market share (how the firm's sales compare with those of the total market) from its sales revenue (the number of units sold × price). Importantly, this means that (unlike sales revenue) market share is not a component variable in any indicators of firm economic performance,[ 6] so there is no synthetic (or ""hard-wired"") market share–firm economic performance relationship.Historically, the empirical literature provided conflicting and equivocal answers concerning the ""main effect"" relationship between firms' market share and their economic performance (e.g., [11]; [36]; [37]). However, the recent E-H (2018) meta-analysis using more sophisticated methodological approaches has provided new insight on this question, showing a generally positive effect of market share on firm economic performance. We corroborate this in our data and focus our hypothesizing on why this relationship exists and how this ""why"" understanding may help explain and predict differences in the strength of the relationship across firms and industries. Mechanisms Through Which Market Share May Impact ProfitWhile several explanations have been independently proffered for why a firm with higher market share may enjoy superior economic performance, three mechanisms are much more widely discussed than others. As Figure 1 shows, we focus our theorizing on these mechanisms and consider how each may link a firm's market share with its profit.Graph: Figure 1. Conceptual framework. Market powerThe first proposed mechanism by which market share may be linked with firm profit is via market power (i.e., the firm's ability to influence the price of its product/service offerings by exercising control over demand, supply, or both; e.g., [10]; [59]). Industrial organization theory posits that firms enjoy superior profit when they are able to charge higher prices than rivals, which is determined by the availability of alternatives to customers and firms' ability to create and/or control resources that give them stronger market positions (e.g., [57]). Market share may be a resource that provides a firm with the opportunity for greater market power over both ""upstream"" suppliers and ""downstream"" channels and customers and thereby control prices in several ways.For upstream suppliers, buyer firms with higher end-user market share are more attractive, which may allow them to negotiate lower prices and/or higher-quality inputs from their suppliers ([ 9]). For example, Apple's smartphone market share allows it to both charge app developers for selling their products and enforce strict quality controls on the apps it sells. It may also increase supplier willingness to cooperate with others in the buyer's supply network to further lower the buyer's input costs and improve input quality ([28]). For downstream channels, higher–market share firms are more attractive upstream partners because they generate end-user demand for more and/or higher-value products. They may also attract larger customer numbers and/or more frequent interactions for channels to engage in cross-selling. This may enable higher–market share firms to negotiate better list prices than rivals in downstream channels and to benefit from greater channel cooperation (e.g., preferred shelf-space, merchandizing support). For example, PepsiCo's snacks division leverages its leading market share position to obtain preferential shelf and display access in many U.S. retail chains. The input and go-to-market cost and quality benefits of higher–market share firms should allow them to provide better value offerings, which may thus allow them to charge higher prices to end users (as in the case with Apple) and/or enjoy higher profit margins on each unit sold (e.g., Walmart). Thus, H1:  The positive effect of market share on firm profit is mediated by the firm's market power. Operating efficiencyThe second theorized mechanism by which a firm's market share may lead to profit is via increasing the firm's operating efficiency (e.g., [17]). Disputing market power arguments, the ""Chicago school"" in economics argues that market share is an outcome of firm efficiency that allows a firm to sell quality-equivalent offerings at lower prices than rivals, attracting greater demand (e.g., [14]; [50]). Following this logic, strategic management scholars propose that higher market share may also allow firms to further increase their efficiency in a recursive relationship with lowering firm costs via learning effects (e.g., [ 1]; [29]). Much of this logic is framed in terms of a firm's position on the production ""experience curve"" as a function of the volume of units sold, with greater experience allowing production-related learning and lower production costs (e.g., [30]). Thus, firms selling a greater number of units produce more and learn how to do so more efficiently. For example, Tesla has used its greater accumulated experience in producing electric vehicles (EVs) to lower its costs compared with rivals.Conceptually, this may also be possible via market share impacting the number of interactions a firm has with suppliers, channels, and customers, enhancing opportunities for higher–market share firms to learn and use knowledge gained to improve their supply-and-demand chains ([55]). For example, Tesla has used its greater EV sales to learn how to drive improvements in battery designs and configurations from suppliers as well as to optimize its own software to increase EV range. More interactions also increase the likelihood that suppliers, channels, and customers will trust higher–market share firms, increasing information sharing, lowering coordination costs, and enhancing cooperation in changes designed to enhance the firm's supply-and-demand chains ([16]; [27]). This should enable higher–market share firms to lower costs and enhance supply-and-demand chain quality and reliability, allowing superior value offerings for customers and/or greater margins. Thus, H2:  The positive effect of market share on firm profit is mediated by the firm's operating efficiency. Quality signalingThe third mechanism by which market share may enhance firm profit is by signaling unobserved quality. Information economics theory posits that customers' limited evaluative knowledge often makes it difficult for them to observe ""true"" product/service quality (e.g., [38]; [41]). Empirical studies also show that customers are often unable to accurately (or confidently) evaluate an offering's quality prior to making purchase decisions, and they frequently rely on indirect cues (e.g., [47]; [61]). Market share may signal quality by increasing the credibility of firm claims and thereby lowering customer perceived risk ([21]; [34]). Customers may also infer that ""everyone can't be wrong"" in choosing the offerings of a high–market share firm (e.g., [18]). For example, Toyota campaigns have touted that its products are ""#1 for a Reason."" Thus, to the extent that market share signals higher quality, it should increase future demand and reduce customer churn. It may also lower the firm's costs relative to rivals, because alternative ways to signal quality (e.g., advertising) may be more costly.Market share may also signal quality to suppliers and channel members. Firms that are perceived to be producing high-quality offerings may be viewed by suppliers as not just attractive buyers, in terms of their own demand, but also as potentially providing a halo image spillover benefit. Similar to customers viewing them as having ""too much to lose"" to provide inferior offerings, supplier choices made by high–market share firms may be viewed as being based on ensuring high quality and reliable inputs to protect their reputation and market position. For example, Apple's suppliers are frequently identified as such in business press reports. This could also apply to channel partners where selling offerings that are perceived as higher quality can provide a halo effect making the channel member more attractive to other suppliers and end-user customers (e.g., [42]). All of these arguments suggest the following: H3:  The positive effect of market share on firm profit is mediated by the firm's perceived quality. Using Mechanism Insights to Predict Where Market Share Is ValuablePrior research suggests that the value of market share varies across industries (e.g., [ 4]), indicating that setting market share goals may be more beneficial for some firms than others. To explore this, E-H's (2018) meta-analysis examines the sample characteristics most commonly reported in prior studies and reports that market share is more valuable in business-to-customer (B2C) markets and in markets with medium market concentration, whereas it is less valuable in the banking industry. While offering initial useful insight to managers, these boundary conditions are limited in number and scope—and the ""why"" mechanisms involved are unobserved. Robust empirical understanding of the mechanisms using direct assessments should allow additional boundary conditions to be identified and provide empirically verified principles for managers to distinguish when they should and should not care about market share.To provide an initial assessment of the predictive value of our mechanism results and offer new insights for managers, we next examine the extent to which the market share–profit relationship varies under conditions in which each of the three mechanism in turn may be expected a priori to be more versus less important. For each mechanism, we identify a condition expected to be particularly impactful on that particular market share–profit pathway. However, in our analyses we also allow for the possibility that each of the conditions we identify may affect the strength of all three mechanisms linking market share with profit. First, in terms of market power we examine industries characterized by higher customer switching costs, where firms are more easily able to retain customers. Firms should benefit more from the market power provided by market share when switching costs are high because they are better placed to increase prices without fear of customers switching ([23]; [58]; [60]).Second, in terms of the value of operating efficiency in explaining the market share–profit relationship, the literature suggests that cost-reducing learning effects are more likely earlier in the life of a firm (e.g., [66]). For example, ""experience effect"" studies of the value of a firm's cumulative doubling of output show that this is more likely to occur early in a firm's existence (e.g., [31]). In addition, learning effects require changing and adapting firms' processes—which tend to become more rigid over time (e.g., [54]). Thus, younger firms are less knowledgeable in their operations and less ""set in their ways,"" providing incentives to seek out the learning opportunities presented by market share and the ability to exploit the efficiency-enhancing knowledge gained via process changes.Third, to explore conditions where the quality-signaling value of market share may be stronger, we examine differences between ""service-dominant"" and ""goods-dominant"" industries.[ 7] A key difference between these markets is the greater intangibility of service offerings, which creates more quality uncertainty for customers ([68]). Under such conditions, customers are more likely to use cues such as market share as indicators of the quality of a firm's offerings (e.g., [12]). Interestingly, this prediction is the opposite of E-H (2018), who reason that physical goods manufacturers may benefit more from efficiency, and that this may be more important in driving profit than any dampening of the quality-signaling effect of market share in physical goods-focused markets. We explore this reasoning empirically when we directly examine the three mechanisms underpinning the market share–profit relationship.We therefore hypothesize the following: H4:  The effect of market share on firm profit via market power is stronger in marketplaces with higher switching costs. H5:  The effect of market share on firm profit via efficiency is stronger for younger firms. H6:  The effect of market share on firm profit via perceived quality is stronger for firms selling service- versus product-dominant offerings. Methodology DataWe combine secondary data from a variety of sources. From Compustat, we obtained data to construct measures of market power and operating efficiency, firm economic performance indicators, firm-specific controls, and a set of industry and competitive context control variables. Equitrend provided data on the perceived quality of firms' offerings. To calculate measures of unit market share, we use unit sales data from the Global Market Information Database (GMID). We assembled our initial data set by merging data from Compustat and GMID. To test the mediation hypotheses, we also require data from Equitrend, for which our access covers only the years 2000–2013. Because each data source has distinct firm and year coverage, the compiled data set used to confirm the main effect of market share on firm profit and test the hypothesized mediation effects contains 3,058 firm-year observations from 244 individual firms, operating in 126 North American Industry Classification System (NAICS) four-digit industries, 2000 through 2013. The average firm in this sample has $13.81 billion in assets and has been operating for 45 years. Table 1 shows summary statistics and correlations for the main variables in our sample and additional details are contained in Web Appendix 1. To test H4–H6, we also required American Customer Satisfaction Index (ACSI) data (to measure switching costs), which reduced our sample for testing these three hypotheses to 2,629 firm-year observations from 207 firms (2000–2013).[ 8]GraphTable 1. Descriptive Statistics and Correlations (N = 3,058). VariablesMeanSD1234567891011121314 1. Firm Profit ($M)839.101,104.101.00 2. Market Share (%)6.859.59.141.00 3. Sales Revenue ($M)3,907.085,747.18.77.141.00 4. Market Power (%)30.7910.90.23.13.091.00 5. Firm Efficiency (Index)50.099.06.10.08.38.231.00 6. Perceived Quality (Index)65.3516.95.18.07.06.35−.051.00 7. Firm Size ($B)13.8162.89.39.16.69.17.03.221.00 8. Market Growth ($M)122.23523.18.10.02.03.14−.04−.10.301.00 9. Advertising ($M)53.92255.75.44.34.36.07−.01.01.31.031.0010. R&D ($M)64.92366.32.40.30.22.15−.02−.07.32.05.481.0011. Service Indicator (0/1).19.39−.02−.10−.15−.03.18−.05.08−.04−.08−.051.0012. Switching Costs (Index)−.011.11.33−.14.06.16.07.02.18.14.19.07.111.0013. Firm Age (Years)45.3041.30.21.05.09.14.10.21.18−.08.49.41−.08.341.0014. Niche Focus (Index)2.438.10.02−.15−.02.12.06.05−.07−.06.06.13.12.01−.061.00 1 Notes: All descriptive statistics are for the ""raw"" (i.e., untransformed) variables. Correlations with an absolute value larger than.046 are significant at p < .01, and those greater than.035 are significant at p < .05. Hypothesis Testing Variable MeasurementThe Appendix contains definitions and operationalization details of all variables described next. Market shareMarket share is the percentage of a market's total sales garnered by a firm over a specified time period ([25]). The market may consist of all suppliers selling products/services with the same characteristics, or those that are thought of similarly by customers and are purchased for the same use. We follow [35] to compute a measure of market share using a set of competitors and market definitions derived from business descriptions in ﬁrm 10-Ks. This allows market definitions to be dynamic, where a firm may move in and out of any given market depending on whether its offerings changed over time and thus compete with a different set of firms.To compute market share, we divide the total sales of each firm by the aggregate sales for that market for that year, where the market is dynamically defined as described previously using data from all 22,076 firms in Compustat for the 2000–2013 period. In defining markets, we note that each firm has a similarity/competition score with respect to any other firm (i.e., all possible dualities are computed) in the Compustat database. In line with [35], the number of competitors can be defined using a threshold of similarity scores and/or specified number of nearest neighbors (e.g., 50 or 20). We combine the two approaches and specify 50 as the largest number of neighbors, while also imposing a minimum threshold limit. Thus, our market definition comprises a maximum of 50 firms per industry, while allowing for fewer firms, to maintain a minimum level of similarity among competitors in the same market.[ 9]To assess the robustness of the findings using this dynamic measure of market share, we also use a more static approach, defining markets via each firm's primary NAICS designation using the four-digit level that researchers suggest most closely represents the real ""competed"" market (e.g., [44]). To calculate this, we first collect the total revenue-by-industry data that comprise gross domestic product (i.e., total expenditures on products and services) for all four-digit NAICS industries from the U.S. Bureau of Economic Analysis, which allows us to account for the sales of firms that are private, small, or otherwise not available in Compustat. We then divide the total sales of each firm by the gross domestic product value for that four-digit NAICS industry for that year. Firm market shares are computed from their revenues in their primary NAICS markets. Firm profitWe use net income as our primary measure of firm profit, obtained from Compustat. We use this indicator of absolute firm profit (while controlling for asset size in our model) because economic theories of the value of market share assume that maximizing the amount of profit—not the efficiency with which profit is generated, which is what ""return on asset"" (or investment) relative profit measures capture—is a firm's superordinate performance objective. Market powerWe use profit elasticity relative to the industry average (similar to [ 8]) to indicate firm-level market power. This is calculated by estimating regressions of firms' profit (net income) on their total variable costs for each industry as follows: ln(πit)=α+βln(tvcit)+εit, Graphwhere π is firm profit and tvc is the firm's total variable cost (Cost of Goods Sold + Selling, General and Administrative Expenses) for firm i at time t. Both profit and variable costs are scaled by firm size (total assets). Because profit and costs are natural log transformed, the β from this regression captures the average profit elasticity within the industry, with less negative βs indicating the average ability of firms within the industry to mark up prices when costs rise and thus exercise market power (e.g., [39]). Firm-specific residuals measure each firm's margins relative to its industry's average, providing an indicator of firm's market power ([ 8]). Positive residuals (equivalent to less negative elasticities) indicate greater market power, and negative residuals (i.e., more negative elasticities) indicate weaker market power. Web Appendix 2a indicates favorable face validity for this measure. Firm efficiencyFrom an economic theory viewpoint, this concerns producing goods and services in ways that optimize the combination of inputs to produce maximum output at the minimum cost ([ 5]). To operationalize productive (in)efficiency, we use a stochastic frontier estimation approach. Following [ 5], we use operating expense as the input and total sales as the output. In stochastic frontier estimation, the firm in the industry with the lowest input requirements to produce a given set of outputs forms the efficiency frontier and the closeness of a firm's inputs-to-outputs to this frontier determines its relative (to the industry's most efficient firm) efficiency. Web Appendix 2b provides evidence of strong face validity for this measure. Perceived qualityWe use the perceived quality measure of brands from the Equitrend database, which comprises consumer ratings on an 11-point perceived quality scale. For multibrand firms, we take the mean perceived quality of all brands owned by the firm.[10] Face validity assessments for this measure (see Web Appendix 2c) provide strong support for the measure. Switching costsWe use ACSI data and follow [53] to construct an industry-level measure of switching costs as the ""excess loyalty"" displayed by customers to suppliers using the residual of regressing each industry's customers' loyalty onto its customers' satisfaction, controlling for time fixed effects (FEs). This measure has been shown to have strong face validity ([53]), and we also find evidence of this (Web Appendix 3). Service- (vs. product-) dominant industriesService- (vs. product-) dominant industries is a dummy variable identifying firms operating in nonbanking (banks have idiosyncratic characteristics we later explore) service-focused industries using Fama–French industry definitions ([22]). Firm ageFirm age is the number of years since the firm's founding using information from annual reports and websites. Control variablesIn addition to firm and year FEs used to control for unobserved heterogeneity, we employ several firm- and industry-level covariates in our analyses, including firm size, operationalized as the logarithm of each firm's total assets to account for scale economies not captured by market share, and the firm's advertising and research-and-development (R&D) expenditures to control for firm-level heterogeneity. We also control for market growth that may affect the profit outcomes of market share ([56]), captured as the year-to-year change in total market sales.The Appendix and Web Appendix 1 summarize descriptive statistics for all variables used in our analyses. To enable log-log specification and interpretation in our analyses and reduce deviations from normality present in several of our measures (market share, firm profit, market power, firm efficiency, perceived quality, advertising expense, R&D expense, and market growth), we applied log transformations to our data.[11] Model SpecificationWe empirically test the hypothesized relationships using a fixed-effects autoregressive (FE-AR) estimation approach ([65]) for several reasons. First, we are using panel data, and the Hausman test indicates that an FE correction is needed to address unobserved heterogeneity and separate between time-variant and -invariant firm-specific errors. Second, several of our measures are longitudinally persistent, raising concerns about serial correlation—the AR correction of the errors addresses any potential bias to the estimates. The modified Durbin–Watson and Baltagi–Wu LBI tests indicate that an AR1 correction is appropriate. In addition, we control for heteroskedasticity using cluster-adjusted robust standard errors at the firm level. Finally, we estimate our hypothesis-testing models using generalized least squares (GLS), because OLS are statistically inefficient and may result in biased inference in the presence of serially correlated residuals.We first verify the average positive relationship between market share and profit (E-H 2018) and estimate the total effect using the following model specification: Profiti,t+1=α0+α1MarketSharei,t+α8FirmSizei,t+α9Advertisingi,t+α10R&Di,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph( 1)where i stands for firm and t for time (year), ζi is a time-invariant firm FE, and εi, t + 1 is the random error representing all unobserved influences on future profit, modeled as an AR1 process such that εi, t + 1 = ρεi, t + ηi, t + 1 and where |ρ|<1 and ηi, t + 1 is an independent and identically distributed (i.i.d) error. Market Share, Firm Size, Advertising, R&D, and Market Growth are as described previously, and Year FEs are mutually exclusive year dummies. Lagged regressors are used to alleviate concerns due to simultaneity and reverse causality (i.e., future profit should not impact past market share).Having selected an appropriate estimation approach given the nature of our data, we next deal with potential endogeneity concerns with respect to omitted variables—of which reverse causality and simultaneity are special cases ([65]). We examine the potential for the presence and effect of such endogeneity concerns using a Gaussian copula correction to Equation 1 and assess the presence and effect of any endogeneity (including potential selection bias introduced by the various data sets on which we draw for our measures) via a likelihood ratio test of whether there is a significant difference between the uncorrected set of parameter estimates and the endogeneity-corrected set ([65]).[12] Once we show that potential endogeneity issues are not material, we empirically test H1–H3 using an identical FE-AR approach by estimating the following equations: Profiti,t+1=α0+α1MarketSharei,t+α2MarketPoweri,t+α3FirmEfficiencyi,t+α4PerceivedQualityi,t+α5SwitchingCostsi,t+α6ServicesDummyi,t+α7FirmAgei,t+α8FirmSizei,t+α9Advertisingi,t+α10RDi,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph(2\rm a) MarketPoweri,t+1=β0+β1MarketSharei,t+β5SwitchingCostsi,t+β6ServicesDummyi,t+β7FirmAgei,t+β8FirmSizei,t+β9Advertisingi,t+β10RDi,t+β11MarketGrowthi,t+YearFEs+τi+ξi,t+1, Graph(2\rm b) FirmEfficiencyi,t+1=γ0+γ1MarketSharei,t+γ5SwitchingCostsi,t+γ6ServicesDummyi,t+γ7FirmAgei,t+γ8FirmSizei,t+γ9Advertisingi,t+γ10RDi,t+γ11MarketGrowthi,t+YearFEs+μi+ςi,t+1, Graph(2\rm c) PerceivedQualityi,t+1=θ0+θ1MarketSharei,t+θ5SwitchingCostsi,t+θ6ServicesDummyi,t+θ7FirmAgei,t+θ8FirmSizei,t+θ9Advertisingi,t+θ10RDi,t+θ11MarketGrowthi,t+YearFEs+νi+φi,t+1, Graph(2\rm d)where Market Power, Firm Efficiency, Perceived Quality, Switching Costs, Services Dummy, and Firm Age are as described in the variable measurement section, and all other variables and subscripts follow Equation 1. Finally, we empirically test H4–H6 by estimating the moderated-mediation contingencies and include interactions between Market Sharei,t and Switching Costsi,t, Services Dummyi,t, and Firm Agei,t in Equations 2a–2d. To estimate the relative effects of the three hypothesized mediation mechanisms (market power, firm efficiency, and quality signaling) and three moderated-mediation contingencies (switching costs, firm age, and services), we follow [51] using [64] approach to augment the FE-AR estimation. Results and Discussion Main Effect of Market Share on Firm ProfitPrior to testing the hypothesized mechanisms, we first verify the main effect results indicated in the E-H (2018) meta-analysis in our sample using several variants of the model specification detailed in Equation 1. We begin by estimating a model with FEs and cluster-adjusted robust standard errors that includes only the covariates as predictors (M1), to which we then add market share (M2), allowing us to verify the main effect of market share on firm profit and reveal its incremental predictive power. We also estimate this same model using an FE-AR error correction and cluster-adjusted robust standard errors (M3) to demonstrate the stability of the estimates across the different statistical corrections proposed. In M4 we examine whether the reported estimates suffer from endogeneity bias by including a Gaussian copula for the Market Share variable as a control function to empirically correct endogeneity bias. The likelihood ratio test for joint parameter differences ([65]) indicates that the endogeneity-corrected estimates in M4 are not statistically different from those in M3.As Table 2 shows, the estimates are consistent across all four models, demonstrating the robustness of the effect of market share on firm profit. In addition, while the Gaussian copula estimate in M4 is significant (.048, p < .05) indicating the presence of some omitted variable endogeneity, the likelihood ratio test indicates no significant difference in the market share parameter estimates between M3 (β = .137) and M4 (β = .159). This supports the use of an FE-AR( 1) (i.e., model specification M3) estimation approach and confirms that any remaining bias is modest and does not substantively impact the estimates. In a robustness check, we also replaced the dynamic market share measure with a four-digit NAICS alternative and again confirmed the main effect (Web Appendix 5). Finally, we further verified that endogeneity bias does not unduly influence our findings using a difference-in-differences version of Equation 1 comparing the market share–profit relationship for firms in industries that experience an exogenous demand shock (exit of bankrupt firms) with those that do not. The results (Web Appendix 6) again confirm the main effect findings.GraphTable 2. Main Effect of Market Share on Firm Profit. Models and Dependent VariablesM1M2M3M4Independent VariablesProfit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).153**.137**.159**(.053)(.038)(.052)Controls Firm Size(t).228*.208***.521***.291***(.113)(.067)(.045)(.051) Advertising(t).234***.130**.073***.098*(.061)(.045)(.023)(.044) R&D(t).061*.044.066***.042***(.027)(.025)(.014)(.010) Market Growth(t).020.012.002.029*(.017)(.020)(.002)(.013) Market Share(t)COPULA.048*(.021)Specification Tests  Wald χ2125.32198.12188.36115.92 R2.57.59.58.59 Rho_AR.40.43 2 *p < .05.3 **p < .01.4 ***p < .001.5 Notes: All model specifications estimated using 3,058 firm-year observations. M1/M2: GLS estimation, FEs and cluster-adjusted robust standard errors. M3/M4/M5: GLS estimation, FEs with AR errors and cluster-adjusted robust standard errors. Z-test difference in share coefficients between M3 (.137) and M4 (.159) = .64 (p > .05).Collectively, these analyses verify the main effect results in E-H (2018) that, on average, firm market share positively predicts future firm profit—and the effect sizes reported on Table 2 are both consistent and aligned with the average elasticity of.132 reported by E-H (2018), further enhancing confidence in our findings. Table 2 results also show the suitability of the FE-AR error correction and cluster-adjusted robust standard errors GLS estimation approach (model specification M3), which we employ in the hypothesis-testing analyses. Hypothesized Mechanism (Mediator) ResultsAs Table 3 shows, in testing H1–H3 we find support for both market power in M1a (.230, p < .001) and quality signaling (.141, p < .05) in M1c as mechanisms linking market share with firm profit. However, while M2 confirms that firm efficiency predicts firm profit (.129, p < .001), M1b reveals that a firm's efficiency is not predicted by its market share (.024, p > .1). Thus, on average we find no evidence supporting efficiency as a mechanism linking firm market share and profit in our sample. Overall, these results provide support for H1 and H3 but not for H2. As M2 shows, all three of the mechanism variables are significant predictors of firm profit, and the main effect of market share becomes insignificant (.031, p > .10) in the presence of these three variables. To examine the relative strength of the mediator role played by the three mechanism variables in explaining the market share–profit relationship, we follow [64] approach. This reveals that the three mechanisms collectively explain 77.37% of the total effect of market share on firm profit, with 63.21% of this flowing through market power, 33.96% via perceived quality, and 2.83% through firm efficiency.GraphTable 3. Mechanism for Market Share Effect on Firm Profit. Models and Dependent VariablesM1aM1bM1cM2Independent VariablesPower(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).230***.024.141*.031(.081)(.016)(.065)(.018)Indirect Effect Market Power(t).302***(.042) Firm Efficiency(t).129***(.029) Perceived Quality(t).274***(.061)Controls Firm Size(t).029*.027***.039***.210***(.013)(.006)(.008)(.029) Advertising(t).020.021.022*.090*(.023)(.020)(.010)(.043) R&D(t).032**.013***.028**.023***(.011)(.002)(.011)(.005) Market Growth(t).012.007.012.008(.019)(.009)(.010)(.007)Specification Tests −Log-likelihood2,810.17 R2.16.18.10.68 6 *p < .05.7 **p < .01.8 ***p < .001.9 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period (Equitrend available 2000–2013). Total effect (from Table 2: M3).137 (100.00%) minus direct effect (from M1a).031 (22.63%) = indirect effect of.106 (77.37%). Indirect effect via ( 1) Power = .067 (63.21%); ( 2) Quality = .036 (33.96%); and ( 3) Efficiency = .003 (2.83%).To check the robustness of the mechanism results, we conducted four additional analyses. First, to check for any potential scale effect of absolute sales revenue beyond firm size, we reestimated our model using market share ranks and adding firm sales revenue as a separate control. The estimates replicated the hypothesis-testing results (Web Appendix 7). Second, to check for any potential biasing effect of firm orientation to market share ([43]) we used text analysis of 10-K reports to construct an annual measure of each firm's market share focus based on the number of times ""market share"" is mentioned relative to the total number of words. When this is added to our model, we find that the results remain essentially unchanged (Web Appendix 8). Third, to ensure that results are robust to alternative firm performance measures, we replaced net profit in turn with return on assets and Tobin's q as dependent variables. As shown in Web Appendices 9 and 10, we replicate the hypothesis-testing results. Fourth, we also checked that a firm's competitor orientation—a potential fourth mechanism linking market share (negatively) with firm profit ([ 3])—does not explain additional variance in the market share–profit relationship. Using 10-K reports and [ 7] text-based measure, we computed the competitor orientation of each firm in our sample and included this in our model. As Web Appendix 11 shows, we find that while competitor orientation predicts firm market share, it does not materially affect the market share–profit relationship. Hypothesized Moderating Condition ResultsHaving demonstrated the robustness of the hypothesized mechanism results, we next examine whether the market share–profit relationship may be stronger in industry and firm conditions in which each of the three mechanism variables in turn may be expected a priori to be more versus less important as captured in H4–H6. The results are summarized in Table 4, with M1 showing that firms in industries with higher customer switching costs are more profitable (.137, p < .05), and M2 supporting H4 by confirming that market share is more valuable in such industries (.087, p < .001) via its stronger effect on market power (.157, p < .05). In addition, M4c reveals that firms also gain stronger perceived quality benefits from market share in industries with higher switching costs (.203, p < .05), suggesting that some of the switching costs we observe are due to customers continuing to choose a provider because of positive relational bonds that may influence both customers and others' perceptions of the quality of such firms' offerings.GraphTable 4. Main Effect and Mechanisms for Market Share Effect on Firm Profit in Hypothesized Moderators. Model Specifications (M) and Dependent VariablesM1M2M3aM3bM3cM3dM4aM4bM4cM4dIndependent VariablesProfit(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effects Market Share(t).118***.114***.105*.031.278**.017.136***.028.149***.030Indirect Effects Market Power(t).210***.223*** Firm Efficiency(t).075**.083*** Perceived Quality(t).169***.163***Moderators Switching Costs(t).137*.149*.093*.005.051*.013.107.015.077*.027 Firm Age(t).178.208−.002.013.006.015*.034−.031.004.019 Services Dummy(t)−.058*−.059*.017−.033*.008−.004.093.509***.028−.006Interaction Effects Market Share(t) × Switching Costs(t).087***.157*.017.203*.033 Market Share(t) × Firm Age(t)−.069***−.048−.109***−.092*−.043 Market Share(t) × Services Dummy(t).056***−.006.148***.012.020Controls Firm Size(t).514***.534***.025***.031***.046***.028***.030***.083*.041***.046*** Advertising(t).278***.281***.008.022.006.011.007.010.039.039*** R&D(t).274***.272***.039***.010.059*.034***.029***.012.062***.031*** Market Growth(t).014.015*.011.008***.002.004.011.017.012.003Specification Tests Wald χ2303.11358.07 −Log-likelihood2,489.312,913.87 R2.50.52.24.21.22.69.25.29.26.70 10 *p < .05.11 **p < .01.12 ***p < .001.13 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).The interactions reported for M2 also show that market share is generally less valuable for older firms (−.069, p < .001), and consistent with H5, the mechanism estimates in M4b provide strong evidence supporting the expected effect of market share on firm efficiency being weaker for older firms (−.109, p < .001). This is aligned with our rationale that efficiency-enhancing learning effects associated with market share accrue mainly to firms that are earlier in their development. M4c estimates also reveal that older firms benefit less from market share via quality signaling (−.092, p < .05). We reason that older firms that have been in the marketplace for longer are likely to be better known and also that firm age may indicate a firm's stability and lower risk, which reduce the signaling value of its market share.In terms of services-dominant firms, the significant positive estimate in M2 for the services × market share interaction (.056, p < .001) indicates that service firms benefit more from market share. However, our mechanism estimates in M4c show that this is not a result of the expected strengthening of the quality-signaling benefit of market share (.012, p > .10) posited in H6 but rather, as shown in M4b, that service firms benefit more from the efficiency-enhancing effect of market share (.148, p < .001).[13] Because controlling for scale effects via firm size isolates the efficiency-enhancing learning effects of market share, this finding suggests that market share provides a greater opportunity for service firms to learn how to operate more efficiently and to use this knowledge to change their operations to do so. We reason that this may be because the greater direct customer interactions from higher market share are more valuable in helping service firms learn how to efficiently deal with customer heterogeneity, and that applying what is learned may also be less capital-intensive for service firms (vs. manufacturers). Additional Analyses of Hypothesis-Testing EffectsTo provide additional insight into how the hypothesized moderators affect the profit value of market share via the three mechanisms, we examined these effects in an additional analysis (Table 5). Of the.086 total effect (elasticity) of market share on profit when the moderator variables are included in the model,.056 is indirect (65% of the total) via the three mechanisms, with 62% of this flowing through market power, 6% through firm efficiency, and 32% via perceived quality. Consistent with the H4 testing results (Table 4), the effect of market share on firm profit is strengthened by switching costs, with the total effect amplified by.287 for each unit increase in switching costs, of which.195 is indirect via market power (50.9%), firm efficiency (2.5%), and perceived quality (46.6%). These direct and indirect effects of switching costs on market share's effect on firm profit are proportionately lower (higher) at lower (higher) levels of switching costs (i.e., ± one standard deviation around average switching costs) with the indirect effects flowing through the three mechanisms in very similar percentages.GraphTable 5. Indirect Effects for Market Share Effect on Firm Profit in Hypothesized Moderators. Market Share–Profit EffectsIndirect Effect MechanismsModerator Variable ConditionsTotal EffectDirect Effect% of TotalIndirect Effect% of TotalPowerEfficiencyQualityOverall.086*.03034.9%.056*65.1%62.0%6.0%32.0%Switching costs.287***.092*32.1%.195***67.9%50.9%2.5%46.6% +1 SD.345***.111*32.2%.234***67.8%51.2%2.4%46.4% −1 SD.218***.073*33.5%.145***66.5%51.1%2.4%46.5%Service dominant.032*.02062.5%.01237.5%41.0%21.0%38.0%Product dominant−.032*−.01031.2%−.02268.8%54.0%3.0%43.0%Firm age−.136***−.01410.3%−.122***89.7%12.1%45.5%42.4% +1 SD−.170***−.01810.6%−.152***89.4%17.1%40.2%42.7% −1 SD−.081*.011−13.6%−.092*113.6%2.7%56.8%40.5% 14 *p < .05.15 **p < .01.16 ***p < .001.17 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).Consistent with H5 testing results (Table 4), the total effect of market share on firm profit is also amplified for service-dominant firms by an extra.032, of which.012 is indirect (38% of the total) and flows through market power (41.0%), firm efficiency (21.0%), and perceived quality (38.0%). Meanwhile, for product-dominant firms, the total effect is reduced by −.032, of which −.022 is indirect, with 54.0% flowing through market power, 3.0% through firm efficiency, and the remaining 43.0% via perceived quality.Finally, in line with H6 testing results (Table 4), Table 5 shows the effect of market share on profit is weakened by firm age with each additional year reducing the total effect of market share on profit by −.136, of which −.122 is indirect (90% of the total) and flows through market power (12.1%), firm efficiency (45.5%), and perceived quality (42.4%). As we expected, the total effect of firm age on the market share–profit relationship is more pronounced for very high (old) versus very low (young) age levels, with a marked increase in the indirect effect flowing through firm efficiency (from 40.2% to 56.8%) and decrease in that flowing through market power (17.1% to 2.7%) in the case of very young firms. This is consistent with our Table 4 hypothesis testing results revealing stronger efficiency gains with market share for younger firms. Market Share–Profit Mechanisms When Market Share Negatively Impacts Firm ProfitAligned with E-H's (2018) finding that 82% of market share–performance elasticities in prior research are positive (82% of the same elasticities in our sample are also positive), our hypotheses are framed in terms of a net positive performance effect of market share. However, conceptual arguments concerning potential negative outcomes of market share have also been proposed (e.g., E-H 2018; [34]). Drawing on our theorizing, we expect that the three mechanisms we identify should empirically capture any negative and positive effects of market share. For example, any associated diseconomies of scale will reduce a firm's efficiency while a reduction in perceived exclusivity will affect the quality-signaling value of market share. To empirically verify this expectation, we identify two conditions under which market share's positive benefits may be outweighed by negative consequences, such that larger market share might reduce firm profit and reestimate the mediation effects of the market power, firm efficiency, and quality-signaling mechanism in these conditions. Niche firmsOne condition in which market share may negatively predict profit concerns firms with a strategic focus on serving a smaller segment of a market, usually a group of customers with a distinct set of needs and requirements (e.g., [49]). For example, Louboutin specializes in high-fashion stiletto shoes. By serving distinctive needs, niche-focused firms make money by occupying positions in a segment of a broader market in which competition is more limited (e.g., [19]). As a result, they may not serve enough customers to gain market power benefits from market share, and their specialist positioning may diminish any quality-signaling benefit. They are also unlikely to gain from any learning effects in production. However, niche-focused firms with higher overall market shares are likely to have achieved this by selling to customers beyond their original niche ([62]). This may negatively impact the firm's profitability by reducing its original niche appeal via a negative effect on perceived quality (e.g., [34]) and also attract more competition (e.g., [32]). These downsides may outweigh any potential market power and/or firm efficiency benefits of having a larger market share. Firms buying market shareAnother circumstance when market share may negatively impact profit is when firms ""buy"" market share by lowering prices relative to rivals. This is analogous to findings in the sales promotion literature that price promotions often produce negative returns (e.g., [33]). In this circumstance, any market share gain via greater market power and the ability to charge higher prices is not only relinquished but reversed. In addition, because there is a price-perceived quality heuristic among customers in many markets (e.g., [52]), charging lower prices may offset any quality-signaling benefit of higher market share, and the net result on perceived quality could be negative. Our previous results suggest that in most circumstances, these negative market power and quality-signaling effects are likely to outweigh any firm efficiency gains via learning produced by increasing market share. Empirical test of the two conditionsTo assess the robustness of our mechanism results under conditions when the market share–profit relationship may be negative, we first identified firms that are likely pursuing a niche strategy by combining a new text measure indicator of the degree to which a firm has a niche strategic emphasis (for details, see Web Appendices 4a and 4b) with the number of brands they market (both firms with both a high niche-focus in their product-market coverage strategy and those that offer only a single brand are likely to be niche firms). The face validity assessments in Web Appendices 4a and 4b support this identification logic. Second, to identify firms that may be ""buying"" market share, we created a dummy variable indicator for firm-years in which a firm both reduced its average prices (computed using GMID data) and experienced a positive market share change.We then reestimated our market share–profit models from Table 3 with the addition of the new niche firm measure and buying share dummy indicator, along with their respective interactions with market share. As Table 6 shows, model M1 shows that higher market share reduces profit for niche firms (−.115, p < .05). As we expected, M2c reveals that this is a result of a strong negative effect of market share via perceived quality (−.062, p < .001). M1 also shows that the effect of market share on firm profit is significantly lower for firms ""buying"" market share (−.036, p < .001).[14] The mechanism results indicate that this is caused by a significant reversal in both the market power (M2a: −.047, p < .001) and firm efficiency (M2b: −.033, p < .001) effects of market share and a reduction of the perceived quality mechanism to insignificance (M2c: −.022, p > .1). These findings suggest that any supplier input cost benefits of greater market power from market share are more than offset by lowering downstream prices to ""buy"" the market share. In addition, consistent with the well-known ""bullwhip"" effect, rapid increases in short-term demand resulting from lowering price seems to disrupt the efficient production and delivery of these firms' products and services. Overall, the Table 6 results provide support for the robustness of the three mechanism variables in mediating the relationship between firm market share and profit, even in the relatively rare conditions under which the relationship is negative.GraphTable 6. Moderating Effect and Mechanism When We Include Conditions in Which Market Share May Have a Negative Effect on Profit. Model Specifications and Dependent VariablesM1M2aM2bM2cM2dProfit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).058***.091***.034.108***.033Indirect Effect Market Power(t).218*** Firm Efficiency(t).095*** Perceived Quality(t).179***Moderators Switching Costs(t).149*.118.021.081***.041 Services Dummy(t)−.042*.088.510***.027−.010 Firm Age(t).193.037−.036.008.021 Niche Focus Firms(t).078***−.025***.027.119**.180* Buying Share Dummy(t).016.024−.032*−.009−.026Prior Moderator Effects Share(t) × Switching Costs(t).050*.162*.022.200*.037 Share(t) × Services Dummy(t).063***−.011.166***.018.019 Share(t) × Firm Age(t)−.053***−.055−.113***−.078−.009Proposed Negative Moderators Share × Niche Focus Firms(t)−.115**−.016−.001−.062***−.010 Share × Buying Share Dummy(t)−.036***−.047***−.033***−.022−.036Controls Firm Size(t).490***.035***.086*.039***.049*** ADV(t).233***.008.010.018.041* R&D(t).241***.031***.015.055***.059*** Market Growth(t).022***.023.018.015.012Specification Tests −Log-likelihood3,104.92 R2.55.30.39.20.72 18 *p < .05.19 **p < .01.20 ***p < .001.21 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability). For Niche Firms, indirect effect = 58%, of which Power = 21%; Efficiency = 0%; and Quality = 79%. For Firms Buying Share, Indirect Effect = 33%, of which Power = 56%, Efficiency = 22%, and Quality = 22%. Comparison with E-H's (2018) Indirect Moderator InferencesHaving provided robust evidence to support the three mechanisms, to offer additional insight on the utility of the direct measures of the three mechanisms employed, we also examined how the results compare with previous indirect inferences regarding these mechanisms drawn from observable moderators of the market share–profit relationship. To accomplish this, we first replicated E-H's (2018) measures as well as main effect and substantive moderator results (banking services, concentration, and B2C). We then examined the mechanisms explaining the effect of these moderators of the market share–profit relationship in our sample, and the results are revealing (Web Appendix 12). For example, we find that while E-H's theorizing focuses on quality signaling, the reason for the stronger market share–profit relationship in B2C industries is a significant strengthening of all three mechanisms relative to business-to-business (B2B) industries (market power:.143, p < .001; efficiency:.044, p < .05; quality:.082, p < .05). In addition, we find that while banks are in general more profitable (.426, p < .01) and have greater market power (.042, p < .05), this is in spite of—not due to—their market share (−.087, p < .05). In fact, results reveal that market share reduces banks' profitability by lowering their efficiency (−.410, p < .001). We also find a direct moderating effect for concentration (.109, p < .05), whereas E-H found a nonlinear effect, and we observe that this is via increasing the market power benefit of market share (.110, p < .01). These results show that using moderators to indirectly infer the three mechanisms underlying the market share–profit relationship often does not do a good job of isolating these mechanisms. This reinforces the value of direct empirical understanding of the mechanisms linking market share with firm profit in predicting when market share is more valuable and thus when managers should set market share goals. When Its Value Is Indicated, How Should Managers Measure Market Share?The new empirical understanding of the mechanisms linking market share with firm profit revealed in our analyses can help managers evaluate when market share may be a valuable goal. When its value is indicated, a manager's next task is to decide how to measure market share for goal setting and performance monitoring. To provide insights on this question, we examined two key market share measure design choices facing managers. First, ""share of what?,"" in terms of unit sales volume or sales revenue, should be used in computing market share ([ 6]). Managers use both types of indicators to track market share, and both rank among the most popular measures of marketing performance in practice (e.g., https://marketbusinessnews.com/financial-glossary/market-share/). The second is ""relative to what?,"" in terms of whether and how the firm's market share is benchmarked—as an absolute value (% of total market sales) or relative to others in the market (the market share leader or the top three players). Revenue versus unit shareTo provide insights on the first question, we replicated model M3 in Table 2 and replaced the sales revenue market share with unit sales volume market share using the same dynamic market definition. As we show in Table 7, in contrast to revenue market share (M2:.151, p < .05), unit market share (M1:.009, p > .1) does not predict firm profit. This result is robust to all of the same checks performed on our revenue market share main effect testing analyses and also to using benchmarked (vs. absolute) values of unit market share. Post hoc analysis of the mechanisms associated with unit share (Web Appendix 13) reveal that although it has a small positive effect on both market power and firm efficiency (consistent with the learning effect logic that market share is a proxy for number of units produced), this is insufficient to overcome the significant negative relationship with quality signaling. We reason that the weaker effect of unit (vs. revenue) market share on market power is a result of unit market share ignoring prices charged to customers (a downstream indicator of market power). The negative quality-signaling effect of unit market share is consistent with both ignoring price (which is often a quality cue for customers) and the notion that ubiquity reduces perceived exclusivity (e.g., [34]). These results show that when the presence of the three mechanisms indicates market share's value, managers should set market share goals and monitor performance in terms of revenue market share.GraphTable 7. Market Share–Profit Relationship Using Alternative Market Share Measures and Benchmarks. Market Share Measure, Model, Benchmark, and Dependent VariableUnit Market ShareRevenue Market ShareRevenue Market ShareRevenue Market ShareM1M2M3M4Independent VariablesAbsoluteAbsoluteRelative to Market LeaderRelative to Top 3Profit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).009.151*.222***.392***Controls Firm Size(t).201***.270***.213***.243*** Advertising(t).081**.121***.121***.123*** R&D(t).033.024.033.030 Market Growth(t).001.001.004.006Specification Tests Wald χ2115.23188.91210.81167.81 R2.18.59.52.52 22 *p < .05.23 **p < .01.24 ***p < .001.25 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period, except for model specification M1, which is estimated using 2,214 firm-year observations covering 235 firms for the period 2004–2013 (due to GMID data availability). In a subsequent robustness check, model specifications M2 through M4 were reestimated using the same 2,214 firm-year observations, and estimates remain identical. Absolute versus relative shareIn terms of the ""relative to what?"" question, in Table 7 we compared the market share–profit estimates of the absolute value of market share used in the main effect testing (M2) and two different relative market share benchmark operationalizations: relative to the market share leader (M3) and relative to the combined market share of the top three market share firms (M4).[15] The results indicate that benchmarked measures of firm market share provide stronger predictive power (of future profit) (M3:.222, p < .001; M4:.392, p < .001, respectively) than using absolute market share (M2:.151, p < .05). Subsequent analysis of the three mechanisms show that this is a result of the relative market share measures ""dialing up"" the market share–market power link (Web Appendix 14). This is likely due to such ""relative to others in the same industry"" measures capturing some of the industry-level market concentration power that our previous analysis showed increased the market share–market power relationship in terms of both switching costs (which are higher when markets have fewer equivalent players) and average market share (as an indicator of market concentration in the E-H [2018] replication analyses). Implications Implications for TheoryThis study offers several new insights into theories of firm behavior and performance. First, economic theory assumes that market share predicts firm profit but offers different reasons for why this relationship exists. We provide the first simultaneous test of three mechanisms proffered in competing economic theories for this relationship and show that in combination, they explain the vast majority of the variance in the market share–profit relationship. This suggests that individual single-theory lens explanations of the mechanisms linking market share with profit are incomplete, and all three mechanisms can provide higher (or lower) explanatory power under different conditions. While, on average, market power provides the highest level, and firm efficiency the lowest level, of explanatory power, we also identify conditions under which the reverse is true (e.g., for young firms). Thus, none of the three theories from which the hypothesized mechanisms arise is ""correct"" or ""incorrect,"" but market power and quality signaling generally explain more of the variance in the market share–profit relationship across firms and industries.Second, our results offer new insights into efficiency-enhancing experience-based ""learning effects"" identified in strategic management theorizing ([ 2]). Management scholars have used this logic to explain why market share (a proxy for the number of times a firm may have produced a value offering) may be positively related to firm profit (e.g., [29]). We find that while firm efficiency is valuable (predicts profit), on average it is explained mainly by a firm's size rather than its market share. This suggests that for most firms, scale economies are more important in driving profit than economies of learning. However, for young firms, we find that market share delivers significant efficiency benefits above and beyond those associated with size, and we also find significant efficiency benefits from market share among service businesses. This suggests that ""learning by doing"" effects occur where organizational routines are less set and when firms can use experience gained to update and change their processes with lower investments.Third, we find support for information economics theorizing on the value of signals of unobservable firm quality. While prior research has explored market share's role in consumer evaluations of quality ([34]), we provide the first empirical evidence that market share generally signals firm quality and thereby increases firm profit. The negative effects on perceived quality we observe when using unit (vs. revenue) market share also suggest that price combines with market share in signaling quality to customers. In addition, we find that market share's positive quality-signal effect depends on previously unidentified industry and firm conditions (stronger for younger firms, in B2C markets, and for those with switching costs).For researchers, our study also has broader implications. Not least, it clearly shows the effect that sampling can have on the findings and inferences drawn in firm-level empirical research. We find wide variance in both the main market share–profit relationship and in the specific mechanisms accounting for the relationship across industries. Thus, samples made up of a single industry, or an industry dominated by certain types of firms, would lead to very different results and widely varying inferences being drawn as to which theory may be supported in empirical tests. This is unlikely to be unique to the market share phenomenon we examine. In addition, our study also reveals the desirability of directly observing (or at least finding direct indicators of) mechanisms believed to underlie relationships of interest. In particular, our results highlight the need for researchers to be careful about using indirect contingencies to infer such unobserved mechanisms when there may be more than one mechanism involved. Implications for PracticeThis study also provides new insights for managers regarding how market share should be measured. Although unit (volume) market share is widely used in practice to set marketing goals and monitor performance (e.g., auto and motorcycle manufacturers, many consumer packaged goods companies), our results reveal that it is not predictive of firm profit, whereas revenue (value) market share is. We also find that in terms of predicting profit, relative (to others) measures of revenue market share can be superior to absolute measures. Post hoc analyses suggest that such relative measures can enhance the market power value of the observed market share, and that benchmarking a firm's market share relative to the top three market share firms versus the market share leader offers a stronger predictor of future profit. This is aligned with the intuition that benchmarking against others provides an indicator of both the firm's market share and the concentration present in the marketplace, which we show interact significantly in predicting firm performance.To provide finer-grained managerial insights, we also examined ( 1) which measures of market share were the strongest predictors of future profit for different types of firms to help managers select the most appropriate market share metrics for goal setting and performance monitoring and ( 2) the average profit value of a 1% increase in the average firms' market share for different types of businesses to give managers a calibration of the dollar-value benefits that may be expected when evaluating costs associated with share building strategies. Given our sample size, we are somewhat limited in how fine-grained we can be in these analyses without running into power issues. We therefore split our sample in a managerially meaningful way by identifying firms on the basis of whether they serve primarily consumer or business customers and whether their value offerings are mainly product- versus service-based. As shown in Table 8, the results vary across the four cells, with B2C product firm and B2B service firm profit being most strongly predicted by absolute revenue market share, whereas for B2C service and B2B product firms, it is revenue share relative to the top three market share players. The one-year profit increases associated with a 1% improvement in the average firm's market share vary across the four cells from a low of just over $1 million to almost $6 million. These findings have clear and important implications for managers setting market share goals and monitoring market share performance in their firms and offer a useful dollar benefit scale calibration for managers with respect to the potential payoffs they may expect from investments in market share–building strategies.GraphTable 8. Managerial Matrix: Metrics. ProductsServicesB2CStrongest market share–profit predictorAbsolute revenue shareRelative to top three revenue shareMean firm market share6.80%7.19%Profit value of 1% increase in mean market shareFrom 6.80% to 6.87%:.121% (p < .001) × $840 million = $1.02 millionFrom 7.19% to 7.26%:.704% (p < .001) × $840 million = $5.9 millionObservations1,910 firm/year observations (136 firms)484 firm/year observations (52 firms)B2BStrongest market share–profit predictorRelative to top 3 revenue shareAbsolute revenue shareMean firm market share6.68%7.31%Profit value of 1% increase in mean market shareFrom 6.68% to 6.75%:.309% (p < .001) × $840 million = $2.6 millionFrom 7.31% to 7.38%:.146% (p < .01) × $840 million = $1.2 millionObservations322 firm/year observations (32 firms)342 firm/year observations (24 firms) 26 Notes: Unit share is not predictive of firm profit in any one of the four cells. Reported elasticities estimated via a model specification equivalent to M3 in Table 2, with the noted strongest market share predictor measure as a regressor and using the observations specific to each of the Product/Services and B2C/B2B cells. Profit increase $ values are for a 1% increase in the mean firm's market share in each cell (e.g., 7.310% to 7.383%) not an increase of 1 point of total market share (e.g., from 7.310% to 8.310%). Because we estimate log-log models, the estimated coefficients in each condition can be interpreted as market share–profit elasticities (%) which can be converted to a dollar profit value by multiplying them by the mean profit in our sample (i.e., $840 million).In terms of where managers would be advised to pursue market share to a greater or lesser degree, our results provide several new insights (Table 9). For younger firms and for nonbanking services firms, it may make sense to set market share goals and monitor performance. It may also be more beneficial for firms operating in marketplaces with high levels of quality uncertainty and those with higher switching costs. However, it may make less sense for banks and firms in industries in which pricing power is low and/or quality is relatively certain. Older firms may also find market share to be of less value as a marketing goal and performance metric. Firms pursuing a niche strategy would be well advised to either ignore market share or ensure that they assess it only within their selected niche market definition. Finally, we show that, while relatively rare, ""buying share"" is not a profitable move.GraphTable 9. Managerial Matrix: Contingency Effects on Share-Profit Mechanisms. Relative Mechanism ImportanceContingencyMarket PowerFirm EfficiencyPerceived QualitySwitching costs (high)+n.s.+Service (vs. product)n.s.+n.s.Firm age (older)n.s.−−Concentration (more)+n.s.n.s.B2C (vs. B2B)+++Banking (vs. others)n.s.−n.s. 27 Notes: n.s. = not significant. This table summarizes analyses reported in Table 4 and Web Appendix 12, with mechanism importance indicated relative to the average displayed by all firms in our sample. Implications for PolicyFor policy makers, this study provides new insights with respect to when market share may lead to market power and potential abuse that requires regulation. Importantly, our results show that firm profits from market share result from quality signaling and learning-based efficiencies as well as market power. Thus, policy makers need to be careful not to directly equate market share and market power; we show that while they are often related, they are far from synonymous. Rather, our results suggest that regulatory authorities can be less concerned by a firm's market share in marketplaces where customer quality uncertainty is significant and where efficiency-enhancing learning benefits from market share may exist (e.g., young firms, service firms). In such conditions, market share could enhance rather than harm consumer welfare by reducing consumer–firm information asymmetry and potentially lowering costs. Limitations and Future ResearchThis study has some limitations that should be taken into account when considering the findings. First, because we require public data to explore our research questions, our sample is naturally skewed toward larger firms. While we include small, nonpublic firms in the definition of the total market sales used in constructing the robustness check NAICS measure of market share, we are unable to include such firms' individual market shares in the hypothesis testing because these firms' sales data are private. Although we have a wide range of market shares in our sample (with a low of less than 1%, a high of 77%, and a mean of less than 7%), and no evidence of range restriction in our key variables, researchers with access to private firm data could test the generalizability of this study's findings to firms with much smaller market shares.Second, our data are focused on firms with U.S. listings. However, including studies covering broader geographies and longer time period data, E-H (2018) suggest that the market share–profit relationship is weaker in recent times in Western Europe than the United States, so future research in other regions is required to examine how the mechanisms we identify may differ across geographies. Third, our study examines market share at a firm level. However, market shares may also be computed at other levels (e.g., brand or geographic market level). A post hoc analysis of monobrand firms in our sample suggests that the same market share–profit main effect and mechanism relationships hold (Web Appendix 15); however, research is required to confirm this.Our study also reveals several new avenues for theoretically interesting and managerially relevant research. First, we find that the vast majority of market share's effect on profit is mediated through its effects on firm market power, perceived quality, and efficiency. This suggests that new theorizing regarding why market share is valuable may be of limited value. However, in light of our findings, new research on the details of how each of the three mechanisms works is clearly required. For example, what is the relative effect of market power on upstream versus downstream parties, and how much is contributed by cost reductions versus pricing versus coordination benefits? Similarly, what types and levels of quality uncertainty create conditions that lead to market share's value in signaling quality? How much of market share's signal value is to upstream versus downstream parties?Second, this study reveals market power, quality signaling, and operating efficiency as the mechanisms linking market share with firm profit. Because market share is a market-based outcome of firms' marketing efforts, this raises the interesting possibility that these three mechanisms may also mediate the relationship between other marketing-related phenomena and firm performance. For example, are market-based assets such as brand equity and customer relationships also linked to firm profit via the same three mechanisms? Are there also other mechanisms that may be available to such market-based assets but not to market share?Third, given that market share is more or less valuable under different market and firm conditions—and that buying share is both rare and ineffective—does it also matter how firms create and leverage market share? For example, are market shares more or less valuable to firms pursuing low-cost business strategies versus those pursuing differentiated advantages? Are the three mechanisms linking market share and profit the same for these different strategies, or are some mechanisms more important to one strategy than another? Addressing these questions would provide important new insights for both managers and researchers. Appendix: Measure DetailsGraph Appendix: Measure Details VariablesMeasurement DetailsData Source/LiteratureFirm ProfitNet income of the firm (Item NI).CompustatMarket Share (Revenue)Percentage of an industry or market's total sales garnered by a particular firm over a specified time period. Markets are defined through text analysis of similarity between product-market descriptions within 10-Ks. Sales for each firm obtained from Compustat.SEC, CompustatHoberg and Phillips (2010)Market Share (Units)Units sold by each firm were obtained directly using the GMID (Euromonitor) database. Market definition for firms with unit share data calculated as for revenue share.GMIDMarket Power (Power)Operationalized based on a profit elasticity measure following Boone (2008), estimated by regressing (at the industry level) firms' profit (Item NI) on their total costs (Items COGS and XSGA). Firm-specific residuals are used to calibrate each firm's margins relative to industry average, providing a firm-level indicator of market power.CompustatBoone (2008)Firm Efficiency (Efficiency)Concerns producing goods and services with the optimal combination of inputs to produce maximum output at the minimum cost. We use a stochastic frontier estimation approach with operating expense (Item XOPR) as the input and total sales (Item SALE) as the output.CompustatBauer, Berger, and Humphrey (1993)Perceived Quality (Quality)Measured using customer perceived quality ratings of the firm's brand(s) from Equitrend database.EquitrendMorgan and Rego (2009)Switching CostsThese are perceived costs associated by the firm's customers with moving to an alternative supplier. We calibrate these costs as the degree to which customers exhibit loyalty to a firm that cannot be explained by the level of satisfaction delivered by the firm's offerings. Using ACSI data, we estimate customer-level loyalty as a latent factor comprising variables capturing customers' repurchase intentions and price sensitivity. Satisfaction is the ACSI measure detailed previously. We estimate switching costs for each firm/year as the residual of regressing each firm's customers' loyalty onto its customers' satisfaction, controlling for industry and time.Loyalty(it) = β0 + β1 × Satisfaction(it) + ID(it) + YD(it) + ε(it), where ID(it) are industry and YD(it) year dummies. ε(it) is the residual of this regression and is used as our estimate of switching costs, which are firm- and year-specific.ACSI (firm/year-level aggregation of individual-level respondent survey response data).Rego, Morgan, and Fornell (2013)Niche-Focused Strategy (Niche)Text analysis employing a new dictionary utilizing an inductive word search with exemplar niche firms. The analysis is then performed using a bag-of-words approach where each firm gets a score corresponding to the ratio of niche-related words and total words in each firm 10-K. To ensure that we were isolating the types of niche firms where market share was expected to be negatively associated with profit, suggested in the theorizing (i.e., those pursuing a single niche in a market vs. those targeting several different segments with different offerings), we then identified mono- versus multibrand firms by multiplying the niche-focus score for each firm by the dummy variable (1 for monobrand firms, 0 for multibrand firms).New measureService-Dominant Markets (Services)Dummy variable identifying service firms/ industries using Fama–French NAICS industries.Fama and French (2008)Firm AgeNumber of years of operation of the firm since incorporation, obtained from the firm's annual reports and websites.Industry ConcentrationIndustry-level average market share.Edeling and Himme (2018)B2C versus B2B FirmsDummy variable capturing whether the firm caters mainly to business customers. Each firm was coded manually by three coders who used information on categorization from secondary sources such as Hoover's. Reliability was >85%.Services (Banking)Dummy variable capturing whether a firm belongs to the banking sector (SIC Code 602).CompustatCompetitor OrientationText analysis of 10-K reports following dictionaries on competitor orientation (as a part of Market Orientation) developed in prior literature (Zachary et al. 2011).SECZachary et al. (2011)ControlsFirm SizeThe firm's reported total assets (Item AT).CompustatMarket Growth Annualchange in cumulative industry sales (Item SALE).CompustatR&D ExpenseFirm's reported expenditures on Research and Development (Item XRD).CompustatAdvertising ExpenseFirm's reported expenditures on Advertising (Item XAD)CompustatRobustness Check VariablesROAThe ratio of current year income before extraordinary items (Item IB) to the firm's previous year total assets (Item AT).CompustatTobin's qRatio of the firm's market value to the replacement cost of physical and intangible capital of the firmWe measure the firm's market value as the market value of outstanding equity (Items PRCC_F × CSHO), plus the book value of debt (Items DLTT + DLC), minus the firm's current assets (Item ACT). The firm's replacement cost of physical capital is measured as the book value of property, plant, and equipment (Item PPEGT). Intangible capital is estimated as the sum of the firm's knowledge capital (the capitalized value of firm R&D expenditures) and organizational capital (a fraction of the capitalized value of firm SGA expenditures) following Peters and Taylor (2017).Peters and Taylor (2017)Alternate Market PowerOperationalized based on Lerner Index as profit margin relative to price. Average variable costs are used as a proxy for marginal costs, operationalized using total variable costs divided by sales (Items XOPR and SALE). Average price was estimated dividing sales revenues (Item SALE) by unit sales (obtained from GMID database).GMID, CompustatBoone (2008)Market Share FocusBased on text analysis of 10-K reports, estimated as the ratio of the number of times ""market share"" is reported relative to the total number of words in the annual 10-K report.New measurePerceived QualityMeasured via average annual perceived quality ratings of the firm's brand(s) from the Brand Asset Valuator database.Brand Asset Valuator Mizik and Jacobson (2005)Perceived QualityMeasured using average annual firm quality ratings from Fortune's World's Most Admired Companies database.AMACCretu and Brodie (2007) 28 Notes: SEC = Securities & Exchange Commission; SGA = selling and general administrative.  "
16,"Fields of Gold: Scraping Web Data for Marketing Insights Marketing scholars increasingly use web scraping and application programming interfaces (APIs) to collect data from the internet. Yet, despite the widespread use of such web data, the idiosyncratic and sometimes insidious challenges in its collection have received limited attention. How can researchers ensure that the data sets generated via web scraping and APIs are valid? While existing resources emphasize technical details of extracting web data, the authors propose a novel methodological framework focused on enhancing its validity. In particular, the framework highlights how addressing validity concerns requires the joint consideration of idiosyncratic technical and legal/ethical questions along the three stages of collecting web data: selecting data sources, designing the data collection, and extracting the data. The authors further review more than 300 articles using web data published in the top five marketing journals and offer a typology of how web data have advanced marketing thought. The article concludes with directions for future research to identify promising web data sources and embrace novel approaches for using web data to capture and describe evolving marketplace realities.Keywords: web scraping; application programming interface; API; crawling; validity; user-generated content; social media; big dataThe accelerating digitization of social and commercial life has created an unprecedented number of digital traces of consumer and firm behavior. Every minute, users worldwide conduct 5.7 million searches on Google, make 6 million commercial transactions, and share 65,000 photos on Instagram ([76]). The resulting web data—enormous in size, diverse in form, and often publicly accessible on the internet—is a potential goldmine for marketing scholars who want to quantify consumption, gain insights on firm behavior, and track social activities difficult or costly to observe otherwise. The importance of web data for marketing research is reflected in a growing number of impactful publications across all methodological traditions, including consumer culture theory, consumer psychology, empirical modeling, and marketing strategy.Researchers can use web scraping and application programming interfaces (APIs) to efficiently collect web data at scale. Web scraping is the process of developing software to automatically collect information displayed in a web browser. For example, researchers can scrape Amazon's website to construct data sets of online consumer reviews. Because many websites and web apps are publicly accessible, data sets can be generated without involving data providers. In contrast, some data providers also offer APIs for programmatic access to their internal databases. For example, scholars can apply for academic research access to retrieve data from the Twitter API. Researchers can also access a wide range of algorithms via APIs. For instance, Google offers advanced image and video recognition through its Cloud Vision API (for additional examples and explanations, see Table W1 in Web Appendix A).Data extracted from the internet, at first sight, might resemble other organically generated data sets that address related research questions (e.g., a firm's clickstream data). Yet, collecting web data for academic use in a highly automated manner may prompt a set of novel and sometimes insidious validity challenges. Validity concerns may arise from, among others, ( 1) failing to capture contextual information in a rapidly changing environment (e.g., updates to the website's data-generating process), ( 2) not sufficiently aligning the psychological processes of interest with the frequency of data extraction on review platforms (e.g., the collected information does not capture the time when the behavior occurred), ( 3) overlooking the influence of algorithmic interference on e-commerce websites (e.g., the effect of personalization algorithms on information display), or ( 4) failing to retain raw website or API data necessary for construct validation, sampling, and analysis.Against this background, this article makes three interlinked contributions. First, we develop a methodological framework that highlights how addressing validity concerns arising from web scraping and APIs requires the joint consideration of idiosyncratic technical and legal/ethical concerns. Within marketing, guidance exists for collecting web data in the consumer culture theory research tradition, particularly using netnography (e.g., [46], [47]). A handful of articles address selected challenges that occur during the automatic extraction of web data (e.g., sampling; [39]). Outside of marketing, tutorials and books primarily focus on technical details for the automatic extraction of web data (see Table W2 in Web Appendix B). Yet, neither these resources nor methodological articles in other disciplines (e.g., [19]; [52]) address the broad spectrum of validity concerns arising from the automatic collection of web data for academic use. It is this void that our methodological framework fills. In discussing the methodological framework, we offer a stylized marketing example for illustration and provide recommendations for addressing challenges researchers encounter during the collection of web data via web scraping and APIs.Second, despite the use of web data in marketing for two decades, no systematic review reflects on how it has and could advance marketing thought. Importantly, understanding the richness and versatility of web data is invaluable for scholars curious about integrating it into their research programs. To offer these insights, we have systematically reviewed more than 300 articles in the top five marketing journals across two decades that have used web data. We leverage our coding to reveal which web sources have been considered and how data have been extracted. The resulting typology of web data may spark the imagination of researchers interested in generating new marketing insights from web data.Finally, we use our methodological framework and typology to unearth new and underexploited ""fields of gold"" associated with web data. We seek to demystify the use of web scraping and APIs and thereby facilitate broader adoption of web data across the marketing discipline. Our future research section highlights novel and creative avenues of using web data that include exploring underutilized sources, creating rich multisource data sets, and fully exploiting the potential of APIs beyond data extraction. We particularly highlight the value of web scraping and APIs for research streams that have not yet embraced them at scale.In what follows, we provide an overview of the use of web data in marketing and document four pathways through which web data have advanced marketing thought. We then introduce our methodological framework to help researchers make sensible design decisions when automatically extracting web data. We conclude with directions for future research. Using Web Data to Advance Marketing ThoughtAcross the top five marketing journals, marketing researchers increasingly use information available on the internet. For example, the share of web data–based publications has more than tripled in the last decade, from about 4% in 2010 to 15% in 2020 (see the thick line in Figure 1). The growing use of web data has been fueled by its increased accessibility and associated time and cost savings. Most of the 313 identified web data–based articles rely on web scraping (59%); APIs are used much more sparingly (12%), and some articles combine web scraping and APIs (9%). The remaining articles—especially netnographic work—use web data but tend to extract it manually (20%). The median annual citation count of articles using web data is 7.55, compared with 3.90 for publications not using web data.Graph: Figure 1. Increased use of web data in marketing (2001–2020).Some of the earliest uses of web data in marketing can be attributed to the development of netnography to study online communities (e.g., [46], [45]). Subsequently, the first quantitative marketing scholars extracted web data at scale (e.g., [27]). Today, all subfields—including marketing strategy and consumer behavior—have embraced web data.Online word of mouth and social media are the most prominent domains of inquiry using web scraping (see Table W3 in Web Appendix C). The most widely used data source in academic marketing research is Amazon (38 articles). Other prevalent sources are Twitter (30), IMDb (24), Facebook, and Google Trends (both 22; see Table W4 in Web Appendix C).Via a comprehensive literature review, we next identify the four central pathways through which web data facilitate the creation of new knowledge in marketing. Studying New PhenomenaWeb data can boost the field's relevance by enabling marketing scholars to study novel phenomena. For example, initial work using web data focused on novel online phenomena that emerged at the beginning of this century, such as online conversations ([27]) and the impact of consumer reviews on sales ([14]). Web data are well suited to provide fertile grounds for inductive research to develop novel theories about emerging marketing phenomena (e.g., brand public; [ 4]).Gathering data via web scraping or APIs often decreases the time between the occurrence of a marketplace phenomenon and the availability of data for academic research. This inherent timeliness of web data continues to be an essential lever for marketing scholars to advance our understanding of emerging substantive topics such as the sharing economy (e.g., Airbnb; [93]), access-based business models (e.g., Spotify; [15]), and fake online content (e.g., [ 3]). More generally, web data enable researchers to weigh in on contemporary issues before any ""conventional"" data sets become available, such as measuring the effect of pandemic lockdown policies on consumption ([74]). Boosting Ecological ValueWeb data can create knowledge by allowing researchers to move closer to marketing's ""natural habitat"" ([83]). Some of the most used web sources contain commercial outcome variables relevant to marketing stakeholders and are difficult or costly to collect otherwise. Examples are sales (e.g., The-Numbers.com), sales ranks (e.g., Amazon), online searches (e.g., Google Trends), and donations (e.g., contributions to a Kiva project).As web data can be collected unobtrusively, they can effectively complement more controlled data collection methods. Using web data, researchers can demonstrate that focal psychological processes occur outside the confines of a controlled laboratory environment and stylized experimental stimuli ([64]). Consider, for instance, the controversy around the decoy effect ([37])—one of the most prominent context effects in consumer behavior. Using experiments, [23] questioned the robustness and practical relevance of the decoy effect. In response, [90] built a panel data set from an online diamond market using web data. Their work not only shows that the decoy effect emerges in a high-stakes setting but also, more importantly, reaffirms its practical significance by quantifying its profit implications for the diamond retailer.Another benefit of using web data to boost ecological value is that they can often be collected without the data provider's direct involvement. Thereby, researchers can limit the interference of data suppliers or collaborating firms to ensure that the societal relevance of a particular research question is given precedence over business objectives (e.g., firms might be unwilling to share data about the tracking tools they use on websites; [81]). Further, using web data, researchers can ensure the publication of research findings, regardless of how palatable they are to the organizations that are being studied. Facilitating Methodological AdvancementAs much of the data produced by consumers and firms is inherently unstructured, extracting insights can be challenging ([88]). Thus, marketing researchers have leveraged web data for developing methods that deal with and extract insights from different types of unstructured data, such as textual, image, and video data. For instance, web data have fueled the rapid improvement of automated text analysis (see [ 6]) and the large-scale analysis of image and video content (e.g., [55]; [57]).The availability of network data on the internet (e.g., friend or product networks), along with outcome variables (e.g., posts, likes, sales ranks), has further enabled the use and advancement of methods for analyzing networks (e.g., [67]). Given their wealth and richness, web data have also stimulated the development of novel methods that can complement or replace traditional marketing research methods (e.g., using user-generated content to construct accurate multidimensional scaling maps of brands; [65]). Improving MeasurementWeb data can advance marketing knowledge by allowing researchers to measure constructs more precisely and obtain more valid inferences. For example, the collection of adequate control variables is often difficult. To capture seasonality in purchase patterns across a wide range of geographical markets and calendar years, researchers have used APIs to construct continuous (vs. dichotomous) variables that accurately reflect national holidays (e.g., HolidayAPI; [16]). Web data also allow researchers to efficiently operationalize new measures at scale, such as weather conditions based on the location of users' IP addresses (e.g., Weather Underground; [53]).Relative to non-web data sources, researchers can collect data on the behavior of many consumers and firms at higher frequencies ([ 1]). Such data enhance statistical power, enable identification of causal effects, and facilitate the examination of theoretically relevant variation within individuals over time (e.g., how various psychological distances shape review content for the same consumer; [35]) or how effects unfold over time (e.g., the impact of video elements on virality over time; [77]). SummaryTable 1 presents a typology of the four central pathways through which web data have advanced marketing thought. The typology highlights web data-based studies that investigate key marketing constructs across different entities, from consumers to organizations and other marketing stakeholders. For example, [80] explored a new phenomenon (tweeting), focusing on consumers (i.e., their motivation to tweet). These pathways for knowledge creation from web data are not mutually exclusive. Combining different pathways might be particularly promising for making breakthrough contributions.GraphTable 1. How to Create Knowledge Using Web Data: A Typology. Effect on ...Primary Pathways of Knowledge Creation Using Web DataPathway 1:Studying New PhenomenaPathway 2:BoostingEcological ValuePathway 3:Facilitating Methodological AdvancementPathway 4: Improving MeasurementConsumers(e.g., social media use, consumer learning)Toubia and Stephen (2013) test the motivations of users to contribute content to social media.Sridhar and Srinivasan (2012) explore peer effects in evaluating online product reviews.Huang (2019) studies how picture quality improves due to consumer learning.Huang et al. (2016) exploit within-user variation to measure how psychological distances interact.Organizations(e.g., sales and profits of firms, donations to nonprofits)Chevalier and Mayzlin (2006) demonstrate the impact of online reviews on book sales.Wu and Cosguner (2020) probe the prevalence and profit implications of decoy effects.Netzer et al. (2012) mine user-generated content to identify market structures.Datta et al. (2022) gather national holidays across 14 countries and 11 years to capture seasonality.Other marketing stakeholders(e.g., market reaction of investors, public health outcomes)Hermosilla, Gutiérrez-Navratil, and Prieto-Rodríguez (2018) examine how consumers' aesthetic preferences create biases in firms' hiring decisions.Blaseg, Schulze, and Skiera (2020) examine whether consumers are protected against false price advertising claims on Kickstarter.Tirunillai and Tellis (2012) develop novel online metrics based on user-generated content to predict stock returns.Kim and KC (2020) explore the effect of ads for erectile dysfunction drugs on birth rates. 1 Notes: The table highlights illustrative and diverse examples of web data–based studies and corresponding outcome variables, cross-tabulated by four pathways through which web data have advanced marketing thought (the columns) and three of the most studied actors in marketing research (the rows).Next, we introduce our methodological framework, which outlines an approach for making design decisions that enhance the validity of web data collected via web scraping and APIs. Researchers interested in learning more about the technical details of automatically extracting web data can consult our curation of technical tutorials in Web Appendix B or the digital companion to this article (available at https://web-scraping.org), which features a searchable database of all marketing articles in the top five marketing journals using web data. Methodological Framework for Collecting Web DataIn automatically collecting web data using web scraping and APIs, researchers make seemingly innocuous design decisions. However, as we will show, these decisions often involve trade-offs about research validity, technical feasibility, and legal/ethical risks[ 5] that are not always apparent. How researchers resolve these trade-offs shapes the credibility of research findings by enhancing or undermining statistical conclusion validity, internal validity, construct validity, and external validity ([73]).We develop a methodological framework to provide guidance for the automatic collection of web data using web scraping and APIs. Figure 2 offers a stylized view of this process involving three key stages—source selection, collection design, and data extraction. Researchers typically start with a broad set of potential data sources and eliminate some of them as a function of three key considerations—validity, technical feasibility, and legal/ethical risks. These three considerations appear in the corners of an inverted pyramid, with validity at the bottom to underscore its importance. Given the difficulty in projecting the exact characteristics of the final data set before it is collected, researchers often revisit these considerations as they design, prototype, and refine their data collection. Failure to resolve technical or legal/ethical issues might mean that web data cannot inform the research question meaningfully.Graph: Figure 2. Methodological framework for collecting web data.Our framework deliberately focuses on collecting web data rather than its subsequent analysis. Analyzing web data involves many familiar methodological challenges encountered with organically generated data (e.g., cleaning to remove erroneous data or create measures, selecting observations, addressing endogeneity). However, approaches for the valid collection of web data are not yet documented nor commonplace in marketing research.The methodological framework—designed to guide the automatic extraction of web data at scale—is agnostic to research paradigms. It is applicable to both deductive (i.e., identifying compelling web data to test hypotheses) and inductive (i.e., observing interesting irregularities in web data to identify novel marketing concepts and/or novel relationships between constructs) approaches to theory building. We next highlight the idiosyncratic challenges encountered when collecting web data and summarize solutions to these challenges in Tables 2–4. For expositional clarity, we focus on web scraping in our text.[ 6] To illustrate the key challenges encountered in designing the data collection, we gradually introduce a stylized marketing example involving the collection of book reviews from Amazon.GraphTable 2. Challenges and Solutions in Selecting Web Data Sources. Challenge #1.1: Exploring the Universe of Potential Web SourcesReason for importanceAs web sources vastly differ in quality, stability, and retrievability, researchers might be tempted to consider dominant or familiar platforms only. A thorough exploration of the data universe permits compelling theory testing and identifying novel, emerging marketing phenomena that may be difficult to notice otherwise.Solutions and best practicesAssume the perspective of different stakeholders (e.g., consumers, analysts, managers) during the search processBrowse through public API directories (e.g., ProgrammableWeb, GitHub)Broaden geographic search criteria (e.g., non-Western)Identify adjacent data sources (e.g., using Google Trend's ""related search queries"")Expand search to nonprimary data providers (i.e., aggregators, databases)Carefully vet the provider's description of relevant metricsDetermine the conditions necessary to access data (e.g., requirement to log in on a website, creating an API key, subscribing to an API, possibility to signal academic status/scientific use)Verify whether it is possible to opt out of firm-administered experiments or whether the site is accessible without cookiesUse the website or make some initial API requests to assess information availability (in the case of APIs, assess which authentication procedure is necessary to obtain data)Challenge #1.2: Considering Alternatives to Web ScrapingReason for importanceBecause web scraping is the most popular extraction method for web data, researchers may overlook alternative ways to extract data. APIs provide a documented and authorized way to obtain web data for many sources. Some sources also provide readily available data sets. Using such alternatives leads to time savings and minimizes exposure to legal risk.Solutions and best practicesExpand search by explicitly including terms such as ""API"" or ""data set download""Explore whether the source or third parties (e.g., public data platforms, researchers) offer data sets for download and assess their terms of useIf a data source provides an API and a website, understand the differences in what data could be retrieved from them (e.g., by screening the API documentation) and how well the API can be accessed (e.g., using packages)Use stable versions of an API, and subscribe to a source's API support updatesChallenge #1.3: Mapping the Data ContextReason for importanceWeb data are usually not accompanied by extensive documentation. Identifying potentially relevant contextual information early on is essential for the relevance and validity of the research.Solutions and best practicesScreen blogs, press releases, a source's software ""changelogs,"" or use Google's reverse search to identify important (technical) developmentsBuild an initial understanding of the presence of algorithms by visiting the source with different devices at different times or by inspecting the site's source codeUnderstand changes to the data-generating process (e.g., by studying changes over time using archive.org)Inspect the robots.txt file and assess how the source requires users to agree to their terms of service (e.g., preferrable ""browsewrap"" vs. less preferable ""clickwrap"" agreements)Scrutinize popularity, legitimacy, and business model of data sources (e.g., by using firm reports, stock filings, news, and social media, other data providers like Statista)Explore forums where users of the source talk about the source (e.g., Reddit)Assess whether the data links to other data sets (e.g., by spotting common IDs)Map out ""worst-case"" scenarios for research objectives in the case that the data source changes (e.g., discontinuation of an API, removal of a website) GraphTable 3. Challenges and Solutions in Designing Web Data Collections. Challenge #2.1: Which Information to Extract from Which Pages?Validity Challenges [V]Which information is necessary to justify construct operationalization and allow analysis?Which metadata might enhance internal and external validity?Is information subject to algorithmic biases or missing data?Are there significant changes to the data-generating process?Legal/Ethical Challenges [L]Is all of the required information publicly accessible, or is a login required?Does the data contain personal or sensitive information, and can subjects be identified?Is there a sufficient scientific justification for using the data?How large is the overlap between the research objective and the original intent of subjects disclosing the data?Technical Challenges [T]Is all information extractable?Are there any limits to iterating through pages or endpoints?Does the extraction software obtain information reliably?Solutions and Best PracticesExplore different types of pages to detect unique vs. identical information [V]Explore whether alternative ways to browse/navigate the site (e.g., URLs, clicking, scrolling, logging into the site) provides different or reveals new information [T]Explore how extraction methods (e.g., ""headless"" HTTP requests vs. simulated browsing, different user agents, screen width, login status, use of different packages) affect information display [V, T]Assess the accuracy of timestamps (e.g., time zones) [V]Save screenshots of pages that describe the calculation of metrics [V]Explore (temporarily available) information in the source code of a website using the browser's ""inspect"" tools [V]Assess the presence of technical roadblocks (e.g., captchas) [T]Assess how data was generated historically at the source (e.g., via archive.org) [V]Explore limits to iterating through pages [T]Obtain information from various sources to reduce dependency on data provider [L]If possible, opt out of firm-administered experiments or block cookies; alternatively, identify relevant metadata that can be used to control for the presence of algorithms [V]Challenge #2.2: How to Sample?Validity Challenges [V]Is the sample size sufficient to effectively inform the research question?To which population does the sample generalize?Is the sampling frame corresponding to the research objective (e.g., randomness)?How prevalent is panel attrition?Legal/Ethical Challenges [L]Does the data represent an excessive portion relative to all data available?Can the data be obtained in similar forms elsewhere, or is the research question only answerable with the targeted data?Are some of the sampled units (potentially) vulnerable?Technical Challenges [T]Is the required sample size technically feasible?Can external information (e.g., IDs) be consistently matched to the data?Solutions and Best PracticesAssess characteristics of the population (e.g., using secondary sources) [V]Explore options to sample directly from the source (e.g., from different pages, randomization through filtering/searching, obtaining usernames from forums, see also Neuendorf 2017 and Humphreys and Wang 2018) [V]Choose lists or pages that are not affected by algorithmic influence [V]Refresh sample (or use multiple types of sampled units) to assess the stability of sample and counterbalance panel attrition [V]Discard units from the sample to prevent data collection from subjects falling under prohibitive national and supranational legislation (e.g., GDPR) [L]Explore external sources to inform the sampling frame [V], or facilitate linkage [T]Assess the efficiency of different navigation paths and their impact on sample size [T]Pseudo-anonymize or discard sensitive or personal information [L]Ensure that no excessive amount of data (e.g., data on all users) is collected (absolute volume, relative volume) [L]Reexamine alternative sources to improve justification of data extraction [L]Challenge #2.3: At Which Frequency to Extract the Data?Validity Challenges [V]Is the extraction frequency in sync with the studied phenomena?Is the refresh rate of the source sufficient?Is the data (thought to be archival) really archival?Is the information consistently available across all periods of interest?Does the order and frequency in which information is retrieved induce bias?Legal/Ethical Challenges [L]Does the extraction frequency pose an excessive load on the source?Does collecting more data at higher frequencies make the data more sensitive?Technical Challenges [T]Does the desired extraction frequency pose new technical hurdles?How can the stability of data collection be guaranteed, and different collection batches be distinguished?Solutions and Best PracticesExplore the gains in collecting data multiple times rather than once (e.g., in a ""live"" data collection) [V]Adhere to best practices in setting the extracting frequency (e.g., five requests per second for APIs, one request per two seconds for web scraping) [L, T]Experiment with technical parameters (e.g., number of computers) to balance technically feasible sample size and desired frequency of data extraction [T]Formulate, test, and refine data source theory (Landers et al. 2016) [V]Reinspect the robots.txt file to avoid exceeding retrieval limits for selected pages [T]Consider randomizing extraction order for sampled units over time [V]Consider (cost) implications for storage and computation time [T]Consider getting in touch with the data provider if the targeted data set is infeasible to extract via web scraping or APIs [T, L]Devise a schedule for the automatic extraction of the data (e.g., using Windows Task Manager or Cron) [T, V]Challenge #2.4: How to Process the Data During the Collection?Validity Challenges [V]Could erroneous processing lead to unexpected data loss?Could there be any significant scientific value in retaining the raw data?Legal/Ethical Challenges [L]Is the collected data in conflict with prohibitive laws (e.g., GDPR)?Is the collected data sufficiently secured from unauthorized access?Is anonymization or pseudonymization required?Technical Challenges [T]Which storage facilities to use to accommodate the expected data (size, location, format, encoding)Is normalization necessary?Solutions and Best PracticesRetain raw data (e.g., HTML pages, JSON responses) whenever possible [V, T]Always parse some minimal amount of data (e.g., timestamps) to facilitate monitoring checks in real-time [V, T]Remove sensitive and personal information on the fly; if personal or sensitive information is strictly required to meet the research objective, consider pseudo-anonymizing (potentially via third parties) [L]Verify data storage during collection meets legal requirements for potentially sensitive or personal data [L]Ensure proper encoding of (non-English) characters, retain correct digit separators and correct data format GraphTable 4. Challenges and Solutions in Extracting Web Data. Challenge #3.1: Improving PerformanceReason for importanceIn scaling up the data collection, researchers might encounter new technical issues. For example, the data collection could stop unexpectedly or proceed much slower than anticipated.Solutions and best practicesWhen scraping, use stable selectors (e.g., tags, classes, attributes, styles associated with particular information) and make only selective use of error handlingWhen using APIs, choose a stable and supported versionAttempt to reparse data from stored raw data if the extraction failedCheck for traces of being banned/blocked/slowed down by the website (e.g., by scanning the content that was retrieved)Notify data sources about potential bandwidth issues with APIsUpdate the technically feasible retrieval limit, and recalculate desired sample size, extraction frequency, etc.Verify that computing resources are appropriate and reliable (e.g., scale up or down servers, verify that database runs optimally)Move data to a remote (and more scalable) file storage or databaseConsider potential benefits from using cloud computing (e.g., for extended, uninterrupted data collection) vs. benefits from local setups (e.g., due to security or privacy concerns)Budget the expected costs of API subscriptions, cloud computing and data storage and transferChallenge #3.2: Monitoring Data QualityReason for importanceMonitoring is critical to be timely alerted to data quality issues. Setting up a monitoring system allows researchers to intervene before discarding data altogether.Solutions and best practicesLog each web request (i.e., URL call), along with response status codes, timestamps of when the collection was started, and when the request was madeSave raw data (i.e., source code of HTML websites), along with the parsed data for triangulationVerify whether the raw data was correctly parsed (e.g., for a sample of information, compare raw data and parsed data)Check file sizes or the number of observations at regular intervalsSet up a monitoring tool to timely alert you to any future issues (e.g., based on the number of files retrieved or requests made, file sizes retrieved, time the collection last ran, budget spent)Automatically generate reports on data quality (e.g., using RMarkdown)Record issue(s) in a logbook (e.g., in the documentation); especially if considered critical for data qualityChallenge #3.3: Documenting DataReason for importanceResearchers are responsible for documenting the data set they produce from web data. Building documentation during the collection is important to guarantee accuracy and completeness, which facilitates use, reuse, and replicability.Solutions and best practicesMaintain a logbook in which to note important events (e.g., when the collection broke down and why)Start writing the documentation in the early stages of collecting the data, and make use of templates (e.g., Datasheets for Datasets; Gebru et al. 2020)Keep and organize copies of relevant files (e.g., screenshots of the website at the time of data extraction, the API documentation, details on variable operationalization with summary statistics, information about the context)Have a plan for long-term archival storage (e.g., re3data.org, The Dataverse Project, Zenodo), anonymization (e.g., generating synthetic versions of sensitive data), and consider which license to use for the data (e.g., Creative Commons)  Data Source SelectionA critical first step in the use of web data is selecting the data source(s). We examine three challenges faced by researchers in this selection process. First, it is essential that researchers explore the universe of potential sources (challenge #1.1). Second, researchers need to consider the range of possible extraction methods (challenge #1.2). Third, it is crucial to map the context in which the data are generated (challenge #1.3). Table 2 summarizes our recommendations for tackling these challenges. Exploring the Universe of Potential Sources (Challenge #1.1)In the absence of conventional gatekeepers (e.g., data providers), researchers can select from countless web data sources. For example, there are 2.1 million online retailers in the United States alone ([20]). Further, websites and APIs differ greatly in scope (e.g., number of users), data quality (e.g., consistency), and retrievability (e.g., extraction limits). Even within the same product category, data sources differ vastly. For example, Amazon reports a book's sales rank (an aggregate outcome metric for product sales), whereas Goodreads reports users' reading behavior (an individual outcome metric for consumers' usage intensity).Faced with a vast universe of potential sources, researchers may be tempted to focus on familiar platforms only ([89]). For instance, Amazon is the most used web data source in marketing (see Table W4 in Web Appendix C). Amazon might be a relevant source to extract book reviews, given its broad assortment and user base. Yet, in other cases, researchers might miss opportunities for identifying novel, emerging marketing phenomena or conduct more compelling theory testing without a thorough exploration of potential sources. Researchers can avoid the pitfalls of defaulting to dominant sources by actively considering a broad spectrum of websites and APIs, ranging from highly popular (e.g., Amazon) to less popular sources (e.g., Goodreads), from primary data providers (e.g., YouTube) to data aggregators (e.g., Social Blade), and from platforms with global reach (e.g., Twitter) to more regional ones (e.g., Taringa!). Another strategy to move beyond familiar sources is to adopt alternative perspectives. For instance, researchers can consider websites or APIs that are used by consumers, analysts, or managers. API directories at GitHub or programmableweb.com can facilitate identifying potentially relevant APIs.A broad exploration of web data sources may lead researchers to discover sources that may be more permissive for (academic) data extraction or less likely to trigger ethical concerns. For example, websites that do not require logging into the site to reveal information are typically more scraping-friendly than sites that first require registering a user account. In the case of Amazon, researchers can obtain most information without logging in and do not have to explicitly provide their agreement to the website's Terms of Service. To reduce legal (e.g., breaches of contract, as researchers have provided explicit agreements to the terms of service) and ethical (e.g., website users may consider their data private) risks, researchers should refrain from creating fake accounts to access information requiring a login. By explicitly declaring their academic status (e.g., when registering at the site using the institutional email address), researchers might be able to diminish their exposure to legal risk.When exploring web sources, researchers need to examine whether theoretical constructs can be operationalized in a valid manner ([91]). A healthy level of skepticism is warranted when using idiosyncratic metrics from APIs or websites. For example, researchers might be interested in scraping the price tier of restaurants from Yelp. Yet, it is not entirely clear how Yelp computes this metric from individual consumer ratings.To determine when to stop exploring sources, researchers need to assess to what extent the selected source(s) improve(s) on alternatives. One way to justify selecting a single web source is the presence of unique features. For example, a researcher studying how observers react to humor in reviews might prefer Yelp to alternative platforms as it is the only source featuring ""funny"" votes (e.g., [60]). At other times, researchers may be indifferent between potential sources and can draw from multiple sources to boost the generalizability of their findings (e.g., tweets and restaurant reviews; [61]). Collecting data from multiple sources is often useful because even similar types of information (e.g., consumer comments) may affect marketing outcomes differently, depending on source characteristics (e.g., forums vs. microblogs; [72]). Data aggregators—some of which offer authorized data access via APIs—facilitate the collection of such multisource data. Considering Alternatives to Web Scraping (Challenge #1.2)The popularity of web scraping may lead to the conclusion that it should be preferred over other methods. However, some web sources offer access to data via APIs ([12]). In general, extracting data via APIs is more scalable and less likely to invoke the same level of legal risks compared with web scraping. Although some sources offer unconstrained APIs that do not require authentication, others require (paid) subscriptions and authentication procedures. Some sources, such as Twitter, have recently started offering APIs for academic research. In the case of Amazon, an API offering access to consumer reviews is currently not available.In addition to APIs, numerous other options exist for researchers to obtain web data. For example, some data providers (e.g., Yelp, IMDb), public data platforms (e.g., Kaggle, The Dataverse Project), and researchers (e.g., [59]) provide documented web data sets that can readily be used for academic research. There are many potential use cases of such data sets, but less than 5% of all web data–based articles in marketing used such data sets. To avoid the pitfall of defaulting to web scraping for data extraction, researchers can expand their search by explicitly including terms such as ""API"" or ""data set download"" in their search queries. Mapping the Data Context (Challenge #1.3)Relative to other frequently used archival sources in marketing, web data entail large and often undocumented complexities. Thus, it is critical that researchers map the data context, which involves identifying relevant contextual developments that may undermine the validity of the research if gone unnoticed.First, mapping the data context may reveal changes in the underlying data structure. For example, a major change in a platform's user interface may affect subsequent consumer behavior. Second, mapping the data context enables researchers to identify relevant pieces of information for collection together with the focal web data. For example, researchers may discover an external website (e.g., Statista) that offers information about the composition of a focal data source's user base. If stored, such data could eventually be used to detect changes in the composition of the user base or verify the representativeness of the extracted data. Third, mapping the data context may reveal unknown information, potentially allowing researchers to discover novel research opportunities. For example, researchers may use the (unexpected) launch of a new recommendation system at a music streaming service as a natural experiment to investigate the impact of recommendations on music consumption.To understand and map the contextual complexity of web data, researchers can immerse themselves in the ecosystem surrounding the focal source by signing up and using the source, tracking press releases, social media, and scanning the competitive environment. Helpful tools include a search engine's advanced search features, newsletters, and alerts on leading business and technology magazines (e.g., TechCrunch.com, WSJ.com, FT.com). The website's source code may also hold valuable information about potentially relevant environmental changes. Sometimes, researchers may also detect the presence of algorithms on the site that may threaten the validity of the collected data. For example, Amazon's product pages personalize information based on which preceding products were viewed—even without users logging into the site. Designing the Data CollectionAfter narrowing down potential sources, researchers decide which information to extract from them (challenge #2.1), how to sample (challenge #2.2), at which frequency to extract the information (challenge #2.3), and how to process the information during the collection (challenge #2.4). Table 3 summarizes these challenges and corresponding solutions. Which Information to Extract? (Challenge #2.1)In the absence of any ""downloadable"" data set, the first challenge lies in deciding which information to extract from a source. Researchers begin by browsing the web page to identify from which pages to extract which information. In our Amazon example, some of the most commonly used pages are product pages (e.g., [14]) and review pages (e.g., [84]). Generally, pages such as those from e-commerce platforms contain information from the company's database, offering researchers the opportunity to capture some of the information available at a company. Collecting such data involves iterating through a set of related pages (i.e., browsing through many product pages and corresponding review pages in our Amazon example) and saving the data as it becomes visible.As the goal of websites like Amazon is rarely the provision of data sets for academic research, it is often necessary to combine information from different pages (e.g., book descriptions from the product page and ratings from the review page). It is particularly difficult to recognize the subtleties of available information, which makes the decision from which pages to extract information challenging. For example, researchers interested in building a data set of book reviews would find total ratings both on the product and review page, but only the review page reveals all product reviews.[ 7] Yet, neither the product nor the review pages contain all the biographical information available on a reviewer's profile page. Widely exploring a website or API is necessary for identifying information relevant for subsequent analysis (e.g., construct operationalization). The amount and type of information also often vary (e.g., depending on screen width or whether the user is logged in). In this phase, researchers should assess the degree to which the information could be considered personal or sensitive under different regulatory regimes (e.g., the European Union's General Data Protection Regulation [GDPR]), which may require planning measures such as pseudo-anonymization of reviewer names. Researchers may also reassess whether all information needs to be captured to meet the research objective. Suppose reviewer names are strictly necessary (e.g., because they allow for matching different sources). In that case, researchers can explore whether the targeted web data source offers ways to exclude subjects governed by prohibitive privacy regulations (e.g., by using filters).An important threat to internal validity in any study involving web data is algorithmic interference (e.g., [91]). The (visual) design of websites that facilitates usability can undermine the validity of the collected data if gone unnoticed and unaddressed. Especially when deciding which information to extract, it is important to reexamine the website or API for the presence of algorithms. For example, the order in which the researchers in our example visited the website while designing their data extraction could affect which related books are displayed on the product pages on Amazon. Other algorithms that often affect the display of data on websites are sorting algorithms (e.g., by popularity or mixed with sponsored search results) and filtering algorithms (e.g., showing subsets of the data). Algorithmic interference is often hard to detect without being sensitive to it. To account for potential algorithmic interference, the researcher might extract variables as part of an algorithm's more extensive set of input variables, which offers opportunities to control for them in the empirical analysis (e.g., the order in which books were extracted in our Amazon example).Researchers also need to establish the intertemporal stability of available information. Because the web is constantly evolving, the information on a page might not have been generated via the same process over time, undermining the internal validity of the data. Some changes to sources are drastic enough to alter how the data were created in the first place, introducing measurement error ([87]). For example, Amazon shifted to a positive-only evaluation of reviews by removing the ""not helpful"" vote button in 2018, and it no longer displays ""not helpful"" counts next to reviews ([28]). This change might have impacted review content (e.g., users writing shorter review texts). Yet, researchers collecting data today cannot find any traces of these ""not helpful"" votes. A tool for examining changes to relevant information on a website is the Wayback Machine (archive.org). Researchers can use this tool to retroactively inspect websites over time (e.g., [58]) or submit their own website links for archiving.Finally, collecting metadata that ""annotates"" the data collection enhances internal and external validity (e.g., storing the timestamp of data extraction, whether an API request was completed successfully, or the IP address from which the data request was made). Such metadata can be used not only for diagnostic purposes but also to link the extracted web data to other data sets. For example, in our Amazon example, the collected data could be linked to other data using IP-based geolocations (e.g., linking geolocation and web search data; [86]) or timestamps (e.g., linking reviews to stock prices; [78]). How to Sample? (Challenge #2.2)A second challenge in designing the data extraction lies in deciding how to sample from the data source. In particular, in the absence of access to the data source's entire database, it is difficult or impossible to draw a random sample from the population (e.g., all products) available at the data source. Instead, researchers need to devise their own sampling frame to reveal the units they want to sample from the website ([66]). For example, researchers could scan the site for an index of all products that could inform their sampling. In our example studying reviews at Amazon, multiple such indexes may be available. Should products be sampled from the bestseller page for books (so-called exposure-based populations; [66]) or instead from the category page for books (i.e., availability-based populations)? Choices like this result in different data and may even invalidate inferences, as sampling frames might inadvertently induce systematic bias ([39]).One common validity challenge in choosing how to sample is determining how many units (e.g., books) are sufficient to inform the research question. From a validity standpoint, it would be ideal to collect information on the entire population (e.g., all books available at Amazon). However, Amazon does not have an obvious page to extract all books. Imagine that a research team wanted to collect information about all marketing books sold on Amazon. The bestseller page, for example, lists only the top 100 bestsellers. By manually changing pagination parameters in the URL, the top 400 bestsellers can be revealed. Yet, this list of 400 books neither constitutes the entire population nor represents a random sample of marketing books sold at Amazon. Alternatively, when starting from the product overview pages, these pages list an imprecise number of books (e.g., ""over 60,000""), which can only be viewed up to page 50. With each result page featuring 24 organic search results, this approach would produce 1,600 books per category at best. Thus, researchers need to consider other ways to identify more books on Amazon, such as searching for books using various keywords. To expand the number of sampled units, researchers could collect data multiple times, use other keywords, or tweak search parameters to reveal more data by requesting narrower subsets from the database (e.g., only books published during a specific month).Even if a list of the population (e.g., all books) could be retrieved, it may be infeasible to extract data within a reasonable time frame. While sample size requirements are mostly concerned with a researcher's inferential goals (e.g., [50]), few articles make the resource constraints that affect collecting web data explicit (e.g., [68]). For example, with web data, a study's sample size critically depends on technical parameters such as the number of computers used for data extraction or the number of pages that need to be visited. We illustrate how to calculate the technically feasible sample size in Web Appendix F, which may effectively complement traditional sample size calculations commonplace in marketing.As a result of these complications, researchers often restrict their sample size. One way to motivate a compelling sampling frame is to use external sources that can be linked to the web data. For instance, the New York Times or Publishers Weekly bestseller lists might be a starting point for sampling books ([14]). An alternative approach focuses on internal data available at the source itself. Researchers may have to allocate substantial time to identify ways to sample from the focal source. Sometimes, starting the data collection from a page unrelated to the focal pages of interest might facilitate collecting a more representative sample (e.g., by reducing geographical biases; [86]). For example, on Amazon, researchers could first sample reviewers and associated demographic information (available at the user profile of reviewers) and subsequently retrieve data on all reviewed products. Similar to how researchers build network data from an initial set of products or users, the sampling units retrieved from an initial set of pages can be considered seeds. In choosing seeds, researchers should be cautious about drawing from vulnerable populations (e.g., minors) or infringing on prohibitive privacy regulations. At Which Frequency to Extract Information? (Challenge #2.3)Web data are nonstatic, as they change often or might disappear altogether. Therefore, researchers need to consider at which frequency to extract information. This decision encompasses whether to collect data once or multiple times and when to run (and potentially schedule) the data extraction. Consideration of the frequency and schedule is challenging but required to ensure the intertemporal stability of measurement, which is critical for internal and construct validity.From a technical and legal perspective, it is most desirable to extract data only once. Single extractions are less likely to represent a burden on the firm's servers, and the extracted data often only represent a limited snapshot of the entire database, reducing the risks of copyright infringement. Further, such data may be more likely to respect users' ""right to be forgotten,"" which is part of the privacy laws in some jurisdictions. Yet, single data extraction might raise several validity issues that can easily go unnoticed. For instance, in our example, researchers extracting book reviews once from Amazon will not be able to identify whether any of the archival information has changed. Only when extracting data multiple times can researchers systematically notice changes on the site, which may lead to the identification of ""fake"" reviews that have been removed by the platform (e.g., [29]). More generally, researchers can compare information over time to detect whether data that initially appeared to be archival is truly archival (i.e., does not change over time).Another concern is that a single extraction may not produce a data set that adequately maps onto the focal processes of interest. For example, suppose researchers in our example want to examine whether a review by a so-called ""Top 1000 Reviewer"" leads to more subsequent reviews from other users. However, the researcher merely observes that the reviewer is a top reviewer at the time of data extraction. This does not necessarily imply that this user had the same status when the review was first posted and thus was most likely to affect subsequent reviewing behavior of other users. Formulating and testing the essential assumptions about the data, including the relation between the time of data extraction and the focal (psychological) processes, is thus critical. The formulation of such assumptions is called a ""data source theory"" ([52]). Testing and refining the data source theory helps take proactive steps to enhance internal and construct validity. In the preceding example, it would thus be necessary to collect data from these review pages closer to the original posting date, ensuring that reviewers classified as ""Top Reviewers"" had that status when their reviews became visible.When extracting data more than once, automatic scheduling can help ensure consistency and contribute to validity. Scheduling is beneficial if the required information is only available in real-time. For example, sales ranks at Amazon are updated hourly for popular products, and historical sales ranks cannot be retrieved. Suppose researchers in our example were interested in studying the sales performance of books over time. In that case, they could repeatedly extract the books' sales ranks from the product pages at Amazon. Sometimes fixed intervals enhance validity (e.g., every Monday, 8 a.m.). In other circumstances (e.g., when collecting data from many pages), it may be better to vary the starting time or weekday of the data extraction.Another decision is whether to set an end date for the data extraction. Collecting data over extended periods offers the potential for researchers to build a programmatic stream of research and stumble into unexpected natural experiments (e.g., [13]). Especially for longitudinal data collections, continuing the data collection while the project is in the review process brings numerous benefits, such as the ability to update the data (e.g., a longer time frame, new measures). Yet, concerns about technical feasibility (e.g., storage requirements, continued availability of data source) grow as the data extraction horizon extends. Similarly, from an ethical perspective, the longer the data extraction, the greater the likelihood of potentially identifying individuals via triangulation. Next to ethics, long-term data collection also places a heavier load on servers, potentially increasing exposure to legal risks. How to Process the Information During the Extraction? (Challenge #2.4)As a final step in designing the data extraction, researchers decide how to process the information while it is collected. Any kind of web data collection requires a minimal degree of processing, given that the information is available in a computer's memory (e.g., in the browser or the software processing the API output) and still needs to be stored in files or databases. Thus, this processing step occurs before data sets are cleaned or analyzed.When deciding on how to process information during the extraction, researchers must balance potential efficiency gains from molding raw web data into readily usable data sets with the potential threats to validity due to ""on-the-fly"" processing. For example, in our Amazon example, researchers may be tempted to remove seemingly unnecessary information (e.g., image links in reviews), apply text processing (e.g., removing characters used as separators), or force specific information (e.g., prices) to be stored in a strictly numeric format. Such on-the-fly processing promises to produce essential efficiency gains, as the data set resulting from the extraction could directly be analyzed. However, because on-the-fly processing decisions are usually made after the inspection of only a limited number of pages in early prototypes of the data collection, it is difficult to guarantee their correctness. For example, using our example, what if the initial screening revealed only pictures posted in a review, while the extensive data collection revealed the need to capture video files? Given this and related challenges, keeping the raw data (such as the source code of websites, API output, or any media files loaded at the time of data extraction) is ideal from a validity perspective. For example, even if the data collection breaks, researchers could still process and use the information after debugging their extraction code. Retaining the raw data can also help reduce Type 1 errors by increasing transparency about researchers' degree of freedom in collecting and processing the web data. Yet, retaining the raw data prompts significant concerns about the technical feasibility and ethical risks. From a technical standpoint, storing the raw data might require databases to retain their original structure and facilitate processing, especially for projects involving many raw data files collected over extended periods. Keeping all raw data might raise questions regarding the right to store the raw data—especially if it is not (pseudo-) anonymized before storage.Finally, retaining the raw data allows researchers to refine their extraction design at later project stages. For example, a researcher might have collected Amazon reviews in 2018—around the time of the removal of the ""not helpful"" voting feature. Although extracting ""not helpful"" votes was not part of the original extraction design, researchers would be able to use the raw web data to examine the effect of the removal of these ""not helpful"" votes. Collecting the DataAfter source selection and designing the data collection, researchers gradually transition to turning their small-scale prototype into stable extraction software. In so doing, researchers face three challenges. First, researchers may need to improve the performance of their extraction software when operating it automatically at scale (challenge #3.1). Second, they may need to implement monitoring checks to be alerted to any issues arising during extended data collections (challenge #3.2). Third, researchers should compile information important for documenting the final data set (challenge #3.3). Table 4 contains a summary of solutions and best practices to these challenges. How to Improve the Performance of the Data Extraction? (Challenge #3.1)In scaling up their data extraction, researchers may notice that the extraction software frequently breaks across a larger number of pages or runs significantly slower than expected. Such technical challenges, if unaddressed, have the potential to undermine research validity (e.g., missing data, not meeting sample size requirements). A practical solution to preempt these and similar challenges involves capturing the focal information in different ways and storing raw data—especially in the early stages of data collection and for more ambitious, large-scale web data collection projects. To track whether the extraction targets are met, researchers can log the (timestamped) URLs of scraped pages and visualize the performance of the extraction software over an extended period. The resulting ""effective"" extraction frequency can then be used in recomputing the technically feasible sample size (see Web Appendix F). Novel web scraping services promise to handle technical difficulties efficiently (e.g., ScrapingBee, Zyte). How to Monitor Data Quality During the Extraction? (Challenge #3.2)As a next step, researchers consider which metadata can help them diagnose issues with the data collection in real-time. Especially when websites constantly change, monitoring the health of web scrapers can be a tedious task. Researchers should consider performance at a higher level (e.g., the file sizes of extracted raw data) and lower level (i.e., the accuracy of the information in resulting data files) to assess whether the collection is proceeding as expected. When collecting over long periods, automatic reporting can greatly facilitate monitoring. Finally, alerts (e.g., via email or mobile) can help researchers detect predefined data issues quickly. How to Document the Data During and After the Extraction? (Challenge #3.3)During the data extraction, researchers need to record relevant information about the data in real-time. This is an essential step in building documentation, enabling future data usage by the researcher(s) who collected the data and other scholars. Even after the data extraction has ended, researchers can continuously refine the documentation as they become familiar with the characteristics of the data (e.g., variables that were erroneously captured, missing values).Accurate and comprehensive documentation is particularly critical given that collecting web data tends to involve repeated iterations between discovery (and often troubleshooting) and confirmation (i.e., subsequent analyses that are outside the scope of our framework). Designing web data extractions requires a different mindset compared with experiments or archival research. Unlike running experiments, the extraction design for collecting web data may be in flux, even when the collection is already running. Relative to traditional archival research in which data sets are sufficiently annotated, researchers are in charge of accurately recalling details about the data collection. Such details encompass information about the data composition (e.g., sampled units), extraction process (e.g., annotated code, detected errors during the collection), and processing details (e.g., applied cleaning steps). The template of [24] provides a useful starting point for building the documentation for a data set collected via web scraping or APIs. Given that contextual changes are inevitable (see challenge #1.3), documenting the source's institutional background (e.g., screenshots, corporate blog posts, API documentation) is crucial. Future Research Opportunities with Web DataAn unprecedented gold rush of web data has enriched the marketing discipline for two decades—over 300 published articles provide countless examples of impactful marketing insights using web data. With the ever-increasing digitization of social and commercial life, it is hard to imagine that the heyday of this gold rush might subside any time soon. Yet, are marketing's currently productive mines the only or the most promising sources of marketing insights in the future? Which novel approaches and technologies are necessary to capture and describe evolving marketplace realities?To identify directions for future research, we have reviewed more than 300 articles to provide a snapshot of the current state of web data in marketing. We use these insights to inform the subsequent discussion, which we organize along the four pathways through which web data can advance marketing thought (as summarized in Table 1). We supplement our discussion with key elements from our methodological framework (see Figure 2) and inspiring use cases from other disciplines. Direction 1: Identify New Web Data SourcesNext, we discuss how researchers can use source selection to branch out to new or underutilized sources for studying emerging substantive topics. We also highlight how researchers can design more complex, longitudinal, and multisource web data sets to reveal otherwise invisible phenomena. Draw from underutilized sourcesOur review reveals that marketing research draws from a somewhat concentrated list of web sources (see Table W4 in Web Appendix C). We encourage researchers to focus on underused or niche sources that have received limited or no attention in marketing. Web data are often prized, as they allow for collecting ""consequential dependent variables from the 'real world'"" ([40], p. 357). Identifying new sources or novel consequential variables constitutes a promising avenue for discovering emerging phenomena.Consider, for example, the twilight state of the nascent legal cannabis industry in the United States. While more states are legalizing cannabis for medical and recreational use, the market value of the legal U.S. cannabis industry was still less than a third of the illegal market in 2020 (i.e., $20 billion vs. $66 billion; [22]). Using surveys, media coverage, and in-depth interviews, marketing scholars have begun to explore how such legalized markets emerge and seek legitimacy ([38]). Sociologists and organizational scholars, in turn, have already used web data to compile intriguing data sets from sources such as Weedmaps. Using these data, they examine, for example, how existing medical cannabis dispensaries have repositioned themselves after the entry of recreational dispensaries ([34]) or how consumers deal with potential stigma transfer (Khessina, Reis, and Cameron Verhaal 2021). By leveraging similar web data, marketing researchers could explore intriguing marketing questions. For instance, how should brands position themselves (e.g., brand personalities, emphasis on product vs. service), depending on the strength of categorical stigma? What are the potential public health and welfare implications of the increasing competition among cannabis dispensaries or their growing social media activities?In addition to being attuned to work in other disciplines, a low-tech route for source exploration is provided by Similarweb, which allows researchers to browse website rankings by region or category. Given the broad accessibility of web sources worldwide, the dominance of Northern American and European data sources is surprising. Not a single article focuses exclusively on African web sources, and only a handful of articles use some African data (e.g., [49]). Possible starting points for branching out into these underexplored marketplaces could be popular websites such as Nairaland.com (online community), bidorbuy.co.za (auction platform), and Jumia.com.ng (e-commerce). Build unique and rich data sets by drawing from multiple sourcesMost published marketing articles use web data gathered from a single source. Only very few articles collect data from a large number of web sources (i.e., 50 or more web sources). Following the lead of these articles, we encourage marketing researchers to envision unique data sets compiled from many and diverse sources. For example, in economics, [10] collected online and offline prices for individual goods sold by 56 large multichannel retailers in ten countries (i.e., United States, United Kingdom, Argentina, Australia, Brazil, Canada, China, Germany, Japan, and South Africa) between 2014 and 2016. This ""Billion Prices Project"" (bpp.mit.edu; [11]) exemplifies how creative and ambitious data collection from diverse web sources can fuel entire research programs. Especially if sufficiently documented, such web data are poised to unearth new fields of gold for the marketing discipline. Rediscover frequently used sourcesAs researchers decide which information to extract (see challenge #2.1), they may overlook novel information on sources they already know. Therefore, refocusing on different information may also reveal how to study novel phenomena on frequently used sources. Adopting a ""discovery mode"" may reveal that phenomena of high societal relevance such as gender or racial issues are occurring at frequently used sources such as TripAdvisor ([69]), Kickstarter ([92]), and DonorsChoose ([ 2]). For example, in entrepreneurship, [92] scraped Kickstarter information to examine whether male African American founders are less successful in crowdfunding. Researchers in marketing, in turn, could build on these and similar ideas to explore whether biases exist in other online market exchanges. Alter the extraction frequencyAnother promising lever for exploring emerging phenomena is the extraction frequency (challenge #2.3). In most articles, the data were extracted once (e.g., on a single occasion). Extracting data once is sufficient for many research objectives, such as demonstrating the prevalence of a phenomenon in the marketplace (e.g., [79]). Yet, researchers can also uncover novel marketing phenomena by creatively envisioning web data sets that only reveal the phenomenon if the information is extracted multiple times. For example, [29] leverage the observation that Amazon removed certain reviews to study the market for ""fake"" reviews. Specifically, they combine repeatedly web-scraped data from Amazon with hand-coded data from large private groups on Facebook used to solicit fake reviews to examine the short- and long-term impact of such rating manipulations. This example illustrates that data imperfections (e.g., data modifications discovered when mapping the data context, see challenge #1.3) can be opportunities to pose novel research questions rather than merely nuisances that warrant correction. Direction 2: Harvest the Versatility of Web Data to Boost Ecological ValueAs a second direction for knowledge discovery, web data are often used to increase the ecological value of marketing research by complementing carefully controlled experiments. Triangulating findings generated via different methods is fruitful. Yet, there are many other underutilized avenues for how researchers can select and extract web data to infuse ecological validity into experiments and other types of marketing studies. Infuse ecological validity into experimental stimuliBy carefully selecting websites and APIs, researchers can enhance the ecological validity of their experiments (e.g., through more realistic or diverse stimuli and measures). This enormous potential has hardly been realized in marketing, particularly at scale (for a creative smaller-scale application, see [62]). Social psychologists demonstrate the full potential of such an approach. Consider, for example, [33], who scraped 87 real-world profiles of doctors (including their fitness habits) from the website of a health insurance provider. These profiles served as the foundation for a novel stimulus-sampling paradigm wherein participants in experiments were presented with randomly selected subsets (i.e., five fitness-focused and five non-fitness-focused profiles). In doing so, the authors first ground the phenomenon in the field (i.e., that doctors signal their fitness habits) and then use stimuli created from real profiles to demonstrate that overweight and obese individuals are less likely to choose fitness-focused doctors for their own care. Such triangulation and the creation of larger and more representative samples of naturalistic stimuli enhance the replicability and generalizability of experimental effects ([42]). The experimental paradigms in core marketing topics (e.g., branding, advertising, pricing) and methods (e.g., lab experiments, conjoint studies) could benefit from similar applications to mimic real marketplaces. For instance, branding or advertising researchers might develop stimuli based on data extracted from sources like crowdfunding platforms or Bing's Image Search API (e.g., brand logos, ads, and slogans). Run self-administered field experiments via APIsWhile field experiments continue to be prized for their realism and high ecological value ([83]), very few published marketing articles use APIs to run field experiments (e.g., [51]; [80]). There are many untapped opportunities to run field experiments administered by researchers rather than cooperating partners (e.g., firms or charities). Using APIs to run field experiments gives researchers more control over the design and debriefing processes and allows for monitoring of granular participant behavior over longer periods. Thus, web data–based field experiments potentially produce more precise effect sizes and allow researchers to capture long-term effects ([26]). In such experiments, researchers might randomly assign users to different treatments, such as adding (vs. not adding) followers on Twitter ([80]) or assigning (vs. not assigning) Reddit's Gold Awards to user posts ([ 9]). By gathering high-frequency data via APIs, researchers can analyze how experimental treatments influence outcomes such as posting or the creativity of user-generated content. Alternatively, APIs can be leveraged to infuse realism into experiments, as embodied in [56], who developed ""Hoogle,"" a mock search engine that relies on APIs offered by Google but only displays organic search results that are not altered based on previous user queries. We foresee many more creative future applications of web data to facilitate such field experimentation. Direction 3: Adopt New Metrics and Methods for Generating Marketing InsightsA core topic in marketing research is to develop marketing metrics that can guide managerial decision making. Traditionally, many metrics have been based on offline information and established data providers (e.g., [21]). Given the continued growth and diversification of web data, it is tempting for marketing managers to focus more on web data for managing firm growth and profitability. Yet, deciding which information to select and extract for marketing insight is challenging (see challenge #2.1). More research is needed to help managers avoid succumbing to the streetlight effect (i.e., an ""overreliance on readily available data due to ease of measurement and application, irrespective of their growth objective""; [18], pp. 164–65). But, how can researchers get started? Explore which web sources provide cheaper, faster, or better marketing metricsOver the last decade, scholars have begun to explore which types of web data could proxy or improve on existing core marketing metrics. For example, managers may use search data extracted from Google to spot trends in the relative importance of their firm's product attributes, which is more cost effective than traditional methods ([17]). Mining Twitter data provides cheaper, real-time, and more actionable measures and insights about brand reputation than existing survey-based metrics like the Brand Asset Valuator data from the advertising agency VMLY&R ([71]). Yet, in other circumstances, readily and cheaply available web data might not be a good substitute for more expensive or established proprietary data sources to uncover market structure ([70]).An exciting direction for future research is to explore what web data sources should be selected or combined to generate marketing insights that fuel firm growth. For instance, many novel metrics rely on textual data ([ 6]). This focus limits applications to markets using the same language employed by the original method (i.e., mostly English). Future research could explore what other types of web data might enable the creation of metrics and insights that allow real-time monitoring and managing diverse global markets. What insights can managers draw from differences and commonalities between the volume of different kinds of internet searches available via Google Trends (e.g., web search vs. image search vs. Google Shopping vs. YouTube search)? Alternatively, what insights about consumer preferences (or any other stakeholder) can be extracted from short videos posted on platforms such as TikTok? Operate API-based microservicesA fascinating opportunity arises from providing microservices via APIs to marketing stakeholders. This means that researchers not only use APIs to retrieve data but can also operate their own APIs to examine real marketplaces (e.g., using rplumber.io in R). Researchers in data science, for example, offer firms a framework for testing multiarmed bandit policies via APIs while at the same time gathering field experimental data ([48]). Marketing researchers could use similar API-powered microservices to study emerging topics such as recommendation systems (and resulting biases) or tap into a firm's customer relationship management system to validate new customer churn models. At a small scale, researcher-powered APIs could lower the entry barriers for firms to experiment with novel algorithms that have not yet been implemented in major software packages.The provision of APIs provides access to novel types of data, while also increasing the timeliness and ecological value of such data. For example, consider the differences between web data collected by a web scraper and the underlying clickstream data stored in the company's database. The website may merely show aggregate statistics about the number of reviews posted. At the same time, the underlying clickstream data also feature information on every website visit (e.g., time, IP address). As with self-administered APIs, researchers define which information a company should submit (e.g., as input to a recommendation algorithm). Thus, researchers can gain access to unique firm data that are otherwise difficult to obtain. For example, large-scale studies with image and video data are still scarce in marketing. Offering image and video analysis as microservices may generate knowledge discovery for new image sources, such as GIFs used in social media (e.g., Giphy). Direction 4: Exploit Efficiency Gains to Improve MeasurementWeb data also have advanced marketing by improving measurement by efficiently collecting diverse variables. Therefore, as a fourth direction, we discuss how web data can improve measurement across the discipline, particularly by rejuvenating interest in core marketing topics (e.g., market orientation, advertising; for an overview of these topics, see [41]). Relatedly, researchers can also leverage APIs to effectively integrate algorithms for processing unstructured data at scale into empirical analyses ([88]). Leverage web sources to describe diverse online and offline behaviorsMost marketing articles gather web data to describe and examine behavior occurring online. As documented in Table W4 in Web Appendix C, many of the used sources in marketing are focused on online consumer behaviors, such as e-commerce websites (e.g., Amazon), online reviewing platforms (e.g., Yelp), social media sites (e.g., Twitter), and search engines (e.g., Google Trends). Relatively less research has focused on firm behavior online. Yet, by doing so, researchers could explore many core marketing constructs (e.g., service orientation, sustainability). For example, researchers could systematically collect information available on the websites of many firms to analyze which organizational factors influence how firms signal their service orientation (e.g., employees' digital presence; [30]) or environmental credentials (e.g., the B Corporations certification; [25]) to customers and other stakeholders.We encourage marketing researchers who have not yet used web data in their research to consider websites and APIs as valuable, rich, and timely sources to exploit the increased digitization of all forms of behaviors—not only online behavior. A recent example of bringing web data into an established ""offline"" research stream is [32], who scraped the annual reports of more than 8,000 firms from AnnualReports.com between 1998 and 2016. Web sources contain historical information about periods, even long before the web in its current form existed (e.g., 1998 in this case). The authors subsequently use these reports to develop a novel text-based measure of marketing excellence derived from firm letters to shareholders. Many other untapped online sources (e.g., job posting platforms) offer new insights into how firms communicate their marketing capabilities to external stakeholders beyond consumers, such as prospective employees, social activists, and investors.Particularly for the marketing–finance interface, the web features many understudied forms of investor-facing communication that are ripe for collection at scale. For example, which type of marketing topics besides marketing excellence (e.g., marketing capabilities, brand positioning, pricing) should top management emphasize to investors to increase firm valuation during investor relations presentations, investor days, or earnings calls? Researchers could also examine the relative importance of the content versus the delivery (e.g., the tone of the speaker on the recording of an investor day; see [85]). Such multimodal data can also benefit the inferences made in established research streams. Embrace APIs for better measurementAPIs offer many opportunities for improving measurement—some of which are unexpected. For example, consumer researchers planning to run longitudinal studies might consider APIs for automating processes for managing participants at scale, thereby reducing the operating costs (and potentially boosting sample size). The Amazon Mechanical Turk API and the various Prolific Academic APIs (e.g., Study API) are good starting points for running multiwave studies.APIs also enable much more than just retrieving data. For example, to reduce validity concerns in long-term data collection, researchers can use the Pushover API (https://pushover.net/api) to send monitoring alerts to their smartphones. The API of Amazon Web Services allows for the orchestration of virtual computing infrastructure (e.g., to capture data from different countries). Another fruitful avenue in which APIs are currently underused in marketing is facilitating stimuli selection. For example, a classic area of inquiry in marketing is how (background) music affects product and brand perceptions and choices (e.g., [ 8]). In 2022, background music is quite different (e.g., self-chosen, more diverse use cases). Researchers could use the Spotify Web API to select stimuli from millions of mood, sleep, or study playlists, thereby discovering perfect ""lookalikes"" that only differ on one focal attribute (e.g., tempo) but not on other acoustic attributes available at the API (e.g., valence, loudness). Even in this simple example, there might be a substantive interest in better understanding the effect of new background music on consumption choices, especially given the shift to working and studying from home. Concluding ThoughtsWeb data have unearthed many fields of gold in marketing. However, extracting data for generating relevant and valid research insights is challenging. Our article highlights validity concerns that require the joint consideration of idiosyncratic technical, legal, and ethical questions. We introduce a novel methodological framework (Figure 2), offer practical solutions (Tables 2–4), and outline directions for future research to enable researchers to create impactful and credible marketing knowledge. While our focus is primarily on authors, our work also spotlights crucial validity concerns to scholars reviewing web data–based research and practitioners interested in deriving accurate and actionable marketing insights from web data.We hope that our work encourages marketing scholars to integrate web data into their research programs. While web data often provide compelling answers to the question, ""Assuming that this hypothesis is true, in what ways does it manifest in the world?"" ([ 5], p. 1455), this does not imply that web data are relevant for all research projects. Web data sources tend to feature a large N (i.e., many users) with many V (i.e., different pieces of information for potential variables) observable over a large T (i.e., many observations over extended periods of time and at a very granular level; [ 1]). Yet, collecting web data via web scraping or APIs provides limited information about the browsing behavior of individuals on the website that led to the creation of the data in the first place. Significant synergies exist by enriching clickstream stream data capturing such browsing processes with web data retrieved from web scraping and APIs (e.g., [54]).Our work aims to bridge entrenched training silos (e.g., between quantitative marketing and consumer behavior). We encourage scholars to further integrate and leverage existing best practices with regard to the collection and analysis of web data (e.g., preregistration, addressing endogeneity). There is significant untapped potential for collaborations across methodological traditions to explore and exploit new fields of gold. Collecting valid web data can enable marketing as a discipline to enhance its relevance and assert intellectual leadership on important emerging substantive topics that are also increasingly studied in fields such as computer science, information systems, and management science ([63]).We would be remiss not to mention the nonmonetary costs of collecting web data via web scraping and APIs. While browsing the web is (mainly) free, researchers should not assume that collecting web data is costless. The prototype of a data collection can be ready and running in a matter of hours. Yet, researchers will often find out that the data collection does not work entirely as intended or encounter some of the challenges discussed in our methodological framework. Just like with any other method, the devil is in the details.Web data democratize data access and make our discipline more inclusive for scholars who would otherwise find it difficult to obtain access to data. To further reduce entry barriers, it would be helpful to create incentives (e.g., journal space) for rich web data sets and their documentation, like the Billion Prices Project ([11]). Similarly, authors can make their algorithms or data available for other researchers by sharing code publicly or deploying API-based microservices that can increase their methods' adoption and offer unique opportunities for field experimentation. In summary, web data present a golden opportunity to examine important marketing questions, now and in the future. "
17,"Genetic Data: Potential Uses and Misuses in Marketing Advances in molecular genetics have led to the exponential growth of the direct-to-consumer genetic testing industry, resulting in the assembly of massive privately owned genetic databases. This article explores the potential impact of this new data type on the field of marketing. Drawing on findings from behavioral genetic research, the authors propose a framework that incorporates genetic influences into existing consumer behavior theory and use it to survey potential marketing uses of genetic data. Applications include business strategies that rely on genetic variants as bases for segmentation and targeting, creative uses that develop consumers' sense of community and personalization, use of genetically informed study designs to test causal relations, and refinement of consumer theory by uncovering biological mechanisms underlying behavior. The authors further evaluate ethical challenges related to autonomy, privacy, misinformation, and discrimination that are unique to the use of genetic data and are not sufficiently addressed by current regulations. They conclude by proposing an agenda for future research.Keywords: behavioral genetics; big data; consumer genomics; DNA data; ethics; segmentation; technologyIn September 2018, the music streaming service Spotify announced that it would allow its 217 million users to upload their genetic data and create playlists that ""match their genetic ancestry"" ([61]). A few months later, Mexico's national air carrier, Aeroméxico, launched a ""DNA Discounts"" campaign, offering to some customers discounted flights to Mexico, with discount rates that matched the traveler's ""Mexican DNA"" percentage, determined by a genetic test ([144]).[ 5] These actions mark the dawn of a new age, when consumers and firms alike may access information that until recently was rarely accessible: individual-level measures of the human genome.Such data are now available through the direct-to-consumer genetic testing (DTC-GT) market, whose total sales in 2019 exceeded all previous years combined. Most sales come from personalized DNA testing kits—plastic tubes that consumers spit into and then ship off for genomic analysis. The motives for taking a DNA test vary, ranging from the desire to uncover forgotten family histories to assessing genetic predispositions for diseases. As of 2020, more than 30 million people have already taken such personalized DNA tests ([120]). A by-product of the growing DTC-GT market is the accumulation of massive genetic data sets. Industry leaders, such as AncestryDNA and 23andMe, encourage consumers to participate in research by answering surveys about anything from dietary habits to personality, generating enormous data sets for investigating genetic associations to numerous outcomes. Because the sales growth of DTC-GT kits might be slowing down ([45]), DTC-GT companies are aiming to monetize their data to maintain growth. For example, Patrick Chung, a 23andMe board member, noted in an interview that ""the long game here is not to make money selling kits, although the kits are essential to get the base level data"" ([101]). In line with this notion, 23andMe has already accredited access to its data to the pharmaceutical company GlaxoSmithKline in a $300 million deal ([16]).The abundance of privately owned DNA data is concurrent with large-scale data collection efforts of public endeavors such as the UK BioBank, which genotyped nearly half a million U.K. citizens ([20]). National genome projects have also taken off in other countries, including Sweden and Singapore ([134]). The accumulation of genetic data has already fueled the discovery of associations between genes and individual differences in many traits ([91]; [100]; [143]), from dietary habits such as coffee and tea intake ([135]), to psychological traits such as adventurousness ([68]).The current research explores the potential impacts of the DNA revolution on the field of marketing and discusses possible uses and abuses of genetic data by marketers. It is organized as follows. First, we introduce key terms and review recent advances in the fields of behavioral genetics and genealogy. Drawing on these findings, we introduce a theoretical framework that incorporates genetic variables into existing consumer behavior theory. We rely on this framework to conceptually explore applications of genetic data for marketing strategy and research and evaluate under what circumstances genetic tools may be of value to marketers. We then raise ethical challenges that are unique to the use of genetic data in marketing, survey how current regulations address them (or not), and suggest potential solutions. Subsequently, we identify gaps in the current state of knowledge that must be filled to further advance the field and draw a research agenda to address them. A Primer on Human GeneticsThis section introduces basic concepts in human genetics and reviews related research that is relevant for the field of marketing (see Table 1). Our review is intended for readers who are not acquainted with the topic, and it focuses on research using DNA measures (the only type of genetic data currently available at scale). We admittedly abstract away from many subtleties and refer interested readers to other publications for more comprehensive reviews ([21]; [83]) and surveys of research using other genetic data modalities (for epigenetics, see [82]; for RNA sequencing, see [132]; for gene therapy, see [149]).GraphTable 1. Illustrative Genetics Literature of Marketing-Relevant Outcomes. ReferenceOutcomesTwin Studies of Behavioral TendenciesCesarini et al. (2009)Altruism, risk-takingCesarini et al. (2010)Investment portfolio riskSimonson and Sela (2011)Default bias, tendency to choose a compromise option, preference for utilitarian goodsCesarini et al. (2012)Procrastination, default bias, conjunction fallacy, cognitive reflectionLoewen and Dawes (2012)Voting turnoutMiller et al. (2012)Mobile phone usage (call and text frequency)Cronqvist and Siegel (2014)Stock market participation, asset allocationPedersen et al. (2015)Blood donationHall, Loscalzo, and Kaptchuk (2015)Susceptibility to placebo effectsGenetic Variants with Direct Influence on Target OutcomesHealth CareGiri, Zhang, and Lü (2016)Early onset Alzheimer's diseaseKuchenbaecker et al. (2017)Breast and ovarian cancerRedondo, Steck, and Pugliese (2018)Type I diabetesAl-Khelaifi et al. (2019)Muscle compositionBlauwendraat, Nalls, and Singleton (2020)Parkinson's diseaseNutritionEng, Luczak, and Wall (2007)Alcohol intoleranceMattar et al. (2012)Lactose intoleranceNehlig (2018)Caffeine metabolismBeautyShimomura et al. (2008)Wooly hairBataille et al. (2000)FrecklesPirastu et al. (2017)Hair loss (alopecia)Genome-Wide Association StudiesHu et al. (2016)Propensity to be a morning personTaylor, Smith, and Munafò (2018)Nonalcoholic beverage consumption (including coffee and tea)Lee et al. (2018)Educational attainmentDay, Ong, and Perry (2018)Loneliness; attendance of gyms, clubs, or pubs; participation in religious groupsSanchez-Roige et al. (2018)Personality traitsYengo et al. (2018)Height, body mass indexAl-Khelaifi et al. (2019)Athletic performanceBaselmans et al. (2019)Life satisfaction, positive affect, neuroticism, depressive symptomsErzurumluoglu et al. (2020)Smoking initiation, cigarettes/day, smoking cessationKarlsson Linnér et al. (2019)Risk tolerance, adventurousness, speeding, alcohol consumption, smoking, number lifetime sex partners  The Human Genome and Its MeasurementThe human genome is a sequence of about 3 billion base pairs. There are four types of bases: adenine (A), thymine (T), guanine (G), and cytosine (C). The base pairs are packaged into structures called chromosomes and are indexed based on their location on the sequence. Every human has two copies of each chromosome, one inherited from each parent. The base pairs in most genome locations are identical across all humans and are thus not informative about interindividual variability. However, there is a small number of locations (<2%) called polymorphisms where individuals commonly differ. The most common type of polymorphism is the single-nucleotide polymorphism (SNP), which denotes locations where a single base pair differs across individuals.[ 6] For most SNPs, only two possible base pair types are observed in a given species. The more frequent base pair is called the major allele, and the other is called the minor allele. As all humans inherit one chromosome from each parent, they also inherit two copies of each SNP, and thus have either zero, one, or two minor alleles in every SNP location. This property allows for the storage of an individual's genetic data in terms of numbers of minor alleles at each SNP location (0, 1, or 2). Certain SNPs are located in subsequences of base pairs called genes. Genes shape the structure and function of every cell in the human body and are involved in many biological processes, most notably the construction of proteins ([44]). The human genome includes 20,000 to 30,000 genes.Until recently, it was extraordinarily time consuming and expensive to measure genetic variation of individuals. However, technological advances following the sequencing of the human genome by the Human Genome Project ([29]) have enabled cost-effective measurements of the genome across individuals. Common measurement techniques quantify variations in selected genome locations (typically under 1 million SNPs) where humans commonly differ. From there, around 20 million other SNPs are imputed. Twin Studies and the Three Laws of Behavioral GeneticsBehavioral genetics is a discipline dedicated to studying the relationship between genetic code and behavioral traits (also called phenotypes). Early research in the field mainly consisted of twin studies—which rely on the fact that identical (monozygotic) twins are on average twice more genetically similar to each other than fraternal (heterozygotic) twins. Under some strong assumptions ([43]), twin studies enable us to estimate a trait's heritability—the part of its interindividual variance that can be attributed to genetics. Surprisingly, twin studies have shown that most human behavioral traits are, to some degree, heritable. This finding is commonly known as ""The First Law of Behavioral Genetics"" ([138]) and was illustrated for manifold phenotypes, from psychological traits such as personality to real-life outcomes such as marital status (see Table 1). Two other empirical regularities characterize findings from behavioral twin studies. The Second Law of Behavioral Genetics states that the effect of being raised in the same family is typically smaller than that of genetics. The Third Law of Behavioral Genetics denotes that substantial behavioral variations are not accounted for by either genetics or family environment. Nonetheless, the Three Laws are not without exceptions. On the one hand, many biological phenotypes that are highly relevant for marketing of health care, nutrition, and beauty products, such as lactose intolerance (for additional examples, see Table 1) are highly heritable. The downstream behavioral consequences of these traits (e.g., the tendency to buy dairy alternative products)[ 7] are expected to be more heritable. On the other hand, various culture-related characteristics, such as one's native language or nationality, are entirely driven by the environment yet can be predicted from genetic ancestry (see the ""Genetic Ancestry"" subsection). The Three Laws demonstrate the promises and drawbacks of using measurements of the genome in marketing. Although genomes are informative about a wide range of relevant outcomes, genetic information is usually not informative for making individual-level predictions of most behavioral traits without additional variables ([60]). A unique feature of DNA data is that they are currently immutable across one's lifespan. Thus, such measures may be informative of one's future behavior long before any other variables become informative. Genome-Wide Association StudiesAlthough twin studies produce heritability estimates, they remain silent about contributions of specific genetic variants to a trait's variability. The first wave of research addressing this gap consisted of candidate gene studies—theoretically motivated examinations of associations between phenotypes and SNPs located in specific genes that were a priori hypothesized to be related to them ([77]). For example, the known role of serotonin in depression motivated studies investigating the association between depression and SNPs located on serotonergic genes ([109]). Although candidate-gene studies have yielded eye-catching findings for some applications, most in the behavioral domain have failed to replicate in subsequent studies. This failure is attributed to low statistical power, a lack of appropriate correction for multiple hypotheses testing, and a lack of control for confounding factors ([25]). Development of genotyping techniques, together with massive data collection efforts, has led to a paradigm shift from candidate-gene studies to genome-wide association studies (GWAS; [143])—data-driven investigations of the relationships between phenotypes and SNPs across the entire genome. Due to the large number of associations studied, GWAS methodology emphasizes stringent correction for multiple testing, preregistration, and replication in independent samples. Over the past decade, GWAS samples have grown from thousands to millions of participants, and the increase in statistical power has allowed researchers to identify numerous replicable associations between SNPs and behavioral phenotypes (see Table 1). However, a typical behavioral trait is associated with numerous variants, each of them accounting for a very small part (R2 <.01%) of its variance, an observation known as the ""Fourth Law of Behavioral Genetics"" ([25]).While the contribution of individual SNPs to the variability of most human behavioral traits is minute, one can obtain greater explanatory power by aggregating their effects to a polygenic risk score (PRS). The PRS is a linear combination of the most significant SNPs identified in its GWAS, and it becomes increasingly accurate as sample sizes increase. For example, a PRS constructed from a recent GWAS in 1 million people was able to predict 13% of the variance in the educational attainment in an independent sample ([80]). Polygenic risk scores are similarly informative on many behavioral traits, and firms that possess genetic data can construct them using GWAS summary statistics that either are publicly available ([91]) or can be obtained from other organizations. Yet a significant share of current publicly available PRSs of behavioral phenotypes are not accurate enough for making individual-level predictions. Furthermore, the predictive accuracy of PRSs typically decreases when applied to populations different from those used to estimate them (e.g., in ethnicity and socioeconomic status; [37]; [94]). Nonetheless, publicly available PRSs are typically computed from samples of only up to a million individuals, whereas DTC-GC companies have access to samples that are an order of magnitude larger. Moreover, advanced statistical techniques show a high potential for obtaining more accurate genome-based predictions (see the ""Advanced Targeting and Prediction"" subsection). Genetic AncestryThe possibility to quantify genetic variations in individuals has also opened up the path for studying genetic variation between populations. A common approach is to perform principal component analysis on the genetic data of a population sample in search of high-order factors that capture its variability ([ 2]). These principal components (PCs) are highly informative about one's genetic ancestry and location.[ 8] For example, a study of individuals from 51 populations worldwide found that the first PC distinguished sub-Saharan Africans from non-Africans and the second PC differentiated populations from Eastern and Western Eurasia ([84]). These findings were echoed by studies of less diverse samples that used the same method for high-resolution ancestry mapping (e.g., [106]). The PCs are also commonly used as control variables in GWAS and other population studies to account for environmental factors that vary across ethnic groups ([116]; [102]). The relevance of genetic ancestry for marketing stems from its noncausal correlation with environmental factors such as language and culture. For example, individuals of Irish ancestry are more likely to be interested in a cultural heritage trip to Ireland or celebrate Saint Patrick's Day with a pint of Guinness, and their interests may stem from cultural influences related to their ancestry. Marketers with access to genetic data may be able to infer such behavioral tendencies and the motivations underlying them and use such insight for targeting and positioning. Incorporating Genetics into Marketing TheoryThe Four Laws of Behavioral Genetics provide solid empirical grounds from which explorations of genetic effects on consumer behavior can embark. Translating these fundamental insights into applications, however, depends on incorporating them into consumer behavior theory and models. This section proposes such a framework, illustrated in Figure 1. Our theory extends the well-known stimulus-response model, which describes behavior as arising from the interaction between the organism (consumer) and stimulus ([10]). The stimulus is described via object variables, such as the products, prices, and brands offered, and situational variables, such as location, time of day, and context. The organism has traditionally been marked by personal variables denoting characteristics that are ""stable over times and places of observation and may therefore be attributed consistently to the individual"" ([10], p. 36). Typical personal variables include demographics, psychographics, and behavioral dispositions.Graph: Figure 1. A model of genetic effects on consumer behavior.Notes: Arrows represent causal relations and interactions. Dashed lines denote important noncausal correlations.Our framework extends the stimulus-response model by incorporating the elementary factors described in the previous section. We group these factors into three categories: ( 1) environment, which includes stable cultural, social, and geographical factors, as well as the flow of time influencing development and aging; ( 2) family factors, such as parenting style; and ( 3) the individual's genome, which depends on familial background, except for cases of adoption and recomposed families. Our framework considers familial and environmental factors as external to the organism, where the genome is within the organism and constitutes the most stable type of personal variables: it is fixed at conception and remains mostly stable throughout the lifespan. Our framework also extends the description of the organism by incorporating stable biological traits such as physiology (e.g., height), anatomy (e.g., brain structure), and typical brain function (e.g., connectivity between brain areas at rest). These biological traits are more directly influenced by genetics and typically mediate the influence of genetics on nonbiological personal traits. When such mediation occurs, the mediating biological trait is commonly referred to as an endophenotype.As indicated by the Three Laws of Behavioral Genetics, the environment plays a major role in the development of most personal traits. Nonetheless, genetic influences affect many outcomes of interest, starting from prenatal development and early-life stages. These effects occur via interactions with familial and environmental factors and are mediated via endophenotypes that are more directly susceptible to genetic influences, such as brain anatomy ([137]). The relative impact of genetics varies by trait. In some cases, few genetic variants have strong direct effects on a biological endophenotype (e.g., lactose tolerance; for other examples, see Table 1), and genetic data will be highly informative of their downstream consequences (e.g., interest in dairy alternatives). Most personal traits, however, are only moderately heritable and are influenced by interactions between numerous genetic and environmental factors. Importantly, the genome is also informative about characteristics that are not influenced by genetics at all, due to the noncausal correlations between genetics and environmental or familial factors (dashed lines in Figure 1). If genetic data are available, such links allow for the inference of consumer characteristics such as cultural heritage and language.The impact of genetics continues through the lifespan via two main channels. First, genetics affects later-life outcomes through its prior influence on traits that had developed earlier. For example, variants that contribute to early-life intellectual development continue to affect one's educational attainment and career in adulthood. Second, genetics continues to interact with environmental factors (e.g., time, nutrition) to influence later-life development of personal traits through biological endophenotypes such as brain anatomy and function ([131]). Although the heritability of later-life traits is typically moderate, characteristics that have a strong biological basis are well-explained by interactions between genetics and time. For example, a few SNPs explain 38% of the variance in hair loss (alopecia) in men ([111]), whose associated market size is expected to reach $3.9 billion by 2026 ([54]). These SNPs likely capture behavioral variance in this trait's downstream consequences.The final influence of genetics on consumer behaviors, such as information search, purchase decisions, satisfaction, and word-of-mouth activity, occurs through interactions with situation and object. These effects are mediated via biological processes (e.g., changes in neural activity and hormonal levels) that regulate the consumer's emotional and physiological state, as well as cognitive processes such as attention, valuation, and memory ([112]). For instance, genetics affects one's tendency to be an early riser or a night owl ([63]), and this disposition affects arousal via interaction with the time of day (situational variable) to influence behavior. Likewise, situational stressors interact with genetics to generate a person-specific stress response, regulated by activation of the hypothalamic–pituitary–adrenal axis ([46]). This response, in turn, influences decision making (e.g., [92], [93]). Genetics also interacts with object variables, as products and marketing messages may affect genetically regulated attention, reward, and valuation processes. For example, the presence of a desirable food item (e.g., in a supermarket tasting counter) elicits an appetitive (or Pavlovian) response that may increase its subjective valuation ([19]). Animal studies suggest that individual differences in this tendency, which is biologically implemented by the dopaminergic system, is partly accounted by genetic variation ([48]). Additional interactions between genetics and object occur via indirect genetic influences on heritable traits such as personality ([96]) and behavioral dispositions such as the tendency to choose the default or compromise option in a choice set ([24]; [128]). Genetic data may allow for approximating these tendencies without having to rely on large-scale customer surveys. Applications for Marketing StrategyBuilding on the framework introduced in the previous section, the following two sections discuss how the availability of genetic data may advance marketing practice and research (see Figure 2 and Table 2). We highlight that some of these applications, especially when employed by private entities in a for-profit setting, raise legal and ethical challenges (discussed in a subsequent section). It remains to be seen whether their potential benefits outweigh these concerns.Graph: Figure 2. Genetic applications for marketing strategy.GraphTable 2. Using Genetics to Advance Marketing Research. ApplicationApproachRelated LiteratureEstimating causal relationsUsing genetic measures as instrumental variables via MR and other PRS-based analytical techniquesSmith and Ebrahim (2004); Zhu et al. (2018); DiPrete, Burik, and Koellinger (2018); O'Connor and Price (2018); Davies et al. (2019); Koellinger and De Vlaming (2019)Accounting for otherwise unobserved heterogeneityIncluding genetic PCs and PRS as control variables in observational studies and randomized-controlled trialsPrice et al. (2006); Nave et al. (2018); Buniello et al. (2019); Harden and Koellinger (2020); Benjamin et al. (2012)Studying person–subject and person–situation interactionsApproximating unobservable traits using their PRS and exploring their interactions with other observed variables (e.g., treatment effects)Buniello et al. (2019);Studying relationships between traitsQuantifying the variance that traits share due to genetic causes via their genetic correlations (rg), which can be estimated between any two traits for which a GWAS has ever been conductedLynch and Walsh (1998); Bulik-Sullivan et al. (2015)Identifying biological mechanismsOnce genetic variants are linked to a behavioral trait, they can be tied to neurobiological and physiological systems via bioinformatic toolsFinucane et al. (2015); Dass et al. (2019); Aydogan et al. (2021)  Gene-Based SegmentationWhen genetic variations correspond with consumer needs, firms may rely on genetic data to divide the market into distinct, stable, and identifiable subsets to be reached with unique marketing mixes ([49]). In some cases, genetic variants are indeed directly associated with consumer needs via known mechanisms. A firm or institution could thus rely on genetic data to identify segments that benefit from its products and services. Prior research has uncovered mechanisms that link genetic variants to phenotypes that closely map onto consumer needs in various domains (see Table 1). Most current knowledge concerns outcomes related to health care, nutrition, and beauty, with applications such as promoting screening or prevention products to individuals who are at increased risk of developing pathologies such as cancer, diabetes, or Alzheimer's disease. Indeed, leading DTC-GT companies already provide information on such risks to their consumers and aspire to use their data to become the ""Google of personalized health care"" ([101]). As genetic databases grow in size, research for nonmedically relevant causal effects is expected to increase and yield new discoveries that are relevant for marketing strategy across domains. For example, a brand manager of a product for preventing men's hair loss could rely on a specific genetic variation linked to male pattern baldness ([111]) to identify segments that are genetically disposed to alopecia. The brand manager may even be able to identify future customers long before they show any behavioral indication that they may need the product (e.g., via web searches) and increase their awareness of the brand (e.g., by advertising to males in their late 20s who are genetically disposed to baldness in their mid-30s). Using Whole Genomes to Infer Other Segmentation BasesAs Figure 1 illustrates, genetic variation correlates with almost every personal characteristic. As a result, genetic data can be used for reaching market segments when nongenetic managerially relevant variables cannot be easily observed at scale. In contrast to the direct use of specific genes as segmentation bases, most SNP associations to behavioral traits occur outside of genes, and marketers can leverage their cumulative information to infer other (nongenetic) segmentation bases. Once genetic data are available, a firm can construct for every individual in a target population PRSs that are predictive (to some degree) of every trait for which a GWAS has ever been performed ([18]). Similarly, a firm can rely on previous findings of genealogical research for calculating individual-level ancestry estimates to infer various culturally distinct motivations, interests, and behaviors. The usefulness of genomes as proxies for other segmentation variables crucially depends on how predictive they are of the target trait relative to other measures. Although genetic data might not be the most predictive of a target trait, it may be more convenient than other data sources such as surveys, which might be costly and subject to low response rates. Furthermore, adding genes to predictive models that use other variable types may improve their predictive accuracy at the individual level (see the ""Are Genes More Predictive Than Other Measures?"" subsection). Advanced Targeting and PredictionMarketers often aim to predict the probability of a single behavior (purchase, click on an ad, etc.), without necessarily understanding the underlying mechanism. As such, even a simple PRS constitutes a straightforward tool for targeting. Firms that obtain genetic data, but not samples that are large enough to estimate the coefficients used to construct a PRS, could potentially recover them from the public domain ([18]) and other organizations. More advanced statistical learning methods ([86]), including deep learning algorithms ([157]; [41]), have been adapted to genetic data to generate more accurate predictions. Furthermore, when genetic predictive estimates are available, they can be used in conjunction with other variables for early identification of consumers with high lifetime value. For instance, a coffeehouse chain may want to target consumers with a high genetic potential to enjoy espresso before they show any prior espresso-purchasing patterns in their behavioral data. While counterintuitive, such an approach would potentially allow for reaching consumers who have not yet developed an espresso consumption habit and thus are not ""locked in"" to a particular brand. This is in contrast to targeting based on more traditional variables (e.g., behavioral measures) that are likely to become predictive only after the person has already tried and developed the habit of consuming a competitor's brand.A different approach using genetic data for behavioral prediction is to consider that genomes are representative of family relations and, as such, can be used to compute a comprehensive map of relatedness between individuals. Such a map can then be used for targeting in a similar manner to social network graphs ([142]; [148]). Another possibility is to compute genetic relatedness (or inversely, genetic distance) between individuals ([117]), either for the whole genome or chromosome-wise, and leverage this metric for behavioral prediction. For instance, a company could target people who are within a small genetic distance from existing clusters of loyal customers. Methods such as collaborative filtering, nearest neighbors, or more advanced machine learning algorithms could be applied to implement such strategies ([87]). Notably, geneticists are already using similar techniques that do not depend on identifying links between specific genes and a phenotype to estimate the variance in a trait that can be explained by SNP-derived genetic distance ([151]). For example, such methods have shown that 51% of the general population variance in fluid intelligence could be explained by genetic distance, quantified from SNPs, using a sample of a few thousand people ([33]). Creative Uses: Product Development and PositioningFinally, DNA has a unique status as a ""cultural icon"" ([105]), which opens the door for creative uses, including new product development and repositioning of existing products and brands. Genetic data provide a new means of ""knowing thyself,"" connecting to previously unknown genetic relatives, and building bridges between people and their ancient family histories ([139]). Leading DT-GTC companies have created several new products and positioning strategies that translate their customers' fascination with DNA into applications that promote their sense of community and personalization. Notable examples are the aforementioned partnership between Spotify and AncestryDNA and the collaboration between Airbnb and 23andMe, which developed a service that helps travelers organize cultural experiences tailored to their ancestry. Ancestry-based positioning strategies of products and services in other domains, including entertainment (e.g., period dramas such as Downton Abbey and Braveheart), food (e.g., traditional cookbooks) and tourism (e.g., museums, heritage sites) could similarly benefit from such partnerships and creative uses. Similar strategies could employ PRS or single genetic variants. For example, most elite power athletes have a specific variant of the ACTN3 gene that encodes a protein expressed in muscle fibers ([ 3]; [79]). A sporting brand may be able to develop positioning strategies that generate a sense of community among amateur athletes who carry this variant and promote their sense of identification with brand ambassadors who also carry it. Using Genetics to Advance Marketing ResearchGenetic data can refine and substantiate existing theories of consumer behavior by illuminating the nature of relationships between traits and revealing the biological mechanisms underlying individual differences in behavior. Some of these applications are similar in nature to uses of genetics in other fields of the social sciences ([11]; [60]), where others are unique to marketing research. Estimating Causal RelationsIn many domains of consumer research, it is not feasible to study causal relations between variables experimentally. For example, experimentally studying the causal relationship between one's consumption habits and long-term happiness ([52]; [124]) would require randomly assigning individuals into groups that differ in their consumption habits or in their well-being. Such assignment could be extremely difficult for some variables and even unethical (e.g., if a group is required to worsen dietary habits, creating a threat to their health). Furthermore, studying such causal relations using observational data is also not straightforward. First, many personal and environmental factors (e.g., socioeconomic status, personality) confound the relationship between the explanatory variable and outcome. Second, there exists a possibility of reverse causality.Sometimes, it is possible to overcome the aforementioned limitations using instrumental variables (IVs; [ 4]). Instrumental variables are factors that cause changes in the explanatory variable of interest (e.g., consumption habits) and have no other independent effects on the outcome (e.g., happiness), enabling one to estimate the causal effect without bias due to confounds and reverse causality. Under some circumstances, genetic measures can be used as IVs. This is possible because the transmission of genetic variants from parents to offspring is determined via a ""genetic lottery"" that is independent from environmental factors (conditioned on the parents' genomes). Furthermore, because genetic variations are not influenced by one's environment or habits, reverse causality is not a concern.The most common method that uses genetic measures as IVs is Mendelian randomization (MR; [130]), which can be thought of as a natural experiment that occurs at the time of conception. Mendelian randomization uses genetic variants that have well-established causal influences on the explanatory trait as IVs to quantify the trait's causal effect on an outcome. For example, medical researchers have been using variants that regulate alcohol metabolism as IVs for studying the long-term causal effects of alcohol consumption on outcomes such as cardiovascular disease and cognitive decline (e.g., [26]). When using genetic data to infer causal relations, it is important to keep a careful eye on the assumptions of the methods used to estimate the effects. One crucial issue is that the transmission of genes occurs at random only within a family. Therefore, MR studies should ideally rely on within-family designs that compare genetic variation between related individuals (e.g., sibling pairs, parent–offspring trios). Mendelian randomization studies that do not use such designs are susceptible to biases of various sources ([34]). A second important assumption of MR is that the genes used as IVs affect the outcome only via their effect on the explanatory trait (a criterion called ""exclusion restriction""). It is therefore important that the mechanisms linking the genetic IVs and the explanatory variable are well-understood, and that the genes' prevalence in the population studied does not correlate with unobservable environmental factors that might influence the outcome ([74]).One limitation of MR is that most genetic variants are relatively weak instruments, because their associations with personal traits of interest are small. Moreover, genetic variants typically correlate with multiple traits that could influence an outcome, a phenomenon called ""pleiotropy."" Several statistical techniques that rely on summary statistics from large-scale GWAS (instead of single variants) have been recently proposed to overcome these issues ([36]; [108]; [156]). Each of these methods relies on a different set of assumptions concerning the relationships between genetics and other variables that are included in (or omitted from) the model, for estimating a causal effect. To mitigate concerns that claims of causality are driven by any specific assumptions, it is crucial to verify that a study's conclusion is consistent across methods. We anticipate that continuing development of such methods, together with the growing availability of data sets that include genetic measures of related-individuals, will provide a fertile ground for investigations of causal relations for a broader range of settings in the near future. Accounting for Otherwise Unobserved Heterogeneity using PRSsGenetic variation between individuals is fixed across the lifespan and can be related to many outcomes of interest to consumer researchers. As such, including genetic variables (most notably PRSs and genetic PCs) in statistical models that quantify any nongenetic effects provides a means to control for unobservable factors that would otherwise be a part of the model's error. Such reduction of the model's error would increase the study's statistical power and allow estimating model parameters of interest with less uncertainty ([11]). For illustration, consider a field experiment aiming to test the efficiency of different campaigns for preventing smoking initiation among teenagers. In such settings, PRSs can explain one's genetic tendency to smoke, as well as variance related to many preexisting personal characteristics that are not contaminated by the treatment and could be related to future smoking (e.g., extraversion). Including such PRSs in the model would therefore allow for quantifying the treatment effect more accurately. Studying Person–Object and Person–Situation InteractionsGenetic measures are also useful in studying how consumers differentially respond to marketing stimuli or situational contexts. As noted previously, generic variants per se are not of great interest to marketers, but they allow for calculation of PRSs (based on any previously published GWAS) to approximate personal characteristics that cannot be easily measured in large samples (e.g., intelligence, personality) or when the participants' tendencies are not yet expressed behaviorally. Going back to the smoking-prevention field experiment example, constructing PRSs for many unobservable traits in the sample could be used for carrying a post hoc analysis to investigate whether certain individuals more strongly respond to a certain treatment versus another. Studying Relationships Between Traits Using Genetic CorrelationsBecause genetic variation correlates with many personal characteristics, it provides a means for studying the relationships between traits and whether they arise from genetic or environmental causes. A useful method for quantifying the genetic overlap between traits is estimating their genetic correlation (rg), which measures the amount of variance they share due to genetic causes ([90]). A useful feature of genetic correlations is that they can be estimated between any two traits for which GWAS has ever been conducted—even for traits that have not been measured in the same sample ([17]). A recent example for insight obtained from genetic correlations comes from a GWAS of general risk tolerance in a sample of over 1 million people (Karlsson [68]). This study found that the genetic correlations between general risk tolerance and many domain-specific risky behaviors—such as substance use, speeding on motorways, and self-employment—were substantially larger than the correlations observed between the behavioral phenotypes. This finding indicates that common genetic causes influence all these phenotypes, where the translation of this genetic tendency to each of the domain-specific risky behaviors depends on environmental factors. Identifying Biological MechanismsGenetic data can enrich marketing theory by illuminating biological mechanisms that underlie behavior, akin to research in the field of consumer neuroscience ([113]). Apart from straightforward genetic effects on traits like lactose intolerance, genetic analyses can provide insight into how different brain systems mediate the influence of genetics on complex behavioral traits, such as economic preferences and consumption patterns. Although brain imaging studies have long ago uncovered multiple systems that are functionally involved in emotional and cognitive processes, linking functional brain measures to differences across individuals is not straightforward, because of their low test-retest reliability ([39]) and the high cost of obtaining such measures at scale. Genetic variation, in contrast, can be measured reliably and inexpensively in large samples, and once genetic variants are linked to a behavioral trait, they can be tied to neurobiological systems via bioinformatic tools (e.g., [47]). For example, the recent GWAS of general risk tolerance pointed to multiple brain systems that are genetically associated with the trait, including the prefrontal cortex, the amygdala and mid-brain regions involved in reward processing ([68]). An alternative promising approach is to derive biologically informed PRSs, which reflect aggregate effects of variants related to known biological systems (e.g., the dopaminergic genes) on a target phenotype, and investigate their relationship with biological endophenotypes ([32]). The rapid development of bio-annotation techniques, together with the formation of data sets that include both genetic and brain-imaging measures ([ 6]), will facilitate additional discoveries of gene-brain-behavior pathways in the near future. Ethical and Legal ChallengesSimilar to other data types, some marketing uses of genetic data can improve individuals' well-being and have a positive impact on society as a whole. For example, focused early interventions based on genetic data may help health care providers reach patients at high risk for conditions such as diabetes and hypertension and provide them strategies that mitigate these risks (e.g., via physical exercise and diet; [146]). However, genetic data might facilitate manipulation and exploitation of vulnerable individuals ([133]). For example, e-cigarette companies could use genetic data to target teenagers who are more genetically prone to develop nicotine addiction ([121]). Yet the use of human genetic data by marketers raises even further ethical and legal challenges. These issues are the result of several unique features of genetic data, which contain immutable and identifiable information that is predictive of future behavior and disease, both for the individual and their genetic relatives. For this reason, genetic data have been considered particularly sensitive even within the medical field, a view known as ""genetic exceptionalism"" ([56]). In this section, we highlight serious ethical issues that emerge from these unique properties, review the current state of legislation in this area, and propose possible solutions. Identifiability and Informed ConsentExcept for monozygotic twins, genetic data can be uniquely attributed to one person: A mere 60 to 300 randomly selected SNPs are sufficient to identify an individual ([154]). Anonymizing genetic data without destroying a large share of the information is not a simple task. Some methods, for instance, try to balance anonymity and information preservation by clustering the data before analysis ([88]). Even when the data are labeled as anonymized, however, the inherent information they contain could allow for potential reidentification attacks ([150]). Due to the combination of this unique identifiability property and the rich information content of genetic data, using them for research requires obtaining informed consent from study participants ([12]). Nonetheless, even in the ethically stricter research setting, acceptable anonymization and consent practices have been subject to heterogenous standards ([38]).Because most current human genetic research involves analysis of secondary data that have been typically collected long before hypotheses are formed, obtaining consent is challenging. A common solution has been to ask participants to consent for all future research that falls within a broadly defined scope. For example, 23andMe informs customers who volunteer to participate in research that ""the topics to be studied span a wide range of traits and conditions"" and that ""some of these studies may be sponsored by or conducted on behalf of third parties.""[ 9] Similar consent procedures are used in practice by other DTC-GT firms and biobanks. Advocates of the broad consent approach argue that it provides an ideal trade-off between participants' autonomy and the public interest to benefit from research outputs ([59]). However, it is unclear whether genetic research subjects can fully appreciate the potential benefits and risks of any future research at the time of consent. For instance, it is unlikely that 23andMe customers could foresee that access to their data would be sold to a pharmaceutical company under the broad label of ""research."" To overcome these issues, scholars have proposed using dynamic or hybrid consenting protocols, where individuals can opt in to studies or withdraw their consent online ([71]; [114]).To complicate matters further, one's genetic data are informative not only about oneself but also about one's nongenotyped relatives. This issue was recently illustrated in the apprehension of Joseph James DeAngelo, the alleged Golden State Killer, who was arrested after a fraction of his genome could be matched to the DNA of distant relatives, who uploaded their genetic data to a searchable public genealogical database ([118]). Although relatives of genetic research participants are potentially identifiable, current guidelines do not require obtaining their consent yet recommend that participants consult relatives when deciding to take part in research ([98]). These guidelines may change in the future, as genetic identification technology advances.In summary, several unique issues make it difficult to anonymize data and obtain fully informed consent from participants of genetic research. Because this is an active area of study, we recommend that researchers closely monitor the emerging literature on the topic and ensure that their studies comply with the latest ethical guidelines. It is imperative that analyses of publicly available genetic data, collected thanks to public funding, produce discoveries that benefit society as a whole. As for research using privately owned genetic data, it is crucial that informed consent is obtained and that all studies fall beyond a shadow of doubt under the scope of research to which participants had consented. Privacy and SecurityMany of the features that turn genetic data into a marketing opportunity also raise fundamental privacy and security challenges. Genetic data are identifiable, predictive of virtually every aspect of one's life, and are even informative about one's relatives—and thus, could enable firms to target individuals who never opted to share any information. Given that major companies have been known to keep ""shadow profiles"" of individuals who did not register for their services ([50]), this potential privacy threat is imminent. The assembly of privately owned genetic databases also gives rise to security concerns, as major data breaches become increasingly common ([27]). In these cases, third parties obtain data against the will of both the consumer and the data holder. Once leaked, data will likely be used regardless of any regulation or ethical norm.As massive volumes of genetic data reside on the servers of private firms, the question arises as to whether legislation and practice sufficiently protect the privacy of consumers from having their data exploited against their interests. While leading DTC-GT companies argue that their research complies with ethical guidelines, and they have a clear interest to avoid public controversies, it is unclear whether they follow the same principles when using data for marketing. As of July 2020, market leader 23andMe indicates in its (unilaterally modifiable) privacy statement that it would not process genetic data for marketing purposes without explicit consent, implying that it may do so if consent is given.[10] Furthermore, ethical recommendations are likely not a priority for all entities that own genetic data. In the absence of legal regulation and transparency, it becomes difficult to know exactly how private companies use the data.Surprisingly, current federal laws in the United States concerning the use of genetic data have little implications on the DTC-GT industry, and U.S. lawmakers have mostly remained silent (with some notable exceptions) regarding potential regulations on the use of genetic data. As a consequence, the license to use and share genetic data for marketing purposes depends on the privacy policy of each individual company. Currently, a large number of U.S.-based DTC-GT companies do not provide their customers with any privacy information (on their website or the testing-kit packages) prior to the purchase of DNA kits, and the policies of many of the remaining companies indicate that they may use genetic data for purposes other than delivering ancestry and health reports. Furthermore, companies often reserve the right to share genetic data with third parties in cases of merger, acquisition, or bankruptcy, or to modify their privacy policies without notification ([62]).In contrast to U.S. federal law, the recent European General Data Protection Regulation, commonly known as GDPR, explicitly recognizes genetic data as ""sensitive"" under Article 9 ([125]) and provides unique protection against sharing of genetic data (even semianonymized). Under current European regulations, one has to provide ""explicit consent to the processing of personal data for one or more specified purposes.""[11] Nonetheless, consumers have been known to easily approve mining of their data without reading the legal terms and services conditions ([107]). Once such consent is provided, virtually every marketing application becomes possible, despite the strict sharing restrictions in place. Furthermore, DTC-GT companies can process genetic data and use them for running marketing campaigns on behalf of other companies, without having to directly share them. For example, DTC-GT companies can offer to forward a message to a subsample of their clients satisfying some criteria on behalf of other entities, without disclosing any data, just as Facebook allows advertisers to target its own users without sharing their data ([96]). Thus, regulatory limits to genetic data sharing may end up simply granting DTC-GT companies a monopoly over the data. Finally, it is important to recognize that the power of regulation might be limited. Industry practices typically advance faster than the policies trying to regulate them, with regulations doing too little too late after malpractice had already been exposed ([64]). Moreover, technology giants have a long history of violating data protection laws and do not appear to be deterred by financial disincentives, as indicated by numerous condemnations and legal battles between regulatory agencies and these entities.A possible solution to the privacy and security issues—which, in our view, is crucial for the continuing growth of the DTC-GT market—is adoption of industry standards that guarantee acceptable practices. One such framework, previously proposed to address similar challenges in artificial intelligence (AI) research, can be directly applied to genetic data ([136]). This framework, namely the ""Four Pillars of Perfectly Privacy-Preserving AI,"" articulates four principles for maintaining privacy, security, and usability of data: ( 1) training data privacy: a malicious actor will not be able to recover genetic data from other accessible information (e.g., model output); ( 2) input privacy: a user's genetic data should not be observed by other parties, including the model creator; ( 3) output privacy: the output of a model should not be visible by anyone except for the user whose data are being analyzed; and ( 4) model privacy: the model (trained or not) should be protected from being stolen by a malicious party.A specific strength of this framework is that privacy is considered from both sides. From the consumer side, data and the inferences (e.g., genetic reports, ads selected for the consumer) are not visible to the company. From the company's side, its algorithms and parameters (e.g., GWAS weights) are not visible to the consumer. Importantly, no data have to be kept on the consumer side if a sufficiently strong encryption algorithm is applied to them before delivery to third-party servers. Thus, the consumer would only need to preserve the encryption key.While algorithms satisfying some or all of the aforementioned criteria are still under active research, several methods to perform privacy-preserving GWAS already exist ([65]; [141]; [153]). With these methods, the GWAS's summary statistics (e.g., weights, p-values) are known to the analyst, yet the PRSs can only be computed on the consumer side. Concurrently, general methods to allow for privacy-preserving versions of machine learning algorithms are being developed and can be expected to be adapted to genetic data mining, following their nonprivacy preserving counterparts ([85]; [127]). An additional advantage of such methods is that users can withdraw their data from the pool unilaterally by deleting either the data or the encryption key. However, even though such methods are constantly being developed, it is far from clear whether companies will end up adopting them. Major industry actors might not feel compelled to change their practice without strong incentives. A possible solution would be to enforce the use of these technologies through regulations. We can, for instance, picture a legal framework wherein a DTC-GT kit cannot be sold in a country without adhering to a framework of this type. MisinformationIn November 2013, the U.S. Food and Drug Administration ordered 23andMe to suspend its genetic health reporting service until the company provided sufficient evidence to support clinical claims made in its reports. The company, which at that time had already sold half a million kits, relaunched the service only two years later, with less elaborate reporting that emphasized the probabilistic nature of genetic diagnosis ([115]). Although regulation of health-related genetic applications has tightened up, this cautionary tale illustrates how companies might use the scientific image of genetics in their consumers' minds to oversell the utility of genetic information. When doing so, marketers could rely on genetic data to make pseudo-scientific claims that promote the appeal of products and services, as commonly done in the wellness industry ([ 7]).In the United States, because nonmedical genetic applications do not pose direct health risks to consumers, the Federal Trade Commission, rather than the Food and Drug Administration, is responsible for regulating potentially deceptive marketing messages that make claims on the utility of genetic data ([70]). However, such oversight might be difficult to exercise, for three main reasons. First, although genetic data are only moderately informative of most human behavioral traits, they do indeed contain some information. As a result, it is difficult to argue that genetic-based recommendations are entirely deceitful. Second, consumers' perceptions of genetics ([155]) might make them prone to believe that genetic-based recommendations are always backed by scientific evidence, even when such claims are not made explicitly. Third, people have a poor intuitive sense of probabilities and thus might be prone to overestimate the informativeness of genetic-based recommendations even when their probabilistic nature is communicated ([140]). In our view, regulation should ensure that companies disclose the science underlying any scientific claims (and its limitations), attempt to communicate probabilistic information intuitively, and avoid the use of deterministic language when appropriate ([147]). DiscriminationSimilar to discrimination based on other unchangeable characteristics, negative treatment of individuals based on their actual (or assumed) genetic markup is a potential source of distress, exclusion, and loss of opportunities ([14]). Furthermore, such discrimination might deter individuals from taking genetic tests that could improve their health care or from participating in genetic research that benefits society as a whole ([67]). To date, most conversations concerning genetic discrimination among ethics and law scholars have focused on potential abuses of genetic data by insurance providers and employers ([66]; [81]). Yet marketing applications of genetic data give rise to similar concerns. Aeroméxico's aforementioned DNA-discounts campaign is a recent prominent example of what is essentially genetic-based price discrimination. While it is unclear whether customers indeed received DNA discounts, the campaign was covered by major popular media outlets and, in general, was received positively by the public.From a legal standpoint, the 1996 Health Insurance Portability and Accountability Act and 2008 Genetic Information Nondiscrimination Act prohibit insurance companies (for specific types of policies) and employers from discriminating against people based on genetic information, but they do not protect individuals from discrimination in other circumstances. However, some state laws, most notably California's Unruh Civil Rights Act, explicitly ban businesses from discriminating against consumers based on genetic information. Similarly, Florida statutes have provisions requiring notification of an individual if genetic information was used in any decision to grant or deny any insurance, employment, mortgage, loan, credit, or educational opportunity. In our view, discrimination based on one's genetic information is a serious issue that should be addressed in the same way as other types of discrimination. Self-Reinforcing LoopsA final nontrivial concern is that marketing strategies that rely on consumers' genes for predicting their preferences and behavior might generate self-reinforcing loops ([55]) that perpetuate inequality and deprive consumer's exploration of options that do not align with their genetic markup. For example, providers of SAT preparation kits could offer promotions to high school students who are genetically disposed to higher education ([80]) and, by doing so, give preferential treatment to individuals who are already in an advantageous position. Open QuestionsForthcoming discoveries in the field of behavioral genetics will undoubtedly advance our understanding of how genetics interacts with the environment to influence behavior. However, assessing the utility of genetic tools for the advancement of marketing theory and practice, and accurately evaluating the severity of ethical concerns, would require addressing several gaps of knowledge in the current literature (summarized in the Appendix). Unveiling the Genetic Underpinnings of Consumer BehaviorMany genetic associations of phenotypes that are of interest to marketers have been identified over the past decade. Nonetheless, the genetic underpinnings of many traits that are more closely tied to consumer behavior and are known to be heritable (see Table 1) have remained elusive. There are two likely reasons for this gap. First, marketing scholars have largely neglected the influence of genetics on consumer behavior (with a few notable exceptions, e.g., [128]). Research in related fields, however, points to genetic effects on many traits that are central to consumer behavior theory and practice. Examples include investment decisions ([23]; [31]), altruism and trust ([22]; [110]), susceptibility to placebo effects ([58]), voting turnout ([89]), and mobile phone usage patterns ([99]). Molecular genetic studies of these traits would be a straightforward extension that can enrich marketing theory and support industry applications.Second, genetic data sets that include fine-grained behavioral measures are scarce. Behavioral geneticists have overcome this limitation by using measures that are more readily available at scale as proxies for traits that are laborious to measure, an approach that was shown to boost statistical power of genetic discovery ([122]). Genetic research of consumer behavior can similarly benefit from such an approach. For example, a twin study found that one's disposition to display decision biases shares a common genetic variance with performance in the cognitive reflection test ([24]), suggesting that this brief measure could serve as a proxy for such behavioral dispositions. Another possible solution would be to preselect genetic loci that have already been identified as associated with related phenotypes in large-scale GWAS. This would drastically reduce the number of hypotheses to be tested and, thus, the sample size required to obtain sufficient statistical power.On a final note, the capacity (or lack thereof) to obtain detailed phenotypic measures at scale to complement the genetic measures may be less of a concern for behavioral marketing metrics. Companies in the DTC-GT industry possess relationship management data of millions of customers and likely know whether they were early adopters, responded to email advertisements, shared coupons with their friends, and consulted health or ancestry reports and, furthermore, what device was used to access them. Thus, large-scale genetic data sets that contain high-resolution measures of consumer behavior already exist and could be employed to unveil the genetic foundations of many aspects of consumer behavior. Such explorations will generate insights that advance not only the field of marketing, but also the discipline of behavioral genetics. Are Genes More Predictive Than Other Measures?Behavioral genetic research typically focuses on identifying variants that have causal effects on a target trait and quantifying the variance they account for. Many marketing applications of genetic data, however, do not depend on whether genetic variants are indeed causally related to a trait but, rather, on whether they are more informative than other readily available measures. These two questions are not interchangeable for two reasons. First, genomes correlate with many personal characteristics that have no genetic basis. In traditional genetic analysis, noncausal correlations are of no interest ([116]). For marketing applications, however, noncausal genetic associations carry information that is useful for identifying segments and reaching targets. Second, many behavioral dispositions can be accurately predicted from records of their downstream consequences (e.g., personality can be estimated from digital footprints; [75]; [102]). This empirical observation is not of particular interest to geneticists, yet it is crucial for marketers deciding on what data to base their strategy. As noted previously, the degree to which genomes are more predictive than other measures likely varies by trait. To the best of our knowledge, only one study to date (whose outcome measure was longevity) systematically compared the predictive accuracy of models that use different sets of variables ([69]). This constitutes an important gap that should be filled as genetic data sets become more available to marketing researchers. Extreme Ends of DistributionsMarketing applications, such as segmentation and targeting, often depend on identifying people at the extreme ends of a trait's distribution as opposed to explaining the variance in the general population. For example, a manager of an eco-friendly luxury car brand would be interested in reaching people who are willing to pay a lot for ""green"" products ([78]) rather than accounting for heterogeneity in this tendency in the general population. However, the goal of most behavioral genetic research to date has been to estimate how much of a trait's variance in the general population can be attributed to genetics (using summary statistics such as estimated heritability or R2). Future studies should shed light on the capacity to use genetic data for identifying segments at the extreme ends of the behavioral distribution, using techniques such as discriminant analysis. When We Do Not Have Genetic Data for EveryoneAs described in the ""Applications for Marketing Strategy"" section, there are several cases when a marketing researcher can use genetic data to accurately predict a variable of interest, denoted by y (e.g., propensity for pattern baldness). However, genetic information might not be available for the entire population of potential consumers. In such cases, it may be possible to leverage the share of the population with genetic information to predict y for the remaining nongenotyped population. To this end, researchers must first predict the variable of interest in the population using genetic information (e.g., using a genetic estimator y′, such as a PRS) and then estimate a model to capture the link between nongenetic variables (e.g., demographics) and the predicted variable of interest y′. Finally, the model can be used to predict the variable of interest in the nongenotyped population, without having to rely on genetic data. The feasibility of this approach crucially depends on the capacity to estimate y′, which is a function of the genome, from other observables. We are not aware of any research relying on this approach to date, and its potential performance remains to be studied. Answering this question is crucial for evaluating the utility of genetic variables as segmentation bases. How Will Consumers React?A final important open question concerns how consumers would feel about the use of their genetic data by marketers. On the one hand, it seems plausible that at least some people would welcome marketing applications of genetic data if it got them discounts or better recommendations, which saves search costs. On the other hand, such usage is expected to raise privacy concerns that are similar to those invoked in relation to other data types. Yet there are several additional unique matters, related to the image of genetics in the minds of consumers. One major concern relates to historical misconceptions surrounding genetics, which were used in the past to justify racist worldviews and policies responsible for some of the worst crimes against humanity ([72]). Business strategies that insensitively use consumers' genetic data might therefore invoke strong negative reactions. Furthermore, although the true causal effects of genetic factors on most human traits are moderate in size and occur via interactions with the environment, genetics is often associated with biological determinism ([30]). As such, the use of genetics for matching consumers with products, services, and ads might increase beliefs in the existence of potentially deterministic aspects of behavior ([13]; [155]) and threaten consumers' sense of autonomy ([145]). A final concern is that an individual's genome contains sensitive information that they may not be aware of, for example, about future health risks such as cancer or Parkinson's disease. Marketers should be cautious to avoid exposing consumers to information they might not want to know ([51]) or prefer to receive with the appropriate counseling in a medical setting.The substantial size of the DTC-GT market, despite poor regulation, suggests that these issues may not be a major concern for many customers. Moreover, many individuals voluntarily share their genetic data with third-party interpretation services ([57]) and websites that use them solely for making product recommendations. Indeed, in addition to ancestry-based playlists (Spotify) and cultural experiences (Airbnb), other services have recently emerged, recommending wines, travel destinations, and even romantic matches purportedly tailored to their consumers' DNA. Yet it is possible that the market trends merely reflect consumer ignorance. A recent survey found that while many DTC-GT customers presumed that they were sufficiently informed about privacy issues, their expectations were often inconsistent with company practices ([28]). For example, consumers' most common expectation, that DTC-GT companies would not share their data with third parties, was often at odds with the firms' actual privacy policies. Thus, it remains to be seen whether consumers' attitudes toward the use of genetic data for marketing differ from how they (dis)regard the use of other types of digital records, and whether there are means to mitigate such effects (e.g., by increasing transparency; [73]). ConclusionThis article is a first attempt to assess how the massive amounts of data accumulated in genetic databases will influence the field of marketing. We developed a framework that incorporates genetic variables into consumer behavior theory and used it to explore potential applications of genetic data in marketing. We further evaluated ethical and legal challenges, and we highlighted gaps of knowledge that should be addressed by future research. Despite the gaps of knowledge in the published literature, we note that DTC-GT firms and governments already have access to the genetic data of millions of individuals. Therefore, business strategies that employ genetic data are likely already implemented, to some degree, by organizations. With the fast accumulation of genetic data and the rapid advances in methodology for genetic-based inference, the use of genetic data for marketing research and practice is likely to become increasingly common in the future. Appendix Directions for Future Research Which specific genetic variants are linked to relevant marketing outcomes (e.g., customer relationship management measures)? To what extent is genetic data predictive of consumer behavior, above and beyond nongenetic variables traditionally used in marketing? Do genetic data allow identification of individuals at the extreme ends of distributions (e.g., heavy espresso drinkers)? To what degree can genetic variation be approximated from nongenetic measures traditionally used in marketing (e.g., geodemographics)? How will consumers react to the use and monetization of their genetic data by marketers?  "
18,"GMO Labeling Policy and Consumer Choice Most scientists claim that genetically modified organisms (GMOs) in foods are safe for human consumption and offer societal benefits such as better nutritional content. However, many consumers remain skeptical about their safety. Against this backdrop of diverging views, the authors investigate the impact of different GMO labeling policy regimes on the products consumers choose. Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on findings from four experiments, the authors show that consumer demand for GM foods depends on the labeling regime policy makers adopt. Both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labeling regimes reduce the market share of GM foods, with the reduction being greater in the latter case. GMO labels reduce the importance consumers place on price and enhance their willingness to pay for non-GM products. Results indicate that specific label design choices policy makers implement (in the form of color and style) also affect consumer responses to GM labeling. Consumer attitudes toward GMOs moderate this effect—consumers with neutral attitudes toward GMOs are influenced most significantly by the label design.Keywords: GMO; food claim; voluntary labeling; mandatory labeling; public policy and marketingAlthough the use of genetically modified (GM) foods has become widespread across the world, scientific and public opinions diverge about their safety. Most scientists agree that GM foods are invaluable because they offer increased nutritional content, a higher yield per acre, and a better shelf life ([51]). They also agree that GM foods are as safe for humans and the environment as non-GM foods ([18]). Yet some scientists disagree ([29]), citing concerns about possible long-term effects of GM foods on human health and the environment ([10]). On the demand side, many consumers question the safety of GM foods and their scientific promise ([30]). Indeed, a 2013 New York Times poll showed that 75% of Americans expressed concern about genetically modified organisms (GMOs) in their food, and most worried about their potential health effects. It is important to note here that the baseline consumer knowledge on this issue is low ([64]). The most extreme opponents of GM foods think they have the most knowledge about the issue, but research shows that their scientific literacy is low ([21]).The opposing views that firms and consumers have about GM foods create a fundamental tension in how such foods should be labeled, which is the central focus of this research. On the one hand, consumers and advocacy groups believe that GM foods are potentially risky; therefore, policy makers must mandate GMO labeling. In a mandatory labeling regime, food manufacturers are required to include labels such as ""contains GMO"" when their foods are GM. The most commonly used argument in support of such labeling is consumers' right to know. On the other hand, food manufacturers rely on scientific evidence to claim that GM foods are as safe as conventionally grown foods. As a result, they argue that mandatory labeling arbitrarily singles out GMO technology for specific attention and misleads consumers into thinking that they should be concerned about consuming GM foods ([60]). Therefore, food manufacturers support a voluntary GMO labeling policy, where firms have the freedom to use a ""non-GMO"" label when appropriate.The discordant views about the safety of GM foods between firms and consumers, as well as the demands for GMO labeling by consumer advocacy groups (e.g., the Non-GMO Project) create a substantial challenge for policy makers in their efforts to develop a GMO labeling policy. As a result, GMO labels vary a great deal around the world (see Figure 1). For example, the United States allows firms to display non-GMO labels on their products if they wish. Brazil, the world's second-largest GM producer after the United States, adopted a mandatory GM label that features a black T inside a yellow triangle. The letter T stands for the Portuguese word transgenicos (""transgenics""), and the symbol resembles a caution sign indicating an upcoming T-junction ([ 6]). Similar logos have been adopted by South American countries such as Bolivia and Uruguay.Graph: Figure 1. GMO labels.In light of the diverse GMO policy regimes that currently exist, an important prerequisite for carefully constructing a GMO labeling policy is a theory-based understanding of whether and how consumers shift their choices under the different GMO labeling regimes. The intention behind a labeling policy that requires the disclosure of a GMO ingredient as a horizontally differentiated attribute is that it simply allows consumers to make choices that reflect their taste differences. However, an externality of such a policy may be that it leads consumers to treat GMO ingredients as a vertically differentiated attribute, signaling that non-GM foods are of a higher quality than GM foods.To investigate the product quality–related implications empirically, we examine how the different GMO labeling policy regimes impact consumers' choice and their willingness to pay for non-GM products as well as the market shares of GM and non-GM products. Guided by the policy question of mandatory versus voluntary labeling for GM foods, we investigate the substantial impact of different GMO policy regimes on choices consumers make. More specifically, the purpose of our research is to answer the following research questions that have significant GMO labeling policy implications. Using the theory-driven terminology adopted by [ 1], in the remainder of the article we refer to the mandatory labeling regime as ""presence-focused"" (contains GMO) and the voluntary labeling regime as ""absence-focused"" (non-GMO). Does the labeling policy (absence-focused vs. presence-focused) affect a consumer's choice of GM products? Does the labeling policy (absence-focused vs. presence-focused) affect other aspects of the consumer's choice process, such as their price sensitivity and willingness to shop in a category? Is the consumer's choice impacted according to whether the GMO related information disclosure is complete (presence-focused and absence-focused) or partial (presence-focused or absence-focused)? Complete GMO information disclosure occurs when policy makers mandate presence-focused labeling and firms that produce non-GM products display an absence-focused label. Partial disclosure occurs when either presence or absence labels are present. Do consumers behave differently depending on the GM label's presentation format (e.g., color, theme)? Which consumers are most likely to alter choices because of the label format?To answer these key policy-related questions, we combine insights from the social psychology literature with rigorous consumer choice models to make novel predictions about the effect of GMO labeling changes on consumers' demand for GM products. We develop our theory based on the literature on negativity bias (e.g., [31]), structural alignment theory (e.g., [25]), and message presentation (e.g., [33]). We use choice experiments grounded in microeconomic theory ([40]) and a hierarchical Bayes model to test our hypotheses.Across four studies involving 3,913 respondents, we study the impact of different GMO labeling regimes on demand for GM products. Our first between-subjects experiment (Study 1) examines whether consumer choice depends on the GMO labeling regime (i.e., no labeling, absence-focused, presence-focused, or both labeling conditions). Study 2 investigates how the GMO labeling regime may impact the importance consumers place on product price and product category. Study 3 shows how the alignability of GMO information, whether partial information or full information is disclosed, affects consumer choice. Finally, Study 4 investigates how the signal used in a GMO label (e.g., color) can impact consumer demand for GM foods and reveals which market segment is most likely to be affected by the signal used.Our findings have substantive implications for two key stakeholder groups: policy makers and food manufacturers. By quantifying the effects of various GMO labeling regimes, we offer policy makers guidance on the impact of each labeling system on consumer demand. Absence-focused policies result in the smallest change in demand for GM products compared with a regime with no GM labels. Presence-focused labeling policies can substantially alter demand for GM products, and the signal contained in the GMO logo (e.g., color) also plays a critical role in consumers' perceptions of GM products. Both policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to incentivize such firm behavior.For food manufacturers, our research reveals that GM labels add an important product feature for consumers to evaluate. Such labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snack). All of the aforementioned effects are amplified when moving from an absence-focused to presence-focused regime. Background GMOs: What Science SaysThe [63], p. 1) defines GMOs as ""organisms (i.e., plants, animals, or microorganisms) in which the genetic material (DNA) has been altered in a way that does not occur naturally by mating and/or natural recombination."" Proponents of GM crops argue that they increase yield, lower food prices, reduce damage to crops after the harvest, make crops tolerant of stresses such as cold and heat, help fight malnourishment, and reduce reliance on chemical pesticides ([51]). Most scientists claim that there is no substantiated evidence that genetic crop modification makes foods less safe. For example, the National Academies of Sciences and Medicine ([46]) reported that food from GM crops is no more dangerous than food produced by conventional agriculture. More than 150 Nobel laureates in areas such as chemistry, physics, and medicine signed an open letter in 2016 to endorse the safety of GM foods, noting that ""opposition based on emotion and dogma contradicted by data must be stopped"" (Support Precision Agriculture 2016).Although the dominant view among scientific organizations is that GMOs do not harm human health, this view is not ubiquitous. In one review article, [36] noted that a group of scientists believe that each GM product should be tested over long periods for possible side effects. The author reviewed 26 animal feeding studies that identified adverse effects or animal health uncertainties, leading him to conclude that ""putative consensus about the inherent safety of transgenic crops is premature"" (p. 909). A joint statement by a group of researchers ([29]) claimed that no consensus on GM food safety exists. They indicated that a conflict of interest exists in many reported studies supporting GM food because biotechnology companies often fund this research ([15]). They further noted that no epidemiological studies have examined the effects of GM food consumption on humans. They concluded that it is necessary to test the effect of GM foods on humans and over longer periods. GMOs: What Consumers ThinkSeveral studies have documented consumers' lack of knowledge about GMOs, as noted by [64] in their review. These studies also document an overall negative attitude toward GMOs among consumers. Such negative attitudes could be driven by negative press associated with occurrences such as GM crops causing a decline in monarch butterflies, which a recent article refutes ([ 7]). The primary concerns are that growing and consuming GM crops may cause health problems and allergic reactions.Research has shown that the most extreme opponents of GM foods know the least about GMOs but think that they know the most ([21]). People's misplaced confidence stemming from the mismatch between what they think they know about science and what they actually know ([45]) polarizes attitudes even more ([22]).The controversy around GM foods also relates to the growing literature on science denial ([54]) that identifies social mechanisms as the basis for extreme confidence in beliefs that contradict scientific consensus ([33]). Specifically, many people have insufficient information to establish their own opinions on new technologies and scientific developments ([20]) and instead accept the opinions of people they trust ([54]). Well-known examples of science denial include vaccine safety, global warming and climate change, the rise in antibiotic resistance, and the safety of GM foods. GMOs: What Policy Makers DoGMO labeling policy in the United States was absence-focused when GM foods were first released in 1994. Some food companies use third-party verification, such as the Non-GMO Project (https://www.nongmoproject.org), to highlight the non-GMO aspect of their products. However, various consumer groups and nongovernmental organizations have argued for presence-focused labeling based on consumers' right to know what is in their food. They contend that the potential harm of GM foods needs to be made explicit.Over the years, political pressure to introduce presence-focused GMO labeling in the United States has grown. In July 2016, U.S. Congress passed a bill requiring the U.S. Department of Agriculture to establish a national disclosure standard for GMOs. The new policy has a two-year phase-in period that began in January 2020. The proposed label under this policy has a nature-friendly theme on a green or black-and-white background and uses the term ""bioengineered (BE)."" Dozens of nations around the world have enacted presence-focused GMO labels based on the percentage of GMOs in ingredients or how the seed was developed. The GMO percentage thresholds vary among countries that have regulations. For example, the European Union (EU) and United Kingdom set this limit at.9%, whereas Australia set it at 1% ([31]).Policy makers in many countries are uncertain whether GM foods are safe, and their labeling rules are based on such a perspective. For example, the EU has adopted the precautionary principle (European Commission [19]) for GMO labeling. This principle is often cited in cases of scientific uncertainty and the possibility of irreversible damage. It states that ""where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation"" ([49], Principle 15).Not surprisingly, GMO labeling policies are controversial. In a compelling counterargument to EU policies, [59] acknowledged the rationality behind the precautionary principle but cautioned that rigid regulatory controls based on the idea of ""possible risk"" can paralyze progress. He explained that, for GMOs, the precautionary principle results in substitute risks because it interferes with the promise of mitigating hunger and disease in developing countries by using foods such as golden rice, which is bioengineered to be rich in vitamin A.In the United States, debate over the recently adopted logo is intense because it appears to signal the government's positive attitude toward GM foods. In Brazil, opponents of the current mandatory presence-focused GM logos accuse the country's policy makers of scaremongering. In Canada, which currently has no GMO labeling legislation, petitions have been circulated supporting mandatory presence-focused GMO labeling; in a survey of adults living in Canada, 90% of the respondents expressed support for mandatory GM labeling ([56]). PredictionsIn this section, we develop predictions on how GMO labeling affects different aspects of consumer choice, including preference for GM foods, price sensitivity, and product category purchase. We also outline how these predicted effects are moderated by the GMO label format. Figure 2 summarizes our main hypotheses.Graph: Figure 2. Overview of predictions. GMO Labeling and Consumer ChoiceIn an absence-focused labeling policy, manufacturers may use a ""non-GMO"" label when appropriate. In a world in which many consumers have negative attitudes toward GM products, non-GMO producers often use absence-focused claims (e.g., a TV ad for Triscuit[ 5]). Such claims are in line with those that highlight the absence of negatives—namely, no preservatives, no artificial colors, no chemicals, and so on. These nature-based claims that remove a negative strongly affect consumers' inferences about product's taste and healthfulness ([ 1]), even when they are irrelevant to the actual quality. In contrast, in a presence-focused GMO labeling policy, regulators mandate that GM-food products display a ""contains GMO"" label. For many consumers who have negative attitudes toward GM products, this information signals potential risk.The absence- and presence-focused labeling policies outlined thus far may impact consumers' evaluations of GM foods via two separate mechanisms—information valence and information source. With regard to information valence, it is well known that people place greater weight on negative information than positive information. This negativity bias ([50]) is at the core of how consumers may evaluate a GMO label. From an evolutionary perspective, this bias occurs because we have a greater chance of surviving and thriving if we pay greater attention to negative information; negative events are more consequential than positive ones. Some argue that negative information is more informative because it is rarer ([24]), attracting more attention and thus being more ""diagnostic or informative"" ([53]). Previous research has documented negativity bias in a variety of contexts. For example, negative attributes are more diagnostic of product quality than positive ones ([28]), and negative reviews have a stronger effect on purchase decisions ([ 9]).In addition, absence- and presence-focused labeling differs by their information source. In presence-focused GMO labeling, a regulatory body mandates the display of GMO labels, whereas in absence-focused labeling, this decision is voluntary and made by the firm. The perceived credibility of a message's source can affect the recipient's cognitive response ([58]). For trusted information sources, consumers accept a message without undertaking an extensive assessment of its content ([23]). Evidence suggests that consumers trust public sources (e.g., a government) more than private ones (e.g., a firm). For example, [17] found that advertising from a government source (the Federal Trade Commission) is more credible than that from a firm. Similar results have been noted for safety hazard information ([38]), environmental information ([11]), and forest-product certification seals ([48]). Given the differences in information valence and source between the two labeling regimes discussed thus far, we propose the following hypothesis: H1:  Presence-focused GMO labeling makes consumers more sensitive to the GMO attribute when choosing a product than absence-focused GMO labeling. GMO Labeling and Price SensitivityPrevious studies have shown that negative information is more diagnostic and, as a result, attracts more attention ([53]). In line with the argument that attention is a scarce resource, [12] demonstrated that greater attention to a previously unconsidered attribute reduces the relative importance of other attributes. Extending these theoretical findings to our research context, greater attention to GMO ingredients likely diverts consumer attention away from other product-related information, such as price. Negative presence-focused GMO labeling should reduce consumers' focus on price information more compared with absence-focused GMO labeling. Thus, H2:  Presence-focused GMO labeling makes consumers less price sensitive than absence-focused labeling. GMO Labeling and Product Category PurchaseChoice deferral is a means of mitigating the negativity generated in uncertain or difficult choice contexts. Previous research shows that this negative feeling in such contexts increases the likelihood that consumers will defer their decision ([39]). Deferral occurs when no single option dominates a choice set or when consumers face difficult trade-offs between product attributes ([13]).In our context, consider a Brand A, which contains GMOs, and a Brand B, which does not. Assume that a consumer prefers Brand A but prefers non-GMO ingredients. In the absence-focused condition, this consumer will choose between Brand A, not knowing whether it is a GM product, and Brand B, fully aware that it is a non-GM product because of the ""non-GMO"" label. Conversely, in the presence-focused condition, the same consumer will choose between Brand A, fully aware that it is a GM product because of the ""contains GMO"" label and Brand B, with no GMO-related knowledge. The trade-off between brand name and GM ingredients may be more difficult in the presence-focused condition because the consumer has to analyze the costs and benefits of a brand they prefer (Brand A) and an attribute they do not (GMO). This increased task difficulty may enhance the likelihood of choice deferral.[42] show that consumers tend to view a government's default option as an implicitly recommended course of action. In their studies, when the government uses the ""organ donor"" default, most participants inferred that ( 1) the policy makers were willing to be donors and ( 2) people ought to be donors. In the GMO labeling context, a GMO label mandated by a regulatory body may send a negative signal that consumers should avoid a product with GMO ingredients. Growing concerns and perceived risks associated with GMOs could increase customers' uncertainty about brand quality, thus leading to choice deferral. Therefore, H3:  Presence-focused GMO labeling makes consumers less likely to purchase the relevant product category than absence-focused GMO labeling. GMO Labeling Alignment and Consumer ChoiceThe first three hypotheses focus on scenarios where the product alternatives available apply either an absence-focused label or a presence-focused label. However, when a government mandates presence-focused labeling, firms that produce non-GM products may be free to use absence-focused labeling, as is the case in the United States today. Because many consumers view non-GM products favorably, firms offering non-GM products have a strong incentive to include such information on their product packaging to differentiate themselves from firms offering GM products. Therefore, when a mandatory GMO labeling policy is implemented in the marketplace, it is plausible that most—if not all—products will display either GMO or non-GMO labels. We use structural alignment theory ([35]; [55]; [65]) to discuss the impact of partial or complete GMO-related information on consumer choice and how they drive our predictions.Consider the following example involving two marinara sauce brands, A and B. Brand A is sold at $2.00, without providing any information on GMO attributes; Brand B is sold at $2.50 and includes a non-GMO label. In this example, the price is alignable information because the attribute is present in both options. In contrast, under either the absence-focused or presence-focused labeling, GMO-related information is only available for Brand B, making it nonalignable. The structural alignment literature suggests that consumers pay more attention to alignable attributes ([25]) and put greater weight on them ([55]).Consistent with this discussion, when both types of GMO labels are included (i.e., ""non-GMO"" and ""contains GMO""), consumers will give greater weight to the GMO attribute. According to the arguments used previously for H2, giving greater weight to the GMO attribute would ( 1) further reduce the weight consumers give to price information and ( 2) make them more reluctant to purchase a product in the category. Formally, H4:  Compared with a situation where only presence-focused (""contains GMO"") labels are displayed, when both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labels are displayed, consumers become (a) more sensitive to GMOs, (b) less sensitive to price, and (c) less likely to purchase in the product category. GMO Labeling Format and Consumer ChoiceA regulatory body's choice of GMO label reveals its beliefs or attitudes about GMOs and is, therefore, a critical policy decision—consumers tend to view a government's default option as an implicit recommended course of action ([42]). Moreover, [52] showed that a speaker's description signals their attitude toward an object. For example, if someone likes a team, they describe its successes, and if they do not, they note the team's failures. The descriptions a speaker chooses, even of seemingly equivalent objects, are important for listeners ([41]).As a concrete example involving the color of a GMO label, consumers tend to infer that a product has positive, nature-related attributes when it prominently displays the color green ([61]). Similarly, the color blue signals openness, peace, and tranquility ([43]), whereas yellow signals caution. Such color choices and their associated signals are highly relevant for GMO labeling. We hypothesize that policy makers' choice of a GMO label (e.g., the color green, blue, or yellow) is important as it delivers an implicit recommendation that may influence consumers' choices. H5:  The graphical format of the label determines how much impact the GMO attribute has on consumer choice, including (a) how sensitive consumers are to the GMO attribute, (b) how important price is to consumers, and (c) how likely consumers are to purchase in a given product category.Consumers' prior beliefs about GMOs could also play a role in how much a labeling policy impacts them. Previous research showed that most individuals do not know enough details to establish their own perspectives on new technologies and scientific developments ([20]) and accept the position of others they trust ([54]). As a result, we anticipate that consumers in the middle, who neither like nor dislike GM products, are affected the most by the label format policy makers select. Overview of StudiesWe include four empirical studies. The first study uses a simple between-subjects design to examine the effect of different GMO labeling policies (i.e., absence, presence, or both) on consumer choice. We subsequently conduct three choice-based conjoint studies to test H1–H5. Study 2 focuses on H1, H2, and H3 by disentangling the impact of GMO labeling on different aspects of consumer choice. Study 3 tests H4, focusing on how the findings pertaining to H1–H3 are affected by GMO information disclosure (partial vs. full). Lastly, Study 4 tests H5, focusing on how the different graphical formats of GMO labeling impact our previous findings. Study 1The goal of Study 1 was to demonstrate that different GMO labeling regimes (i.e., no GMO labeling, absence-focused labeling, presence-focused labeling, and both labeling conditions) can lead to systematic differences in demand for GM foods. Procedures and ParticipantsUsing Amazon Mechanical Turk (MTurk), we recruited respondents in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal categories (marinara sauce and pickle) and whether they were paying attention to the study instructions. Of the 2,110 respondents who completed this first step of the study, 767 (36.4%) respondents did not qualify to continue because they did not shop in the two focal categories (N = 644; 30.5%) or failed to correctly answer the attention check questions (N = 123; 5.8%). A total of 1,343 respondents (Mage = 41.0 years; female = 62%) qualified to participate in the main study and completed it. We randomly assigned these respondents to one of the four study conditions in a between-subjects design (Ncontrol = 340, Nabsence = 331, Npresence = 335, Nboth = 337).We presented respondents with choice sets in two different product categories (marinara sauce and pickles) and asked them to select their preferred brand. We selected these two product categories because ( 1) they are frequently purchased and ( 2) they complement the less healthy product category (potato chips) that we use in our subsequent studies.In the marinara sauce category, the first two (Prego and Newman's Own) of the three brands included in the study were GM products while the third (Barilla) was a non-GM product. Similarly, in the pickles category, two brands (Claussen and Vlasic) were GM products and the third (Mt. Olive) was a non-GM product. In both categories, we gave respondents in the control condition no information on GMOs. In the absence labeling condition, only the non-GMO label was displayed. In the presence labeling condition, only the GMO label was displayed. Finally, in the both labeling condition, both non-GMO and GMO labels were displayed. Figure 3 shows the four GMO labeling conditions for the pickles category in this study. The stimuli for the marinara sauce category are included in the Web Appendix A (Figure W1).Graph: Figure 3. Stimuli for four labeling conditions (pickles). ResultsFigure 4 shows the share of non-GM and GM products in each category across the four labeling conditions. We found that the GMO labeling regime significantly impacts consumer choice of the brands included. In all three GMO labeling conditions (absence, presence, and both), more participants preferred the non-GM product than in the control condition, where no GMO-related information is displayed. The magnitude of these shifts in demand toward the non-GM product (Barilla or Mt. Olive) depended on the GMO labeling condition.Graph: Figure 4. Non-GMO vs GMO choice share across different labeling conditions.In the marinara sauce category (Figure 4, Panel A), the choice share of non-GM product was lower in the control condition (  π^control   = .171) than in the absence (  π^absence   = .236; z = −2.094, p = .046), presence (  π^presence   = .370; z = −5.825, p < .001), and both labeling conditions (  π^both   = .407; z = −6.780, p < .001). In addition, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .236) than in the presence (  π^presence   = .370; z = −3.761, p < .001) and the both (  π^both   = .407; z = −4.730, p =   .001  ) labeling conditions. These results imply that consumer preference for non-GM products increases progressively from control to absence to presence labeling conditions. The difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .370 vs.  π^both   = .407; z = −.984, p =   .374  ).Graph: Figure 5. Example choice tasks in study 2.The findings for the pickles category (Figure 4, Panel B) are largely consistent with those for marinara sauce. The choice share of the non-GM product was lower in the control condition (  π^control   = .229) than in the absence (  π^absence   = .293; z = −1.888, p =   .074  ), presence (  π^presence   = .421; z = −5.327, p < .001), and both (  π^both   = .469; z = −6.552, p < .001) labeling conditions. As in the marinara sauce category, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .293) than in the presence (  π^presence   = .421; z = −3.446, p < .001) and both (  π^both   = .469; z = −4.685, p < .001) labeling conditions. Once again, the difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .421 vs.  π^both   = .469; z = −1.252, p =   .241  ).Study 1 supports our central thesis—namely, that the way the GMO message is conveyed to consumers affects their choices. In all GMO labeling regimes, the share of non-GM products is greater than when no GMO-related information is revealed. The share of non-GM products increases progressively from control to absence to presence labeling conditions. Although these findings are interesting on their own, Study 1 raised important follow-up research questions. For example, should we expect GMO labeling to affect the importance of price to consumers and their willingness to pay (WTP) for a non-GM product? Does the manner in which the GMO message is delivered affect the product choice consumers make? Furthermore, Study 1 confounds brand and GM ingredients, as only the Barilla brand carried the GMO-free label and only Prego and Newman's Own displayed the ""contains GMO"" label in the marinara sauce category. To answer our next set of research questions and remove the confounding between brand and GMO ingredients, we conducted three choice-based conjoint experiments, which we report on next. Study 2The purpose of Study 2 was twofold. First, we test H1, H2, and H3, which examine the impact of GMO labeling (absence vs. presence) on consumers' sensitivity to the GMO attribute, price, and category purchase, respectively. Second, we explore the behavioral processes associated with consumer choice under the different GMO labeling regimes. Unlike Study 1, we do not have a confound between brand and GMO ingredients in this study. Procedures and ParticipantsTo introduce Study 2, we used incentive alignment instructions similar to those employed by [27], informing respondents that 25 of them would receive a total value of $5 based on their answers to the survey: either a product of their choice and a Walmart e-gift card for the remaining value, or a $5 Walmart e-gift card. Existing literature shows that incentive-alignment techniques make consumers likely to provide more realistic responses. When a study offers as an incentive a version of the product that is predicted to give consumers the highest utility, respondents put greater effort into the choice task, and their responses are more likely to reflect their actual preferences ([14]; [27]).We informed respondents that the study's purpose was to understand how they evaluated potato chips. We asked the respondents to complete 14 choice tasks. In each choice task, we showed the respondents four brands of potato chips: Lay's, Herr's, Ruffles, and a private label. The price varied by brand. As in the marketplace, we set the prices of the national brands ($2.79, $3.29, $3.79) and the private brand ($1.99, $2.39, $2.79) at different levels. The two GMO label conditions were included or not included on the package. We displayed all four brands in each choice set, varying the GMO ingredients and prices from one scenario to the next in the 14 choice tasks.We used a statistically efficient choice design based on the D-optimality criterion ([37]) for the main-effects model that included three attributes varied orthogonally across choice tasks, which prevented us from testing higher-order interactions. For example, Lay's chips could have a GMO label in one task but not in the next; by design, the brand and GMO label attribute are unconfounded, enabling us to assess consumer preference for each. For more information on conjoint analysis and how it relates to our study, please see Web Appendix B.We adopted a dual-response method, asking respondents to indicate ( 1) which of the product alternatives they prefer and ( 2) whether they would actually buy the product they had just selected ([ 4]; [27]). The dual-response method has the advantage of encouraging respondents to slow down to think through the purchase task, making the no-choice option more likely, which is similar to a real market situation. Next, respondents answered questions about their attitudes toward GMOs, which enabled us to investigate how consumer attitudes vary across the different labeling regimes. We used a nine-point ""disagree/agree"" scale to ask questions regarding attention to the GMO ingredient, risk perception of the GMO ingredient, and decision uncertainty.We randomly assigned respondents to one of the two conditions (absence- vs. presence-focused labeling) in a between-subjects design. These two between-subjects conditions differed by how the GMO label was displayed. In the absence-focused condition, only the non-GMO label was displayed, whereas in the presence-focused condition, only the GMO label was displayed. Figure 5 presents a screenshot of one of the 14 choice tasks for each condition in Study 2.We recruited students from a Midwestern university in the United States, and they participated in the study in exchange for course credit. To begin, we asked the students questions to assess whether they shopped in the focal category (potato chips) and whether they had a dietary restriction (gluten intolerance). Of the 665 students who completed this first step of the study, 55 (8.3%) respondents did not qualify to continue because they did not shop in the focal category (N = 49; 7.4%) or were gluten-intolerant (N = 6;.9%).[ 6] A total of 610 students (Mage = 19.7 years; female = 44.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nabsence = 303, Npresence = 307). Results Model-free resultsThe proportion of respondents purchasing a non-GM product was higher in the presence condition (56.38%) than in the absence condition (48.84%). The proportion of respondents who decided not to make a purchase was also higher in the presence condition (37.06%) than in the absence condition (32.77%). The average purchase price was similar in the two conditions ($3.07 in absence vs. $3.05 in presence). Statistical modelThis study focused on the impact of GMO labeling on brand and category choice. Given this, we modeled two decisions—whether to buy and which brand to choose—using a nested framework to model brand choice and purchase incidence ([ 2]; [26]), where the joint probability of a given consumer choosing brand j and buying it (B) is given by Pr(j,B)=Pr(j)×Pr(B|j). Graph( 1)We conceptualized our model at the brand level and assumed that individual h evaluating j (j = 1, ..., J) brands chooses the brand j. Each brand j has a design vector xj that contains discrete variables to indicate the different attribute levels (e.g., GMO) and a continuous variable (e.g., price). The deterministic part of individual h's utility for brand j is linear in the predictor variables (xjβh) and, with a Type I extreme value error structure, yields a multinomial logit model [40]. The probability of individual h choosing brand j is given by Pr(yhj′=1)=exp(x′j′βh)∑Jj=1exp(x′jβh), Graph( 2)where βh is an individual-level parameter vector that includes brand preference and sensitivity to attributes such as GMO and price. To model the buy/no-buy decision, we specified a threshold utility (γh) parameter in the model. The utility of the most attractive alternative (j) in the choice set must exceed γh for the individual to buy it (B = 1). A larger estimated threshold parameter implies a lower probability of buying in the category.We introduced heterogeneity across individuals hierarchically with a random effects specification ([ 4]) as  θh∼N(θ¯,Vθ)  , where  θh={βh,γh}  and  θ¯={β¯,γ¯}  . In our empirical context, the hyperparameter  β¯  includes the average brand preference parameters, GMO sensitivity (  β¯GMO  ), and price sensitivity (  β¯price  ), while  γ¯  is the average threshold for category purchase. For the remaining technical details of the model, refer to Web Appendix C. Model-based inferenceWe employed the Markov chain Monte Carlo (MCMC) method to estimate the hierarchical Bayes model. Similar to others in the literature ([ 2]; [ 3]; [ 5]; [16]), we estimate the model for each experimental condition separately (i.e.,  θhabsence∼N(θ¯absence,Vθabsence)  and  θhpresence∼N(θ¯presence,Vθpresence)  ) and use Bayesian inference to test for differences in estimates between the experimental conditions. To obtain a one-sided p-value, we calculated the fraction of the empirical posterior distribution that is inconsistent with the formulated hypothesis. For example, to test H1, we calculated the proportion of the GMO sensitivity distribution that is inconsistent with H1 (i.e.,  β¯GMOpresence−β¯GMOabsence>0  ). To claim statistical significance, we report a two-sided Bayesian p-value, which equals two times the one-sided p-value ([62]). We used similar analyses to test the remaining hypotheses.Table 1 shows the findings of Study 2 based on the model in Equations 1 and 2. This table contains the posterior means and standard deviations of the hyperparameter estimates (  β¯  ,  γ¯  ) for each experimental condition. The first three rows correspond to the  β¯Lay′s  ,  β¯Herr′s  , and  β¯Ruffles  , which are the posterior means of preference for each brand (relative to the private label). The next two rows report  β¯GMO  and  β¯price  , the posterior means of sensitivities associated with the GMO and price attribute, respectively. The last row reports the average threshold parameter  (γ¯)  estimate.GraphTable 1. Study 2 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceLay's6.114.19(.45)(.33)Herr's−1.95−1.49(.94)(.54)Ruffles6.523.55(.53)(.34)GMO.00−1.12(.11)(.12)Price−4.31−2.25(.27)(.14)Category threshold−6.49−2.15(.89)(.50) 1 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.A negative parameter estimate means that, on average, consumers like this product characteristic less than the baseline. For example, a negative estimate for GMO (  β¯GMO  ) implies that, on average, customers prefer a non-GM product over a GM product. In our studies, we can compare this parameter estimate across the GMO labeling conditions. For example, if the parameter estimate of the GMO attribute in the presence-focused condition (  β¯GMOpresence  ) is more negative than in the absence-focused condition (  β¯GMOabsence  ), we interpret this as showing that presence-focused GMO labeling amplifies consumer preference for non-GM over GM products.The impact of GMO ingredients on product choice was not statistically significant in the absence-focused condition, but was negative and statistically significant in the presence-focused condition (  β¯GMOabsence   = .00 vs.  β¯GMOpresence   = −1.12; p < .001). This finding supports H1. The parameter  β¯price  shows how sensitive consumers are to price changes. The more negative it is, the more consumers are sensitive to price changes. We find that the price coefficient is more negative in the absence condition than in the presence condition (  β¯priceabsence   =   −4.31  vs.  β¯pricepresence   =   −2.25  ; p < .001); this implies that participants were less sensitive to price changes under the presence-focused labeling, thereby supporting H2.To assess consumers' decision to buy in the product category or not,  γ¯  captures the average threshold value for product category purchase. The product category is purchased when the utility of at least one product exceeds this threshold; therefore, the higher the threshold parameter  γ¯  , the lower the probability of the category purchase. The category purchase threshold was higher in the presence condition than in the absence condition (  γ¯absence   = −6.49 vs.  γ¯presence   = −2.15; p < .001), indicating that consumers were less likely to purchase in the potato chips category in the presence-focused condition. This result supports H3.In summary, Study 2's findings support H1, H2, and H3 in an incentive-aligned conjoint experiment. The main takeaways thus far are that the GMO labeling regime (absence vs. presence) affects how sensitive consumers are to the GMO attribute. The labeling also impacts consumers' price sensitivity and their likelihood of making a purchase in a product category.The second aim of Study 2 was to understand why consumers' choices differ between the two GMO labeling conditions. We examined whether GMO risk perception differs by labeling format and, if so, whether it affects the extent to which consumers pay attention to the GMO information. In each condition, we asked the respondents a series of questions pertaining to GMOs (e.g., concerns, perceived risk, attention paid). We then asked questions relevant to choice deferral. To measure choice task difficulty, we asked, ""Was it difficult to decide which product to pick?"" ([32]). To measure choice task uncertainty, we asked, ""How certain were you that the product would be the best in each choice task?"" and ""How much did you regret choosing the product you picked in each choice task?"" ([34]). We used a nine-point scale (1 = ""not at all,"" and 9 = ""extremely"") for all of the questions asked.Figure 6 shows that consumers have greater concern about GMO products (Mabsence = 1.92 vs. Mpresence = 3.36; p < .001) and a greater perceived risk from them (Mabsence  = 2.08 vs. Mpresence = 3.65; p < .001) in the presence condition than in the absence condition. Consumers paid greater attention to GMO products (Mabsence  = 2.67 vs. Mpresence = 4.01; p < .001). These results confirm two things. First, presence labeling makes consumers experience more negative feelings (greater concern and risk perception) about GMOs. Second, as documented in the negativity bias literature, these negative perceptions result in consumers paying greater attention to the GMO attribute.Graph: Figure 6. GMO labeling: perception, attention, and choice difficulty.Regarding the questions pertaining to choice deferral, consumers in the presence labeling condition found the choice tasks to be more difficult (Mabsence = 2.52 vs. Mpresence = 3.20; p < .001). Consumers were less certain about their choice (Mabsence  = 6.12 vs. Mpresence = 5.70; p = .009), which is consistent with the choice deferral findings we reported previously. Consumers also experienced greater regret about the choices they made (Mabsence = 2.32 vs. Mpresence = 2.91; p < .001). Quantifying the impact of labeling changesA useful way to quantify the impact of the two labeling regimes is to examine consumers' WTP for the non-GMO attribute by condition. The basic assumption in a choice model is that respondents consider trade-offs between attributes when they select an alternative. Therefore, one can estimate each attribute's marginal utility from the parameter estimates and obtain the WTP for an attribute as a ratio of its marginal utility and the price coefficient ([47]; [57]). For the GMO attribute, WTP is measured by evaluating the quantity  −β¯GMO/β¯price  . We determined that the WTP for the non-GMO attribute was much higher (p < .001) in the presence condition ($.50, SD = .07) than in the absence condition ($.00, SD = .03).In summary, Study 2 shows that presence-focused labeling makes consumers ( 1) more sensitive toward the GMO attribute, ( 2) less sensitive toward price information, and ( 3) more reluctant to make a purchase in a category. Thus, Study 2 supports H1, H2, and H3. Presence-focused labeling enhances consumers' concerns about GMOs, encourages them to pay greater attention to GMO information, and makes their choice decision more difficult. In a presence-focused labeling regime, firms producing non-GM products could benefit from increased WTP for the non-GMO attribute, providing them with an incentive to offer more non-GM products and to charge more for them. Study 3The objective of Study 3 was to investigate the effect of aligned GMO information on consumer choice, as predicted by H4. To this end, we added a third condition (both) whereby all product alternatives display either the absence or the presence of GMO information. Procedures and ParticipantsThe structure of Study 3 was similar to that of Study 2, except for the following changes. First, we used different absence- and presence-labeling stimuli from those in the previous two studies (see Figure 7). Second, we did not use incentive alignment. Third, as previously noted, we added a labeling condition (both) in which the ""non-GMO"" label appeared on products that did not contain GMO ingredients and the ""GMO"" label appeared on products that did.Graph: Figure 7. Visual descriptions of the stimuli in study 3.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category for this study (potato chips) and whether they were paying attention to the study instructions. Of the 925 respondents who completed this first step of the study, 43 (4.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 17; 1.8%) or failed to correctly answer the attention check questions (N = 26; 2.8%). A total of 882 respondents (Mage = 38.3 years; female = 51.5%) qualified for the main study and completed it. We randomly assigned them to one of the three study conditions (Nabsence = 290, Npresence = 299, Nboth = 293). Results Model-free resultsAs in Study 2, the proportion of participants choosing the non-GM products was higher in the presence (vs. absence) labeling condition. In addition, more participants chose the non-GM product in the both labeling condition than in the other two conditions (51.56% in absence, 54.98% in presence, 58.78% in both). The proportion of participants deciding not to purchase the product category was lowest in the absence labeling condition (10.05% in absence, 13.93% in presence, 13.24% in both). The average purchase price was slightly higher in the both (vs. absence) labeling condition ($2.93 in absence, $2.95 in presence, $2.98 in both). Model-based inferenceTable 2 summarizes the findings from Study 3 based on the model previously outlined in Equations 1 and 2. We began by investigating whether the findings in Study 2 also held for Study 3. As before, we found that the impact of GMO ingredients was stronger in the presence labeling condition (  β¯GMOabsence   = −.41 vs.  β¯GMOpresence   = −1.21; p < .001). The price coefficient was more negative in the absence labeling condition (  β¯priceabsence   = −4.47 vs.  β¯pricepresence   = −3.54; p = .032), implying that consumers were less price sensitive in the presence (vs. absence) labeling condition. The threshold parameter was higher in the presence labeling condition (  γ¯absence   = −12.39 vs.  γ¯presence   = −9.04; p = .018), meaning that consumers were less likely to purchase the product category in the presence condition. Overall, Study 3's findings also support H1, H2, and H3.GraphTable 2. Study 3 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceBothLay's3.994.093.90(.44)(.37)(.43)Herr's1.941.47.30(.60)(.43)(.54)Ruffles4.783.573.59(.48)(.38)(.45)GMO−.41−1.21−1.93(.13)(.18)(.23)Price−4.47−3.54−2.85(.33)(.26)(.21)Category threshold−12.39−9.04−7.28(1.21)(.90)(.80) 2 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.Returning to the primary purpose of Study 3—to investigate how GMO-related information alignment affects consumers' choice behaviors—the comparison of parameter estimates between presence and both labeling conditions revealed an interesting pattern. We found that the parameter estimates for GMO, price, and category incidence were higher in the both (vs. presence) labeling condition. Respondents became more sensitive to GMO ingredients when both non-GMO and GMO labels were displayed (  β¯GMOpresence   = −1.21 vs.  β¯GMOboth   = −1.93; p = .010); they also became less price sensitive in the both labeling condition (  β¯pricepresence   = −3.54 vs.  β¯priceboth   = −2.85; p = .036). The difference in the threshold parameters between the both and the presence labeling conditions is not statistically significant (  γ¯presence   = −9.04 vs.  γ¯both   = −7.28; p = .140). Thus, Study 3 supports H4a and H4b—namely, structurally aligned information makes the GMO attribute more salient and even more important for brand choice. The evidence pertaining to category choice in Study 3 does not support H4c.All our findings thus far have focused on the average market response. The hierarchical Bayes model also captured individual-level responses within each labeling condition. We found that some respondents had a positive GMO coefficient, meaning they preferred products with GMOs. The segment size of this ""prefer GMO"" segment was 39.31%, 29.43%, and 17.06% in the absence, presence, and both labeling conditions, respectively. The important takeaway is that the proportion of consumers who prefer GMO-based products decreases as we move from absence-focused to presence-focused to both label conditions. Therefore, the labeling policy regime showed a substantial impact on the size of the ""prefer GMO"" segment. Study 4The goal of Study 4 was to test H5—to determine whether the graphical format of the GMO label affects the impact of the GMO attribute on consumer choice. More specifically, we aimed to understand whether the graphical format affects ( 1) how sensitive consumers are to the GMO attribute, ( 2) how important price is to consumers, and ( 3) how likely consumers are to make a purchase in a product category. To accomplish this goal, we varied the appeal of the GMO label using different colors and themes, creating a ""positive"" green label that casts GM products in a favorable light and a more moderate ""neutral"" blue label (see Figure 8). The positive-looking green label bears some resemblance to the one recently adopted in the United States,[ 7] while the neutral blue label has a similar design to the one adopted in Uruguay.[ 8]Graph: Figure 8. Logos in three labeling conditions. Logo PretestThe purpose of the pretest was to show that the two GMO logos used in Study 4 (Figure 8) convey different signals to respondents. More specifically, we tested whether the green GMO label delivered a more positive signal to respondents (i.e., GMOs are beneficial and less harmful) than the neutral blue label.Using MTurk, we recruited respondents for this pretest in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they were paying attention to the pretest instructions. Of the 208 respondents who completed this first step of the study, 6 (2.9%) did not qualify to continue because they failed to correctly answer the attention check question. The remaining 202 respondents (Mage = 41.3 years; female = 56.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nboth-positive = 101, Nboth-neutral = 101).We asked respondents to indicate the extent to which they agree or disagree with the following statements on a nine-point scale (1 = ""very unlikely,"" and 9 = ""very likely""): ( 1) ""A product with the GMO label above is likely beneficial for me,"" and ( 2) ""A product with the GMO label above is likely harmful for me."" The second statement was reverse-coded, and we reported the average for the two questions for each label. Results indicate that the natural green label led to GMOs being perceived as more beneficial than the blue label (Mgreen = 5.76, SD = 2.64 vs. Mblue = 4.82, SD = 2.32; t(200) = −2.69, p =   .008  ). Procedures and ParticipantsThe structure of Study 4 was similar to that of Study 3. We created three between-subjects conditions, one absence labeling condition and two both labeling conditions. The absence condition used the same non-GMO label as Studies 1 and 2. In the both labeling conditions, we used the two different GMO label formats (see Figure 8): the ""positive"" green label and the ""neutral"" blue label. Our primary interest was to compare the two both conditions with the absence condition.Figure 9 shows the stimuli for the three labeling conditions that we used in the choice-based conjoint experiment. As in Studies 2 and 3, we focused on three attributes: brand, price, and GMO; the choice design for the conjoint part was the same as before. The rest of the procedure was similar to Study 3, except that we added questions to measure consumers' prior attitude toward GM products. As we discuss subsequently, this approach enabled us to assess the interplay among the GMO labeling regimes, consumer attitudes, and demand for GM products.Graph: Figure 9. Example of stimuli in choice tasks in Study 4.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category (potato chips) and whether they were paying attention to the study instructions. Of the 1,089 respondents who completed this first step of the study, 213 (19.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 109; 10.0%) or failed to correctly answer the attention check questions (N = 104; 9.6%). A total of 876 respondents (Mage = 37.5 years; female = 49.5%) qualified for and completed the main study. We randomly assigned them to one of the three conditions (Nabsence = 291, Nboth-positive = 294, Nboth-neutral = 291). Results Model-free resultsThe descriptive statistics show a similar pattern to Study 3. A larger proportion of respondents chose the non-GM products in the both labeling conditions (50.15% in absence, 51.34% in both-positive, 53.91% in both-neutral). Model-based evidenceTable 3, Panel A, reports the parameter estimates for Study 4. Comparing the absence and both-neutral (blue) labeling conditions, produced results similar to those from Study 3. Participants in the both-neutral condition were more sensitive to the GMO ingredients (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-neutral   = −.79; p = .034) and less price sensitive (  β¯priceabsence   = −5.60 vs.  β¯priceboth-neutral   = −4.67; p = .018). The difference in the likelihood to purchase in the product category is not significant (  γ¯absence   = −13.04 vs.  γ¯both-neutral   = −12.18; p = .616). However, a comparison between the absence and both-positive (green) GMO labeling conditions revealed a different pattern. There was no significant difference in consumers' GMO sensitivity (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-positive   = −.43; p =   .832  ) or category purchase probability (  γ¯absence   = −13.04 vs.  γ¯both-positive   = −12.00; p =   .536  ). The only difference occurred in price sensitivity—consumers were less price sensitive in the both-positive labeling condition than in the absence condition, and it is marginally significant (  β¯priceabsence  = 5.60 vs.  β¯priceboth-positive   = −4.67; p = .06). The primary takeaway from the results is that the GMO labeling format determined the extent to which the GMO attribute affected consumer preference for GM foods. Stated differently, a comparison between the both-positive and both-neutral conditions revealed that the difference in the GMO sensitivity is marginally significant (  β¯GMOboth-positive   = −.43 vs.  β¯GMOboth-neutral   = −.79;  p=.059  ). This finding marginally supports H5a and has substantial policy implications, which we discuss subsequently. Neither the price sensitivity (  β¯priceboth-positive   = −4.67 vs.  β¯priceboth-neutral   = −4.39; p = .54) nor the category purchase probability (  γ¯both-positive   = −12.00 vs.  γ¯both-neutral   =−12.18; p = .912) is significantly different between the two conditions. These findings in Study 4 do not support H5b and H5c.GraphTable 3. Results from Study 4. A: Posterior Estimates of β¯ and γ¯BothParametersAbsenceBoth-PositiveBoth-NeutralLay's6.145.354.46(.52)(.47)(.42)Herr's3.282.011.32(.63)(.70)(.51)Ruffles6.315.333.99(.59)(.54)(.44)GMO−.39−.43−.79(.12)(.14)(.14)Price−5.6−4.67−4.39(.41)(.34)(.30)Category threshold−13.04−12.00−12.18(1.35)(1.33)(1.06)B: GMO Sensitivity for Each Subgroup Across ConditionsAbsenceBoth-PositiveBoth-NeutralBeneficial−.21−.2−.41(N = 238; 27.2%)(.16)(.15)(.16)No strong opinion−.20−.18−.66(N = 389; 44.4%)(.15)(.14)(.16)Harmful−.41−.92−1.21(N = 249; 28.4%)(.16)(.22)(.22) 3 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations. Moderating role of prior attitudesIn our final analyses, we examined whether a respondent's prior attitude toward GMOs affected their reactions to the signals in the GMO logos. To test this idea, we asked the respondents whether they think GMOs are beneficial or harmful; we then divided them into groups based on their prior attitude toward GMOs (1–3: ""Beneficial,"" 4–6: ""No strong opinion,"" 7–9: ""Harmful""). Of the 876 respondents, 27.2% (N = 238) answered that GMOs are beneficial, 44.4% (N = 389) responded that they have no strong opinion about GMOs, and 28.4% (N = 249) reported that they think GMOs are harmful. Using individual-level parameters, we report the average GMO sensitivity by attitude group and label condition in Table 3, Panel B.[ 9]Drawing on the findings in the top row of Table 3, Panel B, for the group that considered GMOs beneficial (27.2% of the sample), the both-neutral GMO labeling format had a significant impact (p = .025) on GMO sensitivity. For the group with no strong opinion about GMOs (44.4% of the sample), the both-neutral GMO labeling format had a significant impact (p = .002) on GMO sensitivity (see middle row, third column of Table 3, Panel B). The other two GMO label formats (absence and both-positive) did not have any impact on GMO sensitivity for this group. This is the most important finding in Table 3, Panel B. From these results, we concluded that, for a very large segment of the sample, the GMO label format had a decisive impact on how consumers view GMOs in the product choices they make. Lastly, among participants who viewed GMOs as harmful (28.4% of the population; bottom row of Table 3, Panel B), compared with the absence condition, GMO labeling in the both-neutral condition had a larger (p = .004) impact on GMO sensitivity.Study 4 examined the impact of GMO label format on consumer choice. Our results showed that a GMO logo can systematically influence consumer choices. The signal contained in the label format can cause large shifts in consumer preference for GM foods. Importantly, we found that the GMO label format had a greater impact on consumers who had no strong opinions about GMOs, suggesting that preference for GM foods is highly pliable for a large segment of consumers. This effect occurs when using a neutral label format in this study, suggesting that a label that signals caution (e.g., Brazil's yellow transgenico logo) is likely to have an even more pronounced effect. DiscussionMost scientists claim that GM foods are safe for human consumption and that they offer substantial advantages; meanwhile, many consumers who lack scientific knowledge are skeptical about GM foods. Fueling these conflicting views, consumer advocacy groups have asserted that consumers need to know what they consume. Against this backdrop of diverging stakeholder views, we investigated the impact of different GMO labeling regimes. In particular we studied the impacts of voluntary, absence-focused labeling (""non-GMO"") versus mandatory, presence-focused labeling (""contains GMO""). A natural extension of the latter is a third regime in which both label types are displayed on products in the marketplace. We showed that each GMO policy has a substantial impact on consumer choices and creates incentives for firms that are important for policy makers to consider.Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on the findings of our four studies, we show that each of the three labeling regimes (absence, presence, and both) greatly affects consumers' demand for GM foods. Labels such as ""non-GMO"" and ""contains GMO"" serve as negative signals for GM foods and tend to shrink their market share. The market share shrinkage effect is stronger under the mandatory policy than the voluntary policy. GMO labeling reduces the importance consumers place on product price and impacts the consumers' WTP for non-GM products. The finding pertaining to increased preference for the non-GM products is amplified when both non-GMO and GMO labels are displayed on the products.Finally, we found that the signal policy makers decide to send via the GM label (e.g., a green logo may be viewed as an endorsement, a yellow logo as a cautionary signal) significantly affects consumer choice. Consumers' prior attitudes toward GMOs moderate this finding; consumers who are neutral toward GMOs are impacted most by the signal contained in the label. Implications for Policy MakersIn line with their relative impact on demand for GM foods, label regulations could be viewed at three levels: low, medium, and high impact. They correspond respectively to the three policy regimes: absence, presence, and both. Compared with a situation where no GMO labels exist, the voluntary GMO labeling policy (""non-GMO"") affects the demand for GM products the least. Mandatory labels (""contains GMO"") substantially affect the demand for GM products, and when both types of labels are present (""non-GMO"" and ""contains GMO""), the demand shifts are the highest.Figure 10 shows that GMO labeling has an economically significant impact on consumer WTP for non-GM products and reveals several important insights. First, voluntary, absence-focused labeling results in the lowest increase in WTP for non-GM products. In two of the latter three studies (Studies 3 and 4), this increase in WTP was nonetheless statistically and economically significant (9 cents and 7 cents, respectively). In comparison, in Studies 2 and 3, the increase in WTP in the mandatory, presence-focused labeling was substantially higher, at 50 cents and 34 cents, respectively. In Study 3, when both labels were present, WTP was the highest (68 cents). These higher WTP measures in Studies 2 and 3 should be viewed in light of the GMO label used: a yellow GMO logo similar to Brazil's in Study 2, and a red GMO label in Study 3. In Study 4, in which we tested positive (green) and neutral (blue) ""contains GMO"" logos, the WTP was still positive and economically significant (9 and 18 cents, respectively). The upshot here is that consumer WTP critically depends on the label policy makers adopt. Across studies, findings indicate that both the voluntary and mandatory labeling regimes create incentives for firms to add premium-priced, non-GM products to their portfolio of offerings. These incentives are substantially greater in the mandatory labeling regime than in the voluntary regime.Graph: Figure 10. Impact of GMO labeling on Willingness to Pay (WTP $).Contrary to the opinion that mandatory GMO labeling will merely satisfy consumers' right to know and give them complete information, our findings show that any form of GMO labeling has significant externalities that policy makers must consider carefully. GMO labeling reduces the demand for GM foods and creates incentives for firms to offer higher-priced non-GM foods, which raises the question of whether policy makers intend to promote this effect.In addition to deciding which labeling regime to implement, policy makers also have to wrestle with another critical decision related to the signal contained in the GMO label. For example, consumers may view a green label as an endorsement of GM foods and a yellow label as a signal to exercise caution. The United States has decided to implement the use of a mandatory GMO labeling system by 2022; the proposed label has a nature-friendly theme on a green or black-and-white background, along with the term ""bioengineered (BE)."" In contrast, the GMO label in Brazil is a yellow triangle resembling a caution sign. Our findings indicate that even a neutral GMO label may lead consumers to focus on the negative aspects of GMOs, pay less attention to price information, and become more reluctant to make a purchase in the product category. In Brazil, all these effects are likely amplified. The two labeling regimes at each end of the spectrum in Brazil and the United States, along with the insights offered by this article, may offer guidance to other countries about which labeling regime they should adopt. Implications for Marketing PractitionersOur research reveals that GM labels add an important product feature for consumers to evaluate. It draws attention away from factors such as price, allowing firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snacking category). All of these effects are amplified as we move from absence-focused (voluntary) to presence-focused (mandatory) policies. Both regimes create incentives for firms to expand their offerings to include more non-GM products for a segment of the market that prefers such products and is willing to pay more for them. As Figure 10 illustrates, consumer WTP for a non-GM product is higher in a presence-focused labeling regime. GM manufacturers inevitably lose market shares when presence-focused labeling is enforced. They face both reduced brand share and reduced category demand. Because presence-focused labeling makes consumers less price sensitive, GM food manufacturers may attempt to compensate for their sales loss by considering promotions other than price cuts. Limitations and Future DirectionsUnlike the debate on mandatory labeling, the discussion of regulatory aspects of voluntary labeling is limited. Regulators intervene when products make unsubstantiated health benefit claims ([ 8]). Along the same lines, future research could investigate the need for policies to regulate voluntary non-GMO disclosures. To complement our findings based on choice experiments, it would be instructive to rely on purchase data over time (e.g., [44]) from sources such as Nielsen and IRI to investigate how GMO labeling affects consumer behavior. Our choice experiments offer limited evidence for category shrinkage effects because of GMO labeling; a rigorous test for such effects would be in a multicategory context using household-level panel data. ConclusionGMO labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. Even a voluntary GMO labeling policy deserves regulatory scrutiny because it causes a decrease in demand for GM products. In comparison, a mandatory GMO labeling policy shrinks the demand for GM products even more, and the signal contained in the GMO logo (e.g., color) plays a critical role in consumers' perceptions of GM products. Both voluntary and mandatory policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to promote such consumer and firm behaviors. "
19,"Halo or Cannibalization? How New Software Entrants Impact Sales of Incumbent Software in Platform Markets Platform markets involve indirect network effects as two or more sides of a market interact through an intermediary platform. Many platform markets consist of both a platform device and corresponding software. In such markets, new software introductions influence incumbent software sales, and new entrants may directly cannibalize incumbents. However, entrants may also create an indirect halo impact by attracting new platform adopters, who then purchase incumbent software. To measure performance holistically, this article introduces a method to quantify both indirect and direct paths and determine which effect dominates and when. The authors identify relevant moderators from the sensations–familiarity framework and conduct empirical tests with data from the video game industry (1995–2019). Results show that the direct impact often results in cannibalization, which generally increases when the entrant is a superstar or part of a franchise. For the indirect halo impact, superstar entrants significantly increase platform adoption, which can help all incumbents. Combining the direct and indirect impacts, the authors find that only new software that is both a superstar and part of a franchise increases platform adoption sufficiently to overcome direct cannibalization and achieve a net positive effect on incumbent software; all other types of entrants have a neutral or negative overall effect.Keywords: cannibalization; entertainment franchise; halo effect; indirect network effects; platform markets; superstars; two-sided markets; video gamesA platform market involves two or more user groups (i.e., sides of a market) whose interactions are mediated through a platform. These markets are typically characterized by indirect network effects, as the actions of agents on one side of the market affect the outcomes of agents on another side ([44]). Many of these markets consist of both platform devices and corresponding software ([24]; [42]). When a new software entrant launches into such a platform market, its ultimate success might depend on a variety of factors or be measured in a variety of ways—sales of the entrant, impacts on sales of incumbent software already on the market, or platform sales. Yet with few exceptions (e.g., [24]; [30]), extant research has focused on sales of the core platform, rather than specifying how an entrant might alter the sales of the existing (i.e., incumbent) software portfolio. In particular, new software might cannibalize incumbent sales ([46]), but it also arguably can create a halo effect that increases the sales of competitive, incumbent software ([32]). These outcomes are critical across the platform ecosystem; as [42], p. 1232) explain, ""Members of a platform ecosystem often have strong vested interest in each other's fates. Because it is the overall appeal of the ecosystem that attracts end users to the platform, the success of individual members depends, at least in part, on the success of other members of the ecosystem—even those with which they may be simultaneously competing.""Consider the paths in Figure 1. A new software entrant can influence incumbent software sales directly (Path A) and positively if the entrant stimulates usage among existing platform owners. However, a negative competitive effect is more likely, as consumers choose to purchase the entrant instead of the incumbent. In addition, the entrant might exert an indirect, positive impact on the incumbent by stimulating sales of the platform (i.e., new platform adoptions) (Path B). Consumers who newly adopt the platform likely backfill their collection of platform-compatible software by purchasing incumbent software (Path C).Graph: Figure 1. Paths by which new software entrants impact sales of incumbent software.But how much of the entrant's impact on incumbent software sales is through direct Path A versus indirect Paths B and C? This central question has not been addressed, and answering it will provide a more holistic view of entrants and their sales implications. While variations of Path B (software → platform sales) have been studied (for a review, see [48]), scholars have ""focused almost exclusively on quantity"" ([29], p. 39) or size of the network. Scholars rarely differentiate the impact of entrants (instead of the quantity of software stock) or new platform adoptions (instead of the quantity of the platform's installed base).In addition, Paths A and C have yet to be broadly addressed. A few studies examine individual software launches but do not quantify the impact on incumbent software. For example, [ 8] study how software entrants impact hardware sales but do not measure whether incumbent software sales are affected; [30] investigates both platform and software sales but assumes no competition between available software. This gap is important because research outside of platforms shows that new products can impact incumbents both positively (e.g., via spillover; [ 1]) and negatively (e.g., by increasing competition; [46]). Given this evidence, we argue that it is vital to understand the holistic impact of new software launches on incumbent software sales.Further, it is important that new product research accounts for the complexities of platform markets, in which entrants influence incumbents both directly (as substitutes or complements [Path A]) and indirectly (through platform demand [Paths B and C]). Such insight is useful, as many new products release into platform markets (e.g., apps, video games); our approach distinguishes the impact of an entrant versus classic software supply. Rather than measure the impact of an increase in overall software stock, we examine the attributes of new software that differentially affect incumbent software (and platform) sales. These findings are not a simple extension of extant knowledge (i.e., new software affects platform sales) into a similar setting. Whereas software entrants are complements to platforms, they are primarily substitutes to incumbent software. Further, in these complex markets, purchases can put customers into active states that prompt them to purchase additional (potentially incumbent) software ([23]). We thus rely on established evidence that platform sales influence software demand, but we go further to detail the specific effects on incumbent software driven by new platform adoptions. In Figure 1, our contributions refer mainly to the understudied Paths A and C (see also Table 1).GraphTable 1. Studies of Indirect Network Effects in Platform Markets. Traditional Network LogicPaths Presented in Current StudyAuthorsRelevant Research QuestionImpact of Software Stock on Platform AdoptionImpact of Platform Installed Base on Software Stock or SalesPath A: Direct Impact of New Software on Incumbent SoftwarePath B: Direct Impact of New Software on Platform AdoptionPath C: Indirect Impact (Through Platform) of New Software on Incumbent SoftwareModerators: The Impact of New Software on Incumbents Differs Based on Software CharacteristicsBinken and Stremersch (2009)What is the differential impact of superstar versus nonsuperstar software on hardware adoption?YesNoNoYes (including impact of software characteristics)NoNoCox (2014)Do software characteristics determine if software will be a blockbuster?NoYesNoNoNoNoGretz et al. (2019)How is the impact of superstar software on hardware moderated by hardware's product life cycle?Yes (software characteristics)YesNoNoNoNoHaviv, Huang, and Li (2020)Does software stock in a given period affect software demand in subsequent periods?YesYesNo, but measures how increases in stock affect later periods' demandNoNoNoHealey and Moe (2016)Does platform recency and innovativeness affect software adoption?NoYesNoNoNoNoKim, Prince, and Qiu (2014)Does hardware quality, beyond the installed base, affect the supply of software products?YesYesNoNoNoNoLandsman and Stremersch (2011)How does multihoming impact platform sales?YesYesNoNoNoNoLee (2013)Does vertical integration and exclusive software affect hardware and software?YesYesYes, robustness check of how superstars affect other superstarsNoNoNoStremersch et al. (2007)Does software lead hardware or does hardware lead software?YesYesNoNoNoNoCurrent studyHow do new software entrants affect the sales of incumbent software, directly and indirectly?YesYesYesYesYesYes To determine these effects, we use a dynamic model to measure the impact of software entrants on incumbent software sales and platform sales, while also integrating the contextual factors that determine heterogeneous effects. We build on [ 4] model and incorporate heterogeneity through random coefficients, along with fixed effects in a system-of-equations estimation. We use a feasible generalized least squares (FGLS) approach to test how incumbent software–specific factors influence the new product effects. Following [29], we apply these methods to a video game data set, comprising monthly sales of 13,064 games (software) and 19 consoles (platform) over 1995–2019, as well as the dates of software introductions and advertising expenditures. These recent data enable us to report current effect sizes and establish timely evidence for this rapidly evolving industry.Overall, we contribute to research in platform markets by studying the relative impact of new software products on incumbent software sales. We report empirical generalizations for effect sizes for Paths A, B, and C, distinguishing direct from indirect impacts. With our holistic approach, we are then able to quantify the total (net) impact of an entrant by aggregating the effects of each path. In determining effects, we add context by identifying characteristics of entrants and incumbents that moderate each path. Drawing from the sensations–familiarity theoretical framework ([25]), which fits well for hedonic software (e.g., apps, video games), we operationalize software characteristics by capturing whether the software is a superstar or member of a franchise.We find that the direct impact (Path A) of an entrant usually, but not always, results in cannibalization, depending on its characteristics. When entrants are superstars and/or members of a franchise (vs. not a superstar or franchise member—""standard software"" hereinafter), they directly cannibalize incumbents more. The incumbents' characteristics offer little protection against such cannibalization. We find that new superstars significantly increase new platform adoption through the indirect Path B, which benefits incumbent superstar software and franchise members more than standard software (Path C). By combining both the indirect and direct impacts, we determine the net overall impact: Standard entrants hurt all incumbents, but a new superstar can produce a net positive halo, depending on the context. For example, a 1% increase in entrants with both superstar and franchise status drives a.0207% net increase (direct + indirect) in the sales of incumbent franchise software; the direct cannibalization of incumbent sales (via Path A) of −.0130% is overcome by a halo effect from new platform adopters (via Paths B and C) that indirectly increases incumbent sales by.0337% (.0207 = −.0130 + .0337). We show managers how to estimate both direct and indirect impacts of different types of new software based on the specific makeup of a firm's portfolio. We also show how these estimates translate to financial outcomes. We provide new insights for platform markets and new product research, while extending the sensations–familiarity framework. ConceptualizationTo connect our findings to extant literature, we ground our conceptualization in network effects research. We then outline relevant theory from which we identify key variables. Network Effects in Platform MarketsIndirect network effects in systems markets are of enduring interest to scholars (e.g., [12]; [45]; [48]). Systems or platform markets often comprise hardware, such as a computer, game console, or smartphone, and related hardware-compatible software, such as computer programs, video games, or apps. A systems logic applies to digital platforms too (e.g., Netflix); so, to ensure the broad applicability of our study logic, we use the terms ""platform"" (vs. hardware; [47]) and ""software.""Platform markets create both direct and indirect network effects. Direct network effects arise when the value to a user increases with the number of other platform users; for example, a multiplayer online game with many fellow players is more desirable ([33]). Indirect network effects exist if the number of platform users entices software producers to create new offerings for the platform. As the software portfolio improves (e.g., quality, variety), so does the attractiveness of the platform to users ([12]). Indirect effects apply to all members of the system: platforms need an attractive software portfolio to attract customers, and software providers prefer to develop software for platforms with large installed bases. In the vibrant research dedicated to these markets, briefly summarized in Table 1 (for a more expansive list, see Table W1 in the Web Appendix), we know of no efforts to calculate the extent of the total impact (direct and indirect) of new software entrants on sales of incumbent software. Sensations–Familiarity FrameworkMany platform markets exist for entertainment products (e.g., video games, movie streaming services, apps), so we rely on a theory that predicts consumer responses to hedonic products, namely, the sensations–familiarity framework ([25]). However, a similar logic can apply in nonentertainment settings (e.g., [38]). According to this theory, a person consumes hedonic products for pleasure (vs. functional benefits) that can be derived from the very act of consumption. This consumption act (e.g., playing a game) generates emotional responses (e.g., happiness, melancholy) and cognitive responses, which define people's mental representations of the experience. [ 6]) argue that consumers combine these responses to form holistic product judgments (vs. evaluating the attributes piecemeal). The impact of sensation and familiarity on consumersThe sensations–familiarity framework indicates that both sensations and a sense of familiarity can each drive emotional and cognitive responses that underlie judgment. Sensations are physiological (vs. purposive) responses, felt as a sense of arousal when nerves are stimulated and hormones (e.g., dopamine) released. Humans become satiated easily and prefer novel, multidimensional sensations, which leads to an innate desire to seek out rich, new sensations instead of repeating the same one ([35]). Familiarity refers to a consumer's sense of connectedness to a product (or its elements). Built through prior exposure, when a new product includes elements that are familiar, the sense of familiarity triggers memories and emotions that transfer to the new product ([ 9]). Familiarity also enhances processing fluency by helping consumers quickly make sense of new products ([39]).Sensations and familiarity thus form a delicate balance. A new product that is too familiar (few new sensations) seems stale and unappealing. A new product that is too novel (stark new sensations with little link to the familiar) can overwhelm consumers by the sheer intensity of sensations (e.g., never-ending explosions in a Michael Bay movie; [25]). Thus, new product managers introducing hedonic products aim to provide the ""right"" levels of sensations and familiarity to maximize the products' attractiveness. Relevant variablesThe variables we derive from this framework in turn reflect familiar elements of prior products and/or promise rich new sensations. For video game software, the task is to offer enough familiarity to connect with consumers (e.g., familiar worlds, product designs, characters) while also providing new sensations to arouse consumers (e.g., exciting new patterns of play, beloved characters in new worlds, novel designs) ([25]). Thus, we focus on two variables with distinct influences in extant studies. First, software products (e.g., video games, apps, movies) of extraordinarily high quality offer rich, desirable sensations. We thus operationalize the sensations factor as ""superstar software,"" defined as ""software titles of exceptional high quality"" that often ""possess unique and superior attributes"" ([ 8], pp. 88–89). Superstars may achieve high payoffs, but this term refers explicitly to product quality, unlike the terms ""blockbusters"" or ""hits,"" which reflect sales volume known only after product release ([ 8]). Second, we operationalize familiarity as software that is part of a franchise (e.g., Super Mario Bros., Star Wars). Extensions of a franchise leverage the influence of familiarity, but they also offer some new sensations and contexts (e.g., Super Mario Bros. 2, The Empire Strikes Back). By integrating elements of an existing brand into additional products, ""the resulting set of products, in its entirety, then constitutes the 'franchise'"" ([25], p. 429). Accordingly, software that is part of an established franchise should be desirable to consumers ([40]), and real-world evidence consistently shows that franchise products (e.g., sequels, prequels, reboots/remakes) often outsell similar, nonfranchise products ([33]). Both superstar and franchise variables have appeared in prior studies, but we do not know of their use to determine how new product entrants affect incumbent sales. Next, we explicate the roles these variables play in the three paths of Figure 1. Path A: The Direct Impact of New Software Entrants on Sales of Incumbent SoftwareIn Path A—the direct impact of new software entrants on sales of incumbent software—we are conceptualizing the behaviors of existing platform owners. In this case, a new software entrant is a potential substitute for incumbent software ([14]); they all compete for the consumer's (i.e., platform owner's) limited budget ([18]). Thus, to accurately quantify Path A, we should consider not only average effects but also how effects differ according to the characteristics of both the new entrant and the incumbent.The characteristics of the new entrant should be particularly powerful, as the effects suggested by the sensations–familiarity framework may be most influential when consumers encounter new stimuli. For example, when they first play a new video game, the sensations offered are, to some degree, new to the world; if these novel sensations also are rich, as in the case of superstar software, they likely exert the strongest impacts on consumer preference. However, product newness also evokes uncertainty, so familiarity likely shapes consumers' responses as well. Being part of a franchise offers a comforting connection to known objects, such that relevant memories and emotions can transfer to the new entrant. Thus, we expect characteristics of both superstar and franchise status to increase the direct cannibalization caused by a new entrant.In Path A, which reflects the consumer's choices between the new entrant and the incumbent, the characteristics of the incumbent software might matter too, but their power is less clear. At first glance, the rich sensations offered by a superstar incumbent might enhance its attractiveness and provide some level of protection against cannibalization by standard new entrants. Likewise, the familiarity benefits of being part of a franchise should leave the incumbent from a franchise (vs. nonfranchise) less susceptible to cannibalization by new entrants. However, prior applications of the sensations–familiarity framework have focused almost exclusively on new products ([ 5]), so it is not entirely clear how the findings generalize to incumbents. Being a superstar or part of a franchise may not provide incumbents with protection from competition. First, the specific sensations offered by this incumbent would be new to consumers who had yet to purchase it, but they are not new to the world. Even new purchasers likely have gained some predictive information about them, such as through word of mouth from other consumers with experience. Second, word of mouth about these experiences reduces not only novelty but also the level of uncertainty for new consumers, which might attenuate the familiarity benefit of being a franchise incumbent. Path B: The Direct Impact of New Software Entrants on Platform AdoptionIt is well known that new software influences sales of platform devices (for a review, see [41]]). We test this path to ensure a complete model and to isolate the indirect impact of new software entrants on sales of incumbent software by stimulating platform sales (Paths B and C). However, we differ from extant literature by focusing on the impact of new software entrants (vs. software stock) on new platform adopters (vs. installed base). That is, we expect software to increase platform adoption, consistent with traditional network logic, which may benefit incumbents, but we also test for distinct effects of new entrants. For example, because new superstars create excitement, they might attract increased attention and more new platform adopters ([ 8]; [21]). Franchise video games also are disproportionately attractive to consumers, so they prompt higher sales ([16]; [33]), and new franchise software might draw more new adopters to the platform. Path C: The Impact of New Platform Adopters on Sales of Incumbent SoftwareWhereas Path A taps how a new software entrant competes directly with incumbent software among existing platform owners, Path C represents sales of incumbent software generated by new platform adoptions and thus reflects consumers' incumbent-versus-incumbent choices upon adopting the platform—specifically, whether and which incumbent offerings to buy. We thus calculate how the influx of new adopters affects demand for incumbent software (Path C), independent of the direct competitive effect of new software entrants.Traditional cannibalization viewpoints frame a market as a zero-sum game with a finite number of customers. But in platform markets, a new product can help other products by driving increased traffic ([18]; i.e., product spillover effects; [ 1]). If a new software entrant induces new consumers to adopt the platform, they might buy other software from the existing portfolio. For example, if PlayStation loyalists wanted to play Super Smash Bros. Ultimate, released on December 7, 2018, and therefore bought a Nintendo Switch console to gain access to it, the consumers likely bought other Switch games to make full use of the console's capabilities (e.g., on-the-go gaming). The players also might try to ""backfill"" a collection of software favorites, played previously on the PlayStation; such cross-platform adoption is common for entertainment products ([30]).The notion that platform sales lead to software sales is foundational to network effects theory, but we extend this traditional view. That is, our argument is not about new platform adopters in general, but on the purchases of new adopters attracted by the release of new software. We study Path C as part of our novel effort to understand the totality of Paths B and C. Further, we know of no other study that measures whether new platform adopters influence incumbent software sales immediately, that is, in the same month as platform adoption, when new adopters' budgets are likely depleted from buying the platform and new software entrant. Studies that span the platform's entire installed base cannot specify how platform adoption affects software sales immediately, because they lump existing platform owners in with new adopters. Assessments of software stock overall, rather than individual incumbent software, also cannot determine which type of software (e.g., superstar, franchise) benefits the most from greater platform adoption.For Path C, we again apply the sensations–familiarity framework: for choices between two or more incumbent products, superstars and franchise software are more attractive than standard incumbents, so their sales may benefit most from new platform adopters. Overall (Net) Impact of New Entrants on IncumbentsFinally, to quantify the total net impact of a new software entrant on sales of incumbent software, we combine the direct (Path A) and indirect (Paths B and C) impacts. We want to understand in what circumstances a positive halo effect (i.e., increasing the number of platform adopters who subsequently buy incumbent software) is enough to overcome the direct cannibalization effect caused by increased competition.On the one hand, a new product could be so keenly attractive that it cannibalizes sales of the incumbent ([46]), despite the overall increase in platform customers ([14]). With limited budgets, consumers cannot purchase an infinite amount of software; in turn, the cannibalization argument suggests that new software entrants prompt customers to choose them instead of an incumbent, resulting in a negative net impact on sales of incumbent products. On the other hand, the new entrant might attract enough new adopters, who subsequently purchase incumbent software, that it overcomes the direct competitive effects, resulting in a positive net impact. We note that [24] find that platforms with newer customers sell more content.To reconcile these competing ideas, we use a decision rule suggested by [11]: to increase net software demand, the network effect of new platform users must be greater than the competitive effects among software products. With empirical tests, we can determine the sizes of both the direct cannibalization impact (Path A) and the indirect impact (Paths B and C) from new platform adopters, then calculate the overall net impact. But we also consider the characteristics of the new software entrant that might determine the strength of the direct and indirect impacts. Any new entrant increases the size of the software portfolio and thus should incrementally increase platform attractiveness and sales ([48]); however, to overcome cannibalization, the new software entrant needs to generate so many new platform adopters, who then purchase so much incumbent software that they create an overall net halo effect ([11]). It is likely that only software that is highly valued will attract a sufficient number of new platform adopters to overcome cannibalization. For example, prior research has demonstrated that superstar software produces a large surge in platform sales ([ 8]). Therefore, we measure if Paths B and C produce enough incremental sales to overcome Path A, and if this total effect differs based on superstar and franchise status. Control variablesProduct-level competition ([18]) implies cannibalization due to substitution ([14]). Well-established factors that affect substitutability include sequel status, genre, price, exclusivity, and advertising ([25]; [33]; [42]). We control for these either directly or through use of fixed effects. MethodologyOur empirical context is the console-based video game industry, a large and significant part of the economy. Games for video game consoles generate more than $34 billion in annual sales. The entire industry is larger, including over $32 billion earned annually by computer-based and online games; mobile-device games earn over $70 billion annually ([51]).Our setting is generalizable to other networked markets and is ideal for exploring our research questions for several reasons. First, studies in marketing (e.g., [ 8]; [29]) and related fields (e.g., [13]; [23]) use video game data to test platform market theories because video games are ""a classic example of a high-tech networked market"" ([20], p. 284). Second, managing product introductions is an important decision in networked industries ([31]), especially for video game firms ([ 8]). These markets observe frequent new product introductions and offer the ability to observe many products from inception to decline ([10]). Third, games are not natural complements; owning one game does not increase the utility of owning another, reducing confounds when examining cannibalization and halo effects. Fourth, characteristics in gaming (e.g., superstar status, franchise membership) map directly to the variables suggested by the sensations–familiarity framework ([ 8]; [25]; [30]).We obtain monthly quantity and revenue data for 8,470 unique games and 19 consoles on the U.S. video game market from January 1995 through June 2019 from the market research firm The NPD Group. Many games are released on multiple consoles, creating 13,064 software/platform combinations. We structure the data as an unbalanced panel with 698,703 software/platform/month observations. The average console is on the market 100.37 months, for 1,907 platform/month observations. We match NPD data to advertising data from Kantar Media. These data include U.S. radio, television, cinema, online, outdoor, and print advertising spend for each game and each console in each month. We aggregate and obtain advertising expenditure at the software/platform/month level for games and at the platform/month level for consoles. Key Definitions and Operationalizations New versus incumbent softwareWe classify software as new in the first month we observe sales on the platform. Software is an incumbent any time after its first month on the platform. FranchisesWe operationalize familiarity with game franchise status. NPD gives a game's franchise affiliation (e.g., Call of Duty) when applicable. This identifies the initial entry in the franchise along with any extensions. We use this to classify whether incumbent or new software belongs to a franchise. Note that the first entry in a franchise is not a franchise when it is introduced; rather, it becomes a franchise when the first extension is introduced. Overall, 62.5% of software/platform observations are connected to a franchise in some way: 17.0% are first entries that become part of a franchise when an extension is introduced, and 45.5% are franchise extensions. SuperstarsWe use superstar status to identify software that provides rich sensations. We assess superstar status using critic evaluation, similar to other game industry studies ([ 8]; [33]). Importantly, critics typically review and provide assessments before a game's release ([53]). This means that the quality measure is determined independently from sales, which lessens endogeneity concerns. We obtain data from Moby Games (https://www.mobygames.com/), which has track record in video game research (e.g., [15]). Moby Games provides a Mobyrank, or average critic rating, for each software/platform observation. Mobyrank ranges between 0 (low) to 100 (high); we classify superstars as games with Mobyrank ≥ 90, similar to other studies ([ 8]; [21]; [26]). This operationalization has external validity; it is a popular industry cutoff as game-developer bonuses are often tied to achieving average critic scores of 90+ ([49]). We use superstar-franchise to label software that are both superstars and part of a franchise. In total, 2.427% of software/platform observations involve superstars, similar to other video game studies ([ 8]; [21]); 1.85% are superstar-franchise; and.57% are superstars that are not a part of a franchise. New Platform Adopters Model: Variable Operationalization and Equations Dependent variableFor Path B, our goal is to estimate the effect of new software entrants on new platform adopters. We log-transform the continuous variables when estimating our model because we compare new platform adopters (and software sales) among a disparate set of platforms (and software) that have differing sales volumes ([ 8]). We operationalize new platform adopters as the natural log of unit sales of platform j at time t (  lnNewPlatAdoptersjt  ) using each platform's monthly sales. Independent variables lnIntSoftjt  ,  lnIntSuperjt  ,  lnIntFranjt  , and  IntSuperFranjt  are the natural logs of the count of all, superstar, franchise, and superstar-franchise software new to platform j at time t, respectively. Note that assessing the impact of superstar-franchise software with a classic interaction effect using  lnIntSuperjt  and  lnIntFranjt  is incorrect because these are binary, software-level characteristics. Rather, superstar-franchise classification occurs at the software level before creating the aggregate variables at the platform level. In addition, we use mutually exclusive classifications of superstar, franchise, and superstar-franchise to aid in interpretation. ControlsWe include five controls that influence platform demand. First, the natural log of price of platform j in time t,  lnPricejt  , adjusted for inflation using the Consumer Price Index for All Urban Consumers (CPI-U) with January 2007 = 100. Second, we control for the stock of available software ([17]) for each group (  lnAvailSoftjt  ,  lnAvailSuperjt  ,  lnAvailFranjt  ,  lnAvailSuperFranjt  ). Stock is measured using the natural log of the count of the active software catalog (i.e., software for platform j introduced prior to t with positive sales in the period). This enables us to distinguish the effect of the software entrant variables from software stock. Third, we use natural log of advertising for platform j in t,  lnPlatAdvjt  , adjusted for inflation using the CPI-U ([45]). Fourth, we include platform age (  PlatAgejt  ) as months since introduction, and its square, because evidence suggests that the effect of age on platform demand is nonlinear ([26]; [30]). Fifth, month and year fixed effects address month-specific (e.g., holidays) and year-specific (e.g., economic shocks) trends.Finally, we add 1 before logging the software entrant, software stock, and advertising variables to ensure that the log-transformation is defined when the level is 0. Platform adoption equationPrevious research finds that platform sales are influenced by lagged realizations of independent variables ([ 8]), so we use a dynamic model where  lnNewPlatAdoptersjt  depends on previous values ([52]): lnNewPlatAdoptersjt=βIntSoftlnIntSoftjt+βIntSuperlnIntSuperjt+βIntFranlnIntFranjt+βIntSuperFranlnIntSuperFranjt+∑p=1PβlagplnNewPlatAdoptersjt−p+Controlsjt+εjt, Graph( 1)with Controlsjt=βPricelnPricejt+βASoftlnAvailSoftjt+βASuperlnAvailSuperjt+βAFranlnAvailFranjt+βASuperFranlnAvailSuperFranjt+βPlatAdvlnPlatAdvjt+βAgePlatAgejt+βAge2PlatAgejt2+MonthDummiest+YearDummiest+νj. Graph MonthDummiest  and  YearDummiest  represent coefficients with their respective dummies for the month and year in time t. We find the appropriate lag length (P = 1 in our case) by experimenting with longer lags and eliminating those that are insignificant ([19]). Including lags allows changes in variables to affect future periods; the coefficients are current-period effects; multiplying by  1/(1−∑βlagp)  gives total long-term effects ([19]).Our log-log specification means that the coefficients on the software entrant variables are elasticity estimates. Specifically,  βIntSoft  captures the elasticity of platform adoption with respect to entry of standard software. The mutually exclusive classification means that  βIntSuper  ,  βIntFran  , and  βIntSuperFran  are the moderating effects for superstar entrants that are not part of a franchise, franchise entrants that are not superstars, and entrants that are both superstars and part of a franchise, respectively.We also include platform fixed effects,  νj  , to control for time-invariant, platform-specific heterogeneity (i.e., individual console characteristics) that impacts adoption rates. We identify coefficients from within variance given the fixed-effects approach. We display panel-level descriptive statistics, which show measures of within variance for each variable, in Web Appendix Table W2. Table 2, Panel A, summarizes variables and definitions for the platform equation; it also includes variables that we define when discussing endogeneity.GraphTable 2. Select Variable Names and Definitions. Variable NameDefinitionA: Platform Adoption EstimationlnNewPlatAdoptersjtNatural log of unit sales of platform j at time t.lnIntSoftjt, lnIntSuperjt, lnIntFranjt, lnIntSuperFranjtNatural log of the counts of all, superstar, franchise, and superstar-franchise (i.e., both) software entry on platform j in time t; classifications of superstar, franchise, and superstar-franchise are mutually exclusive.lnPricejtaNatural log of average price of platform j in time t.lnAvailSoftjt, lnAvailSuperjt, lnAvailFranjt, lnAvailSuperFranjtNatural log of the counts of all, superstar, franchise, and superstar-franchise software for platform j introduced prior to time t that are still active (i.e., positive sales in t).lnPlatAdvjtaNatural log of total platform level advertising expenditure for platform j in time t.PlatAgejtPlatform age at time t measured in months since platform j was first on the market.MonthDummiest, YearDummiestMonth fixed effects, year fixed effects.PlatIBjtInstalled base of platform j at time t (i.e., cumulative sales prior to time t).AvgSoftAdvDifPlatjtAverage software unit advertising on competing platforms other than j in time t.AvgSoftAgejtAverage age of active nonsuperstar software for platform j introduced prior to time t.ECMt, MORMMtThe producer price indexes for (1) electronic computer manufacturing in time t and (2) magnetic and optical recording media in time t, respectively.B: Addition Variables in Incumbent Software Sales EstimationlnSoftSalesijtNatural log of unit sales of incumbent software i corresponding to platform j in time t.lnIntSoftDifGenreijt, lnIntSuperDifGenreijt, lnIntFranDifGenreijt, lnIntSuperFranDifGenreijtSoftware entry as defined for the Platform Adoption Estimation but only counting software in a different genre than incumbent software i.lnPriceijtaNatural log of average price of software i on platform j in time t.SeqeulIntroedijtIndicator: a franchise extension to software i is introduced on platform j at time t.lnAvailSoftijt, lnAvailSuperijt, lnAvailFranijt, lnAvailSuperFranijtPrevious software introduction as in the Platform Adoption Estimation but recalculated to exclude software i.lnAvailSoftDifGenreijt, lnAvailSuperDifGenreijt, lnAvailFranDifGenreijt, lnAvailSuperFranDifGenreijtPrevious software introduction as defined for the Platform Adoption Estimation but recalculated for software in a different genre than incumbent software i.lnSoftIBijtNatural log of installed base of software i on platform j at time t.lnSoftAdvijtaNatural log of total software-level advertising expenditure for software i in time t.SoftAgeijtSoftware i's age on platform j in time t in months since software entry on the platform.PeriodDummiestPeriod fixed effects (i.e., month fixed effects interacted with year fixed effects).Superij, Frani, SuperFranijIndicators: software i on platform j is superstar, franchise, or superstar-franchise; classifications are mutually exclusive.GenreDummiesiGenre fixed effects.AvgPriorSoftPriceSameAgeijtAvg. ln of software price for software released on platform j prior to software i when they were the same age as software i in time t (e.g., if software is six months old, then the variable is calculated from the prices of previously released software when these products were six months old).AvgPriorSoftAdvSameAgeijtAvg. ln of software advertising expenditure on software released on platform j prior to software i when they were the same age as software i in time t (e.g. if software is six months old then the variable is calculated from advertising expenditure of previously released software when these products were six months old). We set AvgPriorSoftAdvSameAgeijt = 0 when SoftAdvijt = 0.PerFranGenreijtPercentage of franchise software introduced on platform j in the same genre as software i prior to time t out of all software introduced in the same genre as software i prior to time t.lnTotSequelsDifPlatGenreijtNatural log of the number of franchise extensions introduced in the same genre as incumbent software i on competing platforms other than platform j in time t. 1 aCorrected for inflation using the CPI-U, with January 2007 = 100. Incumbent Software Sales Model: Variable Operationalization and Equations Dependent variableFor Paths A and C, our goal is to estimate the effect of new software entrants and new platform adopters on sales of incumbent software. We operationalize sales of incumbent software as the natural log of sales of software i, corresponding to platform j, at time t (  lnSoftSalesijt  ). We measure all incumbent software on the market at time t that meet two criteria: ( 1) the software has observed sales in t and ( 2) the platform on which the software is available also has observed sales in t. The first condition ensures that we measure only active software. The second ensures that we observe the indirect (through platform sales; i.e., Path C) and direct (through competition; i.e., Path A) effects of new software on the sales of each incumbent. Independent variablesTo measure the direct impact of a new entrant via Path A, we include the natural log of the count of new entrants (  lnIntSoftjt  ,  lnIntSuperjt  ,  lnIntFranjt  , and  lnIntSuperFranjt  ) on console j at time t, operationalized as in the platform equation. However, as discussed in the conceptualization, a new entrant affects sales differently when the incumbent is in the same genre ([50]); competition will be less salient for incumbents in different genres. Thus, as controls we include the natural logs of the count of the entry variables in a different genre than incumbent software i (e.g.,  lnIntSoftDifGenrejt  ).[ 5]To assess the indirect impact of an entrant via Path C, we include the natural log of platform sales in time t,  lnNewPlatAdoptersjt  , the dependent variable from the platform equation. ControlsWe separately include the platform's installed base, operationalized as the natural log of cumulative sales of platform j prior to t (  lnPlatIBjt  ), to allow for differential effects of new platform adopters and previous platform adopters on incumbent software sales. We also include eight additional groups of control variables. First, we include the natural log of average price of software i on platform j in t adjusted for inflation using the CPI-U,  lnPriceijt  ([30]). Second, a dummy variable (  SequelIntroedijt  ) takes the value 1 if any software entrant on platform j in t is a franchise extension to incumbent software i ([33]) to control for any differential effect a new entrant may have on incumbents from the same franchise (e.g., the introduction of Tomb Raider 2 may be more likely to cannibalize incumbent Tomb Raider compared with other entrants). Third, we use the natural log of the active catalog of software for platform j similar to the platform equation, but we exclude incumbent software i from the count (e.g.,  lnAvailSoftijt  ). We add the natural logs of the active catalogs in a different genre than incumbent software i (e.g.,  lnAvailSoftDifGenreijt  ) to control for the effect of competition within and outside the genre. Fourth, we include software installed base, operationalized as the natural log of sales of software i on platform j prior to t,  lnSoftIBijt  ([53]). Fifth, we include the natural log of advertising expenditure for software i in time t, adjusted for inflation using the CPI-U,  lnSoftAdvit  . We also include  lnPlatAdvjt  to address any platform advertising spillover to software sales. Sixth, we utilize software/platform specific effects (  μij  ) to account for software-/platform-specific variation. Seventh, period fixed effects address shocks common across all software/platform observations specific to any month from January 1995 to June 2019. Lastly, we measure the age of software i on platform j (  SoftAgeijt  ) in months since introduction on the platform. Note that period fixed effects are perfectly collinear with the trend captured by software age. However, we include software age squared (  SoftAgeijt2  ) because game sales show an exponential decline with age ([10]). Period fixed effects are not perfectly collinear with  SoftAgeijt2  because software enters at different periods throughout our time frame.Similar to the platform equation, we add 1 before logging the software entrant, stock, and advertising variables to ensure that the log-transformation is defined when the level is 0. Incumbent software heterogeneityWe are interested in the effect of incumbent superstar, franchise, and superstar-franchise status on the impact of new software entry and new platform adopters. We let  Superij   = 1 if software i on platform j is a superstar but not part of a franchise, 0 otherwise;  Frani   = 1 if it is part of a franchise but not a superstar;  SuperFranij   = 1 if it is both a superstar and part of a franchise. Incumbent software sales equationThe functional form for the software equation is lnSoftSalesijt=δIntSoftijlnIntSoftjt+δIntSuperijlnIntSuperjt+δIntFranijlnIntFranjt+δIntSuperFranijlnIntSuperFranjt+δNewAdoptersijlnNewPlatAdoptersjt+∑s=1SδlagslnSoftSalesijt−s+Controlsijt+ηijt, Graph( 2)where δGSij=θGS+θSuperGSSuperij+θFranGSFrani+θSuperFranGSSuperFranij+GenreDummiesi+νjGS+ωijGS Graph( 3)for  GS   =   IntSoft  ,  IntSuper  ,  IntFran  ,  IntSuperFran  , and  NewAdopters  and Controlsijt=δPricelnPriceijt+δSequelIntSequelIntroedijt+δIntSoftDifGenrelnIntSoftDifGenreijt+δIntSuperDifGenrelnIntSuperDifGenreijt+δIntFranDifGenrelnIntFranDifGenreijt+δIntSuperFranDifGenrelnIntSuperFranDifGenreijt+δASoftlnAvailSoftijt+δASuperlnAvailSuperijt+δAFranlnAvailFranijt+δASuperFranlnAvailSuperFranijt+δASoftDifGenrelnAvailSoftDifGenreijt+δASuperDifGenrelnAvailSuperDifGenreijt+δAFranDifGenrelnAvailFranDifGenreijt+δASuperFranDifGenrelnAvailSuperFranDifGenreijt+δSoftIBlnSoftIBijt+δPlatIBlnPlatIBjt+δSoftAdvlnSoftAdvit+δPlatAdvlnPlatAdvjt+δSoftAge2SoftAgeijt2+PeriodDummiest+μij. Graph PeriodDummiest  represents coefficients with their respective dummies for the period in time t. We include lags of the DV for a dynamic specification and follow [19] for the appropriate lag length (S = 1 in our case). As with the platform equation, the mutually exclusive classification of software along with the log-log specification means the coefficient on  lnIntSoftjt  is the elasticity of incumbent software sales with respect to software entry; the coefficients on the other software entry variables are moderation effects. Similarly, the coefficient on  lnNewPlatAdoptersjt  is the elasticity with respect to new platform adopters.Importantly, including the new entrant variables and new platform adopters in Equation 2 means we can interpret their impacts holding the other constant. The effect of the new entrant variables via Path A is their impact on incumbent software sales independent of new platform adoption (i.e., it represents sales to prior platform adopters). The impact of new platform adopters via Path C represents incumbent software purchases by those who just adopted the platform. Connecting Path C with Path B means that Path B–C represents the impact on incumbent software sales from new adopters spurred by new software entrants.Note the relationship between Equations 2 and 3. The coefficients  δIntSoftij  ,  δIntSuperij  ,  δIntFranij  ,  δIntSuperFranij  , and  δNewAdoptersij  from Equation 2 are random and allow for heterogeneous effects of the entry variables and platform adopters on each incumbent. Equation 3 specifies that the random coefficients are determined by the incumbent characteristics (  Superij  ,  Frani  ,  SuperFranij  ), software genre fixed effects, and platform fixed effects at the software level (  νjGS  ).[ 6]Our mutually exclusive classification for entrant and incumbent software means the constant in the estimation of  δIntSoftij  using Equation 3,  θIntSoft  , is the base impact that standard software entry has on incumbent software sales via Path A. The constants in the estimates for the other entry variables (  θIntSuper  ,  θIntFran  ,  θIntSuperFran  ) show how the base impact is moderated when the new entrant is a superstar, franchise, or superstar-franchise software. The coefficients on  Superij  ,  Frani  , and  SuperFranij  capture the moderating effect of the incumbent characteristics.Similarly, the constant in the estimation of  δNewAdoptersij  is the base impact of new platform adopters on standard incumbent sales via Path C; the coefficients on incumbent characteristics are moderating effects. Subsequently, we discuss how we combine estimates from all equations to obtain the overall net (direct + indirect) impact. We display panel-level descriptive statistics in Web Appendix Table W3. Table 2, Panel B, presents variable names and definitions. Estimation and Results Key Issues and Overall ApproachOur estimation framework addresses several issues. First, platform and software sales form a system of equations. New platform adoption and incumbent software sales are determined simultaneously; platform sales also affect the market potential of incumbent software. We jointly estimate Equations 1 and 2 to address this issue and improve efficiency by taking into account the likely correlation of error terms.Second, we allow for parameter heterogeneity in Equation 2 with random coefficients at the software/platform level. Unfortunately, time-invariant software-/platform-specific characteristics (e.g., software exclusivity, quality, release date) likely correlate with time-variant variables (e.g., software price, advertising) so a traditional hierarchical linear approach, where  μij  is modeled as random, will produce biased estimates. To address this, we use fixed effects for  μij  along with the method from [ 4] to incorporate random coefficients. However, we expand their method to consider a system of equations with endogenous variables. The Web Appendix includes the derivation along with a detailed step-by-step guide to implementation.We recover unbiased estimates of  δIntSoftij  ,  δIntSuperij  ,  δIntFranij  ,  δIntSuperFranij  , and  δNewAdoptersij  for each software/platform panel as part of the procedure. We then run auxiliary regressions of Equation 3 to find the impact of incumbent software characteristics on the random coefficients. [52] notes that a more efficient FGLS estimator is available when regressing heterogeneous coefficients in random parameter models like ours. We derive and use the FGLS estimator for Equation 3 (for details, see the Web Appendix). Endogeneity and InstrumentsThere are several endogeneity concerns in the platform and software equations. We use instruments that have a track record in the literature (e.g., [17]; [27]) or are derived from best practice ([22]; [36]). We give a detailed description of endogeneity concerns, instruments, and first-stage estimations in the Web Appendix (Tables W4 and W5). We emphasize that the usual diagnostics are always satisfied (e.g., first-stage F-stats > 10; insignificant Hansen's J). We briefly discuss the validity of key instruments used to identify our endogenous variables. New platform adoptersFirst, the software entry variables are endogenous in the platform equation. A platform becomes more attractive to software providers when more consumers adopt it; the platform becomes more attractive to consumers as it introduces more software ([48]). The theory of indirect network effects suggests platform installed base as a valid and relevant instrument. Platform installed base satisfies the exclusion restriction because its impact on new platform adoption is indirect, through its influence on software provision ([12]; [30]).Second, platform price and advertising are likely endogenous as managers consider unobservables when choosing strategies. We include producer price indexes (PPIs) from related industries to capture cost shocks that impact pricing decisions ([17]; [21]). PPIs are valid because they are not set considering unobservables that impact platform demand ([36]). We interact the PPIs with platform characteristics and other instruments to aid in identification and create platform-specific instruments ([43]).In addition, we use the average age of nonsuperstars in the active catalog to instrument for price and software entry, similar to [27]. Firms likely compensate for old catalogs with lower prices and new software. This instrument is valid if consumers are unaware of all nonsuperstar release dates when considering platform purchase, which is likely because consumers are typically unfamiliar with nonsuperstars ([ 8]).We use the average advertising expenditure of each software available on competing platforms as an instrument for platform advertising. This is valid because individual software on competing platforms is unlikely to take into account unobservables that impact focal platform demand in a coordinated and systematic fashion when choosing advertising strategy. It is relevant because it is related to software-level advertising on the focal platform ([36]) and captures relative investments in software versus platform advertising. Incumbent software salesTime-invariant software-/platform-specific characteristics (e.g., software exclusivity, quality, entry timing) likely correlate with other variables in Equation 2 (e.g., price). Fixed effects address this, as these characteristics do not change over the software's life. In contrast to the new platform adopters estimation, traditional fixed effects produce Nickell bias ([37]) in dynamic models when N ( = 13,064) is large relative to T ( = 53.48, on average).[ 7] Instead, we first-difference Equation 2 and instrument with lagged levels of the dependent variable ([ 3]).We do not consider new platform adopters or software entry to be endogenous. It is unlikely that all platform adopters in a particular period act in a coordinated fashion and account for unobservables of a particular incumbent software. Similarly, it is implausible that managers coordinate software entry systematically and consider all the individual software/platform/month unobservables that affect the performance of all software already on the market.However, software price, software advertising, and  SequelIntroedijt  likely are endogenous. Price and advertising strategies may account for unobservables that impact software sales; the launch of a franchise sequel may relate to unobservables of franchise members on the market.We exploit the panel nature of the data to construct instruments for price and advertising using values from other software on the platform when they were the same age as the focal software. For example, for advertising, if software is six months old, we calculate the variable from the advertising expenditure of previously released software when it was six months old. These instruments are relevant because they capture pricing and advertising trends based on software age. Similar instruments have been used in studies of movies ([28]) and games ([30]); they are valid, because it is unlikely that managers choose their software pricing and advertising strategies considering future unobservables of yet-to-be-released software.We use two instruments for  SequelIntroedijt  that capture trends in franchise extension introduction for similar software. First, we use the percentage of franchise software introduced out of all software introduced on the platform in the focal software's genre prior to the current period. This is relevant as it captures past trends in franchise entrants for similar software; it is valid as it is unlikely that managers coordinate franchise software entry in a systematic fashion accounting for future unobservables for all other software in the genre. Second, we use the natural log of the number of franchise extension introductions in the current period on different platforms but in the same genre as the focal software. This is valid, as it is unlikely that software on different platforms takes into account unobservables that influence focal software performance in a systematic and coordinated fashion; it is relevant because it captures franchise introduction intensity of similar software in the current period. Estimation Results and DiscussionWe assess Paths A, B, and C using the joint generalized method of moments estimation of the platform-adoption and incumbent-software sales equations, along with the heterogeneous-coefficients equation. (See estimated variance/covariance of the heterogeneous coefficients in Web Appendix Table W6.) Results for Path AWe first turn to the estimation of Path A, which captures the direct competitive impact new entrants have on incumbents. The full results for estimating incumbent software sales (Equation 2) are shown in Table A1 in the Appendix. While Table A1 is useful for understanding the impact of the control variables and model diagnostics, the coefficients for the new entry variables are not as meaningful because they are simply the average of the heterogeneous effects and do not address our research questions. The full heterogeneous effects, showing the results of every combination of incumbent/new entry characteristic (i.e., higher-order interactions), are provided in Table A2 in the Appendix. To simplify our presentation, Table 3 shows key results from Table A2 relating to the base impact that standard new entrants have on standard incumbents. Panel A shows how entrant type moderates the base effect. Entrants with superstar status and/or franchise affiliation increase their competitive edge over incumbents. While entry by standard software negatively impacts incumbent sales (β = −.0110, p < .01), the effect is larger when the new software is a superstar (β = −.0074, p < .10), part of a franchise (β = −.0128, p < .01), or both (β = −.0058, p < .05). New entrants that exhibit rich sensations and/or leverage familiarity hurt standard incumbents' sales more. However, familiarity in an entrant does not augment the impact of rich sensations; the effect of a superstar entrant is not significantly different from a superstar-franchise entrant (z = .3127, p > .10).GraphTable 3. Path A: Direct Impact of Software Entry on Incumbent Software Sales. A: Moderation of Software Entry on Standard Incumbent Software Sales by New Software Characteristics% Change in Standard IncumbentSoftware Sales Due to a 1% Increase in New Software EntryEntry by standard software−.0110*** (.0020)Moderation if entry by superstar software−.0074* (.0043)Moderation if entry by franchise software−.0128*** (.0012)Moderation if entry by superstar-franchise software−.0058** (.0029)B: Moderation of Standard Software Entry on Incumbent Software Sales by Incumbent Software Characteristics% Change in Incumbent SoftwareSales Due to a 1% Increase inStandard Software EntryImpact on standard incumbent−.0110*** (.0020)Moderation if incumbent is superstar software−.0208 (.0154)Moderation if incumbent is franchise software.0013 (.0029)Moderation if incumbent is superstar-franchise software.00043 (.0064) 2 *p < .1.3 **p < .05.4 ***p < .01.5 Notes: SEs robust to platform clustering are in parentheses.Panel B shows how incumbent characteristics moderate the impact of a standard entrant. The negative impact of a standard entrant is not attenuated if the incumbent is a superstar (β = −.0208, p > .10), part of a franchise (β = .0013, p > .10), or both (β = .00043, p > .10). Incumbents that possess rich sensations and/or familiarity do not have additional protection from direct cannibalization caused by new entrants. In addition, the combination of rich sensations and familiarity in an incumbent does not offer extra protection compared with rich sensations alone; the impact on superstar incumbents is not significantly different from the impact on superstar-franchise incumbents (z = 1.1209, p > .10). Results for Path BThe combination of Paths B and C is the indirect impact of software entry on incumbent software sales via new platform adoption. Considering Path B first, we show the platform adoption estimation (Equation 1) in Table 4. Consistent with prior research (e.g., [ 8]), we find that only new entrants with superstar status affect platform adoption. While entry by standard software does not have a significant impact (β = −.3935, p > .10), the effect is positively moderated when entrants are either superstar (β = 2.7717, p < .01) or superstar-franchise (β = 5.3839, p < .01). Combining the base and moderating effects, we find that new superstars (β = −.3935 + 2.7717 = 2.3782, p < .01) and new superstar-franchise (β = −.3935 + 5.3839 = 4.9904, p < .01) entrants positively influence platform adoption. The coefficient on superstar-franchise entrants is greater than the coefficient on superstar entrants (z = 1.7831, p < .10), which suggests that franchise status augments the effect of new superstars. However, franchise status alone does not increase platform adoption (β = −.2700, p > .10). Our results suggest that only new entrants with superstar status will impact new adopters enough to spur an indirect halo effect on incumbents.GraphTable 4. Path B: Impact of Software Entry on Platform Adoption. CoefficientSElnIntSoftjt−.3935.3308lnIntSuperjt2.7717***.9727lnIntFranjt−.2700.4524lnIntSuperFranjt5.3839***1.3533lnPricejt−.3569*.2078lnPlatAdvjt.0420*.0224lnNewPlatAdoptersjt−1.9166***.0195lnAvailSoftjt.7066***.2006lnAvailSuperjt.2441***.0456lnAvailFranjt−.7584***.2156lnAvailSuperFranjt.3879***.0947Platform, month, year, platform age, and age2 effectsIncludedHansen's J21.0896(p = .3319)N1,828 6 *p < .1.7 **p < .05.8 ***p < .01.9 Notes: SEs are robust to platform clustering; DV =   lnNewPlatAdoptersjt  ; first-stage F-statistics for endogenous variables  lnIntSoftjt  ,  lnIntSuperjt  ,  lnIntFranjt  ,  lnIntSuperFranjt  ,  lnPricejt  , and  lnPlatAdvjt  are all >10. First-stage estimates are shown in Web Appendix Table W4. Results for Path CPath C considers the impact of new platform adopters on incumbent software sales. Table 5 displays the results of the heterogeneous coefficient estimation for new platform adopters (from Table A2). Standard incumbents benefit from new platform adopters (β = .0024, p < .10). However, incumbents who are superstars (β = .0340, p < .01), part of a franchise (β = .0043, p < .05), or both (β = .0153, p < .01) benefit more. Furthermore, superstar status matters more than franchise status: the impact of new platform adopters on superstar and superstar-franchise incumbents is larger than franchise incumbents (  χ2(2)   = 6.0213, p < .05). However, franchise status does not augment the benefit to incumbents with superstar status. Our results suggest that incumbent superstars benefit more from new platform adopters when they are not part of a franchise (z = 1.7414, p < .10). While sensation-rich and/or familiar incumbents are not protected more from the direct competitive effects (Path A), these same incumbents benefit indirectly from software entry as they attract more interest from new platform adopters.GraphTable 5. Path C: Impact of New Platform Adopters on Incumbent Software Sales. % Change in Incumbent Software Sales Due to a 1% Increase in New Platform AdoptersImpact on standard incumbent.0024* (.0013)Moderation if incumbent is superstar software.0340*** (.0116)Moderation if incumbent is franchise software.0043** (.0022)Moderation if incumbent is superstar-franchise software.0153*** (.0049) 10 *p < .1.11 **p < .05.12 ***p < .01.13 Notes: SEs robust to platform clustering are in parentheses. Net overall impact (direct + indirect)We present the calculations for the overall net impact for every incumbent/new entrant combination in Table 6, Panel A, with a summary in Table 6 Panel B. Next, we walk through an example of how Table 6, Panel A, is calculated.GraphTable 6. Combining Paths A, B, and C. A: Net Overall Impact (Direct + Indirect) of New Software Entry on Incumbent SoftwareEntry By ...Impact On ...StandardSuperstarFranchiseSuperstar-FranchisePath A: Direct Impact from Increased CompetitionStandard incumbent−.0110***(.0020)−.0184***(.0048)−.0238***(.0024)−.0168***(.0036)Superstar incumbent−.0318**(.0151)−.1482***(.0359)−.0275*(.0159)−.0254(.0391)Franchise incumbent−.0097***(.0010)−.0093***(.0014)−.0076***(.0013)−.0130***(.0020)Superstar-franchise incumbent−.0106**(.0052)−.0699***(.0115)−.0238***(.0061).0040(.0120)Paths B and C: Indirect Impact from New Platform AdoptersStandard incumbent−.0010(.0009).0058(.0036)−.0016*(.0010).0121*(.0072)Superstar incumbent−.0143(.0130).0866**(.0416)−.0242**(.0105).1817**(.0800)Franchise incumbent−.0027(.0023).0161***(.0059)−.0045***(.0014).0337***(.0106)Superstar-franchise incumbent−.0070(.0062).0422**(.0188)−.0118**(.0047).0885**(.0347)Net Overall Impact = Direct + IndirectStandard incumbent−.0120***(.0022)−.0127**(.0060)−.0254***(.0025)−.0047(.0081)Superstar incumbent−.0462**(.0199)−.0616(.0549)−.0516***(.0191).1563*(.0890)Franchise incumbent−.0123***(.0025).0068(.0061)−.0121***(.0019).0207*(.0108)Superstar-franchise incumbent−.0176**(.0081)−.0277(.0220)−.0356***(.0077).0926**(.0375)B: Summary of Net Overall Impact (Direct + Indirect)Entry By ...Impact On ...StandardSuperstarFranchiseSuperstar-FranchiseStandard incumbentCannibalizationCannibalizationCannibalizationNet neutralSuperstar incumbentCannibalizationNet neutralCannibalizationHaloFranchise incumbentCannibalizationNet neutralCannibalizationHaloSuperstar-franchise incumbentCannibalizationNet neutralCannibalizationHalo 14 Notes: SEs in parentheses given by the linear and nonlinear restrictions embodied in the coefficient calculation ([19]). Cannibalization = negative and significant net impact; Net neutral = insignificant net impact; Halo = positive and significant net impact.First, we calculate the total indirect impact (Paths B and C) by multiplying the elasticity of platform adoption to software entry (Path B in Table 4) with the relevant elasticity of incumbent sales to new platform adopters (Path C in Table 5). Consider the indirect impact of a superstar-franchise entrant on a franchise incumbent. For Path B, a 1% increase in superstar-franchise entry increases platform adoption by 4.9904% (see previous explanation); for Path C, a 1% increase in new platform adopters increases incumbent franchise software sales by.00676% (=.00242 + .00434 from Table 5, p < .01). Combining both, the full indirect impact of a 1% increase in superstar-franchise entry on incumbent franchise software sales is.0337% ( = 4.9904 ×.00676, p < .01).Second, the direct impact is calculated by combining the relevant coefficients from the heterogeneous coefficient estimations. For example, the direct impact of standard software entry on franchise incumbents is −.00969% (= −.01103 + .00134 from Table A2, column 1, p < .01); this is moderated by −.00335% if the entrants are superstar-franchise software (= −.00575 + .00240 from Table A2, column 4, p < .01). Totaling, the direct impact of a 1% increase in superstar-franchise entry on incumbent franchise software is −.0130% (≈ −.00969 + −.00335, p < .05).Third, we obtain the net overall impact by combining the direct and indirect impacts, which, for our example, equates to.0207% (=.0337 + −.0130, p < .10), a significant overall halo effect. For incumbent franchise software facing new superstar-franchise entrants, the indirect impact of greater sales from increased platform adoption outweighs the direct competitive effect.Table 6 performs these calculations and displays results for each type of entrant on each type of incumbent, including higher-order interactions (e.g., superstar-franchise new entrant on a franchise incumbent). As firms' incumbent-software portfolios differ, Table 6 enables managers to estimate the direct, indirect, and total impact new entrants have on their own portfolio.Importantly, we find significant net cannibalization of incumbents by new entrants without superstar status. The negative direct competitive effect of standard and franchise entrants (via Path A) outweighs any indirect effect (via Paths B and C). Incumbent franchise or superstar status does not offer protection from cannibalization due to competition with new nonsuperstars.In contrast, the impact of superstar and superstar-franchise entrants on new platform adopters is large enough to meaningfully counteract their direct competitive effects. The impact of new superstar-franchise software results in a net halo effect for superstar, franchise, and superstar-franchise incumbents and a net neutral effect for standard incumbents. Further, while new superstars do not produce any instances of a net halo effect, the indirect impact completely offsets losses due to direct competition for all but standard incumbents. Long-term effectsSo far, we analyze effects of entry in the current period. An advantage of dynamic models is they allow us to see how a current change in a variable impacts long-term results by working through the lags on the right-hand side. We show the long-term net overall impact of all entrant types on all incumbents in the Web Appendix, Table W7. Results are similar to those in Table 6; however, effect sizes are larger. The impact of new software entry on incumbents persists long after the introduction date, and the cumulative effect is large. Robustness checksWe run robustness checks for the Equation 3 results by including an indicator variable for exclusive software, dropping genre fixed effects, and dropping platform fixed effects. We also drop software/platform observations from the counts of available software if the observation is more than three years old ([26]) and estimate Equations 1–3. We detail these checks in the Web Appendix; results are similar to those presented previously. We also check single equation estimations to ensure that results are not driven by improved efficiency of joint estimation. The sign and significance level of all coefficients are the same. Discussion and ImplicationsBy observing how thousands of new software entrants affect the sales of incumbent software, we find evidence of both cannibalization and halo effects, depending on the software attributes. We also establish specific effect sizes for the direct and indirect impacts using large-scale data. Implications for Theory and ResearchResearch in the domains of new product development and platform markets can use our findings. First, with regard to Path A, prior network market studies rarely consider the direct relationship of new software entrants with incumbent software. A common assumption is that more software stock increases the platform's installed base, which increases software sales. However, we highlight the need to incorporate competitive cannibalization effects; otherwise, the results likely overestimate the impact of new software. We also reveal contingencies that alter whether new entrants help or hurt incumbent software sales, drawing from the sensations–familiarity framework. Software characteristics moderate the effects; by applying the superstar and franchise variables in novel ways, we show that they help quantify a new entrant's impact.Second, pertaining to Path B, which has been studied in various ways in the network effects literature, our findings move beyond investigating the impact of overall software stock on platform installed base, because we treat new software entrants as new products with unique impacts on market dynamics, not simply as increases in software stock. Furthermore, by accounting for new platform adopters separately from the past installed base of consumers, we show that new software entrants have different roles for facilitating new platform adoption.Third, pertaining to Path C, we quantify the impact of new platform adopters on sales of incumbent software, above and beyond purchases by the platform's existing users. Table A1 shows that the impact of platform installed base on incumbent software sales is positive (β = .4359, p < .01). However, it is not comparable to the impact of new platform adopters on incumbent sales shown in Table 5, as a 1% increase in new adoptions represents a very different number of consumers than a 1% increase in platform installed base. We observe 168,758 new adopters on average in a month; the average platform installed base in a month is 18,261,152. A 1% increase in platform adoption is equivalent to a.009% (≈ 1,688/18,261,152 × 100) increase in platform installed base, resulting in a.0039% (≈.009 ×.4359) increase in incumbent software sales. Compared with results in Table 5, incumbent superstar (β = .0352, p < .01), franchise (β = .0028, p < .01) and superstar-franchise (β = .0138, p < .01) software benefits more from new adopters than an equivalent increase in platform installed base; standard incumbents (β = −.0015, p > .10) benefit less, though the difference is not statistically significant.[ 8]Further, by accounting for new adopters attracted by new software entrants, we can test the totality of the indirect impact through platform adoption (Paths B and C). To the best of our knowledge, no study has examined and quantified this entire indirect path. Conventional wisdom suggests that new entrants help incumbents by increasing the platform's installed base, but we clarify that only new entrants with superstar status can drive platform sales so much that they result in a net positive impact (after accounting for direct cannibalization) on incumbent sales.Fourth, we offer an extension to the sensations–familiarity framework. This theory highlights the need for a balance between sensations and familiarity, without specifying their interaction. By showing that a measure of sensations (i.e., superstar) interacts with a familiarity variable (i.e., franchise), we establish that researchers should account for the additional effects that can be created by combinations of sensations and familiarity, beyond their independent direct effects.Fifth, we offer a new approach that accounts for the autoregressive process for platform and software demand. Using [ 4] random coefficients model and fixed effects to address heterogeneity in our panel, we extend that approach to incorporate a system-of-equations estimation while also dealing with endogeneity. We thus allow for heterogeneity in the impacts of new software entrants on incumbent software sales. Finally, by applying FGLS, rather than the less efficient ordinary least squares, we discern software-specific factors associated with halo and cannibalization effects. The Web Appendix has a step-by-step guide to implement this approach. Implications for ManagersOur study shows managers how to measure new product performance holistically. Among existing approaches, there have been ""few attempts to provide measures to quantify the effects of new products on the current product portfolio"" ([46], p. 359). Practitioners often adopt ad hoc estimates of cannibalization (e.g., [ 7]; [34]). Without historical data, managers might turn to A/B testing, but that approach is expensive and also requires clean test manipulation. With our method, leveraging the elasticities in Table 6, managers can estimate the net revenue of a new entrant by including both halo and cannibalization effects, along with the new entrant's own revenue.Here is a hypothetical, numerical example in which we calculate the impact of a new superstar-franchise entrant in the current month. The median number of superstar-franchise entrants in a month is zero; we consider an increase to one. Recall that we add 1 to the entry variables before taking the natural log; thus, one superstar-franchise entrant is a 100% increase in median superstar-franchise entry given the variable transformations (i.e., going from one to two instead of zero to one). Because a 1% increase in superstar-franchise entrants leads to a.0207% increase in franchise incumbent sales (Table 6), a 100% increase in superstar-franchise entrants leads to a 2.07% ( = 100 ×.0207) increase. Rounding from the descriptives in Tables W2 and W3, an incumbent franchise game earns ∼$179,000 per month, on average, and there are ∼235 franchise games on the market per month. Thus, the superstar-franchise entrant causes an additional $870,000 (≈ $179,000 × 2.07% × 235) in revenue for incumbent franchise games. We repeat this exercise for superstar and superstar-franchise incumbents because they also experience a significant impact from superstar-franchise entry (Table 6). The total financial impact on superstar, franchise, and superstar-franchise incumbents is ∼$1.3 million additional revenue in the month of introduction. Compared with the average of $5.9 million that new superstar-franchise software earns in its first month, the halo effect of a superstar-franchise entrant is 22% (≈ 1.3/5.9) of its own revenue. Thus, measuring only the superstar-franchise entrant's earnings underestimates the total impact.Consider a similar exercise with a standard entrant. The median number of standard entrants is two. Going from two to three is a 33% increase given the variable transformations. Table 6 shows standard entrants have significant cannibalization effects; one standard entrant equates to a loss of roughly $224,000 in revenue from all incumbents. The average revenue earned by a standard entrant in its first month is roughly $525,000; this suggests that 43% (≈ 224/525) of the revenue earned is simply from cannibalizing incumbents. A manager of a standard entrant might overestimate profitability if considering only revenue from the entrant's own sales.Moreover, because halo and cannibalizations effects are contextual, we show how to assess new entrants on a case-by-case basis, while accounting for specific characteristics of both the new product and the incumbent product portfolio. Managers of platforms or software firms can leverage our methods to assess the impact of a new product introduction; for managers of firms that provide platforms and software, we show how to estimate the holistic effect on revenues earned from sales of both platforms and incumbent software. Software-only firms can predict the impacts of their own new entrants and competitor entrants on their existing portfolio sales too.Finally, platform growth strategists can look to the introduction of the right type of new software products to spur demand. Thus, it is not sufficient to recommend only the traditional viewpoints of increasing the installed base (direct network effects) or software stock (indirect network effects); instead, managers should encourage strategic introductions of the right types of software. To design such strategies, managers can apply our method to gain a holistic view of the likely performance effects of new software introductions, based on their portfolio makeup. Limitations and Further ResearchLimitations of our study suggest further research. First, we study one industry; studies should build on and confirm our findings' applicability in other contexts. The video game industry is similar to other platform industries, but our theory and findings most likely generalize to other settings in which customers ( 1) might favor one platform but use several platforms, ( 2) view specific software products as imperfect substitutes, ( 3) consume multiple software products that address similar needs (often hedonic vs. functional), ( 4) repeatedly consume favorite software products, and ( 5) eventually become satiated and seek variety. Accordingly, we anticipate that our findings might apply to markets for streaming platforms, apply somewhat to markets for ride-sharing platforms/drivers, but apply less so to markets such as health insurance and hospital networks.Second, our econometric approach does not account for psychological mechanisms that underlie consumer behaviors in response to new entrants. The lack of process measures also is a common limitation of studies that use secondary data ([ 2]). Our theory development aligns with extant literature regarding why the observed effects occur, but additional research could test the psychological underpinnings of the behaviors we observe.Third, we evaluate some important moderators in platform markets; further research could evaluate others. Studies might address other operationalizations of superstar software. Our method dichotomizes quality, but examining quality on a continuum could yield new insights. "
20,"Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping Behavior Economic conditions may significantly affect households' shopping behavior and, by extension, retailers' and manufacturers' firm performance. By explicitly distinguishing between two basic types of economic conditions—micro conditions, in terms of households' personal income, and macro conditions, in terms of the business cycle—this study analyzes how households adjust their grocery shopping behavior. The authors observe more than 5,000 households over eight years and analyze shopping outcomes in terms of what, where, and how much they shop and spend. Results show that micro and macro conditions substantially influence shopping outcomes, but in very different ways. Microeconomic changes lead households to adjust primarily their overall purchase volume—that is, after losing income, households buy fewer products and spend less in total. In contrast, macroeconomic changes cause pronounced structural shifts in households' shopping basket allocation and spending behavior. Specifically, during contractions, households shift purchases toward private labels while also buying and consequently spending more than during expansions. During expansions, however, households increasingly purchase national brands but keep their total spending constant. The authors discuss psychological and sociological mechanisms that can explain the differential effects of micro and macro conditions on shopping behavior and develop important diagnostic and normative implications for retailers and manufacturers.Keywords: business cycle; income shocks; consumer packaged goods; private label; national brand; discounter; supermarketHouseholds are subjected to constantly changing economic conditions. These changes may take place at a personal, microeconomic level, such as if the main breadwinner receives a pay raise or a household member loses a job (micro conditions). Alternatively, changes may manifest at a macroeconomic level, in terms of the business cycle, with its recurring expansions and contractions or in response to global events such as the Great Recession or the COVID-19 pandemic (macro conditions). These changing micro and macro conditions substantially affect household spending and, in turn, companies' profits. By one estimate, the Great Recession led to an average 8%, or $4,000, decrease in real annual spending among U.S. households, which amounts to $500 billion in forgone revenues ([20]).While households tend to simply postpone purchases of durable goods to times of economic prosperity ([16]; [18]), they engage in a variety of adjustments when shopping consumer packaged goods (CPGs): switching from national brands (NBs) to cheaper brands or private labels (PLs), from supermarkets to discounters, from regular to promotional prices, or decreasing the amounts purchased altogether (e.g., [17]; [43]; [46]).While research to date has focused intensively on how households adjust individual CPG shopping outcomes in response to changing macro conditions (e.g., [17]; [41]; [43]), this work takes a holistic view on households' CPG shopping behavior by uncovering how it is differentially affected by micro and macro conditions. This explicit distinction is important because changes in macro and micro conditions are not necessarily aligned. In fact, even the Great Recession, during which unemployment rates skyrocketed and housing prices and stock portfolios plummeted, did not equally affect the personal income and wealth of all demographic subgroups of the population ([37]) or all geographical regions ([17]). Similarly, the economic downturn caused by the COVID-19 pandemic implies particularly severe microeconomic consequences for industry sectors that depend on tourism, events, or gastronomy, with less effect on banking or the public sector ([48]). Of course, an income loss, for example, as result of sudden unemployment, may as well occur during prosperous economic times and be no lesser of an individual hardship.Furthermore, the consequences of changing micro and macro conditions differ considerably. Whereas changing micro conditions directly affect households' ability to purchase, changing macro conditions, all else being equal, affect only households' willingness to purchase ([39]). Accordingly, households' response to changing conditions depends on whether they are affected at a micro or macro level (or both) and may manifest in very different shopping outcomes. For example, households may alter what they purchase (e.g., NBs or PLs) and where they shop (e.g., in discounters or supermarkets), as well as how much they spend and purchase. Thus, to properly disentangle the distinct effects of micro and macro conditions and to provide differentiated implications for retailers and manufacturers, holistic observations of households' shopping behavior are crucial.We analyze a total of seven measurable and managerially relevant shopping outcomes. These outcomes reflect how households allocate their budget across brand types and store formats—their shopping basket allocation (in terms of PL and NB spending in discounters and nondiscounters)—as well as how much they spend and purchase—their shopping basket value (in terms of total spending, purchase volume, and an index of prices paid). Through the analysis, we uncover and characterize the differential effects of micro and macro conditions on households' shopping behavior by addressing the following research questions: To what extent do micro (i.e., income) and macro (i.e., the business cycle) conditions affect households' CPG shopping behavior? How do micro and macro conditions differ in terms of their effects on households' shopping basket allocation and shopping basket value? Do asymmetries exist between negative (i.e., income losses/economic contractions) and positive (i.e., income gains/economic expansions) conditions, and if so, do these asymmetries differ between micro and macro conditions?We use a unique, comprehensive data set tailored to the research objectives. Drawing on the GfK Germany ConsumerScan panel, we obtain detailed information about daily CPG transactions for more than 5,000 households in Germany over a period of eight years including the Great Recession. Drawing on this, we identify what and where households shop, how much they purchase, what prices they pay, and how much they spend. Annual surveys administered to the panel provide us with longitudinal data on households' demographics and psychographics, including micro conditions in terms of household income. In addition, the panel data enable us to control for important marketing-mix elements concerning prices, assortments, and promotional activities. We further enrich the data set with macroeconomic data from the German Federal Statistical Office and advertising data from the Nielsen Company on advertising spending by all manufacturers and retailers in the sample.The analyses show that micro and macro conditions both have a substantial impact on households' shopping behavior. Importantly, households adjust their shopping behavior without a concrete change in their budget constraints. In addition, micro and macro conditions differ substantially in their effects on households' shopping behavior. Whereas micro conditions primarily have an impact on households' basket value, macro conditions not only affect households' basket value but also cause shifts in households' basket allocation. During adverse micro conditions, households buy lower volumes and spend substantially less in total but do not shift spending to other brands or store formats. In contrast, as macro conditions change, households shift spending to PLs (from both discounters and nondiscounters) during contractions and to NBs during expansions. In addition, they increase their total spending and purchase volume during contractions. We argue that the shifts during macro conditions are driven by a greater society-wide acceptance of frugal consumption that does not emerge during changing micro conditions. These discrete effects of micro and macro conditions and the proposed underlying mechanisms have distinct managerial implications. The results also address some of the counterintuitive findings of prior studies, such as increasing total spending and purchase volumes ([46]) as well as higher prices paid ([ 8]) during the Great Recession. Related LiteratureOur study relates to business cycle research in marketing as summarized in Table 1.GraphTable 1. Literature Overview. AuthorsMacro ConditionsMicro ConditionsShopping Behavior(s)Data BasisGicheva, Hastings, and Villas-Boas (2007)Gasoline pricesSpending share of income, out-of-home consumption, promotion shares (individually)Weekly household-level consumption surveys, repeated cross-section, two U.S. regions from 2000 to 2004Lamey et al. (2007)Business cycle (asymmetries)PL shareAnnual, country-level longitudinal data, four countries spanning multiple decadesLamey et al. (2012)Business cyclePL shareAnnual, category-level longitudinal data, U.S. from 1985 to 2005Ma et al. (2011)Gasoline prices, GDP growth rateShopping trips, total spending, purchase volume, store format, brand type, price tier, and promotion shares (individually)Monthly, household-level longitudinal panel data, U.S. metropolitan area from 2006 to 2008Kamakura and Du (2012)GDP growthHousehold budgetSpending share of budgetAnnual, household-level consumption surveys, repeated cross-section, U.S. from 1989 to 2003Lamey (2014)Business cycle (asymmetries)Discounter shareAnnual, country-level longitudinal data, 15 countries, spanning 17 yearsCha, Chintagunta, and Dhar (2016)Regional unemployment levelTotal spending, purchase volume, prices paid, store format, brand type, price tier, and promotion shares (individually)Annual, household-level panel data, repeated cross-section, U.S. from 2006 to 2011Dubé, Hitsch, and Rossi (2018)(Post)recession phaseIncome, wealthPL shareMonthly, longitudinal household-level panel data, U.S. from 2004 to 2012This articleBusiness cycle (asymmetries)Income (asymmetries)Total spending, purchase volume, price index, brand type and store format shares (simultaneously)Quarterly, longitudinal household-level panel data, Germany from 2007 to 2013 1 Notes:[26] and [46] argue that changes in gasoline prices reflect changes in household budgets. We regard gasoline prices as macro effects because they are experienced simultaneously but not necessarily equally by all households, as some households may rely on their cars more than others. As such, they are more similar to macro rather than micro events.Pioneering studies in this stream show that during recessions, PL market shares ([43]) and discounter market shares ([41]) increase, and some of these effects carry over into subsequent expansion periods. [17] generally confirm these findings by analyzing PL demand at a household level, accounting for heterogeneous income and wealth effects caused by the Great Recession. They find significant short- and long-term effects on PL demand, albeit with notably smaller elasticities. [ 8] further extend the number of shopping behaviors observed. They find that unemployment caused by the Great Recession has led households to increasingly purchase products on price promotion, cheaper brands, and in cheaper store formats. Instead of traditional macroeconomic indicators, [26] and [46] use gasoline prices to operationalize changing economic conditions. They show that gasoline prices relate to a multitude of shopping behaviors such as spending, prices paid, and store format and brand type shares.In addition to macro conditions, some of the studies in the field observe households' micro conditions. However, they are either used as time-invariant demographic control variables ([ 8]; [26]; [46]) or conceptualized as direct consequences and part of macro conditions rather than distinct conditions with idiosyncratic effects ([17]). Our study thus contributes to this literature stream by delineating the distinct effects of changing micro as well as macro conditions on households' shopping behavior. Importantly, we also account for different magnitudes and asymmetries between adverse and beneficial micro and macro conditions.First insights into the differences between micro and macro conditions show that overall household spending on food products and alcoholic beverages increases during adverse macro conditions but decreases when micro conditions worsen ([38]). We complement these findings by analyzing a variety of shopping outcomes beyond overall spending, using actual purchase data (thus increasing external validity), and controlling for a large variety of confounding factors such as changes in the marketing mix that are associated with changes in macro conditions ([58]).Notably, studies to date either focus on individual shopping outcomes (e.g., [17]; [43]) or model several shopping outcomes independently from each other ([ 8]; [26]; [46]). However, households have a variety of means to adjust their shopping behavior that are also highly interdependent—for example, discounters carry substantially more PLs and fewer NBs and usually feature fewer promotions in favor of an everyday low-price strategy. As such, when households switch store formats, it almost automatically also affects their brand type and promotion shares ([11]). Failing to account for these interdependencies can overestimate the effect of changing conditions on individual shopping outcomes. Therefore, we analyze multiple shopping outcomes simultaneously, controlling for their interdependencies, and thus contribute to the literature by offering a holistic picture of micro and macro conditions' effects on households' shopping behavior. Conceptual FrameworkThe conceptual framework (Figure 1) depicts the two main components of our study: micro and macro conditions and their effect on households' shopping behavior. We observe these behaviors through concrete and measurable shopping outcomes that, in essence, boil down to households' shopping basket value (i.e., how much households purchase and at what price) and their shopping basket allocation (i.e., how households allocate their expenditures across brand types and store formats). To get a holistic picture of micro and macro conditions' effects on households' shopping behavior, we consider the various shopping outcomes simultaneously. We also control for household demographics and psychographics as well as manufacturer and retailer adjustments to the marketing mix.Graph: Figure 1. Conceptual framework. Economic Conditions: Micro Versus MacroWe analyze changing macro conditions in terms of the business cycle on the basis of gross domestic product (GDP) (e.g., [43]; [58]) and derive micro conditions in terms of households' income. Although changing macro conditions are experienced by an entire region, by a nation, or even globally, they do not necessarily affect all households at a micro level. For example, not all households may experience income reductions, job loss, or shrinking wealth during a recession ([17]). Thus, by differentiating between micro and macro conditions, we isolate the distinct effects on shopping outcomes of changes in households' ability to purchase (micro level) and their willingness to purchase (macro level) ([39]). A negative micro shock, for example, restricts some households' shopping budgets, while households that face only adverse macro conditions lack this budget constraint. Importantly, whereas changing micro conditions are usually a personal matter, changing macro conditions affect a society at large. Thus, shifts in macro conditions can alter what type of shopping behavior is considered the norm. During recessions, for example, frugal consumption such as buying PLs or visiting discounters may become socially acceptable and even fashionable ([22]; [38]).In addition, beneficial and adverse economic conditions exercise asymmetric effects on consumers' shopping behavior for several possible reasons, such as general pessimism following a recession, inertia in maintaining newly adopted habits, or the need to pay off debts that have accrued during a period of lower income ([11]; [43]). Thus, we investigate asymmetric effects by splitting micro and macro conditions into both adverse and beneficial changes. Households' Shopping OutcomesWe distinguish between a household's shopping basket value and shopping basket allocation. We examine shopping basket value outcomes in terms of a household's total budget spent, total volume purchased, and an index of prices paid that indicates whether a household purchases products below average market prices of these products, for example, through temporary price promotions. In this way, we can differentiate the degree to which households adjust how much they purchase and how much they spend. We discern shopping basket allocation outcomes by considering brand types and store formats jointly and differentiating between households' spending on ( 1) PLs in discounters, ( 2) PLs in nondiscounters (e.g., supermarkets, hypermarkets), ( 3) NBs in discounters, and ( 4) NBs in nondiscounters. Prior research has taken a similar approach to households' budget allocation, with studies distinguishing between PLs and NBs as different brand types (e.g., [ 3]; [63]; [56]) or discounters and nondiscounters as different store formats (e.g., [10]; [40]; [41]). This approach has the following conceptual merits. Brand typesRegarding brand types, PLs, NBs, and their competition have received ample attention from both academics and practitioners ([40]). PLs have evolved from pure economic options to covering all price tiers and even special segments such as organic foods ([27]; [35]). They have thus developed into major competitors for NBs; for example, in Germany they have gained a market share of 41%, with 95% of consumers buying PLs ([25]; [36]). The competition between NBs and PLs is distinct in that PLs are managed by retailers and, thus, they introduce an aspect of competition into their otherwise collaborative relationship with manufacturers through downward price pressure. However, at the same time, NBs and PLs benefit each other by increasing store traffic and reinforcing quality disparities ([24]; [49]). From a consumer perspective, NBs and PLs differ substantially. First, consumers perceive PLs as inexpensive and as a good value for money. Further, while NBs are generally still better known and are perceived as being of higher quality, PLs are catching up in terms of quality perception ([36]). These differences in terms of price and quality perceptions generally suggest that households will switch between these two brand types in response to changing micro or macro conditions. Thus, the explicit distinction between NBs and PLs is relevant for our research. Store formatsIn terms of store formats, previous research has contrasted discounters with ""traditional retailers"" ([40]; [41]), supermarkets ([10]), and large retail formats ([28]; [29]). In contrast to other formats, discounters are highly optimized for cost efficiency, resulting in a substantially different retail marketing mix: store design and product presentation are austere, consumer services are reduced to a minimum, and serviced fresh foods and baked goods counters are lacking. The assortment is typically limited, especially in terms of produce; shallow, with few alternatives in each product category; and dominated by PLs, featuring relatively few NBs. As such, discounters are able to offer substantially lower prices than other store formats at the cost of service quality ([41]; [64]).In contrast, the major nondiscount store formats, such as supermarkets, superstores, and hypermarkets, vary in floor size and assortments offered beyond CPGs (e.g., clothing, home decor, hardware) but are similar to each other in terms of prices, service quality, and CPG assortments ([40]; [41]; [64]). This is also evident from Table 2, in which we contrast market data from discount and nondiscount store formats in Germany. Therefore, distinguishing between discounters and nondiscounters is most obvious from both retailer and consumer perspectives. Despite their distinct characteristics, however, discounters and nondiscounters do not merely address different target groups but also compete directly with each other for the same consumers, as consistently argued and shown in previous research (e.g., [10]; [33]).GraphTable 2. Store Format Characteristics. Store Format# of StoresaSales Area (m2/store)aRevenues(€ mil.)aMarket ShareaSpace Prod. (€/m2)# of SKUsaSKU Prod.(€ mil./SKU)PL SharebService ScorecPrice Scorec1. Discounters16,05477969,80045.44%5,5842,29530.465.6%67.182.92. Small retailers8,7502974,8003.13%1,846—————3. Supermarkets10,90098244,90029.23%4,19611,8303.821.6%82.073.64. Superstores1,1273,46115,2009.90%3,89725,005.684.574.06. Hypermarkets8517,05118,90012.30%3,15048,870.419.6%79.177.9Discounters (1)16,05477969,80045.44%5,5842,29530.465.6%67.182.7Nondiscounters (2–5)21,6281,07383,80054.56%3,61223,2263.221.2%82.574.7 2 aSource:[21], based on 2016 data.3 bSource:[25], based on 2018 data.4 cSource:[16], based on 2018 data.5 Notes: Data are based on the German market. Aggregated values for nondiscounters based on sums or averages weighted by market shares. Service and price scores are indexes (0–100), scores for store formats are aggregates from the 12 major retail brands that were tested. We assigned retail brands to their primary store format based on industry convention and average store size: small retailers <400 m2, supermarkets 400–2,500 m2, superstores 2,500–5,000 m2, hypermarkets >5,000 m2 average sales area. Brand type and store format combinationsImportantly, we do not consider the defined brand types (NBs and PLs) and store formats (discounters and nondiscounters) in isolation but in combination. This combined view is important because the brand choice cannot be viewed independently of the underlying store format. For example, because discounters carry a larger PL share than nondiscounters, PLs are more visible to households at discounters and also compete with fewer NBs. At the same time, nondiscount formats usually offer more price tiers (e.g., economy, standard, and premium) and variants (e.g., organic, locally produced, or diet) for NBs as well as PLs within a product category than discounters ([27]; [35]). As such, PL and NB assortments differ structurally between discounters and nondiscounters, and we account for these differences by the combined consideration of these brand types (PLs and NBs) and store formats (discounter and nondiscounters). Thus, by crossing the two brand types and store formats, we obtain a parsimonious, mutually exclusive, collectively exhaustive, and meaningful conceptualization of households' shopping basket allocation. Altogether, the three shopping basket value outcomes and the four shopping basket allocation outcomes holistically cover the essence of households' CPG shopping behavior. Control VariablesWe control for household demographics, which play an important role in explaining differences in shopping baskets (e.g., [46]). In addition, we control for a set of household psychographics: price and quality consciousness, deal proneness, and out-of-home consumption preference. Psychographics control for household heterogeneity that is not necessarily captured by demographics because, for example, even households with high income may be deal-savvy or highly price-conscious ([ 2]). Such psychographics strongly resemble consumer traits that are largely stable in short-term environmental changes but also reflect long-term societal trends, cultural developments, and the process of consumer aging ([54]).As prior research has shown, retailers and manufacturers also react to macro conditions by adapting their marketing mix (e.g., [13]; [42]). We are less concerned with this relationship per se but control for adjustments in the marketing mix owing to their substantial influence on households' shopping behavior. Data Research ContextAs presented in Table 2, the German CPG retail market is split rather evenly between discounters and nondiscounters, with discounters accounting for 45% of revenues and 43% of stores.[ 6] Discounters in Germany are usually located in easily accessible and densely populated areas ([64]) and have an average sales area of 779 m2, which is slightly smaller than a typical supermarket (982 m2) and substantially smaller than superstores (3,461 m2) and hypermarkets ( 7,051 m_SP_2_sp_) ([21]). However, they carry far fewer stockkeeping units (SKUs) and offer a much larger PL share (65.6%) that typically outweighs NBs ([25]). Discounters' PL shares may vary by retailer (e.g., Aldi: 96%, Lidl: 61%), but even discounters with a relatively strong focus on NBs have a substantially larger PL share than nondiscounters (e.g., Penny: 42%, Netto Marken-Discount: 40% vs. nondiscounters: 21.2%). Discounters offer substantially lower prices but also limited service, as is evident from a study by the German Institute for Service Quality ([16]), which scores stores on the basis of their prices and service (higher scores mean better prices/service). The tested discounters received substantially higher (lower) price (service) scores than their nondiscounter counterparts. Discounters' focus on functionality rather than service is also reflected in their high space productivity (i.e., revenues per store space). Similarly, annual revenues per SKU are considerably higher in discounters (€30.4 million) than in nondiscounters (€3.2 million) ([21]).These data underline the similarity of the nondiscount store formats and their dissimilarity to discounters for the German market from both retailer and consumer perspectives, thus corroborating the previously introduced conceptual distinction between these two groups. Interestingly, this distinction is also reflected in the branding of different retail store formats in the German CPG market. For example, two major German retail companies—the REWE Group and the EDEKA Group—operate both regular supermarkets and superstores under their REWE and EDEKA umbrella brands. Their hypermarkets (REWE Center and E-Center) also incorporate many of the same brand cues. In contrast, their discounters—Penny and Netto Marken-Discount—carry retail brands that are completely distinct from their respective umbrella brand. Data SourcesTo reflect the particularities of the German CPG market, the data set draws on several sources and combines information across distinct aggregation levels. The primary data source is the ConsumerScan panel provided by GfK Germany, which includes transaction and survey data for panelists at the individual household level. As a major advantage, this panel covers private consumption comprehensively and representatively, spanning all German CPG retailers, including discounters that typically do not offer data for market research purposes through retail panels.[ 7] This data availability is particularly crucial, considering the substantial market share of discount stores in Germany (see Table 2). The panel also contains survey data for all panelists, based on self-reported annual demographic information (age, household size, and income) and psychographic measures (e.g., price and quality consciousness). In addition, we obtain data on weekly advertising spending that covers all major channels as well as all manufacturers and retailers from the Nielsen Company. Finally, we add publicly available GDP data from the Federal Statistical Office that indicate the aggregate economic condition. We thus build a unique, encompassing data set that combines behavioral measures with survey-based household demographics and psychographics, macroeconomic measures, and brand- and store-level advertising spending. Data PreparationThe initial raw data set from the ConsumerScan panel is composed of household characteristics and purchase decisions by 85,428 unique households—with 24,000 to 37,000 in any given year—that made more than 13 million shopping trips and 48 million purchases between 2006 and 2013. Purchase information is available at the SKU level for 39 product categories from 467 retailers, most of which maintain multiple stores. These products include alcoholic and nonalcoholic beverages (e.g., beer, fruit juice) and food (e.g., cereals, pasta, ice cream) as well as nonfood items (e.g., deodorants, detergents, toilet paper). For each purchased item, we have access to the unique product code, date and place of purchase, price paid, identifiers for store format, brand type, and temporary price reductions as well as specific product characteristics such as brand and manufacturer name and package size. In preparing these data, we took several cleaning and filtering steps at the purchase record and household levels. In particular, we eliminated inconsistent transaction records and households that did not remain in the panel for the entire period. This procedure is conservative and in line with prior literature (e.g., [17]). Data cleaning involved the following steps: ( 1) Removal of all cases with missing values, ( 2) removal of all cases with unusually high (more than four times the median) or unusually low (less than one-fourth the median) prices at the SKU level, ( 3) removal of all cases with SKUs purchased fewer than 25 times in the entire period.These data-cleaning steps preserved 97.4% of all observations and 96.1% of all expenditures. To exploit the analytical potential of panelists with long purchase histories and extensive survey information, we retain only households with at least one transaction per quarter (7,441 households) and full survey information from 2006 to 2013, leaving 5,101 unique households.To avoid structural differences between samples, we compared the filtered households with the remaining households in terms of shopping outcomes and demographics. Overall, we find only marginal deviations in purchase behaviors and demographic composition. Thus, we assume that the selected households with complete purchase histories are not structurally different from households with shorter or incomplete purchase histories. We also compare the filtered sample with information from the 2006 Microcensus ([15]). As in other studies using this type of data (e.g., [17]), our sample is only slightly older and has higher income, fewer single households and more two-person households, and fewer children. However, we find a sizable overlap in the distributions of the demographic variables, and we control for these demographics at the individual household level throughout the empirical analyses. Therefore, a lack of sample representativeness is not an issue. Detailed comparisons of the household samples are available in Web Appendix A. Variable Operationalization Shopping basket valueIn line with the conceptual framework, we consider multiple dependent variables to capture the two domains of shopping outcomes as exhaustively as possible. The first domain relates to a household's shopping basket value—that is, how much is spent by the focal household, as represented by three dependent variables. TotalSpendinght relates to the total CPG spending of household h at time t, measured in euros. PurchaseVolht refers to the total CPG purchase volume of household h at time t, again measured in euros. Note that a household's shopping basket typically contains products with different volume units (e.g., liters, grams, pieces) that cannot directly be combined into a total volume measure. Therefore, we follow [46] and use an average category price per volume unit from a one-year (here: 2006) initialization period and multiply it by the total equivalent volume units purchased in each category. This enables us to aggregate the purchase volume across categories. Accordingly, the resulting variable is expressed in euros. We note that any variations in this variable are caused by changes in volume and not changes in prices being paid that may result from switching between brand types and store formats. Therefore, we are able to clearly disentangle households' consumption (volume) from households' spending (value) of CPG purchases. Finally, PriceIndexht is constructed as an index ([ 1]) and compares, for household h at time t, the costs of the shopping basket at average market prices with the actual costs incurred by the household. These price differentials are considered for identical goods identified at the SKU level. As such, they do not reflect differences in the quality of goods purchased but whether specific SKUs in the basket were purchased at cheaper prices (e.g., through temporary price promotions). An index greater than 1 implies that a household paid more than average for the specific goods in its basket, and a value less than 1 implies that the household paid less than average. This variable, therefore, reflects households' cherry-picking behavior ([23]) and is not related to households' switching behavior between different brand types or price tiers. We provide further details on the construction of purchase volume and the price index in Web Appendix B. Shopping basket allocationThe second domain of shopping outcomes relates to a household's shopping basket allocation between combinations of brand types and store formats—that is, it captures how the household is allocating its budget. We measure this allocation with the dependent variable Spendingbht in terms of household h's total spending (in euros) at time t on the respective brand type–store format combination b: (b = 1) PLs in discounters (PLDisc), (b = 2) NBs in discounters (NBDisc), (b = 3) PLs in nondiscounters (PLNonDisc), and (b= 4) NBs in nondiscounters (NBNonDisc). Altogether, these four spending variables encompass each household's total spending.[ 8] Macro conditionsThe focal explanatory variables represent a household's individual micro conditions and the overall macro conditions. At the macro level, we first apply the Christiano–Fitzgerald random-walk filter ([ 9]) to the log-transformed quarterly GDP data to assess the general state of the economy itself. The extracted cyclical component of the GDP series constitutes the deviation from the economy's underlying long-term growth trend. Thus, periods with increases in the cyclical component indicate economic expansions, whereas periods with decreases indicate economic contractions. However, it is important to account for not only different phases of the business cycle but also the severity that comes with the depth of up- and downturns (e.g., [55]). To do so, we follow prior research ([43]; [58]) and define the magnitude of an expansion (contraction) period relative to the prior trough (peak) of the cyclical series, or the point in the cyclical component at which the quarter-on-quarter growth turns from negative to positive (from positive to negative). Therefore, we operationalize the symmetric measure of the business cycle (BCyclet) as changes in the cyclical component of GDP at time t relative to the prior peak or trough. In addition, to study potential asymmetries of macro conditions, we use the same operationalization to construct two semidummy variables that separately capture periods with an increase in the cyclical component relative to the prior trough as expansions (Expansiont) and periods with a decrease relative to the prior peak as contractions (Contractiont) of the economy. That is, Expansiont (Contractiont) takes values increasing (decreasing) with economic expansion (contraction) and 0 values during contractions (expansions).[ 9] Micro conditionsAt the individual level, micro conditions reflect a household's financial situation, captured by the household's monthly net income. The original income data included in the ConsumerScan panel are at a yearly aggregation level and are measured in 16 income brackets.[10] We construct a continuous income variable by taking midpoint values of these brackets in euros and transform the resulting series to a quarterly sequence (the aggregation level of the shopping outcome variables) by applying linear interpolation for each household.[11] We adjust income for inflation using the consumer price index. In line with the operationalization of macro conditions, we define micro conditions as a household's income change (IncomeChangeht) relative to its previous income peak or trough. This step enables us not only to capture income changes from one period to another but also to take the higher magnitude into account, which results from income changes along consecutive periods. Furthermore, we construct semidummy variables for positive (IncomeGainht) and negative (IncomeLossht) income changes that are equivalent to the operationalization of asymmetric measures at the macro level. Thus, IncomeGainht (IncomeLossht) is defined as the difference of the log-transformed net income at time t and the prior log-transformed income trough (peak), allowing us to account for the accumulated magnitude of income gains and losses over time. IncomeLossht and Contractiont are converted to positive values for ease of interpretation. Control variablesAs control variables, we include a household's value of the dependent variable from a one-year (here: 2006) initialization period t0 (TotalSpendinght0, PurchaseVolht0, PriceIndexht0, and Spendingbht0). In addition, we include demographics to control for household heterogeneity regarding household size (HhSizeht), age of the household head (Ageht), presence of children (Kidsht), and employment status (Unemployedht). We also include psychographic variables to control for heterogeneity in shopping-related traits and preferences in terms of quality (QualConsht) and price consciousness (PriceConsht), deal proneness (DealProneht), and preferences for eating out (EatOutht). While QualConsht and PriceConsht are based on fixed constructs provided by the GfK, we construct DealProneht and EatOutht from several survey questions. The associated items, factor loadings, and Cronbach's alphas appear in Web Appendix B, Table WB1. Demographic and psychographic controls are measured at an annual level, and we transform the psychographics to a quarterly series using linear interpolation.Finally, we include controls for the marketing mix. We compute this group of variables at different levels of aggregation as appropriate for each set of models and use household-specific product category weights to incorporate household heterogeneity ([46]). Except for the advertising measures, marketing-mix controls are based on transaction information from the ConsumerScan panel. Because we construct the marketing-mix controls on the basis of observed household transactions, we use only transaction information (e.g., prices, SKUs, price-promoted SKUs) of households that are not part of the analysis sample. Thus, we avoid potential biases resulting from nesting the transactions of these focal households into the marketing-mix controls. For the basket value models, we construct absolute measures for price (Priceht), assortment size (Assortht), price promotions (Promoht), PL share in assortments (PctPLht), and advertising spending of NBs (AdvNBt) and of store format j (with j = 1 for discounters and j = 2 for nondiscounters) (AdvStorejt), which includes advertising spending on retailer brands as well as their PLs. For the basket allocation models, the marketing-mix variables for each brand type–store format combination are computed relative to the average across all brand type–store format alternatives. Thereby, we parsimoniously account for potential cross-effects. In particular, we construct relative measures for price (RelPricebht), assortment size (RelAssortbht), price promotions (RelPromobht), PL share in assortments (RelPctPLjht), and advertising spending at the store level (RelAdvStorejt). Because advertising spending at the brand level refers to NBs only, we use it as an absolute measure.We adjust all spending and price variables for inflation using the consumer price index and advertising spending using the GDP deflator. Table 3 presents an overview of all variables and their operationalization, while Web Appendix B shows the detailed construction of the marketing-mix variables. Tables 4 and 5 provide the descriptives and correlations for variables in the shopping basket value models and shopping basket allocation models, respectively. Note the small correlations between micro and macro conditions, in support of the conceptualization of differential effects.GraphTable 3. Variable Operationalization. Variable GroupVariableOperationalizationShopping outcomesTotalSpendinghtTotal spending (in euros) by household h at time t.PurchaseVolhtTotal purchase volume by household h at time t measured in constant euros.PriceIndexhtIndex of prices paid by household h at time t.SpendingbhtSpending (in euros) by household h at time t for brand type–store format combination b.Micro- and macro conditionsBCycletDifference between the cyclical GDP component at time t and the prior trough/peak.ExpansiontDifference between the cyclical GDP component at time t and the prior trough.ContractiontDifference between the cyclical GDP component at time t and the prior peak.IncomeChangehtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough/peak.IncomeGainhtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough.IncomeLosshtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income peak.Marketing-mix controlsPricehtNet price facing household h at time t.RelPricebhtRelative net price of brand type–store format combination b facing household h at time t.AssorthtNumber of unique SKUs facing household h at time t.RelAssortbhtRelative number of unique SKUs of brand type–store format combination b facing household h at time t.PromohtNumber of price-promoted SKUs facing household h at time t.RelPromobhtRelative number of price-promoted SKUs of brand type–store format combination b facing household h at time t.PctPLhtPercentage share of PL SKUs in the assortment facing household h at time t.RelPctPLjhtRelative share of PL SKUs in assortment of store format j facing household h at time t.AdvStoretStore-level advertising spending (in million euros) at time t.RelAdvStorejtRelative store-level advertising spending of store format j at time t.AdvNBtAdvertising spending (in million euros) of NBs at time t.Demographic controlsHhSizehtNumber of persons in household h at time t.AgehtAge of the leading person in household h at time t.KidshtDummy variable, 1 if children are present in household h at time t, 0 otherwise.UnemployedhtDummy variable, 1 if the principal earner of household h is unemployed at time t, 0 otherwise.Psychographic controlsQualConshtScale indicating quality consciousness of household h at time t; provided by GfK.PriceConshtScale indicating price consciousness of household h at time t; provided by GfK.DealPronehtFive-item scale indicating deal proneness of household h at time t.EatOuthtThree-item scale indicating preference for eating out of household h at time t.Time controlsTimetContinuous variable for time t.QuarterqtIndicator variable for quarter q of the year at time t.Other controlsCopulakhtGaussian copula for marketing-mix variable k to account for potential endogeneity.InvMillsbhtInverse Mills ratio to account for potential selection effects. 6 Notes: Items, factor loadings, and Cronbach's alphas for DealProne and EatOut are presented in Table WB1 of Web Appendix B.GraphTable 4. Descriptive Statistics and Correlation Matrix for Variables in the Shopping Basket Value Models. MSD1234567891011121314151617181920212223241. TotalSpending184.44114.4012. PurchaseVol183.06115.62.9413. PriceIndex.99.05−.02−.1514. BCycle1.054.50.00.01−.0115. Expansion2.422.69.01.02.00.8716. Contraction1.372.53.01.01.01−.85−.4917. IncomeChange51.93510.04.03.03−.01−.01−.02−.0118. IncomeGain176.70341.42.00.01−.01−.01−.05−.04.8019. IncomeLoss124.77315.41−.05−.04.00.01−.02−.03−.74−.19110. Price.99.06−.11−.16.02−.13−.10.11.02.02−.01111. Assort444.89113.02.15.13−.01.03−.03−.09.02.06.03.08112. Promo217.6555.96.04.02−.01.05−.08−.17.04.14.08.07.86113. PctPL.33.04−.17−.24.05−.03−.06.00.01.02.00.72.17.18114. AdvStore254.9827.22.03.02.00.02.17.15.00−.02−.01−.02−.03−.03−.04115. AdvNB294.8346.12.00.00.00.13.08−.15.01.04.04−.02.05.10−.01.25116. HhSize2.331.13.49.52−.14.00.01.01.07.05−.07.18.06−.01.03.01.00117. Age54.3612.07−.12−.13.07−.01−.04−.02−.10−.14.01−.18.02.10−.08−.02.01−.37118. Kids.18.38.19.21−.08.00.02.01.07.06−.04.25.03−.03.12.01.00.56−.54119. Unemployed.06.24−.06−.01−.01.01.01.00−.05.01.09−.01.00−.03−.02−.01−.01−.04−.07.02120. QualCons2.94.86.03−.07.12.00.00−.01.01−.01−.03.03−.01.03.05−.01.01−.04.11−.07−.10121. PriceCons3.14.93.00.13−.28.00.00.01−.01.02.03.02.02−.01−.03.01.00.14−.11.11.08−.39122. DealProne11.262.47.10.18−.28.00.01.01.02.02−.01−.02.00−.04−.04.01.00.18−.10.11.03−.10.38123. EatOut5.412.38−.07−.10.05.01.01−.01.05.07.00−.02−.01−.02−.01−.01.00−.07−.32.09.03.03−.05−.01124. Time−.05−.05−.01−.16−.27.00.06.20.12.05.25.61.05.08.19−.04.13−.05−.04.04−.02−.03−.011 7 Notes: Means and standard deviations are based on untransformed values, correlations are based on log-transformed variables except dummy variables. BCycle, Expansion, and Contraction are multiplied by 100 to be expressed in percentage deviations.GraphTable 5. Descriptive Statistics for Variables in the Shopping Basket Allocation Models. PLDiscNBDiscPLNonDiscNBNonDiscMSDMSDMSDMSDSpending45.4746.1023.6433.2912.9816.91102.3593.18Price.76.031.17.04.73.041.34.04Assort.74.12.66.05.46.082.15.17Promo.65.13.72.05.43.082.20.18PctPL1.46.041.46.04.54.04.54.04AdvStore1.09.071.09.07.91.07.91.07AdvNB294.8346.12294.8346.12294.8346.12294.8346.12 8 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. ModelWe define regression models for the individual shopping outcomes and estimate them jointly in a system of seemingly unrelated regressions. To control for unobserved household heterogeneity, we use a random intercept specification. The three shopping basket value equations for total spending, purchase volume, and price index, as well as the four basket allocation equations for spending across four brand type–store format combinations, are specified in log-log form (excluding the dummy variables Kidsht, Unemployedht, and Quarterqt). This approach allows for an interpretation of coefficients as elasticities and accounts for the fact that households vary substantially in magnitudes of the dependent variables ([46]).[12] We first assume symmetry in each model with regard to the focal micro- and macroeconomic measures, where MacroEcont = δ1BCyclet and MicroEconht = δ2IncomeChangeht. Subsequently, we introduce asymmetric effects, where MacroEcont = γ1Expansiont + γ2Contractiont and MicroEconht = γ3IncomeGainht + γ4IncomeLossht.We provide the specifications for the shopping basket value and shopping basket allocation models subsequently. Shopping basket value modelsThe three shopping basket value models are defined as follows: ln(BasketValueaht)=αha+MacroEcont+MicroEconht+α2aln(BasketValueaht0)+α3aln(Priceht)+α4aln(Assortht)+α5aln(Promoht)+α6aln(PctPLht)+α7aln(AdvStoret)+α8aln(AdvNBt)+α9aln(HhSizeht)+α10aln(Ageht)+α11aKidsht+α12aUnemployedht+α13aln(QualConsht)+α14aln(PriceConsht)+α15aln(DealProneht)+α16aln(EatOutht)+α17aln(Timet)+∑24κq−1aQuarterqt+∑kωkaCopulakht+εaht, Graph( 1)where BasketValueaht is (a = 1) TotalSpendinght, (a = 2) PurchaseVolht, (a = 3) PriceIndexht,  αha=α0a+μha,μha∼N(0,σμ2)  , k is marketing mix variable k (k = 1, ..., K), q is quarter q in a given year (q = 1, ..., 4), and t is time period t at a quarterly level (t = 1, ..., T).We control for potential endogeneity in the marketing-mix variables resulting from unobserved shocks by including Gaussian copulas ([47]), which directly model the joint distribution of the potentially endogenous regressor and the error term through control function terms. An advantage of this method is that it does not require instrumental variables that may, as in our case given the number of marketing-mix variables across brand type–store format combinations, be difficult to find ([50]). A requirement is that the endogenous regressor is not normally distributed. Anderson–Darling tests and Kolmogorov–Smirnov tests confirm this nonnormality for all marketing-mix variables at p < .001. Given the large size of the sample, we also visually inspect quantile–quantile plots, which confirm nonnormality for all marketing-mix variables. The Gaussian copula for each marketing mix variable Xht for household h at time t is Copulaht = Φ-1[H(Xht)], where Φ-1 is the inverse distribution function of the standard normal and H(·) is the empirical cumulative distribution function of Xht. Shopping basket allocation modelsWe define the four models as follows: ln(Spendingbht)=βhb+MacroEcont+MicroEconht+β2bln(Spendingbht0)+β3bln(RelPricebht)+β4bln(RelAssortbht)+β5bln(RelPromobht)+β6bln(RelPctPLjht)+β7bln(RelAdvStorejt)+β8bln(AdvNBt)+β9bln(HhSizeht)+β10bln(Ageht)+β11bKidsht+β12bUnemployedht+β13bln(QualConsht)+β14bln(PriceConsht)+β15bln(DealProneht)+β16bln(EatOutht)+β17bln(Timet)+∑24κq−1bQuarterqt+∑kωkbCopulakbht+β18bInvMillsbht+εbht, Graph( 2)where  βhb=β0b+μhb,μhb∼N(0,σμ2)  , and the subscripts are as defined before.One issue with Equation 2 is that expenditures are zero where a household does not patronize a specific brand type–store format combination during a period. Considering only those observations with existing expenditures or adding a small constant may lead to biased estimates ([45]). This bias may be quite substantial in our case, where zero expenditures make up between 2.6% for NBs in nondiscounters and 20.8% for NBs in discounters of all the observations. To solve this issue appropriately, we follow the procedure for Type II Tobit models ([63], pp. 560–66). In a first step, we apply a probit model with a random intercept specification and pooled coefficients for brand type–store format choice. This approach allows for the fact that households may patronize multiple brand type–store format combinations. We use the same set of independent variables as in the basket allocation models and additional instrumental variables (average number of shopping trips and unique retailers visited, share of income spent on CPGs, and per capita CPG spending) for identification purposes. In a second step, we compute the inverse Mills ratio, InvMillsbht, based on the probit model results for each brand type-store format combination as InvMillsbht = φ(Xbht′η/Φ(Xbht′η), where φ is the standard normal density function, Φ is the standard normal cumulative distribution function, and η is the vector of parameters from the probit model. The inverse Mills ratio is then added for each brand type–store format combination as an additional independent variable in the basket allocation model to correct for interrelations between brand type–store format choice and spending. As before, we also add Gaussian copulas for all brand type–store format combination specific marketing-mix variables to account for potential endogeneity issues. Results Model Estimation and ValidationWe use Latent GOLD 5.1 ([59]) to estimate the seemingly unrelated regression system consisting of seven equations with a maximum likelihood approach. All the models converged before reaching the maximum number of iterations. Because we use data from 2006 for parts of the variable operationalization, we run the model on data from 2007–2013. For holdout validation, we randomly sample 500 households from the filtered data set and run the final estimations on the remaining 4,601 households. Starting with an intercept, time, and sample selection control model (Model 1), we sequentially add the dependent variable from the initialization period (Model 2); marketing-mix variables and endogeneity controls (Model 3); and demographic (Model 4), psychographic (Model 5), and symmetric micro and macro variables (Model 6). Finally, we replace the symmetric with the asymmetric micro and macro variables (Model 7). Table 6 provides an overview of the model-building process and fit statistics. Relying on the Akaike and Bayesian information criteria, Model 7 offers the best fit. We further scrutinize Model 7 for overfitting. We compare its mean squared errors and mean absolute errors between the estimation and holdout sample and find that they are very similar, showing no sign of potential overfitting.GraphTable 6. Model Building and Fit Statistics. ModelComponentsEstimation SampleParametersLLBICAICM1Intercept + Time + Sample Selection Controls−395,450791,524791,04874M2M1 + Dependent Variable from Initialization Period−287,496575,674575,15381M3M2 + Marketing Mix + Copulas−281,705564,801563,739165M4M3 + Demographics−273,889549,407548,165193M5M4 + Psychographics−270,205542,273540,852221M6M5 + Symmetric Economic Conditions−270,034542,051540,539235M7M5 + Asymmetric Economic Conditions−269,968542,036540,434249 9 Notes: LL = log-likelihood; BIC = Bayesian information criterion; AIC = Akaike information criterion. Note that only models M3–M7 can be compared to one an other as they incorporate the same set of instruments and vary only by their exogenous variables ([19]). Symmetric Effects of Micro and Macro Conditions on Shopping OutcomesAlthough the asymmetric model (Model 7) shows the best fit, we briefly present the results from the symmetric model specification (Model 6) to check for internal consistency across the two models. Table 7 provides an overview of all significant elasticities of micro and macro conditions on basket value and basket allocation measures. The complete results of the symmetric model are available in Web Appendix C, Table WC1. Overall, we find significant influences on household shopping behavior for changes in households' micro and macro conditions. However, the nature of these influences clearly varies.GraphTable 7. Overview of Significant Elasticities. Basket AllocationBasket ValueVariablePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotal SpendingPurchase VolumePrice IndexSymmetric model (M6)BCycle−.70***−.63***.27***−.06*−.06*−.01*IncomeChange.08***.07***.06***Asymmetric model (M7)Expansion−.94***−.71***.52***−.01*Contraction.36**−.32*.51***.14**.11*IncomeGainIncomeLoss−.10**−.16***−.12***−.11*** 10 *p < .1.11 **p < .05.12 ***p < .01.13 Notes: The table illustrates only significant elasticities. PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Complete results of the asymmetric Model 7 are provided in Table 8. Complete results of the symmetric Model 6 are provided in Table WC1 of Web Appendix C. Micro conditionsIn line with economic theory, we find significant positive elasticities of income change on shopping basket value in terms of total spending (δ = .07, p < .01) and purchase volume (δ = .06, p < .01). Given that these elasticities are very similar in size and both variables are representations of a household's shopping basket in euros featuring comparable means, we can deduce that the majority of the expenditure effect is merely driven by volume adjustments. In fact, these volume adjustments are mainly attributable to purchases of NBs in nondiscounters, as indicated by the significant positive elasticity of income change on NB spending in nondiscounters (δ = .08, p < .01). Importantly, we do not find any structural shifts in households' basket allocation in that households increase (decrease) spending for a specific brand type–store format combination and simultaneously decrease (increase) spending for another. Macro conditionsUnder changing macro conditions, the results are different. We find marginally significant negative elasticities of the business cycle on shopping basket value dimensions (i.e., total spending [δ = −.06, p < .1], purchase volume [δ = −.06, p < .1], and price index [δ = −.01, p < .1]). Though intuitively surprising, the results confirm previous studies showing countercyclical CPG spending behavior of households (in value and volume) along the business cycle (e.g., [46]). In addition, we also find several significant elasticities of the business cycle on households' shopping basket allocation. In particular, the elasticity of the business cycle on PL spending in discounters (δ = −.70, p < .01) and nondiscounters (δ = −.63, p < .01) is significantly negative, respectively, whereas it is significantly positive on NB spending in nondiscounters (δ = .27, p < .01). This finding indicates that, to some degree, households shift from PLs in discounters and nondiscounters to NBs in nondiscounters—and vice versa—when macro conditions change. Moreover, when shifting their basket allocation across brand types–store format combinations, households also tend to purchase items at lower prices, for example, through temporary price promotions, as indicated by the negative effect of macro conditions on the price index. Asymmetric Effects of Micro and Macro Conditions on Shopping OutcomesTable 8 shows the estimation results of the asymmetric model. For better comparability of the impact of micro and macro conditions, Figure 2 provides an overview of the asymmetric effects of micro and macro conditions on basket allocation and basket value at their respective mean values—specifically, 2.42 (1.37) for Expansiont (Contractiont) and €176.70 (€124.77) for IncomeGainht (IncomeLossht), which translates to 7.8% (5.5%) of mean income. The findings from the symmetric model are confirmed by the asymmetric model, although the asymmetric estimation results show that the underlying effects are not symmetric but differ strongly in terms of size as well as significance between beneficial and adverse conditions.Graph: Figure 2. Asymmetric elasticities at mean values for micro and macro conditions.GraphTable 8. Results of Asymmetric Model 7. VariableBasket AllocationBasket ValuePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotalSpendingPurchaseVolumePriceIndexIntercept2.6310**(1.3047)−4.4137*(2.3163)−2.0639(1.3452)4.6453***(1.0345)−1.0209(2.6274).4872(2.6826).0460(.1136)Random intercept−.4634***(.0131)−.1435***(.0308).1604***(.0241).2319***(.0260).0279***(.0107)−.0194*(.0103)−.0006(.0006)Micro and Macro ConditionsExpansion−.9387***(.1925)−.0063(.2001)−.7139***(.1816).5186***(.1132).0198(.0546).0024(.0548)−.0083*(.0048)Contraction.3578**(.1496)−.3242*(.1968).5068***(.1602).0485(.1198).1357**(.0605).1086*(.0600).0015(.0050)IncomeGain−.0341(.0413).0479(.0471).0284(.0428).0112(.0352).0183(.0206).0153(.0205).0011(.0014)IncomeLoss−.0977**(.0447).0124(.0532)−.0508(.0463)−.1567***(.0380)−.1208***(.0224)−.1053***(.0219)−.0005(.0017)ControlsDV(t = 0).3535***(.0245).2117***(.0116).3187***(.0135).5997***(.0207).5910(.0148).5719***(.0152).7263(.0162)(Rel)Price−.6183(1.2878)1.8388(2.1456)−1.5194(1.7101)−.2376(1.5296).3850(.7840).0194(.8050)−.0384(.0863)(Rel)Assort−.4142**(.1789).4310(1.1226)−.2307(.2178)1.5306***(.3764).4395*(.2495).1980(.2603).0239(.0169)(Rel)Promo.1519**(.0725)1.2602(3.2248).4613*(.2525).8275*(.4941)−.4199(.2621)−.2884(.2691)−.0300**(.0141)(Rel)PctPL−7.0982***(2.0868)5.3710*(2.9790)−1.4206**(.6413)−.1210(.2981)−.5991***(.0864)−.5763***(.0930)−.0025(.0096)(Rel)AdvStore−.2452**(.1031).0052(.1281).5303***(.1342)−.2811***(.0778).1275***(.0173).1022***(.0194).0005(.0022)AdvNB.1265***(.0453).2301***(.0763).1619***(.0594)−.2903***(.0418)−.0396*(.0223)−.0488**(.0230)−.0008(.0022)HhSize.3706***(.0479).4305***(.0297).3406***(.0272).3722***(.0276).3077***(.0154).3250***(.0164)−.0018**(.0008)Age−.1958***(.0689)−.1135*(.0632)−.1397**(.0543).1270***(.0491).0078(.0228)−.0085(.0224).0039**(.0019)Kids−.0057(.0308)−.0594*(.0334)−.0104(.0298)−.0593**(.0234)−.0148(.0133)−.0086(.0135)−.0002(.0010)Unemployed−.0617**(.0274)−.0092(.0364).0563(.0342)−.1157***(.0278)−.0533***(.0167)−.0169(.0165)−.0010(.0012)QualCons−.0224(.0262).0286(.0278)−.1167***(.0253).0860***(.0200).0291***(.0109)−.0175(.0108).0012(.0008)PriceCons.0654***(.0216)−.0814***(.0275).0516**(.0231)−.1366***(.0177)−.0583***(.0109).0018(.0110)−.0098***(.0009)DealProne.0196(.0349).2549***(.0402)−.1277***(.0349).0485*(.0268).0499***(.0153).0830***(.0152)−.0158***(.0012)EatOut−.0581**(.0243)−.0373(.0258)−.0100(.0232)−.0282(.0186)−.0273**(.0107)−.0360***(.0105).0017**(.0008)Time−.0197(.0121).0781***(.0112).0382***(.0123)−.0425***(.0090)−.0233***(.0051)−.0270***(.0056)−.0001(.0006)Quarter 2.0327*(.0188)−.0842***(.0310)−.0355(.0220).1078***(.0174).0379***(.0098).0440***(.0102).0005(.0010)Quarter 3.0154(.0134)−.0695***(.0219)−.0559***(.0157).0260**(.0102).0002(.0066).0089(.0070).0005(.0007)Quarter 4−.0026(.0136)−.0076(.0231)−.0193(.0168).1187***(.0113).0258***(.0063).0228***(.0065)−.0001(.0006)Copula (Rel)Price.0483(.0587)−.0626(.0717).0594(.0937).0506(.0458)−.0243(.0446)−.0255(.0459).0019(.0049)Copula (Rel)Assort.0010(.0284).0143(.0742).0191(.0317)−.1207***(.0297)−.0719(.0603)−.0222(.0629)−.0041(.0041)Copula (Rel)Promo.0785***(.0136)−.0515(.2441)−.0289(.0420)−.0212(.0414).0829(.0659).0607(.0676).0051(.0036)Copula (Rel)PctPL.2990***(.0623)−.0955(.0824).1306***(.0470).0149(.0228).0431***(.0103).0201*(.0109).0015(.0012)Copula (Rel)AdvStore.0063**(.0028).0001(.0048)−.0330***(.0061).0074**(.0033)−.0017(.0012)−.0023*(.0014).0003**(.0001)Copula AdvNB−.0057*(.0031)−.0018(.0047)−.0124***(.0045).0027(.0028)−.0042***(.0016)−.0031**(.0015).0001(.0001)InvMills.1907(.1656)−.2506***(.0703)−.0832(.0734)−.0260(.2121)N131,566113,092121,787139,163142,828142,828142,828Pseudo-R2.59 14 *p < .1.15 **p < .05.16 ***p < .01.17 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Standard errors are in parentheses. Micro conditionsRegarding micro conditions, we again find that micro conditions primarily have an impact on households' shopping basket value but do not cause shifts in households' shopping basket allocation. However, the results reveal substantial asymmetries between beneficial and adverse micro conditions. Most notably, income gains have no effect on households' basket value or basket allocation; only income losses show significant effects. More precisely, a 1% loss in income decreases total spending and purchase volume by.12% (p < .01) and.11% (p < .01), respectively. Owing to the similar size of the elasticities, we can again assume that expenditure reductions are largely driven by volume reductions.[13] Given that income losses show no effect on households' price index, we can rule out the notion that expenditure reductions stem from households' shopping for lower prices.Importantly in the context of income losses, we also see no evidence that households shift their basket allocation to less expensive brand type–store format combinations. Rather, we find significant negative elasticities of income losses only on NB spending in nondiscounters (γ = −.16, p < .01) and PL spending in discounters (γ = −.10, p < .05), respectively. Thereby, we can conclude that the adjustments in purchase volume—and subsequently total spending—predominantly stem from abandoning NBs in nondiscounters and PLs in discounters when income losses occur. Instead of shifting to cheaper store formats, brand types, or both, households give up the relatively more expensive NBs in nondiscounters without substituting them with cheaper alternatives such as NBs in discounters or PLs in general. This lack of substitution is also true for PLs in discounters, but in this case options for shifting to even cheaper alternatives to reduce spending are limited, and therefore, volume adjustments are households' last resort. That is, households' primary means of coping with adverse micro conditions is to reduce expenditures on specific brand types and store formats and thereby reduce shopping basket value (i.e., spending less by purchasing lower volumes) rather than adjusting basket allocation by shifting to cheaper brand types or store formats. Macro conditionsIn contrast to adverse micro conditions (i.e., income losses), economic contractions not only have an impact on households' shopping basket value but also cause shifts in basket allocation. With regard to basket value, we find a significant increase in total spending and a marginally significant increase in purchase volume when the economy contracts: a 1% decrease in GDP, compared with its prior peak, increases total spending by.14% (p < .05) and purchase volume by.11% (p < .1). As already indicated for the symmetric model, previous studies also find countercyclical buying behavior of households during adverse macro conditions ([46]).[14] The results confirm and extend these findings by showing that increased total spending and purchase volume are not the only effects during economic downturns, as contractions also cause shifts of households' shopping basket allocation. In particular, we find significantly positive elasticities of contractions on PL spending in discounters (γ = .36, p < .05) and nondiscounters (γ = .51, p < .01), respectively; as well as a marginally significant negative elasticity of contractions on NB spending in discounters (γ = −.32, p < .1). These findings suggest that households shift from NBs to PLs during unfavorable macro conditions. Although previous studies find comparable changes (e.g., [17]; [43]), the combined results further illustrate one important phenomenon: even though households purchase PLs to a greater extent, they increase total spending and purchase volume. Moreover, the results suggest that by switching from NBs to PLs, NBs are not affected by economic downturns per se, but only in the context of discounters. That is, we only find the contraction elasticity of NB spending in discounters to be marginally significant and negative.The estimated elasticities during economic expansions further substantiate that changing macro conditions cause shifts in households' shopping basket allocation. Inversely to contractions, we find significant negative elasticities of expansions on PL spending in discounters (γ = −.94, p < .01) and nondiscounters (γ = −.71, p < .01), respectively. At the same time, we find a significant positive effect on NB spending in nondiscounters when the economy expands (γ=.52, p < .01). In addition, the results show a marginally significant and negative elasticity of an expansion on the price index (γ = −.01, p < .1). This result complements the findings on households' shifts from PLs in discounters and nondiscounters to NBs in nondiscounters during favorable economic times. In fact, to keep their purchase volume and total expenditures steady while shifting to more expensive NBs, households seem to actively seek price-promoted items to keep the prices they pay low.Overall, the results show major differences in the effects of micro and macro conditions on households' shopping behavior. While favorable micro conditions show no effect at all, adverse micro conditions lead households to reduce expenditures for specific brand types and store formats, resulting in lower total spending and purchase volumes. In contrast, favorable and unfavorable macro conditions primarily result in shifts of shopping basket allocation. These results highlight the importance of separating micro from macro conditions to identify their unique properties, effects, and implications. Effects of Control Variables on Shopping OutcomesAlthough the control variables included in the asymmetric Model 7 are not of primary interest, they are important to rule out rival explanations and thus to support the causal interpretability of the main results. Therefore, we briefly summarize them here; a more detailed discussion can be found in Web Appendix C. For the most part, when significant, the effects of the included control variables are intuitive and in line with prior research. Marketing-mix variablesAs expected, we find a marginally significant positive effect of assortment size (in terms of unique SKUs) on total spending and a significant positive effect on NB expenditures in nondiscounters. We also find several effects of promotion activity (in terms of unique SKUs sold on promotion): a negative effect on the price index, a marginally significant positive effect on NB spending in nondiscounters, a positive effect on PL spending in discounters, and a marginally significant positive effect on PL spending in nondiscounters. It is noteworthy that the effects for PLs are of smaller magnitude and confirm prior research showing that retail promotions are less positive for PLs than for NBs ([57]). We also find that the share of unique PL SKUs in the total SKU assortment has a negative effect on total spending and purchase volume, suggesting that focusing too strongly on PLs can have unfavorable consequences for retailers (e.g., [ 2]). Finally, advertising at the store level has the expected positive effect on total spending, purchase volume, and PL spending in nondiscounters, while NB advertising has an expected positive effect on NB spending in discounters.However, we also note that some of the effects are counterintuitive. This is particularly true for the negative effects of assortment size and PL share in assortments, negative own-advertising effects, and positive cross-advertising effects as well as the absence of significant price effects. Varying perceptions of PLs and NBs in assortments (e.g., [ 5]; [14]; [34]), underlying advertising spillover effects ([ 4]), or potential difficulties when measuring advertising effects ([51]; [52]) may provide reasonable explanations for these findings. Counterintuitive marketing-mix coefficients may, however, also be caused by the aggregation level of the data (quarterly, national-level aggregation across many individual brands, retailers, and product categories). Demographic variablesAs expected, we find that larger households tend to spend more across all four brand type–store format combinations, spend more in total, purchase larger volumes, and maintain a lower price index. Older households typically spend less on PLs in general as well as spend marginally significantly less on NBs in discounters, but more on NBs in nondiscounters while exhibiting a higher price index. Furthermore, the results suggest that households with children spend less on NBs in nondiscounters and marginally significantly less on NBs in discounters, respectively. Households that suffer from unemployment of the main breadwinner tend to spend less in total, corresponding to fewer expenditures on both NBs in nondiscounters and PLs in discounters. Psychographic variablesIn terms of psychographics, the analyses reveal many significant effects, generally underscoring the importance of accounting for such types of consumer characteristics ([ 2]). In particular, we find that quality-conscious households tend to spend more in total, more on NBs in nondiscounters, and less on PLs in nondiscounters. In comparison, price-conscious households typically spend more on PLs and less on NBs in general, spend less overall, and exhibit a lower price index. Deal-prone households, furthermore, spend more in total, purchase larger volumes, exhibit a lower price index, and spend less on PLs in nondiscounters, but significantly more on NBs in discounters and marginally significantly more in nondiscounters. Finally, households with preferences for eating out tend to spend less overall, purchase lower volumes, but exhibit a higher price index and typically show lower spending for PLs in discounters. Robustness ChecksWe perform several robustness checks to confirm the validity of the findings by applying alternative measures and indicators for micro and macro conditions. First, we use the growth rate of real GDP (e.g., [38]; [46]) and an index of consumer confidence (e.g., [ 3]) to assess the general state of the economy. To a large extent, the results are consistent in significance, direction, and magnitude with the main symmetric model (Model 6). Second, we use first-difference specifications of micro conditions rather than differences relative to prior income peaks and troughs as in the main asymmetric model (Model 7). All effects are consistent in significance and direction, even though the elasticities are of a higher order of magnitude. Third, we introduce an individual-level measure of a household's perceived financial situation into both main models. This measure captures changing perceptions of micro conditions that are not reflected in household income (e.g., wealth). Controlling for individual financial perceptions does not alter the findings regarding income, and we can confirm all effects to be consistent in terms of significance, direction, and the order of magnitude. All significant effects of the financial perception measure itself are in line with economic theory. We present and discuss these results in greater detail in Web Appendix C. DiscussionMicro and macro conditions have significant effects on households' shopping behavior and outcomes that, by extension, may affect firm performance of retailers and manufacturers. By observing shopping basket allocation across brand types and store formats as well as shopping basket value in terms of total spending, purchase volume, and an index of prices paid, this research provides an extensive analysis of how (through shopping basket allocation) and how much (through shopping basket value) households adjust the various facets of their CPG shopping behavior. Thus, we distinguish the effects caused by micro conditions in terms of income and macro conditions in terms of the business cycle. In addition, we account for possible asymmetries between adverse and beneficial conditions. These findings, based on a rigorous modeling approach and longitudinal field data, have important diagnostic and normative value for managers and contribute to previous research on business cycle effects. We provide an overview of the results and associated implications in Table 9.GraphTable 9. Overview of Results and Implications. OutcomesMain FindingsInterpretation and ImplicationsShopping basket allocationPL discounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions. Decreases with adverse micro conditions.As social acceptance of and demand for PLs increase during contractions, discounters can narrow their price gap to NBs. This allows for more profitable price reductions that discounters should deploy cyclically to counteract shifts to nondiscounters and NBs during expansions and adverse micro conditions. Soft discounters should extend their PL portfolio during contractions.NB discounter spendingMoves cyclically with macro conditions, decreasing substantially in contractions.Buffer discounters' and manufacturers' revenue losses during adverse micro conditions. Brand managers should extend their portfolio to discounters in these conditions to counteract losses from NBs sold in nondiscounters. Especially hard discounters may profit from a larger NB portfolio.PL nondiscounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions.Allow nondiscounters to grow revenues even during contractions. Nondiscounters can use this opportunity to extend their PL portfolios to new product categories and price-tiers and strengthen their branding to counteract shifts back to NBs during expansions. As they are unaffected by increasing budget constraints, nondiscounters may adjust prices countercyclically to reap additional revenues during contractions and defend against NBs by deploying price reductions during expansions.NB nondiscounter spendingMoves cyclically, increasing during expansions. Decreases with adverse micro conditions.Are affected the strongest by adverse micro conditions. Manufacturers and nondiscounters can react to this through status appeals in their communication. As households do not switch due to budget constraints, marketers should not waste budgets on price promotions but provide ""cheap"" mechanisms that provide consumers with a sense of control and frugality such as loyalty and reward programs or (digital) store fliers.Shopping basket valueTotal spendingGrows with adverse macro conditions. Shrinks with adverse micro conditions.As long as households are not affected at a micro level, they increase their purchased volumes and total spending during contractions. Managers can leverage households' increased consumption and cognitive load from shifts in spending through larger package sizes and in-store promotions. Measures that provide a sense of control and frugality such as loyalty programs or quality and status appeals may further increase compensatory consumption. During expansions, retailers and manufacturers should utilize the increased deal proneness and price savviness through price promotions and couponing.Purchase volumeGrows with adverse macro conditions. Shrinks with adverse micro conditions.Price indexGrows with adverse macro conditions. The results uncover and juxtapose the specific effects of micro and macro conditions on shopping behavior. We find that both micro and macro conditions have pronounced effects on households' shopping behavior that are distinct from one another and asymmetric for positive versus negative conditions. Some findings are especially intriguing: micro conditions affect only households' overall consumption levels, whereas macro conditions also lead to structural shifts in households' budget allocation across brand types and store formats. In addition, during changing macro conditions, household adjust their shopping behavior even if they are not affected financially (as we control for income). In this section, we first summarize the results and subsequently discuss potential underlying psychological and sociological mechanisms before addressing interaction effects and asymmetries. Micro ConditionsAlthough no significant adjustments in shopping basket allocation or value emerge for income gains, income losses lead to a general decline in CPG expenditures. This drop is largely driven by households purchasing less and thus spending less. The overall decrease in consumption specifically affects PLs purchased in discounters and NBs purchased in nondiscounters. These findings show that, rather intuitively, budgetary constraints lead to decreased consumption, adding to extant research that has mostly taken a spending perspective (e.g., [38]). However, the absence of structural shifts in households' budget allocation is noteworthy. Theoretically, households could also reduce spending by switching to a cheaper store format or brand type, but instead they generate savings primarily through volume reductions. Macro ConditionsIn contrast, changing macro conditions evoke structural shifts in households' basket allocation. During contractions, we see expenditures for NBs purchased in discounters being reallocated to PLs purchased in discounters and nondiscounters. While this seems intuitive, it is interesting to note that this shift is accompanied by a general increase in total spending driven by households buying more. In other words, even though households switch to PLs during contractions, they end up spending more in total.During expansions, households reallocate their purchases from PLs (purchased in nondiscounters as well as discounters) to NBs purchased in nondiscounters. Interestingly, we also find that total spending and volumes purchased remain unaffected at the same time, because households focus more on getting deals, as indicated by a decline of the index for prices paid. As such, households switch to a more expensive brand type during expansions although their budget remains constant (as we control for income), which seems to be feasible as they increasingly purchase products on price promotion. Plausible Mechanisms Underlying Micro and Macro EffectsSeveral theoretical mechanisms can explain our findings. First, the findings suggest that adverse macro conditions may have a societal impact that trickles down to individual households even if they are not affected at a financial level. In trying times, frugal consumption, such as buying PLs or shopping at discounters, seems to become more socially acceptable and even fashionable ([22]; [38]), which is in line with the shifts of budgets toward PLs in (non)discounters that we observe during contractions. Just as much as frugal consumption may become increasingly commonplace during contractions, purchasing NBs may become a societal norm and is required if households want to maintain their social standing during expansions ([38]). In accordance with that norm, households seem to drop PLs in favor of NBs in nondiscounters even though they have no increase in budgets, as we see in the results. They seem to accommodate this shopping behavior by being price-savvy, shopping products on price promotion. Price promotions may also offer a welcome justification for households to abandon the PLs they have adopted during prior contractions in favor of NBs.This reasoning is also consistent with the lack of shifts in the face of adverse micro conditions, as described previously. An income loss, independent of macro conditions, is first a personal hardship rather than one shared by society. Therefore, there is not a general move to and acceptance of PLs and discounters, as in the case of adverse macro conditions ([22]; [38])—households do not switch to these cheaper brand types or store formats but instead reduce their overall consumption. In addition, income losses may weaken self-confidence and, thus, awaken a desire to bolster one's social status ([31]; [53]), which may lead households to continue buying NBs while economizing on volume to accommodate their lower income.Another explanation for these findings may lie in households' perception of the nature of micro and macro conditions. While a nationwide or global contraction is beyond households' direct control, personal income can be influenced through concrete actions. This discrepancy in the ""mutability"" of the conditions leads to different reactions in households: whereas high-mutability conditions (here: micro conditions) result in high self-regulation, planning, and prioritizing, low-mutability conditions (here: macro conditions) elicit a desire for restoration of control ([ 7]; [31]). Adverse micro conditions lead households to self-regulate by reducing their overall consumption, whereas adverse macro conditions result in a desire to restore control through actions that are perceived as more frugal (i.e., purchasing PLs). Control-restoration behaviors are also associated with compensatory consumption, such as in the form of overspending and higher food intake ([ 7]; [44]), which may explain the overall increase in household spending and which is potentially aggravated by the lack of a budgetary constraint that would limit this behavior ([62]).Other explanations of the increased consumption may lie in households' shift to PLs, which usually are associated with larger package sizes and lower product prices and which have been shown to increase consumption ([ 6]; [61]). Similarly, these factors contribute to households' purchase of increased quantities when shopping in warehouse club stores ([ 4]). In addition, adding discounter visits to a shopping trip may increase households' spending owing to self-licensing and self-control depletion ([31]). Asymmetries and InteractionsLike previous studies in the field, we find asymmetries between adverse and beneficial conditions for both micro and macro conditions. In the case of micro conditions, we find that income gains generally have no significant effects on shopping outcomes, whereas income losses do. This finding suggests that households are quick to decrease spending when income decreases but are slow to respond when income increases, potentially because they need to compensate for postponed purchases of durables or paying off debts ([11]). While contractions affect households' shopping basket value more extensively than expansions, the expansion elasticities for shopping basket allocation are mostly larger than during contractions. This response seems reasonable, as failing to keep up with one's surroundings during an expansion would translate into a loss of status, whereas not adopting a more frugal shopping behavior during a contraction implies an increase in status ([38]). In addition, we find more pronounced asymmetries between adverse and beneficial conditions at the macro level than at the micro level. Thus, adjustments in shopping behaviors may reverse more quickly when they are caused by changing micro conditions compared with macro conditions. Given that adverse macro conditions shift the societal acceptance of certain brand types and store formats, households' attitudes may change ([32]). This reasoning implies that macro conditions' effects on shopping outcomes linger longer than micro conditions, during which households engage in status-maintaining shopping behaviors. Therefore, these status-maintaining shopping behaviors may be a means to an end rather than an attitudinal shift and households would quickly discard them once conditions improve.Finally, we investigate whether micro and macro conditions and the underlying mechanisms that affect households' shopping behavior moderate each other. Thus, we perform a post hoc analysis to test for possible interaction effects for which we present complete results in Web Appendix C, Table WC3.[15] Interestingly, the main effects remain unchanged while all interaction effects are insignificant, which suggests that micro and macro conditions do not moderate each other. Thus, the results indicate that the effects and mechanisms that micro and macro conditions elicit occur independently from each other. That is, if both conditions change simultaneously, their individual effects on households' shopping outcomes work in parallel. Managerial Implications Micro ConditionsChanging micro conditions affect shopping outcomes only when households suffer income losses rather than gains, leading to a decrease in PLs purchased in discounters and NBs purchased in nondiscounters. To buffer the negative effects of when and where they expect wages to decrease, manufacturers as well as discounters can profit from listing NBs in discounters. In particular, hard discounters such as Aldi and Lidl, whose overwhelming majority of revenues stem from their own PLs, may profit from this strategy. Thus, we provide an additional perspective to the literature investigating the role of NBs in discounters (e.g., [12]). If households indeed suffer from weakened self-confidence and want to bolster their social status as a result of adverse micro conditions ([31]; [53]), NB manufacturers and nondiscounters may leverage this reaction by using status appeals in their advertising. Because adverse micro conditions lead to a general decline in consumption, retailers and manufacturers could target product categories that are affected the most with marketing-mix actions. Changing micro conditions may be especially hard for manufacturers and retailers to identify, but with increasing availability of data through loyalty cards and online shopping, managers could detect the specific shopping outcomes associated with these changes and address those households through personalized coupons and deals. Macro ConditionsChanging macro conditions substantially affect households' shopping basket allocation as well as value. Given the increased acceptance of PLs during contractions, retailers can use the opportunity to extend their PL portfolio into higher price tiers and product categories with high involvement and complexity ([56]). In addition, they may narrow their price gap to NBs and strengthen their branding to preemptively counteract households' shifts back to NBs during subsequent expansions. During expansions, they could then offer more attractive and profitable price promotions. In particular, nondiscounter PLs may get away with raising prices because they are unaffected by increasing budgetary constraints. Given the countercyclical susceptibility of PLs, retailers should adjust their assortment accordingly, reducing their PL share in expansions and increasing it in contractions. While hard discounters are especially susceptible to adverse micro conditions, soft discounters (i.e., discounters with a relatively low PL assortment share) should be aware of contractions owing to the substantial negative effect of NBs purchased in discounters and their comparatively low share of PLs that may compensate the losses.Because we control for micro conditions, the reallocation of budgets to PLs that we observe during adverse macro conditions is apparently not driven by monetary factors but instead may result from changing attitudes toward frugal consumption across society ([38]) and a desire to restore control ([ 7]). If this reasoning holds, it has important implications for managers. NBs and retailers can avoid costly price reductions that are ineffective given the lack of a more constrained budget and instead use measures that provide a perception of frugality.[16] These measures may allow households to engage in behaviors that they associate with economizing but, at the same time, are economical for the retailer or manufacturer. For example, loyalty programs can offer low price discounts and small rewards, giving households the perception that they engage in frugal consumption ([45]). Distribution of (digital) store fliers may create a sense of greater control over the planned shopping trip. In addition, communication may highlight the quality and reliability of products to reduce uncertainty and increase compensatory consumption. NB managers might also consider increasing package size, as larger package size is often associated with a lower per unit price ([ 6]). Finally, NB managers and retailers can leverage the higher cognitive load and depletion of self-control resulting from switching stores and/or brands ([60]), rendering shoppers more susceptible to in-store promotions ([31]). Limitations and Directions for Future ResearchWhen individual income is controlled for, changes in observed shopping behaviors resulting from macro conditions are clearly linked to households' willingness, rather than ability, to purchase. Potential underlying changes in attitudes and societal acceptance of certain shopping behaviors provide a conclusive basis for our argumentation. However, we do not observe these changes of attitudes in the data directly. Therefore, we encourage field experiments and laboratory studies to dive deeper into the underlying psychological and sociological mechanisms that might drive these findings. These insights can be crucial in predicting how households will change their CPG shopping in reaction to other types of macro conditions, such as a worldwide pandemic.Including demographics and psychographics, we control for household characteristics but do not account for heterogeneity in households' reaction to changing conditions, which should be addressed by future research. Heterogeneity may originate, for example, from households' differing preferences for high-quality products, with those preferring high quality potentially opting for adjustments in the volume purchased and the price paid for a good over switches to low-tier NBs and PLs. Alternatively, heterogeneity might stem from households' usual ""baseline"" shopping behavior because it influences whether and how they are able to economize during adverse conditions.Future analyses could also differentiate among different product categories, especially relating to the reduction in consumption levels caused by adverse micro conditions. Some product categories may be more essential than others and, thus, consumption may not simply be reduced ([38]). Some product categories may even experience increasing consumption—for example, as households shift from soft drinks and juices to plain water.Finally, previous research has shown that macro conditions affect marketing-mix decisions ([58]). Thus, future research could take a corporate rather than household perspective, investigating how managers detect and react to changes in micro conditions. "
21,"How Consumer Orchestration Work Creates Value in the Sharing Economy Sharing economy platforms have become increasingly popular, but many platforms do not create all the value that is possible because consumers face challenges while cocreating their experiences. The authors situate the origin of these challenges in the sharing economy's hybrid cocreation logics, which combine competing communal and transactional logics. Using a qualitative study of Couchsurfing, a platform for sharing free accommodation, the authors find that consumers engage in orchestration work to overcome cocreation roadblocks and extract greater benefits from sharing economy platforms. This orchestration work consists of many actions reflected in four overarching mechanisms: consumer-to-consumer alignment, rewiring relations, trust investment, and network experimentation. The authors connect these mechanisms to known sources of value for firms (i.e., complementarities, efficiency, lock-in, and novelty) to make recommendations for how platform firms can foster consumer orchestration work and unlock the full value of consumer cocreation in the sharing economy.Keywords: experimentation; platform firms; sharing economy; trust; value cocreationSharing economy platforms are common and growing quickly across industries ([45]). These platforms work as digital marketplaces in which consumers (peer service providers and service users) act as cocreation partners to one another, and platforms facilitate this cocreation. While increasingly popular, many platforms may not be creating all the value that is possible because their consumers face competing institutional logics (i.e., guiding principles) that make cocreation challenging. For example, platform consumers need to cocreate experiences despite differing in terms of their goals and values (e.g., Airbnb homeowners and guests may have different understandings of what ""comfortable,"" ""clean,"" or ""convenient"" means). Likewise, platform consumers need to reconcile their desire for impersonal transactions and meaningful social interactions when cocreating experiences (e.g., an Uber driver and rider may differ in whether they prefer a quiet ride or a pleasant conversation). Platform consumers need to manage the risk of cocreating with strangers (e.g., Couchsurfing hosts and guests must assess whether to sleep next to a stranger). Further, platform consumers need to find ways to create personalized experiences while collaborating with others who may also want to create their own personalized experiences (e.g., TaskRabbit ""taskers"" need to figure out how to offer their skills while attending to the specific needs of those who ask for help). We argue that consumers try to solve these challenges by engaging in orchestration work, which we define as the set of actions that consumers engage in to overcome cocreation challenges.This article explains what platform firms can do to help consumers resolve these challenges and unlock the full value of the orchestration work that consumers are willing to undertake to acquire benefits from cocreating in the sharing economy. Whereas the platform firm could also be considered a cocreation partner, here we focus on how consumers cocreate among themselves, using platform affordances, which are the opportunities for action shaped by a platform's design features. To develop these insights, we conducted a qualitative study of Couchsurfing, a sharing economy platform launched in 2004, in which consumers, through sharing free accommodation, cocreate cultural experiences of hospitality. In some platforms, the roles of peer service provider and user are clearly separate (e.g., Uber's drivers and riders); in others, platform consumers perform both roles at the same time (e.g., Tinder's consumers). We use the term ""consumer"" to capture both roles given that both parties are actively participating in the platform; if only the provider or user role is creating value, we note this.This article makes four contributions to value cocreation research and the understanding of marketing in the sharing economy. First, it introduces the hybrid cocreation logics of the sharing economy. Considering these logics, we clearly identify and outline the challenges that platform consumers face when cocreating: the challenge of heterogeneity in cocreation, the challenge of networked sociality for cocreation, the challenge of interpersonal trust, and the challenge of personalizing cocreation.Second, this article introduces consumer orchestration work, offering a novel understanding of how platform consumers overcome cocreation roadblocks as they navigate the identified challenges. It also maps orchestration actions to associated mechanisms that constitute orchestration work: consumer-to-consumer (C2C) alignment, rewiring relations, trust investment, and network experimentation. In doing so, this article highlights the role of cocreation partners and platform affordances helpful in shaping these actions and mechanisms.Third, this article addresses a missing link between value cocreation benefits to consumers and platform firms by explaining how consumer orchestration work can lead to complementarities, efficiency, lock-in, and novelty ([56])—known sources of value creation for platform firms. Drawing on these findings, we offer actionable and practical recommendations to sharing economy platforms for leveraging consumer orchestration work in support of value creation.Finally, this article provides a series of research questions to spur more work on value cocreation in digital platforms, consumer experience in the sharing economy, and sharing economy governance and policy. Overall, we offer marketers a novel framework for understanding and managing value cocreation by consumers in the sharing economy. The Challenge of Cocreating Value in the Sharing Economy The Backdrop of Competing Institutional LogicsSharing economy platforms draw on multiple, and often competing, institutional logics that prescribe goals, norms, and identities and shape how actors feel, think, and act in that context ([47]). In particular, the contrast between communal and transactional logics ([45]) and the hybrid resulting from their interplay ([52]) creates specific conditions that guide cocreation in this context. Aligned with the understanding that institutions shape value cocreation in service ecosystems ([51]), we refer to the normative cocreation principles established by institutional logics as ""cocreation logics.""Communal cocreation logics refer to the principles that guide cocreation when community-oriented partners, who are motivated by shared values and goals, interact to perform shared social practices. With roots in the personal social interactions of Gemeinschaft ([48] [1887]), communal cocreation logics usually entail relationships based on mutuality ([ 3]) and high consociality, that is, high physical and/or virtual copresence of social actors in a network ([34]). In contrast, transactional cocreation logics refer to the principles that guide cocreation when self-interested actors, who have diverse values and goals, interact through formal, contractual, and socially distanced relations ([21]). With roots in the impersonal roles of Gesellschaft ([48] [1887]), transactional cocreation logics predominantly entail one-off quid pro quo exchanges that require money or another token ([39]).The interplay of these two logics produces the hybrid cocreation logic of the sharing economy, defined as the competing set of principles that combines communal and transactional logics to guide cocreation among consumers in the sharing economy. Mutuality may be present, but most interactions are one-off, quid pro quo exchanges between strangers guided by heterogeneous values and self-interested motivations ([16]). As such, shared practices that could work as normative structures ([51]) for organizing cocreation tend to not develop. Whereas platform firms may regulate the exchange of money and services through informal contracts, these are insufficient to organize the vastly varied cocreation interactions that happen in the sharing economy ([12]). As a result, consumers face specific challenges when cocreating experiences in the sharing economy. Four Challenges Created by the Hybrid Logics of the Sharing Economy Challenge of heterogeneity in cocreationA key challenge is that platform consumers are often very heterogeneous in terms of their resources, goals, and values. As a result, consumers may end up cocreating with partners who differ largely from them, which research has found can be destabilizing. Under a communal cocreation logic, consumers deal with the challenge of heterogeneity through frame alignment practices that ""facilitate the accommodation of differences, legitimize heterogeneity, and protect community continuity"" ([46], p. 1011). However, the communal approach implies that consumers are trying to find a common frame for cocreation, which is often not the case in the sharing economy ([12]). Challenge of networked socialityOnline platforms afford intense, highly consocial connections among consumers ([34]; [54]). However, because a hybrid logic is operating, these relations are more self-interested and transactional than those of communal sociality ([22]). As a result, consumers need to navigate hybrid relationships with cocreation partners who are situated between the close, identity-shaping relations of communities and the often impersonal, transactional relations of commercial settings. Although this is a ubiquitous challenge in relations mediated by digital platforms ([22]), it becomes much more crucial in hybrid cocreation logics in the sharing economy. Prior research shows that digital platform consumers navigate tensions that emerge from contrasting logics of sociality in the sharing economy ([52]). However, it remains unclear how consumers use platform affordances to establish relationships without relying on communal logics. Challenge of interpersonal trustSharing economy platforms are environments of distributed trust that are underscored by digital anonymity and/or ambiguity ([ 5]). Guided by hybrid logics, consumers cocreate experiences with strangers without assurance that informal regulation mechanisms, such as reputation ratings, are effective in reducing opportunism ([12]; [45]). This requires that consumers engage in ""trust leaps"" ([ 6]) as they cocreate with others on the platform. It is challenging, then, for consumers to assess the integrity and reliability of potential cocreation partners in ways that reduce the risk of cocreating experiences with strangers in one-off exchanges. One such risk is potential exploitation by cocreation partners, who may act on the basis of divergent values and norms. This challenge becomes even more pressing when cocreation involves intimate experiences that demand higher levels of interpersonal trust, such as sharing a home or going on a date. Prior research on the sharing economy (e.g., [29]) has highlighted that trust develops as a combination of relational and calculative aspects, and that cocreation partners should work to cultivate trust. However, it is unclear how such work unfolds. Challenge of personalizing cocreationGuided by hybrid cocreation logics, sharing economy consumers pursue personalization and continuously reconfigure resources to accommodate their shifting individual desires ([36]; [37]). Whereas cocreation partners often expect interactions to lead to personalized experiences, the cocreated nature of these experiences demands that personalization be sought through collaboration with others. This leads to personalization roadblocks. Prior research indicates that individuals strive to tailor their cocreated experiences to their specific interests through self-directed customization ([28]). However, it is unclear how consumers navigate personalization roadblocks during cocreation when their partners are guided by different values or have conflicting goals.These challenges become more pressing when direct marketing controls (e.g., service standards and quality checks) by the platform provider are difficult to implement (e.g., when experiences happen mostly offline or outside of the platform's control), or are not desirable (e.g., when nonstandardized experiences are part of the value proposition). Given these challenges to value cocreation in the sharing economy, we develop an empirically grounded framework of consumer orchestration work aimed at addressing these challenges. Our study is motivated by the following question: How do consumers navigate the challenges of cocreating experiences in the sharing economy, especially in the absence of direct platform firm control? Research ContextCouchsurfing is often considered ""the original sharing economy platform"" ([44], p. 38). Like other hospitality platforms (e.g., Airbnb, HomeAway), Couchsurfing connects hosts (i.e., platform consumers who are offering accommodation) with guests (i.e., platform consumers searching for a place to stay), and the term ""couchsurfer"" is often used to define all platform consumers independently of the role they take at each interaction. Like many consumer collectives in the sharing economy, exchanges and interactions in the Couchsurfing network are facilitated by an online platform that also keeps track of couchsurfers' reputations ([12]). Couchsurfing is nevertheless considered a purer, more authentic collective than other platforms, such as Uber and Airbnb, because this platform firm only minimally intervenes in how its 12 million consumers cocreate experiences ([41]). At its inception, Couchsurfing was largely guided by the principles of mutuality and generalized exchange ([ 3]), with consumers sharing free accommodation and collaborating to design and maintain this nonprofit platform. Since 2011, however, the platform has been managed by Couchsurfing International Inc., a for-profit organization that has actively pursued options, including a monthly/yearly charge, to capture the many forms of value that its consumers cocreate. Changes in the platform's configuration and its recruiting of consumers have led to the proliferation of quid pro quo exchanges on it ([12]) and the implementation of advertisements and freemium features, characterizing it as a hybrid economy ([39]). Similar to the way that some Airbnb consumers are guided by the logic of hospitality, treating their guests as friends ([52]), some couchsurfers subscribe to both communal and transactional logics, seeking friendships while keeping tabs on what they give and receive when cocreating through the platform.For the platform firm, the challenge in capturing value is that ""the actual time that the two (or more) Couchsurfing partners spend together [and which] constitutes the most important part of the Couchsurfing experience"" ([20], p. 510) happens offline, in intimate home settings, and is largely outside the platform firm's direct control. Thus, in contrast to other platforms in the sharing economy, where the platform firm exerts some control over the setting of the experience (e.g., Uber determines the characteristics of the cars in its fleet, Airbnb checks the features of the properties it lists), Couchsurfing presents an extreme case of how the orchestration of experiences unfolds when it is led by consumers. If seen from [34] framing, Couchsurfing offers consumers low platform intermediation and high consociality. Couchsurfing relies little on the standardization afforded by commercial mechanisms (e.g., prices are not available as a parameter for comparing potential hosts, the absence of set rules for check-in/out times requires negotiation between cocreation partners), and the platform's technical features are limited (e.g., guests cannot browse a map of hosts). Thus, experiences cocreated by couchsurfers are extremely heterogeneous, making consumer orchestration actions more necessary, frequent, and salient.Participants consider their Couchsurfing experience in terms of a stay[ 7] (including pretrip, on-trip, and posttrip expectations, interactions, and responses). Before traveling, a couchsurfer searches the platform for hosts to find accommodation or other couchsurfers to socialize with at the travel destination. Couchsurfers then select potential hosts and exchange messages on the platform or elsewhere (e.g., via social media or SMS) to get to know each other better and plan the stay. When a stay is confirmed, the host and guest continue to interact online before their first face-to-face meeting, which usually takes place in a public place. At home, hosts may choose to share their house keys with guests, cook for them, or include them in their household routines. Often, hosts act as local guides and give insider tips to guests, enabling them to experience a place from the perspective of a local. After a stay, the Couchsurfing platform invites guests and hosts to log their experiences on the platform by writing and posting references within 14 days. References are posted on a couchsurfer's profile and classified as would stay again or would not stay again. If a participant does not leave a reference, the stay is not registered on the platform.Consistent with consumers' behavior on other sharing economy platforms, couchsurfers may assume different roles, depending on their goals, interests, and their available resources. Couchsurfers may exclusively host, exclusively stay at other members' houses when traveling, both host and stay, or neither host nor stay, instead simply interacting with other couchsurfers in hangouts (i.e., a feature that allows members to easily find nearby couchsurfers to meet up with) or at events (i.e., gatherings organized regularly by couchsurfers). Research Methods Data CollectionTo investigate how platform consumers orchestrate their experiences and cocreate value in the sharing economy, we used a multimethod approach that combines netnography, participant ethnographic observations, and interviews. Netnography is a qualitative method for investigating online groups, communities, and cultures for marketing purposes ([22]). It requires participant observation in existing online environments and allows the researcher to unobtrusively collect data on consumers' culturally embedded experiences.By observing and participating through Couchsurfing and related websites, as well as social media platforms, we developed familiarity with the Couchsurfing experience, as lived by consumers. We created profiles on Couchsurfing.com, downloaded the Couchsurfing application to our smartphones, logged in regularly to check couchsurfers' profiles, read discussions in Couchsurfing groups, and followed links offered by couchsurfers while writing field notes and systematically collecting data about experience cocreation. In addition to observing and interacting with couchsurfers, we created an interactive research web page that described this research project and invited consumer participation. We also searched for other content created by couchsurfers, such as posts on Reddit, YouTube, and blogs. The netnographic research lasted 12 months (July 2016 to July 2017), but we continued accessing Couchsurfing-related websites sporadically until December 2019.The netnographic participant observation seamlessly became ethnographic when one of this study's authors organized two Couchsurfing events to meet local hosts and travelers. Through the platform, this researcher interacted with several people who planned to attend the events and then met some of them face-to-face during one event. We also posted upcoming trips on our Couchsurfing profiles and interacted with potential hosts in the cities where we were planning to travel. We met some of these couchsurfers in hangouts during their travels, stayed at others' homes, and hosted travelers who were visiting our local areas.Complementing these participant ethnographic observations, we conducted interviews with 40 couchsurfers (see Table 1). We recruited participants through this research project's web page, recommendations provided by members of our personal networks (who are couchsurfers), and snowball sampling. We developed an interview guide that enabled us to capture couchsurfers' perceptions of value cocreation through their experiences ([18]). With the help of two trained assistants, we conducted interviews in person and on Skype and voice recorded and transcribed them. We performed preliminary analyses following each interview and conducted additional interviews until new data became redundant.GraphTable 1. Description of Informants. PseudonymGenderAge (Years)OccupationCouchsurfing Role(s)Couchsurfer Since …AdamMale29Marketing managerGuest and host2010AlejandroMale25StudentMostly host2015AnnaFemale25Events producerGuest2012ArthurMale30IT developerMostly host2013BrimaFemale65Social workerGuest and host2005CallumMale26StudentGuest2015CarolinaFemale20ArtisanGuest2016CatalinaFemale29LawyerHost2013DanielMale32SalesHost2016DavidMale30CookGuest2011DeboraFemale25Support workerMostly guest2012DennisMale55English teacherMostly host2010DidiFemale25Child protectionGuest and host2015ElizabethFemale29FreelancerHost and guest2016EricMale25Plant pathologistMostly guest2015FabioMale30TeacherMostly host2013FernandoMaleLate 20sIT technicianGuest2017FredericoMale32Geologist/street musicianMostly guest2014GastónMaleN.D.IT developerGuest and host2016GinaFemale30TranslatorGuest and host2012JennaFemale56HousewifeMostly host2012JenniferFemale23Personal trainerMostly guest2013JohnMale50EngineerMostly host2014JoséMale25Graphic designerGuest and host2013LenaFemale32WaitressMostly host2011LinusMale33Business analystGuest and host2008LucasMale38IT developerHost2010MarcMale30PsychologistGuest and host2009MariaFemale32N.D.Guest2015MarkMale37Financial adviserMostly host2009MasonMale43DoctorHost2012MillesMale33Advertising (unemployed)Guest and Host2014OlgaFemale23TeacherHost2016OliviaFemale32CommunicationsGuest and host2009PaulMale54Air force pilot (retired)Host2015RonMale29AccountantHost2014RoxanaFemale23StudentGuest2017SamuelMale37LawyerHost2012TeresaFemale26NurseGuest2017ZikaFemale23StudentGuest2012 1 Notes: IT = information technology; N.D. = not disclosed.Finally, we systematically searched for media reports about Couchsurfing and books written by couchsurfers. Although the media reports contain important information on how the general public has perceived Couchsurfing over time, the books written by couchsurfers contain detailed descriptions of the experiences they cocreate in the network (see the Web Appendix). Through this extensive fieldwork, we amassed a large volume of data in multiple formats, such as field notes, texts, videos, pictures, and audio files. A trained research assistant downloaded data related to value creation and experiences from the web pages we had observed and prepared them for coding and analysis using qualitative data analysis software. Data AnalysisWe engaged closely with the entire data set. Consistent with inductive reasoning from grounded theory ([ 8]), we initially conducted emergent coding to identify how different aspects of Couchsurfing's consumer experiences create value for consumers and the platform firm. This approach made salient the multiple actions that consumers enact as they attempt to overcome the cocreation challenges. We refined our analysis and clustered these actions into types by identifying the similarities and differences and by noting the challenges consumers seemed to be solving when enacting each action. At this stage, it became clear to us that consumers were going beyond just collaborating with others to cocreate value for themselves and the community ([40]). They purposefully found workarounds for the challenges they faced when cocreating. This led us to use ""orchestration"" and ""orchestration work,"" which refer to the coordination of cocreation by multiple actors ([19]) as sensitizing concepts for discussing the data. Thinking about orchestration work as a series of actions allowed us to identify theoretically meaningful patterns that were reciprocally adjusted to the literature ([24]).In searching for patterns among how consumer actions worked to address the challenges of cocreation, we identified four orchestration mechanisms. We purposefully searched for cases that could dispute our framework, discussed discrepancies, and used several descriptors until we settled on those that clearly delineated the data patterns. In doing so, we adjusted our categories to progress toward a theoretical understanding of the phenomenon in a way that was consistent with the data.Finally, with this grounded understanding of consumer orchestration work, we reconnected to existing theories of consumer experience, value cocreation, and the sharing economy, and then examined the premise that consumer orchestration actions could be mapped onto the known sources of value creation for firms ([56]). This last stage was an iterative process in which we independently classified excerpts, discussed data exemplars, and used different forms of data as triangulation tools while adjusting their interpretations to the literature ([43]). How Consumers Orchestrate Experiences to Cocreate Value in the Sharing EconomyAs consumers attempt to address the challenges of cocreating experiences in the sharing economy, they engage in multiple, often overlapping actions. These actions and their underpinning mechanisms constitute consumer orchestration work, as they assist consumers in overcoming challenges to cocreating unique, valuable experiences for themselves. We identified four mechanisms of consumer orchestration work: C2C alignment, rewiring relations, trust investment, and network experimentation (see Table 2), clustering consumer actions into groups according to the key challenges that these actions aim to address. In this section, we introduce these mechanisms and their respective orchestration actions and illustrate them with examples from our data set. For the sources of numbered quotes cited in this section and additional examples, see the Web Appendix.GraphTable 2. Challenges, Roadblocks, Orchestration Mechanisms, and Actions. Cocreation ChallengesCommon Sharing Economy RoadblocksOrchestration MechanismsTypes of Orchestration ActionsChallenge of Heterogeneity in CocreationHow can consumers cocreate experiences with partners who differ in resources, goals, and values?Need to select suitable partners for cocreationC2C Alignment:Enables platform consumers to align experiential elements (i.e., expectations, interactions, and responses) with those of heterogeneous cocreation partners.ScreeningNeed to better understand current cocreation partners' preferences and expectations about cocreating experiencesCueingNeed to accommodate variability in the interactions with cocreation partnersFlexingNeed to reduce the impact of mismatches and unclear preferences on cocreated experiencesBufferingChallenge of Networked Sociality for CocreationHow can consumers reconcile communal and transactional relations with other platform consumers?Need to establish desired connections with potential partners within the platform environmentRewiring Relations:Enables consumers to use platform affordances to navigate and integrate the communal and transactional aspects of their relations.Interest groupingNeed to reconnect cocreated experiences with individual and collective identity projectsLifestyle signalingNeed to establish closer relationships within the platform collectiveEnclavingNeed to balance transactional and communal relations within the platform collectiveReconcilingChallenge of Interpersonal TrustHow can consumers reduce the risk of cocreating experiences with strangers in one-off exchanges?Need to signal one's integrity and assess that of potential cocreation partnersTrust Investment:Enables consumers to manage platform resources to mitigate the risk of engaging in one-off interactions with strangers.RevealingNeed to signal the consistency of one's cocreation behaviorCultivating reviewsNeed to establish a higher level of interpersonal trust when cocreating riskier or intimate experiences with strangersScaffoldingChallenge of Personalizing CocreationHow can consumers explore new opportunities for collaborating with strangers to cocreate unique experiences through sharing economy platforms?Need to expand cocreation beyond the limits established by available resourcesNetwork Experimentation:Enables consumers to try new resources, roles, and goals when cocreating experiences and thus extends the possibilities for the cocreation of unique, personalized, and valuable experiences in the network.Creative resourcingNeed to expand cocreation beyond the limits established by normative roles present in the platformRole improvisingNeed to stretch platform usage to meet personal goals when cocreatingRepurposing  The Mechanism of C2C AlignmentWhen platform consumers cocreate in the sharing economy, they have expectations regarding what they will be able to achieve and how. Because sharing economy consumers are highly heterogeneous, these expectations often differ largely between cocreation partners. For example, a Couchsurfing host, Mariam, struggled to understand a cocreation partner's expectations and to have her expectations fulfilled, as she shared on a Couchsurfing group:I have been hosting just for a couple of weeks and the experiences were incredible until today. I accepted a female surfer from Ukraine. She is 29 and she seemed really nice when she sent me a request. But when she arrived, she is all shy, not talkative at all. I invited her to hang around (she said no because she wanted to charge her phone, which is acceptable), I invited her for pizza which she refused, I also invited her for a beer and she said no. I have been having these amazing surfers and I am not sure what to do now. Maybe I am missing something? (Initial post on thread ""No interaction/adjusting interactions"")We uncovered several actions through which platform consumers (both guests and hosts) overcome common roadblocks to cocreation (such as those that Mariam describes) and navigate the challenge of heterogeneity. We detail these actions next, grouping them into similar types, as illustrated by examples from our data set.""Screening"" refers to actions that help consumers select the desired cocreation partners for an experience. These actions include applying search filters (""As a single guy, I do filter out if they're traveling with their partner"" [ 1]), checking social media profiles (""For those with no references, I try to connect through Facebook or something where I can verify their IDs"" [ 2]), or validating references and identity (""Do you know Bemelieu? He PMd me for a couch"" [ 3]), among other actions.""Cueing"" refers to actions that cocreation partners or potential partners undertake to orient the cocreation of their experiences in desired ways. Cueing includes orchestration actions such as listing in one's profile expectations regarding partners (""I love cooking! We can cook together and share good recipes"" [ 4]), messaging before a stay to fine-tune expectations (""Hi again, just so you know, I am working from eight to five but we have a spare key that you can borrow"" [ 5]), and enacting welcoming rituals (""When someone arrived, someone in the house would show them around the house quickly, say 'if you can see it, you can use it,' and hand them a copy of the keys"" [ 6]), among other actions.""Flexing"" refers to actions that imply concessions and adaptations that cocreation partners make concerning one another while interacting to cocreate their experiences. This group of orchestration actions includes proposing or accepting changes to planned activities (""Minni also hosted me a night as I was stuck in SF, even though his profile was on 'no couch.'"" [ 7]), requesting/offering additional resources from/to a partner (""I said: OK, if you want to stay longer, stay, no worries"" [ 8]), and making adjustments to one's environment, habits, or routine to accommodate partners' needs (""I had a host who gave me and my sister his bed and slept on the living room couch because he didn't want to disturb us when he [left] to work early in the morning"" [ 9]), among other actions.""Buffering"" refers to actions taken by one or more partners to overcome heterogeneity impediments to value cocreation and to attenuate the potential loss of reputation due to setbacks in their cocreation of experiences with heterogeneous partners. Buffering includes actions such as establishing boundaries (""I tried to kiss her. She, was like, 'No, no, no—I don't want to make it awkward.'"" [10]), apologizing (""I apologized for missing dinner that they had cooked for us"" [11]), offering peacemaking gifts (""One girl, despite being certifiably crazy and her stay at my home being a little slice of hell, showed up with free passes to … Dirty Dancing at a theater near my work"" [12]), or impeding further deterioration by ending the cocreation (""I've cut visits short, and only once have I just kicked someone out but that was due to some egregious s—"" [13]).These orchestration actions happen at the micro-level of cocreation, that is, within a particular cocreation experience. They allow consumers (both hosts and guests) to identify potential partners who are likely to be aligned with them and adjust to cocreation partners who are not. To capture this function, we categorized screening, cueing, flexing, and buffering actions under the mechanism C2C alignment. We define C2C alignment as the mechanism that enables platform consumers to overcome the challenge of heterogeneity by aligning experiential elements (i.e., expectations, interactions, and responses) with those of heterogeneous cocreation partners.Take Hilkka, for example, a host who shared her cocreation knowledge in response to Mariam's question in the Couchsurfing ""Advice for Hosts"" group:Everybody has different expectations. That's why I tell in my profile [cueing] that I like independent surfers who manage on their own in the downtown. Before I used to guide my guests, taking them to the city center and showing them around for a couple of hours. Having done sightseeing many, many times over the years, I appreciate it if my surfer will do it without me. If it seems that my surfer enjoys my company (and vice-versa), we spend more time together, otherwise it's enough for me to have some interaction in the evening, as well as in the morning at breakfast. There have been surfers who are a little shy, not too willing to have a chat with me, but I don't mind as long as they are friendly and respectful [flexing]. It is difficult to say how much (or little) interaction is good, it varies from case to case. It depends on the surfer and me, how we feel and get along, on mutual interests etc [screening]. Sometimes it's better to limit the interaction to small talk [buffering], although more often it's just great to learn what my surfer has to tell e.g., about her country, family and travels.Hilkka's post highlights several actions[ 8] that she undertakes as a host to align her expectations with those of potential and actual guests to cocreate experiences, thereby offering an illustration of how the many actions of orchestration through C2C alignment are entangled and work for the common purpose of assuaging the potential differences among cocreators. The actions highlighted in Hilkka's account, when effective, leave cocreation partners with the impression that they ""get along"" and that the cocreated experiences were ""great,"" despite the heterogeneity among cocreation partners. The Mechanism of Rewiring RelationsAs consumers partner to cocreate in the sharing economy, they develop platform-mediated relationships that are a hybrid of transactional exchanges and communal forms of sociality. Navigating such relations and shaping them in ways that are conducive to the creation of valuable experiences is challenging. As a guest, for example, Phil was on a journey through West Africa. While in Ghana, he needed information about Côte d'Ivoire, the country he would visit next. He chose to connect to other consumers on the Couchsurfing platform to obtain information:I have joined groups and introduced myself, explaining why I was traveling to a particular country or city. Most guidebooks on Cote D'Ivoire are worthless. They are filled with information that is outdated and unreliable. Some have not been updated since the country was at war. ([32])In planning his trip, Phil needed additional information, and it seemed to him that connecting to other guests and hosts on the Couchsurfing platform could address this need. However, some Couchsurfing consumers want the close, identity-shaping relations of communities; others, the impersonal, transactional relations of commercial settings; and others, a mix of both, depending on the situation. Thus, to achieve his instrumental goal of collecting information for his trip, Phil faced roadblocks associated with the need to form social relations with other couchsurfers whose social goals and interests were very different from his. Phil had to count on the platform's affordances to establish and maintain relationships in this hybrid environment. We identified four types of orchestration actions that help platform consumers overcome the roadblocks associated with the challenge of networked sociality.""Interest grouping"" refers to orchestration actions through which consumers use platform affordances to create explicit links to existing or imagined groups within the collective to access information and leverage shared values and interests. We found that the multiple interest groups hosted on Couchsurfing vary widely and can be based on identity (""Queer Couchsurfers, 53,070 members"" [14]), the purpose of an experience (""Worldwide Volunteering, 59,402 members"" [15]), information seeking (""Airlines: low-cost, budget, cheap flight, 83,504 members"" [16]), location (""South America, 32,914 members"" [17]), or needs (""Help! Need a place in Lyon for today"" [18]), among other themes.Most group participants are not committed to frequent participation, and most threads are started by new members. Unlike online communities, these groups do not cultivate communitas ([50]) or foster a sense of belonging among members. Instead, these fleeting associations are appealing because they help sharing economy participants locate sources of complementarities in the larger Couchsurfing network. For instance, those traveling to new places can pool local information and resources for cocreation that would otherwise be unavailable and identify potential cocreation partners with whom they can develop closer, more personal relationships.""Lifestyle signaling"" refers to orchestration actions that use platform affordances to help consumers connect cocreated experiences to a specific lifestyle. These actions consist of a subtler form of identification than interest grouping and allow consumers to connect identity projects to experiences cocreated in a network of which each participant is, at the same time, unique and interchangeable. Under this type, we include actions such as highlighting in one's profile skills that have been developed through participating on the platform (""As a keen amateur photographer, I enjoy chronicling our Couchsurfing adventures in pictures, and introducing our guests to studio photography in our little home studio"" [19]) and showcasing identity or creative content that is based on one's experiences with the platform (""We have recently published a book about our Couchsurfing experiences"" [20]), among others.""Enclaving"" refers to orchestration actions that use platform affordances to help create ephemeral communal spaces in which consumers experience the advantages of network proximity, despite the ephemeral nature of sharing economy interactions. Similar to consumers' efforts to build a hyper community ([21]), enclaving actions orchestrate experiences around the promise of temporary but intense communality. Enclaving actions include proposing and attending hangouts with strangers (""I have two hours before my train leaves. Wanna grab a beer?"" [21]), establishing regularity in local meetings among strangers (""Weekly CS LYON Monday Meeting"" [22]), and organizing extraordinary events (""A camp is a multiday event where couchsurfers from all of the world come together and hang out for a whole weekend"" [23]).""Reconciling"" refers to orchestration actions that use platform affordances to attempt to reconcile relations based on quid pro quo and generalized reciprocity. These orchestration actions reinforce both the quid pro quo nature of exchanges and the ties among potential cocreation partners. These actions include toning down negative reviews as a way of preserving reputation and future relations while still maintaining the quid pro quo nature of the feedback (""I see a lot of people leave neutral references when they actually want to leave a negative reference but have mixed feelings"" [24]), using the platform to achieve personal goals while strengthening communal bonds (""I am hosting in my city because I am kind of new here and it is difficult to meet people"" [25]), and organizing activities out of self-interest and that enhance communal activities. Local tourist guides are often involved in organizing pub crawls via hangouts; while they do this for personal gains, they also help foster communal bonds or even seek advice from platform consumers about how to navigate conflicts between communal and transactional logics (""How do you handle situations like this [host seducing guest]? All these people seemed to like us and wanted us to stay with them. We on the other side felt in a very awkward situation"" [26]).These types of actions—interest grouping, lifestyle signaling, enclaving, and reconciling—are categorized under a mechanism we call ""rewiring relations."" We define rewiring relations as the mechanism that enables consumers to overcome the challenge of network sociality by using platform affordances to navigate and integrate the communal and transactional aspects of their relationships. The mechanism helps consumers leverage platforms' affordances and rework the sociality in the network to better suit their individual goals.Consider how Phil describes his engagement with the utility of joining Couchsurfing groups nested within the Couchsurfing platform:While I was still in Ghana, I joined the Cote D'Ivoire group and began searching through previous posts [interest grouping]…. Twenty minutes of browsing through the group and I know what to expect when traveling overland, where I should listen to reggae in Abidjan, and whether it's dangerous in the North of the country. I posted a question of my own about overland travel from Abidjan to Bamako and received some great advice. Within the Cote D'Ivoire group, I noticed several members were particularly active. Their profiles provided a lot of information about them, but what they said in the group was more revealing [lifestyle signaling]. I got a sense for who was proficient in English. I was hoping to unearth my French after several years of neglect, but I liked the idea of staying with someone who spoke English when I first arrived. One particular member spoke English and French and had posted some funny, enthusiastic, and informative messages. I contacted her, and she became my first host in Cote D'Ivoire [reconciling]. I've been staying with her and her boyfriend for almost three weeks. I got in on a meetup for reggae lovers at a bar called Parker Place. Missing my weekly dose of Patty Boom, my favorite reggae spot in DC, Parker Place has been an excellent stand-in, and it has allowed me to meet some awesome Ivorians who share the same musical tastes as me [enclaving]. ([32])Phil's post highlights several orchestration actions that he enacted through the platform to benefit from being connected to the collective. His temporary association with the Côte d'Ivoire group helped him access updated and personal information on potential experiences (""overland travel from Abidjan to Bamako""). His sharing of activities and goals in the group (""introduced myself, explaining why I was traveling, describing my trips"") helped him signal himself to others as a genuine couchsurfer and to find potential hosts who have desirable characteristics (i.e., spoke English and French and were ""funny, enthusiastic, and informative""), and his attendance at events organized through the platform (""meetup for reggae lovers"") allowed him to cocreate experiences with local couchsurfers who share his interests. These actions enable consumers (both guests and hosts) to navigate the challenge of network sociality to accommodate communal and transactional relations, which shapes sociality in the sharing economy collective in ways that enable the cocreation of valuable experiences. The Mechanism of Trust InvestmentWhen cocreating in the sharing economy, consumers face roadblocks associated with the challenge of interpersonal trust, which makes it difficult to cocreate valuable experiences while collaborating with other platform consumers whom they do not know. Guest Janaina, who opened a thread called ""Trusting Your Host"" on the ""Advice for Surfers"" group on the Couchsurfing website, explained how challenging it was for her to trust strangers in cocreating intimate experiences:This will be my first time Couchsurfing alone overseas or couchsurfing at all I should say. So, my friends and family are very skeptical about Couchsurfing especially if it's a male being the host. Just looking for some advice on staying with a male; even if I read good reviews [I am] still a little nervous just because of a totally random person!… How do you really trust the host, leaving your bags there and everything? I almost feel safer just looking for a family or couple rather than a single person. Thanks.These roadblocks to cocreation, Janaina noted, are not uncommon among couchsurfers. We identified three types of actions that platform consumers (guest and hosts) engage in that help them navigate the challenge of interpersonal trust in cocreating with strangers.""Revealing"" refers to actions that allow platform consumers to signal their integrity and assess that of potential cocreation partners. Revealing helps address a common trust roadblock that consumers face, which is determining how to assess the integrity of potential cocreation partners who may have different values, norms, and behaviors. Revealing includes requesting additional information from others (""For safety reasons, we want complete profiles and personal requests. We want to know who we are hosting and why they chose us"" [27]), providing additional information about oneself in profiles, discussion forums, messages, and face-to-face interactions (""When a surfer sends me a link to his/her VLOG and after reviewing a few videos,… my decision to host is usually immediate"" [28]), and asking friends for personal testimonies (""If you are new to [Couchsurfing] and don't have any reviews, ask your friends who use the service to write you a review and describe you as a friend"" [29]). Cultivating reviewsA common roadblock that consumers face when cocreating with strangers is the need to demonstrate how reliable they are to potential cocreation partners. We found that consumers overcome this roadblock by engaging in orchestration actions that signal consistency in behavior and reliability. We refer to this type of action as ""cultivating reviews."" This includes actions such as hosting friends in exchange for reviews (""The host wants to be a surfer when they travel, so they host to build more reviews so that they can get accepted when they send out Couchsurfing requests"" [30]), asking for reviews of a specific kind (""A host asked me to make sure I made specific comments about the freedom he gave to all guests staying at this place"" [31]), and boosting reviews (""[Couchsurfer] asked me to leave him a reference so that he could achieve a better gender balance in his references"" [32]).""Scaffolding"" refers to orchestration actions that allow cocreation partners to progressively trust one another. Scaffolding results in familiarity, security, and control, as it reduces trust gaps by allowing participants to make gradual assessments of their counterparts' reliability and integrity. Scaffolding actions include proposing short experiences before a stay (""Later he asked if he could stay tonight and I said yes, then he asked for a second night and I accepted that as well"" [33]), meeting cocreation partners in public spaces first (""We met at the station and then headed to his home"" [34]), using temporary gatherings as a way of getting to know partners (""I went to a hangout to try to find a couch"" [35]), creating if/then rules for interactional behavior (""I don't even write a request to these men [who only host women]"" [36]), restricting access to personal resources (""Laundry at the house is not on offer, but I can assist you in finding a coin laundry for you to get those clothes washed!"" [37]), and pacing responses (""I see this constantly, mostly from young female surfers, where they say they are receiving multiple requests and will respond later or something similar to that"" [38]), among other actions.These types of actions—revealing, cultivating reviews, and scaffolding—enable consumers to reduce risks in cocreation. To capture this function, we categorize these actions under a mechanism we call ""trust investment."" We define trust investment as the mechanism that enables platform consumers to address the challenge of interpersonal trust by managing platform resources to mitigate the risk of engaging in one-off interactions with strangers in the sharing economy. In interpersonal relationships, trust develops when ""one party has confidence in an exchange partner's reliability and integrity"" ([30], p. 23); thus, it signals the degree to which a person can depend on others to do what they say they will. Whereas the actions of C2C alignment and rewiring relations may also occasionally end up reducing risk among cocreating consumers in a sharing economy collective, trust investment actions are enacted specifically to overcome the challenge of interpersonal trust.Take, for example, how host Marie advised Janaina to host others to build a trustworthy track record rather than worrying about potential unsafe hosts:[You should host, but] if you really really really cannot host, please show a CS member around … meet up and share a meal and conversations [scaffolding] … i.e., invest some of your time and resources,… And that way, get some references [cultivating reviews], your potential future host wants to be safe with you, too! as someone said above: if you request a couch from a host with references: you could double check with former guests about safety. but if you have none: how could I, your host, double check to make sure I am safe with you? after all you'll be invading all of my privacy - while all I could potentially invade would be your backpack :-)"" (Marie, comment on the ""Trusting your host"" thread in the Couchsurfing ""Advice for Surfers"" group)Marie recommends orchestration actions that work as trust investments. She recommends that the new couchsurfer make efforts to meet other consumers and share resources with them (""a meal and conversations"") as a way to progressively build a reputation (""get some references"") and signal her integrity and reliability on the platform. She also explains how references are not to be taken for granted, as they are indicators of a potential cocreation partner's reliability but should be mobilized to reveal further information (""doublecheck with former guests about safety"") that could help reduce the risks. The Mechanism of Network ExperimentationPlatform consumers seek personalized experiences in the sharing economy. To achieve these, they need to continuously work with cocreation partners, often improvising as they cocreate and adapting for personalization. Take, for example, the case of Nahim, a self-described ""couchsurfing-drifter"" (39) who was using Couchsurfing to find accommodation in Afghanistan. Given the country's weak security and deteriorated tourism infrastructure, Couchsurfing emerged in the region as an alternative (Kabul has approximately 1,000 Couchsurfing hosts), allowing local hosts to quench their thirst for cosmopolitan experiences and intrepid travelers to find hospitality. However, cocreating experiences in unconventional conditions is not easy. Nahim faced roadblocks associated with not knowing the hosts' culture, not being dressed adequately, and not understanding the language. We identified three types of orchestration actions that help platform consumers (both guests and hosts) overcome roadblocks associated with personalized experiences, such as those experienced by Nahim, and cocreating unique value outcomes in the sharing economy.""Creative resourcing"" refers to the type of actions whereby consumers introduce new resources to cocreate experiences that are unique and personalized and share these experiences on the platform. These actions include making new resources available for cocreation, which extends the range of cocreated experiences (""[Host] hosted me for two months at the university housing building. I had my own room, and even attended classes [laughs]"" [40]) and offering different sets of resources within a traditional experience (""The [host] had coffins that went into the wall, would pull them out, and it became a bed"" [41]), among others.""Role improvising"" refers to the type of actions whereby cocreation partners step into new roles and scripts while cocreating to extend the value potential of their experiences. We identified a series of actions for this type, including taking on new roles and scripts to enhance the range of cocreation activities (""[The host's] mother is sick, has cancer, so I … chose to spend more time with him, talk to him, just like a psychologist"" [42]) and immersing oneself in these roles and scripts to prolong or intensify the cocreation experience (""we are still in touch, and [guest] invited me to his wedding in Brazil"" [43]). Through these actions, cocreation partners move away from the normative guest and host roles to improvise as confidantes, psychologists, and stylists, among other roles.""Repurposing"" refers to orchestration actions whereby consumers introduce new goals or value propositions for interactions that are enabled by the platform. Under this label, we grouped actions such as using the network and the platform for cocreation purposes other than those endorsed by the platform firm (e.g., ""sex surfing"" [44]) and using current resources to repurpose activities toward achieving additional goals (""[Couchsurfers] will hire a bus to visit a remote area outside the city [as a Couchsurfing event]"" [45]), among others.These types of actions—creative resourcing, role improvisation, and repurposing—constitute a mechanism we call ""network experimentation."" We define network experimentation as the mechanism that enables platform consumers to overcome the challenge of personalizing cocreation by trying new resources, roles, and goals when cocreating experiences, thus extending the possibilities for the cocreation of unique, personalized, and valuable experiences. Through network experimentation, orchestration actions increase the field of potential expectations, interactions, and responses among cocreation partners in the network. Consider, for instance, how the couchsurfers in Afghanistan engaged in actions of network experimentation to help Nahim and other couchsurfers overcome the challenge of personalizing cocreation, as recounted in a blog post covering Nahim's travel experience:[He] managed to hitchhike through Iraq by displaying a sign in Arabic to passing drivers, written by one of his hosts [role improvising]. After arriving in the western Afghanistan city of Herat, he became acquainted with some local members of the Taliban, whom he described as ""actually really nice people."" His disguise [to travel safely in Afghanistan] consisted of a white shalwar kameez (traditional Afghan clothing) and a taqiyah (cap for observant Muslims). The clothing was provided by his Couchsurfing hosts [creative resourcing], who also taught him how to pray to Mecca [role improvising], should the need arise. ([23])Nahim's story highlights how orchestration actions that operate through the mechanism of network experimentation help consumers personalize cocreation with strangers. Nahim reported on the incorporation of new resources into cocreated experiences (""offering disguise clothes to help him reach his goals"") and described how his hosts improvised by stepping into a new role (""writing signs in Arabic for his hitchhiking"" and ""teaching him how to pray""), which provided value outcomes beyond the usual Couchsurfing experience.Overall, our findings point to four overarching mechanisms of orchestration work and 14 specific actions that consumers engage in to cocreate value in the hybrid setting of sharing economy platforms. Figure 1 consolidates consumer orchestration work, detailing the actions and mechanisms that help consumers cocreate unique and valuable experiences for themselves. Our focus thus far has been on examining the nature of consumer orchestration work on platforms. As we uncovered these mechanisms, we also found that they connected to well-known sources of value for platform firms. The next section explains how orchestration work creates value for platform firms.Graph: Figure 1. How consumers orchestrate experiences and cocreate value in the sharing economy. How Consumer Orchestration Work Creates Value for Platform FirmsAs orchestration work enables value creation for platform consumers (service providers and users), we found that it also appears to activate known sources of value for platform firms. In this section, we identify how consumer orchestration work leads to efficiency, complementarities, lock-in, and novelty ([56]) in the platform environment.Efficiency, which refers to cost savings enabled by interconnections, is one of the primary value drivers for platform firms ([56]). Orchestration work contributes to increasing the efficiencies associated with cocreating on the platform. As consumers align their expectations, interactions, and responses with those of heterogeneous cocreation partners through C2C alignment, the cost of organizing exchanges among peer-service providers and users is reduced. C2C alignment is also likely to reduce costs arising from unsuccessful cocreation partnerships (e.g., consumer dissatisfaction and defection; reputation damage). Specifically, screening enhances consumers' likelihood of finding partners who cocreate in ways that these consumers consider desirable, thereby reducing the overall effort needed to integrate resources. Cueing allows cocreation partners to signal to one another the type of experience they envision, reducing the risk of misalignment during cocreation. Flexing allows cocreation partners to negotiate access to and provision of resources to better accommodate their individual needs. Buffering ensures that experiences gone wrong are dealt with quickly, preserving some of the cocreated value and preventing the escalation of tensions. These orchestration actions make consumers' cocreation in the sharing economy more efficient and create value for the platform firm by further reducing direct and indirect costs associated with organizing exchanges among platform consumers.Complementarities, which refer to the ""value-enhancing effect of interdependencies"" ([56], p. 21), are another important source of value for the platform firm. Orchestration work leads to complementarities in cocreation by spurring beneficial interdependencies among platform consumers. Rewiring relations leads to complementarities by creating opportunities for generating synergies with others in the platform. Specifically, interest grouping and lifestyle signaling build on the idea that close-knit groups can exist in the sharing economy, even though we found, in line with prior research ([ 4]), that most consumers do not consider these collectives as being communities. Such groups can provide opportunities to establish desired relationships within the collective and reconnect identity projects to cocreated experiences, adding value to these experiences. Enclaving provides actors with opportunities to experience intense, albeit temporary, communal sociality at the local level. This intense sociality is likely to enhance opportunities for value cocreation through the platform. Reconciling helps consumers use the platform connections to achieve both individual and communal goals. These orchestration actions increase complementarities by capitalizing on the existing interdependencies in the network. As the network becomes more valuable to consumers, the platform firm that hosts it may be able to capture additional value in turn.Lock-in, which refers to ""business model elements that create switching costs or enhanced incentives for business model participants to stay and transact within the activity system"" ([56], p. 21), is yet another source of value for the platform firm. One important way of generating lock-in in sharing economy platforms is to reduce the risk of cocreating with strangers ([53]). By improving interpersonal trust among platform consumers, orchestration work increases the pool of potential trustworthy cocreation partners, offering additional incentives for platform consumers to stay and transact within the platform. Trust investment actions help platform consumers become better at signaling and assessing the integrity and reliability of those with whom they will cocreate. Revealing increases opportunities for consumers to promote their ability to integrate resources and cocreate with others, promoting themselves as low-risk partners. Cultivating reviews is indicative of the likelihood of the success of future collaborations with cocreation partners. Scaffolding enables cocreation partners to slowly test their ability to cocreate together and adapt their behavior to reduce the risk in extended cocreation efforts. These orchestration actions promote lock-in by increasing the level of interpersonal trust that exists among platform consumers. A higher level of interpersonal trust is a source of value for platform firms because when perceived risk in cocreation is reduced, the platform is perceived as safer ([53]), and platform consumers are likely to increase their engagement for longer periods of time.Novelty, which refers to ""degree of business model innovation that is embodied by the activity system"" ([56], p. 21), is also a source of value for the platform firm. Consumer cocreation becomes a source of innovation for the platform firm when it helps the platform incorporate resources, roles, and goals that are not only novel but also meaningful to its consumers. Actions of network experimentation enable platform consumers to explore new ways of cocreating unique experiences with more personalized value outcomes. Creative resourcing encourages platform consumers to share their creative use and integration of resources, resulting in a unique experience for those involved and in novel configurations of resources for cocreation by others in the network. Role improvising helps consumers step into new cocreation roles, allowing for diverse and unique sets of scripts to be available for platform consumers. Repurposing creates new opportunities for value cocreation by introducing new value propositions into the network; when these become ubiquitous (as is the case of ""sex surfing""), they allow for a different set of repurposed activities to emerge. These orchestration actions lead to value for the platform firm by aggregating new resources, scripts, and purposes to the platform offering, which, in turn, becomes attractive to a larger number of potential consumers.Finally, although we have noted clear links between the actions of each identified mechanism—C2C alignment, rewiring relations, trust investment, and network experimentation—and well-known sources of value for the firm ([ 2]), we highlight that the actions of each orchestration mechanism can activate multiple sources of value for firms. For example, although screening has the immediate effect of creating efficiencies through the selection of suitable partners, it may also lead to further complementarities in the platform if consumers identify cocreation partners who have resources that complement their own. Screening may also promote lock-in as consumers become increasingly skilled at identifying ideal partners on the platform and are likely to remain loyal to this platform; they can then continue using their screening skills to reduce the risks associated with cocreating via the platform. For more entrepreneurial platform consumers, screening can result in novelty for the platform firm when these consumers learn how to select partners who are more likely to propose or welcome atypical experiences or disseminate their novel, cocreated experiences to others on the platform. As with screening, other orchestration actions can also be linked to more than one known source of value creation for a firm ([ 2]). The capacity of each orchestration action to impact multiple sources of value for platform firms highlights the value-creating power of consumer orchestration work. Marketing Implications Implications for Platform Firms in the Sharing EconomyA key marketing issue for the managers of platform firms is how best to capture the value that is cocreated by platform consumers to achieve long-term sustainability in the sharing economy ([12]). Despite the enormous potential of consumer cocreation, many platform firms still approach cocreation from a traditional business mindset; they aim to manage the onstage aspects of consumers' experiences rather than assume a backstage role and focus on developing support processes and structures ([ 9]). We propose that platform firms can improve the value they obtain from consumer collectives in the sharing economy by understanding and supporting consumer orchestration work, which is geared toward helping consumers independently overcome cocreation roadblocks.Platform consumers engage in orchestration work to address the challenges they face while cocreating experiences in the hybrid logics of the sharing economy. To best support consumer orchestration work, managers of platform firms must identify these roadblocks and know how to leverage orchestration actions and respective mechanisms. The case of Couchsurfing, with its low level of platform control over consumers' cocreation, allows us to better understand the types of actions and mechanisms that consumers put into place to overcome these challenges and cocreate value for themselves.By facilitating C2C alignment, platform firms can help consumers overcome the challenge of cocreating with heterogeneous partners. For example, this challenge is evident with Airbnb, which, similar to Couchsurfing, has a vast and diverse network of consumers ([26]). Airbnb encourages hosts to be upfront in their online profiles and initial message exchanges. It also encourages guests to pay attention to cues provided by hosts. Drawing on our findings, Airbnb could further support consumers in dealing with the challenge of heterogeneity by encouraging screening through additional filters that more specifically account for expectations and preferred ways of cocreation (e.g., allowing guests to indicate their desired amount of contact/conversation with the host). Airbnb could further support cueing by reminding hosts and guests to complete profile tabs associated with preferred modes of interaction (e.g., by asking questions such as ""Chat over coffee or no talk before breakfast?""). Furthermore, Airbnb could encourage its consumers to engage in flexing by educating them on the need to accommodate variability while cocreating (e.g., disseminating curated, user-generated how-to videos that address common misalignment issues, such as ""Three ways to deal with guests who leave your kitchen messy""). Finally, Airbnb could help consumers reduce the impact of cocreation mismatches through buffering by providing guidelines on how consumers can curtail issues and recover from negative experiences (e.g., crowdsourcing a flowchart for common cocreation problems and ways to address them). These initiatives would allow this platform firm to increase opportunities for its consumers to pursue cocreation efficiencies while maintaining orchestration work under consumers' control.Platform firms can help consumers overcome the challenge of networked sociality by helping them rewire relations for cocreation. For example, Tinder successfully developed a platform for high-involvement customer experiences ([42]), yet it has been criticized by consumers and the media for making someone's search for a partner too transactional. Tinder could address these critiques by helping its consumers reconcile the app's transactional logics (e.g., swipe left/right) with communal logics and the desire for more meaningful relationships. It could, for example, assist consumers with using the platform for interest grouping (e.g., offering tags for members who are into pet walking or cooking together), and assist consumers in lifestyle signaling by providing a means for them to connect over preferred hobbies or passions (e.g., forums for those into watching and discussing movies about art). It could also enable enclaving by supporting the creation of spaces that enable closer relationships among platform consumers (e.g., helping entrepreneurial consumers promote local singles' face-to-face night and facilitate these events by enabling communication about them on the platform). Tinder could also support consumers in their work of reconciling personal goals with the need for strengthening communal bonds. For instance, Tinder could create a forum for storytelling about successful and failed hookups, a bonding activity that often encourages sociality and sharing (communal goals) while equipping consumers to become better at finding dates for themselves (transactional). This type of forum currently happens outside this platform (e.g., on Reddit), which demonstrates that consumers already engage in orchestration work.Overall, these initiatives could help consumers better utilize the power of social relations to address both their quid pro quo hookups and their need for meaningful social connections. Couchsurfing and other platforms can assist consumers in overcoming the challenge of interpersonal trust by helping them reduce the risk of cocreating experiences with strangers. At Uber, for instance, the perceived lack of authenticity (withholding one's true self) is a major challenge in building interpersonal trust ([15]).While trust mechanisms such as protection insurance and identity verifications are fundamental to raising consumer trust in the platform and its consumers ([25]), platform firms can increase interpersonal trust by supporting and leveraging the trust investments made by consumers to overcome their interpersonal trust issues. Thus, supporting consumers' work to reveal more about themselves through profiles, tags, and prompts, safely and progressively (scaffolding), and helping them cultivate authentic reviews about their cocreation behavior, could increase interpersonal trust among consumers of the Uber platform.Similarly, BlaBlaCar, a ride-sharing platform that matches empty car seats with potential passengers looking for long-distance rides, has a well-researched history of helping platform consumers (drivers and riders) overcome the interpersonal mistrust that usually exists when cocreating with strangers. BlaBlaCar supports revealing by encouraging consumers to share more information about themselves in their profiles and to connect this information with their existing online identity (e.g., Facebook profile). Research has found that 88% of BlaBlaCar consumers highly trust a member who has a complete digital profile, which is higher than the trust levels of colleagues or neighbors ([27]).Our findings suggest that BlaBlaCar could further boost revealing by encouraging consumers to share more specific information about their cocreation preferences during interactions (e.g., ritualizing the sharing of cocreation stories as icebreakers, offering platform-specific personality quizzes, allowing consumers to associate their profile with a particular consumer persona—chosen from a pool of existing personas). Currently, BlaBlaCar asks its consumers to rate one another after having shared higher-stakes, real-life, offline experiences. Our findings suggest that BlaBlaCar could further leverage consumers' work of cultivating reviews by providing badges that consumers could win or gift each other for each review, or by allowing consumers to create additional questions to be asked by their reviewers. This would help future cocreation partners identify behavioral consistency (e.g., if John has great taste in music, he could get a ""great music"" badge and ask partners, ""What did you think of your driver's soundtrack?"" rather than the generic ""What did you think about this experience?"").BlaBlaCar currently encourages its consumers to speak on the phone before agreeing on a ride. We suggest that the platform could further support consumers' scaffolding work by highlighting opportunities for them to progress into more trustworthy relationships with others in the platform. For example, BlaBlaCar could use notifications to prompt cocreation partners to discuss things ahead of time that could potentially lead to pain points along the customer journey (e.g., after agreeing on a ride and a day before the trip) to allow interpersonal trust to be built through multiple interactions. Overall, these initiatives would support consumer orchestration work aimed at increasing interpersonal trust among the platform's consumers.Finally, platforms can help consumers overcome the challenge of personalizing cocreation with strangers by supporting them as they try new resources, roles, and goals when collaborating with others to cocreate experiences and, thus, extend the possibilities of creating unique and valuable experiences for themselves and others in the platform. Platform firms often propose new features and processes to continuously innovate and personalize the customer experience. Platforms can achieve these goals faster by leveraging the network experimentation conducted by consumers in their quest to create unique personalized experiences. For example, TaskRabbit, a platform that connects consumers who need help with local workers and specialists (e.g., plumbers, translators), has maintained a broad definition of what tasks can be offered or hired through the platform (thus helping consumers' innovations) and adopted some innovative skills offered by consumers (e.g., offers to wait in line for others) as standard categories in the platform. To further leverage consumers' improvising actions, TaskRabbit could confer badges to consumers who successfully break role boundaries, encourage consumers to identify the innovators among their cocreation partners, offer alternative profile categories (beyond their general label for service providers [""taskers""]), prompt consumers to teach each other something new while interacting, and encourage them to become local ambassadors.To further support consumers in personalizing their cocreation, TaskRabbit should encourage consumers to engage in creative resourcing by actively prompting them to innovate or highlight the presence of new resources through badges, tags, and curations. For example, taskers, to describe all of their resources, often find workarounds to the limited options available on their platform profiles (e.g., offering American Sign Language as a skill-for-hire to showcase their language fluency). The platform could help consumers share these hacks with others through the TaskRabbit app or create a customizable language tag (rather than tags only for the most popular languages). Furthermore, TaskRabbit could support consumers' repurposing actions. For example, consumers may search for friendship or companionship through the platform, and the firm could create categories of social tasks to reflect these alternative purposes. Overall, by developing business models that support consumer experimentation actions, platform firms in the sharing economy can facilitate consumers' cocreation of experiences that are unique and valuable to participants. These innovations can eventually be picked up by the firm and help it continuously offer novelty to its stakeholders. When Consumer Orchestration Works BestIn the sharing economy, business models vary in terms of how much control they allow consumers to have over their cocreation activities. The decision of how much control to give to consumers is often a strategic one. The platform firm may prefer to keep a tight grip on consumers' interactions as a way of reducing costs, optimizing efficiencies, and enabling preset complementarities among partners. This type of firm-led orchestration of experiences works well in contexts in which experiences are expected to be similar or consistent (e.g., Kickstarter) and/or those in which heterogeneity does not affect the outcome of experiences (e.g., Lending Club). This also works well when sociality is not important (e.g., Zipcar), interpersonal trust is not required (e.g., Quirky), or experimentation is not valued (e.g., Lime). In these cases, the firm-led orchestration of experiences via traditional marketing controls (e.g., standard service quality metrics) should be preferred, as this saves costs and enables the firm to harvest the value of maximizing resource allocation and business scaling.Other platform business models thrive on enabling unique experiences (e.g., Airbnb). Many require high consociality ([34]), as consumers need interaction to cocreate experiences (e.g., Uber); this often demands interpersonal trust (e.g., BlaBlaCar, TaskRabbit) and the alignment of consumers' expectations, actions, and responses (e.g., Tinder). It is for these groups of platform firms that consumer-led orchestration work dominates and the findings of this research are most relevant.Orchestration work, although performed by consumers, can be influenced in important ways by the affordances of the platform. For example, the specific way in which C2C alignment manifests depends on the search filters, algorithms, and peer-to-peer communication tools available on the platform. Orchestration work is also shaped by how the platform firm incentivizes or limits consumers' interactions outside the platform. Importantly, Couchsurfing is a low-intermediation platform ([34]), in that it does not interfere much with what consumers are doing.These findings align with [34] contention that marketers, to effectively hone and market their value propositions, must understand their market stakeholders. We support and extend their claim that each platform type has ""particular forms of value creation that should focus managers' business investment decisions and resource deployment"" ([34], p. 33) by considering how consumer-led value cocreation impacts each type differently. We argue that the platform types with business models that depend on a high degree of consumer interaction, such as Couchsurfing, Airbnb, and the others that [34] call ""forums"" and ""matchmakers,"" are most likely to profit from delivering unique experiences. These platforms should focus on leveraging consumer-led orchestration actions by way of the mechanisms we have outlined. In contrast, platforms with a limited degree of consumer interaction—for example, Kickstarter and other ""enablers"" and ""hubs"" ([34], p. 20)—have less to harvest from consumer-led orchestration. In these cases, platform forms should be selective in terms of how they engage with consumer-led orchestration. These firms should consider whether each source of value creation (i.e., novelty, efficiency, lock-in, and complementarities) they wish to foster is better activated by consumer- or firm-led orchestration. Theoretical Implications Expanding on How Consumer Work Creates Value for FirmsIn line with prior research that examines how consumers cocreate value within a consumer collective (e.g., Schau, Muñiz, and Arnould 2009), we focus on identifying what consumers are doing to create value for themselves, and we see value for the firm as a welcomed, if not desired, outcome of consumer activity. However, the sharing economy's consumer collectives differ from the more communal collective contexts previously explored in the literature, such as online brand communities (Schau, Muñiz, and Arnould 2009) and interest-based communities ([ 7]; [46]). As such, most prior research has examined value cocreation in consumer collectives as an outcome of shared practices within the collective (e.g., [17]; [19]). In contrast, we identify the challenges to cocreation that are intrinsic to sharing platform collectives and map the emergent orchestration actions that consumers enact to surpass the roadblocks in cocreation and to have more valuable experiences. Thus, the orchestration work we identify here does not directly cocreate value. Instead, by overcoming the challenges to cocreation that are characteristic of these sharing platforms, these orchestration actions enable the conditions for the cocreation of value.We also show how by engaging in orchestration work to enable the cocreation of value for themselves, the enterprising consumers ([14]) who cocreate in the sharing economy also support value creation for platform firms. Going beyond prior research, we explain how each orchestration mechanism may lead to multiple sources of value creation that are at the core of platform business models ([56]). This newly identified type of consumer entrepreneurial work advances the understanding of prosumer work (see, e.g., [57]). In orchestration work, platform consumers (both service providers and users) enact forms of governance that are traditionally associated with marketing managers. Through orchestration actions, enterprising consumers take control of the platform's value-creating activities.Consumer orchestration work then explains how it is possible for platforms, such as Couchsurfing, to function well and generate value for millions of heterogeneous participants who are guided by hybrid logics despite maintaining limited control over the quality of service providers and consumers' experiences ([12]). We show how, for example, through trust investment orchestration actions (e.g., revealing additional information, cultivating reviews, scaffolding interactions), consumers complement platform-based mechanisms for reducing risk (e.g., rating systems, insurance for payments or damages) with interpersonal trust. Considering the myriad of ways in which consumers address trust-related roadblocks can extend knowledge on the interdependency of platforms' and consumers' reputations in the sharing economy ([11]).We also note how, in the realm of cocreated experiences in the sharing economy, innovation is emergent ([35]); it comes largely from consumers' activity as they ideate, collaborate, and experiment in the network to develop solutions tailored to their needs ([ 1]). Our findings highlight the serendipitous nature of consumer orchestration, which favors the discovery of value potential in the collective ([31]), in turn bringing novelty to the consumer collective and the platform firm. This way of understanding innovation as an outcome of the orchestration of experiences can address recent calls for research on the specific mechanisms that drive consumer-led innovation in the sharing economy ([12]). In Table 3, we outline directions for future research on various areas connected to value cocreation by consumers.GraphTable 3. Future Research Directions. Research AreasDirections for Future ResearchValue cocreation in digital platformsHow does consumers' engagement in orchestration influence consumers' assessments of the value of cocreated experiences?How can consumer-led orchestration support systemic value creation (Figueiredo and Scaraboto 2016)?Are there instances in which consumer orchestration may lead to value destruction for the firm (Echeverri and Skålén 2011)?How can marketing controls be employed to help consumers overcome the four challenges of hybrid value cocreation logic?How can mechanisms of network experimentation be fostered as market-shaping devices for innovation (Nenonen, Storbacka, and Windahl 2019)?How can consumer orchestration be leveraged to more efficiently address consumers' needs, minimize resource waste, and turn consumer orchestration into a force for the social good (Rinne 2018)?How can the mechanism of trust investment be used to help build a digital reputation (Eckhardt 2020)?What sort of digitalized networked arrangements of artifacts, persons, processes, and interfaces might best support orchestration work (Ramaswany and Ozcan 2018)?Consumer experience in the sharing economyHow can firms design experiences with the ideal balance between company- and consumer-led orchestration to achieve optimal value creation?What are some pain points of the customer journey associated with the challenges of cocreating in the sharing economy?How can consumer-led orchestration be deployed to generate service innovations in the sharing economy (Tronvoll and Edvardsson 2020)?Sharing economy governance and policyHow has the onus of orchestrating cocreation been transferred to consumers as free work in the sharing economy (Zwick, Bonsu, and Darmody 2008)?How does orchestration work shape customers' risk perceptions in the sharing economy (Wang, Ma, and Wang 2020)?What economic and noneconomic rewards can be offered to consumers engaged in orchestration, thereby recognizing their engagements (Pazaitis, De Filippi, and Kostakis 2017)?Under what conditions are consumers more likely to engage in orchestration work or enact one type of action versus another? How can these conditions best be shaped?How does orchestration work influence consumers' relation to the platform (Zhou et al. 2021)?Does increased trust among cocreation partners lead to increased trust on the platform (Möhlmann and Geissinger 2018)?How can consumer orchestration in the sharing economy inform the regulation of business models in the sharing economy?  ConclusionThis article makes important contributions to research in marketing. First, it discusses the challenges that the hybrid logics of the sharing economy raise for consumers who are cocreating experiences. Second, it identifies orchestration as an important form of work conducted by platform consumers (service users and providers) to overcome these challenges. Third, it connects consumer orchestration work to known sources of value for platform firms and provides recommendations for platform firms to leverage the power of orchestration work. Finally, it offers a series of research questions that can inspire marketing researchers to further explore how consumers cocreate in the sharing economy.  "
22,"How Industries Use Direct-to-Public Persuasion in Policy Conflicts: Asymmetries in Public Voting Responses Industries use persuasion strategies to gain public support when challenged by activist groups on consumer-relevant issues. This marketing practice, termed ""direct-to-public persuasion,"" has received limited attention in the field, and thus we have little understanding of when such campaigns fail or succeed. This research introduces a theoretically derived and empirically supported framework that draws from multiple areas, including marketing persuasion, political campaign strategy, sociopolitical legitimacy, and perceptual fit, to identify important differences in the effectiveness of these persuasion strategies on attitudes and voting behavior. The multimethod approach includes a field study of ballot measure voting during a national U.S. election and three experimental studies. The findings contribute new knowledge of asymmetries in public response to industry and activist arguments. Stronger arguments from both sides lead to more favorable outcomes, but activist groups benefit most. Suspicion of activist arguments weakens the impact on attitudes and voting; industry argument suspicion has limited impact, though it does increase the likelihood of voter switching. A financial argumentation strategy works best for the industry side, while societal argumentation is more effective for the activist side. The insights offer guidance for industries and activist groups as argument strategy success is contingent on the side that uses it.Keywords: direct-to-public persuasion; persuasion; political campaign strategy; political marketing; sociopolitical legitimacy; voting behaviorIndustries often try to persuade the public at large to support their positions on prominent issues that are consequential for consumers and industry members. We term this phenomenon ""direct-to-public persuasion,"" a marketing-driven political influence strategy used to convince the public that the industry is on the correct side of an issue. Industries ""talk"" to the public not only to sustain or gain advantageous conditions, such as preventing policy actions viewed as costly or restrictive, but also to align the public with practices that further industry objectives.Industries use direct-to-public persuasion to influence public voting on ballot measures in state and local referendums, public opinion in advance of legislative policy maker decisions, and ongoing public attitudes toward the industry. We focus on conflict scenarios in which industries are challenged by activist groups—collectives that advocate for the public interest—in ballot measures (e.g., [67]). Such conflicts are commonly high-profile and a strategic priority for industries, which spend substantial funds to secure favorable outcomes and protect industry practices ([42]; [59]). The outcomes often have meaningful consequences for the public, including consumers, but the question of what drives voters to support one side or the other remains unanswered.Table 1 presents examples of how industries and their activist opponents use direct-to-public persuasion to influence the public and gain support. The examples represent diverse issues, including food labeling, recycling, pharmaceutical drug pricing, and tobacco taxes. In the scenarios we examine in this article, industries defend the policy status quo to resist changing disputed practices (e.g., price limits on pharmaceuticals). However, in other settings, industries fight the status quo to expand or develop new markets.[ 5]GraphTable 1. Examples of Direct-to-Public Persuasion in Issue Referendums. Issue, Location, YearOverview of Activist (Vote Yes) Side on Issue and Industry (Vote No) Side on IssueTop DonorsTotal Campaign ContributionsExample of Financial Argumentation StrategyExample of Societal Argumentation StrategyResult(% of Votes)Renewable Energy Standards(Proposition 127)Arizona2018A ""yes"" vote supported requiring electric utilities in Arizona to acquire a certain percentage of electricity from renewable resources each year, with the percentage increasing annually from 12% in 2020 to 50% in 2030.NextGen Climate Action, League of Conservation Voters$24.1 millionThe price of renewable energy continues to drop every day and is already competitive with coal and gas.Cleaner air and water means healthier families—and less respiratory illnesses like asthma.31.4%A ""no"" vote opposed requiring electric utilities in Arizona to acquire a certain percentage of electricity from renewable resources, thereby leaving in place the state's existing renewable energy requirements of 15% by 2025.Pinnacle West Capital Corporation, Grand Canyon State Electric$40.9 millionCorporate and industrial rates would rise over 100%, and residential ratepayers would see an average annual increase of $1,250.The measure would dramatically harm Arizona's competitiveness and put our utilities' reliable delivery of power at risk.68.6%Drug Price Standards(Proposition 61)California2016A ""yes"" vote supported regulating drug prices by requiring state agencies to pay no more than the U.S. Department of Veterans Affairs (VA) pays for prescription drugs.AIDS Healthcare Foundation, California Nurses Association$19.1 millionThe proposition would save taxpayers billions of dollars in health care costs.The proposition would provide better access to life-saving drugs.46.8%A ""no"" vote opposed this regulation to require state agencies to pay no more than the VA pays for prescription drugs.Pharmaceutical Research and Manufacturers of America, Merck & Co. Inc., Pfizer Inc.$109.1 millionThe proposition would increase state prescription drug costs.The proposition would reduce patient access to medicines.53.2%Tobacco Tax Increase(Proposition 56)California2016A ""yes"" vote favored increasing the cigarette tax by $2 per pack, with equivalent increases on other tobacco products and electronic cigarettesTom Steyer, California Hospitals Committee on Issues, Million Voter Project Action Fund$35.5 millionThe proposition would reduce tobacco-related health care costs and would help pay for those costs.The proposition would prevent youth smoking and address tobacco marketing targeting a youth market.64.4%A ""no"" vote opposed increasing the cigarette tax by $2 per pack, with equivalent increases on other tobacco products and electronic cigarettes.Philip Morris USA, R.J. Reynolds Tobacco Company, Altria Client Services LLC$71.0 millionThe proposition would fund insurance companies and special interests more than it would fund treatments for smoking-related illnesses.The proposition will not solve real problems like solving the drought and fighting violent crime.35.6%Expansion of Bottle Deposits(Question 2)Massachusetts2014A ""yes"" vote would expand the state's beverage container deposit law to require deposits on all nonalcoholic drink containers, except beverages derived from dairy, infant formula, or medications.Massachusetts Sierra Club, Elm Action Fund, Massachusetts Public Interest Research Group$1.6 millionA yes vote equals big savings for towns' waste management costs.A yes vote equals more recycling and less trash and litter.26.6%A ""no"" vote would keep the state's beverage container deposit law in its current form.American Beverage Association, Nestle Waters NA, Stop & Shop Supermarket Company$9.6 millionThe measure would cost nearly $60 million a year, more than three times the price of curbside programs.Massachusetts should be a recycling leader, but Question 2 will keep us in the past.73.4% 1 Notes: Information on ballot measures retrieved from https://ballotpedia.org/Ballot_Measures_overview.Industries have resource advantages relative to activist opponents that suggest they should win policy issue battles, and they often do ([56]; [65]). Because industries are often in the advantageous role of defending the policy status quo, they can leverage the uncertainty of any policy change, whereas activist groups often have the more difficult task of convincing the public that the status quo is failing ([19]). Despite their advantages, industries do not always prevail, which may be due to an underlying predisposition of the public to be more skeptical of the industry and its motives ([ 8]; [45]; [62]). The fact that industries do frequently win suggests that they succeed in overcoming the public's skepticism with effective persuasion arguments ([44]). Our research examines the dynamics of competing campaigns to better understand and explain voting outcomes.The industry-level practice of direct-to-public persuasion has received limited attention in the marketing field, and we know little about persuasion involving political campaigns with competing positions on an issue. The marketing literature has examined consumer responses to persuasion attempts in a variety of important contexts, and advertising research has examined aspects of argument effectiveness (e.g., [46]; [66]), but political arguments are believed to differ from commercial arguments ([27]). Political marketing research has largely focused on candidate elections rather than election voting on policy issues (see [35]; [38]), and the lobbying literature addresses influencing policy makers rather than influencing the public (see [43]; [67]). Further, although studies show that voters often switch their support during campaigns, and even a nominal shift can change the outcome ([44]; [50]), knowledge of what causes the switching is limited.To expand knowledge of this understudied topic, we develop a framework that draws from multiple research areas, including marketing persuasion, political campaign strategy, sociopolitical legitimacy, and perceptual fit. We apply legitimacy theory to explain our predicted asymmetry of argument effectiveness across the competing sides and draw on perceptual fit theory to explain the efficacy of specific argumentation strategies on the public's attitudes and voting behavior. We use a multimethod approach that includes a field study of ballot measure voting on two policy issues during a national U.S. election and three experimental studies involving additional issues to test our ideas. The policy issues included prescription drug pricing, tobacco product taxes, renewable energy standards, and container recycling laws.Our findings offer insight into how the public's attitudes and voting intentions change during the campaign when they are exposed to competing arguments from industry and activist groups. First, we uncover important asymmetries that should be considered in understanding and in using direct-to-public persuasion strategies. Specifically, stronger arguments from both industry and activist groups lead to more favorable attitudes and voting outcomes, but activist groups benefit more from stronger arguments. Greater argument suspicion weakens the impact on attitudes and voting, but primarily for activist groups. Therefore, overall, voters' evaluations of activist-side arguments have a greater impact than industry-side arguments on outcomes. Second, an exploration of vote switching found that industry argument suspicion has limited impact, though it does increase the likelihood of voter switching, particularly for voters who switch their support to the activist side. Finally, using follow-up experiments, we show that a financially focused argumentation strategy works best for the industry side, whereas a societally focused strategy is more effective for the activist side.We contribute to marketing research with new knowledge of a marketing practice that influences voters and critical political outcomes. We add to the scope of persuasion research with insight into how voters respond to industry versus activist political campaigns, finding novel evidence of an asymmetric public response, where voter judgments of competing arguments—and the degree to which those arguments are strong or suspicious—help predict the relative impact of each side's campaign. We use sociopolitical legitimacy theory in a new way, proposing that an industry-activist legitimacy gap helps explain why industry argument strength has less impact, and apply persuasion theory to explain why voters give greater weight to suspicion of activist arguments. Our pre- and postelection recontact field study design captures individual voter switching, which is not commonly examined, and provides a rare indication that determinants differ for voter segments that flipped their allegiance from one side to the other. Finally, we offer the first known concrete guidance for the use of direct-to-public persuasion strategies with recommendations that differ for industries and activist groups, as we find that a given strategy's success is contingent on the side that uses it. Direct-to-Public Persuasion Model and Hypothesis DevelopmentFigure 1 depicts the conceptual relationships we investigate. Our model predicts that the public's evaluations of industry- and activist-side arguments, in terms of argument strength and argument suspicion, play a central role in predicting outcomes including attitudes toward the issue, voting on the issue, and vote switching. These evaluations depend on each side's argumentation strategy. We begin by describing the two competing sides in the issue conflict scenarios we study and key distinctions between them, and then we develop the hypothesized asymmetric effects of argument strength and suspicion on the voting outcomes.Graph: Figure 1. Conceptual model. Industry and Activist Conflicts over IssuesWe define ""competing sides on the issue"" as the competitors in a policy issue conflict and focus on industry groups and activist groups, traditional policy adversaries ([ 8]). While industries comprise firms that produce a certain class of goods or services, public interest activist groups are political constituents working in a range of policy areas who claim to represent the public or collective good (e.g., [57]; [67]). Evidence suggests that the public responds differently to the industry and activist sides because of enduring skepticism about the undue influence of business interests ([25]; [49]. While attitudes toward individual industries vary, public favorability ratings of business have trended downward for decades and eroded further since the 2008 financial crisis ([ 1]; [68]).A legitimacy theory perspective indicates that response to direct-to-public persuasion from the industry and activist sides should be asymmetric due to a legitimacy gap caused by the belief that industry-serving interests drive industry-side motives whereas commitment to the public good drives activist-side motives ([56]). Normative and sociopolitical legitimacy underlie the public's judgment of whether an industry practice is acceptable—meeting cultural and political norms—or should be sanctioned (e.g., [40]; [61]). Legitimacy judgments are likely to discourage many voters from initially supporting the industry side, but election polling and outcomes suggest that voters come to judge the industry more favorably during the course of a campaign. For example, in recent ballot measures that sought genetically modified organism labeling on food products (Oregon Measure 92 in 2014), regulations on dialysis clinic pricing (California Proposition 8 in 2018), and expansion of bottle deposit laws (Massachusetts Question 2 in 2014), the majority initially supported the activist side but switched to support the industry position, allowing the industry to win the vote (see https://ballotpedia.org/List_of_ballot_measures_by_year). The Influence of Competing Persuasion Campaign ArgumentsArgumentation is defined as a strategic approach used to justify a particular policy or political position and promote or challenge its implications ([ 6]; [20]). Effective argumentation is critical for industries battling controversy and for activist groups that confront them, as both strive to legitimize their positions ([21]; [39]). We explore the effectiveness of each side's argumentation on two key dimensions: argument strength and argument suspicion. Research on argument strength, including studies in competitive political contexts (e.g., [14]), has not concurrently examined the effects of argument suspicion.We define ""argument strength"" as perceived persuasiveness determined by an individual's cognitive response, either favorable or unfavorable, to an argument ([ 4]; [71]). Empirical findings show that strong arguments consistently shift attitudes and beliefs but to varying degrees (e.g., [ 3]). Although argument strength has had limited study in real-world scenarios of competing arguments over time, some research reports that in the presence of a strong argument from one side, a weaker argument from the opposing side is viewed even more negatively and can actually backfire ([14]). As baseline predictions, we hypothesize: H1:  Industry argument strength increases (a) favorable attitudes toward and (b) voting in favor of the industry side on the issue. H2:  Activist argument strength increases (a) favorable attitudes toward and (b) voting in favor of the activist side on the issue.We define ""argument suspicion"" as the perception that an argument is implausible or has a hidden intent (e.g., [13]; [24]). Although there is far less theoretical and empirical knowledge on the influence of argument suspicion than argument strength, marketing research on persuasion knowledge and skepticism toward advertising offers rich evidence that suspicion-driven responses to persuasion can have significant negative effects on attitudes and behaviors.Consumers draw on persuasion knowledge, beliefs about how marketers influence consumers, to determine whether something is a persuasion attempt (e.g., [13]). In our research scenario, arguments have clear intent to persuade to vote no or vote yes, but whether the arguments' tactics are viewed as appropriate impacts their effectiveness (e.g., [34]). In some cases, voters may infer finer-level motives in an argument's tactics that trigger suspicion (e.g., Is the ""vote no"" side using arguments that demonize the ""vote yes"" side because it lacks more valid arguments?; [37]). Research on skepticism toward advertising messages has examined dispositional ad skepticism (e.g., [48]) as well as situational skepticism (e.g., [22]). Marketing studies suggest that when voters respond to an industry's arguments with skepticism, they are likely to scrutinize its underlying intentions, suspicious that they are deceptive as well as self-serving (see, e.g., [22]).While this suggests that the public is inclined to view industry arguments as suspicious, other research suggests that activist arguments can also face public doubt, such as when they appear to exaggerate the problems and risks related to contested issues ([ 7]; [62]). As such, we expect voter evaluations of argument suspicion to be relevant for both sides on the issue. We predict the following: H3:  Industry argument suspicion decreases (a) favorable attitudes toward and (b) voting in favor of the industry side on the issue. H4:  Activist argument suspicion decreases (a) favorable attitudes toward and (b) voting in favor of the activist side on the issue. Asymmetric Response to Competing Industry and Activist Arguments The impact of argument strengthWhile we predict that argument strength will have a favorable effect for both the industry and activist sides, there is some evidence suggesting that effects are not equally favorable. We suggest, based on the difference in sociopolitical legitimacy ([ 9]; [61]), that strong industry arguments will have a weaker effect relative to strong activist arguments. Due to their negative views of industry practices, voters' legitimacy judgments may trigger doubt of the underlying intent of industry arguments. As a result, voters may not be as influenced even by strong arguments to support the industry's side as by strong arguments to support the activist side. For example, voters assessing a proposition to cap drug prices (e.g., Proposition 61 in Table 1) may mistrust the motives underlying pharmaceutical industry pricing practices, which could diminish the impact of a strong industry argument. In contrast, because voters are likely to have more positive legitimacy judgments of activist groups (e.g., the AIDS Healthcare Association, which works to make life-saving drugs more affordable), the strength of their arguments will not be diminished. This type of voter response is consistent with research showing that when a prior ad triggers doubts about that ad's intent, the effects of argument strength in subsequent ads are diminished ([16]). We hypothesize: H5:  Industry argument strength has a weaker positive effect than activist argument strength on (a) attitudes toward the issue and (b) voting on the issue. The impact of argument suspicionVoters' responses to influence attempts reflect their persuasion beliefs, and they are likely to be far more guarded against industry arguments than activist arguments (see [37]). This initial disadvantage for the industry side may ultimately offer it an opportunity to use campaign tactics that lead voters to put more weight on activist argument suspicion and/or less weight on industry argument suspicion.A method used by industries to increase voter suspicion of activist opponents is to intensify the public's doubt about activist-supported policy changes. Industries often argue that activist-side policy positions are impractical, ill-conceived, unrealistic, and costly ([ 6]; [31]). For instance, industry arguments might warn of hidden complexities or expenses, implying that activist arguments are not as genuine and aboveboard as they may seem ([19]). The tactic of using cogent, rational challenges to activist arguments is likely viewed as credible, as it seems cautious and shows due diligence. With this approach, the industry can heighten voters' formerly low persuasion knowledge of the activist side, increasing the weight of activist argument suspicion in voting decisions. This aligns with findings that previously trusted persuasion claims are further scrutinized when they are revealed to have hidden intentions (e.g., [13]), particularly in competitive contexts such as ours (e.g., [14]).[44] study of a 2012 California ballot measure (Proposition 37) to require labeling genetically engineered foods supports this view. They found that industry advertising helped flip voter support for the activist side (around 70% less than four weeks prior to the election) to majority support for the industry. Industry advertising focused on loopholes that would make implementation difficult and costly, while activist advertising (which lost traction over time) focused on the ruthlessness and deceit of the large industries involved. Although research in ballot measure, referendum, and other issue-focused election scenarios is limited, similar tactics are described in other sources (e.g., [33]; [53]). Following this logic and evidence, we hypothesize: H6:  Industry argument suspicion has a weaker negative effect relative to activist argument suspicion on (a) attitudes toward the issue and (b) voting on the issue. Exploring Responses of Vote-Switching SegmentsDirect-to-public persuasion is most effective when it actually switches voter support from one side to the other. While brand switching is well examined, less is known about vote switching, when persuasion campaigns are short in duration and have a common deadline ([58]). An analysis of vote switching is needed to show whether voting outcomes are due to each side's arguments solidifying voters' initial support or causing them to switch their support over the course of a campaign ([38]).There is limited guidance on whether the response asymmetry our model predicts would hold for voters who switched their support compared with those who did not or whether the drivers of switching could differ for those initially supporting the industry side compared with those initially supporting the activist side. Swings in public support are well documented by public opinion research, yet there is limited knowledge of how competing campaign arguments motivate voters to switch allegiance. Because research provides little insight on which to base predictions, we do not formally hypothesize effects on switching behavior but rather allow the empirical results to address the knowledge gaps. Field Study of Ballot MeasuresTo test our hypotheses, we conducted a large field study examining attitudes toward and voting on two California ballot measures involving distinct issues from different industries: Proposition 56, which proposed a tobacco tax increase, and Proposition 61, which proposed price limits on prescription drugs purchased for state programs (for examples of campaign media, see Web Appendix W1). These propositions featured highly salient, consumer-relevant issues in which the industries invested significantly in direct-to-public persuasion campaigns to defeat the measures. By investigating different issues and industries, we reduce the potential that our analyses capture idiosyncratic factors particular to one issue or one industry. MethodologyWe examined the impact of industry and activist arguments on attitudes and voting using survey data we collected using a national consumer research panel of California voters in two waves: five weeks before (measuring respondent characteristics, preelection attitudes, and voting intentions) and one week after (measuring argument strength, argument suspicion, postelection attitudes, and self-reported voting) the November 2016 national election. We collected data for the same respondents in both waves to analyze the extent to which industry- and activist-side persuasion campaigns shifted individual voters' attitudes and voting. SampleWe recruited respondents through Qualtrics, an online market research panel aggregator whose quality certification includes checking each person's IP address to exclude duplication and replacing those who finish the survey in less than one-third of the average completion time. Respondents were registered voters in California who indicated ""I will vote"" in the election in a screening question at the beginning of the preelection survey. After the election, we invited all 1,806 respondents of the preelection survey to answer the postelection survey. Of the 904 postelection survey respondents who voted in the election,[ 6] 58.4% were female, the mean age was 51 years old, 92.1% had some college education or higher, and 70.7% reported their socioeconomic status (SES) as middle to upper class. MeasuresIn the preelection survey, we captured preelection attitudes toward the issue in the context of the drug price and tobacco tax propositions using a three-item (""wrong/right,"" ""harmful/beneficial,"" and ""unacceptable/acceptable""), seven-point, previously validated scale and voting intention on the issue for both propositions as a binary variable (0 = intend to vote ""no,"" in favor of the industry side; 1 = intend to vote ""yes,"" in favor of the activist side). In addition, we measured several individual-level characteristics, including political affiliation, need for orientation, and demographic characteristics (age, gender, education, and SES).In the postelection survey, we measured our two dependent variables, attitudes toward the issue and voting on the issue, using the same multi-item scale used to capture preelection attitudes and a binary variable (0 = voted ""no,"" 1 = voted ""yes""), respectively. Respondents were then presented with two principal arguments from both the industry and activist sides for both propositions. To emphasize external validity, we identified the principal arguments from the California Official Voter Information Guide published by the office of the [12], which includes a set of Official Arguments formally submitted by each side (see Web Appendix W2). Respondents were asked to indicate their perceptions of argument strength and argument suspicion for each pair of industry arguments and activist arguments. We measured the two variables with two-item, seven-point, previously validated scales measuring the extent to which each of the arguments was convincing and aroused suspicion.[ 7] Finally, we measured familiarity with the arguments. For all multi-item scales, the mean of the items was used in the model estimation. Table 2 presents the descriptive statistics for all variables, and measurement details are provided in Web Appendix W2.GraphTable 2. Descriptive Statistics for Field Study Variables. VariablesDrug PriceProposition M (SD)Tobacco TaxPropositionM (SD)123456789101112131415161. Attitudes toward issue4.25 (2.09)5.10 (2.10)1−.408.753.006−.561.724−.240.162−.224.064−.126.100.118.010−.865.6722. Industry argument strength4.02 (1.78)3.64 (1.80)−.4481−.257−.157.406−.347.394−.009.099.027−.031−.071−.048.027.385−.3453. Activist argument strength4.41 (1.85)4.54 (1.93).739−.3321.109−.546.628−.147.319−.205.161−.150.040.117−.011−.707.5554. Industry argument suspicion3.96 (1.73)4.16 (1.79).220−.255.2891.045.079.062.025−.045−.020.008.041−.022−.073−.078.0725. Activist argument suspicion3.84 (1.90)3.51 (2.01)−.535.478−.488−.0061−.438.228−.068.150−.040.001−.023−.085.000.524−.4196. Preelection attitudes toward issue4.48 (1.86)4.94 (2.11).448−.272.433.184−.2731−.215.091−.218.088−.120.112.134.012−.708.8327. Familiarity with industry arguments4.88 (1.71)4.73 (1.79)−.178.401−.091−.007.237−.1121.378.046.120.138−.004.017−.118.268−.2618. Familiarity with activist arguments5.24 (1.52)5.46 (1.53).255−.042.400.110−.081.087.3591−.018.208.060.002.010−.028−.086.0409. Political affiliation3.31 (2.15)3.30 (2.14)−.240.140−.273−.139.181−.244.037−.1151−.097.083−.111.011−.102.224−.18210. Need for orientation5.32 (1.42)4.41 (1.78).057.025.106.059−.001.176.128.226−.0571−.010−.008−.011−.037−.053.03011. Age51.33 (15.60)51.02 (15.70)−.109−.067−.156−.037.020−.134.060.055.069.1151.003.066−.054.122−.13412. Education3.23 (.58)3.23 (.58).044.025−.032.023.028.038−.007−.038−.109−.040−.0011.321−.078−.087.10713. SES2.83 (.88)2.85 (.88).020.043−.028−.008.013.005.042−.014.026−.064.083.3051−.083−.145.12914. GenderM = 41.6%, F = 58.4%M = 41.6%, F = 58.4%−.032−.018.023−.028−.064−.035−.093−.014−.088.066−.052−.092−.0871.026.00615. Voting on the issueY = 52.3% N = 47.7%Y = 68.2% N = 31.8%−.789.437−.660−.191.482−.381.196−.207.249−.057.124−.070.019.0261−.70416. Preelection voting intention on issueY = 62.0% N = 38.0%Y = 67.2% N = 32.8%.381−.248.360.130−.274.722−.099.059−.211.135−.188−.027−.036−.021−.3941 2 Notes: Correlation coefficients below diagonal relate to drug price proposition (>|.067| are significant at p < .05; N = 851). Correlation coefficients above diagonal relate to tobacco tax proposition (>|.066| are significant at p < .05; N = 871).SES = socioeconomic status. ModelWe tested our hypotheses by estimating attitudes toward and voting on the issue as a function of argument strength and argument suspicion. For postelection attitudes toward the issue (Attitudest,i), we used an ordinary least squares (OLS) approach, specified as follows: Attitudest,i=β0+β1Industry_Argument_Strengtht,i+β2Activist_Argument_Strengtht,i+β3Industry_Argument_Suspiciont,i+β4Activist_Argument_Suspiciont,i+β5Attitudest−1,i+β6Familiar_Industryt,i+β7Familiar_Activistt,i+β8Political_Affiliationi+β9Orientationi+β10Agei+β11Genderi+β12Educationi+β13SESi+εi, Graphwhere the key predictor variables are industry argument strength (Industry_Argument_Strengtht,i), activist argument strength (Activist_Argument_Strengtht,i), industry argument suspicion (Industry_Argument_Suspiciont,i), and activist argument suspicion (Activist_Argument_Suspiciont,i). Covariates include preelection attitudes toward the issue (Attitudest − 1,i), familiarity with industry and activist arguments (Familiar_Industryt,i, Familiar_Activistt,i), political affiliation (Political_Affiliationi), need for orientation (Orientationi), age (Agei), gender (Genderi), education (Educationi), and socioeconomic status (SESi). For our voting on the issue outcome (Votet,i), we used a logit model with the same predictors and covariates as the attitudes toward the issue model, substituting preelection voting intentions (Votet − 1,i) in place of preelection attitudes.Our analysis was subject to several forms of bias, which we wanted to resolve. First, because we measured our dependent variables and key predictor variables using the same instrument, we adopted [41] approach to test for common method variance using a method factor capturing need for orientation on an unrelated ballot measure included in the survey instrument. Correlation coefficients corrected for common method variance indicate that this does not significantly bias our data (see Web Appendix W3).Second, we addressed endogeneity due to either reverse causality between argument strength and attitudes toward the issue or omitted variables using two-stage least squares estimation and comparing the estimated coefficients with the OLS estimated coefficients. We specified the first-stage equation to include a set of exogenous variables (the covariates from the Attitudest,i model) plus an instrumental variable, Reactancei, capturing individual i's resistance to a perceived threat to free choice or behavior. The instrument was relevant (i.e., correlated with argument strength) and met the exclusion criterion (i.e., uncorrelated with the error term of the Attitudest,i equation). In the second-stage equation, the estimated residuals from the first-stage regression were included in the Attitudest,i model to control for the endogenous regressors. Our test indicates that endogeneity does not bias the OLS coefficients (for details regarding the appropriateness of our instrument and model results, see Web Appendix W4).Third, because a subset of respondents from the preelection survey opted to complete the postelection survey, we addressed selection bias using [30] two-step estimation of the effect of argument strength on attitudes toward and voting on the issue. In the first-stage selection equation, Votedt,i (binary variable indicating whether or not the respondent voted on the proposition in the election) is regressed on our covariates plus the conceptually unrelated variable Reactancei. The second-stage outcome equations for attitudes and voting were estimated using the Attitudest,i and Votet,i model specifications, respectively, conditioned on whether the respondent voted on the proposition in the election. From the estimated coefficients for the second-stage model, including the inverse Mills ratio, we conclude that selection bias did not adversely affect model estimation (see Web Appendix W5). Results The Influence of Competing Persuasion Campaign ArgumentsA model-free examination of the data offers preliminary evidence of our predicted relationships (see Web Appendix W6). Table 3 contains the results related to our hypothesized effects. Note that for our analysis, as industry is opposed to the proposition, negative coefficients indicate favorable attitudes toward and voting in favor of the industry side. We find consistent support for H1 and H2. Stronger industry arguments led to more favorable attitudes toward the industry side on both the drug price (βIndustry_Argument_Strength = −.178, p < .01) and tobacco tax (βIndustry_Argument_Strength = −.128, p < .01) propositions and increased the likelihood of voting in favor of the industry side on the drug price (βIndustry_Argument_Strength = −.623, p < .01; 65.1% likelihood of voting no[ 8]) and tobacco tax (βIndustry_Argument_Strength = −.476, p < .01; 61.7% likelihood of voting no) propositions. Similarly, stronger activist arguments led to more favorable attitudes toward this side for the drug price (βActivist_Argument_Strength = .630, p < .01) and tobacco tax (βActivist_Argument_Strength = .471, p < .01) propositions and increased the likelihood of voting in favor of this side for the drug price (βActivist_Argument_Strength = 1.300, p < .01; 78.6% likelihood of voting yes) and tobacco tax (βActivist_Argument_Strength = 1.206, p < .01; 77.0% likelihood of voting yes) propositions.GraphTable 3. Impact of Industry and Activist Argument Strength and Suspicion on Attitudes Toward and Voting on the Issue. Hyp.VariablesAttitudes Toward the IssueVoting on the IssueDrug Price PropositionTobacco Tax PropositionDrug Price PropositionTobacco Tax PropositionCoefficientEffect Sizeηp2CoefficientEffect Sizeηp2CoefficientOdds RatioCoefficientOdds RatioConstant1.672***2.413***–3.126***–1.672*Predictor VariablesH1 Industry argument strength−.178***.035−.128***.026−.623***.536−.476***.621H2 Activist argument strength.630***.285.471***.2021.300***3.6571.206***3.339H3 Industry argument suspicion−.008.000−.026.002.194**1.214.0781.082H4 Activist argument suspicion−.172***.037−.120***.026−.391***.676−.283***.753Covariates Preelection attitudes toward the issue.129***.025.349***.185 Preelection voting intention on the issue1.233***3.4302.604***13.520 Familiarity: industry arguments−.032.001−.027.001−.088.916−.092.912 Familiarity: activist arguments.023.001.003.000−.177*.838−.221**.801 Political affiliation−.007.000−.028*.002−.072*.931−.145**.865 Need for orientation−.015.000−.047**.005−.069.934−.0051.005 Age−.002.001−.002.001−.006.994.0021.002 Gender.213**.006−.028.000.2841.328.424*1.527 Education.192**.007.117*.003.692***1.998−.090.914 SES.060.002−.009.000−.090.914.2181.244 N851871851871 R2.628.703.534.574 AIC1179.7601091.398H5 Test of equality of industry vs. activist argument strength coefficients (t-value)−4.077***−2.821***−3.551***−3.400***H6 Test of equality of industry vs. activist argument suspicion coefficients (t-value)−2.618***−1.969**−2.249**−2.192** 3 *p < .10.4 **p < .05.5 ***p < .01.6 Notes: One-tailed tests of significance. AIC = Akaike information criterion. SES = socioeconomic status. For attitudes toward the issue, negative coefficients indicate favorable attitudes toward the industry side on the issue (as industry opposes the proposed policy); positive coefficients indicate favorable attitudes toward the activist side on the issue (as activist side supports the proposed policy). For voting on the issue, negative coefficients indicate voting in favor of the industry side on the issue (as industry opposes the proposed policy); positive coefficients indicate voting in favor of the activist side on the issue (as the activist side supports the proposed policy).Our results offer mixed support for H3, as suspicion of industry arguments had little impact on voting outcomes. It had no effect on attitudes toward either proposition, contrary to H3a. It significantly decreased the likelihood of voting in favor of the industry side for the drug price proposition (βIndustry_Argument_Suspicion = .194, p < .05; 45.2% likelihood of voting no), but not for the tobacco tax proposition, partially supporting H3b. By contrast, suspicion of activist arguments played a consistent role, fully supporting H4, resulting in less favorable attitudes toward that side for the drug price (βActivist_Argument_Suspicion = −.172, p < .01) and tobacco tax (βActivist_Argument_Suspicion = −.120, p < .01) propositions and decreased likelihood of voting in favor of that side for the drug price (βActivist_Argument_Suspicion = −.391, p < .01; 40.3% likelihood of voting yes on the proposition) and tobacco tax (βActivist_Argument_Suspicion = −.283, p < .01; 43.0% likelihood of voting yes) propositions. Asymmetric Response to Competing Industry and Activist Arguments The impact of argument strengthTo compare the relative impact of industry and activist argument strength in predicting our key outcomes, we used the delta method to test the equality of the estimated coefficients for the two variables ([28]). Specifically, we calculated t = (βIndustry_Argument_Strength − βActivist_Argument_Strength)/SE, where βIndustry_Argument_Strength and βActivist_Argument_Strength are the absolute value of the two parameter estimates and SE is the standard error of their difference calculated as Sqrt[Var(βIndustry_Argument_Strength) + Var(βActivist_Argument _Strength) − 2 × Cov(βIndustry_Argument_Strength, βActivist_Argument_Strength)]. The results provide full support for H5, indicating that activist argument strength has a significantly greater impact than industry argument strength on attitudes toward the drug price (t = −4.077, p < .01) and tobacco tax (t = −2.821, p < .01) propositions and on voting for the drug price (t = −3.551, p < .01) and tobacco tax (t = −3.400, p < .01) propositions. This is consistent with a comparison of effect sizes, which indicates that activist argument strength has a strong effect on attitudes toward both the drug price (  ηp2   = .285) and tobacco tax (  ηp2   = .202) propositions, while industry argument strength has a moderate-small effect for the drug price (  ηp2   = .035) and tobacco tax (  ηp2   = .026) propositions.[ 9] The impact of argument suspicionUsing the delta method to compare the relative impact of industry and activist argument suspicion in predicting attitudes and voting revealed that activist argument suspicion has a significantly greater impact than industry argument suspicion on attitudes toward the drug price (t = −2.618, p < .01) and tobacco tax (t = −1.969, p < .05) propositions and on voting for the drug price (t = −2.249, p < .05) and tobacco tax (t = −2.192, p < .05) propositions, in support of H6. A comparison of effect sizes reflects this difference, such that activist argument suspicion has a moderate-small effect on attitudes toward the issue for both the drug price (  ηp2   = .037) and tobacco tax (  ηp2   = .026) propositions, but industry argument suspicion has a negligible effect for the drug price (  ηp2   = .000) and tobacco tax (  ηp2   = .002) propositions. Effect sizes for all parameters and overall model statistics are reported in Table 3. Robustness ChecksAlthough our survey respondents' characteristics generally align with the demographics of California voters in the 2016 election based on U.S. Census Bureau data in terms of gender, age, and income, our sample included higher proportions of individuals with higher education levels and whites/Caucasians and lower proportions of Hispanics (see Web Appendix W7). To help ensure that our results are representative of California voters, we performed a robustness check in which we weighted observations using an iterative proportional fitting procedure, which takes the marginal distributions of demographic variables from the California voter population and returns weights such that the marginal distributions of the demographic variables in the weighted survey data match those in the population ([69]). Compared with results for the unweighted models, our analysis finds consistent effects for 15 of the 16 total hypothesized effects of the impact of argument strength and argument suspicion on attitudes toward and voting on the issue for the two propositions (see Web Appendix W8). Exploration of Vote Switching SegmentsOf the two ballot measures we study, the drug price proposition experienced significant swings in public support. Political polling in California leading up to the election indicated that 73% of registered voters supported the proposition in late July ([63]), but this decreased to 66% in mid-September (USC Dornslife/LA Times) and further to 51% in mid-October (Hoover Institute/YouGov). Following intensive pharmaceutical industry media spending in the six weeks before the election, the proposition was defeated in the November election with 48.6% of support (all polling data retrieved from https://ballotpedia.org/California%5fProposition%5f61,%5fDrug%5fPrice%5fStandards%5f(2016)).[10] Such a swing in support did not occur for the tobacco tax proposition.We considered two scenarios. First, we focused on voters who intended to vote in favor of the activist side in the preelection survey (i.e., would vote yes on the proposition) and measured our dependent variable by identifying those who actually voted in favor of that side (voted yes) versus those who switched and actually voted in favor of the industry side (voted no) as measured in the postelection survey (1 = switched vote, 0 = did not switch vote). Second, we focused on those who intended to vote in favor of the industry side and measured the dependent variable by identifying those who actually voted in favor of the industry side versus those who switched and voted in favor of the activist side.As Table 4 shows, argument strength and argument suspicion play important roles in vote switching (all results apply to both the drug price and tobacco tax propositions). Strong industry (activist) arguments significantly increased (decreased) the likelihood of voters switching support from the activist side to the industry side and decreased (increased) the likelihood of switching support from the industry side to the activist side. Consistent with our previous analyses, activist argument strength had a greater impact than industry argument strength in both scenarios. The results related to argument suspicion reveal a notable effect that may have been masked in our previous analyses. Consistent with our previous findings, suspicion of activist arguments significantly increased (decreased) voter switching to the industry (activist) side. However, in contrast with our previous findings, suspicion of industry arguments significantly increased the likelihood that voters who initially supported the industry switched their support to the activist side. This distinctive finding is bolstered by a test of equality on the impact of argument suspicion for the opposing sides, which indicates that suspicion of activist arguments does not play a significantly greater role than suspicion of industry arguments in this scenario.GraphTable 4. Impact of Industry and Activist Argument Strength and Suspicion on Vote Switching. VariablesSwitched from Intending to Vote in Favor of Activist Side to Voting in Favor of Industry SideSwitched from Intending to Vote in Favor of Industry Side to Voting in Favor of Activist SideDrug Price PropositionTobacco Tax PropositionDrug Price PropositionTobacco Tax PropositionCoefficientOdds RatioCoefficientOdds RatioCoefficientOdds RatioCoefficientOdds RatioConstant.633.553−5.874***−.140Predictor Variables Industry argument strength.635***1.887.394**1.483−.592***.554−.642***.526 Activist argument strength−1.275***.280−1.315***.2681.360***3.8951.306**3.689 Industry argument suspicion−.155.856.1181.126.294**1.342.400**1.487 Activist argument suspicion.463***1.589.287**1.332−.256**.774−.346**.707Covariates Familiarity: industry arguments.202*1.224.1301.138.0561.057−.044.957 Familiarity: activist arguments.1441.155.2431.275−.239.787−.203.817 Political affiliation.120**1.127.0111.011−.018.982.367***.693 Need for orientation.0571.059−.010.990−.054.948.0771.080 Age.0071.007−.005.995−.006.994−.002.998 Gender−.163.850−.726**.484.671**1.956.091.913 Education−.730***.482−.448.639.692**1.998−1.191**.304 SES.1691.184−.001.999.1041.109.773**2.166 N851871851871 R2.460.237.464.432 AIC665.533352.953380.347298.456 Test of equality of industry vs. activist argument strength coefficients (t-value)−2.525**−3.140***−2.551**−1.971** Test of equality of industry vs. activist argument suspicion coefficients (t-value)−1.970**1.691*.151.191 7 *p < .10.8 **p < .05.9 ***p < .01.10 Notes: One-tailed tests of significance. AIC = Akaike information criterion. SES = socioeconomic status. For attitudes toward the issue, negative coefficients indicate favorable attitudes toward the industry side on the issue (as industry opposes the proposed policy); positive coefficients indicate favorable attitudes toward the activist side on the issue (as activist side supports the proposed policy). For voting on the issue, negative coefficients indicate voting in favor of the industry side on the issue (as industry opposes the proposed policy); positive coefficients indicate voting in favor of the activist side on the issue (as the activist side supports the proposed policy).Our field study findings affirm the relationships predicted in our framework and hypotheses. In the next section, we develop hypotheses predicting how two distinctive and prominent argumentation strategies, one focused on financial factors and one focused on societal factors, vary in effectiveness for the industry and activist sides. We then test our predictions using three experimental studies. Comparing Argumentation Strategy Effectiveness for the Industry and Activist SidesPrior research on perceptual fit suggests that argument effectiveness would vary depending on fit with the persuader (e.g., [60]). The positive effects of high fit between a firm or a brand and entities with which it is associated, such as sponsorships and cause-related marketing, are well established ([54]; [60]). Despite this, [ 6] document that opposing sides, including industry and activist groups, tend to use the same few strategies in the same policy battle, indicating that the fit between the persuader and the argument appears to be often overlooked. This raises an important question: Would a more focused approach be more effective and, if so, how should it be chosen?To address this question, we examine argumentation strategies consistent with the image of the two sides in the conflicts we study. Financial argumentation is based on factors related to imposing or reducing financial benefits or costs, whereas societal argumentation is based on (nonfinancial) factors related to inhibiting or promoting shared societal goals or values. With financial argumentation, an industry may claim that there will be onerous financial costs for the public if current policy changes, whereas an activist group may argue that current policy needlessly burdens taxpayers ([19]; [67]). Using societal argumentation, an industry may claim that a policy change would weaken consumer access to important products or services, whereas an activist group may argue that current policy serves only a privileged few ([57]).Fit between an organization and an argument is high when the two are perceived as congruent because they share similar intangible associations ([60]). Argumentation based on financial factors is congruent with how industries are viewed, as an industry's image is characterized by economic and operational aspects ([10]). Analysts' reports profile industries in terms of financial and market performance and industry actions directly affect the economic status of consumers (e.g., [ 2]). Because financial argumentation is more congruent with public expectations of industry, this strategy is a better fit with the industry side and will lead to more effective persuasion ([54]). As such, we predict that financial argumentation will be perceived as stronger and less suspicious when used by the industry side.In contrast, argumentation based on societal factors is congruent with the image of activist groups, which generally are perceived as organizations that represent the collective good for citizens ([59]). These organizations build consensus on higher-level principles and fundamental values such as social justice ([ 7]; [56]). Because the activist side is focused on promoting shared societal goals or values, its image is more congruent with societal argumentation. As such, we predict that societal argumentation will be perceived as stronger and less suspicious when used by the activist side.Although empirical evidence to inform our predictions is limited, research on organizational legitimacy provides some conceptual support by suggesting that industries battling to control policy most often assume a market and economic orientation, expressed by referencing price, competitiveness, and efficiency, whereas groups representing the public interest often assume a civic orientation, expressed by referencing collective interest, solidarity, and integrity ([10]). In summary, we predict: H7:  The effect of argumentation strategy on argument strength and argument suspicion is moderated by the competing side on the issue, such that For the industry side, financial argumentation leads to higher argument strength and lower argument suspicion than societal argumentation. For the activist side, societal argumentation leads to higher argument strength and lower argument suspicion than financial argumentation. For financial argumentation, use by the industry side leads to higher argument strength and lower argument suspicion than use by the activist side. For societal argumentation, use by the activist side leads to higher argument strength and lower argument suspicion than use by the industry side.Building on our previous predictions that argument strength and argument suspicion impact attitudes toward and voting on the issue, we expect these evaluations to mediate the effect of an argumentation strategy on voting outcomes. Specifically: H8:  The mediating role of argument strength and argument suspicion depends on the competing side on the issue, such that For the industry side, financial (vs. societal) argumentation increases argument strength and decreases argument suspicion, which then increases favorable attitudes toward and voting in favor of the industry side on the issue. For the activist side, societal (vs. financial) argumentation increases argument strength and decreases argument suspicion, which then increases favorable attitudes towards and voting in favor of the activist side on the issue. Experimental Studies of Argumentation Strategy EffectsWe test H7 and H8 in three experimental studies on different issues addressed in recent ballot measures (see Table 1): pharmaceutical drug price standards, which aligns with one of our field study contexts; recyclable bottle deposits; and renewable energy standards. In a controlled, randomized setting, we exposed participants to two opposing messages (one from each side on the issue) that used either financial or societal argumentation strategies, thus separating the effects of argumentation strategy from those of frequency of exposure ([14]). Experimental Study 1 MethodologyWe used a mixed experimental design with two manipulations administered at two points in time within the same survey. Participants were first introduced to a hypothetical scenario in which the residents of a U.S. state are scheduled to vote on a proposition to adopt statewide pharmaceutical drug price standards and were sequentially shown two messages related to the proposition. First, participants saw one of four messages in a 2 (argumentation strategy: financial vs. societal) × 2 (side on the issue: industry vs. activist) between-subjects experimental design (time 1). Next, in the same survey, we manipulated argumentation strategy within subject; depending on which argumentation strategy participants saw at time 1, they saw one of the two strategies (financial or societal) for the opposing side (time 2). In each condition, participants saw a different combination of arguments from each side (both financial; both societal; or one of each, in a different order), enabling us to examine perceptions of argument strength and suspicion of each strategy in the presence of opposing arguments.In the introduction, participants were told that Proposition 25 asks voters to decide whether their state should adopt standards to restrict the amount that some state agencies and programs pay for selected prescription drugs (for stimuli, see Web Appendix W9). We then introduced each message one at a time, indicating that it is a message designed by supporters (opponents) of Proposition 25 to persuade voters to vote yes (no) and support (defeat) the proposition. We derived the arguments used in our manipulations from the official arguments in the California Official Voter Information Guide for Proposition 61 (Drug Price Standards, November 8, 2016).[11]After participants viewed the two arguments, they indicated their attitudes toward the proposition and how likely they were to vote yes or no in the referendum (1 = ""I will definitely vote NO and oppose this proposition,"" and 7 = ""I will definitely vote YES and support this proposition""). Next, they saw the message viewed at time 1 and evaluated its argument strength and suspicion. After this, they undertook the same evaluations for the message viewed at time 2. We then assessed political affiliation, need for orientation, age, gender, education, and SES. Detailed descriptions of each of the measures appear in Web Appendix W10.To enhance external validity, we recruited a nationally representative sample of registered voters. We partnered with Qualtrics to recruit 659 registered voters (49.68% female; Mage = 52.05 years; age range: 18–91 years). Qualtrics enforced quotas so that the sample was proportional to the U.S. population in terms of region, gender, age, household income, education, and ethnicity based on census data. Because the issue was voted on in California in 2016 and Ohio in 2017, we dropped 33 participants who indicated residency in these states, which left a sample of 626 participants for analyses. We conducted a pretest to confirm that our argumentation strategy manipulation was successful (for details, see Web Appendix W11). ResultsGiven the within-subject nature of our design, in which we measure strength and suspicion of the second argument after participants evaluate the first argument, carryover and anchoring effects inherent to the design limit the interpretation of the second message evaluation ([51]). Thus, to test the hypothesized effects in the presence of arguments from both sides, while ensuring that the argument strength and suspicion measures are not subject to bias, we use participants' evaluations of the first argument and control for the argument evaluated second.We used PROCESS Model 58 ([29]) to test our full conceptual model (see Table 5). An OLS regression on argument strength, with argumentation strategy (1 = financial, 0 = societal), side on the issue (1 = industry, 0 = activist), and their interaction as predictors. Covariates were individual-level characteristics and the argumentation strategy participants saw at time 2. The analysis revealed significant negative main effects of argumentation strategy (bfinancial = −.529, p < .01) and side on the issue (bindustry = −.607, p < .01) and the predicted interaction effect (bfinancial × industry = .841, p < .01). A similar regression on argument suspicion revealed significant positive main effects of argumentation strategy (bfinancial = .384, p <= .05) and side on the issue (bindustry = .627, p < .01) and the predicted interaction effect (bfinancial × industry = −.744, p < .01).GraphTable 5. Response to Argumentation Strategies from Competing Sides: Drug Price Standards Proposition. Dependent VariablesHyp.Predictor VariablesCoefficient (Effect Size,ηp2)Moderating Effect of Issue Side on Argumentation StrategyArgument StrengthFinancial argumentation strategy (time 1, 0 −1)−.529*** (.001)Industry side (time 1, 0−1)−.607*** (.003)Financial argumentation strategy × Industry side.841*** (.026)Intercept4.104***R2.123Model significanceF(10, 611) = 8.581***Moderating Effect of Issue Side on Argumentation StrategyArgument SuspicionFinancial argumentation strategy (time 1, 0−1).384** (.000)Industry side (time 1, 0−1).627*** (.007)Financial argumentation strategy × Industry side−.744*** (.017)Intercept3.747***R2.044Model significanceF(10, 611) = 2.851***Effects of Argument Strength and Argument SuspicionAttitudes Toward the IssueVoting on the IssueFinancial argumentation strategy (time 1, 0−1).014 (.000).019 (.000)Argument strength.589*** (.032).631*** (.045)Argument suspicion−.266*** (.051)−.211*** (.050)Industry side (time 1, 0 −1).766 (.000).621 (.004)Argument strength × Industry side−.683*** (.073)−.681*** (.065)Argument suspicion × Industry side.533*** (.210).509*** (.186)Intercept2.134***2.016***R2.330.299Model significanceF(13, 608) = 23.063***F(13, 608) = 19.977***H8aConditional indirect effect through argument strength, industry−.029 [−.120,.032]−.015 [−.100,.053]H8aConditional indirect effect through argument suspicion, industry−.096 [−.230, −.006]−.107 [−.261, −.007]H8bConditional indirect effect through argument strength, activist−.312 [−.564, −.101]−.334 [−.578, −.110]H8bConditional indirect effect through argument suspicion, activist−.102 [−.245, −.005]−.081 [−.207, −.001]Index of moderated mediation (argument strength).282 [.059,.541].319 [.086,.576]Index of moderated mediation (argument suspicion).006 [−.158,.174]−.026 [−.203,.130]Simple Effects of Issue Side on Argument Strength and SuspicionArgument StrengthArgument SuspicionH7aSimple effects of financial (vs. societal) argumentation for industry side.312*−.360**H7bSimple effects of financial (vs. societal) argumentation for activist side−.529***.384**H7cSimple effects of industry (vs. activist) side for financial argumentation.234−.117H7dSimple effects of industry (vs. activist) side for societal argumentation−.607***.627*** 11 *p < .10.12 **p < .05.13 ***p < .01.14 Notes: Results from PROCESS Model 58, which includes need for orientation, political affiliation, age, gender (male), education, SES, and argumentation strategy at time 2 (0−1) as covariates; 95% confidence intervals reported for conditional indirect effects; n = 622 for these analyses due to missing values on age and SES covariates.Tests of the simple effects of argumentation strategy for each side on the issue showed that for the industry side, financial argumentation led to marginally significantly higher argument strength (bfinancial = .312, p < .10) and significantly lower argument suspicion (bfinancial = −.360, p < .05), as compared with societal argumentation, in support of H7a. For the activist side, societal argumentation led to significantly higher argument strength (bfinancial = −.529, p < .01) and lower argument suspicion (bfinancial = .384, p < .05), as compared with financial argumentation, supporting H7b (negative coefficients indicate enhanced effects and positive coefficients indicate diminished effects because side on the issue is coded as industry = 1 and activist = 0).Tests of the simple effects of side on the issue for each argumentation strategy revealed that, for financial argumentation, use by the industry side was not significantly different than use by the activist side for both argument strength (bindustry = .234, p > .10) and argument suspicion (bindustry = −.117, p > .10), contrary to H7c. However, in support of H7d, for societal argumentation, use by the activist side led to significantly higher argument strength (bindustry = −.607, p < .01) and significantly lower argument suspicion (bindustry = .627, p < .01) than use by the industry side.For the full moderated mediation model, for the industry side, argument suspicion mediated the effects of argumentation strategy on attitudes and voting (conditional indirect effect on attitude: b = −.096, 95% confidence interval [CI]: [−.230, −.006]; conditional indirect effect on voting: b = −.107, 95% CI: [−.261, −.007]), but argument strength did not (conditional indirect effect on attitude: b = −.029, 95% CI: [−.120,.032]; conditional indirect effect on voting: b = −.015, 95% CI: [−.100,.053]), partially supporting H8a. For the activist side, both argument strength (conditional indirect effect on attitude: b = −.312, 95% CI: [−.564, −.101]; conditional indirect effect on voting: b = −.334, 95% CI: [−.578, −.110]) and argument suspicion (conditional indirect effect on attitude: b = −.102, 95% CI: [−.245, −.005]; conditional indirect effect on voting: b = −.081, 95% CI: [−.207, −.001]) mediated the effects of argumentation strategy on attitudes and voting, fully supporting H8b.[12] Experimental Studies 2 and 3To assess the generalizability of our findings across different issues, we conducted two additional experimental studies. Both studies used the same design as Experimental Study 1. Experimental Study 2 (331 U.S.-based participants from the Prolific Academic panel; 44.11% female; Mage = 33.19 years, age range: 18–73 years) focused on expanding a state's bottle deposit law to require deposits for all nonalcoholic, noncarbonated drinks (Expansion of Bottle Deposits Initiative, Massachusetts, November 4, 2014). Experimental Study 3 (340 U.S.-based participants from Prolific Academic; 50.88% female; Mage = 34.70 years, age range: 18–73 years) focused on increasing a state's renewable energy standards, requiring electricity providers to obtain at least 50% of their electricity from renewable sources by 2030 (Renewable Energy Standards Initiative, Arizona, November 6, 2018). The stimuli are presented in Web Appendix W9 and pretests to confirm our argumentation strategy manipulations are included in Web Appendix W11.Results from the two studies were largely identical to those of Experimental Study 1 (see Table 6). For both propositions, as expected and in line with Experimental Study 1, for the industry side, financial argumentation led to higher argument strength and lower argument suspicion than societal argumentation, while for the activist side, societal argumentation led to higher argument strength and lower argument suspicion than financial argumentation. Comparing side on the issue effects for each argumentation strategy, for societal argumentation, use by the activist side led to higher argument strength and lower suspicion than use by the industry side but, as in Experimental Study 1 and contrary to predictions, participants did not perceive financial argumentation as significantly stronger or less suspicious when used by the industry versus the activist side. Across both propositions, argument strength mediated the effect of argumentation strategy on attitudes toward and voting on the issue for both the industry and activist sides. Argument suspicion also had significant mediating effects for the industry and activist sides across both propositions with two exceptions. For the renewable energy standards proposition, argument suspicion mediated the effect of argumentation strategy on voting but not on attitudes toward the industry side, and argument suspicion had a significant mediating effect on attitudes but not on voting for the activist side.[13]GraphTable 6. Response to Argumentation Strategies from Competing Sides: Bottle Deposits and Energy Standards Propositions. Bottle Deposits PropositionEnergy Standards PropositionHyp.Predictor VariablesDV: Coefficient (Effect Size,ηp2)DV: Coefficient (Effect Size,ηp2)Moderating Effect of Issue Side on Argumentation StrategyArgument StrengthArgument StrengthFinancial argumentation strategy (time 1, 0 −1)−.911*** (.001)−.747*** (.040)Industry side (time 1, 0−1)–1.959*** (.138)–2.566*** (.168)Financial argumentation strategy × Industry side1.575*** (.075)2.573*** (.165)Intercept3.842***5.186***R2.232.322Model significanceF(10, 310) = 9.386***F(10, 322) = 15.355***Moderating Effect of Issue Side on Argumentation StrategyArgument SuspicionArgument SuspicionFinancial argumentation strategy (time 1, 0 −1).587*** (.001).538** (.004)Industry side (time 1, 0−1)1.658*** (.169)2.054*** (.195)Financial argumentation strategy × Industry side−.982*** (.032)–1.334*** (.054)Intercept2.837***2.287***R2.201.242Model significanceF(10, 310) = 7.819***F(10, 322) = 10.300***Effects of Argument Strength and Argument SuspicionAttitudesVotingAttitudesVotingFinancial argumentation strategy (time 1, 0−1).451*** (.041).503*** (.029).205 (.024).024 (.007)Argument strength.428*** (.008).602*** (.008).208** (.024).430*** (.017)Argument suspicion−.273*** (.010)−.234** (.002)−.271*** (.005)−.068 (.001)Industry side (time 1, 0 −1).546 (.015)1.180 (.003).675 (.008)2.215** (.012)Argument strength × Industry side−.817*** (.145)–1.068*** (.149)−.522***(.126)−.833*** (.192)Argument suspicion × Industry side.648*** (.376).756*** (.358).383*** (.320).255* (.311)Intercept3.579***2.172**5.173***3.057**R2.477.460.480.497Model significanceF(13, 307) = 21.558***F(13, 307) = 20.119***F(13, 319) = 22.654***F(13, 319) = 24.249***H8aConditional indirect effect through argument strength, industry−.258 [−.492, −.081]−.309 [−.623, −.075]−.573 [−.900, −.270]−.735 [–1.144, −.365]H8aConditional indirect effect through argument suspicion, industry−.148 [−.326, −0010]−.205 [−.461, −.010]−.088 [−.238,.035]−.148 [−.353, −.006]H8bConditional indirect effect through argument strength, activist−.390 [−.672, −.158]−.549 [−.895, −.222]−.155 [−.335, −.024]−.321 [−.607,.097]H8bConditional indirect effect through argument suspicion, activist−.161 [−.344, −.021]−.137 [−.350, −.004]−.146 [−.319, −.017]−.037 [−.177,.108]Index of moderated mediation (argument strength).132 [−197,.473].239 [−.209,.669]−.417 [−.775, −.086]−.413 [−.891, 026]Index of moderated mediation (argument suspicion).013 [−.213,.245]−.068 [−.365,.226].057 [−.140,.263]−.111 [−.365,.088]Simple Effects of Issue Side on Argument Strength and SuspicionArgument StrengthArgument SuspicionArgument StrengthArgument SuspicionH7aSimple effects of financial argumentation for industry side.664***−.394*1.826***−.796***H7bSimple effects of financial argumentation for activist side−.911***.587***−.747***.538**H7cSimple effects of industry side for financial argumentation−.383.676 ***.006.719***H7dSimple effects of industry side for societal argumentation–1.959 ***1.658 ***–2.566***2.054 *** 15 Notes: Results from PROCESS Model 58, which includes need for orientation, political affiliation, age, gender (male), education, SES, and argumentation strategy at time 2 (0 −1) as covariates; for bottle deposits study, 10 participants from MA, where proposition was voted on, were dropped from analyses; for the energy standards study, 7 participants from AZ were dropped from analyses; 95% confidence intervals reported for conditional indirect effects; * p < .10, ** p < .05, *** p < .01.In summary, across the three propositions, we obtained consistent support for the mediating role of argument strength and argument suspicion (H8a and H8b). Our results consistently show that societal argumentation is more effective for the activist side: it is more effective than financial argumentation for the activist side (H7b), and is perceived as stronger and less suspicious when used by the activist side than the industry side (H7d). For financial argumentation, results were somewhat mixed: as expected, financial argumentation was more effective than societal argumentation for the industry side (H7a). However, it was not perceived as significantly stronger and less suspicious when used by the industry versus the activist side (H7c). We summarize the results across all experimental studies in Web Appendix W12. Exploration of the Role of Political AffiliationWe contend that battles between industry and activist groups over consumer-related issues tend to be less partisan than other political scenarios such as candidate elections. For example, major early polls reported 77% of Democrats and 70% of Republicans supported Proposition 61, the drug price standards proposition examined in our field study and Experimental Study 1 ([52]; [63]).To gain insight into the impact of partisanship in our context, we performed additional regression analyses to examine the extent to which political affiliation moderates the effects of side on the issue and argumentation strategy on argument strength and argument suspicion across the three issue contexts. Overall, our results indicate that only one of the six interaction effects between political affiliation and argumentation strategy was significant, but all six interaction effects of political affiliation and side on the issue were significant (see Web Appendix W13). The strongest Democrats perceived activist argumentation more favorably than industry argumentation, but there were no significant differences in how Republicans perceived the two sides' argumentation. These results suggest that political affiliation may play a role in how voters view the industry and activist sides. This is consistent with recent polls indicating that the majority of conservatives (liberals) have a positive (negative) view of big business ([55]) and is in line with current polarization along party lines in many domains in the United States ([ 8]; [57]). DiscussionWe find support for our direct-to-public persuasion model across a field study and three experimental studies examining four policy conflict scenarios recently voted on in U.S. state ballot measures. Our findings show that industry and activist arguments play a key role in voting decisions, but industry arguments have less impact than activist arguments. Stronger arguments from both sides lead to more favorable outcomes, but activist groups benefit most. Similarly, industry argument suspicion has limited influence, except for voters who switch their support to the activist side. While societal argumentation is the preferred strategy for the activist side and financial argumentation is preferred for the industry side, the industry's competitive advantage is far less pronounced than expected. Theoretical ImplicationsOur exploration of direct-to-public persuasion increases the breadth of persuasion theory within the marketing domain. Persuasion campaigns in political settings—in which opinions are often strongly held, outcomes are win or lose, and consequences are societal rather than tied to individual consumers—are more complex in their effects than persuasion in commercial settings ([27]; [58]). As there is little theoretical grounding to build on, our research offers new ways of understanding this unique form of persuasion and adds to the scope of sociopolitical legitimacy theory, research on voting behavior, and perceptual fit theory.Our findings related to an asymmetric public response in industry versus activist conflicts caution against assumptions that competing campaigns have an equal opportunity to persuade the public. By addressing unexplored differences in public response to competing campaigns, including legitimacy differences, our research updates persuasion theory by examining how and why arguments from competing sides affect voting behavior. Our results show that competition in politics is uneven, encompassing not just resource imbalances that may advantage one side, but also asymmetries across sides in the extent to which persuasion knowledge plays a role in voters' response (see [17]; [37]). Because the effects of dueling campaign arguments over time are not well known, yet are central to political outcomes, our findings highlight an important direction for future persuasion research.Although theoretical guidance on the impact of argumentation on vote switching behavior is scarce, our results reveal an unexpected role of industry argument suspicion in that, although it has limited effect when voters are considered in aggregate, it is a key driver for the segment of voters who switch their support to the activist side. This finding is novel for persuasion research, as it indicates that the industry argument suspicion effects that drive a change in voter support over the course of a campaign differ somewhat from those driving overall voter support. This implies that response to the competing sides is not only asymmetric but nuanced and that research needs to consider various outcome measures to capture underlying factors.Our finding that the industry side is most effective with a financial argumentation strategy while the activist side is most effective with societal argumentation offers new theoretical support regarding the need for fit between the persuader's identity and arguments. This complements the perceptual fit literature that has traditionally addressed the fit between a company's actions and claims ([64]) or a company's identity and cause-related marketing choices ([54]). We contribute to perceptual fit theory by finding that fit does not have equal importance for both sides, as an image-congruent argumentation strategy is vital for the industry, but voters may be more tolerant of a broader strategy for the activist side. Implications for PracticeOur results offer practical guidance that differs for the industry versus activist sides related to the substance of their arguments and their goals of acquiring or retaining supporters (see Table 7). The findings are relevant for marketers and practitioners who design and implement direct-to-public campaigns with industry associations, corporations, public interest advocacy and activist groups, consultancies, and other policy-focused coalitions ([59]; [65]).GraphTable 7. Key Findings and Guidance for Industries and Activist Groups. Key Findings for Winning Public SupportGuidance for Industry PracticeGuidance for Activist Group PracticeSubstance of Principle ArgumentsStrong arguments benefit both sides, but activist argument strength has a greater impact than industry argument strengthDevelop campaign arguments focused on diminishing the strength of activist argumentsDevelop campaign arguments with ironclad strength and the potential to preempt industry argumentsSuspicion of activist arguments negatively affects outcomes, but industry argument suspicion has limited impactTake an aggressive approach, raising skepticism of activist tactics, but avoid triggering a backlashTake a defensive approach, preserving the legitimacy of activist group practices and argumentsIndustry arguments with a financial focus are perceived as stronger and less suspicious; activist group arguments with a societal focus are stronger and less suspiciousFocus on a financial argumentation strategy based on factors related to imposing or reducing financial benefits or costsFocus on a societal argumentation strategy based on (nonfinancial) factors related to inhibiting or promoting shared societal goals or valuesAcquiring and Retaining Voter SupportIndustry argument suspicion is only a key factor for voters who switch their support to the activist sideFocus on acquiring supporters by targeting the segment of voters whose initial support for the activist side is tentativeEmphasize retention of early supporters by reenforcing public perceptions that activist group legitimacy is higherWhen both sides use financial argumentation, the industry side's competitive advantage over the activist side is limitedBuild voter support with a narrow argumentation strategy focused on specific financial implications to votersRetain voter support with a broader argumentation strategy focused on societal factors and counter industry claims with financial arguments While intuitive that both sides benefit from strong arguments, it is most critical for the activist side to be certain that voters will view its arguments as strong, as this is the effect with the highest impact on voting outcomes. This implies that it should be more effective for the industry to focus on diminishing the strength and increasing suspicion of activist arguments. As suspicion of industry arguments has limited impact, the industry has some license to encourage skepticism of activist tactics. This approach may require finesse, however, so as not to trigger a backlash if the issue is salient and the industry is controversial ([57]).Preelection polls often indicate majority support for the activist side, suggesting that the public is initially drawn to a public interest perspective. When the activist side has this early advantage, it needs to retain initial supporters by ensuring that its arguments are strong and by utilizing counterarguments to protect its legitimacy. In contrast, the industry side must focus on acquiring voters and build support with arguments that highlight the hidden complexities and costs of the proposed policy change. As industries often succeed in acquiring supporters during a campaign, as voting nears, the activist side may need to shift its focus in order to cultivate a switching segment.Our results show that the fit between an argumentation strategy and the identity of the persuader is key, especially for the industry side, which is safer emphasizing financial arguments closely aligned to its business sector profile. The activist side has some degrees of freedom as it is viewed positively by the public with either a financial or societal argumentation strategy. While societal argumentation is likely to have the most favorable impact for the activist side, financial argumentation may be necessary to challenge overstated cost analyses used by industry.In summary, our results suggest the industry side does best to follow an aggressive approach of attacking activist-side tactics while using a narrow argumentation strategy. Conversely, the activist side does best by vigilantly preserving its legitimacy in reaction to any industry attacks, while using a broader argumentation strategy. Implications for Public PolicyGiven the consequential outcomes of the ballot measure venue we study and continued increases in funding ($1.24 billion in 2020; https://ballotpedia.org/2020%5fballot%5fmeasures), it is surprising that there is limited emphasis on the transparency of industry and organizational support, such as comprehensive information about donors and the extent of their financial support. Our findings indicate that response to campaign arguments may be driven by legitimacy perceptions of the opposing sides, suggesting that policy makers should facilitate increased information to voters. As the public is primarily informed and persuaded by advertising during political campaigns, including the ballot initiative process, advertising law changes might be the most effective approach to reform ([53]; [70]).The Federal Election Commission regulates campaign finance law and political advertising, but its authority does not extend to ballot measure advertising, which is largely a state issue ([26]). California, under the jurisdiction of the California Fair Political Practices Commission (www.fppc.ca.gov), has among the strictest regulation for ballot measure advertising, following the logic that it is less clear who is responsible for these ads compared with candidate election ads. Because California already requires that top donors be explicitly listed in television, electronic media, and print advertisement as well as in mass mailings and robocalls, it can serve as a model for other states' ballot measure commissions to develop more comprehensive disclosure and increased transparency requirements. Alternatively, the Federal Election Commission could take steps to facilitate more unified federal standards for ballot measure advertisements. Without federal guidelines, consumers will continue to experience a patchwork approach to information that is unequal across states, making it difficult to assess the impact of direct-to-public persuasion on voters' decisions and policy outcomes. Limitations and Directions for Future ResearchOur investigation addresses one type of direct-to-public industry persuasion but not others, such as the more indirect route of shaping legislator decisions by influencing public opinion (for an examination of this issue, see [11]) or expensive direct-to-public industry image campaigns. In our field study, both ballot measures took place in one U.S. state and, thus, lack geographic diversity to mitigate potential unobserved influences. While our experimental studies address well-established argumentation strategies, they represent a partial picture of argumentation used to justify (or sanction) industry practice to the public.There is an important need for more field studies, natural experiments, and other real-world persuasion research initiatives, as well as more research explaining asymmetric public response ([17]). We conducted a posttest to investigate our position that the public perceives that activist groups are more motivated than the industry side to serve the public interest (see Web Appendix W14). While our findings confirm this assertion, future experimental research should examine the extent to which this perception is a driver of voting outcomes. Future studies should explore whether our results indicating that societal argumentation strategies create substantially higher suspicion when used by the industry side could be attenuated for industries with a high proportion of companies with strong corporate social responsibility reputations. Although our findings across four consumer-relevant issues suggest that our hypotheses are likely to generalize across other high-salience issues, further research should investigate whether asymmetry may be muted in contests over lower salience issues.We believe our findings will largely generalize to other contested issue scenarios, including situations where ( 1) the industry is challenging rather than defending the status quo and ( 2) industries battle other industries. When an industry tries to establish or expand a market for growth, it may challenge the policy status quo. A recent example includes the petroleum and gas industry using political clout to expand fracking onto previously protected Native American land in New Mexico ([47]). In these cases, the asymmetric impact of activist arguments having greater impact than industry arguments may be even more pronounced because the industry's self-interested motivation for market expansion may be more obvious to the public. Further, financial argumentation may actually backfire by magnifying the industry-serving objectives unless public attitudes have shifted over time to align with industry initiatives—for example, to promote casino development and marijuana legalization across the United States ([32]; [36]). In situations where industries battle other industries, when there is a legitimacy gap and one industry's public image is more favorable than the other's (e.g., renewable energy industries fighting fossil fuels industries over energy subsidies and standards; see [25]), arguments by the higher legitimacy industry could be more influential in driving outcomes. However, if there is little or no legitimacy gap (e.g., two agricultural industries battling over source of origin labeling laws), asymmetrical effects would not manifest.Despite the fact that direct-to-public persuasion occurs across a variety of industries and scores of issues and has been prevalent for decades, further research is needed because the many dimensions that characterize this form of marketing may represent significant boundary conditions on the impact of argumentation on voting outcomes. Our research offers an important step forward in advancing knowledge of this underexamined area of marketing.  "
23,"How Physical Stores Enhance Customer Value: The Importance of Product Inspection Depth The authors investigate the role of the physical store in today's multichannel environment. They posit that one benefit of the store to the retailer is to enhance customer value by providing the physical engagement needed to purchase deep products—products that require ample inspection for customers to make an informed decision. Using a multimethod approach involving a hidden Markov model of transaction data and two experiments, the authors find that buying deep products in the physical store transitions customers to the high-value state more than other product/channel combinations. Findings confirm the hypotheses derived from experiential learning theory. A moderated serial mediation test supports the experiential learning theory–based mechanism for translating physical engagement into customer value: Customers purchase a deep product from the physical store. They reflect on this physical engagement experience, and because it is tangible, concrete, and multisensory, it enables them to develop strong learning about the retailer. This experiential knowledge precipitates repatronage and generalizes to future online purchases in the same category and in adjacent categories, thus contributing to higher customer value. This research suggests that multichannel retailers use a combination of right-channel and right-product strategies for customer development and provides implications for experiential retail designs.Keywords: customer relationship management; experiential learning; hidden Markov model; multichannel retail; offline store; product inspection depth; sensory marketingThe online channel has assumed a dominant role in many industries, the result of a 15% annual growth in e-commerce (U.S. [52]). In this environment, retailers seem ambivalent about the role of physical stores. Industry surveys show that many consumers still prefer them: ""Half of the shoppers surveyed stated that they preferred to shop with online retailers who also operated physical stores"" ([47], p. 7; see also [ 7]; [43]). However, many retailers such as Macy's, Walgreens, and Bath & Body Works are closing physical stores ([41]). At the same time, digital-native online retailers such as Amazon, Alibaba, Blue Nile, Warby Parker, Bonobos, Google, and Indochino are opening them ([57]). Given these mixed messages, we find it natural to ask, ""What in fact is the value of the physical store?""To answer this question, we first recognize that consumers buy products, not channels. They must decide which products to buy in which channels. [31] advocate for studying this joint product/channel decision. They maintain that the product and channel choice processes are ""intertwined"" (p. 320) and that ""there is significant academic and managerial motivation for the studying the interrelationships between brand and channel choice"" (p. 320). Although, strictly speaking, they discuss channel and brand choice, their message can also be interpreted as calling on academics to research ""the consumer's decision of where and what to buy"" (p. 329).Our research draws on this call to action and the seismic shifts in the retail landscape to propose and empirically examine a central hypothesis: physical stores can enhance customer profitability by providing the physical engagement that customers value when purchasing ""deep"" products. We argue that products differ in the amount of inspection customers need to make a purchase. Some products require relatively ""shallow"" inspection, where a picture and written description suffice (e.g., a mobile phone charger), whereas other products require ""deep"" inspection, such as touch and physical interaction (e.g., a shirt). Drawing from experiential learning theory (ELT), our thesis is that customers value physical engagement when buying deep products and that the store provides such physical engagement, creating a favorable learning experience that increases repatronage. The lesson is that the product/channel purchase combination—deep product purchases in-store—develops more profitable customers.Our research objective is therefore to investigate the following questions: ( 1) Does buying deep products in the physical store[ 5] enhance customer value more than other product/channel combinations? ( 2) If so, what are the implications for the customer's future channel choices?We adopt a multimethod approach to answer these questions. In Study 1, we analyze customer-level transaction data for 50,387 customers of a large multichannel retail chain that sells outdoor recreation gear, sporting goods, and clothing. We first classify products as ""deep"" and ""shallow"" on the basis of the novel concept of product inspection depth that builds on [23] concept of digital and nondigital attributes and [37] findings around the importance of haptic information to consumer experiences. We then use a hidden Markov model (HMM) to examine consumers' product/channel choice dynamics and uncover two latent states: ( 1) a low-value state characterized by lower purchase frequency and profitability and ( 2) a high-value state characterized by higher purchase frequency and higher profitability. We find that customers are more likely to transition to the high-value state and remain there to the extent they have purchased deep products in the physical store.To replicate these findings and understand the underlying process, we conduct two lab experiments. Study 2 verifies that the deep product/physical store combination produces the highest repatronage intentions. Moderated serial mediation analysis supports the following process proposed by ELT: concrete experience → reflection on physical engagement → hypothesized learning → repatronage. This finding suggests that purchasing deep products in-store provides consumers with concrete, tangible, multisensory experiences that enable them to reflect on and then generate hypothesized learning that encourage them to repatronize the retailer, thus enhancing customer value.Experiential learning posits that on gaining experience, consumers generalize their learning beyond the contexts they have experienced. Study 3 verifies that once customers have purchased a deep product in a physical store, they are more amenable to purchasing the same as well as adjacent deep products online from the retailer in the future. They thus generalize from the retailer's store to its website and to adjacent deep product categories.Our findings support the trend of online retailers establishing an offline presence to enhance customer value by providing customers with concrete, tangible, multisensory experiences. This corroborates that many consumers still prefer to buy in-store. A recent survey found that ""if given the opportunity, 71% of consumers said they would even prefer to shop at an Amazon store over Amazon.com"" ([49]). Another survey revealed that ""50% of shoe buyers, 64% of sports equipment buyers, 59% of furniture buyers and 68% of jewelry buyers still prefer the physical store"" ([43]). Thus, despite two decades of innovation aimed at making e-commerce more engaging, many consumers still perceive the benefits of buying deep products in physical stores.Practitioner quotes support consumers' needs for concrete, tangible, multisensory experiences, which facilitate physical engagement, for why online retailers seek physical store presence:[Alibaba's physical stores are] providing an option for consumers to physically inspect, touch and feel products before purchase. This appears to be the right strategy. The physical store should attract … online shoppers, who want a more human shopping experience (Trefis [51]).Many stores will be giving customers an experience and providing insight and information. Want new pants? Find your size and the style you like in the store. Looking for a new smartphone or tablet? Try them out at a store and have a clerk walk you through the different features ([ 9]).With certain products, seeing and feeling makes a difference … even the most elegant descriptions and images can't replace the feel of organic, high thread count cotton sheets ([57]).These quotes echo our proposition that the physical store provides customers with the engagement they value when purchasing deep products. The fact that online retailers are pursuing an offline presence today validates our findings and supports our physical engagement theme. In turn, our findings support this trend. Our findings also suggest that online retailers who cannot afford the investment required for a physical presence should mimic the physical engagement found in stores and make the digital experience more concrete, tangible, and multisensory.In summary, we offer three key findings. First, buying deep products in the physical store increases long-term customer value. Second, consumers who purchase deep products in-store are subsequently more likely to buy the same and adjacent deep product categories online. Third, an important underlying mechanism is experiential learning. We also find that direct mail marketing encourages customers to purchase deep products in-store and increases customer value, suggesting that retailers can onboard new customers and revive lapsed customers via the promotion of deep products and in-store purchasing.These findings make the following contributions. First, we empirically identify an important role of physical stores, which is to enhance customer value by providing the physical engagement needed to purchase deep products. Second, we provide evidence that physical stores fulfill this role through experiential learning. Third, we advance theory by introducing the notion of ""product inspection depth"" and highlighting the distinction between physical engagement and digital engagement. Finally, we expand multichannel research that has focused on channel choice ([50]; [53]; [58]) by demonstrating that management of the joint product/channel decision is crucial for better understanding customer behavior. Framework and PredictionsWe aim to identify which product/channel purchase combination enhances customer value more than other combinations. We posit that customers value physical engagement when buying deep products. The store provides this engagement, creating a favorable learning experience that increases patronage and customer profitability. Product Inspection Depth and Physical EngagementWe distinguish products in terms of inspection depth, defined as the degree to which customers examine the product to make an informed purchase decision. Inspection depth can be ordered along a continuum: ( 1) pictures and descriptions are adequate, ( 2) visual inspection of the product is needed, ( 3) touching the product is needed, and ( 4) interaction with the product is needed (e.g., trying on, testing). We refer to products that require less inspection as ""shallow products""; those that require more inspection we refer to as ""deep products.""The concept of product inspection depth follows from the literature that examines how extensively the customer must examine a product to make an informed purchase. [45] stress the importance of physical inspection in the fashion industry. [14] find the inability to inspect shoes, DVD players, flowers, and food is an impediment to patronizing online stores, but not so for books and toothpaste. [33] suggest that profits for the multichannel firm decrease when consumers find it important to inspect the product before purchase. Product inspection depth is rooted in [30] theory of search and experience goods. Search goods can be evaluated prior to purchase, whereas experience goods need to be consumed to be evaluated. Relatedly, [23], pp. 487–88) advance the concept of digital attributes, which can be ""communicated online"" versus nondigital attributes, which ""can only be evaluated through physical inspection.""Product inspection depth synthesizes these ideas yet differs in important ways. For example, it differs from experience/search in that a deep product does not have to be consumed to be evaluated—it just needs to be inspected. It extends digital attributes, such as appearance, into the physical domain (e.g., ""I will try on this clothing to see how it actually looks on me""). Furthermore, it is not confounded with price. For instance, a pair of shoes requires deeper inspection than a (higher-priced) computer, whose specifications can be read off a product description. Inspection depth is particularly relevant to multichannel shopping, where the ability to inspect differs by channel.To acquire product inspection depth, consumers need to physically examine, inspect, or even interact with the product. That is, the customer has to physically engage with the product. More generally, the literature defines engagement as ""a behavioral manifestation toward the brand, beyond purchase"" ([54], p. 253). This manifestation can include touching and examining the product, reading descriptions, watching a demonstration, reading reviews, and interacting with a sales representative.The literature further delineates two forms of engagement: digital and physical ([55]). Digital engagement entails nonphysical actions such as seeing the product on a printed image. Formally, we define ""physical engagement"" as when the customer goes beyond visual inspection to gain multisensory knowledge of the product (e.g., by touching and using it). [38] link physical touch to object valuation, and [39] link touch to persuasion, suggesting that physical engagement can increase customer satisfaction. The potential for physical engagement differs by channel. The physical versus digital distinction is important because physical stores offer both physical and nonphysical engagement, whereas the online channel only offers nonphysical (digital) engagement. Experiential Learning TheoryThe product inspection depth consumers acquire through physical engagement defines a shopping experience. What do consumers learn from this experience? This is the bailiwick of experiential learning theory (ELT). David Kolb developed ELT as a synthesis of work by Lewin, Piaget, and others ([15], [16]; [17]; [28]). It is relevant for our purposes because it translates experience into learning. In Kolb's words, ""Learning is the process whereby knowledge is created through the transformation of experience"" ([16], p. 38), and ""Knowledge results from the combination of grasping and transforming experience"" ([16], p. 41).ELT is a process whereby people learn in four recursive stages: experiencing, reflecting, thinking, and acting ([17], p. 194). People first experience something concrete or tangible. They then reflect on the experience. Reflection enables people to hypothesize what they have learned from the experience and the extent to which this learning generalizes beyond the recent experience. People then act (i.e., test this hypothesis when the opportunity arises). The action provides more experience, and the four-stage process repeats.[ 6]A review of the ELT literature reveals four themes we draw on in forming our hypotheses. First, experiential learning builds on concrete and tangible experiences ([16]). [16], p. 21) notes, ""The emphasis is on here-and-now concrete experience,"" and ""Immediate personal experience is the focal point for learning, giving life, texture, and subjective personal meaning to abstract concepts.""Second, experiential learning is a feedback process: Consumers develop hypothesized learning from experience. They use subsequent experiences to test how well these hypotheses generalize, modifying them as needed. ""Learning is described as a process whereby concepts are derived from and continuously modified by experience"" ([16], p. 26).Third, experiential learning draws on the five senses. Under the rubric ""sensory marketing,"" researchers have connected experience to the five senses—touch, taste, smell, hearing, and seeing ([18]). Thus, three findings from sensory marketing prove critical to our hypothesis development. First, touch experience influences attitudes. Research has found that touch leads to more confident conclusions ([37]), is more persuasive ([39]), generates affect ([38]), and influences quality judgments ([ 1]). Second, multisensory experiences are more effective in generating learning than are single sensory experiences ([19]). Finally, touch can be an end in itself or a gateway to enhancing other senses, particularly visualization ([35]). Consider the case of buying jewelry and wristwatches. Touching the jewelry or the watch to feel its texture and weight distribution is valuable in itself, but it also enhances visualization, as one can pick it up, view it in natural light from various angles, and try it on to see how it looks and feels. Touching and seeing naturally go together.Figure 1 integrates product inspection depth, engagement, experiential learning, and customer value. The process starts with a shopping experience, characterized by the type of product the consumer buys (deep vs. shallow) and the purchase channel. This initiates the experiential learning process. Iterating through ELT yields learning that consumers want to confirm and determine if it generalizes to other contexts. These tests take the form of future shopping behaviors, which serve as the experiences that initiate future ELT cycles.Graph: Figure 1. Experiential learning: translating engagement to future behavior. Predictions The impact of deep/offline purchase on customer valueDeep products require inspection, touch, and trial. The store provides this physical engagement, and the shopping experience therefore is concrete, tangible, and multisensory—prerequisites for effective experiential learning. These factors in synchrony encourage deeper reflection, stronger hypothesized learning and potential generalizations, and, ultimately, more action.Importantly, we assume that providing physical engagement for a deep product purchase enables the consumer to make a more informed decision. The experience, therefore, will be positive, encouraging favorable learning that propel the consumer toward repatronage and higher customer value. We therefore hypothesize, H1:  Purchasing deep products in the physical store is associated with higher future customer value more than any other product/channel purchase combination. The role of experiential learning on the impact of deep/offline purchase on customer valueELT contributes to the translation of deep products/in-store to future customer value. It suggests the following mechanism underlying H1: customers purchase a deep product from the physical store. They reflect on this physical engagement experience, and because it is tangible, concrete, and multisensory, it enables them to develop strong hypotheses of what they learned about the retailer. Because they are satisfied with their purchase, this learning is favorable, precipitating repatronage.In summary, the mechanism is as follows: customers purchase a deep product in-store → they reflect on the physical engagement experience → they hypothesize favorable learning → they repatronize the retailer. This parallels ELT's process of experience → reflection → hypothesized learning → action. We thus predict: H2:  Experiential learning contributes to the process by which purchasing deep products in the physical store is associated with higher future customer value.Two assumptions underlie H1 and H2. First, the customer purchases a satisfying product. It is possible that despite the concrete experience, the customer emerges dissatisfied. Experiential learning is still at play, but the consumer learns that this retailer is not suitable, and customer value declines. Second, the customer's favorable experience spills over to both the retailer and the brand. Thus, H1 and H2 assume the customer emerges from the deep/offline experience with favorable learning that transfers to the retailer.Importantly, H1 is comparative; it maintains that deep/offline purchasing enhances customer value more than any other product/channel combination for the following reasons. First, the online channel entails only one sense—visual. Thus, the online experience is less concrete, less tangible, and not multisensory, and the consumer learns less. Second, shallow/offline purchasing has the potential to provide a concrete, tangible, and multisensory experience because the store offers this opportunity. However, by definition, shallow products do not require this physical engagement. As a result, the customer does not reflect enough to generate strong hypothesized learning, and repatronage is not enhanced as much. The impact of a deep/offline purchase on future deep/online purchasesELT posits that customers will test the extent to which their learning generalize beyond their experience to date. We consider two types of generalization. First, customers may generalize to a new channel. Consider customers buying a shirt in the physical store, and assume that they hypothesize from this experience that the retailer is a good place to buy shirts. They can then see how well this learning generalizes to another channel by purchasing deep products online from the retailer's website. This lets them test for generalization while taking advantage of the convenience of the website. As elaborated in our discussion of H1 and H2, customers who purchase shallow products in the physical store do not learn enough to encourage them to explore whether to purchase deep products online. In other words, these consumers are less apt to experiment with the website if they need a shirt. We thus hypothesize: H3:  Purchasing deep products in the physical store is associated with buying deep products online in the future, compared with purchasing shallow products in the physical store.Second, learning inferred by deep/offline customers can generalize not only to new channels but also to new products. Product generalization can occur because the initial purchase experience allows customers to learn about the retailer's overall product quality and product fit—factors that are especially important for deep products. Obviously, it is easier to generalize to something that is most related to the current context, and we believe the strongest impact of buying a shirt offline will be buying a shirt online in the future. But the generalization could extend to adjacent deep products such as sweaters, coats, and other apparel, and possibly to different product categories altogether. We thus hypothesize: H4:  Purchasing a particular deep product in the physical store is associated with buying adjacent deep product categories online in the future. Overview of StudiesStudy 1 uses retail transactional data and an HMM to verify H1 and H3. We test whether the translation of product/channel into customer value is most favorable for the deep/store combination (H1) and whether this combination encourages future usage of the online channel (H3). We conduct two randomized experiments in Studies 2 and 3 to replicate Study 1's findings. In addition, Study 2 tests the hypothesized learning mechanism (H2), and Study 3 tests product generalization (H4). Study 1: An HMM of Customer Product/Channel Dynamics DataOur data are from a national retailer that sells outdoor recreation gear, sporting goods, and clothing in 140 retail stores and on its website. The data chronicle customer-level purchase occasions from January 2003 to July 2005. For each purchase occasion, we observe stockkeeping units (SKUs) purchased, price, purchase amount (dollars spent on the entire order), cost of goods sold, channel choice, timing, and returns. We know each customer's zip code and tenure with the retailer.Our sample contains 50,387 customers buying more than 30,000 SKUs on 585,577 purchase occasions, an average of 12 purchase occasions per customer. Table 1 shows that online purchases make up 11.1% of purchase occasions. Because we are interested in dynamics, we select only customers who had at least two purchase occasions. Of these, 8,391 are ""new customers,"" with 80,751 purchase occasions, acquired after the beginning of the data set. Deep products constitute 54.7% of purchases, with an average spend of $56.21 per purchase occasion. The firm uses direct mail to communicate new styles and special events. On average, customers receive 19.6 direct mail pieces annually. The retailer does not target on the basis of past purchases. Prices and cost of goods sold do not vary between online and offline. Only 5.6% of customers purchased the same SKU more than once, either on the same purchase occasion or over multiple purchase occasions. So there are few instances of rebuying the same product. Half of the customers shop in a single channel; the others shop both in-store and online. Consistent with previous research, multichannel shoppers are more profitable (see Web Appendix Table W1.1).GraphTable 1. Descriptive Statistics per Customer. MeanSDMinMdnMaxTotal number of purchase occasions11.628.6759128Tenure (weeks)498.2365.124.33507708Total purchase amount per purchase occasion$102.81$60.13$2.70$92$913Purchase amount of deep products per purchase occasion$56.21$42.64$0$48$757Purchase amount of shallow products per purchase occasion$46.6042.47$0$37$865Interpurchase time between purchase occasions (days)75.3736.00271186Number of direct mailings received within 7 days of purchase.46.26001Number of direct mailings received within 14 days of purchase1.07.43012Annual number of direct mailings received19.65.21121626Percentage of purchases in-store88.87%.19011Percentage of purchases online11.13%.19001 The retailer categorizes the 30,000 SKUs into ten ""specialties,"" such as camping, travel, cycling, snow sports, and clothing, followed by 373 ""classes"" or categories, such as jackets, pants, and shorts, and finally specific SKUs. For details, see Web Appendix Table W2.1.Three independent judges rated each of the 373 product categories on inspection depth as well as digital or nondigital (Web Appendix W3) on a scale of 1 to 7. Intercoder reliability for inspection depth is.92. This suggests that the inspection depth concept is robust across coders and covers dimensions such as touch and interaction. The correlation between inspection and digital/nondigital ratings is.77, thus offering discriminant validity from digital/nondigital.[ 7] Not surprisingly, clothing and footwear generally have high ratings, but there is much variation within a specialty (Web Appendix Figure W3.1). This suggests that specialties are not perfect indicators of inspection depth. For example, within footwear, hiking boots are rated 7, women's sandals are rated 4, and insoles are rated 2 (on a 7-point scale). Deep products are not necessarily more expensive than shallow products; the correlation between price and inspection depth is.12 and insignificant. We categorize products as deep or shallow using a median split. This enables us to model purchase amounts of each type on each purchase occasion.[ 8] Model-Free EvidenceOur key hypothesis is that buying deep products in the physical store increases future customer value. New customers provide model-free evidence for this. Table 2 shows four cohorts of new customers defined by when they are acquired. Profits for the one-year period after acquisition differ depending on the first product/channel choices; buying deep products offline as the first purchase yields the highest profit, consistent with H1.GraphTable 2. Model-Free Evidence: First Product/Channel Choice and One-Year Profit. Customer CohortFirst Purchase Deep Product OfflineFirst Purchase Deep Product OnlineFirst Purchase Shallow Product OfflineFirst Purchase Shallow Product OnlineCohort 1: Jan.–Apr., 2003$77.75$69.51$65.29$62.75Cohort 2: May–Aug., 2003$82.38$72.98$71.54$59.40Cohort 3: Sep. –Dec., 2003$77.30$72.78$69.19$62.71Cohort 4: Jan.–Apr., 2004$82.60$74.77$75.63$73.08 Figure 2 illustrates dynamics. It depicts product and channel choices for new customers' first purchase and the same set of new customers on their eighth purchase. We see that 5.76% of first purchases are deep products bought online. This increases to 8.45% by the eighth purchase. This finding is consistent with H3 and shows that new customers' buying patterns evolve and alleviates the concern that new customers' purchase patterns are set before the retailer acquires them.Graph: Figure 2. Model-free evidence of product/channel choice evolution.It is still possible that new customers self-select into the relationship with the retailer. We address this in Study 1 by ( 1) developing a model designed to flexibly detect dynamics and control for unobserved customer heterogeneity, ( 2) separately analyzing new and existing customers via robustness checks, and ( 3) conducting a propensity scoring analysis. Studies 2 and 3 alleviate the self-selection concern using random treatment assignment in experiments. Modeling FrameworkWe use a multivariate HMM to study customers' joint decisions for channel choice, purchase amount, and interpurchase time. HMMs are often employed to study the dynamics of customer–firm relationships (e.g., [21]; [24]; [26]; [29]; [32]; [46]; [61]; [62]). HMMs incorporate experiential learning in that they include dynamics and feedback and model changes in behavior arising from experience (reflected in ""latent states""). HMMs also identify the drivers of these dynamics by studying customer transitions between latent states. A simpler model such as regression would have difficulty capturing these dynamics and rich insights. Furthermore, HMMs can distinguish temporal dynamics from customer heterogeneity. We will model time-invariant customer heterogeneity as well as dynamics.We use the HMM to discern when the customer is in a high- versus low-value state and predict transitions between these latent states by descriptors such as customers' previous purchases of deep versus shallow products and their use of offline versus online channels. In this way, the HMM provides tests for H1 and H3.Specifically, we model a customer's purchase occasion by four interrelated decisions: ( 1) channel choice, ( 2) purchase amount (in dollars) of deep products, ( 3) purchase amount (in dollars) of shallow products, and ( 4) purchase timing (in terms of interpurchase time). These four dependent variables not only paint a rich and multifaceted picture of customer behaviors beyond overall purchase amount but also enable us to calculate customer value within a particular time frame. We incorporate covariates in customers' utility functions and thus predict customers' decisions at each purchase occasion. We include covariates in transition functions to predict how customers migrate between latent states.Because HMMs are popular in marketing, we detail the specific components of our HMM to Web Appendix W4. Next, we highlight the covariates in the specification. Covariates in the HMM Previous channel choiceH1 and H3 both predict that previous channel choices influence future customer value. We include the customer's cumulative number of channel choices prior to purchase occasion j, offline_choices(j − 1) and online_choices(j − 1),[ 9] for the store (offline) and website (online), respectively. This is consistent with [22], [48], and [59].[10] MarketingDirect mail is the only firm-initiated marketing activity in the data and did not promote specific channels. Discussion with management revealed that communications were not customer-targeted, and we test and confirm that there is no endogenous relationship between customers' past purchase behavior and the likelihood of receiving direct mail. Thus, it reflects a baseline advertising effect. The variable marketingj equals the number of mailings the customer received within 30 days prior to purchase occasion j. We use a squared term,  marketingj2  , to capture decreasing returns.[11] HolidayThe variable holidayj indicates whether purchase occasion j occurs within two days preceding the following gift-giving holidays: Mother's Day, Father's Day, Valentine's Day, Christmas Eve, and New Year's Day. Because last-minute online shopping risks that a gift will not arrive on time, people may be more likely to shop offline when it is very close to these holidays. Purchase amountWe measure customer spend (in dollars) on deep and shallow products on previous purchase occasion j − 1 (deep_amount[j − 1] and shallow_amount[j − 1]). Both H1 and H3 predict that the type of product purchased is crucial for determining future customer value. Customer tenureTenure (tenurej) denotes how long the customer has been purchasing from the retailer as of purchase occasion j. It represents the length of the relationship. Product returnsWe calculate cumulative returns prior to the current purchase occasion (in dollars) and distinguish between deep and shallow product returns (deep_returns[j − 1] and shallow_returns[j − 1]). [40] find that product returns enhance customer relationships. Interpurchase timeWe define interpurchase_timej as the length of time between the previous and current purchase occasion. Shorter interpurchase time indicates more frequent purchases and, thus, a more valuable customer. Longer times could indicate lapses in loyalty or changes in lifestyle, which could influence subsequent channel or product choice. Specifying Utility and Transition FunctionsHMMs model latent states and estimate ""transition functions"" that predict how the customer migrates in and out of these states over time. HMMs characterize each state by its own set of utility functions—in our case, one for each of the four customer decisions we model. Following HMM conventions, we include covariates expected to have an immediate impact in the utility functions and covariates expected to have a long-term impact in the transition functions.Accordingly, we include previous channel/product decisions—offline_choices(j − 1), online_ choices(j − 1), deep_amount(j − 1), and shallow_amount(j − 1)—in all four utility functions to reflect ELT's dictum that customers test what they learn from experience by taking action—all four behaviors are actions. We include marketingj and  marketingj2  in the utility functions because direct mail can have an immediate reminder impact. We expect channel choice to be driven by proximity to a holiday. Thus, holidayj enters in the utility function for channel choice.We also include previous channel/product decisions (offline_choices[j − 1], online_choices[j − 1], deep_amount[j – 1], shallow_amount[j − 1]) in the transition functions to test our hypotheses that these influence future customer value. Direct mail is advertising that can have long-term effects, so we include marketingj and  marketingj2  in the transition equations. Drawing on [40], we include returns (deep_returns(j − 1] and shallow_returns[j − 1]) in the transition functions. As noted previously, long customer tenure and short interpurchase times could proxy for a strong long-term relationship, so we include tenurei and interpurchase_timej in the transition functions.Note that we use lagged variables in the utility and transition functions to capture dynamics (how previous decisions drive current decisions and state transitions). We are particularly interested in how previous channel and product choices determine future customer value.We do not include current prices as a covariate. From a theoretical standpoint, including current prices in the purchase timing and channel choice models would make the difficult-to-support assumption that customers make these decisions based on prices they do not observe until after they make those decisions. As a robustness check, we included monthly fixed effects and a monthly price index of top 30 best-selling items as covariates in the utility equations. Results, most importantly those pertaining to H1 and H3, were substantively the same.[12] Heterogeneity and EstimationCapturing customer heterogeneity is crucial for distinguishing temporal dynamics from time-invariant customer heterogeneity ([13]). We do this by adding latent class segmentation to the HMM. This allows the coefficients for the transition functions and the four utility functions to vary across segments.We use Markov chain Monte Carlo (MCMC) methods for estimation and use the adaptive Metropolis procedure ([ 3]) to improve mixing and convergence. We use the first 24 months of data for training and the last seven months for testing. We obtain our estimates from the last 50,000 draws from an overall MCMC run of 200,000 iterations. We assessed convergence by monitoring the time-series of the MCMC draws. Interpreting the Results Choosing the number of states and latent classes (segments)To determine the number of HMM states and the number of segments, we consider the in-sample log-marginal density, deviance information criterion, and predictive log-likelihood on the validation sample. Drawing on these criteria, we find that a two-state, two-segment HMM exhibits the best performance. It captures dynamics and heterogeneity while keeping model complexity in check. Therefore, we adopt this model. Web Appendix Table W4.1 shows these criteria for various permutations of the HMM. The table shows that the two-state, two-segment HMM is better than models that do not include dynamics, do not include heterogeneity, or include heterogeneity but as a continuum rather than discrete segments.[13] Interpreting the statesWe used methods described in [27] to infer each customer's state membership at each purchase occasion. We assign customers to the state with the highest probability to which they belong (in our two-state case, greater than.5). Table 3 shows average customer characteristics for each state. The interpretation is clear: customers in state 1 have longer interpurchase times (i.e., buy less frequently) and generate less revenue and profit. We label state 1 ""low-value"" and state 2 ""high-value.""GraphTable 3. Description of the Two HMM States. State 1Low ValueState 2High ValuePercentage who are new customers62%38%Mean spend per purchase occasion$91$127Mean interpurchase time (days)5946Percentage of deep products purchased in-store88%71%Percentage of purchases online11.92%14.27%Revenue per customer$901$1,078Profit per customer$138$169  Interpreting the segmentsWe assign customers to the latent segment to which they have the highest probability of belonging. Table 4, Panel A, shows that segment 1 is more profitable than segment 2, has a more balanced mix between in-store and online buying, transitions more quickly to the high-value state, and lives closer to the retail store. The channel mix and higher profitability suggest that segment 1 is the ""multichannel segment."" Table 4, Panel B, shows that customers in this segment buy more deep products, especially when they move to the high-value state. Table 4, Panel C, shows that customers in this segment are more likely to migrate to the high-value state and remain once they get there. Segment 2 focuses on offline—92% of these customers' purchase occasions are in-store (95% when they are in the low-value state and 76% when they are in the high-value state). Table 4, Panel C, further shows that segment 2 customers are less likely to transition to the high-value state. Accordingly, we label segment 1 ""multichannel"" and segment 2 ""offline.""GraphTable 4. Segment Descriptions and Migration Probabilities. A: Segment DescriptionsMultichannelOfflinePercentage of customers23%77%Number of years customer of the retailer1.57.0Distance from closest stores (miles)23.5743.52Percentage of purchases in-store68%92%Percentage of $ spent in-store43%49%Number of purchase occasions until migrate to high-value state4.87.4Revenue per customer$1,281$736Profit per customer$209$101B: Segment Descriptions by StateMultichannelOfflineLow ValueHigh ValueLow ValueHigh Value$ spent per purchase occasion$103$139$81$119Deep $ spent per purchase occasion$57$75$40$59Shallow $ spent per purchase occasion$46$64$41$60Mean interpurchase time (days)51446251Likelihood of purchase online (conditional on buying a deep product)15%43%11%26%Revenue per customer$975$1,245$710$993Likelihood of online purchase occasion27%87%5%24%Profit per customer$155$228$84$144Mean product depth when buying online (1–7 scale)2.44.32.13.9Mean product depth across all purchase occasions (1–7 scale)3.94.63.84.2C: Probability of Migrating from State to State, from Purchase Occasion j to j +1, by SegmentMultichannel SegmentOffline SegmentLow Value(j + 1)High Value (j + 1)Low Value(j + 1)High Value (j + 1)Low value (j)46.23%53.77%72.05%27.95%High value (j)13.22%86.78%54.06%45.94%  Interpreting the transition functionsTable 5 shows the parameter estimates for the transition functions. A positive coefficient for the low-value state means that customers in that state are more likely to move from low value to high value as the covariate increases; a positive coefficient for customers in the high-value state means they are more likely to stay high value. In both segments, previous choices of the offline channels drive customers to high value or keep them there. In contrast, online choices decrease the likelihood that low-value customers move to the high-value state, as well as the chance they remain high value. Marketing drives customers to the high-value state and keeps them there. Purchasing deep products drives customers to the high-value state and keeps them there, whereas purchasing shallow products drives customers to the low-value state and keeps them there. Longer tenure transitions customers to high value, whereas longer interpurchase times move customers to low value. Importantly, the transition equations reveal plenty of dynamics that vary by segment.GraphTable 5. HMM Transition Functions Parameter Estimates. Multichannel SegmentOffline SegmentLow-Value StateHigh-Value StateLow-Value StateHigh-Value StateParameterMeanSDMeanSDMeanSDMeanSDIntercept.4034(.008).8160(.009)−.8244(.007).1486(.014)Offline_choices(j − 1).0690(.016).2923(.008).5884(.011).9726(.007)Online_choices(j − 1)−1.9515(.004)−.0708(.013)−.2920(.004)−.0723(.019)Marketingj.4038(.004).0689(.005).2608(.007).2139(.004)Marketingj2−.0635(.009)−.0111(.009)−.0397(.015)−.0294(.003)Deep_amount(j − 1).2374(.004).3964(.005).4023(.010).2721(.005)Shallow_amount(j − 1)−.5612(.006)−.2416(.004).0286(.012).0888(.007)Tenure(j − 1).1607(.005).1234(.008).4690(.009).8466(.007)Interpurchase timej−.4391(.007)−.2185(.006)−.4717(.007)−.3881(.005)Deep_return(j − 1)1.1861(.007)−.1345(.011)−.3357(.004).1167(.014)Shallow_return(j − 1).8613(.004).3316(.006)−.3077(.009)−.4951(.006)Initial State and Segment ProbabilitiesLow-value state.588.412.267.733Segment.222.778 1 Notes: Positive coefficient means variable increases transition among customers in the low-value state and increases the probability of staying in high-value if the customer is already there. Interpreting the customer decision modelsTable 6 contains parameter estimates for the four decision models (i.e., the utility functions) for both segments and both states. We also calculated marginal effects (Web Appendix W6), which are consistent with the estimates in Table 6.GraphTable 6. Parameter Estimates for the Four Decision Utility Functions. A: Decision to Choose Offline Rather than Online ChannelaMultichannel SegmentOffline SegmentLow-Value StateHigh-Value StateLow-Value StateHigh-Value StateParameterMeanSDMeanSDMeanSDMeanSDIntercept1.1857(.010).3884(.007)2.2406(.027)1.3280(.027)Offline_choices(j − 1).8685(.003)−.3282(.009).1574(.006)1.1498(.009)Online_choices(j − 1).7893(.010).1641(.007).9579(.006).0561(.011)Marketingj.8612(.015)−.5749(.009).7030(.007).1705(.010)Marketingj2.0244(.012).0552(.007)−.1246(.015)−.0194(.011)Holidayj.5220(.004).1769(.006).5449(.011).6596(.004)Deep_amount(j − 1)−.8965(.005)−.7168(.008)−1.0957(.021).0592(.006)Shallow_amount(j − 1).7063(.009)−.5750(.008).4369(.005).1228(.012)B: Deep Product Purchase AmountMultichannel SegmentOffline SegmentLow-Value StateHigh-Value StateLow-Value StateHigh-Value StateParameterMeanSDMeanSDMeanSDMeanSDIntercept−.3086(.010).5059(.003).0756(.004)1.7441(.026)Offline_choices(j − 1).7237(.012).0949(.005).2403(.008).1535(.008)Online_choices(j − 1)−.1664(.011)−.0918(.011)−.3241(.006)−.0253(.008)Marketingj.8412(.006).8029(.006).3594(.004).2003(.012)Marketingj2−.1373(.005)−.1295(.011)−.0921(.015)−.0261(.007)Deep_amount(j − 1).7765(.021).2501(.008).7159(.008).2423(.004)Shallow_amount(j − 1).0179(.009)−.0104(.006).1602(.007)−.0056(.006)C: Shallow Product Purchase AmountMultichannel SegmentOffline SegmentLow-Value StateHigh-Value StateLow-Value StateHigh-Value StateParameterMeanSDMeanSDMeanSDMeanSDIntercept−.7080(.012).9356(.007).5469(.010)1.7792(.010)Offline_choices(j − 1).5049(.007).3081(.017)1.5740(.007).1758(.008)Online_choices(j − 1).1751(.008).0222(.020).2712(.010).1758(.008)Marketingj.1060(.008).0424(.016).6788(.008).0100(.007)Marketingj2−.0531(.009)−.515(.009)−.0801(.015)−.0092(.003)Deep_amount(j − 1)1.6638(.007).2450(.003)−.0315(.016).0034(.003)Shallow_amount(j − 1).4455(.010).0668(.011).5239(.011).2516(.006)D: Interpurchase Time Between Purchase OccasionsbMultichannel SegmentOffline SegmentLow-Value StateHigh-Value StateLow-Value StateHigh-Value StateParameterMeanSDMeanSDMeanSDMeanSDIntercept−.0228(.008).2083(.003)−.1176(.007).1181(.010)Offline_choices(j − 1).1623(.011).0786(.005).2700(.005)1.1314(.054)Online_choices(j − 1)−.1899(.011).4226(.004)−.1683(03015).2242(.016)Marketingj.1003(.012).2382(.007).2826(.011).0271(.009)Marketingj2−.0041(.009)−.0388(.010)−.0554(.013)−.0096(.005)Deep_amount(j − 1).2784(.006).0515(.010).0242(.006).0147(.003)Shallow_amount(j − 1)−.7165(.014)−.4262(.006).1064(.006).0243(.003) 2 aPositive coefficient means variable increases likelihood of choosing offline channel.3 bPositive coefficient means variable decreases interpurchase time.Table 6, Panel A, displays the utility functions for the channel choice decision. The offline and online previous choice coefficients are mostly positive, suggesting that previous purchase in either channel encourages customers to buy in-store for their next purchase. Interestingly, higher previous deep product purchases mostly encourage customers to buy online, whereas shallow product purchases encourage them to buy offline. Marketing primarily stimulates offline purchases, with the exception of multichannel/high-value purchases. Holiday shopping more likely takes place in the physical store, as expected. Overall, we find strong effects of previous channel choice, previous spending by product type, and marketing.Table 6, Panels B and C, show the utility functions for deep and shallow purchase amount. In general, previous offline purchase increases both deep and shallow spend, whereas previous online purchase has the opposite impact. Marketing increases deep and shallow spend, with a stronger impact on deep products. Previous deep product spend begets higher spend for both deep and shallow products. Shallow has the same direction of impact, albeit relatively smaller. Table 6, Panel D, shows that, for all segments and states, previous offline purchase decreases interpurchase time, possibly because the customer is more satisfied and thus buys again sooner. Testing HypothesesOur model relies on temporal precedence, the association between previous and subsequent decisions, to support a causal interpretation of the results. As we show next, these estimated dynamics suggest that buying deep products in the physical store creates higher customer value and encourages customers to buy online in the future. However, as with any model of field data, we cannot claim the model unequivocally establishes causation. This is one reason we rely not only on Study 1 but also on controlled experiments that use randomization.H1 hypothesizes that purchasing deep products offline is more likely to transition the customer to the high-value state than any other product/channel combination. The transition function estimates in Table 5 support this. Coefficients for deep_amount(j − 1) (row 7) and offline_choices(j − 1) (row 3) are positive in all four transition functions. Coefficients for online_ choices(j − 1) (row 4) are negative, so online purchases make it less likely that the customer will transition to high value. The coefficients for shallow_amount(j − 1) (row 8) are negative—or, at best in the offline segment, positive but far smaller in magnitude than the coefficients for deep_amount(j − 1). Overall, we find that buying deep products offline is most positively associated with transitioning to the high-value state (i.e., to higher customer value). Study 1 thus supports H1.H3 proposes that customers who buy deep products offline are more likely to purchase deep products online in the future. We have shown that deep/offline purchasing drives customers to the high-value state. Table 3 then shows that these high-value customers purchase a higher percentage of their deep products online (vs. in-store) than do the low-value customers (29% vs. 11%, row 4). Table 4, Panel B, further illustrates this at the segment level: high-value multichannel customers make 43% of their deep product purchases online, compared with 15% if they are low value. The same holds true for the offline segment: these customers purchase 26% of their deep products online if they are in the high-value state, compared with 11% if they are in the low-value state. Study 1 thus also supports H3. Robustness Checks New versus existing customersWe ran the model only on existing customers (41,996 customers, 504,826 purchase occasions). The substantive results were very similar to those for the full data set, indicating that the evolution of customer behavior based on channel/product experiences exists for both new and existing customers. New customers' first channel/product choiceWe tested whether the results in Table 2 are due to self-selection—that is, that new customers who start by purchasing deep products in-store already prefer the retailer. We conducted propensity score matching with new customers whose first purchase is deep products in-store as the treatment group and all other new customers as potential controls. We matched on ( 1) demographic variables extracted from each customer's zip code and ( 2) variables calculated using data from the first two months after the initial purchase. These included distance to store, city versus rural, average prices paid, the gender category of the products purchased, purchase of children's products, number of categories purchased per visit, and average interpurchase time. We then calculated subsequent profit, excluding those first two months.The average treatment effect (incremental value among new customers with deep/in-store as the first purchase) is +$132.74, suggesting that purchasing deep products in-store generates higher long-term customer value (Web Appendix W7). The Rosenbaum test ([44]) states that our treatment effect is significant even if the impact of an unobserved covariate were to increase the odds ratio of buying deep products offline by 50% (Γ = 1.5; see Web Appendix Tables W7.2 and W7.3). Product definitionWe ran the model with products classified as digital/nondigital instead of deep/shallow. The same substantive results hold for the digital/nondigital classification, with slightly worse model fit and prediction compared with the deep/shallow classification (deviance information criterion = 796,253 for digital/nondigital vs. 792,594 for deep/shallow, predictive likelihood = −198,237 for digital/nondigital vs. −194,899 for deep/shallow). This suggests that the deep/shallow product categorization yields a better-fitting model but similar findings to a ""digital/nondigital"" categorization. It also suggests that the sensory-rich inspection depth concept is particularly relevant in multichannel environments. Use of median split for categorizing productsOur analysis categorizes deep and shallow products using a median split. We examined how much our results would change if we used different split thresholds. We reran the model using thresholds ranging from 30% to 70% (i.e., from a 30th percentile rating used to classify a product as deep up to a 70th percentile threshold). Web Appendix W8 indicates that the 50/50 (and 60/40) splits provide the best fit and performance.[14] In addition, our substantive results hold up between 30% or 70% thresholds, suggesting that the results are robust within a reasonable range of rules for classifying products as deep versus shallow. Finally, the predictive likelihood for the 30% threshold is better than that of 70% threshold, suggesting that it is safer to classify shallow as deep than deep as shallow. Marketing Simulation for TargetingThe positive coefficient for marketingj in Table 5 (row 5) suggests that direct mail marketing moves customers to the higher-value state or keeps them there if they are already in a high-value state. A reasonable strategy is to target marketing to increase the probability that customers are in the high-value state. The question is, which customers should be targeted on the basis of their current state, segment membership, and previous product/channel choice?We conducted a simulation to investigate this question. Details are in Web Appendix W10. We used model parameters to simulate purchase behavior over a 30-month horizon. In the base case, marketing is set so that each customer receives two direct mail pieces per month. In the ""+1"" case, we increased this to three per month. One could use dynamic programming to optimize targeting, but our purpose is simply to demonstrate the potential of targeting.Profits in the base case were $127.36 per customer ($131.21 under the +1 strategy).[15] We found that the +1 strategy increased the probability of transitioning to or staying in the high-value state, ""Prob(Hi),"" for all customers except multichannel customers who currently are in a high-value state. They already have a high probability of staying high value (86.78%), so there is not much to gain by increasing marketing. A key result is that the gain in Prob(Hi) is largest for customers who just bought shallow products. For example, we found the gain for offline-segment low-value customers who just bought shallow offline is 29.20% − 24.67% = 4.53%, while the gain for those who just bought deep offline is 36.39% − 33.27% = 3.12%. This finding is consistent with H1: deep offline purchases naturally boost customers to a high-value state, so they have less need for marketing. Study 2: Replicating H 1 and Testing the Experiential Learning Mechanism (H 2)H1, supported by the HMM, proposes that one ""sweet spot"" for generating future customer value is for the customer to purchase deep products in the physical store. Study 2 employs a lab experiment to replicate H1 and test H2, the ELT mechanism we propose underlies it: deep product purchased in-store → physical engagement → favorable learning → repatronize the retailer. MethodOur sample is 411 Amazon Mechanical Turk subjects. The average online and store patronage experience, age, and gender are statistically equal across treatment groups (details in Web Appendix Table W11.1).We use a 2 (deep vs. shallow product) × 2 (physical store vs. online) between-subjects design with random assignment to treatment. We used ""sports shirt"" for the deep product and ""portable cell phone charger"" for the shallow product.The survey (Web Appendix W11) instructed subjects that there is ""a new sports and outdoor-gear retailer in town"" and that ""you have never shopped at this retailer."" We then asked, ""Now, imagine that you shop at this retailer for the first time. You visit its physical store (website) and buy a sports shirt (portable cell phone charger) that costs $40. Please take a few minutes to describe in detail the specific steps you would have taken to purchase the sports shirt (portable cell phone charger) in this new physical store (on this new website)."" We provided a text box for subjects to write a description of the steps they would have undertaken.We then asked subjects to state their intention to shop at this retailer again, using the [ 4] three-item repatronage scale (e.g., ""I would be willing to buy from this retailer again in the future"" [1 = ""Strongly disagree,"" and 7 = ""Strongly agree""]). We asked subjects to rate how much they thought they would have learned about the retailer's ""product offerings and quality, as a result of this experience,"" on a seven-point scale. This was followed by a question asking subjects to rate the product they bought on product inspection depth (1 = ""Picture and description would be adequate,"" 2 = ""Visual inspection of actual product needed,"" 3 = ""Touch of product needed,"" and 4 = ""Interaction of the product needed [e.g., trying on, testing the features]""). This last step enables us to verify the deep versus shallow product manipulation. Results and DiscussionWe begin with the product manipulation check. Results indicate that the sports shirt attained statistically higher means on inspection depth relative to the portable cell phone charger (M = 2.42 vs. M = 1.64; t(409) = 6.88, p < .001). Testing H 1Figure 3 shows mean repatronage intentions by treatment. The 2 × 2 interaction analysis using analysis of variance demonstrates that both deep product (F( 1, 407) = 6.94, p < .01) and store condition (F( 1, 407) = 6.18, p < .05) positively contribute to higher repatronage. Central to our prediction, the deep × store interaction also positively contributes to higher repatronage (F( 1, 407) = 4.82, p < .05). A planned contrast indicates the deep/store treatment clearly evoked the highest repatronage intentions among the three other conditions (Mdeep/store = 5.38 vs. Mdeep/online = 4.88, Mshallow/store = 4.86, and Mdeep/online = 4.83; F( 3, 407) = 6.09, p < .001), consistent with H1. Web Appendix Tables 11.2A–D provide additional details of the analysis of variance and contrast tests.Graph: Figure 3. The effect of first product/channel purchase combination on repatronage (Study 2). Testing H 2We measured the extent to which subjects reflected on the physical engagement component of the experience by analyzing how they articulated the experience in their written descriptions. We recruited two research assistants blind to the research agenda to rate a physical engagement variable, ""try_touch,"" from each respondent's description. We instructed the research assistants to read each description of the customers' shopping experiences. We told the research assistants that some shopping experiences involve elements of touching, trying on, and feeling and instructed them to code try_touch as 1 if the description contains words, synonyms, or themes related to ""try,"" ""touch,"" or ""feel""; alternatively, we instructed them to code try_touch as 0 in the absence of these themes. Intercoder correlation was.92.The following are examples of subjects' descriptions that clearly suggest physical engagement:I would look at the size of it. I would feel its texture. I would test it out. I would see how it would look on me. I would see if my favorite team is on the shirt.I would go into the store and do quite a bit of browsing first. I would allow myself extra time in this store as it is my first time going. I would familiarize myself with the brand and touch everything to test out its quality.I would go into the store and browse shirts. I might try on the shirt before buying it, unless I was sure it would fit and/or I didn't want to spend extra time. Then I'd buy it.We tested the proposed ELT mechanism using try_touch to measure physical engagement. We used Preacher and Hayes's PROCESS Model 83 ([12]) for moderated serial mediation, which combines serial mediation (Model 6) and moderated mediation (Model 7). Following our theory, we used store versus online as the ""X variable"" moderated by deep versus shallow product (the ""W variable"").[16] PROCESS generated estimates and standard errors via bootstrapped sampling with 5,000 iterations. The ELT-based mechanism we propose (deep/store purchase → physical engagement → learning → repatronize retailer) is the ""indirect effect"" reflecting the mediating role of physical engagement and learning on the relationship between a deep/store purchase and repatronage intentions.We first conducted several preliminary analyses to demonstrate the value of moderation and mediation. We first regressed store on repatronage, which yielded a significant direct effect (bstore = .273, p = .013). Then, we added the deep/shallow moderator to this model, yielding a significant interaction effect of deep × store (bdeep × store = .472, p = .029) but rendering the main effect of store insignificant (bstore = .031, p = .84; bdeep = .047, p = .76). Similarly, the direct effect of store becomes insignificant once try_touch and learning are added as regressors (bstore = −.143, p = .146; btry_touch = .425, p < .001; blearning = .442, p < .001).The results of Model 83 regarding moderated serial mediation show that the direct effect of store on repatronage is insignificant (−.143, 95% confidence interval [CI] = [−.336,.0501]). The index of moderated mediation, however, is.188 (95% CI = [.100,.293]), supporting the moderating effect of deep versus shallow product on the impact of the store versus online on repatronage. For the deep product, the conditional indirect effect for store on patronage is.219 (95% CI = [.122,.332]); for the shallow product, it is significantly weaker at.031 (95% CI = [.011,.059]). The results suggest that both the proposed moderation and mediation are at work and that store purchase increases patronage through physical engagement and learning, more so when purchasing a deep product. This confirms H2's prediction that ELT contributes to the mechanism translating deep/store purchasing into repatronage.We conducted two robustness checks to reinforce this analysis. First, recall that the deep/store condition stands out and the other three essentially are equal. We therefore created a deep/store dummy variable equal to 1 if the subject was in the deep/store condition and 0 otherwise. Although regressing deep/store on repatronage yields a significant direct effect (b = .524, p < .001), the direct effect becomes insignificant after we account for the proposed mediation. Serial mediation analysis (PROCESS Model 6) yields a significant indirect effect: deep/store → try_touch → learning → repatronage. Details are in Web Appendix W11 (Table W11.3).Second, we reran Models 83 and 6, switching the order of try_touch and learning, estimating an alternative process: store → learning (moderated by deep product) → try_touch → repatronage for Model 83 and deep/store → learning → try_touch → repatronage for Model 6. While the global model fit and the total indirect effects are unsurprisingly the same across both orderings ([42]), the indirect effects of serial mediation are quite different. For instance, our proposed ordering yields.1882 (SE = .048) for the deep products' index of moderated mediation in Model 83; the alternative ordering yields an index of.0138 (SE = .009). Model 6 highlights this difference more saliently. Whereas the proposed ordering yields a serial mediation indirect effect of.1258 (SE = .03), or 30% of the total indirect effect, the alternative ordering yields the serial mediation indirect effect of.0154 (SE = .006), which translates to 3.7% of the total indirect effect. We acknowledge that [42], p. 698) cautions against trying to infer the correct mediation order by the aforementioned tests. He advocates that the ordering be based on ""strong evidence from logic, theory, and prior research that the hypothesized casual direction is more plausible than indicated alternatives"" ([42], p. 697). We believe the application of ELT we used to specify the try_touch → learning ordering satisfies this requirement.In summary, Study 2 replicates Study 1's finding that the deep/store purchase combination produces the highest repatronage, in support of H1. It also tests the ELT mechanism as proposed by H2. The moderated serial mediation results, and the robustness checks, are consistent with the ELT mechanism. Study 3: Replicating H 3 and Testing Product Generalization (H 4)Study 3 tests our predictions related to generalization of learning. We aim to replicate the HMM's support for H3, which predicts that purchasing deep products in-store increases the likelihood of purchasing deep products online, compared with purchasing shallow products in-store. Further, this study tests H4, which proposes that the impact of purchasing a specific deep product in-store generalizes to related, adjacent deep products. As noted previously, generalization is an important component of experiential learning and could be very powerful for retailers. It demonstrates the temporal interplay between offline and online channels and suggests that deep/store purchase of a particular product ""spills over"" to higher likelihood of purchasing adjacent deep products online when the customer repatronizes the retailer. MethodStudy 3 is a two-treatment between-subjects design. The two treatments were deep product/in-store and shallow product/in-store. We randomly assigned 414 participants from Qualtrics Consumer Panel to the two treatments. To ensure that participants were relevant for our context, we asked Qualtrics to screen them based on age (between 20 and 60 years old), household income (minimum of $30,000; 50% of sample needs to have at least $60,000 household income), and e-commerce experience (need to have purchased a product online at least once in the past six months).We first told subjects that a new sports and outdoor-gear retailer has opened in town. We then asked them to imagine their first shopping trip to this retailer's physical store, where they purchased either a sport shirt (deep product) or a battery charger (shallow product). We then asked them how likely they would be to purchase each of four products from the retailer's website in the future. These four products included two deep products, a sport shirt and a sweater (a product adjacent to the shirt), and two shallow products, a battery charger and an activity tracker watch (neither of which are adjacent to a shirt). Finally, as in Study 2, near the end of the study, after the key measures were collected, we asked the respondents to rate the shirt or the battery charger for product inspection depth, depending on their assigned conditions, to enable us to check the deep versus shallow product manipulation. Details of the questionnaire are in Web Appendix W12. Results and DiscussionManipulation checks confirmed that the shirt was perceived to be deeper on the four-point product inspection depth scale than the charger (shirt = 2.26 (SE = .09), charger = 1.65 (SE = .07); t(412) = 5.51, p < .01).Figure 4 shows that first purchasing a shirt offline increases intentions to subsequently purchase a shirt online, compared with if the first purchase is a charger offline (Moffline_shirt = 4.35, SE = .14 vs. Moffline_charger = 3.62, SE = .13; t(412) = 3.7, p < .01), confirming H3.Graph: Figure 4. The effect of first deep product in-store purchase on future online purchase likelihoods, compared with shallow product purchase (Study 3).We next tested for spillover. H4 posits that generalization will be to adjacent products. Indeed, the results show that the shirt treatment leads to a higher online purchase likelihood for the adjacent sweater than does the charger treatment (Moffline_shirt = 3.88, SE = .14 vs. Moffline_charger = 3.45, SE = .13; t(412) = 2.3, p < .05). This supports H4. While spillover extends from shirt to sweater, it does not extend to the charger and watch—two shallow, nonadjacent products. The likelihood of buying a charger online next time is nonsignificant between the two treatments (Moffline_shirt = 4.48, SE = .15 vs. Moffline_charger = 4.88, SE = .14; t(412) = −1.9, p > .05). Similarly, the difference between purchasing an activity tracker online subsequently is nonsignificant (Moffline_shirt = 4.4, SE = .14 vs. Moffline_charger = 4.5, SE = .14; t(412) = −.5, p > .05). In conclusion, we have support for H4: buying deep products in-store generalizes to adjacent products. Additional regression analyses corroborate these findings and are in Web Appendix W12.In summary, Study 3 demonstrates that buying deep products in-store encourages customers subsequently to purchase deep products online more than does buying a shallow product in-store, in support of H3. Furthermore, buying a shirt in-store increases the likelihood of not only buying a shirt online in the future but also buying an adjacent product (a sweater) online. This supports H4. Study 3 overall testifies to the value of ELT as a theory for understanding the impact of deep/store purchases on future customer value. General DiscussionWe set out to study the role of the physical store in today's multichannel retailing environment. Our thesis was that the physical store increases customer value by providing physical engagement when customers buy deep products. We drew on ELT to formulate four hypotheses: H1 suggests that deep product/in-store purchases increase customer value more than any other product/channel combination. H2 suggests that ELT provides a mechanism that contributes to this effect. H3 posits that customers who purchase deep products in-store are more likely to purchase online from the retailer in the future. H4 proposes that customers will repurchase not only the original deep product but also related, adjacent deep products online. H1 and H2 follow because the store delivers a tangible, concrete, multisensory experience, which facilitates the physical engagement beneficial for buying deep products. This precipitates effective experiential learning. H3 and H4 follow from the generalization phenomenon proposed by ELT.We evaluated these four hypotheses using an HMM applied to field data (Study 1) and two lab tests (Studies 2 and 3). The results support the following conclusions: ( 1) purchasing deep products in-store increases future customer value more than any other product/channel combination (H1), ( 2) the ELT mechanism (deep product/in-store purchases → physical engagement → favorable learning → repatronize the retailer) contributes to this increase (H2), and ( 3) purchasing deep/in-store increases the likelihood of purchasing the focal and adjacent deep products online in the future (H3 and H4). Research ImplicationsOur work has several implications for future research. First, researchers should consider the product/channel combination in studying consumer decisions in a multichannel context. We focus on the physical store and deep products, but the bigger picture is for future research to study product and channel choices together. Not doing so may incur a lost opportunity—the insights in our article would have been diminished if we had focused solely on channel or product.Second, the product inspection depth concept extends theory regarding product categorizations such as digital versus nondigital by incorporating details related to both physical and visual inspection. We believe this is a valuable concept for studying and comparing products because inspection depth varies appreciably across products, and channels differ in the degree of inspection they can provide. We hope researchers will apply and perfect this concept.Third, we demonstrate the applicability of ELT to developing customer relationships. This importantly extends the domain to which ELT has been applied. The pivotal role of the customer experience ([60]) cannot be understated.Fourth, our reliance on ELT proved fruitful, but [25] posit that decision making can be a dual process, drawing on cognitions and feelings. ELT prescribes an involved process of reflection, hypotheses, and repetition, by which consumers form cognitions. ELT does not directly tap feelings. Under this umbrella are concepts such as trust, commitment, affect, and emotions advanced by [34] and [ 5]. These concepts provide additional theories for our results and thus need testing. Managerial ImplicationsStudy 1's data are from 2005, yet our findings are in evidence today. The last decade has witnessed a marked increase in the opening of physical stores by online retailers, despite myriad changes in the retailing environment. This attests that our findings are not ephemeral. The general lesson of our research is for retailers to create a concrete, tangible, and multisensory experience for customers buying deep products. This sets the stage for favorable experiential learning and increased customer value. Retailers can do this in numerous ways:First, when retailers find that a customer is buying online but is decreasing in value, we suggest a promotion for deep products in-store. Our marketing simulations show that there is potential to increase customer value through direct marketing. Second, retailers should facilitate physical engagement for deep products through merchandising and training sales personnel to walk customers through the engagement (e.g., by helping customers try and use deep products in-store). Third, retailers cannot and should not infer product inspection depth solely from predefined product categories, because there is much variation in inspection depth with a particular category. Rather, management should infer inspection depth using our proposed measures or expert, independent judges. Fourth, we recommend retailers use a deep/offline onboarding strategy for new customers. They should use acquisition channels and product promotion strategies that encourage the first purchase to be deep products in-store.Our general lesson applies to recent developments in retailing. For example, showrooming ([10]) starts with customers in-store, where the retailer can provide physical engagement. However, the retailer may lose customers who use their smartphones in-store to find the product elsewhere. There are two possible solutions. Retailers can train sales reps to attend to customers buying deep products and equip reps with a mobile device to place orders, or retailers can provide customers with an app to ""lock them in."" Interestingly, [55] find that store-oriented customers are good targets for retailer apps. Similarly, ""buy online, pickup in-store,"" a form of ""web-rooming,"" can get customers to the store where they can physically engage with additional products to the ones they ordered online.Our central thesis also has implications for retail loyalty program design and its real-time management. Loyalty programs may be more effective if they provide incentives for customers to shop in-store, such as extra reward points for in-store shopping, particularly when it comes to deep products. These programs need to provide the data, system, and incentives needed to route customers to the physical store when needed, such as when customer value is waning. LimitationsOur work has limitations that suggest opportunities for future investigation. First, our hypotheses assume that physical stores provide effective physical engagement and favorable experiential learning. Our results could have turned out differently if our focal retailer's stores did not provide satisfactory experiences. This meant that our hypotheses were nontrivial and falsifiable, and future work should investigate the store features that best provide physical engagement. Second, we could not observe and thus could not incorporate customers' category expertise or preference evolution due to product consumption. Two customers who bought similar tents could have different camping experiences, which would partially determine future purchases that are outside of the firm's control. Third, with transactional data typical in customer relationship management research, we assumed that customers make channel and product decisions jointly. Future work using more granular data could examine the sequence of channel and product choices and explore situations such as planned versus serendipitous purchases. Fourth, we do not know exactly when a customer makes the decision of what and where to purchase. A shopping list study would be useful for future research. Finally, our observational data were from one retailer—a specialist in outdoor products. Future work should consider other product categories. Future DirectionsGuided by our mantra to create physical engagement to enhance customer value, we next discuss additional fertile areas for research. Store or showroom?Is the traditional physical store, with its requisite square footage and inventory, the best way to sell deep products? As fulfillment logistics have gotten faster, more orders can be placed via in-store kiosks and staff-assisted online ordering and can be fulfilled quickly. This suggests stores do not need to allocate a large space for inventory and can use that real estate to help convert the physical store to a showroom. The question is which is best—physical store or showroom?—in terms of customer preferences and the financials. Full or limited assortment?As the physical store's value lies in facilitating physical engagement through deep products, stores may not need to carry a large assortment (not all sizes and not all colors). The counterargument is that stores should carry a broad assortment because customers might want to physically engage with specific sizes/colors. Future research should study and resolve this tension. Full or limited staff?A trained and empathetic sales team would play a key role in delivering physical engagement to the customer. We noted this in discussing showrooming, where the sales rep was needed to keep the customer ""on course"" with the retailer. [20] also delineate an important role for staff in developing customer value. This would suggest full staffing of physical stores (see [10]). However, this strategy is expensive, and customers may be quite capable of physically engaging on their own. The role of private labelAs noted in formulating H1, we assume the favorable learning the customer gains from physical engagement transfer to the store. One way the retailer might ensure this is to emphasize its private label. Nordstrom, L.L.Bean, and Warby Parker are good examples. Leveraging technology to create physical engagementPhysical engagement is a particular challenge for online retailers. What combination of videos, chat, user testimonials, virtual reality, augmented reality, and other interactive features should be deployed to mimic in-store physical engagement?Although the current state of augmented reality technology is probably not realistic enough to fully capture physical engagement ([ 6]), future work could examine how this technology can satisfactorily do so for certain product categories, consumption contexts, and consumer segments.One recent notable example is the virtual tasting experiences offered by Wine.com, an online wine retailer. This program encourages customers to order a featured wine ahead of time, then go online and, either live or via recorded videos, taste the featured wine alongside its winemaker or renowned critics such as Steven Spurrier. This program not only can result in immediate sales of the featured wines and fills the vacuum created by the closure of physical tasting rooms during COVID-19 but, according to our theory, also has the potential to facilitate sensory engagement (in this case, visual, audio, taste, and smell). Research could investigate whether such creative digital efforts can translate into long-term loyalty towards the online retailer. Identifying physical engagement-prone customersRetailers might consider updating their customer segmentation schemes to reflect customers' needs for physical engagement. [36] show that the need for haptic (touch) experiences is an individual trait. An interesting line of future research is to investigate whether this trait evolves over time and what drives this evolution. Retailers could thus explore ways to identify physical engagement-prone customers and design their stores and websites accordingly. Other forms of physical engagementRetailers may want to explore other avenues to physically engage customers in-store, such as through cultural events or prosocial efforts. For example, they may celebrate ethnic holidays and dedicate a certain percentage of sales to charitable causes. Can these be turned into forms of physical engagement? A retailer can advertise online its efforts to fight COVID-19, but it can also demonstrate in-store the personal protective equipment its donations buy for the community. Future work could investigate whether these various in-store efforts generate customer interactions and affect that enhance customer value.These speculations, along with the more direct implications stated previously, demonstrate the richness of marching to a simple yet powerful message: Retailers can increase customer value by providing physical engagement when selling deep products. We hope both researchers and practitioners will leverage this message in future work.  "
24,"Identifying Market Structure: A Deep Network Representation Learning of Social Engagement With rapid technological developments, product-market boundaries have become more dynamic. Consequently, competition for products and services is emerging outside the product-market boundaries traditionally defined based on Standard Industrial Classification and North American Industry Classification System codes. Identifying these fluid product-market boundaries is critical for firms not only to compete effectively within a market but also to identify lurking threats and latent opportunities outside market boundaries. Newly available big data on social media engagement presents such an opportunity. The authors propose a deep network representation learning framework to capture latent relationships among thousands of brands and across many categories, using millions of social media users' brand engagement data. They build a brand–user network and then compress the network into a lower-dimensional space using a deep autoencoder technique. The authors evaluate this approach quantitatively and qualitatively and visually display the market structure using the learned representations of brands. They validate the learned brand relationships using multiple external data sources. They also illustrate how this method can capture the dynamic changes of product-market boundaries using two well-known events—the acquisition of Whole Foods by Amazon and the introduction of the Model 3 by Tesla—and how managers can use the insights that emerge from this analysis.Keywords: artificial intelligence; deep representation learning; social media; competitive market structure; big dataFirms compete in a market to satisfy the specific needs of consumers in the market. The market and the competing products make up a ""product-market,"" with the boundary defining the brands competing within that market. Market structure is defined on the basis of these product-markets and their (possibly overlapping) boundaries. Identifying the product-market boundary and examining the strength of competition between brands within the product-market have long been important issues with strategic implications for next-generation product design, product positioning, new customer acquisition, and pricing and promotion decisions. Rapid changes to the competitive environment, however, have made identifying the product-market boundaries increasingly challenging. With technological advances, the product-market boundaries themselves are changing and competitive threats and opportunities are emerging outside of narrowly defined product-market boundaries.Numerous recent competitive events support the idea that product-market boundaries are highly fluid. For example, the digital camera product-market was upended by technological developments in smartphone categories. Similarly, Tesla, which initially entered the product-market of high-end automobiles with an innovative fuel technology, has since rolled out products for the lower-end market, thereby changing competition in the lower-end product-market as well. Amazon, previously an online platform, essentially crossed product-market boundaries when it acquired Whole Foods and entered the offline product-market. In many such situations, product-market boundaries based on traditional Standard Industrial Classification and North American Industry Classification System codes are inadequate indicators of emerging threats and opportunities. Given the potential for new and unforeseen relationships between brands, managers need deeper insights into the fluid product-market boundaries to be able to spot potential competitors and complements, identify cross-promotion strategies, and develop firm-level strategies.These observations naturally lead to several important questions: How can managers accurately identify potential threats and opportunities? If a competitive threat emerges from a different market, how can managers proactively anticipate such threats? How can we answer these questions and derive marketing insights using easy-to-obtain and publicly available data? Our article aims to answer these questions using large-scale (>100 million) social media user engagement data (likes and comments) spanning several thousands of brands in different product/service categories.Over the years, academics and practitioners have contributed significantly to developing various methods to define and identify market structure (see the review by [46]]). These include survey-based methods such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]), methodologies based on observational purchase data (e.g., brand switching) ([21]; [37]), consideration sets ([42]), and scanner-based purchase data ([10]; [36]; [45]). Within the online context, researchers have used unstructured user click streams ([32]), online search logs ([24]; [42]), and customer reviews ([26]). Many of these methods use data from the bottom of the purchase funnel, such as evaluation- and purchase-stage data, and thus assume that the product-market boundaries are prespecified. Even those methods that use data from the top of the funnel at the awareness or preevaluation stage, such as forum discussions ([35]) and hashtags ([33]), define a product-market boundary first and then examine the competition within the prespecified product-market to make these methods implementable. Thus, many of the methods are unable to capture changes to the product-market boundaries and/or the impact that a brand from outside the boundary may have on brands within a product-market.Our methodology creates a more inclusive representation of brands by examining brand–user relationships at the top of the purchase funnel. Unlike the extant methods for identifying market structure that use data from consumers' lower funnel activities such as purchase data, brand switching, price comparison data, or consideration data that prespecify boundaries (e.g., [10]; [21]; [37]; [42]; [50]), we use upper-funnel user–brand engagement data (such as liking and commenting on brand posts) from social media that spans product-markets. At the lower end of the purchase funnel, consumers winnow down the brands they consider to a few substitutes. Thus, interactions at this stage are not as informative of the broader (and possibly complementary) linkages between the brands across product-markets, which are captured more easily at the upper funnel. For example, a consumer considering travel may consider hotel or Airbnb options, airline options or travel intermediaries. At this early stage (the upper end of the funnel), understanding such user–brand linkages could be more informative of the broader relationships between the brands on a continuum from substitutes to complements. Our methodology uses such upper-funnel user–brand engagement data to identify these latent relationships among a large number of brands.Many extant studies in market structure, including those mentioned previously and those using big data technologies (e.g., [ 7]; [12]; [26]; [35]; [42]), view the competing/complementary brands as brand–brand networks. That is, they specify the relationship between any two brands using similarity metrics derived from brand switching, co-occurrences, and word embeddings, without directly modeling the entities (customers, individual consideration sets, or individual reviews) that give rise to such similarities. Our methodology based on brand–user networks considers both brands and users as primitives and uses as input the relationship in terms of each user's liking and commenting on brands. The essential difference between these approaches and our methodology is that extant research considers aggregate data of relationships between brands (brand–brand) as input, whereas our methodology considers the disaggregate individual-level relationships between users and brands (brand–user) as input.The distinction becomes more salient when a product-market boundary is not prespecified. Consider, for example, User 1, who likes United Airlines and Hyatt, while User 2 likes Southwest Airlines and Hyatt. When the product-market is prespecified as ""airline brands,"" information about the users liking the Hyatt brand is discarded. As a result, information that could provide insights into the relationship between United Airlines and Southwest Airlines through their relationships with Hyatt is not considered. However, when we do not prespecify the product-market boundaries, we are able to leverage all such information and create a more accurate representation of the brands.From this premise, we first construct a large-scale brand–user network based on user engagement on brands' social media public fan pages. Then, we propose a deep network representation learning method to discover relationships within the data. Specifically, we use a deep learning method suitable for ( 1) handling large data efficiently and ( 2) learning complex patterns from data effectively (see [ 2]; [49]). The process leads to a low-dimensional representation (i.e., a vector) for each brand and each user by training a deep autoencoder on the network data. The deep autoencoder is similar to traditional dimensionality reduction methods such as principal component analysis (PCA) in capturing latent factors in data with few dimensions. It is, however, very different from those methods in that it uses a nonlinear transformation function to learn the latent patterns in data while reducing the noise in the data. In our context, the deep autoencoder can preserve the first-order (user–brand direct connection) and the second-order (two users connecting to the same brand, or one user connecting to two different brands) network topologies. As a result, brands with network structural equivalence are located closer together in the representation space, while brands with dissimilar network structures are located further away from each other. This method also projects users and brands onto the same dimensional space, which can be used for many different follow-up analyses. We use an illustrative example (in Figure 1) to demonstrate how network representation learning works.Graph: Figure 1. An illustration of deep network representation learning.Suppose we have three brand nodes (B1, B2, and B3) and five user nodes (U1, U2, U3, U4, and U5) in a network. Our network representation learning approach aims to find a function that maps each node into a low-dimensional vector (e.g., three dimensions, for the sake of illustration) while the network structural information is preserved maximally. That is, when nodes exhibit similar structures (first order and/or second order), they are projected onto similar vectors and located closer in the reduced three-dimensional embedding space. Because U1 engaged with B1, we expect the vector representation of B1 and U1 to be close. Similarly, B2 is closer to B1 than to B3 because B2 shares more common users with B1 than with B3. Because B2 has connections to U4 and U5, this makes B2 lean toward them.We establish the face validity of our approach through the identification of product-market boundaries. Our analysis of the brand–user engagement data of over 5,000 brands and nearly 26 million users reveals product-market boundaries with high face validity—grouping of specific categories, high-end brands, and overlaps. We then conduct external validation checks using additional sources including survey and Google search trend data. The market structure derived using our approach is highly correlated with those derived using external data sources. Our approach also overcomes common limitations in extant methods such as data sparsity. Our event studies on Amazon's acquisition of Whole Foods and Tesla's introduction of the Model 3 illustrate how our methodology captures the changes in product-markets associated with these events. We also discuss how the market structure maps can reveal opportunities and threats facing a brand. For instance, our market structure identifies Disney Cruise Line and Hyatt—two brands outside the airline market—as proximal brands to Southwest Airlines. Such findings provide opportunities for Southwest, as it can target those who like Disney Cruise and Hyatt in social media, cross-promote its brand by teaming up with Disney Cruise and/or Hyatt on each other's websites, or launch coalition loyalty programs.Our article contributes to product-market research by leveraging the information embedded in big data of user–brand engagement networks to identify product-markets without having to prespecify boundaries. User–brand engagement network data at a high level in the purchase funnel (interest phase), together with deep learning techniques, provide us with insights at a greater scale and level of detail than extant methods. Our ability to map a large number of brands and precisely visualize brand relationships using learned vector representations enables managers to identify opportunities and threats that lie beyond product-market boundaries. Moreover, our method satisfies the three elements widely regarded as essential to successful real-world applications of artificial intelligence: data, algorithm, and computing power ([ 2]). In this article, we leverage deep learning and a network representation learning (algorithm) to understand market structure using large-scale social media data (data). This model implementation is efficient under NVIDIA P100 graphics processing unit, with Tensorflow as the backend framework (computing power). In summary, our study is an apt illustration of how artificial intelligence can be used to tackle a traditional marketing problem and provide richer insights for mangers in a rapidly changing competitive environment. Background and PositioningExtant work in identifying competitive market structures dates to the 1970s (e.g., [ 8]; [20]), when diary panel–based brand-switching purchase data and survey-based consumer judgments of substitution in use or similarities were used to construct market structure maps. These studies depended on customer data generated either at a late stage of the customer journey or at the very beginning of the journey. The increased availability of scanner-panel data of purchases, market structure models with marketing mix (e.g., [ 5]; Kannan and Wright 1991), and dynamic market structure models (e.g., [10]) provided more detailed insights into interbrand relationships and competition. Approaches such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]) relied on data collected using surveys and, therefore, were effort intensive. Given the scaling issues with the maximum likelihood–based models and the limitations of survey data, the product-market boundaries were prespecified generally at the industry level so that a smaller number of brands within an industry could be analyzed. The advent of online sources, such as review platforms, social media platforms, and clickstream data, dramatically increased the volume and variety of data for market structure studies, especially at the awareness, search, and consideration stages of the customer journey ([24]; [26]; [35]; [42]; see Tables 1 and 2). Even with a large volume of data, these studies predefine the product-market boundaries at the industry level to make the analyses viable.GraphTable 1. Comparison of Different Types of Work on Market Structure Discovery. Primary/Survey DataText MiningSocial Tag–BasedSearch DataShopping DataSocial EngagementData volumeSmallLargeLargeLargeVery largeVery largeData veracityAuthenticNoisyModerately noisyModerately noisyAuthenticModerately noisyPrivacy preservingYesYesYesNo (need to insert a tracking pixel)YesYesData availabilityLow (need to do survey)High (publicly available)High (publicly available)Low (need to insert a tracking pixel)Low (need to partner with retailers)High (publicly available)Data preprocessing costLow (use consideration set directly)High (text mining is error-prone)High (text mining is error-prone)Low (use consideration set directly)Low (use product co-occurrence)Low (use network raw data) GraphTable 2. Summary of Difference Among Extant Literature on Market Structure Discovery. Kim, Albuquerque, and Bronnenberg (2011)Lee and Bradlow (2011)Netzer et al. (2012)Ringel and Skiera (2016)Culotta and Cutler (2016)Nam, Joshi, and Kannan (2017)Gabel, Guhl, and Klapper (2019)Our StudyObjectiveTo visualize user search behavior and understand market structureTo visualize competitive market structure using text mining on customer reviewTo visualize competitive market structure using text mining on forum discussionTo understand asymmetric competition in the product categoriesTo infer attribute-specific brand ratingsTo analyze user-generated tags for marketing researchTo leverage NLP and ML for analyzing market structureTo propose a novel deep network representation learning framework for market structurePrespecifying market categoryYesYesYesYesNoNoNoNoNetwork typesBrand–brandBrand–brandBrand–brandProduct–productBrand–brandBrand–brandProduct–productBrand–userBrands/products62 products, 4 brands9 brands169 products, 30 brands1,124 products200 brands7 brands133 categories, 30,763 products5,478 brandsConsumers/usersN.A.N.A.76,587100,000+14.6 millionN.A.N.A.25,992,832Data sourcesAmazonCustomer review at EpinionsOnline discussion forumProduct comparison websiteTwitterSocial tagging platform DeliciousRetailerFacebook public fan pageData typeConsumer searchTextTextConsumer searchNetworkSocial tagsShopping basketsNetworkBrand association methodologyConsideration setText-miningText-miningConsideration setNetwork learningNetwork learningNetwork learningNetwork learningBrand relationship asymmetryYesNoNoYesNoNoYesYesDimension reductionYesYesNoNoNoYesYesYesExternal validationN.A.N.A.Purchase data,surveySurveySurveyBrand concept map (survey)N.A.Event study,survey, Google search trends 1 Notes: NLP = natural language processing; ML = machine learning; N.A. = not applicable.There are other studies where the product-market boundaries are not predefined: [11] using online reviews, [33] using social tags, and [ 7] using Twitter hashtags. More recently, using word embeddings, [12] analyze customers' market baskets of items purchased on shopping trips. Still, from a methodological perspective all these studies use brand–brand networks—a distinct disadvantage, as we discussed previously. Our methodology uses brand–user networks, and the scale at which we analyze the data is much larger than any of the extant methods (cf. [12]). Social Media EngagementWe analyze social media engagement data in the form of user–brand links. Social media platforms such as Facebook, Twitter, and Instagram host public fan pages created by firms to facilitate communication with customers and promote products. The user–brand engagement could be in the form of a user liking a post by the brand, sharing a brand post, or commenting on a brand post. Because each of these likes, shares, and comments/posts is a user–brand link in our study, it is important to understand what they represent. Surveys of fans of brands have revealed many reasons as to why users ""like"" a brand or post/share comments. Positive motivations for interacting with a brand include to support a brand they like, to get a coupon or discount, to receive regular updates from the brand, to participate in contests, to share personal experiences, to share their interests/lifestyles with others, to research brands, to imitate a friend who likes the brand, or to act on a recommendation from another fan ([25]; [34]; [39]; [41]). Conversely, users may also leave negative comments to hurt a brand in favor of its rival brand ([17]).In our approach, we make a minimal assumption by creating a user–brand link regardless of the type of engagement (like, share, or comment/post). This assumption is based on the rationale that users interacting with a brand online exhibit their interest toward the brand to some extent. Thus, the two brands are related to one another on a spectrum ranging from substitutes to independents to complements. Prior research has examined such contexts and studied the impact of user engagement on brand image and customer purchase intentions with mixed results ([16]; [34]). [31] use a field experiment to find that users who liked a gym brand online were likely to become members of that gym offline. In another field experiment setting, [18] find that ""liking"" is simply a symptom of a positive brand attitude and does not imply the fan is any more loyal to the brand or any more likely to purchase the brand. In addition, it is only when users who liked the brand are targeted using promotional communication by the firm that purchase probabilities increase. Thus, for our research purposes we treat a like or a comment/post as exhibiting an interest toward the brand at the beginning of the customer journey. Such a tendency for users to connect to brands is generally interpreted as interest and may indicate broader (e.g., offline) interactions ([ 7]; [25]; [34]; [35]), which is consistent with our treatment. Our proposed approach is also consistent with research in social network analysis suggesting that social network structure equivalence reflects value/interest homophily and can be used to measure social proximity ([29]). MethodologySocial network platforms, such as Facebook, Instagram, and Twitter, can be abstracted as a network containing business (firm) accounts and individual user accounts. Firms use the public fan pages of business accounts to communicate with their customers and fans. Users interact with brands and with each other in different ways, such as commenting, liking, sharing, and following. To discover latent relationships among brands, we propose a deep network representation learning framework with the following steps. Step 1: Data collectionWe specify a set of brands that is of interest in the social network platform. We then download all available user engagement data from the brands' public fan pages covering an appropriate time window based on managerial interest. A user engagement is defined as either liking or commenting on a firm's post on its public fan page. Note that for the sake of privacy, we do not attempt to collect any personal information of users. Rather, the only user information we obtain is the unique user identifier, assigned by platform, and the user's public engagement activities, consistent with recent studies on social media marketing ([17]; [23]).[ 5] Moreover, different platforms may have their own specific data policy. For example, Facebook does not permit collecting personal information from individuals who liked a given page. Such data restrictions and potential ethical concerns do come at a research cost, as we are unable to verify how representative they are of the population at large. Step 2: Network constructionWe start with a cleansing operation to remove spurious users. We then construct a brand–user network including all selected brands and all users engaging with them. A brand node and a user node are connected if the user engages with the brand. The strength of an edge between a brand node and a user node is the engagement frequency. Step 3: Deep network representation learningThe deep network representation learning algorithm represents each node (brand or user) as a low-dimensional vector, also known as a node embedding. Embedding techniques are not new in marketing. For example, [49] adopt pretrained word embeddings, where each word is represented as a low-dimensional vector, to extract insights from textual reviews. However, our node embeddings are trained via an unsupervised deep autoencoder. This representation learning is essential to data-driven analysis, and the learned low-dimensional embeddings are useful for the downstream task of identifying and visualizing the product-markets.The objective in using an autoencoder is to learn the representation of the data so that each node can be represented in a lower-dimensional space while the network structure between users and brands is preserved. It trains the network to ignore the ""noise"" in the data and focus on the primary latent structure. The autoencoder reduces the dimensionality of the input data to a ""bottleneck"" (the reduced encoding) and, using the reduced encoding as input, reconstructs a representation of the original data. Learning occurs through backpropagation of the loss (see detailed definition in Web Appendix WA1) to achieve a reconstructed representation as close as possible to the original representation. We are interested in the bottleneck-reduced encoding for developing market structure. In essence, we can compare the dimensionality reduction functionality of the autoencoder with that of PCA. Whereas in PCA the reduced dimensions are linear combinations of the input variables, the reduced dimensions in autoencoder are nonlinear and nonorthogonal, which is achieved through nonlinear activations of the neurons, allowing the model to learn more powerful generalizations than PCA can.In our application, the autoencoder works on the large brand–user network in an attempt to preserve the network structure such that ( 1) nodes that are directly connected have similar vectors (are closer to each other) in the reduced embedding space, and ( 2) nodes that are not directly connected but share structural equivalence (such as many common neighbors) are also similar in the embedding space. These two types of similarity are referred to as the first-order (direct connection) similarity and the second-order (network structural equivalence) similarity. Formally, we denote the aforementioned network as  G=(Vb,Vu,E)  , where  Vb=(v1b,v2b,...,vnb)  represents a set of n brand nodes,  Vu=(v1u,v2u,...,vmu)  represents  m  user nodes, and  E={ei,j},i≤m,j≤n  represents all links between users and brands.  ei,j  indicates an engagement between user  i  and brand  j  . Given such a network  G  , the network representation aims to learn a mapping function  f:vib,vju↦wib,wju∈Rd  , where  d≪min(m,n)  .  wib,wju  are called brand embedding and user embedding, respectively. A commonly used embedding dimensionality  d  is 300 ([30]; [40]). The objective of the mapping function is to develop appropriate embeddings so that the brand proximities, brand–user proximities, and user proximities exhibited in the original network are preserved as much as possible in the reduced embedding space. (Technical details of the autoencoder methodology and parameter tuning are discussed in Web Appendix WA1.) Representing brands as dense low-dimensional vectors allows us to capture brand relations from multiple facets, as opposed to using unique vectors for each user and each brand as in a network adjacent matrix representation. An example is illustrated in Figure 1. Step 4: Market structure discoveryDrawing on vector representation for brands and users, we use learned embeddings to efficiently compute similarity among brands and to visualize natural clusters of related brands. Finding similar brands to a focal brand can be achieved by a nearest-neighbor search based on the widely used cosine similarity, which measures the cosine of the angle between two vectors and has a range [−1, 1]. Visualizing natural clusters of related brands can be achieved by a dimension reduction method, such as t-distributed stochastic neighbor embedding (t-SNE; [30]), which projects high-dimensional data into a low-dimensional space (e.g., two or three dimensions).[ 6] It has been used for visualization in a wide range of applications and is especially well-suited for visualizing high-dimensional representations learned from deep neural networks. t-SNE preserves the distance of data points well, such that data points nearby in a high-dimensional space (d = 300 in our case) would be close in a lower-dimensional (e.g., two-dimensional) space, while distant data points would be further apart in a lower-dimensional space. Thus, we observe that related brands are surrounding each other in the reduced two-dimensional space after t-SNE. DataWe use Facebook as our empirical benchmark, as it is one of the largest and most representative online social network platforms. (Our model can be generalized to other similar social network platforms.) To collect Facebook data, we first obtain a list of U.S. brands with the most followers from the social media marketing website Socialbakers.[ 7] Facebook public fan pages are categorized into several groups on Socialbakers, such as Brands, Celebrities, Community, Entertainment, Media, Place, Society, and Sport. We focus on the ""Brands"" category because it covers a wide range of industries and is more interesting to marketers. On Facebook, every brand is associated with a category chosen from the predefined Facebook option when creating the public page. This category label is solely determined by the brand and is aligned with its core business (e.g., Walmart is in the category of ""retail,"" Amazon is in the ""ecommerce"" category). In total, we obtain 5,478 different brands, covering 25 different categories. The largest brand, in terms of number of followers, is Walmart, with 30 million followers. The smallest brand is Bladz Jewelry in the ""fashion"" category, with 100,000 followers. Figure 2 shows the histogram of number of followers of brand Facebook page. We observe that the data set contains brands with varying popularity, making it representative of brands on Facebook.Graph: Figure 2. Histogram of number of followers of 5,478 Facebook brands.On Facebook, firms post on their public fan pages and allow users to comment, like, and share posts. The posts become an important marketing channel for businesses to interact with their customers. We use Facebook Graph API[ 8] to download all activities visible on a brand page such as posts by the brand administrator, as well as posts by users, including comments and likes on brand posts. It is worth emphasizing that to ensure privacy protection, we do not download any user profile information or examine the content of user comments. All engagement activities are represented by unique user identifiers, regardless of whether the user has a public or private Facebook profile, and brand identifiers. The data set collected for this study covers the period from January 1, 2017, through January 1, 2018. In total, we obtain 106,580,172 user–brand engagement activities from 25,992,832 unique users. Because prior research has shown that online interaction is a reflection of broader and even offline interaction ([38]), given the scale of user online engagement in this study, we believe it is a good proxy of how the overall consumer population perceives these brands.To ensure data quality and robust results (i.e., that the comments on Facebook brand pages reflect genuine user experiences, opinions, and interactions with brands), we design a set of rules, following [54], to remove fake users and their corresponding activities. For example, we find one user who liked posts across 475 different brands. As most users are likely to be interested in far fewer brands, we remove users who like posts on more than 200 brands, which accounts for.01% of the total users and 1.6% of the total user–brand engagement. We also remove users who posted duplicate comments containing URL links. Table 3 describes the data details. The brands' degree distribution (number of connections) exhibits a scale-free distribution (shown in Figure 3), a well-documented phenomenon in most social networks.Graph: Figure 3. Degree distribution of brands in the user–brand network.GraphTable 3. Data Description and Statistics. Number of brands5,478Number of users25,992,832Number of unique user–brand interactions36,927,613Number of like interactions87,876,623Number of unique user–brand like interactions29,611,805Number of comment interactions18,703,549Number of unique user–brand comment interactions7,612,358Total number of user–brand interactions106,580,172  Evaluation and ResultsIn this section, we extensively evaluate the market structure derived from our approach from both quantitative and qualitative perspectives. We also validate the derived market structure using two external data sources: consumer survey and Google search trend. Visualization of Market StructureWith the learned brand representation vectors, we can visualize how the brands are grouped and focus on local fine-grained brand proximity. We use t-SNE to obtain market structure visualization by reducing the learned 300-dimensional brand representations to obtain the associated 2-dimensional visualization map. Figure 4 presents the global structure of the brands in our Facebook data. Each data point in the figure denotes a brand belonging to one of the 25 categories, and each category is indicated by a different color. We interpret the visualization as follows: the closer any two brands are in the figure, the more similar their brand representations are in the 300-dimensional space (see Figure 4). The color codes in the map indicate brands in the same Facebook category, with the category label selected by the brands themselves on Facebook.Graph: Figure 4. The global structure among brands.The global Facebook brand market structure map yields several interesting observations. First, there are clear grouping patterns into clusters, particularly between brands in the same industry (points with the same color tend to be in a group). For example, Cluster 1 in Figure 4 (expanded in Figure 5) includes nonluxury domestic and imported automobile brands such as Toyota, Nissan, and Mazda, as well as some automobile accessories brands such as Michelin, DENSO, and Auto Parts. Note that in our data we have several luxury automobile brands such as BMW, Mercedes-Benz, Audi, Tesla, and Maserati, which are not close to the brands in Cluster 1. In fact, they are clustered in a different region of the map with other luxury brands such as Chanel, Gucci, and Cartier. Such a separation between luxury car brands and nonluxury car brands further confirms that brand representation learned from our approach captures latent semantics in multiple dimensions not only on the industry dimension but also on the price and luxury dimensions. The strength of our methodology lies in its ease of capturing these relationships on a single map, which it does by locating thousands of brands in the market structure map and highlighting the complex and possibly overlapping product-market boundaries characterizing these brands. We present a robustness check for different visualization methods in Web Appendix WA5.Graph: Figure 5. Enhanced view of Clusters 1 (top left), 2 (top right), 3 (bottom left), and 4 (bottom right).We provide an enhanced view of the four clusters in Figure 5 to examine the fine-grained local market structures. Panel A displays automobile brands along with automobile accessories and motorcycle brands at the top. Panel B displays premium vacation resort brands, such as The Signature at MGM Grand and the Coconut Bay Beach Resort & Spa. Panels C and D contain airline brands and cosmetic brands, respectively. Taken together, these maps provide face validity to our methodology in terms of core brands making up an industry and the overlaps among product-markets. Identifying Proximal BrandsWhile visual mapping is sufficient to provide a gestalt picture of all 5,000 plus brands in the aggregate, it does not provide the actual distance between the brand vectors in the reduced dimension space. Because identifying proximal brands for substitute/complement analysis is a critical task in marketing decisions ([ 8]), we focus on identifying proximal brands from the perspective of a focal brand. In doing so, we offer a new perspective that reflects the nature of the varied relationships ranging from substitutes to complements in the social network space.In this illustration, we choose United Airlines and Southwest Airlines from the airlines category and Audi USA and Nissan from the automobile category, as these brands are generally regarded as having different consumer bases and belonging to different submarkets. Each of the four brands is referred to as a focal brand, and we find their top ten proximal brands according to cosine similarity. Table 4 provides several interesting insights. First, our method is able to capture specific brand latent characteristics. For example, Southwest Airlines is generally considered a low-budget airline compared with United. The brands most proximal to Southwest Airlines and United reflect this difference. The proximal brands for Southwest Airlines are JetBlue, Frontier Airlines, and Allegiant, while the most proximal brands for United are major domestic and international airlines, such as American Airlines, Delta, Lufthansa, All Nippon Airways, Air China, LATAM Airlines, and Air New Zealand. Similar results also are identified in the automobile industry. Second, we observe asymmetric competition (see [42]). For example, Southwest Airlines is the fourth-most-proximal brand to United Airlines, while United Airlines ranks sixth in the set of top proximal brands to Southwest Airlines.GraphTable 4. Top 10 Proximal Brands to Each Focal Brand. RankFocal BrandUnitedSouthwestAirlinesAudi USANissan1AmericanJetBlueMercedes-Benz USAMazda2DeltaFrontierBMW USAToyota3LufthansaAllegiantLand RoverVolkswagen4SouthwestDeltaLexusKia Motors America5AlaskaAlaskaChevrolet CamaroSubaru of America6All NipponUnitedMaserati USAChrysler7Air ChinaAirfarewatchdogKawasaki USAFiat8LATAMAmericanFirestone TiresJaguar9Air New ZealandVirgin AmericaTeslaAlfa Romeo10AirfarewatchdogHyattRam TrucksKlim Third, unlike prior market structure analysis, where proximal brands are usually from the same industry as the focal brand, the top most proximal brands derived from our analysis are from different industries. For example, a brand called ""Airfarewatchdog"" is proximal to both United and Southwest Airlines. Airfarewatchdog is a deal-finder for flight tickets and has a large follower base (over 1 million) on Facebook. Traditional market analysis would simply ignore this brand, as it is not an airline. Further, it is also interesting to see that Southwest Airlines is closer to Airfarewatchdog than to United, which may indicate that the fans of Southwest Airlines are more likely to use a deal finder before purchasing flight tickets; thus, Airfarewatchdog could be a complement to Southwest when customers look for cheap flights on that site and end up at Southwest, or it could potentially compete with Southwest. In either case, Southwest could focus more on this site and examine the nature of the relationship. Identifying Opportunities/ThreatsOur market structure map can help managers identify brands outside of the product-market that are close to a specific brand and, thus, identify opportunities and threats posed by different brands. Take the airline product-market (Figure 5, Panel C) as an example. In our analysis, Disney Cruise Line and Hyatt are two brands outside of the airline product-market but are identified as proximal brands to Southwest but not for United. These proximal locations simply are due to a greater number of users in our data set liking both Southwest and Hyatt ( 2,709) versus the number of users liking both United and Hyatt (954). Similarly, a greater number of users like both Southwest and Disney Cruise Line ( 3,050) than like both United and Disney Cruise (729).Such findings can provide opportunities for Southwest, as it could target users who like Disney Cruise and Hyatt on social media. Southwest could cross-promote these brands by teaming up with Disney Cruise and/or Hyatt on each other's websites and launch coalition loyalty programs. From the viewpoint of other hotel chains that are competitors to Hyatt, these could be potential threats, so gleaning such insights early on may help them take proactive actions. Such opportunities/threats are difficult to identify when product-markets are prespecified, and they cannot be obtained easily through other means. Large Brand Versus Small BrandOur user engagement data set contains top 5,478 primarily large brands, ranked by their popularity (number of followers as of data collection period) on Facebook. A key question is whether our proposed approach is still able to identify meaningful market structure for smaller brands. If they can find the right position in the product-market structure, smaller brands have the potential to increase consumer awareness and interest in their brands ([13]), which could lead to a permanent benefit in terms of competitive advantage ([47]). Therefore, to test whether our methodology is able to capture relationships among large brands as well as small and local business brands, we add a set of smaller brands to the original data set. Specifically, we focus on the ""Travel"" category, as it includes many small, local travel agencies, and their followers on Facebook range from a few hundred to a few thousand on average. In total, we have 241 travel brands. Figure 6, Panels A and B, plot the distribution of the number of followers of these travel brands and shows that it is quite diverse.Graph: Figure 6. Size and market structure of 241 travel brands.Upon applying our methodology to the enlarged data set, we observe (Figure 6, Panel B) that these 241 travel brands are predominantly located in two areas. This pattern indicates that the latent brand relationship is well captured, even when brands have few engagement activities due to their smaller user bases. In a brand–brand network, such a small number of shared user bases could result in a failure to capture proximal locations, in essence treating them as noise.The market structure uncovered for these small businesses by identifying their proximal brands has good face validity. For example, ""The Luxury Travel Expert"" is an information portal for luxury travel and premium tours, with about 11,000 followers on Facebook as of our data collection period. Most posts receive fewer than ten comments and likes. The top proximal brands based on the cosine similarity are Smithsonian Journeys, The Peninsula Beverly Hills, Peter Sommer Travels, Quasar Expeditions, and DuVine Cycling. It is noteworthy that these are small travel brands that focus on expert-led, small-group, luxury, and premium tours. The results further confirm that our deep network representation learning method is generalizable to both small and large brands. This analysis also allows brand marketing managers to identify business opportunities. For example, in our analysis, the two brands The Luxury Travel Expert and The Peninsula Beverly Hills are quite close. The former is an information portal for luxury travel and premium tours, and the latter is a five-star luxury hotel. Therefore, the marketing manager of the Peninsula Beverly Hills could promote the brand on the information portal website to attract users from The Luxury Travel Expert to expand its customer base. Within-Industry Market Structure AnalysisExtant methods typically predefine the product-market boundary to derive market structure and brand relationships. In contrast, we allow product-market boundaries to emerge from the data. Therefore, a natural question is whether it is necessary to have a broader range of brands from other industries to derive a highly precise market structure for a specific industry. Although managers would typically focus on engagement data for their brands and for brands within the same industry, how does engagement data from brands in different categories help? To answer this question, we choose the ""auto"" category and only use the engagement data from the auto brands to derive the market structure. In the data set, we have 163 auto brands, including cars and car accessories brands (e.g., tires, oil), with 2.7 million user engagements in total. The analysis shows (Figure 7, Panels A and B) that structures with reasonable face validity still emerge using only the auto brands data. For example, the top left corner in Figure 7, Panel B, presents a cluster of imported auto brands such as Kia Motor America, Toyota, and Nissan. However, compared with the derived auto brand market structure learned from using all brand data, as shown in Figure 5, Panel A, the market structure is less clustered and more ambiguous.Graph: Figure 7. Visualization of market structure of using engagement data only from ""auto"" brands. Notes: The right panel is the zoomed in visualization with BMW as centroid.Next, we compare the market structure using the engagement data from the auto brands alone with that from all brands across categories in a qualitative manner. Specifically, we choose the brand FMF Racing, which is a company that develops dirt bike exhausts for off-road or racing motocross riding. Using the engagement data from the auto brands alone, the top proximal brands are Lucas Oil, KTM USA, Yamaha Motor, Arctic Cat, Two Brothers Racing, Phoenix Pro Scooters, Auto Alliance, Valvoline USA, Lance Camper, and Castrol. Some are related to off-road motocross riding, while others are not. For example, Lucas Oil, Valvoline USA, and Castrol are global automotive oil brands.In contrast, the top ten proximal brands to FMF Racing emerging from using all categories of data are KTM USA, Polaris Snowmobiles, Fox Racing, Mickey Thompson Performance Tires & Wheels, Two Brothers Racing, King Shocks, Arctic Cat, Addictive Desert Designs, NISMO, Skunk2 Racing, and MBRP performance exhaust. Upon further investigation, we find that they are all related to off-road motocross riding. These results indicate that our approach with engagement data from brands across industries can learn better brand representation and thus reveal a highly precise market structure. External Validity and Comparison with Other Approaches Market structure identified based on consumer surveyTo assess the external validity of our approach, we conduct a survey on Amazon Mechanical Turk (MTurk), which is a reliable source for data collection and marketing analytics ([43]). Prior market structure literature has also administered brand perception survey on MTurk ([ 7]). Following this prior study, we surveyed 28 automobile brands (after ignoring the other 150 brands that are related to motorcycle or automobile accessory such as tires, parts, and oil). Specifically, we recruited 500 MTurk participants, each of whom was required to be in the United States and have a good MTurk record (successful completion of at least 100 assignments with a minimum 95% rate of approval). Each participant was asked to rate the similarity between a focal automobile brand and the other 27 automobile brands on a scale of one to five. To avoid fatigue due to information overload, each participant was randomly assigned to work on one task. Participants were also asked to indicate their age, gender, and whether they owned an automobile. Details of the participants' demographics information and the survey design are presented in Web Appendix WA6.In the survey, participants could choose ""N/A"" if they were not aware of the automobile brands. Brand recognition rate was 88.2%, implying that 11.8% of ratings were not applicable due to lack of brand awareness. We aggregated the survey data and built a 28 × 28 matrix, where each cell represented the pairwise brand similarity, and denoted it as the ""survey matrix."" We also used the brand representations learned from our approach to construct another 28 × 28 matrix of brand similarity, which we denoted as the ""deep learning–based matrix."" The correlation between two matrices is significantly positive (r = .385, p = .000). This result provides additional evidence on the validity of our deep learning–based approach for market structure identification. We also did an additional check where we calculated the correlation between the survey response and that constructed by our approach but using only automobile (within-industry) data. The correlation is.152 (p < .05), which is not as substantial and significant as the correlation between the survey response and our approach using all industry data. We present the market structure learned from the survey data in Web Appendix WA7 as an external validity. Market structure identified based on Google TrendsTo provide further external validity of our approach, we use Google Trends data to identify market structure and examine how it aligns with our approach of using online social media users' brand engagement. Google Trends provides an interest score for every search query across regions and languages, as measured by an aggregated search volume over time. A higher interest score means that queries are more popular in a specific region and time. Google Trends data have been widely used by industry ([44]) and academia ([ 6]; [ 9]; [22]; [48]) to address marketing and economic problems (e.g., competitive analysis). Researchers have also shown that this score is consistent with consumers' purchase interest in general ([ 6]; [ 9]).To determine relative popularity for every pair of brands, we make a search query consisting of two brand names—for example, ""Toyota BMW"" for the brands Toyota and BMW. For every brand pair, we can obtain an interest score returned by Google. For example, in the United States in 2017, the interest score is 13 and 85 for the query ""Toyota BMW"" and ""Toyota Honda,"" respectively. This indicates that consumers in general are more interested in searching Toyota and Honda together, compared with searching Toyota and BMW together. Validation on airline industryIn the first validation exercise, we focus on the airline industry and the derived market structure. We have 19 airline brands in our data set, including U.S. domestic airlines and international airlines (Figure 5, Panel C). For every brand pair, we first obtain a Google search interest score in the U.S. region in 2017 (the same as our engagement data period). Then, following previous work ([35]), we calculate the similarity between two brands A and B as  sim(A,B)=interest(A,B)∑b∈Sinterest(b,B)  , where  S  is the set of all brands (e.g., 19 here). [35] use the co-occurrence of two brands in an online discussion forum instead of a Google search interest score. We also calculate similarity for every pair of 19 airline brands using 300-dimensional vectors derived from our deep network representation learning on the engagement data using cosine similarity.To check whether the two aforementioned similarity scores are similar to each other, we calculate their Pearson's two-tailed correlation between two sets of 361 (= 19 × 19) similarity scores. It is significantly and highly correlated (  r=.630,p=.0000  ). This indicates that our social engagement-based market structure is similar to that derived from Google Trends. Because prior studies have shown that the Google search data have a high correlation with a consumer's actual purchase interest ([ 6]; [ 9]), we can conclude that users' social engagement with brands also contains valuable information for deriving brand relationships. Validation on travel industryIn the second validation exercise, we focus on the travel industry, including not only major travel brands but also many small and local travel brands (see the ""Large Brand Versus Small Brand"" subsection). There are 241 travel brands in the data set. Similar to the first validation exercise, for every brand pair, we obtain a Google search interest score in the United States in 2017 (the same as our engagement data period). Among the 241 travel brands, Google Trends does not return scores for 90 brands (i.e., showing ""your search doesn't have enough data to show here""), which results in data for 151 remaining brands. Although individual brands show a considerable amount of search, only four brand pairs return nonempty interest scores.[ 9] This data sparsity may be attributed to the uniqueness of the travel category. Many of the travel brands are local/small businesses, such as the travel agencies ""Spirit of Boston"" and ""Historic Philadelphia."" Naturally, they do not receive as many queries as large brands. Moreover, consumers may search travel agency brands in different queries, but they very rarely search two travel brands in the same query. Therefore, there is not enough data for Google to aggregate and return the cosearch score. This analysis highlights the limitation of the cosearch-based approach, which is likely to suffer from the data sparsity issue. In contrast, our approach built on large-scale brand–user social engagement data can provide valuable marketing insights not only for large international brands but also for small local brands. Practical Actionability How to compare market structure maps?In a practical setting, marketing managers may need to quantitatively determine the quality of derived market structure maps, based on which they can infer actionable insights. We evaluate the conceptual maps using a standard metric—silhouette score ([ 1])—which has been adopted in prior market structure literature ([12]). The silhouette coefficient is calculated using the mean intracluster distance (a) and the mean nearest-cluster distance (b) for each sample, as  b−amax(a,b)  . The values of silhouette score range between −1 and 1 (1 being the best and −1 the worst). Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster. Recall that our approach can naturally group brands that have similar representations in the high-dimensional space. An ideal market structure would favor brands that are concentrated and exhibit clean cluster structures. We conduct K-means clustering on the brand representations and compute the mean silhouette coefficient of all samples.In the ""Within-Industry Market Structure Analysis"" subsection, we qualitatively show that our approach—without prespecifying product-market—reveals more interesting and coherent brand insight than using brand engagement data within only one industry. Next, we vary the number of clusters in K-means and calculate the silhouette coefficient of different methods. The result in Figure 8 shows that our approach using all brand engagement data consistently achieves better clustering than using only the automobile brand engagement data. For example, when we cluster 168 automobile brands into two clusters (i.e., K = 2 in K-means), our approach achieves a silhouette coefficient of.334, while the approach using only the automobile engagement data has a low silhouette coefficient of.043. The silhouette coefficient of our approach gradually converges to.10 as the number of clusters increases. In contrast, the approach using only automobile industry data stays near.01, indicating a poor separation among automobile brands. This analysis not only confirms the superiority of our approach without prespecifying product-market boundary but also enables marketing managers to determine the quality of derived market structure maps. How much brand–user engagement data are needed to derive a good market structure?We have shown that our approach can derive good market structure with large-scale social engagement data. In practice, it is easy to obtain a relatively comprehensive set of brands across different categories and associated user engagement from social media marketing platforms, such as Socialbakers. However, a marketing manager may not have enough resources to collect as large a data set as we have, raising the question as to whether our approach is sensitive to the size of data for obtaining a good market structure. To answer this question, we calculate the correlation between the similarity of pairwise brands generated using the full data set and that generated by a fraction of data selected at random. Figure 9 presents this result, showing that the correlation reaches over.90 when 40% data is used (and.7 when 12.5% data is used), and it starts to converge to the market structure generated by using the full data. This analysis suggests that our approach is relatively robust to the amount of engagement data used. Marketing managers can use this analysis as guidance to determine the amount of data resources needed. In addition, we examine how the number of prespecified industries affects the robustness of market structure maps and present the analysis in Web Appendix WA8.Graph: Figure 8. The clustering silhouette coefficient of 168 automobile brands.Graph: Figure 9. Pearson correlation between the similarity of pairwise brands generated using a percentage of full data and the full data. Evaluation using link predictionIn studying market structure, there is a lack of ground truth about the identified structure, that is, an understanding of what the ""true"" structure is, which makes demonstrating the performance of various proposed methods challenging. We introduce an alternative approach, adopted from network analysis literature ([27]), to evaluate the identified market structure. An identified market structure is a function of the brand representation, and so an accurate representation is more likely to identify valid market structures. This approach is supported by prior research showing a strong relationship between brand image and the characteristics of a brand's supporters and followers ([ 7]; [25]; [34]). If a network learning method were capable of accurately representing network nodes accounting for these relationships between brands and users, then it would be able to predict the future links between brands and users accurately. Therefore, we use a cross-validation procedure under a link prediction research design, where we predict the most likely newly formed links of user–brand engagement in an out-of-sample network given the brand vectors and user vectors learned from a training network. This research design is widely used in the network analysis community to evaluate network clustering algorithm performance ([27]; [51]). In our context, we use the user–brand interactions from the first half of the time span in our data to build a training network (G0,1) and use the second half to build a testing network (G1,2). The likelihood of a link formation is measured by the proximity of a learned brand vector and a learned user vector. Note that link prediction performance is significantly correlated with the quality of learned vectors, given the assumption that a better network representation learning can predict new interactions between users and brands with a high degree of accuracy. We provide details of the link prediction experiments in Web Appendices WA2–WA4. Overall, our analysis shows that ( 1) link prediction using representation learned from our brand–user network performs better than a reduced brand–brand network (a widely used method in extant approaches), ( 2) deep learning–based methods learn better representation than shallow machine learning methods, and ( 3) our deep learning–based model is robust and able to handle sparse networks as compared with baselines. Case Studies on Market Structure DynamicsMarket structure evolves over time and can change dramatically, especially under an unexpected industry shock. Whether our proposed method can be adaptively learned is also of interest as it could provide useful insights to marketing practitioners. In this section, we analyze how market structure changes under exogenous shocks by analyzing two case studies: Amazon acquiring Whole Foods and Tesla introducing the Model 3. We take a before-and-after strategy where we use data for the three months pre- and postevent announcement day and calculate the change in distance from the focal brand (i.e., Amazon and Tesla) to other representative brands selected from the same category. The purpose of the event study is to examine how a focal brand relationship with other brands changes as a major event occurs. Specifically, for Amazon–Whole Foods, we select several brands from the retail and e-commerce category, and for Tesla, we select several brands from the automobile category. We calculate the change between focal brand  i  's representation  wib  and target brand  j  's representation  wjb  before and after the specific event using cosine similarity:  cossim(wiafterb,wjafterb)−cossim(wibeforeb,wjbeforeb)  . Therefore, positive numbers indicate a similarity increase, whereas negative numbers indicate a decrease in similarity. Amazon Acquires Whole FoodsAmazon acquired Whole Foods in June 2017. This acquisition has had a significant impact on the grocery and retail industries. At the time, it was widely believed that Amazon planned to use its acquisition of Whole Foods to enter the online grocery delivery business. Amazon and Whole Foods ran separate Facebook pages. After the merger of the two firms, we see from Figure 10 that Amazon is more proximal to retail brands as measured by cosine similarity, while the proximity to other relevant brands decreases slightly. For example, the cosine similarity between Amazon and Lowe's Home Improvement decreases by.184. In contrast, the cosine similarity between Amazon and other super-market retailer brands increases. Among them, proximity of Amazon to Whole Foods increases by.202, and between Amazon and Kroger by.165. As inferred from our data-driven model, Amazon even becomes more proximal to Walmart, indicating that Amazon's competitive market structure landscape has shifted. By further examining our data, we find that, after the Whole Foods acquisition, the number of common users who interact with both Amazon and Whole Foods on their public Facebook pages increases. Some Amazon users posted comments on Whole Foods' fan page mentioning Amazon. For example, in the Whole Foods post, ""Here are 6 New Healthy Products Coming to Whole Foods in March,"" a user who had liked an Amazon post earlier commented, ""You mean AMAZON... as they bought Whole Foods...right?"" This direct link between Amazon and Whole Foods leads the deep autoencoder to increase the proximity between the two brands. Moreover, in another Whole Foods post, a user who had liked a Kroger post earlier posted, ""The quality has gone downhill and prices have soared.... You've made Kroger look appealing...."" Although we do not find that this user has ever interacted with Amazon's Facebook page before, her interaction with Whole Foods leaves an implicit connection between Amazon and Kroger, which can be captured by the deep autoencoder. In short, after Amazon acquired Whole Foods, online social media users who are Amazon's fans pay more attention to Whole Foods, and users who are fans of other supermarket brands engage more with Whole Foods due to the acquisition event. As a result, the deep autoencoder captures the dynamics and updates the brand representation accordingly.Graph: Figure 10. Similarity change of Amazon to other brands in retail and e-commerce industry.The acquisition by Amazon has an impact on the market structure of Whole Foods as well. In Figure 11, we consider Whole Foods as the focal brand and calculate the change in proximities to other brands before and after the acquisition. Drawing on the results, we observe that Whole Foods' proximity to other retail brands such as Target, Walmart, and Best Buy increases. Among them, the proximity to Amazon increases the most due to the increase in the number of common users between them. In contrast, Whole Foods' proximity to supermarket brands such as Goya Foods, Enjoy Life Foods, and HelloFresh decreases slightly. Second, the magnitude of change in proximity values is smaller than those of Amazon to other brands. This seems to indicate that the acquisition has had less impact on Whole Foods, as it is still positioned around other supermarket brands, while Amazon is expanding closer to the grocery retail category.Graph: Figure 11. Similarity change of Whole Foods to other brands in retail and e-commerce industry.Although this analysis is retrospective, it highlights that our approach offers managers a series of multiple snapshots of the structure over time to measure a brand's relative position change, thus identifying potential market structure change. Suppose a supermarket chain brand A observes that Amazon is moving closer to A's position on the map. This may indicate that Amazon is getting more engagements (likes or comments) from A's customers. Given that one motivation of liking a brand's Facebook post is to receive some benefit from the brand (e.g., coupon, discount), it could further indicate that Amazon is conducting effective promotional marketing campaigns on social media. No matter the underlying reasons, the increasing proximity of Amazon on the brand map can at least provide an early warning to A's marketing managers to the potential threat. Late response to the competition may harm the brand and eventually the whole business.Next, we validate the case study of Amazon's Whole Foods acquisition using Google Trends data. Similar to the first external validity exercise, we choose 29 ""retail"" brands (including Walmart, Target, Macy's, Best Buy, Walgreens, Lowe's, Whole Foods, IKEA, Sears, 7-Eleven, Dollar General, Sam's Club, Dollar Tree, CVS Pharmacy, Aldi, Barnes & Noble, Costco, Kroger, Meijer, Safeway, Office Depot, Rite Aid, Albertsons, ShopRite, and The Fresh Market) plus Amazon and obtain their interest scores for every brand pair in the United States in 2017. Note that we exclude some small retail brands such as Goya Foods because their Google cosearch interest scores with other brands are mostly 0, indicating insufficient search data for the brand.The Pearson's two-tailed correlation between two sets of 900 (= 30 × 30) similarity scores is significantly high before (  r=.675,p=.0000  ) and after (  r=.758,p=.0000  ) acquisition. This result confirms the external validity of our social engagement–based method. We observe that for Amazon, before the Whole Foods acquisition, the most similar brands were Barnes & Noble, Macy's, and Best Buy. After the acquisition, the most similar brands are Whole Foods, Barnes & Noble, and Macy's. For Whole Foods, before the acquisition the most similar brands were The Fresh Market, Albertsons, and ShopRite. After the acquisition, the most similar brands are The Fresh Market, Amazon, and Safeway.We obtain further search interest data for one year after the acquisition (June 2017 to June 2018) to examine whether the market structure change is sustained for a long period after the acquisition announcement. For Amazon, the most similar brands are still Whole Foods, Barnes & Noble, and Macy's. Other grocery ""retail"" brands such as Kroger and The Fresh Market become more similar to Amazon than before the acquisition. For Whole Foods, the most similar brands are The Fresh Market, Safeway, ShopRite, and Amazon. Because Whole Foods is still Amazon's most similar brand among these retailer brands, this indicates that for Amazon, the acquisition impact holds for the extended period of analysis. It seems that the acquisition has less of an impact on Whole Foods, as Whole Foods is still positioned around other supermarket brands. All findings are consistent with our case study using social engagement data, which provides external validity to our results. Tesla Announces the Model 3Tesla sells two types of sedans: the Model S and the Model 3. The Model S is a luxury premium sedan with a larger range of acceleration and customization options, while the Model 3 is designed as a more affordable mass-market electric vehicle. The Model S can cost over $100,000 depending on the configuration, while the Model 3 costs approximately $35,000. After the announcement of the new Model 3, we see that Tesla becomes more distant from luxury car brands and moves closer to nonluxury car brands. We can see in Figure 12 that the cosine similarity between Tesla and the luxury car brand Maserati decreases by.209. Similar trends exist between Tesla and other high-end or luxury car brands such as BMW, Mercedes-Benz, Audi, and so on. Meanwhile, Tesla becomes more proximal to Kia, Mazda, and other more affordable car brands.[10]Graph: Figure 12. Similarity change of Tesla to other selected brands in the auto industry. Testing for SignificanceIn the previous analysis, we compute the distance change between the focal brand (i.e., Amazon or Whole Foods) and other brands before and after the acquisition. We can see that there is a significant increase in similarity between Amazon and Whole Foods after the acquisition. However, whether this distance change is caused by the acquisition or other unobserved factors, such as the difference of data split and/or noise, still remains unclear. Therefore, we conduct further analysis by randomly splitting all data before the acquisition into two parts (i.e., d1 and d2, with d1 before d2). We then measure the distance between Amazon and Whole Foods using d1 and d2 separately. We repeat this process 30 times using different data cuts in the preacquisition data. The average distances between the two brands across using all d1s and d2s are.228 and.232, respectively. The two-tailed t-test on the distance is.055, which indicates that there is no statistically significant difference between the distances between Amazon and Whole Foods before the acquisition in different cuts of the preacquisition data. Accordingly, the substantial increase in similarity between Amazon and Whole Foods is not attributed to sample differences.We perform a similar process on Tesla's introduction of the Model 3. In particular, we choose one nonluxury brand, Mazda, and compute its distances to Tesla before the event using various data splits. The average distances between Mazda and Tesla across using all d1s and d2s are.185 and.191, respectively, with a p-value of.076. This seems to indicate there is no statistically significant difference between Mazda and Tesla when the cutting point of data varies before the event. Therefore, we conclude that after Model 3's announcement, Tesla becomes more similar to nonluxury automobile brands on the social media platform. Note that we also conduct analyses on Tesla and other automobile brands, and the results are consistent. Implications and ConclusionOur proposed approach examines millions of user engagements with thousands of brands and focuses on the early stage of the customer journey. This allows for visualization of potentially overlapping product-market boundaries across many categories and helps managers identify latent threats and potential opportunities, which cannot be done with extant methods that focus on later stages of the customer journey (lower levels of the purchase funnel) within categories. As an example, for Southwest, is Airfarewatchdog a potential competitor that might draw visitors away, or is it a complementor that would increase visits to Southwest? Having identified the overlapping market with Airfarewatchdog, Southwest could invest more attention to evaluate the exact nature of this relationship. If Airfarewatchdog is a competitor, then Southwest might focus on developing strategies to differentiate itself and channel visitors to its website exclusively. If it is a complementor, then Southwest might run display ad campaigns on Airfarewatchdog's website. In addition, both Disney Cruise and Hyatt are closely associated with Southwest, with common users who like these brands on social media; therefore, Southwest could run mutually beneficial joint and cross-promotions with these other brands. In fact, all these brands could join in a dynamic coalition loyalty ecosystem built around a fluid partnership of products, services, and experiences, thereby providing a unifying customer value proposition that could be difficult to compete against ([ 4]). Identifying such unusual or unforeseen insights is the greatest advantage of our approach.Another important strategic use of our market structure maps is to identify competitors and complementors across industries and track how these relationships change over time. While [15] apply text analysis to 10-K statements to identify such grouping based on product descriptions that the firms provide, we provide a more dynamic structure based on actual customer/user social media activities. Moreover, our market structure map is more forward looking and predictive of emerging competition and complementors and more proactive than those based on 10-K statements, which can be viewed as reactive. Because [14] show that merging firms with more similar product descriptions in their 10-Ks results in more successful outcomes, using our market structure maps to identify merger-and-acquisitions targets (firms sharing common users) may have similar benefits. We leave this for future research.The power of our method lies in its ability to capture the dynamic changes in market structure. Because the maps are based on the analysis of big data that can be collected in a relatively short window of time, our methodology can track changes in their relative position when firms introduce new products, new promotions, and new marketing initiatives. The case studies that we highlighted provide good illustrations of this. In addition, although we have not analyzed this in the article, firms can deploy our method to enhance their social network-based marketing efforts by better targeting specific potential customers, because user nodes in the network are also learned and represented as vectors in the same multidimensional space as brands. Our link prediction design demonstrates a possible use for targeting. Finally, our proposed method is generalizable to other similar platforms if we can construct a brand–user network from public fan pages' engagement data. We implemented our proposed method using NVIDIA P100 graphics processing unit, with Tensorflow as the back-end deep learning framework. For future research to replicate or practitioners to adopt, we have provided details regarding data collection, data cleaning, and deep model architecture, and model the fine-tuning process in Web Appendix WA1.While marketing analytics techniques have extensively used consumer personal data to derive valuable insights, they raise many privacy and ethical concerns. How to balance these two important aspects has become a key consideration for many marketing scholars ([ 3]; [52]). Our approach provides a useful example. The only input to our network representation learning method is the brand–user network, which can be publicly obtained from brands' social media page.Our research has some limitations. Given the nature of our data, our method cannot examine stockkeeping unit–level competition as is done by some of the extant methodologies using lower funnel data. From this perspective, we recommend our methodology as a complement to extant methods and for higher-level brand strategies and tactics. Future work could examine how perceptual maps vary by customer segment using lower-funnel data such as purchase frequency and purchase amount. Second, our analysis is conducted on one social network, Facebook. Even though Facebook is one of the largest online social networks, with billions of users and thousands of brands, it is likely that users on different platforms exhibit different engagement behavior, and some of the research findings may not be generalized to other platforms. For example, it is reported that Instagram users and Facebook users fall into different age groups (Pew Research Center 2021). We could apply the same technique to other social media platforms and compare findings. Finally, each link in the user–brand network is created when the user engages with the brand on the public page. Facebook has introduced various reaction emotions to the platform to allow users interact with brands in different ways, such as ""Like,"" ""Love,"" ""Care,"" ""Haha,"" ""Wow,"" ""Sad,"" and ""Angry."" Future work could build a multirelational network to deeply capture brand–user engagement heterogeneity. "
25,"Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of Mouth The authors investigate how the geographical distance between online users is associated with electronic word-of-mouth (eWOM) effectiveness. Their research leverages variation in the visibility of eWOM messages on the social media platform of Twitter to address the issue of correlated user behaviors and preferences. The study shows that the likelihood that followers who are exposed to users' WOM subsequently make purchases increases with followers' geographic proximity to the users. The authors propose social identification as a potential mechanism for why geographical distance still matters online in eWOM: because consumers may form a sense of social identity based on their physical location, information regarding the spatial proximity of users could trigger online social identification with others. The findings are robust to alternative methods and specifications, such as further controlling for latent user homophily by incorporating user characteristics and embeddings based on advanced machine-learning and deep-learning models and a corpus of 140 million messages. The authors also rule out several alternative explanations. The findings have important implications for platform design, content curation, and seeding and targeting strategies.Keywords: electronic word of mouth; social media; geographical distance; deep learningElectronic word of mouth (eWOM) plays a key role in shaping consumer attitudes and behaviors. More consumers digitally share information, research what others say about products and services, and rely on eWOM to gain knowledge or make decisions than ever before ([25]; [81]). While eWOM is becoming one of the most influential sources of information among consumers, the effectiveness of traditional communication channels employed by marketers is declining ([93]; [102]; [104]). These contrasting trajectories have motivated marketers to harness the power of eWOM. To do so, they are paying more attention to their user base and the corresponding eWOM episodes ([ 3], [ 4]; [49]). At the same time, technological advancements and data opportunities in the online space enable marketers to access data such as user-generated content and user location ([ 6]; [112]). As the availability of location data in particular grows, marketers wonder whether location plays an important role in eWOM effectiveness. Some marketers believe that consumers may consider eWOM from farther distances as more widely accepted and universal and, thus, more valuable and informative ([42]). Others believe that eWOM from closer distances may be perceived as more relatable and relevant and, thus, more trustworthy ([19]; [95]). Yet other marketers suppose that distance does not matter online and, thus, does not influence how consumers value eWOM ([90]).The literature on the role of geographical distance in eWOM effectiveness is not decisive either. On the one hand, a common belief for the effectiveness of eWOM is that technology bridges the geographical distance between consumers. Information technology reduces communication costs by decoupling the interaction process from geographic constraints. For instance, mobile devices enable shopping and communication between consumers independent from location, while the internet allows instant access to message exchanges and marketplaces. These reduced communication and access costs have led some scholars to pronounce the ""death of distance"" ([29]) and ""end of geography"" ([52]).On the other hand, geographical distance may still play a role in online consumer behavior and WOM for several reasons ([51]). These include location-specific goods as well as economic costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience ([ 2]; [59]). Local user preferences and spatially correlated social ties as well as limited regional availability of alternative product options can also lead to commonalities in product adoption among nearby consumers (e.g., [21]; [72]; [79]). These reasons suggest that geography may still constrain the effectiveness of eWOM.Our research investigates whether geographical distance is significantly associated with the effect of eWOM, beyond the aforementioned explanations. Specifically, we ask whether the geographical distance between pairs of familiar disseminators and receivers of online WOM messages in social media plays a role in driving recipients' purchase behaviors or whether distance does not matter for eWOM beyond utilitarian reasons—such as transaction costs— and proxying for correlated user behaviors and preferences. We therefore examine this research question in an online setting where economic costs, such as those relating to contracting and travel, are not of concern.To investigate whether geographical distance is associated with the effectiveness of eWOM, we use rich data from the social media platform of Twitter, where users disseminated messages about their real-world product purchases ([103]). Our main identification strategy leverages variation in the visibility of these eWOM messages—enabled by a unique feature of the platform—causing some purchase messages to be visible and others to be invisible to followers ([ 1]). We compare the purchase behaviors of followers for whom users' purchase messages were included in their newsfeeds (i.e., the stream of tweets presented on the home screen of a user from accounts the user follows on Twitter) with the behaviors of followers for whom users' purchase messages were not included in their newsfeeds due to this design feature, while employing an extensive set of controls (discussed in the ""Econometric Model Identification"" subsection). We also use the salience of user locations on Twitter to provide evidence for the potential mechanism. In our empirical setting, users are familiar with each other and thus aware of the location of their peers. Their location is often salient, as users may highlight this information in their profiles; user profile information is observable in peers' timelines when hovering with the cursor over the profile picture, the username, or the person's name.We find that the relationship between eWOM and the likelihood that message recipients make a purchase strengthens as the geographical distance between disseminators and receivers decreases. This relationship is economically significant, managerially relevant, and robust to alternative methods and identification strategies. Social identity may explain why geographic proximity could increase the effectiveness of eWOM, beyond utilitarian reasons and proxying for correlated user behaviors, as consumers may form their social identities based in part on their geographic location ([44]; [83]; [107]; [108]). To provide evidence for this potential mechanism, we investigate, for instance, how the salience of geographic cues as well as conditions that strengthen the role of geographical location in the social identification process ([101]; [109]) further enhance the effectiveness of eWOM.Our findings of geographical distance variation in eWOM effectiveness have important implications for both theory and practice. We contribute to the online WOM influence studies by demonstrating that despite technology's promise, spatial distance continues to play an important role in a digital world ([ 9]; [71]). Specifically, we show how geographic proximity is significantly related to an increase in eWOM effectiveness. Geographic constraints thus tether the impact of electronic communications. This finding also contributes to the literature that shows how features of disseminators and receivers can drive eWOM outcomes ([20]; [49]) by identifying geographic proximity as a relational characteristic that can affect dyadic influence. Our work thus provides actionable strategies for boosting the effectiveness of online interpersonal communications. Our findings also imply that customizing seeding and targeting with more proximate connections or highlighting information and cues associated with social identity formation can increase the effectiveness of online advertising, product recommendations, social advertising, referral programs, and other marketing strategies and tools by leveraging local appeals and social identification. Related LiteratureSeveral studies demonstrate the importance of eWOM, documenting how it can be a major driver of consumer behaviors. For example, user opinions have been found to influence the consumption of movies ([34]; [70]), television shows ([47]; [71]), video games ([114]), and books ([33]; [69]). Whereas these studies examine the direct impact of eWOM on purchase decisions, the present research extends this line of inquiry by focusing on conditions under which the effect of online WOM may be attenuated or accentuated. Contextual Factors Affecting eWOMResearch has begun to investigate the contextual factors that impact the effectiveness of eWOM. These factors include characteristics of the product ([22]), brand ([73]), or WOM message itself ([85]). Scholars have also examined how certain features of disseminators (senders) and recipients shape the effect of eWOM. For instance, a sender's brand loyalty ([49]), expertise ([11]), and identity disclosure ([44]) can each affect the influence of WOM messages. Recipient characteristics such as the number of ties to adopters, demographics ([65]), and product experience ([87]) can also affect the influence of online communications. In addition, characteristics of the sender–recipient dyad, such as the tie strength ([13]; [20]), similarity across personality traits ([ 1]), and sociodemographic similarity ([45]), can also affect online WOM performance. We add to this stream of research by shedding light on an important factor that characterizes the pairwise relationship between senders and receivers of eWOM: the geographical distance between them. The Impact of Geographical Distance in Various Commercial ContextsGeographical distance has been shown to affect certain outcomes in multiple online and offline commerce settings. For example, geographical distance can affect online trade flows and volume due to costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience, as well as in cases of location-specific goods ([59]). Such transaction costs can engender a ""home bias"" that leads consumers to prefer transacting with nearby others ([ 7]; [59]; [68]). Similarly, geographical distance has been shown to correlate with product diffusion in the offline world due to imitation or direct observation, as physically close neighbors are more likely to adopt the same product ([21]; [36]; [43]).[ 5] In this study, however, we investigate how the geographic proximity between eWOM message disseminators and receivers accentuates or attenuates the effectiveness of online WOM, whereas the extant explanations for the impact of geography—albeit in different online settings—are not applicable to the context of eWOM; similarly, other explanations that do not apply to our eWOM setting relate to the location specificity of products, distribution networks, or local network externalities. Geographical Distance as a Potential Factor Affecting Online PersuasionRecent studies hint at the potential role geographical distance may play in facilitating persuasion. For instance, lab studies find that when consumers have no identifying information about online reviewers, they assume that the reviewers are similar to them and so are as persuaded by them as they are by reviewers who appear similar to them, and more persuaded than by reviewers who appear dissimilar to them ([82]). When ambiguous reviewers appear less similar, consumers are also less persuaded by their boastful reviews ([86]). One of the many ways these studies manipulate cues to indicate reviewer similarity is to show that the reviewer appears to live in the same or a nearby city as lab participants. In instances of familiar peers, rather than ambiguous reviewers, field studies find that consumers are more persuaded the more recent or intense their relationships are with their peers ([13]; [32]). One of the ways [13] measure relationship recency is to use as a proxy whether peers currently live in the same town. Whereas the aforementioned studies focus on how ambiguity about, trust in, or relationship with peers affects persuasion in settings without financial transactions or product purchases, our research focuses on whether the geographical distance between senders and receivers of online WOM messages is significantly related to eWOM effectiveness. Prior Research on How Geographical Distance Impacts eWOM EffectivenessMost relevant to our work are two studies examining whether geographical distance affects information diffusion and adoption. Specifically, [45] examine how geographic proximity, measured as contiguous relationships between states, and sociodemographic proximity, measured as similarity in demographics between states, affect overall message propagation at the state level. They find that geographic proximity has no impact when sociodemographic similarity is accounted for. Because of data limitations, their operationalization of geographic proximity may mask the actual effect of distance. Importantly, they conduct an aggregate state-level analysis, noting that the ""state-level nature of the data is a limitation"" (p. 249) and ""access to more disaggregated data would allow for a more granular analysis"" (p. 264). Their analysis also prevents them from observing whether consumers are familiar with the message propagator or the social distance between peers ([75]); familiarity and similarity between users as well as relationship strength have been shown to impact peer influence ([13]; [82]). Besides, diffusion may rely on a different mechanism than eWOM ([98]).[79] is also very relevant to our work. These authors first conduct a descriptive study in the offline world and find that living nearby adopters of a cellular service provider is associated with faster switching to that provider. Because they do not directly observe WOM episodes, they conduct their analysis at an aggregate level, where it is not possible to account for homophily (i.e., the tendency of individuals to choose friends with similar tastes and preferences) or attribute WOM influence among the ties of each user ([75], [76]). In addition, they measure geographical distance as the average distance among all possible ties of each consumer.[ 6] These limitations motivate them to conduct scenario-based experiments, in which they investigate the mediating role of perceived homophily and show that geographic proximity to an ambiguous online reviewer increases the likelihood of following the reviewer's opinions, as geographic proximity is used as a cue for perceived similarity. However, the effects of geography are likely to be different in such lab settings due to the lack of familiarity and social ties with online reviewers, which play an important role in persuasion, as the extant literature shows ([13]; [82]; [86]). In addition, while consumers may rely on any available cues to address these concerns of ambiguity and lack of familiarity, lab settings often provide few such cues beyond geography to inform their decisions ([82]). Thus, it remains unclear whether geographic proximity can still play a role in facilitating eWOM influence in real-world settings, where users are familiar with and socially connected to each other and have access to an abundance of available cues.In summary, we empirically investigate whether the geographical distance between individual pairs of familiar disseminators and receivers of organic eWOM in social media plays an important role in driving recipients' subsequent purchase behaviors of physical goods. Table 1 outlines the related studies and additional important differences of our research.GraphTable 1. Overview of Related Studies on the Role of Geography in eWOM. Study (Chronological Order)Research FocusContextSettingWOM TypeSocial TiesFamiliarity with SenderGeographical Distance MeasureAnalysis LevelNotesMain FindingsNaylor, Lamberton, and Norton (2011)How ambiguous reviewers affect persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (same vs. different city)Choice instanceSame vs. different city domicile is one of many reviewer variables manipulated to match that of a lab participant.Consumers assume that reviewers with no identifying information have similar tastes as them, so they are similarly persuaded as they are by reviewers who appear similar to them and more persuaded than by reviewers who appear dissimilar.Aral and Walker (2014)How tie strength and embeddedness affect app adoptionFree Facebook app adoptionSocial media platform (Facebook)Indirect; passive (automated notifications without variation)ObservedFamiliarity with the subject (not sender)Binary (same vs. different town)Notification episodeThe authors note that they ""estimate how relationship-level covariates [same town] are correlated with the extent or impact of influence"" (p. 1363), and the ""results may have limited generalizability to ... cases where there is a significant financial cost to adopting a product"" (p. 1366).Consumers will adopt a free Facebook (movie review) app more quickly the more recent their relationship is and the more embedded they are with an existing user.Packard, Gershoff, and Wooten (2016)How boastful WOM affects persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (nearby vs. distant location)Choice instanceNearby vs. distant location is one of several variables used to manipulate how similar the reviewer appears to a lab participant.Consumers are less persuaded by boasters' WOM when trust cues are low.Chen, Van der Lans, and Phan (2017)How relationship characteristics in a social network affect diffusionMicrofinance program adoptionPotential offline WOM episodeUnobservedSurveysYesNoneAggregateWOM occurs in geographically bounded networks (e.g., village, university), so geographical distance is not material.Social (economical) relationships are the most (least) important drivers of adoption for a microfinance program.Message propagationUniversity network platformDirect; activeObservedYesNoneWOM episodeRelationship intensity (e.g., message volume) is the most important driver of information diffusion in a network.Fossen, Andrews, and Schweidel (2017)How social vs. geographic proximity affect diffusionMessage propagationSocial media platform (Twitter)Direct; activeUnobservedUnobservedBinary (contiguous vs. noncontiguous state)AggregateThe authors note that ""the state-level nature of the data is a limitation"" (p. 249). The study does not control for advertising or other marketing activities and focuses on diffusion.Sociodemographic similarity propagates online message spread. Geographic proximity has no effect when accounting for sociodemographic similarity.Meyners et al. (2017)How geographic proximity affects adoptionCellular service provider adoptionPotential offline WOM episodeUnobservedObserved (aggregate)Yes (aggregate)Average distance from all peer adoptersAggregateThe authors note their field study ""did not have information on either the valence of the signals from the social network or the location of nonadopters"" (p. 63), averages distance across adopters, does not observe WOM, does not control for network externalities, and studies switch behavior. The lab studies have no homophily cues beyond age and gender, and nofamiliarity with or social ties to reviewers.Geographic proximity to adopters of a cellular provider is associated with faster switching to the provider.Scenario-basedOnline reviewsIndirect; active; artificialNoneNoContinuous (miles), categorical (state)Choice instanceGeographic proximity to an ambiguous reviewer increases the likelihood of following the reviewer's recommendation due to perceived homophily.Present studyHow geographical distance affects eWOM effectivenessProduct purchases of significant financial costSocial media platform (Twitter)Direct; activeObservedYesContinuous (miles)WOM episodeOur study has information on nonadopters and social ties; controls for homophily, advertising, and message content; and studies actual product purchases of significant financial cost in a social network where receivers are familiar with the WOM sender.Geographical distance is negatively associated with the likelihood that eWOM influences purchases, above and beyond other effects.  Empirical Setting and Data DescriptionOur empirical setting concerns a large-scale venture of American Express (the service provider) on the microblogging social media platform Twitter. This collaboration introduced a new purchasing service that was seamlessly integrated for two months into the social media platform (Twitter) to leverage users' connections to stimulate eWOM. Specifically, the service enabled consumers to make purchases on the social media platform while simultaneously spreading the word about their purchases to their social media peers (receivers).We further discuss this novel service by describing the data-generating process. In particular, the service provider first posts a short message (tweet) on the platform broadcasting the list of participating merchants and the corresponding products available for purchase. This announcement includes information about the product offerings (e.g., product, respective sale price) and the designated hashtags (i.e., a phrase or word preceded by a hash sign [#]) consumers must use to make a purchase. Consumers must have a microblogging account and sync their service provider account with their microblogging account. Once the service provider broadcasts the products available for purchase, users can purchase these products by posting a tweet that includes the designated hashtag. In addition to the necessary hashtag, consumers can choose to personalize the purchasing tweets that are (automatically) shared with their social media peers. Typically, such messages are posted on the users' social media profiles, and their followers (i.e., those subscribed to their timeline) automatically receive these messages on their own (home) newsfeeds (for an explanation of how we use the variation in message visibility in our main identification strategy, see the ""Econometric Model Identification"" subsection and Figure A1 in the Web Appendix). The social commerce provider tracks the tweets containing the designated hashtag and pairs them with the corresponding product. After the purchase confirmation, the service provider bills the users and ships the product. Empirical DataOur data set includes all the confirmed transactions generated through this purchasing process. Specifically, each transaction in our data set contains the message ID of the purchasing message, the message content, the designated product hashtag, the date and time the message was posted, the corresponding user ID, and whether the message was rendered visible or invisible to each of the user's followers (i.e., included or not included in the follower's newsfeed).For each user, our data set also contains the screen name of the user on the social media platform, when they joined the platform, the set of the user's followers and followees, all the messages users have posted on the platform since they joined, the geolocation of the user, and the self-reported description of the user's profile. Our data set also contains the same information for users who chose not to make a purchase.In addition, our data set contains information about the product offerings. The service provider collaborated with known retailers and offered various products for purchase (e.g., see Figure A2 in the Web Appendix). In particular, the products involve video game consoles and related accessories, electronics and sports equipment (e.g., high-definition tablets, sports and action cameras), general-purpose gift cards, and fashion accessories (e.g., designer bracelets, handbags). These offerings from the social commerce service provider were available for purchase at a reduced price only through the platform (about a 25% discount, yielding an average retail price of $125).Overall, our data set tracks the corresponding purchasing decisions of 132,995 social media users on Twitter with 1.4% of the users purchasing available products. The users are located across the continental United States, as illustrated in Figure 1, and on average follow 996 Twitter users and have 342 followers. Table 2 presents the summary statistics and description of the main variables and Figure 2 shows the corresponding correlations.Graph: Figure 1. Geolocations of users.Graph: Figure 2. Correlation levels among the main variables of interest.GraphTable 2. Descriptive Statistics VariableDescriptionMean/MedianSDMinMaxPurchaseWhether the recipient of the message made a purchase.014.1201Visible messageWhether the message was visible to the recipient.77.4201Geographical distanceGeographical distance between sender and recipient971.08a894.9005,585Number of followersNumber of followers of user342a101,0000376,000Number of followeesNumber of followees of user996a12,6290115,000Reciprocal relationshipWhether the relationship between the users is reciprocal.08.2701Number of interactionsNumber of interactions between users.266.1001,612Sentiment of messageIntensity of message advocacy.21.35−11Personalized messageWhether the message was personalized by the sender.82.3801 1 aWe report the median instead of the mean value.2 Notes: The values of the variables Number of followers, Number of followees, Reciprocal relationship, and Number of interactions correspond to the time of posting the WOM message.We enhanced our data set with a proprietary data set from the ad intelligence company Kantar Media, which includes the local (and national) advertising expenditures of each brand and for each product. We also supplemented our data set with additional information from the American Community Survey five-year estimates of the U.S. Census Bureau regarding local demographics. Empirical MethodologyWe model users' purchase decisions as a function of eWOM message, sender, and recipient as well as relationship characteristics, including observed and latent homophily controls; homophily creates a natural correlation in behaviors that could be incorrectly interpreted as a causal effect (e.g., [10]; [76]). To further control for any unobserved confounds, we use in our research design the variation in the visibility of eWOM messages. Econometric Model SpecificationConsistent with prior literature (e.g., [79]), we use a continuous-time single-failure survival model. In particular, we model how quickly users purchase a product, if any, using a Cox ([39]) proportional hazard model and correcting for censoring of transactions that might have been intended to occur after the observation window ([64]). Specifically, our main estimation equation for the decision of peer i (eWOM recipient) is as follows: λi(t)=λ0(t)exp(VisiblemessageijβM+GeographicaldistanceijβD+Visiblemessageij×GeographicaldistanceijβDM+XijβX+MjβM+DjβDS+RiβR), Graph( 1)where  λi(t)  is the hazard of peer (follower)  i  of consumer  j  to purchase the same product as consumer  j after[ 7] an eWOM message from  j,λ0(t)  represents the baseline hazard,  Visiblemessageij  captures whether the eWOM message of consumer j (sender) was rendered visible to (i.e., included in the home newsfeed of) peer  i  (recipient), and  Geographicaldistanceij  measures the physical distance of i from consumer j in (log) miles. The coefficient of interest is  βDM  and captures the relationship between geographical distance and the effectiveness of eWOM, while the coefficient  βD  accounts for spatially and nonspatially correlated user behaviors and preferences (e.g., homophily) that would have otherwise manifested as the association between geographical distance and the effectiveness of WOM; put simply, our research design leverages the variation in the visibility of eWOM messages, as influence can occur if and only if the eWOM message is rendered visible, whereas correlated user behaviors and preferences are extant even when the eWOM message is nonvisible. We also control for user-relationship (sender–recipient dyad) characteristics, X; message characteristics, M; disseminator characteristics, D; and recipient characteristics, R; as well as product fixed effects and geography (  statei  ) and time (day of message) fixed effects. Model features and machine-learning methodsTo construct the aforementioned user-relationship, message, and disseminator and recipient controls, we employed machine-learning techniques to leverage the vast amount of unstructured and structured data.More specifically, the user-relationship (sender–recipient dyad) characteristics,  X  , include controls for observed and latent pairwise user similarity and tie strength. This set of variables—in addition to the research design—allows us to capture the correlation in latent tastes and preferences between WOM message disseminators and recipients to better distinguish the relationship of interest. In particular, the user similarity between the disseminator and the recipient of a WOM message is measured based on ( 1) the similarity of topics discussed in social media posts using the results of a machine-learning model for natural language processing (NLP), as well as the overlap of the local communities as captured by the ( 2) Jaccard similarity coefficient of followers ([40]) and ( 3) Jaccard similarity coefficient of followees ([40]).The NLP model we employ for this task is the latent Dirichlet allocation (LDA) model ([21]); LDA is a probabilistic generative NLP model that we use to model the user-generated content of each user in our data set (a document in our corpus) as a distribution over topics and every topic as a distribution over words in the English dictionary. We build this model on the corpus of all the messages of the users in our data set as the topics users discuss online reflect their latent interests ([113]); using the complete corpus instead of only the user messages during the observation window improves the inference of the NLP model. In particular, for the implementation of the LDA model, we used 139,850,033 messages in total. We also use a part-of-speech tagger/tokenizer developed specifically for Twitter ([84]) for more accurate tokenization, the removal of stopwords, symbols, typos and uninterpretable words ([97]), and the creation of ngrams, while retaining online-specific textual features (i.e., hashtags, at-mentions, and emoticons) ([92]; [110]). We also use [58] online variational Bayes algorithm to efficiently estimate the LDA model on our corpus. In addition, instead of arbitrarily determining the value of the LDA parameter corresponding to the number of latent topics, we find the natural number of topics present in our corpus using the procedure and measure proposed by [15], evaluated in terms of the Kullback–Leibler ([66]) divergence measure; nonetheless, the findings are not sensitive to the number of topics. Finally, regarding the hyperparameters of our model, we learn an asymmetric prior directly from our data. Beyond capturing the disseminator–recipient similarity with these metrics, we alternatively measure the similarity as a single standardized factor, based on the principal factors method, to avoid any potential multicollinearity; we present both sets of results.In addition, our model specifications include constructs capturing whether the user relationship is reciprocal and (the log of) the number of interactions between the two users ([32]). Finally, we also control for the sociodemographic distance of the users—based on the difference in average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message using Census data at the zip code level ([45])—as well as the time zone difference. Overall, the various metrics of user pair similarity capture both observed similarity (e.g., the number of user interactions) and latent similarity (e.g., common latent interests) to further control for potentially unobserved confounds and homophily.The message characteristics,  M  , capture the sentiment of the message (i.e., intensity of WOM advocacy) as well as whether the message was personalized (i.e., explicit rather than implicit advocacy). The sentiment of the message (measured on a continuous scale between −1 and +1) provides a richer metric of the advocacy intensity of the sender compared with other simple metrics, such as lexicon-based scores. The main method we employed uses a publicly available commercial sentiment analysis mechanism based on deep learning ([ 9]). Nonetheless, the results are robust to employing alternative machine learning methods for sentiment analysis.[ 8] Moreover, the message controls also include whether a user account handle is mentioned in the message and whether the sender started the message with a period as the first character; starting a message with a period as the first character affects the visibility of the message and is a common norm among Twitter users when they want to explicitly make a message visible to all users and not only the account mentioned in the message.[ 9] Finally, we also control for advertising expenditures of each brand during our observation period in the local region of the eWOM message recipient expressed in (logarithm of thousands of) U.S. dollars.The disseminator and recipient characteristics,  D  and  R  , include the user (opinion) leadership and expertise measures following the extant literature. We capture these disseminator and recipient characteristics to further control for factors that could bias the true relationship between geographical distance and WOM effectiveness due to omitted individual characteristics and preferences. The user expertise levels are measured on the basis of the standardized similarity of the timeline of a user with the corresponding timelines of the participating vendor and product employing the probabilistic NLP machine-learning model we previously described ([23]; [80]). The motivation for this measure is, for instance, that users who frequently tweet about technological trends and topics similar to those in the social media accounts of the specific product and the corresponding vendor are more likely to be perceived by their social media peers as experts in the area of technological products ([61]). In addition, we further control for correlated user preferences and interests ([26]; [72]) by including in the econometric specifications the latent interests based on the aforementioned LDA model ([ 1]). The user (opinion) leadership is measured based on the additive smoothed ratio ([74]) of followers to followees of the user. The additive smoothed ratio is frequently used in empirical studies to prevent this metric from being oversensitive to small-scale changes in the numbers of followees or followers ([ 1]; [41]). Furthermore, we also control for the number of followers of each user, whether the user has a default profile on the platform, and how many months have elapsed since the user joined the social media platform. Finally, we also control for the age, gender, and income of the users based on the Census data ([45]). Econometric Model IdentificationWe next discuss the research design we use to further distinguish the effect of geographical distance on eWOM effectiveness from unobserved confounds and correlated user behaviors and preferences, such as homophily. Our research design is enabled by a unique feature of the platform that causes some WOM messages to be visible and other messages to be invisible (not included) on the newsfeeds of other peers, as described next.Typically, a message posted by a user on the Twitter platform appears in the newsfeeds of all her followers, as the newsfeeds were not algorithmically curated during this venture. Thus, in our context, whenever a user makes a purchase, her social media peers are exposed to her advocacy toward the product if the purchase is visible in their (home) newsfeeds and their purchasing decisions might, in turn, be affected through such WOM episodes; the specific offers were available for purchase only through Twitter. The research design framework we employ leverages information on whether a message broadcasted on the platform was indeed rendered visible in the newsfeeds of other users or not (see Web Appendix Figure A1).More specifically, on Twitter, a publicly broadcasted message may not be included in the newsfeeds of some followers in instances when a Twitter account handle (if any) appears in a specific position of the message (i.e., if any Twitter account user name following the ""@"" symbol appeared at the very beginning of the message). For instance, if a consumer purchases a product by posting the message ""#BuyActionCamPack @AmericanExpress"" (see Web Appendix Figure A2, Panel A), the message is visible on the newsfeeds of all her followers. However, if the same consumer purchases the same product by posting the message ""@AmericanExpress #BuyActionCamPack"" (see Web Appendix Figure A2, Panel B), then the message is visible only to the social media users who follow both this consumer and the mentioned account.Interestingly, due to the design of Twitter during our observation window, if a consumer initialized the purchase process by clicking on the announcements of the product offerings or a post of another consumer, the handle of the corresponding account (e.g., ""@AmericanExpress"") is automatically prepopulated in the purchasing message. Then, the consumer must click anywhere on the prepopulated field and manually write the purchasing message that includes the required designated hashtag. Notably, if the click happens to be recorded on the left of the prepopulated account handle, the cursor will be placed at the beginning of the field and the consumer can write the purchasing message starting at this position (see Web Appendix Figure A3, Panel A). Thus, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel A, starting immediately with her message, and the tweet will be rendered visible to all of her peers (recipients). Alternatively, if the click happens to be recorded on the right of the prepopulated account handle, the cursor will be placed at the end of the field and the consumer can write the purchasing message starting after the mentioned account handle (see Web Appendix Figure A3, Panel B). Then, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel B, starting immediately with an account handle, and the tweet will not be included in the newsfeeds of her peers (recipients) who do not follow the mentioned account.Thus, the visibility of each WOM message to the peers of a consumer depends on ( 1) whether an account is mentioned (e.g., @AmericanExpress or another account), ( 2) the exact position of the account handle (whether it is at the very beginning of the message or not), ( 3) what social media accounts the consumer's peers follow, ( 4) whether a user click will be recorded more to the left or more to the right of the screen, and ( 5) whether an account handle will be prepopulated by the platform. Thus, the visibility of the messages is affected by platform design factors (e.g., prepopulating accounts) rather than certain user characteristics exploited by a curation algorithm. Importantly, with regard to the visibility of eWOM messages, the recipient does not control the visibility of these messages from the users she follows; this is important because the purchase decisions we study are the recipients' decisions.Because our main identification strategy assumes that the visibility of eWOM messages and the geographical distance of consumers are not endogenous in this study, we also empirically investigate whether the visible and nonvisible message groups differ across the pretreatment variables in our model based on the normalized differences tests ([60]) that provide scale-invariant measures of the size of the differences. The normalized differences range from −.1768 to.1692, indicating that there are no significant differences between the two groups in observable characteristics, further enhancing the validity of the identification strategy; differences of.25 or less indicate a good balance between the two groups ([60]). Similarly, kernel distributions, quantile-quantile plots, and the orthogonality test ([60]) further confirm the validity. We also examine the geographical distance in the same way, reaching the same conclusion.Overall, these unique features of Twitter induce an important variation in the visibility of messages, enabling us to examine the behaviors of peers in treatment (i.e., visible message) and comparison (i.e., nonvisible message) groups in a potential outcomes framework. Thus, differences in purchases between treatment and control groups can be attributed to the corresponding WOM messages and their characteristics, addressing the issue of correlated user behaviors and preferences. In this respect, our research is also relevant to the stream of work that has leveraged the variation in the visibility of advertising messages to estimate the effect of online ads ([ 1]; [48]; [46]).We further enhance our identification strategy using only observations corresponding to dyadic relationships and social media peers who did not receive messages (either visible or invisible) from multiple disseminators ([ 1]; [12]). In addition to taking advantage of this nonintrusive research design, we also avoid any observer biases; the subjects are unaware of being part of the study and, thus, do not alter their behavior in anticipation of the study. Nevertheless, we also control for differences in the pairwise relationships between users by employing an extensive set of variables and fixed effects in our data-rich setting. Table A1 in the Web Appendix presents additional identification strategies. Empirical Results Main ResultsTable 3 presents the results of the main econometric specifications of the eWOM effectiveness model. In particular, Model 1 constitutes our baseline specification as it models eWOM effectiveness based on the constructs of dyadic similarity and relationship strength between the disseminator and recipient of the eWOM message (i.e., pairwise user similarity, reciprocity of users' relationship, and number of user interactions), eWOM message advocacy (i.e., personalized message and sentiment of message), and user characteristics (i.e., expertise and leadership). Then, Model 2 introduces the notion of geographical distance, and Model 3 adds the information of eWOM message visibility and leverages the corresponding variation to further distinguish the relationship between geographical distance and eWOM effectiveness  (βDM)  from correlated behaviors and homophily among users. That is, Model 3 disentangles the relationship between geographical distance and eWOM effectiveness from the correlational effect of geographical distance reported by Model 2; the eWOM influence is transmitted through visible messages only, whereas correlation in user behaviors and preferences (i.e., homophily) is present even with invisible messages. Then, Model 4 further controls for whether a user account was mentioned in the eWOM message and if the message was explicitly made visible while introducing additional disseminator and recipient controls (i.e., number of followers, leadership, default profile, time on the platform, interests, age, gender, and income) and other controls (i.e., the difference in average age and percentage of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message, the log of brand advertising expenditures in USD (in 1,000s) in the location of the recipient of the message, and the time zone difference between the locations of the disseminator and recipient of the message). Finally, Model 5 controls for the specific product mentioned in the eWOM message as well as state and day fixed effects.GraphTable 3. Estimation Results of eWOM Effectiveness Model. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.239***1.257***1.264***(.009)(.009)(.009)(.010)(.011)Reciprocal relationship5.868***5.378***5.403***4.829***4.527***(.317)(.290)(.291)(.269)(.254)Number of interactions1.015.979.977.987.980(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.557***1.728***1.556***(.107)(.112)(.111)(.133)(.123)Personalized message1.041***1.046***1.047***1.053***1.041***(.003)(.003)(.004)(.005)(.007)User expertise1.204***1.183***1.184***1.595***1.191**(.030)(.029)(.029)(.130)(.100)User leadership1.006***1.006***1.005***1.011***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.929***.946***.954**(.007)(.017)(.020)(.020)Visible message1.451***1.598***1.608***(.171)(.193)(.188)Visible message × Geographical distance.951**.954**.945***(.019)(.020)(.020)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,307.4−22,035.2−21,763.6χ22,763.5−2,983.42,994.23,538.64,081.8N132,955132,955132,955132,955132,955 3 *p < .1.4 **p < .05.5 ***p < .01.6 Notes: eWOM effectiveness analysis. The hazard ratios (HRs) represent the percent increase (HR > 1) or decrease (HR <1) in postpurchase hazard associated with each attribute.According to the results presented in Table 3, we find that the coefficient of the variable capturing the relationship between geographical distance and WOM effectiveness is negative and statistically significant (Visible message × Geographical distance:.945, p < .01, Model 5); all reported coefficients correspond to hazard ratios (HRs) representing the increase (HR > 1) or decrease (HR < 1) in purchase likelihood associated with each attribute. (Table A2 in the Web Appendix also shows the coefficients of the control variables.) Beyond the effect of interest, we also find a negative and statistically significant spatial homophily–based effect of geographical distance (Geographical distance:.954) as well as a positive and statistically significant effect of similarity (homophily). Interestingly, these findings show that despite the ""death of distance"" postulated in the literature ([28]), geographical distance is negatively associated with the effectiveness of eWOM. Thus, our research is the first to establish that geographical distance has a negative relationship with eWOM outcomes even among familiar social media peers, in addition to the previously known homophily-based effect of geographical distance.Moreover, the coefficients of all the other variables are in accordance with what one would expect as well as the extant literature on WOM (e.g., [13]; [20]; [45]). Specifically, we find that the increased user similarity and strength of relationship between users (User similarity: 1.264; Reciprocal relationship: 4.527) as well as more intense WOM advocacy (Sentiment of message: 1.556; Personalized message: 1.041) are associated with higher levels of purchase likelihood after exposure to WOM (Visible message: 1.608). Similarly, users with higher product expertise (User expertise: 1.191) seem more persuasive; thus, their followers are associated with a higher purchase likelihood after being exposed to their advocacy. Economic Significance and Managerial RelevanceThe relationship of interest is also of economic significance, as a decrease of 10 miles in the distance between users accentuates the effectiveness of eWOM by 12.78% based on the aforementioned coefficients; similarly, an increase of 100 miles in the distance corresponds to a decrease of 25.56%, and an increase of 1,000 miles corresponds to a decrease of 38.34%. For instance, for a recipient living in New York City, the relationship is reduced by 24.39% when the eWOM message originates from a sender in Philadelphia, and 38.82% from Miami, compared with the same message from a sender in New York City.We further assess the economic significance of the findings by measuring the out-of-sample performance of the models. Specifically, we use a holdout evaluation scheme with an 80/20 random split of data and evaluate the models in terms of Harrell's C concordance coefficient, which measures the likelihood of correctly ordering survival times for pairs of senders and recipients of eWOM messages; the concordance measure is similar to the Mann–Whitney–Wilcoxon test statistic as well as the area under the receiver operating characteristic curve. The results show that our model achieves a predictive performance of.840. Thus, it outperforms the baseline by a large margin, as the baseline performance corresponds to a value of.5. This statistically significant difference further illustrates the managerial relevance of the findings, as they can enhance seeding and targeting strategies ([57]; [99]).We further quantify the (dollar) value of this increase in out-of-sample performance ([89]). To conduct this calculation ([89]), we use estimates of the cost of targeting (e.g., promoting eWOM messages) and the average product price (Goldfarb and Tucker 2011); the cost of this type of targeting on Twitter is estimated to be $1.35 based on [111], while the average product price in our data set is $125. Combining these data reveals that our model suggests a profit of $.85 per targeted user, which corresponds to a 9% increase over the baseline of not using the information of geographical distance (i.e., $.78), while for random targeting the corresponding profit is only $.008. Potential MechanismOur findings are surprising, as in such an empirical setting the products are not location-specific, there are no transportation fees for consumers, and there is no contracting or potential conflict or ambiguity between senders and recipients of WOM messages ([59]; [79]). Thus, to fully understand our findings, we delve into a likely underlying mechanism of the identified effect and conduct additional analyses that allow us to assess the likelihood of this potential mechanism.We hypothesize that the negative relationship between geographical distance and eWOM effectiveness could be due to the identification processes of social media users. Specifically, a user who resides near the sender of the message is likely to share a common social identity with the sender based on their geographic proximity ([44]; [62]; [83]; [107]) and thus might be more susceptible to WOM influence originating from this (local) sender.[10] Conversely, a recipient who resides farther away from the disseminator of the WOM message is not likely to share the same location-based social identity and thus is less likely to be persuaded ([37]; [38]; [96]) by mere exposure to eWOM advocacy. Location-based social identity activationTo empirically assess this potential underlying mechanism, we first examine the moderating effect of the salience of the geographical distance from the source of WOM. Salience is activating common social identity identification ([44]; [55]; [56]; [68]; [77]; [106]), and thus, we empirically test the likely underlying mechanism by examining the moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM. If the relationship is accentuated when the geographic proximity of the source of the WOM message is more salient, this would provide empirical support for the hypothesized mechanism of common social identity, as the salience of the location—and thus the salience of the geographic proximity—enhances the social identification processes of the recipient ([44]; [55]; [56]; [68]; [77]; [106]). This would also provide additional empirical evidence in favor of the main identification strategy. Alternatively, if more salient location cues attenuate the relationship, this would provide evidence against the hypothesized mechanism.According to the results presented in Table 4, we find a negative and significant moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM; the salience of location variable corresponds to whether the location of the WOM sender is explicitly mentioned in her profile. That is, the relationship between geographic distance and eWOM effectiveness is even more negative when the distance is more salient. This finding indicates that common social identity is a likely mechanism for the identified relationship. The results are robust to alternative econometric specifications.[11]GraphTable 4. Estimation Results of eWOM Effectiveness Model with the Moderating Effect of the Salience of Geographical Distance. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.238***1.256***1.260***(.009)(.009)(.009)(.010)(.010)Reciprocal relationship5.868***5.378***5.525***4.881***4.606***(.317)(.290)(.298)(.272)(.258)Number of interactions1.015.979.976.989.969(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.642***1.781***1.568***(.107)(.112)(.118)(.138)(.124)Personalized message1.041***1.046***1.043***1.050***1.035***(.003)(.003)(.004)(.005)(.008)User expertise1.204***1.183***1.156***1.581***1.172*(.030)(.029)(.028)(.129)(.099)User leadership1.006***1.006***1.004***1.010***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.947***.961*.965*(.007)(.019)(.021)(.021)Visible message1.430***1.588***1.783***(.168)(.192)(.214)Visible message × Geographical distance.949**.950**.918***(.021)(.021)(.020)Visible message × Geographical distance × Salience of location.920***.948***.974*(.011)(.012)(.013)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,280.2−22,024.7−21,751.1χ22,763.52,983.43,048.53,559.64,106.8N132,955132,955132,955132,955132,955 7 *p < .1.8 **p < .05.9 ***p < .01.10 Notes: eWOM effectiveness analysis with the moderating effect of salience of geographical distance. The salience of location variable corresponds to whether the location of the disseminator is explicitly mentioned in the profile of the disseminator. Additional table notes as in Table 3. Location-based social identity prominenceWe also examine the likelihood of the hypothesized mechanism in additional ways. For instance, we examine the effectiveness of eWOM under conditions that strengthen the role of geographical location in the social identification process. Specifically, increased political homogeneity in the local area of the recipient of the eWOM message is likely to enhance the importance of the location-based social identity of the recipient as it increases the salience and significance of individuals' social identity due to political entities operating at geographic levels (e.g., precinct, county, state) and the characteristics of the local information environment (e.g., increased number of times an individual is reminded of the local identity, positive perceptions of the local community) ([62]; [91]; [101]). As a result, a pronounced location-based social identity of the recipient is likely to engender biases based on geographical distance, accentuating the relationship. Thus, if the relationship is accentuated when the political homogeneity in the local area of the recipient of the WOM increases, this would provide empirical support for the potential mechanism of social identification. Conversely, the opposite would provide empirical support against this potential mechanism.Based on the results in Table A3 in the Web Appendix, we find a negative and significant moderating role of the political homogeneity in the local area of the recipient; we have collected data from the MIT Election Lab (https://electionlab.mit.edu) on political voting patterns at the precinct level for the 2016 elections and measure political homogeneity on the basis of the percentage of voters that would need to switch from the majority party to the minority party for the two parties to have equal votes. Put simply, the negative relationship between geographical distance and eWOM effectiveness is amplified when location-based social identity might be more pronounced due to increased political homogeneity. This finding lends support to the hypothesized mechanism of social identity. The results are also robust to alternative specifications.Similarly, we also examine the moderating role of exogenous hardships in the local area of the recipient of the WOM message. If there have been significant local community hardships or natural disasters, then geography-based common social identity is likely to be more prominent for the residents of the affected area ([109]). Thus, if the relationship is accentuated when the geographic proximity of the source of the WOM message is combined with local community hardships for the recipient, this would provide additional support for the potential mechanism of common identity; we measure local community hardships using (exogenous) deaths related to extreme weather events in the location of the recipient of the message during the last five years prior to our observation window based on data from the National Oceanic and Atmospheric Administration. According to the results in Web Appendix Table A4, we find that the relationship between geographical distance and the effectiveness of eWOM is even more negative for users for whom location-based social identity is likely more pronounced due to local community hardships, lending empirical support to the hypothesized potential mechanism.[12] Location-based social identity versus other peer effectsWe also examine whether the estimated moderating role of geographical distance is above and beyond other potential peer effects. Table A5 in the Web Appendix presents the corresponding results controlling for both the interaction between user similarity and visible message and the interaction between sociodemographic distance and visible message. Our findings remain highly robust, further illustrating that the estimated moderating effect of geographical distance is above and beyond other peer effects, including actual and perceived homophily. The results are also robust to alternative specifications, such as including additional interactions. Ruling Out Additional Alternative ExplanationsIn addition to the aforementioned evidence and identification strategies, we assess various alternative explanations. Table 5 presents an overview of these, with the main ones discussed next and additional ones discussed in Web Appendix B.GraphTable 5. Alternative Explanations Alternative ExplanationRationaleIdentification StrategyTable(s)Location-RelatedLocation characteristicsLocation-based characteristics may affect purchase likelihoodsMain identification strategy including geography fixed effectsAllSpatially correlated user preferencesUser interests and brand preferences may be spatially correlatedMain identification strategy including latent user interestsAllAdditional brand preference controlsA7Local time-difference effectsTime zone differences are correlated with geographical distance and may relate to differences in users' activities or moodsMain identification strategy including time zone differencesAllLocal weather conditionsLocal weather conditions may affect consumers' activities and moodsAdditional weather controlsA9Small-city effectsGeographic distances are shorter in smaller and more remote locations, where the demand for products sold online might be higher due to potentially limited availability of other productsMain identification strategy including geography fixed effectsAllExclusion of small and remote locationsA8Local marketing effectsLocal marketing effects may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6User-RelatedHomophilyCorrelated behaviors among similar (across observed characteristics) peersMain identification strategy including user similarity controlsAllAdditional brand preferences similarityA7Additional user similarity controlsA11Propensity-score matchingA13Latent or unobserved homophilyCorrelated behaviors among similar (across latent or unobserved characteristics) peersMain identification strategy including user latent similarity controlsAllAdditional latent homophily controlsA10Additional user similarity controlsA11Latent variable modelA14Propensity-score matchingA13User characteristicsUser characteristics may affect purchase likelihoodsMain identification strategy including user controlsAllLocal demographic effectsSociodemographic distance between users may affect purchase likelihoodsMain identification strategy including sociodemographic controlsAllIncome-level effectsIncome levels may affect where users select to live; as such, geographical distance may be correlated with income levelsMain identification strategy including income controlsAllMessage-RelatedMessage contentMessage content characteristics may affect purchase likelihoodsMain identification strategy including message controlsAllPropensity-score matchingA13Nonrandom message visibilityMessage visibility may not be random, despite provided evidenceStatistical tests—Main identification strategy including message visibility controlsAllPropensity-score matchingA13Covariate adjustmentA12Product-RelatedProduct characteristicsProduct characteristics may affect purchase likelihoodsMain identification strategy including product fixed effectsAllMarketing promotionsAdvertising or other marketing activities may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6Other-RelatedUnobserved effectsUnobserved effects correlated with geographical distance of visible eWOM messagesPropensity-score matchingA13Other unobserved time-varying effectsAny other unobserved effects that vary with time and are correlated with geographical distance of visible eWOM messagesLimited time-horizon—Main identification strategy including time fixed effectsAllGeographic distribution of tiesUser might have more geographically proximate than geographically distant tiesMain identification strategyAllModel idiosyncrasiesModel and model specification choices could potentially affect the resultsLogistic regressionA15Alternative specificationsAllSpurious effectsSpurious effects or other statistical artifacts""Placebo"" studiesA16  Local marketing promotion effectsOne may be concerned that the results might be driven by unpaid or organic marketing effects in the local region of the eWOM message recipient. To evaluate this, we supplement our data set with local web search trends for each product from Google Trends ([14]). Table A6 in the Web Appendix presents the corresponding results controlling for both local marketing expenditures and ad response (via search behaviors) of the local audience. The results remain robust, alleviating concerns that local marketing promotion activities drive the results. The results are also robust to alternative specifications, such as using national Google Trends and advertising expenditures or estimating separate models for each potential confound. Local user-preferences effectsWe also examine the robustness of the findings to alternative specifications, such as controlling for homophily based on the overlap in brands that each social media user follows on the platform. Table A7 in the Web Appendix presents the corresponding results. The results remain robust, further corroborating our findings. Small-city effectsAnother potential alternative explanation is that the results are driven by disseminator–recipient pairs located in small and remote locations as in such locations distances are in general shorter and demand for products sold online is higher due to the limited availability of other product alternatives ([21]; [79]). We assess this alternative mechanism by repeating the analysis excluding any observations that correspond to small and remote locations, as determined by the Census (i.e., locale assignments). Table A8 in the Web Appendix presents the results; the results remain robust. Local weather conditions effectsWe also assess the alternative explanation that the results are driven by the local weather conditions affecting the moods and activities of users ([46]; [67]). We assess this potential explanation by controlling for the temperature, precipitation, and sunshine levels in the location of the recipient using data from the National Oceanic and Atmospheric Administration. Web Appendix Table A9 presents the corresponding results; the results remain robust. Robustness Checks and Alternative Identification StrategiesWe also undertake an extensive set of tests to assess the robustness of the results and further strengthen the findings, as discussed next; see Table 5 and Web Appendix B for additional details. Extended econometric specificationsFirst, to enhance the employed identification strategy and examine the robustness of the findings, we further control for latent user characteristics by tapping into the social network structure and recent deep-learning advances. Specifically, we use the method of DeepWalk, a deep-learning method for graphs ([88]), to learn the latent representations of the users and their similarity and further account for both network structure roles and latent user homophily. Table A10 in the Web Appendix presents the corresponding results. The results corroborate our findings. The results also remain robust to employing alternative deep-learning methods, such as the node2vec method ([53]).We also repeat the analysis including multiple user similarity measures. In particular, the similarity measures correspond to the similarity levels between disseminators and recipients based on ( 1) the Jaccard coefficient of their followers, ( 2) the Jaccard coefficient of their followees, ( 3) the topics discussed in social media posts using the results of the LDA model, ( 4) the intrinsic brand and product preferences of the users based on the overlap in brands that each social media user follows on the platform, ( 5) the demographic information at the corresponding geographic locations (i.e., average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents based on Census data), and ( 6) the latent characteristics of the users based on the deep-learning methods for representation learning; in addition to ( 7) the reciprocity of the relationship and ( 8) the number of interactions between the users. Table A11 in the Web Appendix presents the corresponding results; the results remain robust. Alternative identification strategiesWe also examine additional alternative identification strategies to control for any potentially remaining differences between the visible and nonvisible messages; Table A1 in the Web Appendix presents a summary of the different identification strategies. First, we enhance our identification strategy following the covariate adjustment method of [60]. Table A12 in the Web Appendix presents the corresponding results. The results remain robust; the results are also robust to including additional covariate interactions.Moreover, as an alternative identification strategy, we combine propensity-score matching with the main research design. In particular, we model the propensity of each message to be rendered visible using all the variables that describe the users' relationship and the message characteristics as well as the geographical distance between the users.[13] We conduct the matching based on the propensity scores before estimating again the same econometric models (for additional details, refer to the corresponding table notes). For this robustness check, we use one-to-one matching with replacement and a caliper of.05, yielding a standardized mean (median) absolute difference of.009 (.007) across all the variables, which ensures that covariate balance has been successfully achieved ([18]); the density distributions of the propensity scores also indicate significant overlap and common support. As shown in Table A13 in the Web Appendix, the results remain robust. The results are also robust to nearest-neighbor matching with the generalized Mahalanobis distance.Furthermore, as an additional alternative identification strategy, we build latent variable models where the sender–recipient similarity is latent and measured based on the various similarity features. Web Appendix Table A14 shows the corresponding results. Model 1 corresponds to the aforementioned latent variable model, while Model 2 combines the latent variable model with propensity-score matching estimating the model over the matched sample. The results of all the aforementioned alternative models are highly consistent and further corroborate our findings.Finally, as an alternative strategy, to estimate the relationship between geographical distance and eWOM effectiveness, we also use a logit model ([105]) examining whether—rather than how quickly—a user purchases a product. As Web Appendix Table A15 shows, the results remain robust. Falsification testsWe supplement these robustness checks with falsification tests to further assess whether the previous models are picking up spurious effects. As shown in Web Appendix Table A16, the results indicate our findings are not a statistical artifact of the specifications.Overall, the findings remain highly robust to various alternative identification strategies, econometric specifications, robustness checks, and falsification tests. Figure 3 illustrates the corresponding estimated effects across the specifications.Graph: Figure 3. Hazard ratios (HRs) with 95% confidence intervals (whiskers) representing the percentage increase (HR > 1) or decrease (HR < 1) in postpurchase hazard across estimated models. Discussion and ImplicationsIn this study, we investigate the relationship between geographical distance and the effectiveness of eWOM. Specifically, we examine whether the geographical distance between familiar disseminators and receivers of eWOM messages plays an important role—beyond utilitarian reasons and proxying for consumer tastes—in driving recipients' subsequent purchase behaviors. Our results show that the relationship between eWOM and the likelihood that message recipients subsequently also make product purchases significantly strengthens as the spatial proximity between disseminators and receivers grows. Implications for TheoryOur findings help advance understanding of conditions that affect online WOM performance. Many of the characteristics previously shown to impact eWOM outcomes relate to the product, brand, or message ([73]; [85]). We contribute by illustrating the role of the important but often overlooked construct of geographical distance in eWOM effectiveness. In showing how geographical distance is still associated with the effectiveness of online WOM in the absence of geography-specific transaction costs between unambiguous users, we demonstrate how the social force field of geography can tether the potential of eWOM. That is, despite the promise of technology to reduce communication barriers and the proclaimed ""death of distance"" ([29]; [52]), we find that geographic constraints persist online in unexpected ways. Therefore, our results also help address the debate on whether and how geographical distance still matters online ([51]) by showing that it can shape the influence of eWOM.We also contribute to the theory of eWOM examining why geographical distance is associated with eWOM effectiveness. Specifically, we find evidence that social identification may explain why the influence of online WOM is negatively related to the distance between WOM message disseminators and receivers. That is, our results suggest consumers are susceptible to online information and cues related to social identification as they can, in turn, enhance message persuasiveness. Thus, information and cues relating to social identity can be agents of eWOM influence. Whereas much of the literature on the role of geographic distance in e-commerce and other online settings offers economically driven explanations for the impact of geography, our study proposes behavioral bias relating to social identification may be an underlying mechanism that drives the relationship between geographic distance and eWOM. This finding highlights the need for future research to study additional non–economically driven explanations that can induce such biases. Implications for PracticeOur findings have important implications for managers as well. For instance, a controversial argument in the industry is that solely characteristics of the disseminators catalyze the adoption of behaviors and products and thus much of marketing efforts to engineer WOM in social media focus on identifying such characteristics. However, our findings indicate that marketers should expand their focus to take into account the disseminator–recipient pairings and understand that factors pertaining to these pairs can be significantly related to the effectiveness of eWOM. In particular, our results suggest geographical distance matters in online WOM and,, thus marketers can readily take advantage of how geographical distance is associated with eWOM persuasion. Marketers may thus adopt data-driven strategies to selectively promote eWOM episodes according to the proximity of such episodes to each consumer, or to strategically engineer such episodes based on geolocation information. Interestingly, although research has begun to identify pairwise characteristics between senders and receivers that shape the influence of eWOM, such as tie strength and similarity across personality traits ([ 1]; [20]), many of these factors are not readily observable to managers who wish to capitalize on them. The distance between social media users, though, is more easily observable to managers.Beyond promoting or engineering geographically proximate eWOM episodes, marketers may also benefit from promoting and/or engineering episodes containing other social identity cues. The likely connection between eWOM outcomes and social identity suggests that firms may also consider other cues relevant to social identity formation to further boost the success of interpersonal communications and WOM messages, as enhancing social identification may significantly increase message persuasion and user engagement in the online world.Our findings also have important implications for the effective design of viral marketing campaigns and ad content. Specifically, brands may boost the persuasiveness of their marketing campaigns by infusing into their content local cues or other identification triggers to induce consumers' social identification processes. Relatedly, marketers are beginning to leverage users' connections on social networks to develop and deliver marketing communications as part of their social advertising efforts. Our research suggests that they could further improve the effectiveness of these strategies by selecting geographically proximate connections to their targets.Furthermore, going beyond advertising strategies, the implications of our work also provide actionable guidelines for optimizing the delivery of digital content. In particular, our findings can help platforms increase the effectiveness of their content curation and ranking algorithms by incorporating information on content location or source origin and by factoring geolocation into their determination of which user-generated content to disseminate. For instance, content generated by spatially proximate consumers may draw more attention due to identification processes and thereby increase the effectiveness of content provision. In a similar vein, social media platforms may also consider incorporating location information in other functions. For instance, social media platforms may incorporate such information into various other machine-learning algorithms, such as their whom-to-follow recommendations. In addition, our findings could also be used by marketers and platforms to better predict the diffusion of information, products, and user behaviors in social media ([ 5]).Lastly, deepening our understanding of the factors that can attenuate or accentuate the effectiveness of eWOM has important implications that extend to public policies. For instance, revealing how geographic proximity is positively associated with eWOM effectiveness is critical for the development of effective public policies to induce positive behavioral changes, such as voter turnout, civic engagement, and public health actions. Limitations and Future ResearchWhile our work makes important strides in understanding how geographic proximity is related to eWOM effectiveness, we acknowledge certain limitations, which mostly stem from data availability issues. For instance, we examine the relationship between geographical distance and eWOM in a single social media platform because the service provider launched this venture on only one platform. Future research could examine whether the observed relationship manifests differently on other platforms. Moreover, we did not manipulate the visibility of the messages on Twitter because the venture did not alter the functionality of the platform in any way; future research could consider directly manipulating the visibility of the messages. Similarly, we did not manipulate the geographical distance of users from their followers. In addition, while we capture actual purchases in our data, we do not capture other consumer behaviors that could indicate interest in the products, such as online searches, as this type of information was not available to us. It would be interesting for future research to further examine such potential effects. Future research could also further examine and validate the underlying mechanisms. While we provide evidence that social identification may account for the relationship, future work may conduct experiments to verify this. Lastly, we do not observe in our data private communications between individuals due to privacy reasons and ethical concerns. Nevertheless, we hope these limitations provide avenues for future research that can deepen understanding of the critical role geographic proximity plays in eWOM and other online settings. "
26,"Leapfrogging, Cannibalization, and Survival During Disruptive Technological Change: The Critical Role of Rate of Disengagement When faced with new technologies, the incumbents' dilemma is whether to embrace the new technology, stick with their old technology, or invest in both. The entrants' dilemma is whether to target a niche and avoid incumbent reaction or target the mass market and incur the incumbent's wrath. The solution is knowing to what extent the new technology cannibalizes the old one or whether both technologies may exist in tandem. The authors develop a generalized model of the diffusion of successive technologies, which allows for the rate of disengagement from the old technology to differ from the rate of adoption of the new. A low rate of disengagement indicates people hold both technologies (coexistence), whereas a high rate of disengagement indicates they let go of the old technology in favor of the new (cannibalization). The authors test the validity of the model using a simulation of individual-level data. They apply the model to 660 technology pairs and triplets–country combinations from 108 countries spanning 70 years. Data include both penetration and sales plus important case studies. The model helps managers estimate evolving proportions of segments that play different roles in the competition between technologies and predict technological leapfrogging, cannibalization, and coexistence.Keywords: cannibalization; disengagement; disruption; leapfrogging; new technologies; switchingIn July 2020, Tesla became the world's most valuable automaker, surpassing Toyota in market value for the first time ([30]). But it was Toyota that in 1997 released the Prius, the world's first mass-produced hybrid electric vehicle. In 2006, Tesla Motors, an upstart entrant, bet that the future of the automotive industry would be fully electric cars. They announced they would produce luxury electric sports cars that could go more than 200 miles on a single charge. Incumbents dismissed the effort as futile because of the high entry barriers for auto production, the high cost of producing in California, and the challenges of establishing charging stations. But Martin Eberhard, Tesla's cofounder, noted in a blog in 2006, ""a world of 100% hybrids is still 100% addicted to oil...Tesla Motors will remain focused on building the best electric cars for the foreseeable future. With each passing year, our driving range will get longer and the argument for plug-in hybrids will get weaker. To hell with gasoline"" ([ 8]).In contrast, Toyota bet that hybrids would be the future. ""The current capabilities of electric vehicles do not meet society's needs, whether it may be the distance the cars can run, or the costs, or how it takes a long time to charge,"" said Takeshi Uchiyamada, Toyota's vice chairman, who had spearheaded the Prius hybrid in the 1990s ([18]). Toyota faced a hard choice: invest in hybrids, all-electrics, or both?Globally, during times of potentially disruptive technological change, both industry incumbents and new entrants face difficult choices. For incumbents, the critical dilemma is whether to cannibalize their own successful offerings and introduce the new (successive) technology, survive with their old offerings, or invest in both. To address this dilemma, they need to know whether disruption is inevitable; that is, will the old technology sales decline due to the growth of the new technology and, if so, how much of their existing sales will be cannibalized over time? Or can both old and new technologies, in fact, coexist in tandem? The entrant's dilemma is whether to target a niche and avoid incumbent reaction or target the mass market and incur the wrath of the incumbent ([38]). To address these dilemmas, both incumbents and new entrants need to know how segments of consumers respond to the successive technology. Examples of technological change abound: electric cars versus hybrid cars versus gasoline cars, OLED TVs versus LCD TVs, streaming versus cable, music downloads versus CDs, laptops versus tablets, and app-enabled ridesharing versus taxicabs. Several incumbent firms have also stumbled or failed during disruptive change: Toyota, GM, HP, Nikon, Canon, Kodak, Sony, Nokia, Yellow Cabs, Comcast, and Sears.Our central thesis in this article is that to effectively manage disruption, we must answer the following substantive research questions: First, when does an old technology coexist with a new, successive technology versus going into an immediate decline? If coexistence occurs, how can one account for the coexistence of two technologies in an empirical model? Second, how can one estimate the extent of cannibalization and leapfrogging of an old technology by a new technology over time? Third, can consumer segments explain coexistence, cannibalization, and leapfrogging in successive technologies, and if so, which segments?These questions represent pressing concerns for senior managers ([21]). To address these questions, we first outline the theory of disruption, discuss research gaps, and define important constructs that are central to the new model and typology. Then, we develop a generalized model of the diffusion of successive technologies. A key feature of the generalized model is the rate of disengagement from the old technology, which is not forced to equal the rate of adoption of the successive technology, allowing both technologies to coexist. Next, we estimate four latent adopter segments from aggregate data, which correlate with the growth of the new technology, the cannibalization of the old, and/or the coexistence of both: leapfroggers, switchers, opportunists, and dual users (defined shortly).We apply our model to three different types of aggregate data to ascertain model fit: ( 1) penetration of seven successive technology pairs across 105 countries (441 technology pair–country combinations) spanning multiple years, ( 2) sales of three contemporaneous technology pairs across 40 countries (92 technology pair–country combinations), and ( 3) case analyses of real disruption of large incumbents in the United States. The major benefit of using aggregate penetration and/or sales data is that such data are available abundantly compared to individual-level data. Indeed, much research uses this type of aggregate data to generate rich insights on adoption, diffusion, and generational competition (see [ 4]; [ 5]; [17]; [37]). In addition, we present a test validating the model using a simulation analysis on individual consumer-level data.Our model and analysis provide both substantive and modeling innovations. Our research provides a better strategic understanding of how, in many situations, old technologies may not necessarily die but survive when new, successive technologies are introduced. The major contributions and implications are the following: First, disruption, though frequent, is not inevitable even when the successive technology grows rapidly, as old technologies can coexist as partial substitutes of the new. Second, the generalized model of diffusion of successive technologies helps strategists and marketers account for this coexistence by allowing the rate of disengagement from the old technology to differ from the rate of adoption of the new. Third, the separately estimated rate of disengagement enables a superior fit to data on technological succession. Fourth, the model helps estimate cannibalization by new, successive technologies, as well as sizes of four critical segments, providing key signals about disruption. The coexistence of both technologies occurs when there is a large segment of dual users. In contrast, the size of the leapfroggers segment correlates with the growth of the new technology, and the size of switchers and opportunists correlates with cannibalization of the old technology. Fifth, the profit implications of leapfrogging and cannibalization may vary greatly depending on which firms market which technology. Major incumbents may fail during the takeoff of new technologies due to underestimating the size of leapfroggers (opportunity cost) and switchers (real cost). Sixth, the generalized model can capture variations in segment sizes across technologies and global markets. The next sections present the theory, new typology, model, empirical applications, and strategic implications. TheoryThe theory of disruptive change ([ 2]; [ 6]) suggests that a new technology enters a market, improves in performance, and then surpasses the performance of the existing technology. During times of such technological change, leading incumbent firms fail, not because they were technologically incapable of producing and marketing these innovations themselves, but because they focus on their existing (mainstream) customers, who were satisfied with the existing technology because it met their needs on the primary dimension of performance ([ 6]).Christensen and his coauthors suggest that the new technology enters, survives, and grows because it offers benefits on a secondary dimension of performance that appeals to niche segment consumers. Over time, the new technology improves in performance and at some point meets the standards of the mainstream segment on the primary dimension of performance. These customers then switch to the new technology. Disruption occurs if the incumbent focuses on the old technology to the exclusion of the new one.Several authors have criticized the theory of disruption because of circular definitions, lack of large empirical evidence or a predictive model, and a failure to examine whether consumer behavior changes (e.g., [25]; [35]; [36]; [34]). However, no study has refuted the essential features of the theory of disruption: that successive technologies do compete, the competing technologies appeal to different segments, the new technology grows in performance over time, and the niche it serves grows in response to this improvement.A major limitation of prior work on disruption is that it does not provide recommendations on some critical issues that concern both incumbents and new entrants: How can they estimate the extent of cannibalization over time and who are the customers most susceptible to the new technology? Could the two technologies coexist, and which segments drive the coexistence of both technologies and the growth of the new technology? This research seeks to address these issues. Definitions and a Typology of New Adopter Segments for Successive TechnologiesTo answer the previous questions using the theory of disruption, we define the concepts of successive technology, substitution, and segments. Successive technologyA new successive technology (which can include both a technology and a product) addresses similar underlying consumer needs as the old technology (e.g., DVR vs. VCR) or may tap simultaneously into multiple needs (e.g., PC, laptop, tablet). Successive technologies do not include new generations of the same product. Note that in this article, we use the term ""successive technology"" synonymously with ""new technology"" and the term ""old technology"" synonymously with ""prior technology,"" given the context of technological succession. ""Cannibalization"" is the extent to which the successive technology ""eats"" into real or potential sales (or penetration) of the old technology due to substitution. Rate of disengagement (F 12)Much research in marketing (e.g., [ 7]; [13]; [24]; [26]; Table 1) addresses the related issue of the diffusion of perfectly substitutable successive generations of the same technology (e.g., iPhone 8 vs. iPhone 7), in which the consumer always prefers the new generation to the old at the same price (e.g., iPhone 9 and 10). Thus, successive generations of the same technology exhibit perfect substitution. Here, consumers completely disengage from the old generation (of the same product) when they adopt the new one.GraphTable 1. A Comparison with Related Literature on Generational Substitution. ArticleKey QuestionDataPartial Disengagement?Leapfrogging Considered?This articleTo examine the diffusion of successive technologies while accounting for coexistence, cannibalization, and leapfrogging.Multicountry penetration and sales data across several countries for technology pairs and triplets; case studies; simulationYesYes (four adopter segments considered)Koh et al. (2019)To quantify generational substitution, unbundling, and piracy effects.Downloadable music; CDs; streamingNoNoGuo and Chen (2018)How consumers strategic behavior affects sales and profits for multigeneration products.Numerical optimizationNoYesShi et al. (2014)To incorporate consumers' forward-looking behavior in multigenerational models.Eight products across four firmsNoYesLam and Shankar (2014)What drives mobile device brand loyalty?Survey data on attitudes toward mobile phone brands spanning two generations: 2.5 G versus 3GNoNoJiang and Jain (2012)To develop an extension of the Norton–Bass model to separate adopters who substitute an old product generation with a new generation into those who adopted the earlier generation and those who did not.Two generations of one category in one country; Three generations of one category in one countryNoYesStremersch et al. (2010)To test whether growth acceleration occurs across multiple product generations.39 technology generations in 12 product marketsNoAssumes no leapfroggingGoldenberg and Oreg (2007)To redefine the laggards concept and link it to the leapfrogging effect.54 products (not specifically successive generations)N/AYesDanaher et al. (2001)To incorporate marketing mix variables in the diffusion of multigeneration products.Two generations of one category in one countryNoYesKim et al. (2000)To develop a model able to incorporate both interproduct category and technological substitution effects simultaneously.One technology market in two countriesNoNoJun and Park (1999)To propose a model that incorporates diffusion and choice effects to capture diffusion and substitution for multiple generations of products.Successive generations of two technology categories, not multicountryNoNot specificallyMahajan and Muller (1996)To develop a model that accounts for diffusion and substitution for successive generations of technological innovations.Successive generations of one technology categoryNoYesNorton and Bass (1987)To develop a model that accounts for both diffusion and substitution for successive generations of high-tech products.Successive generations of one technology categoryNoN/A Technological competition is more complex than intergenerational competition because successive technologies may be only partial substitutes. That is, whereas some consumers prefer the successive technology over the old technology (e.g., teens), other consumers may find value in and prefer to hold both (e.g., homeowners who have PCs, laptops, and tablets or keep both mobile phones and landlines). For example, while the two technologies may differ in terms of the scientific principle, the old technology may still serve a need that the successive technology cannot fulfill. In such a case, a group of adopters could choose to hold both technologies, triggering the need for a model that does not force complete substitution. In this case, consumers do not fully disengage from the old technology and may co-own successive technologies.For example, consider Figure 1a, which plots the penetration of VCRs and the successive technology of DVD players. Here we observe a fast adoption of DVD players, but over this same period, the decline in VCRs (Technology 1) is relatively small. In other words, a number of customers initially held on to both technologies before switching entirely to DVD players. Figure 1a also shows other such examples of the coexistence of successive technologies. Figure 1b shows a similar initial coexistence in sales of technology pairs. Therefore, to model the diffusion of successive technologies, one needs to allow for a rate of disengagement from the preceding technology that is not exactly equal to the rate of adoption of the new technology (i.e., one must allow for partial substitution). This inclusion of a separate rate of disengagement (F12 in this article) is one of the innovations we propose in this research. A low rate of disengagement indicates consumers hold on to both technologies, whereas a high rate indicates they discard the old technology in favor of the new. Thus, the greater the rate of disengagement, the greater the cannibalization of the old technology by the new technology.[ 5]Graph: Figure 1a. Market penetration of select technology pairs.Graph: Figure 1b. Sales of select technology pairs. Adopter segments for a new successive technologyWe define and derive mathematically a typology of four adopter segments for successive technologies: ( 1) ""Leapfroggers"" adopt the successive technology but would never have adopted the old technology and thus present a new consumer segment for the new technology. This is the niche in Christensen's theory of disruption that provides initial sales for the new technology. ( 2) ""Switchers"" are consumers who had already adopted the older technology but who choose to replace it with the successive technology after the latter technology is introduced. In Christensen's theory of disruption, this is the mainstream consumer segment that switches to the successive technology after it improves. The refinement in our empirics is that this segment switches continuously to the successive technology as it improves. Each year, customers switch as the successive technology matches their needs better than the old technology. ( 3) ""Opportunists"" are those who would have adopted the old technology but delayed the decision and instead end up adopting the successive technology. ( 4) ""Dual users"" are those who had already adopted the older technology but who elect to adopt/use both technologies once the successive technology is introduced. This segment also includes those who would have adopted the old technology but had delayed the decision and ended up adopting and using both technologies. A Generalized Model of the Diffusion of Successive TechnologiesMany situations exist in which one technology substitutes for another but the substitution is only partial, either due to incomplete compatibility or because the old technology still has its uses. Thus, it makes sense to hang on to the old technology because it is still useful (e.g., VHS vs. DVD), even in the presence of the new. Currently, no model allows for this coexistence of successive technologies. However, multigenerational models such as [26] and [14] model the diffusion of successive generations of the same technology. Although the [26] model is not right for multitechnology substitution, a modification of the Norton–Bass model is well-suited for this context.Our proposed model uses the multigenerational model of [26] as a starting point and extends this model to consider the context of the adoption of successive technologies that do not fully cannibalize each other (partial substitution). The major difference in our model is that we include a rate of disengagement from the old technology that does not equal the rate of adoption of the successive technology, which accounts for partial substitution in the case of successive technologies versus complete substitution in the case of successive generations of the same technology.Herein, we ( 1) specify our intuition that motivates the derivation of adopter segments for successive technologies, ( 2) outline our model for the diffusion of two successive technologies (the Web Appendix provides an extension to multiple technologies), ( 3) discuss our critical departure from the basic model of multigenerational diffusion (i.e., we provide a more flexible model in which we do not force the rate of disengagement from Technology 1 [this term is used in this section to concisely reflect the old technology] to exactly match the rate of adoption of Technology 2 [we use this term for the successive technology]), and ( 4) illustrate the equations we used to decompose adoption into four adopter segments. The Model for Diffusion of Two Successive TechnologiesWe specify the proposed model for the simplest case of the diffusion over time of two successive technologies as follows. Let  S1(t)  and   S2(t)   respectively represent the penetration of Technologies 1 and 2 at each time period   t  . Then we model  S1(t)  and   S2(t)  as follows: S1(t) = m1F1(t)(1 − F12(t − τ2 + 1)) Graph1  S2(t) = F2(t − τ2 + 1)(m2 + m1F1(t)) Graph2Note we have added the 1 in Equations 1 and 2 to account for the fact that we are only considering whole years.  τ2  corresponds to the introduction year for Technology 2,     and Fg(t) = pg(1 − e−(pg + qg)t)pg + qge−(pg + qg)t, t ≥ 0, g = 1, 2, or 12 Graph3refers to the fraction of all potential Technologyg consumers for each technology at time t. Here, g refers to a technology (rather than a generation of a technology as is typically considered in the literature on multigenerational diffusion). Our model contains eight parameters:  m1, m2, p1, p2, p12, q1, q2,  and   q12  . The parameter   m1  represents the long-run penetration for Technology 1 if Technology 2 had never been introduced. Put another way, prior to the introduction of Technology 2, the penetration for Technology 1 will converge toward  m1  but will never reach  m1   because for  t ≥ τ2,  Technology 2 will start to reduce the market share of Technology 1. Thus, Technology 2 begins to take market share from Technology 1 upon its introduction. Similarly,  m2  represents the additional market share for Technology 2 above that of Technology 1, so our model assumes that the long-run penetration for Technology 2 will equal  m1 + m2  . The parameters  p1  and  p2  are the coefficients of innovation for Technologies 1 and 2, respectively, and  q1  and  q2  are the coefficients of imitation for Technologies 1 and 2, respectively.  p12  and  q12  can then be thought of as the coefficients of disengagement. Thus,  F1  describes the rate at which customers adopt Technology 1 prior to the introduction of Technology 2, and  F2  models the rate of adoption of Technology 2 after its introduction. Finally,  F12  models the rate at which Technology 1 customers disengage upon the introduction of Technology 2.Note that we make two critical departures in this specification from what is typical of multigenerational diffusion models. Typically, multigenerational diffusion models restrict  F2 to equal F12  . The proposed model removes such a restriction for the context of successive technologies. The potential advantage of modeling  F2  and  F12  separately is as follows: when  F2 = F12,   the rate of disengagement by current Technology 1 customers exactly matches the rate of adoption by Technology 2 customers. However, in the case of successive technologies, across categories and countries, consumers may in fact hold both technologies simultaneously. For example, many families with older members have both a landline and a mobile phone. In addition, both technologies may grow simultaneously in different customer segments. Therefore, one of our innovations in developing a corresponding model to fit the context of successive technologies is to allow  F12 to be less than F2  , which corresponds to people adopting Technology 2 at a faster rate than they leave Technology 1. If  F12 = 0  , then there is no substitution effect and people are holding on to both technologies. When  F12  is large, there is a large substitution effect. This is a strength of the model because we can directly measure the substitution effect rather than forcing  F2 to equal F12  .Second, an important distinction from prior models is that we also do not constrain  p1  to equal  p2  or  q1  to equal  q2  , a constraint that is suitable when the changes between the two generations are incremental, as in multigenerational diffusion, but not when the technology is discontinuous ([24]), as in our more general case of successive technologies. Given that each successive new technology provides a substantial improvement in benefits, we expect the diffusion parameters p and q to vary for each new technology in a pair or triplet. Thus, our model does not constrain  p1  to equal  p2  or  q1  to equal  q2  .Note that, similar to previous models, we make certain assumptions. First, we assume a pure Bass model formulation for the first technology ([ 1]). However, we acknowledge that the first technology may have been affected by a previous technology. Second, we model  F12  using the same functional form as  F1  and  F2  for two reasons. Empirically, we find that the model with this form fits our data well. In addition, by modeling  F12  using the same functional form as  F2  , our approach reduces to the standard [26] and [14] formulations whenever  F12 = F2  . Thus, we provide a strict generalization of previous models. Overall, however, our model is a generalized model that can apply to both generational diffusion and technology diffusion. Model EstimationLet  Sig  represent the observed yearly penetration of Technology  g  at time  ti  . Then, estimating the eight parameters in Equations 1, 2, and 3 can be achieved using nonlinear least squares. In particular, we select  m1, m2, p1, p2, p12,q1, q2,  and  q12  as the values that minimize ∑i = 1n(Si1 − m1F1(ti)(1 − F12(ti − τ2 + 1)))2+ ∑i = 1n(Si2 − F2(ti − τ2 + 1)(m2 + m1F1(ti)))2, Graph4where   n  represents the number of years of observation. We minimize Equation 4 using the NLS function in the statistical software package R. Once the parameters have been estimated, it is a simple matter to plug the estimates back into Equations 1 and 2 to predict future penetration for Technologies 1 and 2. Computing Segments of Adopters for the New Successive TechnologyNext, we decompose penetration of Technology 2 into the four major segments defined earlier. Switchers (SW) and opportunists (O) represent a lost market for Technology 1 and thus its cannibalization (CAN), whereas leapfroggers (L) and dual users (DU) represent market growth (MG). Therefore,  S2(t)  comprises the sum of these segments as such: S2(t) = MG2(t) + CAN2(t) = L2(t) + DU2(t)︸Market growth   + SW2(t) + O2(t)︸Cannibalization . Graph5Similarly,  S1(t)  comprises the initial market for this technology (  L1  ) less cannibalization from Technology 2 as such: S1(t) = L1(t) − CAN2(t) = L1(t) − (SW2(t) + O2(t))︸Cannibalization . Graph6We derive the various consumer segments as follows: L1(t) = m1F1(t), L2(t) = m2F2(t − τ2 + 1) Graph7 SW2(t) = m1∑θ = τ2tF1(θ − 1)(F12(θ − τ2 + 1) − F12(θ − τ2)) Graph8 O2(t) = m1∑θ = τ2tF12(θ − τ2 + 1)(F1(θ) − F1(θ − 1)) Graph9  DU2(t) = m1F1(t)F˜2(t − τ2 + 1), Graph10where  F˜2(t) = F2(t) − F12(t)  .It is not hard to verify that the four quantities in Equations 7– 10 satisfy Equations 5 and 6. Let us first consider  L2(t)  . Recall that  m2  represents the total potential additional market for Technology 2 beyond that of Technology 1 and  F2  provides the fraction of potential customers who have actually adopted the new technology. Thus,  L2(t)  corresponds to the total number of additional Technology 2 adopters who would never have adopted Technology 1. Next, consider  O2(t)  . Note that  m1(F1(θ) − F1(θ − 1))   represents the number of customers who would be expected to adopt Technology 1 in time period  θ  . However,  F12  of these customers switch directly to Technology 2, while  F˜2 = F2 − F12  customers adopt both technologies. Therefore, summing from  τ2  up to  t  gives the total number of opportunists (Equation 9).  DU2(t)  corresponds to dual users who adopt both technologies. Here,  m1F1(t)  represents the number of people who have adopted Technology 1, and  F˜2(t)  represents the fraction of these people who have adopted both technologies.Finally, the switchers correspond to the remaining adopters of Technology 2, which can be shown to correspond to Equation 8. At  θ = τ2   , this equation is fairly intuitive because  m1F1(τ2 − 1)  represents the current number of Technology 1 adopters and  F12(t)  represents the fraction of potential customers who drop Technology 1 to adopt Technology 2 in period  θ = τ2  . Thus, Equation 8 assumes that current customers of Technology 1 switch to Technology 2 at the same rate as noncustomers of Technology 1. However, for  θ > τ2  , the intuition becomes more complicated because the number of Technology 1 customers will be less than  m1F1(t − 1)  as a result of prior switching.Note that we have chosen to focus on identifying the adopters of the new technology. While we consider the role of dual users, who continue to find value in the old technology, we do not distinguish, for the sake of simplicity, between other types of old technology adopters—for example, those who may never adopt either technology, those who are yet to adopt the old technology but will not adopt the newer technology, and those who will stay loyal to the old technology.We can extend this model to more than two technologies. In markets characterized by excessive turbulence, a third technology is often introduced in quick succession to the second technology. We can extend our model to account for  G ≥ 2  different technologies:  S1(t), S2(t), ..., SG(t)  . Here, successive technologies cannibalize the market of earlier technologies. In the interest of brevity, we detail the model extension to three technologies and its application for data on technology triplets in Web Appendix W1. Model BenefitsThe proposed model allows us to extract the sizes of the four adopter segments for each year and technology pair in each country using the defined equations. Our model has several additional desirable characteristics. First, the model parameters have natural interpretations. For example,  Fg  corresponds to the rate that individuals would adopt technology  g  in the absence of any competing technologies, and  Fg − 1, g  represents the rate that individuals disengage from Technology  g  −  1  to adopt Technology  g  . Second, by setting  Fg − 1, g = Fg  , our model reduces to that of [26] and [14], so their model can be seen as a special but more restrictive version of our approach for this context. Our empirical results suggest that our model provides a significantly more accurate fit to the data on successive technologies. Third, market growth generated by a particular technology can be easily computed as the sum of leapfroggers and dual users, and cannibalization can be computed as the sum of switchers and opportunists. Fourth, we do not place any restrictions on the size of adopter segments. Thus, market growth can be positive or negative. The latter case occurs when the total market size actually declines with the introduction of a new technology, possibly indicating disruption by yet another technology. While not the norm, our empirical results suggest that market growth can at times be negative when a still newer technology emerges for which we do not have data. Model Validation: Can the Model Recover Meaningful Structure from Individual Data?One may ask what evidence we have that our model can correctly recover individual consumer segments given that we have only aggregate data. To validate our model for this purpose, we ran a series of simulation analyses following precedents in model simulation ([28]; [39]). For our data generation process, we simulated the adoption of two technologies by a large group of individual customers. The simulation demonstrates a good fit with only ten years of data for Technology 1 (i.e., the model yields a reasonably good fit with only five years after Technology 2 enters the market) (Simulation Exercise 1). With more years of simulated data, the fits become even more accurate. Next, we show the robustness of the simulation analysis to the inclusion of a continuous heterogeneity distribution (Simulation Exercise 2) and the absence of some of the segments altogether (Simulation Exercise 3). These exercises provide more confidence that our model can uncover meaningful structure from the aggregate data even when the model assumptions do not hold exactly. Details are in Web Appendix W2. Empirical Applications of the ModelThis section covers applications of the model using data from different contexts. Analysis of Cross-Country Penetration of Technology PairsWe examined the fit of the model using the market penetration[ 6] of seven technology pairs (telephone–mobile phone, dial-up internet–broadband, black-and-white TV–color TV, VCR–DVD player/recorder, DVD player–Blu-ray player, personal computer–laptop, and laptop–tablet) spanning 105 countries (441 technology pair–country combinations). The data were compiled from several sources (Passport Euromonitor, Fast Facts Database, and the telecommunications database of the International Telecommunications Union). Model fitOverall, the proposed model fits the data well. Table 2 presents comparisons of the penetration data for four technology pairs using both mean-squared and median-squared errors of our proposed model with the separately estimated disengagement rate compared to the reduced form model using the simplifying assumption  F2 = F12.   Our proposed model gets much smaller error rates than the latter model.GraphTable 2. In- and Out-of-Sample Fit Statistics for Technology Pairs Using Penetration Data. Training Errors on Model whereF2 = F12Tech 1Tech 2Tech 1 MeanTech 2 MeanOverall MeanTech 1 MedianTech 2 MedianOverall MedianLaptopTablet.0043.0009.0026.0006.0001.0003Personal computerLaptop.0123.0016.0070.0018.0003.0010DVD playerBlu-ray.0015.0001.0008.0004.0000.0002VCRDVD player.0032.0082.0057.0012.0056.0018Test Errors on Model whereF2 = F12Tech 1Tech 2Tech 1 MeanTech 2 MeanOverall MeanTech 1 MedianTech 2 MedianOverall MedianLaptopTablet.0324.0134.0229.0030.0012.0023Personal computerLaptop.0390.0131.0260.0031.0017.0025DVD playerBlu-ray.0491.0073.0282.0013.0034.0020VCRDVD player.0096.1223.0659.0025.0567.0089Training Errors on Our Method with F2 ≠ F12Tech 1Tech 2Tech 1 MeanTech 2 MeanOverall MeanTech 1 MedianTech 2 MedianOverall MedianLaptopTablet.0014.0002.0008.0003.0000.0001Personal computerLaptop.0024.0004.0014.0013.0000.0005DVD playerBlu-ray.0011.0000.0006.0004.0000.0001VCRDVD player.0008.0014.0011.0004.0005.0005Test Errors on Our Method with F2 ≠ F12Tech 1Tech 2Tech 1 MeanTech 2 MeanOverall MeanTech 1 MedianTech 2 MedianOverall MedianLaptopTablet.0072.0017.0045.0012.0001.0003Personal computerLaptop.0084.0035.0059.0012.0003.0007DVD playerBlu-ray.0530.0033.0281.0023.0009.0014VCRDVD player.0027.0622.0325.0006.0053.0015 Table 2 presents the results by old and new technology as well as the average error across both technologies for the four pairs (the subsample is displayed for brevity). We derived the mean errors in the ""training,"" or in-sample data, by excluding the last time point for each curve, fitting each of the two competing models to the remaining time points, and calculating the mean of squared errors between the observed and predicted points for each technology pair across countries. In contrast, we derived the ""test,"" or out-of-sample results, by excluding the last time point from each curve and fitting the models to the remaining time points (K = 1). However, in this case, the mean squared error is calculated using the squared difference between the final year's observed and predicted points and calculating the overall average error across countries for each technology pair. Overall, our model fits much better out of sample as well as in sample, which is the true test for better performance of our model. The median error rate refers to the in-sample and out-of-sample error rate across the different countries—using the median instead of the mean—to account for the fact that some countries may greatly influence the averages.[ 7] See Figure 2 for some illustrative fit plots. Web Appendix W3 presents an analysis for K = 3 and 5 years. Overall, this analysis indicates that our model, which allows  F12 < F2   , still outperforms a model that allows  F12 = F2  . Table 3 provides the mean parameter estimates for these technology pairs.Graph: Figure 2. Sample fit plots from application of model with penetration data.Notes: Displayed are the fit plots for sample technology pairs. The black lines are the real data. The red line is plotted using our model (F2 ≠ F12) and the green dashed line is for the model with F2 = F12. The vertical lines represent the year of introduction of the new technology into the market.GraphTable 3. Parameter Definitions and Estimates. ParameterInterpretationLaptop–TabletPC–LaptopDVD–Blu-ray PlayerVCR–DVD PlayerMSDMSDMSDMSDm1Long-run penetration potential for Technology 1 if Technology 2 had never been introduced76.6554.4981.6930.6273.0841.3566.6535.63m2Additional market share for Technology 2 above that of Technology 120.6024.8311.0617.6116.9346.819.1515.16p1Coefficient of innovation for Technology 1.004.010.002.004.005.014.039.048q1Coefficient of imitation for Technology 1.250.056.221.061.544.164.182.130p2Coefficient of innovation for Technology 2.006.011.007.008.012.015.007.010q2Coefficient of imitation for Technology 2.222.109.172.071.162.140.521.262p12Coefficient of disengagement 1.003.013.001.002.014.025.011.021q12Coefficient of disengagement 2.022.056.025.049.157.295.193.118NCount85854141 Our model allows us to decompose penetration for technology pairs into adopter segments. We provide an illustrative example for telephone–mobile phones in India. In Figure 3a, L1 is the projected penetration of Technology 1 (telephone) if the successive technology (mobile phone) were absent. S1 is the estimated penetration for Technology 1, indicating the effect of cannibalization (L1 − Cannibalization) due to switchers (SW) and opportunists (O). In Figure 3b, S2 (penetration for Technology 2 (mobile phone) is decomposed into leapfroggers (L2), total cannibalization (switchers (SW) + opportunists (O)), and dual users (DU). Here, the penetration of mobile phones is initially dominated by leapfroggers, followed by growth from cannibalization. In Figure 3c, S1 + S2 represents the evolution of the overall market due to market growth from Technology 2 (leapfroggers + dual users) compared to the presence of only Technology 1 (L1). Overall, the introduction of mobile phones in India created market expansion.Graph: Figure 3a. Decomposition of penetration of telephone (old technology) in India.Graph: Figure 3b. Decomposition of penetration of mobile phone (new technology) in India.Graph: Figure 3c. Evolution of the market (India telephone and mobile phone).Notes:Figure 3a shows the projected penetration L1 of Technology 1 if the successive technology were absent and the effect after cannibalization from Technology 2, represented by S1, the estimated penetration. Figure 3b shows the breakdown of the penetration curve (S2) for Technology 2 (mobile phone in India) into leapfroggers (L2), cannibalization (switchers [SW] + opportunists [O]), and dual users (DU). Figure 3c shows the evolution of the overall market (S1 + S2) due to market growth (MG) from Technology 2 (leapfroggers + dual users) compared to the market in the presence of only Technology 1 (L1). The figures are plotted over the lifetime of available data for Technology 1. Are Adopters of Successive Technologies Similar Across Categories?We next present some key results derived from decomposition of the data across the 441 technology pair–country combinations ten years from the commercialization of the new technology, using our model. Figure 4a presents the average size of the adopter segments across categories. Notice that for the transition from dial-up to broadband, on average across countries, switchers form the dominant category in terms of market penetration (8%), followed by leapfroggers (6%), rather than dual users. In terms of validity, these results make sense because most adopters are unlikely to hold both dial-up and broadband. In contrast, for landline telephones–mobile phone, dual users (24%) dominate on average across countries; in other words, most adopters were keen on holding both technologies ten years from the commercialization of the new technology.Graph: Figure 4a. Decomposition by adopter segments across technology pairs.Furthermore, on average, growth of Technology 2 derived from cannibalization of Technology 1 due to switchers and opportunists is greater than from market growth due to leapfroggers and dual users for the Blu-ray and broadband markets. In contrast, market growth is greater than cannibalization for the other technology pairs. Overall, the results indicate the size of adopter segments and the effects of leapfrogging and cannibalization vary across categories. Are Adopter Segments Similar Across Countries?Following marketing research discussing cross-country effects with multiple data sets (e.g., [19]; [29]), we examine if adopter segments vary across countries. We classify countries in our data set into developing and developed countries. Specifically, we use the analytical classification provided by the World Bank and gathered from various historical reports, as income classifications are rigorous and contemporaneous.[ 8] We term low and low-middle income countries as developing and middle and high-income countries as developed. We present the following results using data from 323 technology pair–country combinations in which we were able to identify the country income classification as of Year 10 from new technology commercialization. We identify 131 cases of high-income countries, 88 of upper-middle income, and 104 of low-income (includes low and low-middle income) countries.The mean estimated penetration of Technology 2 ten years after the new successive technology commercialization is 18% for low-income countries and 23% for high-income countries. The mean estimated penetration of Technology 1 ten years after new technology commercialization is 24% for low-income countries and 49% for high-income countries. These estimates were very close to the actual penetration data for that year.Overall, the mean for leapfroggers is significantly higher for low-income countries compared to both high-income countries (MeanLlowinc = 7.04, MeanLhighinc = 2.61, t = 4.10, p =.0001, using a two-sample T-test with unequal variances) and upper-middle-income countries (MeanLuminc = 4.21, t = 2.09, p =.038). The mean for dual users is significantly higher for high-income compared with low-income countries (MeanDUhighinc = 16.29, MeanDUlowinc = 6.23, t = 4.97, p <.0001) and upper-middle-income countries (MeanDUuminc = 9.10, t = 3.12, p =.002).Thus, a key empirical generalization from our analysis is that developing countries exhibit a higher level of leapfrogging adoption than developed countries in the early life cycle of the successive technology, whereas developed countries exhibit a higher level of adoption by dual users than developing countries in the early life cycle of the successive technology (Figure 4b).Graph: Figure 4b. Decomposition of adopter segments across income classifications of countries.Overall, we find that adopter segments of successive technologies have some context-dependent variations, validating the need for a generalizable model that managers can use to understand the extent of cannibalization and/or market growth. Analysis of Data on Cross-Country Sales of Technology PairsNext, we examine whether the model fits aggregate sales data. We use historical sales data (units in thousands) on three contemporary technology pairs (laptops–tablets, DVD players–Blu-ray players, and digital cameras–smartphones) from 40 countries, with 92 product–country combinations in total for the years 1990–2017 from the Euromonitor Passport database[ 9]. Fit statisticsTable 4 shows the fit statistics. Results indicate that our model with a separately estimated disengagement also fits sales data well. The mean parameter estimates across the 92 product–country combinations are p1 =.02 (SD =.09), q1 =.54 (SD =.34), p2 =.02 (SD =.03), q2 =.29 (SD =.32), p12 =.09 (SD =.12) and q12 =.34 (SD =.33).GraphTable 4. Comparison of Fit Statistics for Sales Data of Technology Pairs. 1. Laptop Versus Tablet Across CountriesMean ErrorsLaptopTabletOverallModel with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0067.0073.0114.0106.0090.0090Test.0196.0119.1491.0996.0843.0557Median ErrorsLaptopTabletOverallModel with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0036.0036.0116.0108.0068.0072Test.0114.0046.1509.0900.0354.01632. DVD Versus BD players Across CountriesMean ErrorsDVD playerBD playerOverallModel with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0219.0045.0084.0017.0152.0031Test.0294.0028.1165.0224.0730.0126Median ErrorsDVD playerBD playerOverallModel with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0199.0034.0070.0008.0115.0022Test.0231.0006.1019.0108.0505.00193. Digital Cameras Versus Smartphones Across CountriesMean ErrorsDigital CamerasSmartphonesOverallModel with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0010.0003.0063.0022.0036.0013Test.0008.0002.0214.0124.0111.0063Median ErrorsDigital CamerasSmartphonesOverallModel with  F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Model with F2 = F12Our Model with F2 ≠ F12Training.0008.0002.0018.0009.0012.0004Test.0001.0001.0050.0028.0014.0002 1 Notes: This table represents the in-sample (training) and out-of-sample (test) error rates for sales data. The explanations are similar to those provided for Table 2. All the raw numbers for this analysis were standardized by the largest observed sales level by each country to provide for a valid comparison by countries. The median error rate refers to the in-sample and out-of-sample error rate across the different countries—using the median instead of the mean—to account for the fact that some countries may greatly influence the averages. Case Analyses of Successive Technology Competitions in the United StatesWe next apply our model to the competition within contemporary, emerging technology pairs in the United States. The application leads to some preliminary generalizations: First, an increase in switchers over time is associated with the cannibalization of sales of Technology 1. Especially when switchers dominate dual users, this increase in switchers is associated with a sustained decline of sales of Technology 1, disrupting incumbents (Cases 1, 4, Web Appendix W4 Case WA1 on digital camera–smartphones). Second, an increase in dual users over time compared with switchers buys time for older technologies and enables them to grow despite the growth of new technologies (Case 2, Web Appendix W4 Case WA2 on VCRs–DVD players). Third, an increase in and dominance of leapfroggers over time is associated with the growth of Technology 2 (Cases 2, 3, Web Appendix W4 Case WA1). Incumbents underestimate or ignore these entirely new consumer segments. Christensen mentioned this, but we show how to estimate its size and evolution. Case 1: Music CDs versus digital downloadsCDs were the dominant music format in 2004, and Apple iTunes' music store had been offering legal digital music downloads since 2003. Although most music executives then believed that people would pay for legal online music, big record labels were slow in adopting digital downloads. Some industry analysts predicted that digital music would not replace CDs because either potential buyers would use it only to sample music before buying CDs or it would only be the terrain of teenagers using iPods ([ 9]). According to analyst expectations, digital downloads and CDs could be expected to grow in tandem. A pertinent question in 2004 was whether digital downloads would eventually cannibalize and disrupt music CDs or if both would in fact grow in tandem.We analyzed data on sales (in millions of units) of music CDs (CDs and CD singles from 1983 to 2018) and digital downloads (including singles, music albums, and music videos from 2004 to 2018) from the Recording Industry Association of America. The analysis from our model (Figure 5a) suggests that switchers (red line) dominated other segments right from the beginning, and this segment grew over the years. Both dual users (orange line) and leapfroggers (green line) tapered off by Year 5. Thus, contrary to the analysts' early expectations, our model indicates that the technologies did not coexist. The immediate high cannibalization by switchers was associated with and probably responsible for the relatively rapid decline of music CDs.Graph: Figure 5a. Decomposition of music CDs and digital downloads in the United States.The decline of music CDs from 2005 caused both record labels and music retailers to suffer. About 800 music stores closed in 2006 alone ([33]). Case 2: Tablets versus laptopsWhile PCs and laptops were the dominant older technologies, the tablet, which was in the works for many years, took off with the introduction of the Apple iPad. At the D8 conference in 2010, when Walt Mossberg asked Steve Jobs whether he thought the tablet will replace the laptop, Jobs replied ""PCs are going to be like trucks. They are still going to be around, they are still going to have a lot of value, but they are going to be used by one out of X people...Is the next step the iPad? Who knows? Will it happen next year or five years from now or seven years from now? Who knows? But I think we're headed in that direction"" ([27]). HP dominated the market for the older technologies, but in 2011, CEO Leo Apotheker wanted to get HP out of the PC business ([12]). ""The tablet effect is real,"" Apotheker is reported to have said on the call with analysts, ""consumers are changing how they use PCs."" Apotheker was soon ousted, and the decision was reversed. A pertinent question at this time was whether tablets would eventually cannibalize and disrupt sales of laptops (and PCs).We analyzed U.S. sales data of laptops and tablets from Passport Euromonitor. Figure 5b shows that while leapfroggers (green line) were the dominant segment, switchers (red line) dominated dual users (orange line) in the first ten years, vindicating HP's initial bleak assessment. However, soon after, dual users (using both technologies) dominated switchers. Our analysis indicates why tablets would not immediately disrupt the market for laptops. Apple gained by attracting dual users while also capturing an entirely new adopter segment base: leapfroggers.Graph: Figure 5b. Decomposition of laptop and tablet sales in the United States. Case 3: Hybrids versus all-electric carsNext, we examine the case of hybrid cars versus all-electric cars.[10] When Tesla first commercialized the electric vehicle, senior managers and analysts scoffed at the idea for three reasons: ( 1) no domestic firm had successfully introduced a new automobile for a hundred years; ( 2) automobile manufacturing is asset-intensive, making the break-even point unacceptably high; and ( 3) California was a state with very high labor costs, especially in comparison to Japan, Korea, and China. To resolve these issues, the critical question for the entrant and the incumbent was whether to invest in hybrid cars, all-electric cars, or both.To answer this question, we use our model to decompose U.S. retail car sales (in thousands of units) of hybrids (including plug-in hybrids) and all-electric cars, obtained from the Transportation Energy Data Book in the time interval 2000–2018. Results in Figure 5c indicate that the growth of all-electric car sales is driven by a predominance of leapfroggers (green line), while switchers (red line) also grow, albeit slowly. Because all-electric cars represent an emergent technology, we have only eight years of new technology data up to 2018. We use data until 2018 and predict two years ahead. Our model predicts that sales of electric cars would cross sales of hybrids in 2020 (two years ahead), driven predominantly by leapfroggers.Graph: Figure 5c. Prediction in the hybrid and electric car market in the United States.Investors may be anticipating Tesla to dominate this race. Before the COVID-19 crisis overtook global markets, Tesla reached a market valuation of $102 billion in January 2020, trailing only Toyota ([31]). In July 2020, Tesla was worth more than Toyota ([30]). Investors are putting pressure on leading incumbents in gasoline and hybrids to invest in all-electric ([10]). Case 4: Taxis versus ride-sharing services in New York CityWe next examine the emergent technology of ride-sharing services such as Uber and Lyft. Because the data for this case were available only for New York City, we limit our analysis to only this city. In many American cities, including New York, drivers need a medallion to operate a taxi, and the city issues a fixed number of them. The ride-sharing service Uber arrived in New York in 2011. Ride-sharing services match passengers with drivers typically through smartphone apps and provide estimated time of arrival, driver tracking, prepayment, and driver and passenger rating. Under pressure from taxi service providers, regulators and politicians sought to regulate or limit Uber's service. The question of relevance in 2012 was whether ride sharing would disrupt taxi services or if they would coexist.We analyze data on trips (in thousands) per day from 2010 on yellow taxis and from 2015 on ride-sharing apps.[11] Our analysis (Figure 5d) reveals an increase in cannibalization over time on the rides for yellow taxis due to switchers to ride-sharing services (red line). However, leapfroggers (green line) and dual users (orange line) also contributed to the rise of ride sharing. Thus, ride-sharing services grew by also attracting a whole new segment of consumers. Anecdotally, it seems ride-sharing services have responded to the needs of customers that previously had difficulty availing themselves of taxi services, including low-income consumers and those in remote locations, as well as individuals who are comfortable with app-based technologies. Over time, switchers ended up dominating the other two segments for ride-sharing apps, contributing to the decline of yellow cabs.Graph: Figure 5d. Decomposition of trips by yellow taxis and ride-sharing services.The cannibalization of taxicabs by Uber, Lyft, and other such ride-sharing services led to a crisis for taxi services. Medallion prices plunged, and the stock of Medallion Financial (a publicly traded company that manages loans used to purchase taxi medallions in several large U.S. urban markets, including New York) had gone down nearly 49% since Uber raised its Series C funding, according to an analysis done by [ 3]. Discussion Summary of FindingsFirst, technological disruption is frequent, with dominant incumbents failing in the face of takeoff and growth of a new technologies. However, disruption is neither always quick nor universal because new technologies sometimes coexist as partial substitutes of the old technology. Our generalized model of diffusion of successive technologies can help marketers capture disruption or coexistence due to the presence of a rate of disengagement from the old technology (0–1), which can vary from the rate of adoption of the new technology (F12 ≠ F2).Second, the model enables a superior fit to aggregate penetration and sales data over prior multigenerational models that do not include such flexibility (i.e., they force F12 to equal F2). Furthermore, an added benefit of the generalized model is that when the rate of disengagement from the old technology equals the rate of adoption of the new, it reduces to a model of multigenerational diffusion.Third, we identify four adopter segments that account for competition between successive technologies from aggregate data: ""leapfroggers"" correlate with the growth of the new technology, ""switchers"" and ""opportunists"" account for the cannibalization of the old technology, and ""dual users"" account for the coexistence of both technologies.Fourth, the generalized model can capture variations in segment sizes across technologies and markets. Leapfroggers form a dominant component of adopters in the early life cycle of a new technology in developing markets compared with other segments. Dual users form a dominant component of adopters in the early life cycle of a new technology in developed markets compared with other segments. Strategic ImplicationsThe major strategic implications of our findings are as follows: First, many established incumbents stumble or fail due to a takeoff of a new technology. Our model can provide important signals about disruption and survival by estimating cannibalization versus coexistence and forecasting the evolution of four critical consumer segments from aggregate data. Incumbents often wait until the market for the new technology is large enough to be profitable ([ 6]) before committing resources to its development. Our analysis suggests that senior managers of strategy and managers of new products should be careful not to underestimate cannibalization by switchers, especially when they dominate dual users, or growth of new technologies due to leapfroggers (especially in developing countries).Second, despite its frequent occurrence, disruption is not a given when a new successive technology enters the market. Thus, managers do not have to make a stark choice between the two technologies. Disruption may be averted by effectively targeting dual users and by carefully examining factors driving the prolonged (co)existence of the old technology.Third, the profit implications of leapfrogging and cannibalization vary depending on which firms market which technology. All segments represent a real gain for entrants, as the takeoff of the new technology is always a win. For the incumbent not introducing the successive technology (e.g., HP), the takeoff of that technology is always a loss. Particularly, if the incumbent firm markets the old technology and a new entrant markets the successive technology, then leapfrogging and switching represent a net loss to the incumbent and a net gain to the entrant. For the incumbent introducing the successive technology (e.g., Sony in DVD players), the takeoff of the successive technology is a win if competitors would have introduced it or if the successive technology has a higher margin than the old technology. Leapfroggers are an opportunity loss for incumbents, but switchers are a real loss to incumbents. If the incumbent firm markets both technologies and if the margin on the new exceeds the margin on the old, then switching and leapfrogging represent a net gain to the incumbent. However, if multiple firms market each technology or if margins vary, then the rate of leapfrogging and cannibalization becomes critical to ascertain profitability given the costs.Fourth, marketers may be able to develop forecasts on the basis of early sales or penetration data of the successive technologies, or from similar contexts, to understand how these various segments may grow (or shrink) over time. Such an understanding can help guide a firm's managerial and economic resource allocation strategies across both technologies over time.Table 5 summarizes the major strategic implications of this research.GraphTable 5. Adopter Segments, Firm Type, and Market Outcomes in the Presence of Multiple Technologies. AdopterSegmentsMarket Outcome on the Introduction of Technology 2Firm TypeIncumbent Marketing Technology 1aEntrant Marketing Technology 2aIncumbent Marketing Technology 2bLeapfroggersMarket growth of successive technologyNeutralWinWinOpportunistsCannibalization of old technologyLoseWinNeutralSwitchersCannibalization of old technologyNeutral/losecWinNeutralDual usersMarket growth of both technologiesNeutralWinWin 2 Notes:3 a Assumes that the incumbent (or incumbents) dominated the market for the old technology and entrants pioneered the new technology.4 b Assumes that the incumbent chooses to enter the new technology market rather than wait on the sidelines.5 c Neutral for adoption/lose if sales is considered. Limitations and Future DirectionsThis study suffers from several limitations. First, we used aggregate data to test the model because they were abundantly available. As managers and researchers get access to richer, individual customer-level data, they may be able to provide better support to our modeling insights. Moreover, disaggregate choice models can be utilized to address issues such as cannibalization. However, macro diffusion models still have the ability to produce useful macro-level conclusions in ways that micro approaches sometimes cannot. Second, we consider a demand-based view of disruption in proposing the typology of adopter segments. Future research could complement these typologies and data sets with surveys to determine the characteristics of adopters of the new technology versus those who stay with the old technology, as well as what factors influence the size of adopter types. Third, an incumbent may respond to the new technology by making changes in variables such as price, and the omission of such control variables may violate some of the assumptions of the model. All these remain fruitful areas for future research. "
27,"Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance Opportunities The use of cobranding and brand extension strategies to access new markets has been a topic of significant interest. However, surprisingly few studies have examined cross-category connections of brands using publicly available digital footprints. In this study, the authors introduce a new, scalable automated approach for identifying potential cobranding and brand extension opportunities using brand networks derived from publicly available Twitter followership data. The digital user–brand relationship, established through followership activity, is regarded as an expression of interest toward the brand. Common followership patterns between brands are then extracted to capture cointerest between those brands' audience. By utilizing the cointerest patterns, the approach aims to derive cross-category brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The analysis is conducted at different points in time to help managers track shifts in brand transcendence.Keywords: cross-category; brand networks; asymmetry; cobranding; brand extensions; social media; social networks analysis; TwitterCobranding is a brand alliance strategy to bolster reach, awareness, and sales potential by tapping the prospective customers of partnering brands. Many types of cobranding schemes exist in the marketplace, including joint advertising campaigns (e.g., ads depicting the joint consumption of Coca-Cola and McDonald's), cause–brand alliances (e.g., UNICEF and Target), bundling (e.g., streaming deals that include joint Hulu and Spotify subscriptions), and cobranded products (e.g., Louis Vuitton launching an exclusive luggage line for BMW). Cobranding strategies enable brand extensions, with managers leveraging the existing brand names of their partners to enter new markets and categories ([14]). Cobranding is increasingly viewed as a valuable marketing strategy and has been shown to increase awareness, quality, market value, and brand equity ([ 8]; [46]). Although marketers have been leveraging the synergistic benefits of cobranding for decades, surprisingly little empirical research has tried to identify potential cobranding alliances using modern digital approaches. Most of the existing empirical research either uses observations from fast-moving consumer goods categories ([ 8]; [14]; [19]) or conducts analyses within a single category, such as camcorders in [24], car brands in [34], and LED TVs in [42]. Similarly, [36] and [35] use recommendation hyperlinks between Amazon web pages to create a large-scale network of books and demonstrate the value of shared purchasing patterns.Obtaining broader insights into the identification of cobranding opportunities across diverse categories would generate relevant and meaningful information for brand owners. As [43] notes, ""By mashing up two bona fide brands, especially in diverse industries, the impact can be exponential."" For instance, a well-known cobranding deal between Starbucks and Spotify—two seemingly unrelated brands—enabled both brands to cross-promote their products and grow their customer base. By providing premium coffee-shop music, Starbucks incentivized Spotify users to join its loyalty program. In return, Spotify grew its user base through Starbucks' offer of a free coffee upon joining. Having knowledge about relevant cross-category brand connections is crucial to brand owners ([12]); however, there is little or no research on identifying these broader cross-category effects using current digital approaches.This article introduces a new, scalable approach for generating cross-category branding insights using implicit brand networks on social media. The cross-category branding insights are revealed in the form of brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. Unlike traditional social networks, which involve explicit interaction between the participating entities,[ 5] edges within a brand network are implicit (or tacit) and arise due to common followership between brands. [45] note the relevance of these tacit connections to decision making. This idea has been studied previously within the domain of collaborative filtering ([45]). Implicit networks, which condense the vast digital interest space of millions of users into a parsimonious form, provide direct insight into the digital ecosystem and are the subject of increasing research attention across domains ([45]). In this study, the cross-category connections of a focal brand in the implicit network are leveraged to help brand managers identify cobranding and extension opportunities.The article introduces a new construct, called ""brand transcendence,"" which is defined in the context of a large ecosystem of brands belonging to different categories. The transcendence of a brand into a new category is the extent to which its followers overlap with those of other brands in the new category. From a managerial perspective, this study provides an automated approach for identifying cross-category cobranding opportunities based on user cointerest, which is measured through overlap of brands' followers on Twitter. Importantly, the cointerest patterns captured through common followership do not necessarily reflect overlapping brand associations or guarantee brand fit, which is traditionally measured using the similarity of brand personality dimensions ([48]). However, such patterns are indicative of common tastes or interests among social media users. Following [39], we consider that the composition of a brand's follower base represents the tastes (and likes) of its audience. Thus, the greater the network overlap between two brands, the greater the similarity in tastes and interests between those brands' audiences. Taking these principles together, we study the transcendence of a brand into a new category based on the extent to which its followers overlap with those of other brands in the new category. Our approach also identifies central brands that have strong and consistent connections within their own category ([ 9]), with ""centrality"" being defined as the extent to which a brand's followers overlap with other brands in its own category.By incorporating directionality into the network edges, we also capture the asymmetric relationships between brand pairs, which help identify brands that may potentially benefit more from a cobranding alliance. We outline how cross-category connections can provide both brand–category and brand–brand insights, depending on a brand's marketing goals (i.e., extension vs. cobranding). For instance, brand–category connections capture the transcendence of brands into new categories and show that certain categories are more viable for extensions than others. Brand–brand connections, in contrast, provide a more granular view of transcendence by revealing the individual brands that are suitable for cobranding. As user–brand relationships on social media may change over time, this article analyzes the brand network in both 2017 and 2020. This helps visualize the fluctuations of brand connections over time and investigate the impact of such fluctuations on cobranding alliances. Understanding whether critical connections with certain brands or prospective categories have waned helps managers promptly identify the problem and take appropriate action. Similarly, identifying new connections that have formed over time illustrates how past marketing actions can impact a brand's transcendence in users' minds.Cross-category connections revealed through the network can be used to both assess the effectiveness of previous marketing campaigns and discover new alliance opportunities. For example, Bud Light's connection to Pepsi reflects the cointerest patterns between the two brands and, thus, affirms the effectiveness of joint marketing campaign led by the two brands previously. Similarly, Sierra Nevada's strong connections with travel and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Microsoft) highlight strong cointerest with these brands and present new cobranding opportunities that may not yet be known to its owners. We provide examples of both scenarios using information from external industry sources. Another practical application of our method is competitor analysis, which can help managers identify the differentiating connections of brands with respect to their competitors and gauge the type of users their competitors attract.Finally, we validate the findings of our model against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by fake users or bots. Consistently high correlation between our automated approach and external survey ratings affirms the validity of our methodology for identifying cross-category brand–brand and brand–category connections. Overall, the core contribution of this study is a new digital approach to analyzing audiences' interests across a broad brand ecosystem. The cross-category insights generated by this approach can help researchers and practitioners identify nontraditional branding opportunities that are difficult to infer from traditional survey-based approaches. From a managerial perspective, our brand network can efficiently and cost-effectively generate cross-category insights, given that most of the data collection and network analyses are automated. Furthermore, as our approach uses information that is publicly available on social media, it is easily scalable to a large number of brands, with the resulting network structures reflecting the preferences of a diverse set of users. In the next section, we discuss relevant studies in the marketing literature and describe how our work contributes to the field. Conceptual Motivations Social Networks for Cobranding and ExtensionsResearchers regard cobranding as a source of competitive advantage that helps brands differentiate themselves, gain consumer trust, acquire new channels of distribution, and enter new markets ([47]). Brand extensions (i.e., leveraging the existing brand's name to enter into new categories) are another widely adopted strategy for firms entering new markets ([ 1]). Brand extensions provide greater quality assurance to customers who are familiar with the original brand, reduce the costs of distribution, and increase the efficiency of promotional expenditure ([ 1]). Both brand extensions and cobranding strengthen the focal brand and reinforce customers' value perceptions of the new product ([20]).Naturally, identifying underlying brand-to-brand connections on the basis of users' cointerests may be a key that enables brand managers to discover potential cobranding and extension opportunities. Most studies in this domain have used surveys ([ 5]). Although collecting input from prescreened participants is desirable, recruiting and maintaining a pool of such participants may be unfeasible due to cost or other constraints ([11]). Recent advances in social network analysis have enabled a wide range of scalable solutions that go beyond conventional market research methods. Although previous studies have considered the identification of brand-to-brand connections based on digital user traces, such research has been restricted to brand (or product) relationships within a single category. For example, [34] focus mainly on intracategory connections to create competitive market structures for car brands. Using survey approaches, [13] obtain intracategory maps for centrality and distinctiveness. Finally, [42] develop mapping methods to visualize large market structures within a single category (i.e., LED TVs).The marketing literature acknowledges the importance of cross-category brand connections for generating extensions, licensing, and cobranding deals ([22]; [41]). However, only limited empirical work has been done in this area. This article introduces an automated, scalable approach for identifying cross-category brand cointerest patterns by leveraging the cofollowership data on Twitter. The use of common followership data on Twitter in our analysis follows the recent work of Culotta and Cutler (2016). However, while [11] aimed to derive perceptual attribute ratings from Twitter followership data, the goal of this work is to investigate asymmetric cross-category brand transcendence over time. Further, unlike Culotta and Cutler, our large-scale network approach does not require any supervised knowledge on exemplars and uses categorical affiliations of brands to infer brand perceptions on transcendence and centrality. Specifically, the inclusion of network-derived measures enables us to study both within-category competition and across-category complementarity between brands. Table 1 presents the unique contribution of this research compared with previous network studies.GraphTable 1. Comparison of This Study with Previous Network Studies in Marketing. StudyFocal ContributionData SourceOutputAsymmetrySurvey ValidationAddresses Bots/Fake Users OnlineCompetitor AnalysisAnalysis Presented over TimeNetzer et al. (2012)Create competitive structure maps using text mining and network analysisComentions on online discussion forumMarket structures within a categoryNoYesNoNoNoCulotta and Cutler (2016)Propose a methodology for inferring attribute-specific brand perceptionsCofollowership data on TwitterPerceptual maps for a set of predefined exemplarsNoYesNoNoNoRingel and Skiera (2016)Develop mapping methods for visualizing complex market structuresConsideration sets from online searchMapping solutions for large complex market structures within a category of >1,000 productsYes, using conditional probabilityNoYes, eliminating implausible clickstreamsYesNoCurrent studyUse implicit brand networks to infer asymmetric cross-category brand connections over time.Cofollowership data on Twitter over timeCross-category connections maps at two levels: brand–category brand–brandYes, using conditional probabilityYesYes, using network rewiringYesYes  Cofollowership Patterns for Identifying Cobranding and Brand ExtensionsOur approach to identifying cobranding and brand extension opportunities harnesses the digital cofollowership patterns between brands. Survey research has shown that users follow brands on social media with the intention of purchasing a product or learning more about their favorite brands ([31]; [40]). Aspirations can also motivate consumers to follow brands ([ 3]). This digital user–brand relationship, which is established through followership activity, can be interpreted as an expression of affinity for the brand ([27]; [33]). Alternatively, this relationship can be viewed through the lens of homophily, meaning that people tend to associate with those who are similar to them in socially significant ways ([32]). This is further supported by consumer research studies ([ 6]; [10]), which show a strong relationship between a brand's image and characteristics and the identities of its followers.Similarly, [39] find that the composition of one's follower base represents the tastes (and likes) of their audience. Thus, the more network overlap (i.e., common followers) between two entities, the greater the similarity of tastes and interests among those entities' audiences ([39]). [ 2] find that common friends (i.e., common mutual followers) have a positive effect on the adoption of an application on Facebook. Similarly, we expect that brands that share a high number of followers on Twitter have a user composition that represents similar tastes or interests (e.g., the partnership between GoPro and Red Bull, which leveraged the shared affinities of their common followers: action, adventure, and fearlessness). Given that individuals primarily follow a brand because they like its products and that most followers are customers ([40]), brands having more common followers implies that their customers may have complementary consumption patterns (e.g., the Starbucks–Spotify partnership, which facilitated the complementary consumption of Spotify music and Starbucks coffee). Building on these theoretical principles, we study the cobranding candidates of a focal brand based on the extent to which its followers overlap with the followers of another given brand (and/or category) of interest. Brand Transcendence and CentralitySome of the greatest brands in the world have defied category norms and transcended their initial market boundaries ([23]). For example, in 1995, Amazon positioned itself as ""Earth's biggest bookstore,"" and its success with books enabled it to transcend its origins to become a leader in e-commerce. Although all brands theoretically operate within their categorical boundaries, such boundaries are often considered malleable ([ 7]). Important cobranding and extension opportunities can be missed if managers are not aware of connections that are relevant to brands in other categories ([ 5]). The brand network provides a solution to this problem by relying on a brand's social connections on Twitter to infer category-specific brand connections.At a high level, our proposed algorithm extracts the category-specific connections of a brand by exploiting the overlap in brand followers on Twitter. Whereas some brands may possess strong connections within their own category, others may have diverse connections across new categories. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The transcendence of a nonsports brand along any given category—for example, say sports—is based on the extent to which its followers overlap with those of other brands in the sports category. Further, to measure the connections not shared by the overall brand category, we calculate ""net transcendence"" as the deviation of a brand's own idiosyncratic connections from its category average. Net transcendence is more informative than raw transcendence because it ignores the cross-category connections that are generic to the category and identifies those that are intrinsic to the brand itself.Lastly, in addition to transcendence, there are brands that possess strong connections within their own category. These brands can be viewed as central. The concept of centrality (or typicality) bears direct relation to a brand's probability of recall, consideration, and choice among consumers' minds ([28]). Such central brands are those that come first to consumers' mind and serve as reference points in their categories ([13]). In the next section, we describe how the brand network is generated from followership patterns on Twitter and lay out important network details. The Network Mining MethodologyThe key contribution of this article is the introduction of an automated framework for inferring cross-category branding insights using implicit brand networks derived from social media. With their ability to provide a direct digital window into the interests of millions of social media users, implicit brand networks can help mangers identify nontraditional branding opportunities that would otherwise be hard to perceive. In this section, we generate implicit brand networks using brand communities on Twitter and outline important network details. DataDrawing from the notion that the social signal of ""who follows a brand"" provides a strong reflection of brand image ([11]), we use a set of 507 brands' Twitter accounts as the basis for our analysis. We select the most active Twitter brand accounts based on followership data from the social media directory FanPageList.com. We use Twitter's public application programming interface to collect the brands' lists of followers for 2017 and 2020. We manually verify that all Twitter handles correspond to the official brand account. Overall, the data set consists of brands from many major categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. Each brand is assigned to a specific category based on the basic or superordinate category-level analyses ([28]).[ 6] To prevent bots or spam accounts from influencing the network analysis, all Twitter brand accounts included in the analysis are manually audited using the audience intelligence website SparkToro.[ 7] Furthermore, as we discuss in the section on robustness checks, we conduct network simulations to ensure that our final network estimates are not biased by such bots. Network GenerationThe next step is to extract the common followers between all brand pairs. The raw brand network is a weighted edge list, defined as 〈bi, bj, wij〉, where bi and bj are individual brands or nodes and wij is the common followers between those brands. If Fi and Fj represent the list of Twitter accounts following brands bi and bj, then an edge between two nodes is created if Fi ∩ Fj > 0. Alternatively, the weighted edge list can be represented as a weighted adjacency matrix Aij where Aij=wij0}ifbrandiandbrandjareconnectedotherwise. GraphOverall, we extract two brand networks: one for 2017 and one for 2020. The original brand networks are highly dense, with common followers between almost all pairs of brands. The numbers of common followers vary from a few hundred to more than a million users. Although it is possible to work with such dense networks, valuable information may be lost due to the redundancy generated by the large number of connections ([44]). Further, connections based on too few followers may not indicate significant connectivity. Given the wide heterogeneity in raw edge weights (i.e., the numbers of common followers), we next aim to extract the truly relevant brand–brand connections. Network filteringA common way to extract a relevant network structure is by applying a global threshold to remove the edges with weights below a particular cutoff. This, however, can destroy the multiscale properties of the brand network. Instead, we use a disparity filter ([44]), which is a filtering algorithm for multiscale networks, to obtain a reduced but more meaningful representation of the network. This method preserves the important edges present at all scales by locally identifying the statistically relevant weights at the node level. The statistically relevant edges (at a given significance level; e.g., α) represent a significant deviation from a null model of uniform randomness. Thus, smaller brands with fewer common followers are not ignored during the network reduction process. Although the current analysis focuses on Twitter brand accounts with between a few thousand and more than a million followers, this method could also be applied to smaller brands with fewer than 1,000 followers. Following [44], we use the commonly specified significance level α = .05 to extract the important connections in the brand network. The filtered networks for 2017 and 2020 consist of roughly 14,000 edges between brands. Although we use the disparity filter to obtain a filtered representation of the original network, there are alternative information-filtering algorithms available in the network science literature, including the global threshold and global statistical significance filters. In Web Appendix A, we revisit these alternative methods and discuss our rationale for choosing the disparity filter. Asymmetric normalizationBrand community sizes can vary both within and across categories. Brands with large brand communities (e.g., Chanel, Microsoft, Starbucks) tend to have more common followers than those with smaller communities. The normalization of edge weights is required to account for this variance. Thus, we use the conditional probability measure ([42]) to compute new network weights that not only normalize the effects of brand size but also account for asymmetry between brand pairs. Asymmetry between brand pairs may occur when the degree of connection between any two brands is unequal (i.e., the connection from A to B is not equal to the connection from B to A) ([15]). Ignoring the directionality of brand connections can lead to inaccurate estimates of consumer brand knowledge ([16]). We observe many cases of associative asymmetry in our brand network and use conditional probability to account for such scenarios. For instance, Figure 1 shows the cross-category connection between Starbucks and Stella Artois. A large percentage of Stella Artois fans are interested in Starbucks, and the outgoing directional strength is almost.20. However, fewer Starbucks fans are interested in Stella Artois, and the outgoing directional strength is comparatively much lower, at.0009. Incorporating directionality in the network reveals this crucial information, which is not visible in a simple, undirected, weighted network. Mathematically, the conditional probability measure calculates the normalized edges between any two brands A and B as P(A∩B)=|A∩B||A|, Graphwhere the numerator  |A∩B|  is the number of common followers between brands A and B and the denominator  |A|  is the number of followers of the focal brand.Graph: Figure 1. Calculating asymmetry between brand pairs.Figure 2 shows the entire brand network structure for 2020 using the dimensional reduction algorithm, t-distributed stochastic neighbor embedding (t-SNE). The t-SNE algorithm yields a two-dimensional undirected representation of the brand network, with the distance between brands in the t-SNE space being proportional to the mean conditional probabilities between brands. The colors of the brands correspond to their category affiliation. Interestingly, while most automotive brands in Figure 2 are well-distanced from other nonautomotive brands, Tesla is positioned in the technology category. A similar pattern is observed for certain retail brands such as Adidas and Reebok, which are closer to the sports group than to other brands in their category. Individual brand constructs on transcendence, as described in the next section, help reveal specific cross-category connections for a given brand.MAP: Figure 2. t-SNE map of the undirected brand network.[ 8] Measures of Brand Transcendence and CentralityThis section outlines the process of identifying centrality and transcendence by exploiting the connections of a brand within the network. For any given brand, the first step is to disentangle its connections across the main categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. The algorithm then computes the weighted out-degree centrality of a brand across these categories. In weighted networks, out-degree centrality or node strength is commonly calculated as the sum of weights emanating from a focal node to all its connections ([ 4]). However, to account for the strength of edge weights and the number of connections of a focal node, [37] propose a new measure of weighted degree centrality: Ci=di(1−α)×wiα, Graph( 1)where  di  is the degree of the focal node (i.e., the number of connections),  wi  is the node strength (i.e., the sum of the weighted connections), and  α  is the tuning parameter from 0 to 1. For example, following [37], the transcendence of a given nonsports brand into the sports category is calculated as the function of its number and strength of outgoing connections to all other brands in the sports category.More formally, the set of brands in the network can be represented as B, where any individual brand  b∈B  . Brand categories, G, are subsets of B, such as G ⊆ B.[ 9] The transcendence of focal brand b onto a new category G is evaluated as tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|, Graph( 2)where  ∑k∈Gdb,k  is the number of outgoing edges from brand b to all k brands in category G,  ∑k∈Gwb,k  is the sum of weighted edges from brand b to all k brands in category G, and  |G|  gives the number of brands in category G. Dividing by the total number of brands in a category,  |G|  , helps ensure that large categories with many brands do not dominate the analysis. Following [37], we set  α  to.5 to place equal importance on a brand's number of connections and the weight of those connections.Furthermore, considering that brands whose followers tend to follow many other brands may inflate the network constructs, we divide our transcendence construct by the total degree of a brand in the network (i.e., its number of connections to other brands). Intuitively, brands whose followers tend to follow many brands have a higher degree than brands whose followers follow fewer brands. Thus, in the transcendence construct, the sum of the numerator increases with each new connection of a brand in the network. With the objective of normalizing for brands that inherently have higher aggregate transcendence due to higher degree in the network, we divide our final transcendence construct by the total degree of a brand. Thus, the final transcendence construct becomes tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|×db, Graph( 3)where the additional variable in the denominator,  db,  is the total number of connections of a brand in the network.Furthermore, in our transcendence construct, brand b's connections within its own category  Gp  relate to its centrality (  tbGp  ): the higher the strength of these connections, the more central the brand is in its own category. As noted previously, the notion of centrality is directly related to a brand's probability of being recalled, considered, and chosen by consumers ([28]). We multiply the centrality construct by the size of the focal brand's community (i.e., its number of followers) to account for the brand's popularity among users. Thus, the final centrality construct is tbGp×fbmax(fbi∈Gp), Graph( 4)where  tbGp  is brand b's connections within its own category  Gp  and  fb  is its number of followers. Given that the values of  fb  vary from a few hundred to more than a million users, we use its scaled value. Given a set of nonoverlapping categories G1, G2, ..., Gp, the transcendence of brand b across p categories is a 1 × p-dimensional vector: tb=[tbG1tbG2tbG3⋯tbGp]. Graph( 5)The transcendence vector of a brand can also be analyzed with respect to its competitors in the category. The 1 × p-dimensional vector  tb  can be further extended into an n × p matrix, where n rows represent brands (i.e.,  b1,b2...,bn  ) and p columns represent transcendence across the p categories, as shown in Figure 3.Graph: Figure 3. The transcendence matrix, tbG for n brands across p categories.The average category connections (i.e., connections emanating from one category  Gi  to another category  Gp  ) are then calculated as follows: tGiGp=∑b∈GitbGp|Gi|, Graph( 6)where  i≠p  . This formula measures the average transcendence of brands in one category (for example,  Gi  ) into another category  Gp.  To separate a brand's unique connections (  tbGp  ) from its category average (  tGiGp  ), we calculate the net transcendence of brand b into category  Gp  as follows: t_netbGp=tbGp−tGiGp, Graph( 7)where  b∈Gi  and  i≠p  . A positive value for  t_netbGp  indicates that the brand's transcendence is above the category average, while a negative value indicates that its transcendence is below average. As in the raw transcendence vector  tbGp  , the net transcendence vector of a brand b across p categories is t_netb=[t_netbG1t_netbG2t_netbG3...t_netbGp]. Graph( 8)The 1 × p-dimensional vector  t_netb  can be further extended into an n × p matrix, where rows represent n brands in a category and p columns represent the net transcendence of brands across the p categories, as shown in Figure 4.Graph: Figure 4. Net transcendence matrix of n brands across p categories.Figure 4 provides a more comprehensive view of the competitive landscape of a particular brand by highlighting its cross-category connections as well as those of its competitors. In the next section, we discuss the results of these analyses and identify their key managerial implications. Results and Managerial ImplicationsDepending on the business objective, the category-specific connections generated by the brand network can be visualized on two different levels: brand–category and brand–brand. Using examples from the automotive and beer categories, in this section we present our results on these two levels and discuss the managerial implications of our findings. First, we note the brand–category connections of automotive brands and identify the categories suitable for brand extensions. Second, we focus on brand–brand connections and discuss how asymmetry can be leveraged to attain more nuanced insights into the expected benefits of cobranding. Third, we highlight the network's ability to capture changes that occurred between 2017 and 2020, given that individual brand–brand connections may change over time. Finally, we discuss how the brand network described in this article can help managers identify the differentiating category-specific connections of brands with respect to their competitors and gauge the type of users their competitors attract. Brand–Category Connections: Transcendence and CentralityTo identify the brand–category connections of automotive brands in 2020, we study the net transcendence matrix,  t_netbGp  , as shown in Figure 5. All column values have been scaled, with positive values shown in red coloring and negative values shown in blue on the heatmap. For every brand (n)–category (p) relationship in the heatmap, values closer to dark red indicate a stronger perceived relationship between a brand and category. The stronger the relationship between the brand and category, the greater the user cointerest between that brand and category. As discussed previously, the cointerest patterns captured through the analysis of common followership do not necessarily guarantee brand fit, which is typically based on the similarity of brand personality dimensions ([48]). Instead, values in the transcendence matrix reveal cointerest patterns that enable managers to explore potential extension and cobranding opportunities that are difficult to infer from traditional survey-based approaches. For instance, the audience of the car brand Mercedes has strong cointerest with the luxury, technology, retail, and sports categories, suggesting that extensions may be possible in these categories.Graph: Figure 5. Net transcendence matrix, t_netbG, reflecting brand–category connections of the automotive brands (2020).Similarly, there may be brands that, despite having low net transcendence into different categories, have strong connections with their group, making them central in their own category. For example, Toyota and Dodge are highly central to the automotive category, although their net transcendence across categories is low. Tesla, in contrast, has high net transcendence into the technology category, despite having low centrality in the automotive group. We also observe that some car brands with high net transcendence across multiple categories have moderately low centrality in their group (e.g., Audi, Mercedes, Tesla, and Lamborghini). However, brands such as Chevrolet and Ford share strong cointerest in the automotive category and also have moderate net transcendence into beer, dining, and sports. Thus, centrality and transcendence are not mutually exclusive, and a brand may be perceived as both central and transcendent, depending on its connections in the network. Brand–Brand Connections: Leveraging Asymmetry for Cobranding InsightsThe brand network developed in this study can also help managers obtain a more granular view of transcendence by identifying brand–brand connections across categories. The different levels of analysis (i.e., brand–category and brand–brand) offered by the network can help managers understand why certain cobranding opportunities are more promising than others. Further, the network's analysis of the asymmetry between brand pairs can help identify brands which may potentially benefit more from a cobranding alliance. To identify strong, relevant cobranding candidates for a focal brand, we only consider brand–brand connections in categories where the net transcendence of the brand is positive. This ensures that the identified brand connection is not generic to the category but rather is intrinsic to the brand itself. For instance, the net transcendence of Mercedes into the luxury goods and retail categories is positive, meaning that, on average, its connections with luxury and retail goods are relatively higher than those of other car brands. Thus, for Mercedes, brands in the luxury and retail categories are considered suitable candidates for cobranding.Figure 6 shows the brand–brand connections of Mercedes with brands in the luxury and retail categories. Some of Mercedes' strongest connections are with Louis Vuitton, Nike, Tissot, and Chanel, which highlights these brands' potential for alliances with Mercedes. However, given the asymmetrical nature of these relationships, the benefits gained through such alliances may not always be equal. For instance, the asymmetrical connection between Mercedes and Tissot reflects that a greater proportion of Tissot's audience is interested in Mercedes than vice versa. This indicates that there may be a greater potential benefit for Tissot from such an alliance. These results on asymmetry can provide additional insights to brand managers of both Mercedes and Tissot when evaluating potential cobranding candidates.Graph: Figure 6. Top 20 brand–brand connections of Mercedes with brands in the luxury and retail categories using the Fruchterman–Reingold (1991) layout.Indeed, prior strategic alliance literature suggests that unequal spillover benefits can be expected from asymmetrical brand alliances ([21]). However, the main findings of [21] suggest that even though the magnitude of financial gains in asymmetrical alliances is not equal, it is not a win–lose partnership but rather a win–win or a shareholder value-adding alliance for both the larger and smaller partner firms.[10] Similarly, in the case of Mercedes, even though the expected benefits may not be equal for the asymmetrical relationships (e.g., Mercedes–Chanel, Mercedes–Tissot, Mercedes–Rolex), future deals can be still beneficial to both brands. Although the smaller brand, Tissot, may achieve greater gains from this asymmetrical alliance (e.g., by having a greater proportion of its audience interested in Mercedes), the larger brand, Mercedes, may still gain access to a niche audience that may not be a part of its current demographic. Capturing Shifts in TranscendenceA brand's transcendence in the network, which arises from common consumer interest, may not be static. Brands may, for various reasons, wish to shift their transcendence to new categories in search of new alliances or cobranding opportunities. Our network can track such shifts in transcendence, allowing brand managers to better assess the effectiveness of their marketing actions and identify emerging or waning categories for future brand alliances.Figure 7 shows the change in net transcendence of car brands into the technology category. The dynamic plots for other categories can be analyzed similarly. Interestingly, between 2017 and 2020, the technology connections of many car brands, including Honda, Jeep, Chrysler, Acura, and Chevrolet, decrease. However, some brands, including Tesla, Lamborghini, and Infiniti, show a steep rise in their connections with technology. This may be related to, for example, Infiniti's plans to go all-electric in 2021 and use intelligent technologies to reflect its new ethos ([30]). Thus, our brand network-based methodology can be used to assess the effectiveness of a brand's marketing campaign and showcase how marketing actions can impact the brand's transcendence in users' minds.Graph: Figure 7. Change in net transcendence over time.The network's ability to highlight shifts in brand transcendence over time can be of vital use to managers. The emergence of new connections with specific categories over time (e.g., Infiniti's increasing transcendence into the technology category) provides insight into the effectiveness of brands' marketing campaigns and affirms the possibility of future extensions in those categories. Similarly, the waning of a brand's connections with specific categories (e.g., Jeep's decreasing transcendence into the technology category) allows that company to identify potential problems and take appropriate action to address them. The brand network can also help managers investigate issues in more detail by uncovering the specific cross-category brand–brand connections that have diminished over time. Change in a brand's net transcendence into a category can be caused by several factors, including joint ads, new alliances, embedded promotions, or other external events. Although the method in this study does not examine the causes for such shifts, it provides managers with timely intelligence on the subject. Future marketing studies could build on this work to further investigate the causes for changes in brand transcendence over time. Competitor AnalysisThis subsection discusses how the category-specific brand connections revealed through the transcendence matrix may not only allow managers to understand the position of their brands in consumers' minds but also help distinguish them from their competitors. Figure 8 shows the net transcendence,  t_netbGp  , of two beer brands, Bud Light and Sierra Nevada, into different categories. Whereas Bud Light has high transcendence into food and dining, Sierra Nevada has high transcendence into travel, airlines, and technology. Regarding centrality, Bud Light outperforms Sierra Nevada, with stronger connections within the beer category. Thus, whereas the former brand is positioned strongly among beer and food enthusiasts, the latter brand resonates more with technology and travel enthusiasts. This type of analysis can help brand managers identify the differentiating connections of their brands with respect to their competitors and also gauge the type of users their competitors attract.Graph: Figure 8. Net transcendence matrix, t_netbG, of Bud Light and Sierra Nevada.Brand managers can obtain richer insights into the cointerest patterns of their competing brands by examining the individual brand–brand connections in the different categories. For example, Bud Light is connected to more food, beverage, and dining brands (e.g., Pepsi, Coca-Cola, McDonald's, Subway, Taco Bell), while Sierra Nevada is mostly connected to airline, travel, and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Amazon, Netflix). As one might expect, some brand–brand connections can reflect previous marketing activities (e.g., joint advertisements or promotions, collaborations, licensing deals). In such cases, our brand network–based methodology enables managers to measure the effectiveness of a marketing campaign and showcases how marketing actions can impact a brand's transcendence. For example, Bud Light's connection with Pepsi reflects strong cofollowership patterns between the two brands, affirming the effectiveness of their earlier joint marketing campaign.[11] Alternatively, brand–brand connections can highlight potential new cobranding or alliance opportunities that were previously unknown to brand managers. For instance, Bud Light's strong connections with McDonald's and Taco Bell highlight strong cointerest between these brands, suggesting untapped cobranding opportunities. Similarly, Sierra Nevada could leverage the technology and travel interests of its fans, as revealed by the network, to partner with relevant travel brands such as Southwest Airlines, SpaceX, Discovery, Amazon, and Netflix. In the next section, we validate our results against external survey ratings and test the reliability of our findings. Survey ValidationTo validate the effectiveness of our methodology, we compare the network ratings from our automated approach with directly elicited survey ratings. The survey was conducted through Amazon Mechanical Turk, which is a reliable source for conducting social sciences research ([11]). The survey respondents were asked to report their income, age, and gender to account for any demographic influence in the sample. The participants were required to be located in United States and be over 18 years old. To ensure high-quality responses, a prior task approval rate of 95% was required for all survey respondents. The brands were grouped by sector, and four separate surveys, consisting of 250 participants each, were conducted to validate the brand–category and brand–brand connections of beer and automotive brands. Next, we discuss our survey findings along with several robustness checks. Validating Brand–Brand ConnectionsIn this subsection, we examine whether the cobranding candidates identified by the network are also perceived by consumers to be such candidates. The network edge weights between brands are intended to reflect consumers' perceptions of the brands that could be paired for cobranding; thus, such a relationship should be reflected in the survey responses. For this validation, we select the five most-followed brands in the beer and automotive categories. Then, for each brand, we select nine cobranding candidates: ( 1) the top three cross-category cobranding opportunities (i.e., brands), as identified by the network, ( 2) the top three most-followed brands in the sample that are not included in part 1, and ( 3) three randomly drawn brands that are not included in parts 1 and 2.For each focal brand, we ask the respondents to rate its cobranding candidates on a scale of 1 (""less likely to go together"") to 10 (""highly likely to go together"") according to how strongly they can be paired with the focal brand. The survey scores were then correlated with the outgoing edge weights from the focal brands to their cobranding candidates in the brand network. For every survey question, the brand order was randomized, and attention filters were included to identify invalid responses. To identify loyal fans, participants were separately requested to select their favorite auto and beer brands from the list. Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsTable 2 shows the Pearson correlation coefficients between the survey and network scores. Overall, the survey measures correlate well with the network estimates, with the survey's top-three-box score[12] achieving an average correlation of.67 with the network estimates. The overall correlation between average survey ratings and network constructs is.65.GraphTable 2. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryBrandsr (Mean)r (Top Three Box)AutomotiveTesla.89.88Mercedes-Benz.63.54BMW.76.76Audi.93.93Ford.62.52BeerMiller Lite.64.66Sierra Nevada.53.55Bud Light.56.51Budweiser.53.55Coors Light.57.57Average correlation coefficient (r).67.65 In addition, we compute the correlations between network scores and the survey ratings of users who rate a specific brand as their favorite. When using only data from fans, the overall correlation coefficient increases to.70 for average survey ratings and.71 for the top-three-box survey ratings. Finally, we examine the scatter plots more closely to better understand the circumstances in which the network cobranding candidates align well with the survey responses. For example, Figure 9, Panel A, shows the cobranding candidates for Audi as suggested by the network, together with the corresponding survey ratings. The top three cobranding candidates suggested by the network (i.e., Microsoft, Nike, and Intel) also receive very high ratings from the survey respondents. Brands with low network connectivity with Audi (i.e., Lays, Forever21, and ABC) also receive lower ratings from the survey respondents. These results reaffirm the previous findings that network connectivity patterns between brands are useful marketing metrics for consumers' perceptions of which brands are likely to pair well together.Graph: Figure 9. Network versus survey estimates for brand–brand connections of Audi and Budweiser.Figure 9, Panel B, shows the cobranding candidates for Budweiser, as suggested by the network and survey responses. Some top cobranding candidates suggested by the network, including the NFL and Pepsi, also receive high ratings from the survey respondents. However, despite its strong connection to Budweiser in the network, Starbucks receives low ratings from the survey respondents. Although the strong network connectivity between Starbucks and Budweiser is not directly perceived by survey responders, it may indirectly reflect the complementary taste interests of coffee and beer drinkers, which Starbucks previously leveraged to launch a line of beer-like coffee drinks ([26]; [38]). Similarly, Budweiser has a lower network connectivity score with the soccer brand FIFA than the corresponding survey rating. On further investigation, whereas Budweiser's U.S. Twitter account, which was used in our survey validation, has low network connectivity with FIFA, the brand's global Twitter account is very strongly connected to FIFA. This suggests that brand managers should apply domain knowledge and managerial judgment to explore alternative cobranding candidates based on their market of interest. Using domain customization, brand managers can query the brand network to include brand accounts that best suit their target market and conduct a more tailored network analysis to identify where the proposed cobranding opportunity may work well.The preceding analysis also highlights some of the limitations of survey-based validation. First, consumer responses to direct questions on brand perceptions are based on the respondents' existing notions of brand extendibility and may be confounded by prior user experiences ([25]). Second, asking consumers about their overall perceptions using direct rating scales may not reveal novel or unique brand extensions or cobranding ideas; rather, it may simply facilitate the testing of known concepts ([ 5]). The brand network, in contrast, leverages the cofollowership patterns of millions of Twitter users across a broad brand ecosystem to reveal cross-category cobranding ideas that may not be intuitive to consumers but, in hindsight, are effective. Overall, with an average correlation of.71 between network scores and fans' survey ratings, the validation results suggest that the automated network-based approach enables managers to quickly and inexpensively identify cobranding and brand extension ideas that would otherwise be difficult to anticipate. Validating Brand–Category TranscendenceNext, we validate whether the transcendence measures derived from the brand network align with consumers' perceptions of the brands. For both beer and automotive brands, consumer ratings along the luxury and technology categories were elicited. On a scale of 1 (""least likely"") to 5 (""most likely""), participants were asked to rate the focal brands (e.g., Heineken) according to how strongly they associated them with a new category (i.e., luxury goods and technology). Further, to identify brands with strong centrality within their own group, participants were asked to rate the focal brands, on a scale of 1 (""least likely"") to 5 (""most likely""), according to how strongly they believed them to be central in their own category. Finally, the average survey ratings for each brand across the luxury and technology categories are compared with the brand transcendence constructs obtained using the network measures. Similarly, the average survey rating for centrality is compared with the centrality construct obtained using the network measures. We also calculate the top-two-box score for each brand to assess the proportion of people who rate a brand very highly (i.e., a score of 4 or 5). Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsThe Pearson correlation coefficients between the survey results and network constructs are listed in Table 3. Overall, the survey measures correlate well with the network estimates, with the top-two-box survey scores and mean survey scores achieving an average correlation of.63 with the network estimates.GraphTable 3. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryConstructsr (Mean)r (Top Two Box)AutomotiveTranscendence (luxury).59.61Transcendence (technology).66.67Centrality.58.61BeerTranscendence (luxury).52.53Transcendence (technology).72.69Centrality.71.70Average correlation coefficient (r).63.63 Figure 10 includes the scatter plots for survey versus network measures. Given that the network and survey estimates are measured in different units, the plots have been scaled to 0–1 for easier interpretation. Overall, the scatter points are well distributed along the best fit line, with few outliers. As we discuss next, we then conduct a series of tests to ensure that the network accurately captures the shifts in connections over time. Overall, our general findings pass these tests, supporting the future use of implicit brand networks in marketing research.Graph: Figure 10. Scatter plots of network versus survey estimates for automotive and beer brands. Shift in connections from 2017 to 2020The ""Results"" subsection discusses the brand network's ability to capture shifts in brand transcendence over time. We now test whether the waning of certain connections between 2017 and 2020 in the network is supported by the survey responses. To do this, we first identify the connections between brands and categories that exist in the 2017 network but decline in the 2020 network. Figure 11 illustrates the filtered cases for automotive brands and the corresponding survey results. Panel A shows that brands such as Mazda, Mini, Buick, Chrysler, and GMC all have connections with the luxury category in 2017. According to [17] in Forbes, at the time, the Mazda 2017 CX-9 Signature model was considered the most luxurious vehicle produced by Mazda to date. The author mentions that ""Mazda has never been considered a luxury brand, but maybe it's time to reconsider that classification"" ([17]). However, the results from the brand network in 2020 show that Mazda does not retain its connection with luxury. This is further validated by the survey participants, who also rate Mazda as very weakly associated with luxury. There is a similar pattern in Panel B, in which brands such as Dodge, Chevrolet, Jeep, Honda, and Chrysler show a significant drop in their connections with technology category between 2017 and 2020. This change is also reflected in the survey responses.Graph: Figure 11. Shift in transcendence of car brands from 2017 to 2020. Random connections rejected by survey participantsNext, we test whether survey respondents also reject random connections that do not exist in either 2017 or 2020. To do so, we filter the cases where connections between brands and categories are absent in both the 2017 and 2020 networks and compare them with the survey responses.[13] We find that the average survey ratings are below 2 (out of 5) for most brand–category connections not existing in either network (i.e., 2017 and 2020). Addressing demographic bias on TwitterStudies that mine brand perceptions from social media sources must consider the extent to which brand followers on social media represent the general population. It is also important to consider whether certain Twitter brands accounts are more appealing to a specific audience (e.g., young people, men). Recent studies have reported that Twitter followership data successfully captures attribute-specific consumer perceptions beyond demographic similarities ([11]). We investigate this issue further by comparing the survey ratings, which were provided by users of different demographics, with the transcendence values obtained from the brand network. Table 4 lists the correlation values between the network estimates and income-specific survey ratings for the transcendence of automotive brands into the technology category. For most income groups in the survey, there are adequately high correlations with the network transcendence constructs. Results for all the remaining survey demographic groups (i.e., age and gender) are included in Web Appendix D. We observe adequately high correlations between the demographic-specific survey ratings and the brand network constructs. This affirms that the overall brand network estimates are not heavily influenced by the demographics of Twitter users.GraphTable 4. Income-Specific Survey Correlations with the Network Constructs. Automotive Brands' Transcendence to TechnologyIncomeSurvey-Based Measure (≤$29,999)Survey-Based Measure ($30,000–$59,999)Survey-Based Measure ($60,000–$99,999)Survey-Based Measure ($100,000–$149,999)Survey-Based Measure (≥$150,000)Survey-based measure (≤$29,999)1.00Survey-based measure ($30,000–$59,999).961.00Survey-based measure ($60,000–$99,999).97.981.00Survey-based measure ($100,000–$149,999).93.96.941.00Survey-based measure (≥$150,000).93.91.94.881.00Network-based measure.67.62.72.59.74  Sensitivity of transcendence constructs to network rewiringThe presence of Twitter bots may inflate the number of common followers between brands, which can, in turn, lead to inaccurate network estimates of transcendence and centrality. In this section, we conduct multiple network simulations by repeatedly rewiring the edges to test whether the original brand network structure remains reasonably stable. We incrementally rewire the cofollowers from any random pair of edges and reperform the entire analysis. As Figure 12 shows, the rewiring stage involves the addition and removal of 5% of the cofollowers of any random pair of edges in the network, continuing until 50% of the cofollowership patterns have been altered. In each iteration, once the network rewiring is complete, the algorithm reruns the entire analysis (i.e., it applies the disparity filter to identify statistically significant edges, normalizes the edge weights, and calculates the transcendence across categories).Graph: Figure 12. Process flow for each iteration.Figure 13 illustrates the results of the simulations for the automotive brands and compares the transcendence values obtained after rewiring the network with the original values. The purpose of the test is to ensure that the original network estimates hold for small rewiring changes (i.e., that significant rewiring is needed to yield completely different network estimates). For all plots, the rewired network estimates correlate highly with the original estimates until a large percentage of the network (>30%) is rewired. This demonstrates that the brand network structure is not sensitive to small underlying changes that may occur due to bots.Graph: Figure 13. Correlation of post rewiring transcendence values with original transcendence values for automotive brands. ConclusionDespite its relevance to various marketing decisions (such as cobranding and brand extensions), the identification of cross-category insights across a broad brand ecosystem is currently understudied in the marketing literature. This article uses implicit brand networks to identify the category-specific connections of brands and their competitors by exploiting the overlap in brand followers on Twitter. We introduce a new construct, transcendence, that measures the extent to which a brand shares cointerest with other brands in different categories. Depending on a firm's marketing objectives (i.e., their focus on extensions vs. cobranding), the transcendence of a brand can be studied at different levels: brand–category or brand–brand. These different levels of analysis can help managers identify viable cobranding opportunities.Furthermore, we leverage the concept of asymmetry between brand pairs to provide more nuanced insights into possible cobranding opportunities and determine which brand can potentially benefit more from a cobranding alliance. We conducted the analysis over time to track shifts in brand transcendence, allowing brand managers to both assess the effectiveness of existing marketing strategies and identify new alliance opportunities. To ensure the reliability of our proposed methodology, we validate our findings against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by Twitter bots.From a methodological standpoint, the implicit brand networks utilized in this article condense the high-dimensional interest space of millions of brand followers into a parsimonious form that is more amenable to research and business applications. The readily accessible artifact, which is obtained with little human intervention in the processing of the underlying data, allows managers to efficiently infer cross-category branding insights in a scalable way. Compared with extant digital approaches that rely on extensive preprocessing, this straightforward automated approach enables practitioners to readily obtain the cointerest patterns of brands with respect to their competitors and gauge the types of users that their competitors attract. More specifically, given its automated data collection and network analyses, the brand network can act as an effective business intelligence tool for the identification of cobranding and extension opportunities across a broad ecosystem of brands.Overall, our approach offers several benefits to marketers. It also highlights avenues for future research. First, although our analyses use Twitter brand communities, it would be interesting to compare similar communities on Facebook and Instagram. Brand networks on different social media platforms may vary based on factors such as user demographics, category, platform characteristics, or a brand's marketing strategy. Although consistent brand connections across different platforms can provide additional validity to findings of this study, meaningful insights may also be gleaned if substantial differences are observed. Such differences may, for example, stem from a brand's tailored marketing efforts on a specific platform. Using brand networks to track the effectiveness of such efforts can be beneficial to brand owners. Differing user demographics across platforms may also have an impact on brand network structures. Though this study did not identify substantial differences between the demographic-specific survey ratings and the transcendence values obtained from the brand network, future research could examine platform-specific brand networks to obtain richer insights. Second, future research could consider how to distinguish the content on brand pages that may affect consumers' decisions to follow brands, including promoted content on a brand's page, multichannel advertising across platforms (e.g., email, Facebook), and the use of trending topics or sponsored tweets.Third, the analysis in this article relies on a brand's followers at a given point in time. Twitter does not provide data on when a user starts or stops following an account. The article's analysis of two different periods highlights the potential for our method to examine how transcendence changes over time. Because most aspects of the data collection and network analyses in this approach can be automated, brand managers could collect followership information at more regular intervals to examine changes in transcendence more frequently. Fourth, while our study relies on validation from two categories, future studies can consider expanding the survey-based validation for broader set of brands across multiple categories. However, for such validation, it is important to consider that certain cross-category cobranding candidates, revealed through the cofollowership patterns on social media, may not always be intuitive to survey respondents, though in hindsight they make sense and work. In conjunction with the brand network results, marketers should apply domain knowledge and managerial judgment to explore different extension and cobranding opportunities that may work best for the brand.Fifth, it is also important for brand managers to consider that Twitter users from around the globe are free to follow any account(s) of a brand (which can include global or country-specific accounts). As the data collection and network analyses can be largely automated, marketers can create custom brand networks to include Twitter brand accounts (country-specific and/or global) that best suit their market of interest. Domain customization can help managers conduct a more targeted network analysis of where the proposed cobranding opportunity may work best. Lastly, although our approach relies on cofollowership patterns to identify cobranding opportunities, we do not investigate the drivers of common followership on Twitter and the extent to which these drivers lead to network overlap between brands. The reasons that users cofollow brands on Twitter are varied and complex, with many unobservable factors possibly at play. Industry research by Nielsen ([29]) indicates that 55% of Twitter users say they follow a brand because they like it, followed by 52% of users who want to keep up-to-date on the latest promotions and offers posted by the brand. There are various other reasons that users cofollow multiple brands on social media; thus, future research could use the brand network described in this article to investigate and better understand the drivers of cofollowership between brands on social media.Overall, this work offers a new approach for researchers and practitioners interested in automatically monitoring cross-category brand connections over time. Network-based methods for brand management are relatively new and present many opportunities for future research. The methods introduced in this article provide a foundation for marketing researchers interested in leveraging implicit brand networks to gain richer insights into consumers and brands. "
28,"Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation Behavior Charities are constantly looking for new and more effective ways to engage potential donors in order to secure the resources needed to deliver services. The current work demonstrates that creative activities are one way for marketers to meet this challenge. Field and lab studies find that engaging potential donors in creative activities positively influences their donation behaviors (i.e., the likelihood of donation and the monetary amount donated). Importantly, the observed effects are shown to be context independent: they hold even when potential donors engage in creative activities unrelated to the focal cause of the charity (or the charitable organization itself). The findings suggest that engaging in a creative activity enhances the felt autonomy of the participant, thus inducing a positive affective state, which in turn leads to higher donation behaviors. Positive affect is demonstrated to enhance donation behaviors due to perceptions of donation impact and a desire for mood maintenance. However, the identified effects emerge only when one engages in a creative activity—not when the activity is noncreative, or when only the concept of creativity itself is made salient.Keywords: autonomy; creativity; donation; positive affectCharitable organizations exist to support a wide variety of causes, such as helping malnourished children, caring for the homeless, supporting animal welfare, and meeting environmental concerns, to name a few. The success of these organizations in supporting their causes largely depends on the donations they secure. According to the [58], approximately 1.56 million registered nonprofit organizations exist in the United States, together raising an estimated $390 billion in donations annually. Despite these large numbers, fundraising remains a major challenge for such organizations, with approximately 45% of charities unable to secure the required level of resources needed to deliver their services ([59]).In light of this, it is not surprising that marketers at these organizations seek more effective ways to solicit donations, often utilizing nontraditional approaches and fundraising events (e.g., ice-cream socials, silent auctions, trivia nights) to engage potential donors ([11]). For example, in 2014, the ALS Association invited people around the globe to participate in its ""Ice Bucket Challenge"" to increase awareness of ALS, raising approximately $220 million from individual donors in the process (Holan 2014). Bloodwater.org devised the ""Real Game of Thrones"" campaign, which called on people to participate through Twitter and used a combination of pop culture, humor, and bathroom puns to raise money to build latrines throughout Africa. This creative campaign successfully raised enough money in 24 hours to build 21 latrines in Rwanda. Cookies for Kids, another charitable organization, sponsors creative charity events each year such as cookie swap parties, where participants decorate cookies and swap recipes to raise donations. These fundraising anecdotes suggest that charities are defining new ways of engaging potential donors, while raising questions about which types of activities most effectively enhance donation behaviors.The current work meets this challenge by examining how engaging potential donors in creative activities can positively influence their propensity to donate money to a charitable cause. We argue that engaging in a creative activity induces a positive affective state, which in turn increases both the likelihood and amount of monetary donation made to the charitable organization. While prior work has independently examined links between creativity and positive affect (e.g., [ 9]; [47]), as well as between positive affect and helpfulness (e.g., [ 2]; [ 3]), we provide a deeper understanding of why and how creativity leads to enhanced donation behaviors. Specifically, we show that the link between creativity and positive affect is driven by the sense of autonomy that is induced by engaging in a creative activity (i.e., an attempt to create something novel). Further, by identifying the roles that desire for mood maintenance and perceived donation impact play, we provide insight into why the positive affect resulting from a creative activity leads to enhanced donation behavior.The current research makes several important contributions. From a practical perspective, this research offers a simple and effective way for marketers to improve their donation appeals; it suggests that engaging potential donors in a creative activity enhances subsequent donation behavior. This recommended approach provides a real opportunity for charity marketers to increase the efficacy of their fundraising campaigns. At the theoretical level, the present work advances the marketing and charity literature streams in several ways. First, we demonstrate the positive effect of creative engagement on donation behavior. To our knowledge, no research thus far has examined whether and how engaging in creative activities can impact an individual's subsequent donation behavior toward a charitable organization. Second, we explicate the reasons and conditions that drive the relationship between creativity and donation behavior. We demonstrate that it is the act of actually engaging in a creative activity—rather than simple priming or making the concept of creativity salient—that drives the effect. Further, while prior work has consistently shown that a sense of autonomy can facilitate creativity (e.g., [18]), we demonstrate that engaging in a creative activity also heightens one's sense of autonomy, which in turn induces positive affect. As noted previously, we also highlight that the positive affect experienced during a creative activity bolsters desire for mood maintenance and perceived donation impact, thereby enhancing the likelihood of donation and the monetary amount donated. Finally, we find evidence that the positive effect of engaging in a creative activity on monetary donation is context independent. That is, engaging potential donors in a creative activity not directly related to the charitable cause or organization still has a positive influence on their donation behavior. Thus, the current work not only offers marketers a way to build effective donation campaigns but also provides a deeper theoretical understanding of the relationship between creativity and donation behavior. Conceptual Framework Donation BehaviorResearchers have explored many facets of donation behavior, from the demographic and socioeconomic determinants of donation ([ 6]; [10]; [40]) to the extent to which other factors—such as motivation, psychological characteristics, and social cognition—can affect donation ([34]). In addition, prior research has proposed and examined various marketing strategies and tactics used to increase donations. For example, using public recognition ([73]), taking advantage of price promotions ([78]), designing more attractive appeals ([51]), expressing one's identity ([61]), and using positioning to enhance the effectiveness of the charity ([74]) have all been investigated.More relevant to the current research, recent work has also started to examine the merits of engaging potential donors in different types of activities and tasks before soliciting donations. For example, [62] examined the influence of a storytelling event in the crowdfunding context, finding that direct (vs. indirect) storytelling positively affects customer engagement and donation likelihood. In contrast with more traditional donation requests ([69]), some charities are utilizing physical activities (e.g., walks and runs [[37]], sporting events [[36]], silent auctions [[41]], ice cream socials, trivia nights [[11]]) as precursors to the donation solicitation. Despite the initial academic interest in these tactics, the effectiveness of such approaches has been understudied in the literature, and reporting has shown mixed results. For example, while [37] have argued that positive fundraising outcomes result from physical activity events (e.g., running activities, golf tournaments), [75] did not find a positive relationship between sports activities and charitable event outcomes. The current work aims to add to the literature in this regard by validating the use of activities to increase the likelihood and amount of donation contributions. Specifically, we examine the impact of engaging potential donors in creative activities. Creative Engagement, Autonomy, and Positive AffectActivities involving creation of an output span a continuum ranging from routine tasks, such as simply copying a given design, to highly creative activities, such as creating an original work of art ([18]). Within this context, we argue that the inherent characteristic of creative engagement, which is differentiated from priming or simple salience of creativity and/or creativity-related concepts, is that an individual must engage, physically or mentally, in an activity requiring the production of something novel (i.e., the activity leans to one side of the continuum referenced previously). For example, actively generating an original cookie design would lead to creative engagement, but copying a cookie design or simply being primed by the concept of creativity (e.g., through exposure to creative stimuli) would not.Importantly, we propose that engaging in a creative activity induces positive affect for the creator. In support of this notion, liberal arts literature finds that engaging in creative activities to generate novel outputs (e.g., music composition, visual arts, creative writing) can bring about positive thoughts and feelings ([66]). Relatedly, [ 9] show that engaging in a divergent creative task induces higher levels of positive affect. Results reported in the psychology literature also support these findings. [16], while explicating the construct of flow, interviewed people who engage in creative work on a regular basis (i.e., artists and musicians) and found that these individuals often experience positive affect and happiness when creating something original. [15] confirm these findings in an experimental lab setting and show that engagement in a creative activity induces a state of flow, leading to higher positive affect.Why does engaging in creative activities lead to positive affect? One potential driver of this positive relationship is a heightened sense of autonomy (i.e., having a sense of choice and freedom from external control; [24]; [52]; [63]), attained by engaging in a creative activity.[ 5] By definition, an attempt to generate a creative output requires one to actively recognize remote associations between broad and distant concepts and then combine these loosely connected ideas and concepts in a novel fashion ([20]; [31], [32]; [69]). Such a process requires and encourages one to think freely and make different combinations and choices without being constrained by norms and rules ([ 4]; [30]; [43]). Thus, the process associated with creative generation should manifest a sense of choice and freedom (i.e., autonomy), which we contend induces positive affect.Prior work offers initial support for this proposition. As we have discussed, engaging in a creative activity induces a state of flow, which then leads to higher positive affect ([15]); notably, empirical work has shown the state of flow to be associated with a sense of autonomy ([49]). Similarly, [18] found that being involved in a creative activity can enhance experienced enjoyment, but only when the activity imparts a sense of autonomy. [47] conducted a daily-diary study following the routine of 1,042 hobby musicians and found that the participants reported higher positive affect on the days they engaged in music composition and performance. Importantly, the authors found that this relationship was driven by satisfaction of one's needs for autonomy. Finally, [46] found that creative generation, such as the production of visual art, significantly reduced cortisol levels (a biomarker and proxy measure of stress in humans) and increased feelings of relaxation, pleasantness, and enjoyment. Their work shows that such art making is associated with the experience of being free from constraints (i.e., the sense of autonomy).Given this discussion, we argue that engagement in creative activities heightens one's sense of autonomy, which in turn leads to positive affect. We further propose that the positive affect induced by participation in a creatively engaging activity will lead to enhanced donation behavior. We elaborate on this prediction in the following subsection. Positive Affect and Donation BehaviorFindings reported in the extant literature offer compelling evidence that being in a positive affective state enhances donation behavior ([ 1]; [17]; [26]; [42]; [44]; [60]). While prior work has consistently demonstrated a positive relationship between positive affect and donation behavior, it offers disparate explanations for this relationship ([ 7]). Indeed, we recognize that the relationship between positive affect and donation behavior is likely to be multiply determined, and we therefore identify three mechanisms that are most relevant to the context of creative engagement in question.Perhaps the most common explanation for the clear link between positive affect and helping behavior derives from the mood maintenance model. This line of reasoning proposes that people tend to maintain positive mood states ([ 7]; [33]). Thus, individuals tend to help more when in a positive affective state, because doing so enables them to prolong said state ([12]; [45]). In the context of our work, this suggests that the positive affect realized by participating in a creative activity can best be maintained when a subsequent behavior, such as helping others through enhanced donation behavior, also fosters positive feelings ([ 7]).A second potential mechanism derives from the social aspects associated with positive affect. Indeed, it has been argued that being in a positive affective state can directly influence one's perceived social connectedness ([38]; [39]). As such, the positive affect defined by participation in a creative activity is likely to enhance the value of creating and maintaining social connections ([13]; [25]). Further, valuing social connections has been shown to enhance feelings of care and concern toward others ([ 8]), which should subsequently enhance the donation behaviors of the individual.Finally, prior work reports that positive affect can also boost self-efficacy and/or the perceived impact of one's actions ([ 5]; [64]). That is, experiencing positive affect can lead to the belief that one's actions are more efficacious, thus creating a heightened expectancy of positive outcomes ([53]). Thus, we contend that positive affect can enhance the perceived impact of one's potential donation and, in turn, raise the likelihood and amount of one's donation behavior ([14]). In our empirical work, we explicitly test the proposed chain of effects (i.e., engagement in a creative activity → autonomy → positive affect → enhanced donation behaviors), and the three aforementioned potential mechanisms underlying the impact of positive affect on donation behavior. Summarizing our arguments, we hypothesize the following: H1:  Engaging in a creative activity (vs. an activity that does not provide an opportunity for novel creation) leads to enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated). H2a:  The relationship between engaging in a creative activity and donation behavior is serially mediated by autonomy and positive affect. H2b:  The influence of positive affect on donation behavior is driven by (a) mood maintenance, (b) social connection, and/or (c) perceived donation impact. Overview of StudiesWe utilized a combination of field studies and controlled lab experiments to test our hypotheses. First, we conducted a pilot study in collaboration with a nonprofit organization as an initial test of our focal hypothesis and found support for the prediction that engaging in a creative activity enhances donation behavior (H1). Study 1, conducted in a controlled lab setting, replicated the initial pilot study findings, thus reconfirming support for H1. Study 2, a quasi-field experiment, shows that engaging in a creative activity increases both the likelihood and amount of monetary donations, whereas simply priming the concept of creativity does not (H1). Study 3 tested the full serial mediation prediction (H2a) by demonstrating that creative engagement induces a sense of autonomy, which in turn heightens positive affect, leading to higher donation behavior. Study 4 tested H2b, showing that the path from positive affect to donation behavior is indeed multiply determined. In every study, we report all experimental conditions and measures as collected and disclose any eliminated data points when applicable. The sample size was predetermined for each study based on current experimental norms but varied within an acceptable range depending on actual participant sign-ups. Study 3, a supplementary study (follow-up to Study 3 reported in the Web Appendix), and Study 4 were preregistered on aspredicted.org (see respective studies for details). Pilot Study: Creativity and Monetary Donations—A Field ExperimentTo gain an initial understanding of whether engaging in a creative (as compared with neutral) activity enhances monetary donation, we collaborated with a registered U.S. nonprofit organization operating an animal shelter in a small city in the Southwestern United States (population 46,000 according to 2010 U.S. Census data). Every year, employees of this charity produce T-shirt designs that are printed and used as giveaways or sold in fundraising activities. To test our focal hypothesis, the charity agreed to open the T-shirt design activity to the public and use it as a fundraising event. The T-shirt design campaign was launched by the charity via its social media platform, inviting members of the public (i.e., potential donors) to create T-shirt designs as part of a donation appeal. The charity had set two overarching guidelines for this T-shirt design campaign: ( 1) the submitted designs were to follow the theme of ""Rescue 2020"" and ( 2) the charity's logo had to be part of the design. The charity managed the entire event.Relevant to our prediction (H1), we manipulated the opportunity to be creative (vs. not) within the T-shirt design campaign. While participants in both conditions had to develop a T-shirt design reflecting the charity's yearly theme and including its logo, those in the creativity condition were invited to develop an innovative T-shirt design and were explicitly instructed to be creative while doing so. Those in the neutral condition were not specifically instructed to be creative. Once participants submitted their designs, the charity presented them with a donation appeal that included a link to the donation page. At the end of the campaign period, the charity forwarded us the designs along with the corresponding donation amounts, having removed donors' identifying information (for additional methodological details, see Web Appendix A.1).To examine the relationship between creativity and donation behavior, we first coded the participants who did not donate as 0 and those who donated (any amount greater than zero) as 1. We then conducted a binary logistic regression analysis testing the effect of engaging in a creative T-shirt design activity on donation rate (i.e., the percentage of participants who donated to the charity). We found that a significantly higher percentage of people in the creativity condition (34.48%) donated, as compared with the percentage of people in the neutral condition (12.20%; χ2 = 4.97, p = .026). Next, we examined the effect of creative engagement on the amount of money that was donated. A Shapiro–Wilk test ([65]) indicated that the donation amount data was not normally distributed (p <.001). Thus, in accordance with prior research ([57]; [61]), we used a nonparametric Mann–Whitney U test for the analysis. We found that the average donation amount made by participants in the creativity condition (M = 7.07, SD = 13.06) was significantly higher than that of participants in the neutral condition (M = 1.10, SD = 3.26; U = 739.50, p = .016; for additional results, see Web Appendix A.2).By collaborating with a registered charity and assessing actual donation behaviors, we found initial evidence showing that including a creative activity as part of a donation appeal can be an effective approach to enhance donation behaviors (i.e., both higher donation rates and amounts). Interestingly, one could argue that the creativity of the generated output (i.e., the T-shirt design) may also have impacted the donation behavior. To test this alternative explanation, we asked two trained research assistants (employed within the domains of creativity and advertising, respectively) to rate each T-shirt design on its creativity (1 = ""not at all creative,"" and 7 = ""very creative""). Both raters were blind to the conditions and hypothesis. Validating our manipulation, a one-way analysis of variance (ANOVA) showed that the designs produced in the creative condition (M = 3.62, SD = .80) were rated as significantly more creative than those in the neutral condition (M =��3.05, SD = .98; F( 1, 68) = 6.72, p = .01). However, we did not observe a significant relationship between rated creativity of the generated designs and the donation rate (B = .04, t < 1) or the donation amount (B = .83, t < 1). We conducted a similar analysis using only the creative condition, where natural variability in output creativity may occur (even though everyone was instructed to be creative). Again, we did not find a significant relationship between the creativity of the generated designs and the donation rate (B = −.15, t = −1.34, p = .19) or the donation amount (B = −3.07, t < 1). Importantly, our conceptualization argues that it is the act of engaging in a creativity activity that leads to enhanced donation behavior, not the level of creative output achieved. Subsequent studies replicate this finding, consistently demonstrating that creative engagement itself, rather than the creativity of the generated outcome, positively impacts donation behavior (for brevity, these findings are reported in the respective Web Appendices).This pilot study provided initial evidence of the proposed effect, but as a real-life field study conducted in collaboration with a third party, it is not without limitations (dependence on the charity's social media platform for sampling, the messaging guidelines set by the charity, etc.; for discussion, see Web Appendix A.3). As such, our first study aimed to replicate these initial findings observed in the field in a more controlled lab setting (i.e., provide a more robust test of H1). Study 1: Creativity and Monetary DonationsStudy 1 used a donation context inspired and adapted from a real-life social enterprise known as Elephant Parade. This organization invites everyday consumers to create/paint their own elephant toy using an ""Artbox Kit"" (containing a small white clay elephant and a variety of colors) in return for a monetary donation. The proceeds are subsequently used for elephant welfare and conservation projects worldwide. MethodEighty-nine undergraduate students (49 women; Mage = 20.04 years, SD = 1.27 years) at a large North American university completed this study in exchange for course credit. To begin, participants were checked in and assigned to a designated computer desk, each of which was equipped with a small donation box (see Web Appendix B.1) and a white envelope containing $2 in quarters (i.e., eight quarters). The donation box was labeled with an Elephant Parade sticker and had a slit on the top. Four quarters were left in each donation box, creating the impression that the study administrator would not be able to tell if the participant donated or not, thus reducing any demand effects and obligation to donate. Participants were told that, in addition to the course credit, they would receive $2 (in an envelope on their desks) as a token of appreciation for their participation.The experiment adopted a one-way design in which participants were assigned to complete either a creative or neutral activity, randomized by session (i.e., we ran only one condition per session). A drawing activity (inspired by Elephant Parade's clay elephant painting) was used to induce the focal manipulation. Participants were told that the researchers wanted to put their minds at ease before the study commenced and would therefore like them to engage in a coloring activity. All participants were given a sheet of paper with a picture of an elephant (see Web Appendix B.2) and asked to color it. Those in the creative condition received a box of Crayola markers in ten different colors and were told to be as creative as possible while coloring and decorating their elephant pictures. They were also told to use any number and variety of colors they liked for the task. In contrast, those in the neutral condition received gray crayon markers only, and were told to simply color the elephant picture. In both conditions, participants were asked to spend no more than five minutes on the coloring activity. The elephant coloring task, though based on a real-life activity (i.e., Elephant Parade donation protocol), mimics a widely used creativity task in the literature: the alien task ([72]). In these types of activities, creative thinking encourages people to violate standard characteristics of a stereotypical object (e.g., an elephant, as in our study; [48]; [50]). A stereotypical elephant picture would be colored gray, whereas a nonstereotypical elephant picture would be multicolored.Next, participants were presented with the donation opportunity and informed that the researchers were helping raise money for the nonprofit organization Elephant Parade. Participants read a short description and donation appeal from Elephant Parade (see Web Appendix B.3), and were asked if they would like to contribute; they could donate any amount of the participation money (eight quarters) they wished, and put it in the donation box. The number of quarters each participant donated served as the key dependent variable. Finally, all participants provided their demographic information (age and gender) and were debriefed before being dismissed. (Gender and age were captured in all studies. However, no effects were observed for these variables in either this or any other study. For the sake of brevity, we do not discuss them further.) After each session, the research assistants removed the quarters from the donation boxes and recorded the number of donated quarters (i.e., the total number of quarters in the box minus the four quarters initially placed in each donation box). Results Preliminary analysesThe elephant designs were rated on creativity (1 = ""not at all creative,"" and 7 = ""very creative"") by a research assistant who was blind to the conditions and hypothesis (for sample designs, see Web Appendix B.4). A one-way ANOVA confirmed that the designs produced in the creative condition (M = 4.24, SD = 1.61) were significantly more creative than those produced in the neutral condition (M = 1.51, SD = .95; F( 1, 87) = 97.41, p <.001). Donation behaviorsFirst, we explored whether there was any difference between the two conditions on the donation rate (i.e., the percentage of participants who donated to the Elephant Parade foundation) and found that a significantly higher percentage of participants (80.95%) in the creative condition—as compared with those in the neutral condition (55.32%)—donated (χ2 = 6.83, p = .009). Next, we analyzed the effect of activity type on donation amount, which was assessed by the number of quarters donated to the Elephant Parade after completing either the creative or neutral activity. The donation data did not meet the normal distribution criteria (Shapiro–Wilk test: p <.001; [65]); therefore, a nonparametric Mann–Whitney U test was again used for the analysis. We found that those who completed the creative activity (M = 4.50 quarters, SD = 3.19) donated a significantly higher number of quarters than those who completed the neutral activity (M = 2.98 quarters, SD = 3.54; U = 720, p = .022). DiscussionThe obtained results provide support for our focal hypothesis (H1), namely, that engaging in a creative activity enhances donation behavior, in terms of both the likelihood of donation (i.e., donation rate) and the donation amount. The study utilized a creative activity adapted from a real-life charity and assessed donation behavior through real monetary donations. It demonstrated that engaging potential donors in creative activities, before soliciting them for donations, can be an effective way to enhance donation behavior.One potential criticism of this study could be the different number of colors provided in the creative versus neutral conditions. However, this procedure was necessary to manipulate creativity within the context of the study. Offering a variety of colors provides participants with an opportunity for creativity, that is, to think outside the box and beyond the stereotypical characteristics of an elephant (i.e., all gray). The sole use of gray crayon markers in the neutral condition, in contrast, conforms to the stereotypical characteristics of an elephant and curtails creative opportunity. To address this potential limitation, in future studies we adopt contexts in which we can provide the same materials to participants in both conditions.In both the pilot study and Study 1, the creative activity was directly related to the charitable cause, thereby raising a question about the generalizability of the effect—that is, whether the observed effect is domain specific or whether it holds when the creative activity is independent of the donation context. We explore this possibility in Study 2. In addition, our studies do not delineate whether the obtained results were observed because participants engaged in a creative activity (as hypothesized), or simply because the concept of creativity was salient for the participants in the creativity condition. In other words, is it necessary to actually engage in a creative activity, or can mere exposure to the concept of creativity also enhance donation behaviors? Prior research has shown that priming creativity (making creativity salient without engagement) can influence cognitive processing, thereby affecting people's propensity to engage in dishonest behaviors ([27]). We examine this question in the next study. Study 2: Creative Stimuli Versus Creative ActivityStudy 2 was aimed to discern whether engagement in a creative activity is needed to obtain the identified effects, or rather, if exposure to a simple creative prime would suffice. To this end, we added another focal condition to the experimental design used in Study 1: this time participants were exposed to creative stimuli only, with no opportunity to participate in a creative activity. Interestingly, this condition mimics the default strategy of many charitable organizations, in which potential donors are presented solely with a donation appeal (without an opportunity for active engagement). In addition, to test the context-independent nature of the effect, the creativity activity was kept independent from the donation context (i.e., the charitable cause). Finally, we conducted the study in a real-life setting; we followed a format used by baked goods company C. Krueger's, which hosts a holiday charity event wherein customers are invited to decorate cookies and make purchases. For our study, two booths were set up in the lobby of a university building with high foot traffic, featuring large signs advertising a cookie decoration event sponsored by the charity ChildHelp. Passersby were invited to participate in the event and decorate a cookie before being solicited for a monetary donation. MethodOne hundred seventy adults (82 women; Mage = 21.09 years, SD = 2.47 years) agreed to participate in the event and were assigned to one of the three treatment conditions: creative engagement, creative exposure without engagement, or neutral engagement. The conditions were randomized and rotated by the hour. Once passersby agreed to participate in the event, they were told they would receive $2 as a token of appreciation for participating in the event. They were given a white envelope containing eight quarters and asked to sign a form indicating receipt of the money. The signing process was necessary for participants to feel ownership of the money they had earned, before being solicited for donations later in the study. Prior research has shown that signing one's name increases this sense of ownership ([22]; [68]).Next, one of the ""staff members"" guided individual participants to a table bearing a plain cookie on a paper plate, four different icing colors, and a spatula. They were also handed an event participation instruction sheet, which served as our key manipulation. Each instruction sheet had the ChildHelp Foundation logo at the top, with ""ChildHelp Foundation Annual Charity Event"" printed underneath (see Web Appendix C.1). The task manipulation for the two engagement conditions (i.e., cookie decoration) was adapted from [18]. In the creative engagement condition, participants were told that this was an annual charity event hosted by the ChildHelp Foundation and, as part of the event, we wanted them to decorate their cookie in the most creative manner possible using the provided materials. Those in the neutral engagement condition were given a picture of a routinely (i.e., noncreatively) decorated cookie (see Web Appendix C.2) and asked to ice the cookie as shown in the picture, using the provided materials. Those in the creative condition had the freedom to use their imagination and creativity to come up with a novel cookie design, thereby promoting creative engagement. However, in the neutral condition, participants were simply asked to copy the noncreative cookie as depicted, negating any potential creative engagement process. In the creative exposure (without engagement) condition, in keeping with prior research showing that exposure to creative images can make the concept of creativity accessible ([76]), we simply showed participants three creative cookie designs (see Web Appendix C.3) and asked them to choose the most creative one.To assess whether our manipulation made the concept of creativity salient, we conducted a separate online study. In this study, participants were randomly assigned to complete one of the three treatment condition tasks used in the main study (creative engagement, neutral engagement, or creative exposure without engagement). They were then presented with two types of measures that captured the salience of creativity implicitly and explicitly. The obtained results showed that, as anticipated, the concept of creativity was equally salient for both the creative engagement and creative exposure conditions, and both were significantly higher than the neutral condition (for study details and complete results, see Web Appendix C.4).Next, in the main study, all participants were given a manila envelope with a survey featuring a donation appeal and some questions about the cookie event. Each envelope was marked with a unique identification number to enable us to match participants' survey responses, donation amounts, and their assigned condition. All participants were presented with a donation appeal from the ChildHelp Foundation: a nonsectarian, nonpolitical, registered charity dedicated to helping children living in distress in North America and overseas. Furthermore, participants were told that if they decided to contribute, they could put the quarters they wanted to donate in the manila envelope and leave it in the box beside their table. Lastly, to gain initial insights into the underlying process, we captured exploratory measures of participants' positive affective state and their perceived donation impact (for details, see Web Appendix C.5). At the end of the study, the participants in the two engagement conditions were invited to take their cookie with them, while those in the creative exposure without engagement condition were given a cookie at the end of the study (for consistency with the other two conditions). ResultsWe first examined the donation rate by calculating the percentage of participants who donated in each condition. A logistic regression revealed a significant difference in the donation rates across the three conditions (χ2( 2) = 12.26, p = .002). A significantly higher percentage of participants in the creative engagement condition (81.03%) donated, compared with both those in the creative exposure (without engagement) condition (50.88%; χ2( 1) = 11.01, p = .001) and those in the neutral condition (61.82%; χ2( 1) = 4.98, p = .026). We observed no difference between the latter two conditions (p = .244). Next, we assessed the donation amount, with the number of donated quarters serving as our key dependent variable. A Shapiro–Wilk test ([65]) indicated that our data were not normally distributed (p <.001); therefore, we used the nonparametric Kruskal–Wallis test for the analysis. The obtained results revealed a significant overall main effect of the activity type on monetary donation (H( 2) = 9.8, p = .007). Pairwise comparisons showed that those in the creative engagement condition (M = 6.10 quarters, SD = 3.30) donated significantly more quarters than both those who were in the creative exposure (without engagement) (M = 4.04 quarters, SD = 3.97, p = .003) and neutral engagement (M = 4.49 quarters, SD = 3.85, p = .021) conditions. There was no difference between the latter two conditions (p = .52). DiscussionIn this study, we created a charity event in which individuals participated in different activities before receiving a donation solicitation. In line with our predictions, we found that participating in an activity that enabled creative engagement (i.e., creatively decorating a cookie) enhanced donation behavior (as compared with those who either reproduced a routine cookie design or were merely exposed to creative cookie designs). Importantly, the obtained results showed that donation behavior is only enhanced when participants actually engage in a creative activity, not simply when the notion of creativity is made salient. Further, the creative activity utilized in this study was independent of the charity cause, thereby demonstrating the context-independent nature of the effect.Our findings, so far, provide consistent evidence for the hypothesized relationship between creative engagement and donation behavior. In the following studies, we extend our examination to understand the underlying process through which creative activity and higher donation behavior are connected. In particular, we examine the mediating role of positive affect in this relationship. In addition, in Study 3 we also test the role of autonomy as a driver of creative engagement's impact on positive affect, which consequently influences donation behavior. Study 4 then explores why positive affect has such a significant impact on donation behavior. Finally, given the null findings in the creative exposure condition (on donation behavior), we dropped this condition from subsequent studies. Study 3: Testing the Role of Autonomy and Positive AffectStudy 3 was conducted to fully test H2a and identify the underlying process through which creative engagement impacts donation behavior. In particular, we tested our prediction that engaging in a creative activity heightens one's sense of autonomy, which in turn induces positive affect, leading to higher donation behavior. This study was preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=ag5ci3). MethodTwo hundred adults (117 women; Mage = 32.76 years, SD = 11.51 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization ChildHelp Foundation and were provided with information about the organization (see Web Appendix D.1). Next, all participants were told that the ChildHelp Foundation runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event would be virtual, and the organization needed help planning the function. Thus, the organization was inviting them to participate in this study as if they were actual donors participating in the event.The activity type manipulation used an idea generation task, mimicking the ""Think outside the cereal box"" campaign Kellogg launched several years ago. In the creative condition, participants were told that as part of its annual charity event, the ChildHelp Foundation was inviting them to ""think outside the cereal box"" and generate a fun and creative way to use Froot Loops cereal, besides eating it for breakfast. Further, participants were told to be as creative as possible and use their imagination to generate an innovative way to use Froot Loops cereal. In the neutral condition, participants were asked to ""think about the cereal"" and share a traditional way of how they eat the Froot Loops cereal (for detailed instructions, see Web Appendix D.2).Once participants completed the Froot Loops task, we assessed their donation behavior, sense of autonomy, and affective state. To capture participants' donation behavior, they were told that the ChildHelp Foundation was seeking donations, and they could help by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.Sense of autonomy was measured by adapting established measures defined in the literature (i.e., [18]; [56]). Specifically, participants were asked to indicate how they felt while generating their ideas during the Froot Loops task: ( 1) ""To what extent did you feel you had autonomy in generating your ideas during the Froot Loops task?,"" ( 2) ""To what extent did you feel you had freedom in coming up with your ideas for the Froot Loops task?,"" ( 3) ""How free did you feel in generating your ideas for the Froot Loops task?,"" and ( 4) ""How much did you feel you were able to express yourself when generating your ideas for the Froot Loops task?"" (1 = ""not at all,"" and 7 = ""very much""). Next, to measure positive affect, they were asked to think back to the Froot Loops activity and indicate how they felt during this activity on 11 items adapted from [19] and [18]. Specifically, all participants reported how they felt while completing the Froot Loops activity: ( 1) 1 = ""very negative,"" and 7 = ""very positive""; ( 2) 1 = ""very unpleasant,"" and 7 = ""very pleasant""; ( 3) 1 = ""not at all nice,"" and 7 = ""very nice""; and ( 4) 1 = ""very bad,"" and 7 = ""very good."" This was followed by seven items anchored with 1 = ""not at all,"" and 7 = ""very much,"" asking ( 5) how positive they felt during the Froot Loops activity, ( 6) the extent to which they enjoyed the Froot Loops activity, ( 7) the extent to which they had a good time during the Froot Loops activity, ( 8) how much fun the Froot Loops activity was, ( 9) how satisfied they felt during the Froot Loops activity, (10) how pleasurable the Froot Loops activity was, and (11) how exciting the Froot Loops activity was. Results Preliminary analysesA trained research assistant blind to hypothesis and condition rated the creativity of the generated Froot Loops ideas. As we expected, a main effect of creativity emerged such that those in the creative condition (M = 3.89, SD = 1.29) generated more creative ideas than those in the neutral condition (M = 1.24, SD = .81; F( 1, 198) = 301.96, p <.001, Cohen's d = 2.46). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (63.37%) donated money, compared with the percentage of participants in the neutral condition (45.45%; χ2( 1) = 6.50, p = .01). Next, we examined the difference between the creative and neutral conditions' donation amounts. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. Replicating the results from the previous studies, those who engaged in the creative (M = 41.49¢, SD = 41.60¢) versus the neutral (M = 27.07¢, SD = 37.04¢) activity donated significantly more money (U = 6,034.50, p = .007). Process measuresFactor analysis showed that all four items used to capture participants' sense of autonomy loaded onto the same factor; therefore, we averaged them to create a sense of autonomy index (α = .92). A one-way ANOVA revealed that those in the creative condition (M = 6.05, SD = 1.02) reported a significantly higher sense of autonomy than those in the neutral condition (M = 4.94, SD = 1.73; F( 1, 198) = 30.61, p <.001, Cohen's d = .78). In addition, factor analysis showed that the 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .96). A one-way ANOVA revealed that those in the creative condition (M = 5.24, SD = 1.24) reported a significantly higher positive affective state than those in the neutral condition (M = 4.52, SD = 1.31; F( 1, 198) = 16.15, p <.001, Cohen's d = .56). Mediation analysisTo test the potential underlying process paths, we first examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation rate as the dependent variable) did not include zero (β = .31, SE = .12, bias-corrected 95% confidence interval [CI] = [.11,.59]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analysis to understand the role of autonomy in this relationship. Serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation rate as the dependent variable together revealed the presence of a significant indirect effect (β = .21, SE = .09, bias-corrected 95% CI = [.07,.41]).We also conducted the same mediation analyses for the donation amount. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 4.71, SE = 1.99, bias-corrected 95% CI = [1.32, 9.02]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analyses to understand the role of sense of autonomy. A serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation amount as the dependent variable together revealed the presence of a significant indirect effect (β = 2.99, SE = 1.48, bias-corrected 95% CI = [.29, 6.10]). DiscussionStudy 3 results replicated the findings from the previous studies and showed that engaging in a creative (as compared with neutral) activity enhances donation behaviors (H1). Further, the findings from this study highlighted the underlying processes through which creative engagement affects monetary donation. As we hypothesized, creative (vs. neutral) engagement induces a higher positive affect, which in turn leads to enhanced donation behavior. Importantly, we found that sense of autonomy drives the relationship between creative engagement and positive affect (H2a is fully supported). To further confirm the role of autonomy in this relationship, we conducted a supplementary study in which we directly manipulated the sense of autonomy felt by the participant. Here, we showed that when felt autonomy is mitigated, the positive effect of creative engagement on positive affect (and, in turn, the donation behavior) is attenuated (for the details of this supplementary study, see Web Appendix E).In the next study, we further explicate the underlying process through which creative engagement impacts donation behavior by examining the pathways through which positive affect leads to higher donation behavior. Study 4: Exploring Why Positive Affect Impacts Donation BehaviorWe conducted Study 4 to test H2b and provide additional insight into the underlying process through which creative engagement impacts donation behavior. In particular, we assessed the possible role of mood maintenance, social connection, and perceived donation impact in driving positive affect's influence on donation behavior outcomes. This study was also preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=j8rj2w). MethodTwo hundred adults (109 women; Mage = 34.84 years, SD = 12.91 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization Healthier Tomorrow and were provided with information about the organization (see Web Appendix F.1). Next, all participants were told that Healthier Tomorrow runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event was going to be virtual, and the organization needed help planning the function. Thus, the organization wanted to invite them to participate in this study as if they were actual donors participating in the event.Next, participants were randomly presented with either the creative or neutral version of the event and asked to create (reproduce) a T-shirt design. Those in the creative condition were specifically asked to design an innovative T-shirt and be as creative as possible (for detailed instructions and the sample designs produced by participants, see Web Appendix F.2). In the neutral condition, participants were simply provided with a generic T-shirt design and asked to reproduce it, thus negating any potential creative engagement (for detailed instruction and the T-shirt design provided to the participants, see Web Appendix F.3). Participants were then directed to the T-shirt customization website (customink.com) to complete the design activity. Once participants had finished creating (reproducing) their designs they were asked to save them with a unique ID provided in the survey and then use the save/share function in the T-shirt customization website to email their design to a designated email address created for the study.Once participants completed and submitted information about their designs, we assessed their donation behavior, affective state, mood maintenance, social connection, and perceived donation impact. To capture participants' donation behavior, we told them that Healthier Tomorrow was seeking donations, and they could contribute by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.To assess participants' positive affective state, we asked them to think back to the T-shirt design task and indicate how they felt during this activity, on the same 11 items used in Study 3 (adapted from [19]] and [18]]). Participants responded to a mood-maintenance measure adapted from [23], where they were asked to think about their donation decision and indicate their agreement with the following statements on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""I thought ..."" ( 1) ""I would feel good about myself if I donate,"" ( 2) ""donating will make me feel good,"" and ( 3) ""if I donate it would be a personally rewarding experience."" To measure social connection, we adapted items from [ 8] to suit the context of our study. Participants responded to the following items on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""To what extent did you feel ..."" ( 1) ""closer to Healthier Tomorrow,"" ( 2) ""connected to Healthier Tomorrow,"" and ( 3) ""that completing the design task affected the way you think about the relationship with Healthier Tomorrow."" We measured participants' perceived donation impact by means of three items adapted from [14] and [67]. These items specifically asked the participants how much they thought their donation could ( 1) make a positive difference, ( 2) be valuable, and ( 3) do a lot of good (1 = ""not at all,"" and 7 = ""very much""). Results Preliminary analysisTen participants did not complete the T-shirt design activity and were excluded from the analysis (including these participants in the analysis does not change the significance or pattern of results; see Web Appendix F.4). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (47.87%) donated money, compared with the percentage of participants in the neutral condition (27.08%; χ2( 1) = 8.85, p = .003). Next, we examined the difference in donation amounts between the creative and neutral conditions. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. In a replication of the results from the previous studies, those who engaged in the creative (M = 22.34¢, SD = 31.81¢) versus the neutral (M = 11.25¢, SD = 24.63¢) activity donated significantly more money (U = 5,517, p = .002). Process measureFactor analysis showed that all 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .97). A one-way ANOVA revealed that those in the creative condition (M = 5.32, SD = 1.32) reported a significantly higher positive affective state than those in the neutral condition (M = 4.72, SD = 1.40; F( 1, 188) = 9.26, p = .003, Cohen's d = .45). Mediation analysisAs an initial step, we examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and donation rate as the dependent variable) did not include zero (β = .19, SE = .11, bias-corrected 95% CI = [.03,.48]), indicating a presence of a significant indirect (i.e., mediation) effect.Next, we examined the pathways through which positive affect, as induced by engaging in creative activity, impacts donation rate. We tested a sequential-parallel mediation model with creative engagement as the independent variable, positive affect as the first mediator, three factors (mood maintenance, perceived donation impact, and social connection) as a second set of mediators in parallel, and donation rate as the dependent variable (see Figure 1) using structural equation modeling (for statistics for each path in the model, see Table 1). This model is very similar to a serial mediation model; however, no order is assumed among the second set of mediators (i.e., mood maintenance, perceived donation impact, and social connection) ([21]). We used bootstrapping procedures to compute 95% CIs by generating 10,000 resamples. The results indicated significant serial indirect effects through positive affect and mood maintenance (β = .031, SE = .014, bias-corrected 95% CI = [.011,.068], p = .001) and positive affect and perceived donation impact (β = .031, SE = .014, bias-corrected 95% CI = [.010,.067], p = .001). However, the serial indirect effect of positive affect and social connection on the creative engagement and donation rate relationship was not significant (β = .005, SE = .012, bias-corrected 95% CI = [−.015,.035], p = .57). Interestingly, with positive affect in the model, creative engagement did not directly impact any of the second set of three mediators, thereby demonstrating the importance of positive affect in the conceptualization. Positive affect positively influenced all three potential mediators, but only mood maintenance and perceived donation impact significantly impacted donation rate. Thus, the positive affect experienced during a creative activity bolstered the desire for mood maintenance and the perceived donation impact, which in turn enhanced donation behaviors.Graph: Figure 1. Sequential-Parallel Mediation Model (Study 4).GraphTable 1. Sequential-Parallel Mediation Model (Study 4). PathPredictorOutcomePath Estimates (Standardized)SE95% CIa1Activity typePositive affect.22.20[.08,.35]a2Activity typeMood maintenance−.14.21[−.27, −. 01]a3Activity typePerceived donation impact.09.23[−.05,.23]a4Activity typeSocial connection.09.19[−.03,.21]b′1Positive affectDonation rate−.13.03[−.29,.02]Donation amount−.141.98[−.28,.002]b2Mood maintenanceDonation rate.31.02[.14,.46]Donation amount.251.32[.06,.42]b3Perceived donation impactDonation rate.37.02[.20,.52]Donation amount.321.16[.17,.46]b4Social connectionDonation rate.04.02[−.13,.21]Donation amount.041.43[−.16,.23]c′Activity typeDonation rate.19.06[.05,.32]Donation amount.173.81[.04,.29]d1Positive affectMood maintenance.51.07[.38,.61]d2Positive affectPerceived donation impact.41.08[.28,.53]d3Positive affectSocial connection.63.07[.52,.72] 1 Notes: Path analysis assessing the effect of creative (vs. noncreative) engagement on donation rate/amount through positive affect, mood maintenance, perceived donation impact, and social connection (estimates and 95% CIs for individual paths).A similar analysis was conducted for donation amount. First, a bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 1.80, SE = 1.13, bias-corrected 95% CI = [.04, 4.44]), indicating a significant indirect (i.e., mediation) effect of positive affect. Next, to examine the pathways through which positive affect impacts donation, we used structural equation modeling to test the hypothesized process model, with donation amount as the dependent variable (for statistics for each path in the model, see Table 1). Ninety-five percent confidence intervals obtained by generating 10,000 bootstrap resamples indicated significant serial indirect effects through positive affect and mood maintenance (β = 1.51, SE = .78, bias-corrected 95% CI = [.40, 3.72], p = .004) and positive affect and perceived donation impact (β = 1.60, SE = .76, bias-corrected 95% CI = [.53, 3.65], p = .001). However, similar to what we observed for the donation rate, the serial indirect effect of creative engagement through positive affect and social connection on donation amount was not significant (β = .33, SE = .81, bias-corrected 95% CI = [−1.06, 2.26], p = .58). DiscussionThe Study 4 results replicated the findings from the previous studies and showed that engaging in a creative (vs. neutral) activity induces positive affect, which in turn enhances donation behaviors. Importantly, this study further examined the underlying process, providing understanding of how positive affect influences donation behavior. We found that the path from positive affect to donation behavior was multiply determined. Indeed, perceived donation impact and mood maintenance were both shown to be drivers of the affect/donation relationship. Interestingly, we did not find evidence for social connection as a mechanism that triggered the identified effects. Thus, it appears that attributions flowing from the positive experience of creative engagement are more at play in defining enhanced donation behavior, and efforts to maintain positivity also spill over into the donation outcomes. Future research should continue to explore these identified mechanisms and discern within which charitable contexts they are most applicable and effective. General DiscussionThe current research examined the relationship between creativity and donation behavior. A pilot study and four subsequent experiments demonstrated that engaging in a creative activity induces a sense of autonomy, which leads to a positive affective state, which in turn results in enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated; for a summary of all study results, see Table 2). We further showed that the positive affect experienced by the creator leads to enhanced donation behavior, due to perceptions of increased donation impact and an effort to maintain the resulting positive mood.GraphTable 2. Summary of Study Results. Donation AmountDonation RateUnits DonatedTask EngagementTask EngagementStudyCreativeNeutralNoneCreativeNeutralNonePilot StudyUSD ($)7.07a(13.06)1.10a(3.26)—34.48%a12.20%a—Study 1Number of quarters (maximum 8)4.50a(3.19)2.98a(3.54)—80.95%b55.32%b—Study 2Number of quarters (maximum 8)6.10a b(3.30)4.49a c(3.85)4.04b c(3.97)81.03%a b61.82%a c50.88%b cStudy 3USD (¢)41.49b(41.60)27.07b(37.04)63.37%b45.45%bSupplementary study (follow-up to Study 3)USD ($)Control12.52ab(7.58)8.43ab(7.10)87.88%ad75.41%cdAutonomy inhibited9.60ac(8.30)11.84ac(7.72)70.77%ac80.33%cStudy 4USD (¢)22.34b(31.81)11.25b(24.63)47.87%b27.08%b 2 Notes: Standard deviations are in parentheses. The contrasts are identified with superscript notation: ap ≤.05, bp ≤.01, cp >.1, dp ≤.1. Implications for PracticeOur work was motivated by the documented fact that charitable organizations often struggle to find effective ways to engage donors and solicit donations ([59]). Thus, a central contribution of our research is in confirming that engaging potential donors through creativity can meet this challenge by increasing engagement and enhancing donation behaviors. Substantively, we can recommend incorporating creative activities into fundraising campaigns and charity events as a viable marketing strategy. Indeed, creative activities can be implemented through social media platforms (as exemplified in our pilot study) or in person during charity events and solicitations (as exemplified in Study 2). Current industry practices suggest that some charities have begun testing this approach (i.e., engaging potential donors through creative activities before soliciting them for donations). For example, Roots and Shoots (a charitable organization that supports environmental, conservation, and humanitarian issues) regularly posts a variety of gamified challenges on its website and invites potential donors to participate. Many of these challenges encourage people to incorporate creativity in defining their solutions (e.g., for a ""World Chimpanzee Day Challenge,"" people were asked to design and submit a creative communication graphic to spread awareness about chimpanzee protection).To gain additional insights on practitioners' points of view (concerning our proposed strategy), we sent an email to 220 charities nationwide, inviting them to participate in a short survey. The survey asked three questions that measured the usefulness and applicability of this donation strategy. The first question assessed whether the charity had previously used a creative activity as a preface to a donation request. The second question asked whether, if presented with evidence that engaging donors through a creative activity increases monetary donation, they would implement this strategy in their donation campaigns (1 = ""not very likely,"" and 7 = ""very likely""). The final question assessed how feasible they thought it would be to implement such a strategy (1 = ""not very feasible,"" and 7 = ""very feasible""). We obtained 29 responses from the surveyed national charities (13% response rate). Interestingly, 45% of the charities mentioned that they have previously used a creative activity as a preface to a donation request—showing that our research validates a tactic already in use by some charities today. Most importantly, charities indicated they would definitely be willing to implement this strategy in their donation campaigns (M = 6.55, SD = .69; t(28) = 20.04, p <.001, compared with the midpoint) and thought it would be feasible to implement such a strategy (M = 5.38, SD = 1.68; t(28) = 4.43, p <.001, compared with the midpoint). Though a small sample, these results are encouraging and affirm that utilizing creative activities in charity campaigns is both highly relevant and feasible in the marketplace. Theoretical ContributionsThe current work also provides several theoretical contributions to the field. First, we advance the marketing and charity literature streams by identifying that positive affect experienced during a creative activity is a key mechanism that bolsters subsequent donation behaviors. Second, we offer a deeper understanding of why engaging in a creative activity leads to higher donation behavior through positive affect. Specifically, we show that creative engagement enhances a sense of autonomy, which in turn induces positive affect, which then positively impacts donation behavior. In addition, the relationship between positive affect and enhanced donation behavior is shown to be multiply determined. We identify two specific mechanisms that link affect and behavior: namely, the positive attributions of the impact of one's donation and the mood maintenance tendency of the participant. Third, we establish that the act of creativity itself (not just being primed with creativity as a construct) is a necessary condition to achieve beneficial donation outcomes. Finally, we confirm that the creative activity employed need not relate directly to the organization and/or charitable cause underlying the sought-after donation behavior. This is important both theoretically and practically, as it establishes generalizability in our findings and provides more freedom to charities in defining the type of creativity activity appropriate for their donation campaigns.More broadly, the current research adds to prior work demonstrating the consequences of engaging in creative thinking tasks. While a significant amount of research has been devoted to studying various factors and cognitive processes that impact creativity ([28]; [54]; [55]), much less attention has been paid to the implications and outcomes of being creative ([27]; [71]). Our research shows that there is value in understanding what implications creativity may have for subsequent consumption behaviors. Building up our understanding of the importance of creativity is especially significant in today's consumption environment, where customers are increasingly provided with opportunities to engage in creative activities, from participating in crowdsourcing platforms (e.g., MyStarbucksIdea.com, ideas.lego.com) to engaging in customization processes (e.g., NikeID, Casetify customized phone cases). Limitations and Future ResearchLimitations inherent to our research approach open up several avenues for additional investigation. First, research should be directed toward developing a better understanding of the generalizability of the effects we identify. Although we demonstrated that a creative activity does not have to be specific to the charity in question to provide a positive outcome, we did not assess a broad range of charities and donation appeals. To this end, we conducted a preliminary study examining the impact of the inherent history of the charity (i.e., whether the charity was well-established; for study details; see Web Appendix G) on donation behavior. Here, we found that creative engagement indeed led to enhanced donation behavior, but only when the charity was newly established. When the charity was well-established, the donation behavior was enhanced irrespective of the type of activity utilized in the appeal. Additional research is needed to better explore this potential boundary condition and, more generally, to define other contextual factors that might moderate the effects we have documented here.Second, most of the creative activities tested in this research involved artistry and design (e.g., cookie decorating, T-shirt design, coloring). It remains to be seen whether other forms of creativity could produce similar effects. Indeed, we believe that the effects identified in this research are likely to be observed for any enjoyable creative activities that encourage people to explore and think freely. However, we conjecture that the positive effects defined herein may be attenuated if the creative activity is more convergent in nature (e.g., identifying the one right solution). While we did not directly test the effect of a convergent creative activity on donation behavior, prior research has found that engaging in convergent creative tasks may not lead to a positive affective state ([ 9]). Future research should explore this possibility and outline the breadth of creative activities that are effective precursors to enhanced donation behaviors.Another interesting research question arising from our work concerns the identified difference between creative engagement and creative priming (on subsequent behaviors). We found that engaging in creative activities leads to higher donation behavior, but exposure to creative stimuli does not have a parallel effect. Previous research ([27]), however, has shown that creative priming can influence cognitive processes. Thus, it is important to further distinguish between creative engagement and exposure to creative materials and understand how they differentially impact subsequent behaviors. While both creative engagement and creative priming may influence cognition, perhaps only creative engagement can induce a positive affective state. Future research could further clarify the differences between these stimuli.Finally, future research should continue to build understanding as to when creativity leads to positive (vs. negative) outcomes. Indeed, prior research has shown both positive and negative implications for creative thinking. For example, creativity has been shown to help overcome the burden of secrecy ([29]) and to enhance one's tendency to take a target's perspective ([77]). On the negative side, previous research has found that creativity can lead to dishonesty (e.g., [27]) and enhance unhealthy choices ([31]). We find an opportunity for future work in building on these initial studies to better understand where creativity can influence downstream consumption behaviors. Indeed, we hope that future research will expand on our findings and further investigate the outcomes of creativity for individuals, charities, nonprofit organizations, and the broader marketplace at hand. "
29,"Machine Learning for Creativity: Using Similarity Networks to Design Better Crowdfunding Projects A fundamental tension exists in creativity between novelty and similarity. This research exploits this tension to help creators craft successful projects in crowdfunding. To do so, the authors apply the concept of combinatorial creativity, analyzing each new project in connection to prior similar projects. By using machine learning techniques (Word2vec and Word Mover's Distance), they measure the degrees of similarity between crowdfunding projects on Kickstarter. They analyze how this similarity pattern relates to a project's funding performance and find that ( 1) the prior level of success of similar projects strongly predicts a new project's funding performance, ( 2) the funding performance increases with a balance between being novel and imitative, ( 3) the optimal funding goal is close to the funds raised by prior similar projects, and ( 4) the funding performance increases with a balance between atypical and conventional imitation. The authors use these findings to generate actionable recommendations for project creators and crowdfunding platforms.Keywords: crowdfunding; combinatorial creativity; funding goal; imitation; networks; novelty; Word2vec; Word Mover's DistanceCrowdfunding has grown rapidly and become an important source of capital. In 2014, start-up investment generated through crowdfunding was almost half of investment from venture capital ($16 billion vs. $30 billion; see [ 4]). In addition to its fundraising capability, crowdfunding provides useful marketing opportunities. First, project creators can use crowdfunding platforms to advertise ideas and build a reputation ([ 9]). Second, firms can use crowdfunding sites to test the market reaction to a new project ([31]; [40]).Despite the importance of crowdfunding in investment and marketing, creating successful crowdfunding projects remains a major challenge. The top crowdfunding website, Kickstarter, applies an all-or-nothing policy, whereby a project creator collects funds only if the project's funding is successful (i.e., the raised fund pass the funding goal). On Kickstarter, only about 30% of submitted projects end up being successfully funded.[ 7] Further, about 67% of these successful projects raised no more than $10,000 (Kickstarter [20]). To help project creators, previous studies have explored various drivers of success in crowdfunding (see Table 1).GraphTable 1. Current Study and Literature on Crowdfunding. StudyNumber of ObservationsMain Outcome VariablesMain Source of ExplanationContent AnalysisSimilarity-Based CriteriaThis study98,058 ideasFinal success and funds raisedSimilarity between ideasYesYesMollick (2014)48,526 ideasFinal successSocial networks and quality signalsYesNoVan de Rijt et al. (2014)200 ideasFinal successEarly success in funding periodNoNoYounkin and Kuppuswamy (2017)7,617 ideasFinal successCreator ethnicity and genderNoNoSteigenberger and Wilhelm (2018)197 ideasFunding percentageRhetorical signals in product pitchesYesNoKuppuswamy and Bayus (2017)10,000 ideasFunding dynamicsProximity to goalNoNoDai and Zhang (2019)28,591 ideasFunding dynamicsInvestor prosocial motivesNoNoBurtch, Ghose, and Wattal (2015)128,701 investorsInvestor's decisionPrivacy policy of platformNoNoZvilichovsky, Danziger, and Steinhart (2018)892 investorsInvestor's decisionMake-it-happen motivesNoNoAgrawal, Catalini, and Goldfarb (2015)8,149 investorsInvestor's decisionGeographical distance between investors and creatorsNoNoBapna (2017)519 investorsInvestor's decisionProduct certificationsNoNo 1 Notes: In terms of content analysis, [27] uses the number of spelling errors in project description; [32] use human coders to label signals in idea pitches. For studies on peer-to-peer lending, which is sometimes recognized as a form of crowdfunding, see [43], [23], and [28].One promising yet unexplored area is the similarity pattern among projects. The prior similar projects of a new project can hold important clues for the new project's funding outcome. This thought has its root in the theory of combinatorial creativity, which views every new idea as some recombination of existing ideas (e.g., [29]; [39]). For predicting success, this theory provides a novel perspective to the current literature on crowdfunding. We may evaluate a new project directly using the level of success of its prior similar projects (given that we have a way to measure the similarity between projects). It requires us to neither specify the exact factors underlying the success of projects nor quantify the effects of these factors on success. Although this approach is intuitively appealing from both a theoretical and method point of view, it has never been applied in crowdfunding (or many other contexts in marketing).In addition to predicting success, measuring the similarity pattern among projects enables us to characterize and examine projects in ways novel to the crowdfunding literature. First, we can measure the degree of novelty of a project and then investigate whether novelty is rewarded or penalized; whether repeated imitation of an idea devalues the idea; and, if so, at what point this devaluation starts. Second, we can measure how much a project's funding goal ""overshoots"" the amount of funds that were raised by prior similar projects and examine whether this overshooting (or undershooting) benefits fundraising. Third, we can measure styles of imitation (e.g., does the new project strictly follow a stereotype or also reach out for atypical elements from other types of projects?). Overall, the examination of these similarity-based characteristics can deliver useful insights for designing new projects.Specifically, this study aims to answer the following research questions: How can we measure the degrees of similarity between all the projects on a crowdfunding site in an objective and automated way? How does the similarity pattern relate to funding performance? Specifically, to what extent… … does the success of prior similar projects predict a new project's success? … is novelty rewarded or penalized? … is it better to let the funding goal overshoot or undershoot the funds raised by prior projects? … does atypicality benefit or hurt funding performance? How can the platform use the similarity pattern to provide creators concrete guidance to design better projects?To answer these questions, we collect data on 98,058 Kickstarter projects from 2009 to 2017 in the three largest categories: Film & Video, Music, and Publishing. We measure the semantic similarity between the descriptions of any two projects by applying two recently developed machine learning techniques, Word2vec ([25]) and Word Mover's Distance (WMD; [22]). We calculate the ""effort"" that one must incur to move the words of one document to the words of the other document. The smaller this effort is, the more similar the two documents are.To operationalize the similarity pattern between projects, we represent it with a similarity network. The nodes represent projects. The strength of a link ( 1) increases with the degree of similarity and ( 2) decreases with the time lapse between two projects. When predicting project j, we focus on all the projects prior to j, with each prior project weighted by its link strength with j. Conceptually, the funding outcome of a project reveals the investor preference for this project. However, because investor preferences change over both time and projects, not every prior project is equally relevant for evaluating the investor preference for a given new project. In this regard, the similarity network offers a way to select the most relevant prior projects and thus provides useful information for predicting success.We examine funding performance from two aspects: whether the funding is successful and how much money is raised. There are several novel findings. First, the average level of success by prior projects, weighted by their links to the focal project, is a significant predictor of the focal project's funding performance. This result holds after we control for the project creator's prior success, the project's funding goal, description length, and the presence of images and videos in the description. Overall, the similarity network is an information source to significantly improve the out-of-sample prediction for funding success.Second, a project's funding performance exhibits an inverted U-shaped relation with the novelty of the project. Here, we measure the novelty of a project via the sum of its links with all prior projects. A larger sum means a greater amount of total similarity between the project and its prior projects, indicating a lower level of novelty (or a higher level of imitativeness). This inverted U-shaped relation can be surprising yet is intuitive. It suggests that successful projects tend to strike a balance between ( 1) being novel and ( 2) appearing familiar to investors.Third, it is optimal to set the funding goal close to the amount of funds raised by prior similar projects. Specifically, we define goal overshoot as the focal project's log funding goal minus the average log funds raised by prior projects, weighted by their links to the focal project. In other words, goal overshoot compares a project's funding goal against a benchmark set by prior similar projects. We find that setting a goal either too low or too high compared with the benchmark decreases the funds to be raised (i.e., an inverted U-shaped effect). Setting a goal lower than the benchmark has a limited effect on the probability of success; setting a goal too much higher than the benchmark decreases the probability of success.Fourth, a project is more likely to succeed when it grounds itself in a stream of closely linked projects yet simultaneously borrows from some projects outside this stream. Under combinatorial creativity, this outside-the-stream imitation constitutes a nonconventional or ""atypical"" use of prior ideas (an example of atypical imitation outside the crowdfunding context is when an article in marketing cites research from a largely unrelated discipline such as physics or biology). We find an inverted U-shaped relation between atypicality and funding performance; neither too little nor too much atypicality benefits fundraising.Drawing on these findings, we devise two recommendation tools that the platform may use to help creators improve projects. Our first recommendation tool helps to set funding goals. Choosing the goal is an important decision for project creators, which matters greatly for funding outcomes ([27]). However, research has provided little concrete guidance on setting the goal optimally. Our results enable us to benchmark a project's goal against the funds raised by prior similar projects. For many projects, we find that the goals were far from optimal, in which case we recommend a ±10% goal adjustment to improve expected funding outcomes. Our second recommendation tool helps creators improve project content. Specifically, we recommend a prior project for the creator to imitate. The recommendations are customized for individual projects to increase each project's chance of success.The crowdfunding literature has focused on several aspects of crowdfunding as endpoints, including the final success of funding campaign (which is also a focus of this research), the dynamics during fundraising period, and investor decisions. The literature also has focused on a variety of sources of explanatory variables for these endpoints, including the project content (as in this article), creator characteristics, and investor behaviors. Table 1 provides a summary. Our study contributes to the literature in several important aspects. First, we apply combinatorial creativity in crowdfunding, using machine learning to construct a similarity network among projects. Second, we significantly improve the out-of-sample prediction of funding success. Third, we derive novel insights on how the similarity pattern with prior projects affects a project's funding performance. From a methodological point of view, the machine-learned similarity network provides important advantages. Previous studies look for explanatory criteria based on specific predefined factors, such as quality signals ([ 3]; [27]) and rhetorical signals ([32]). In contrast, our similarity-based criteria do not restrict attention to specific factors. In addition, we base the similarity measures on the contents of project descriptions, which are particularly important in crowdfunding because investors make decisions on the basis of these descriptions. As a result, our models gain insight, explanation, and prediction.This study also contributes to the broader literature on ideation examined under various contexts aside from crowdfunding, such as crowdsourcing, consumer product development, and the music market. Table 2 provides a summary. These studies have analyzed several sources of explanation for idea success, including the templates of innovation, problem decomposition, creator's social network, idea prototypicality, or genre divergence. Compared with these studies, the current study is unique in its focus on crowdfunding, a source of explanation based on the similarity pattern between ideas, novel findings, and the massive number of ideas examined.GraphTable 2. Current Study and Literature on Ideation. StudyContextNumber of IdeasMain Outcome VariablesMain Source of ExplanationMethodology of AnalysisThis studyCrowdfunding98,058Final success; funds raisedSimilarity between ideasEmpiricalGoldenberg, Mazursky, and Solomon (1999)Crowdsourcing359Idea qualityIdea templatesExperimentLuo and Toubia (2015)Crowdsourcing4,298Idea qualityIdea cuesExperimentStephen, Zubcsek, and Goldenberg (2016)Crowdsourcing786Idea qualitySocial networksExperimentToubia and Netzer (2017)Crowdsourcing4,129Idea qualityPrototypicalityExperimentGoldenberg, Lehmann, and Mazursky (2001)Consumer products197Product successIdea templatesEmpiricalBerger and Packard (2018)Music popularity1,879Song popularityGenre divergenceEmpirical The next sections describe the concept of combinatorial creativity, the research design, the results, and the managerial implications. We conclude with a discussion of the findings and possible extensions of the research. Concept of Combinatorial CreativityBefore we describe the details of our study, we briefly overview the concept of combinatorial creativity, which forms the conceptual basis for our analysis. Previous studies on idea generation have addressed how creativity is generated from a cognitive standpoint. A general descriptive framework is the ""Geneplore"" model proposed by [14]. Geneplore is a portmanteau of the words ""generate"" and ""explore,"" signifying that the development of creative ideas is an iterative interaction of two processes: the generation process and exploration process. In the generation process, people retrieve various pieces of information based on prior knowledge. Then, they create seeds of ideas, called preinventive forms, by recombining those retrieved components. In the exploration process, the preinvented forms can be focused, expanded, or evaluated in further depth. After going through these two processes iteratively, people finally come up with creative ideas.Underlying the Geneplore model is the combinatorial nature of creativity, which sees a new idea as a recombination of existing knowledge. [29], p. 66) conceptualizes economic development as ""the carrying out of new combinations."" [39], p. 331) theorizes that ""knowledge can build upon itself in a combinatoric feedback process."" Although the concept of combinatorial creativity has a long history in the literature of innovation and growth, researchers have only recently begun to gather empirical evidence and applications on this topic. A most representative work is [36]. The authors examine the impact of a piece of scientific research in relation to its bibliography. They show that scientific research tends to have a higher impact when it balances the uses between atypical knowledge and conventional knowledge. Later studies attempt to expand the application of combinatorial creativity in different contexts such as patents ([41]), idea competition ([33]; [35]), and motion pictures ([38]).Compared with the aforementioned studies, ours is unique in several ways. First, this research is the first to apply the concept of combinatorial creativity in the context of crowdfunding. Second, we construct the pairwise connections between ideas on the basis of direct comparison between the content of ideas. In contrast, [36] use the citation network between papers, [41] use the classification by the U.S. Patent and Trademark Office, [33] use the communication network among ideators, [35] examine how an idea deviates from the average, and [38] infers the similarity between movies indirectly from consumer revealed preferences. Third, previous studies mostly focus on the similarity or connection pattern itself. We devote attention to the interactions between the similarity pattern and idea attributes. For example, we examine the extent to which a project's funding goal ""overshoots"" the typical level of funds raised by prior similar projects. Research DesignThis section describes the data, construction of the similarity network, network-based metrics for predicting funding outcomes, control variables, and models.Before we give details on these components of our analysis, it is useful to discuss the conceptual role of the similarity network, as it is the core of our research design. When we try to predict the funding outcomes for a new project, the key determinant is investors' preferences—what types of projects they find worth supporting, what elements of ideas they find appealing, and so on. In this regard, the funding outcomes of historical projects provide a database containing many instances of revelation of investors' preferences (where each historical project constitutes one instance). However, the crucial question is which part of this database we should use when predicting the funding performance of a given new project. An intuitive answer is not difficult. For example, if the new project aims to develop a video about motorcycle ride trips, then the success or failure of a prior film project on a family drama is unlikely to offer relevant information. However, the funding outcomes of prior projects on traveling or motorcycles should seem relevant. In addition, we probably should give more weight to more recent projects. Projects completed ten years ago, even if their content is similar to the new project's, may no longer be relevant because investor preferences may have significantly changed since then.We use the similarity network to operationalize the aforementioned intuition. We let each link in the network measure the degree of similarity (computed using machine learning methods) as well as the time proximity between a project pair. Then, when predicting the funding outcome for a focal project, we weigh each prior project by its link strength with the focal project. In addition to selecting relevant prior projects for prediction, the network allows us to characterize a new project in relation to prior projects (e.g., novelty of the new project). These characteristics may affect the investors' preferences and thus funding outcomes. We define and discuss the metrics for these characteristics subsequently in this section. DataWe collect data from Kickstarter, one of the largest reward-based crowdfunding platforms. We acquire the information of each project from May 2009 to the end of 2017.We focus on English-language projects in the United States. We also focus on the projects belonging to the top three largest project categories: Film & Video, Music, and Publishing. To adjust for inflation over time, we normalize all the monetary values (e.g., funding goal and funds raised by projects) to 2017 dollars using the Consumer Price Index. Following [27] procedure, we eliminate the projects with outlying project goals (i.e., goals smaller than $100 or larger than $1,000,000 [1.24% of data]). Furthermore, we do not consider the projects with fewer than 50 words in the description text (2.5% of data), in order to correctly measure the content similarity between projects. In the end, our data include 98,058 projects on Kickstarter (Film & Video: 37,641 projects, Music: 35,943 projects, and Publishing: 24,474 projects).We focus on two measures for the outcome of a project: ( 1) funding success and ( 2) funds raised. Funding success is a binary variable. A project is classified as a success if it reaches the preset funding goal before the project campaign ends (project creators set the campaign lengths of their projects, the vast majority of which are either 30 or 60 days [the maximum length allowed is 60 days]). ""Funds raised"" denotes the total amount of money that the project collects at the end of the project campaign, whether it exceeds the funding goal or not. Note that under Kickstarter's all-or-nothing policy, a project cannot collect the funds unless the funding goal is successfully reached. Nevertheless, both funding success and funds raised are important outcomes reflecting the investors' interests and confidence in the project.The average rate of funding success over our entire data from 2009 to 2017 is about.46. The average success rate is higher in the Music category than the other two categories (Film & Video:.43, Music:.56, Publishing:.34) and has persisted over time. This difference in the success rate is partly explained by Music projects tending to ask for lower funding goals (median goal in each category: Film & Video: $7,500, Music: $4,359, Publishing: $5,107). Construction of Similarity NetworkTo measure the similarity between projects semantically, we apply machine learning techniques on the descriptive texts of crowdfunding projects. Note that our network is constructed to represent the similarity relations between projects (i.e., nodes are projects). This network is different from, for example, the semantic network between words in [35]. The method we adopt to measure similarity between projects is called WMD, which is based on Word2vec. We describe this method next. Alternatively, one could measure similarity with latent Dirichlet allocation (LDA; [ 7]) that factors text documents into topics. However, WMD provides a better prediction performance than LDA in our application (see Web Appendix A). WMD also has some practical advantages for our application, as we discuss subsequently. Word-level similarity (Word2vec)We apply Word2vec ([25]; [26]) to measure the similarity between the words in project descriptions. Word2vec is a machine learning algorithm designed to learn the semantic relations between words. Specifically, it applies a two-layer neural network to convert words into high-dimensional vectors. Taking a large corpus of text as an input, the model generates a vector to represent each word in the corpus. Word2vec positions each word in the vector space such that words that share common contexts in the corpus are located close by. Word2vec is also capable of capturing many semantic relations with vector operations. For example, the vector representing ""King"" minus the vector for ""Kings"" is equal to the vector for ""Queen"" minus the vector for ""Queens.""One implementation of Word2vec is known as the skip-gram model. It uses the two-layer neural network to predict the surrounding words when a central word is given. Google adopts this implementation to provide a pretrained Word2vec using the Google News corpus. The vocabulary contains more than three million unique words or phrases, each of which is presented by a 300-dimensional vector ([25]). We use this Word2vec trained by Google in this study. Alternatively, one can train context-specific Word2vec on the crowdfunding project descriptions. However, using Google's Word2vec here allows for easier implementation, especially for the platform, as we discuss next. Document-level similarity (WMD)A simple way to measure the similarity between two documents is to count the overlapping words that appear in both documents. However, two similar documents do not need to share even a single word (consider, e.g., ""President speaks on immigration"" and ""Trump talks about borders""). Therefore, we want to account for the similarities between words.We apply a recently developed method of measuring document similarity called WMD ([22]), which is built on Word2vec. Specifically, the method regards a document as the collection of word vectors as prescribed by Word2vec and minimizes the total travel distance of moving all word vectors in one document to all word vectors in another document. The minimized travel distance is used as a measure of dissimilarity between the two documents. WMD has been shown to perform better than previous methods in measuring document similarity (see Web Appendix A).The application of WMD with Google's pretrained Word2vec makes the computation of similarity between two crowdfunding projects a truly pairwise operation independent of other crowdfunding projects. This feature is different from the LDA and related text analysis models (e.g., latent semantic indexing; [12]), which require training first on the entire corpus to extract latent topics. As new projects are posted to the crowdfunding website every day, the corpus changes over time, which would require retraining the text model every once in a while. The WMD, coupled with Google's Word2vec, does not require such retraining and thus should be easier to implement for the platform.Before moving on, we briefly describe some summary statistics to help explain our implementation. Take the Publishing category as an example. There are 24,474 projects in this category. We take the text in the description (including the blurb) of each project. After removing stop words (e.g., ""the,"" ""a""), rare words, and words not in Google's Word2vec vocabulary, there are 19,032 unique words that we use in computing the WMDs between projects.[ 8] An average project has 209.64 unique words (SD = 161.73). There are 299,476,101 unique pairs ( = 24,474 × (24,474–1)/2) of projects in this category. A WMD is computed for each pair. We do not consider similarities between projects of different categories (e.g., a Publishing project and a Music project). Accounting for cross-category similarities entails a much larger computational cost, but it may improve predictions and insights. This is an interesting topic for future research.For each category, the distribution of the WMDs is bell-shaped and mostly symmetrical. The distributions of the three categories are very similar (Film & Video: M = 3.10, SD = .18; Music: M = 3.01, SD = .20; Publishing: M = 3.12, SD = .18). We note that the mean WMD in the Music category is slightly smaller than the other two categories. We think this difference may be because films and books tend to involve lengthier and more explicit storytelling, which allows for more ways to differentiate projects. Similarity network between projectsNetworks are generally designed to keep track of pairwise relations between individuals. We use a network to present the similarity relations between projects in a given crowdfunding category. Each node represents a project. An unweighted network can be constructed by placing a link between two projects if and only if their similarity passes a threshold. Such an unweighted network is actually sufficient for deriving the main qualitative results in this article. However, we can achieve better prediction performance by constructing a weighted network, where every pairwise relation takes a continuous value  wij≥0  . We call this value the link strength between node i and j.As discussed, we consider two properties when assigning the value for  wij  : ( 1) the value of  wij  should increase with the similarity between the contents of i and j, and ( 2) the value of  wij  should decrease with the time gap between i and j. As discussed, the second property is to account for the fact that investor tastes may be time-varying. Together, the two properties enable us to focus on the projects that are more similar and more recent to the focal project when predicting the funding outcomes for the focal project.Specifically, let  dij  denote the WMD between any two projects i and j. Let  ti  denote the starting date of any project i. Let  0<δ≤1  be a decay factor. We specify that, for any i and j in the same category such that  i≠j  , wij=δ|ti−tj|×L(γ0−γ1dij). Graph( 1)In Equation 1, L is the logistic function. We choose the exact values of  γ0  ,  γ1  , and  δ  using a 10-fold cross-validation (5-fold and 15-fold cross-validation give very similar values). The logistic function leads to a significantly better prediction performance for funding success than some of the other functions of  dij  , such as  1/dij  ,  1−dij/max{dij}  , or  e−γdij  . Also note the logistic function becomes a step function when  γ0  and  γ1  are sufficiently large, so the specification in Equation 1 contains the unweighted network as a special case. Finally, we note that only the relative link strengths carry a meaning; scaling  wij  by a common factor for all (i, j) pairs does not affect our subsequent analyses. Network-Based MetricsWe focus on several metrics based on the similarity network to help predict new projects' funding performance. Before we detail each metric, it is useful to set up an illustrative example, which we use throughout the article (see Figure 1). For illustration purposes, we restrict attention to only six projects—the actual similarity network contains tens of thousands of projects for each category. The thickness of a link represents the link strength  wij  . The horizontal position of a node reflects its starting date  ti  . Each project also has a completion date  Ti  . In this example, we treat  ti=Ti  for simplicity.[ 9]Graph: Figure 1. An illustration of similarity network.GraphTable 3. Selected Details of the Example Projects. ProjectDetails1Title: World Run—One Step at a Time.Creator: Alex H. (""Alex is a young filmmaker from a small town …"")Goal: $15,000Blurb: ""A documentary exploring what it is like to travel miles on foot. Watch Jesper Olsen break the world record for ultra-running.""Excerpt from Description: ""Jesper Kenn Olsen is approaching the finale of a four-year-long ambitious goal to run around the world. He has gone places that very few people have gone before, and seen the world at a runner's pace—roughly 26 miles a day. This documentary chronicles primarily the second half of Jesper's journey: from the southernmost tip of South America to the northern shores of Newfoundland … Jesper is an ultra-runner from Denmark … Part travelogue, part sports documentary, and part commentary on human identity, it will examine human achievement and investigate how defined personal and politcal boundaries really are.""2Title: Low LifeCreator: Luke E. (""Luke began his career as a freelance photographer …"")Goal: $10,000Blurb: ""A comedy about depression.""Excerpt from Description: ""THE GOAL. We are looking to raise money to shoot this 6 × 5 minute web series set in Los Angeles … Whilst all the cast and crew are working for free or deferred payments, the money raised will help us hire gear, and to pay for locations, catering, and insurances … THE STORY. The modern world is a scary place—suffocating, fractured, and alienating. On the surface, Jef is a regular guy. He' s good looking, charming, and funny. He also has a good job and a beautiful girlfriend. But he just doesn't feel right. And he' s not sure what to do about it. He is bombarded by advertising messages, annoying campaigners, and friends who all have solutions.""3Title: Atlanta to Alaska and Back, a Vintage Motorcycle Documentary.Creator: Aurorah Y. (""I'm an animator and astrologer from Atlanta …"")Goal: $25,000Blurb: ""I'm making a film about restoring a vintage motorcycle and riding it from Atlanta to Alaska and back.""Excerpt from Description: ""I'm the creator behind a feature length documentary film about restoring a 1972 Honda CB 350, and yes; I am also the girl who is doing the restoration (with help), and yes, riding the restored vintage bike from Atlanta to Alaska and back. It's an 11,000 mile round trip. This campaign is to cover the first 1/3 of production on the film which will encompass the bike's restoration. I will be doing the majority of the work on my bike with the help and guidance of skilled experienced mechanics … I'll be camping out a lot, and my camera man and his van will going to follow me and help film the trip.""4Title: Burning MemoriesCreator: Amelia B. (""Amelia B. is a 21-year-old filmmaker …"")Goal: $1,500Blurb: ""A family drama about a woman and her younger half-brother dealing with their father's abandonment issues to find happiness.""Excerpt from Description: ""Burning Memories is a senior thesis student short film about a family drama taking place in current times. The story is about a 30-year-old woman who seeks control over all the men in her life because of her father's abandonment issues; but, it is the arrival of her 12-year-old half-brother that makes her reflect on her life choices and question whether they are what make her happy. We are planning to raise more than $1500.""5Title: Head First Diaries: A Pan-American Adventure Travel SeriesCreator: Miles & Aaron (""Miles is 22 years old, and a recent grad from UCLA …"")Goal: $15,000Blurb: ""Head First is a visual journey across the Americas through the eyes of two young explorers pursuing freedom understanding & adventure.""Excerpt from Description: ""Our goal is to put you in the front seat with us over the course of the next year on this adventure of a lifetime. What sets Head First apart is our passion for telling a story about travel that captures the beauty of not just the destination but also the journey … The show will include 18 episodes that document the outstanding nature, people, and culture life on the road brings … Two young explorers. One dog. A quarter century old VW van.""6Title: US Highway 98, The Florida I Never Knew - Motorcycle JourneyCreator: Drew P. (""Drew is a fourth year student at UCF …"")Goal: $1,000Blurb: ""A five day solo motorcycle camping trip throughout the state of Florida exclusively along U. S. Highway.""Excerpt from Description: ""Hey Kickstarter! My name is Drew Perlmutter and I'm a documentary filmmaker … U.S. Highway 98 was established in 1933 and runs for 964 miles. 671 of those miles run through Florida. The highway coincidentally starts in my hometown of Palm Beach, and runs all the way to the Alabama / Florida state line … I will be spending 5 days traveling exclusively on this highway for its entirety throughout Florida … So, why a motorcycle? Besides the incredible sense of adventure from being on the open road, it' s about being absorbed in the scene, rather than being a passive observer."" The six projects are taken from the Film & Video category in our data (for details, see Table 3). Project 1 is a documentary about a world runner's journey. Project 2 is a comedic portrait of depression issues in modern lives. Project 3 is a documentary of restoring and riding a motorcycle for an Atlanta-Alaska trip. Project 4 is a family drama of dealing with abandonment issues and finding happiness. Project 5 is a pan-American travel journey in an old van. Project 6 is a motorcycle journey throughout Florida. Amount of prior similarityResearch on creativity has long been interested in the role of novelty in new product performance (see, e.g., [15]; [30]). Intuitively, a new idea will be perceived as less novel (or more imitative) when it is very similar to many previous ideas, especially the more recent ideas. The similarity network offers us a unique opportunity to objectively measure an idea's degree of novelty using machine learning techniques.Let  ti  and  Ti  denote the starting and completion dates of any project i. For any focal project i, we define the amount of prior similarity as  log(1+∑j:Tj<tiwij)  . We take the logarithm because the sum  ∑j:Tj<tiwij  has a highly skewed distribution. This metric resembles the definition of ""degree"" commonly used in social network analysis, with the notable exception that it restricts attention to the nodes that arrive before the focal node. Take the network in Figure 1 as an example. For project 5, we sum across the four links between it and projects 1–4. For project 6, we sum across all five links connected to it.The amount of prior similarity (inversely) measures the novelty of the focal project against the projects prior to it. Ex ante, the sign of the effect of novelty on funding performance is ambiguous. On the one hand, a very novel idea may be exciting but also difficult to understand and thus appreciate. On the other hand, an imitative idea is easy to understand but may appear unexciting and mundane to investors. Prior success rateAs discussed, one purpose of the similarity network is to help us select the most relevant prior projects when predicting the success of a project. For any focal project, we define the prior success rate as the average level of success among the projects prior to this project, with each prior project weighted by its link with the focal project. Formally, let  successj  be a dummy indicating whether a project j was successfully funded. The prior success rate for a focal project i equals  ∑j:Tj<tisuccessj×wij/∑j:Tj<tiwij  . By construction,  wij  takes into account both the similarity and recency of a prior project j with respect to the focal project. Conceptually, the prior success rate tries to capture a new project's quality (i.e., how appealing it is to investors) through the quality of historical projects (revealed by funding outcomes).Take Figure 1 as an example. The prior success rate for project 6 depends mostly on project 3 (failed) and project 5 (successful). It also depends on the other three projects, but to a lesser extent. The exact prior success rate for project 6 is.416. As another example, the prior success rate for project 5 is.099, which comes mostly from project 3 (failed) and, to a lesser extent, projects 1, 2, and 4. For those familiar with the social network analysis, this metric should resemble the definition of average peer behavior, except that it is restricted to the peers that arrive before the focal node. Prior success residualIn the calculation of prior success rate, all successes (or failures) of prior projects are treated equally regardless of how the successes (or failures) had been affected by the observed attributes of these projects. This equal treatment is convenient but omits some useful information. Particularly, consider two previous projects that were both successfully funded, but the first had a high funding goal while the second had a very conservative goal. Statistically, it should seem reasonable to regard the success of the former project to be ""bigger"" than the latter.Along this line of thought, we construct a refined version of the prior success rate. Specifically, we first estimate a logistic model regressing funding success onto a set of creator and project characteristics including the goal, description length, time trend, and so on (for the list, see the ""Control Variables"" subsection). Use  ri  to denote the residual of this regression for project i. We define  ∑j:Tj<tirj×wij/∑j:Tj<tiwij  as the prior success residual for project i. In other words, we replace  successj  with  rj  in the definition of the prior success rate. As we show subsequently, this replacement leads to a better prediction performance. Goal overshootFunding goal is an important instrument in the hands of project creators, which affects not only whether a project will be successfully funded but also the amount of funds it will collect ([27]). However, academic research has provided little guidance on how to set the goal optimally. One reason that makes offering guidance difficult is that there is likely no one-size-fits-all solution; the optimal goal should be highly dependent on the nature and content of the project. In this regard, the similarity network offers us a unique opportunity to use the historical funding achievement by similar projects as a project-specific reference point for setting the goal.Along this line, we measure the difference between the focal project's log funding goal and the average of the log funds raised by prior projects, with each prior project weighted by its link length with the focal project. Formally, the goal overshoot for i is defined as  log(1+goali)−∑j:Tj<tilog(1+fundsj)×wij/∑j:Tj<tiwij  . Here,  goali  denotes the funding goal of project i and  fundsj  denotes funds raised by project j. They are put in the log scale because their distributions are highly skewed.Our goal overshoot measures how much more funds the focal project asks than what the prior projects could raise. Under the social network analogy, it compares a focal node's goal with the average achievement of its preceding peers. For example, in Figure 1, the goal overshoot of project 6 mostly compares the project's goal of $1,000 with the funds raised by project 3 ($2,100) and project 5 ($15,673). It also takes into account the funds raised by the other three projects, but to a lesser extent. The exact goal overshoot is negative, at −1.78. This negative overshoot indicates a relatively conservative goal by project 6. AtypicalityA handful of more recent studies have found that the so-called atypical combination of prior ideas has an impact on a new idea's success ([35]; [36]; [38]). The general concept of atypical combination is as follows. Usually, a new idea imitates multiple prior ideas. The imitation of a prior idea is atypical if it looks uncommon or ""out of place"" compared with the other prior ideas imitated by this new idea. An example can be made in the context of scientific papers. Consider the bibliography of a marketing paper. In this case, citing marketing papers can be regarded as conventional, but citing a physics paper would be regarded as atypical. From a network perspective, a salient feature of this physics paper is that it is likely isolated from (i.e., neither citing nor cited by) the other papers in this bibliography.Unfortunately, it is difficult to define atypicality with a weighted network. So, for the definition of atypicality, we work with an unweighted network (like a citation network). We choose a cutoff and regard two projects as similar and thus linked in the unweighted network if their WMD is below the cutoff. Then, for each project i, there is a subnetwork consisting of i's prior similar projects (which corresponds to a paper's bibliography in the analogy to citation networks). We define the atypicality for i as the proportion of the isolated nodes in this subnetwork. In the special cases where i has no or only one prior similar project, the concept of atypicality does not apply, and we set the atypicality to zero. This is also the definition used in [38]. Specifically, we set the cutoff for the WMD at the.5 percentile of the WMDs across all pairs in a category. Higher cutoffs make isolated nodes too rare. Lower cutoffs result in too many projects having no prior similar projects. Control VariablesOur control variables fall into two categories: ( 1) project-related features or characteristics and ( 2) creator-related features or characteristics. Project-related featuresWe control for the features of the focal project that may affect the project's funding performance. Specifically, we include the log funding goal of the project, the log number of images in the project description, whether the project description features a video (dummy), the log length of the project description text, the project category (dummies), and controls for the time of the project (a time trend and quarter dummies). These features were included in [27], [42], and [ 9].[10] Creator-related featuresCreators may learn how to make a successful project through their experience on the platform. Accordingly, we include ( 1) a dummy indicating whether the creator had any prior projects before the focal project and ( 2) the average success rate of the creator's prior projects. The second feature is set to zero if the creator had no prior project. To control for creator heterogeneity more extensively, one could use creator fixed effects. However, this approach is infeasible given the nature of our data: close to 80% of the projects were made by one-time creators. ModelsOur regression models focus on the effects of the network-based metrics on the funding performance of crowdfunding projects. We use a logistic model to analyze the funding success: Pr(successi)=11+exp(−β′Xi−α′Zi). Graph( 2)In Equation 2,  successi  denotes whether project i is successfully funded (i.e., the total funds raised exceeded the funding goal). Vector  Xi  collects the project-related and creator-related control variables. Note that  Xi  includes a time trend, quarter dummies, and category dummies (Film & Video, Publishing, and Music). Vector  Zi  collects metrics based on the similarity network (some of which are information-weighted, as we discuss subsequently).In addition to  successi  , we examine how  Xi  and  Zi  affect the funds raised by projects. We specify the model as log(1+fundsi)=β′Xi+α′Zi+εi, Graph( 3)where  fundsi  denotes the total amount of funds that the project i raised, regardless of whether it exceeded the funding goal.We hold out the last quarter of 2017 at the end of our data for the out-of-sample prediction test. We estimate Models 2 and 3 on the projects from January 2016 to September 2017, which we call the main sample (Web Appendix B shows that our results are not sensitive to the choice of January 2016 as the start date of the main sample). The projects between 2009 and December 2015 are included in the computation of the network metrics for all the projects in the main sample as well as holdout sample. For example, when calculating the network metrics for a project that started on May 1, 2016, we include all the projects completed between 2009 and May 1, 2016 (in the same crowdfunding category). Table 4 provides the size of each sample as well as the summary statistics of variables in the main sample.GraphTable 4. Summary Statistics. Sample SizeFilm & VideoMusicPublishingAllFrom May 2009 to Dec. 201531,46530,30318,64780,415Main sample (Jan. 2016 to Sep. 2017)5,5585,0435,13915,740Holdout (last quarter of 2017)6185976881,903VariableAverageSDMinMaxSuccess.43.4901Funds raised (log)5.783.42014.15Prior success residual.04.35–1.301.28Amount of prior similarity4.942.34.009.43Goal overshoot8.468.11–25.1255.47Atypicality.04.1701Log funding goal8.561.424.6113.82Log number of images.871.0904.70Featuring a video.73.4401Log description length5.77.893.228.68Creator with past projects.19.3901Creator past success rate.11.3001 2 Notes: The reported variable statistics in the lower panel are for the main sample. We add 1 to the variable before taking log for the following variables: funds raised, funding goal, and number of images. The prior success residual and goal overshoot are information-weighted.For parameters  δ  ,  γ0  , and  γ1  , which determine link strengths (see Equation 1), we choose their values by cross-validation on the main sample. Specifically, we maximize the area under the receiver operating characteristic (ROC) curve of the success model. For the computation of the prior success residual, the logistic model used to extract the residual  ri  is estimated using all the projects before the holdout sample. These choices ensure that no data from the holdout sample are used to estimate any part of our models.Beyond prediction, we may make causal interpretations on the coefficients in Models 2 and 3, if there is no omitted variable that affects funding outcomes and correlates with the regressors. In our context, the probable omitted variable is the project quality that is unobserved to researchers but taken into account by creators when posting the project. This omitted variable is largely unaddressed by previous studies (e.g., [27]; [32]). In this article, we control for this quality using the prior success residual. It is possible that the unobserved quality known to creators goes beyond what historical projects can tell; they may use knowledge from other crowdfunding sites or other creative industries. Controlling for such outside knowledge is beyond the scope of this research. In addition, note that while potential endogeneity may affect the interpretation of our coefficient estimates, it does not invalidate the prediction performance of our models ([13]). Information weightingSome of our network metrics are based on averages across prior projects. We multiply these metrics by an ""information weight"" before they enter  Zi  in the models. Next, we use the prior success residual to explain the rationale behind this weighting. The prior success residual is an average weighted by  wij  . Intuitively,  wij  measures the relevant information supplied by prior project j for predicting about i. Thus, the information supplied by the prior success residual, which aggregates across all the j such that  Tj<ti  , increases with  ∑j:Tj<tiwij  .To capture the amount of relevant information, we multiply the prior success residual with an information weight  Ii≡log(1+∑j:Tj<tiwij)  . We make this specification for  Ii  for two reasons. First, the prior success residual should have little relevance when there is little similarity between project i and prior projects. Thus, the information weight should be zero when  ∑j:Tj<tiwij=0  . Second, we know that in Bayesian updating, the value of an additional signal tends to diminish as one obtains more signals. Similarly, the marginal gain in information weight should decrease as  ∑j:Tj<tiwij  increases. Our specification for  Ii  satisfies both of these two properties. Note the definition of  Ii  coincides with the amount of prior similarity, but the two have very different roles in our models. The amount of prior similarity characterizes project novelty, whereas  Ii  measures information.There is also empirical support for the specification of the information weight. In either Model 2 or 3, if we allow the coefficient in front of the prior success residual to vary flexibly with the sum  ∑j:Tj<tiwij  (by using dummies for different levels of this sum), the coefficient estimate will turn out to increase approximately linearly in  Ii  . This result justifies our specification of  Ii  . We show this result in Web Appendix C.In addition to the prior success residual, we also weight the prior success rate and goal overshoot with  Ii  , because these two metrics are also based on averages across prior projects weighted by  wij  . We do not weight the amount of prior similarity or atypicality. ResultsIn this section, we first present the estimates of the regression models outlined in the ""Models"" section. Then, we present the prediction performance of the models in the holdout sample. Model EstimatesTable 5 presents the estimation results for funding success. Table 6 presents the results for funds raised. In each table, we gradually build up the model by adding network metrics, with the last column representing the full model.[11]GraphTable 5. Model Estimates for Funding Success. (1)(2)(3)(4)Coef.SECoef.SECoef.SECoef.SEPrior success residual2.8***(.077)2.87***(.081)2.69***(.100)2.7***(.100)Amount of prior similarity.189***(.038).219***(.040).207***(.040)Amount of prior similarity2−.0199***(.004)−.0214***(.004)−.0203***(.004)Goal overshoot.031***(.008).0309***(.008)Goal overshoot2−.0159***(.002)−.0159***(.002)Atypicality1.12**(.531)Atypicality2−1.18**(.556)Log funding goal−.126(.139)−.108(.140)−.66***(.157)−.658***(.157)Log funding goal2−.0296***(.008)−.0306***(.008).00667(.010).00656(.010)Log number of images.375***(.024).37***(.024).357***(.025).357***(.025)Featuring a video1.2***(.057)1.2***(.057)1.18***(.057)1.18***(.057)Log description length1.47***(.272)1.31***(.274)1.26***(.281)1.28***(.282)Log description length2−.0783***(.023)−.0652***(.024)−.0663***(.024)−.0675***(.024)Creator with past projects−.881***(.085)−.877***(.085)−.88***(.085)−.879***(.085)Creator past success rate2.38***(.117)2.37***(.117)2.36***(.117)2.36***(.117)Trend and quarter dummiesYesYesYesYesCategory dummiesYesYesYesYesR-squared.335.336.339.339N15,74015,74015,74015,740 3 *p < .1.4 **p < .05.5 ***p < .01.6 Notes: The dependent variable is whether the project was successfully funded. See the subsections ""Network-Based Metrics"" and ""Control Variables"" for detailed definitions of regressors. The prior success residual, goal overshoot, and goal overshoot squared are all information-weighted. Pseudo R-squared is used. The cross-validated parameters for network construction are  δ=.9309  ,  γ0=56.84  ,  γ1=21.14  (see Equation 1).GraphTable 6. Model Estimates for Funds Raised. (1)(2)(3)(4)Coef.SECoef.SECoef.SECoef.SEPrior success residual3.31***(.060)3.44***(.062)3.42***(.083)3.43***(.083)Amount of prior similarity.29***(.035).295***(.038).278***(.039)Amount of prior similarity2−.0295***(.004)−.0336***(.004)−.0318***(.004)Goal overshoot.0793***(.006).0792***(.006)Goal overshoot2−.0198***(.001)−.0199***(.001)Atypicality1.12**(.472)Atypicality2−1.02**(.494)Log funding goal.925***(.116).941***(.116)−.103(.131)−.102(.131)Log funding goal2−.053***(.007)−.0534***(.007).00674(.008).00677(.008)Log number of images.65***(.024).638***(.024).62***(.024).62***(.023)Featuring a video1.55***(.052)1.54***(.052)1.5***(.052)1.5***(.052)Log description length2.23***(.232)1.94***(.235)1.87***(.238)1.89***(.238)Log description length2−.131***(.020)−.109***(.020)−.106***(.020)−.107***(.020)Creator with past projects−.875***(.080)−.851***(.080)−.864***(.081)−.863***(.081)Creator past success rate2.1***(.092)2.06***(.093)2.08***(.092)2.08***(.092)Trend and quarter dummiesYesYesYesYesCategory dummiesYesYesYesYesR-squared.501.503.512.512N15,74015,74015,74015,740 7 *p < .1.8 **p < .05.9 ***p < .01.10 Notes: Dependent variable is the log of the funds raised. The notes for Table 5 apply.We first examine the coefficient estimates for the control variables. Consistent across the columns in both tables, accompanying project descriptions with images and videos has positive impacts on funding outcomes.[12] Providing a longer description initially has a positive effect on funding outcomes but starts to impose a negative effect when it becomes too lengthy. The past success of the project creator is indicative of their future success. A creator with past projects is less likely to succeed than a new creator if these past projects all have failed. The effect of the funding goal is more complex, which we discuss subsequently (together with the effect of goal overshoot).Before examining the effects of each of the network metrics, we discuss the parameters used to assign the link strength  wij  (see Equation 1). The cross-validated values for these parameters are  δ=.9309  ,  γ0=56.84  , and  γ1=21.14  . Parameter  δ  is an annual discounting factor, so  wij  decreases by a factor of.9309 if the time gap between project i and j increases by a year. Without considering this time factor (i.e., setting  δ=1  ), about 20% of the project pairs have  wij  larger than.01, 8% larger than.1, and 3% larger than.5. In other words, the values for the vast majority of links are small. As a result, when predicting the funding performance of a focal project, our models rely mainly on those prior projects that are most similar to the focal project. Effect of prior successAll columns of Tables 5 and 6 include the prior success residual. By definition, this metric captures the average level of success of the projects before the focal project, giving more weight to the projects similar to and more recent relative to the focal project (the annual discounting factor is  δ=.9309  ). We see that the prior success residual has a significantly positive effect on both the funding success (coef. = 2.70, p < .01) and funds raised (coef. = 3.43, p < .01). The estimates are consistent across all columns. So, a higher prior success residual is a significant indicator of better funding outcomes, even after we control for various project and creator features.As we have discussed, the prior success residual is a refinement of the prior success rate, in the sense that it adjusts the success or failure of each prior project against the control characteristics of that project. For example, it gives the success of a prior project more importance if that project had a more ambitious goal. For comparison, we estimate the model using the prior success rate (instead of residual). The prediction performance turns out to be somewhat lower, which motivates us to use the prior success residual. Please see Web Appendix A.[13] Effect of noveltyColumns 2–4 of Tables 5 and 6 include the amount of prior similarity. Given the way we have defined this metric, a zero value for the amount of prior similarity means that the focal project does not share any similarity with any of the projects prior to it (i.e., a completely original idea). A large positive value means the focal project has a great amount of similarities with past projects.Our prior belief is that the effect of the amount of prior similarity is likely nonlinear. Therefore, we include a quadratic term. Interestingly, we see an inverted U-shaped effect on funding outcomes. In Table 5, the coefficient for the amount of prior similarity is positive (coef. = .207, p < .01), but the coefficient for its squared term is negative (coef. = −.0203, p < .01). The same shape holds in Table 6, with the linear coefficient being positive (coef. = .278, p < .01) and the quadratic coefficient being negative (coef. = −.0318, p < .01). In either table, the estimated quadratic relation reaches its peak around the median of the amount of prior similarity.[14]This result suggests that a project benefits most from a balanced level of novelty. Featuring ideas that are either too novel or too formulaic hurts a project in raising funds. In the literature on new product development, some studies suggest that novelty has a positive effect on success (e.g., [10]; [30]), some suggest a negative effect (e.g., [18]), and others suggest less clear relations (e.g., [19]). We find a clear inverted U-shaped effect of novelty on funding performance in crowdfunding. This insight is useful for creators when selecting or fine-tuning their crowdfunding ideas, and we take it into account when devising recommendation tools for creators. Effect of funding goal (overshoot)Columns 3 and 4 of Tables 5 and 6 include the funding goal overshoot. Given the way we have defined it, this network metric compares the focal project's log funding goal against the average log funds raised by the prior projects, especially those projects similar to the focal project. In other words, this metric captures the focal project's goal relative to prior similar projects. In contrast, the log funding goal included in the control variables captures the absolute level of the funding goal.We include linear as well as quadratic terms for the goal overshoot (each term is information-weighted). In Table 5, we see that the coefficient for the linear term is positive (coef. = .0309, p < .01), while the coefficient for the squared term is negative (coef. = −.0159, p < .01). We plot this estimated quadratic curve in Figure 2, Panel A. We see an inverted U-shape. Note that this curve does not capture the complete effect of funding goal on success. When a creator increases the goal, it affects two variables in our model: the goal overshoot and the log funding goal. In Panel B of Figure 2, we plot the quadratic effect estimated for the log funding goal. We see the curve is always downward sloping. Taking the two curves together, we see that an increase in funding goal has a relatively small impact on the success probability when the goal overshoot is negative or around zero, but the impact becomes negative and exacerbates quickly when the goal overshoot becomes positive and large.Graph: Figure 2. Effect of funding goal (overshoot) on success.In Figure 3, we plot the same curves but for Table 6, which predicts funds raised instead of funding success. Panel A shows an inverted U-shape, but Panel B is almost flat. Taking the two effects together, we see that the funding goal has an inverted U-shaped effect on the expected funds to be raised, with the peak achieved when there is a positive but close-to-zero level of goal overshoot.[15]Graph: Figure 3. Effect of funding goal (overshoot) on funds raised.These results are interesting and not straightforward; yet they are also intuitive. When the goal is set too high compared with prior similar projects, investors find the creator requesting an unnecessarily large amount of funds. As a result, investors may question the creator's intention or the profitability of the project. When the goal is set too low compared with prior similar projects, investors may start to question whether the project is viable with the asked amount of funds. In this sense, the zero overshoot point sets up a benchmark for how much funds investors typically consider reasonable for a new project.One interesting question is why creators set suboptimal goals. In our main sample, more projects have goal overshoots than undershoots. More interestingly, we find that the extent of goal overshoot tends to decrease with creator experience, measured by the number of creator's past Kickstarter projects. This relation suggests that, on average, experience corrects creators' overconfidence and allows them to set more optimal goals. On Kickstarter, raised funds cannot be claimed by the creator unless they surpass the goal. Thus, setting a higher goal increases the risk of a funding failure. However, as our results show, lowering the goal too much decreases the amount of raised funds. Therefore, setting the optimal goal means a careful trade-off between increasing the chance of success and raising more funds. We take this trade-off into account when designing a recommendation tool for creators to adjust their funding goals. Effect of atypicalityThe last columns of Tables 5 and 6 include the measure of atypicality. This network metric relies on the observation that a new idea is typically grounded in a stream of highly related prior ideas. It then measures the extent to which the new idea also imitates prior ideas that are outside this stream. Recall that our measure of atypicality ranges between 0 and 1.In Table 5, the linear coefficient for atypicality is positive (coef. = 1.12, p = .03), and the quadratic coefficient is negative (coef. = −1.18, p = .03). Table 6 shows a similar result, with the linear coefficient being positive (coef. = 1.12, p = .02) and the quadratic coefficient being negative (coef. = −1.02, p = .04). These estimates indicate that the effect of atypicality has an inverted U-shape. That is, neither too little nor too much atypicality benefits funding performance. Thus, the creator should keep a balance between conventional and atypical combinations of prior ideas.[16]Among our network metrics, atypicality actually brings the smallest improvement for predicting success in our holdout sample. However, this small improvement is likely due to the fact that the vast majority of the projects in data have near-zero levels of atypicality. Thus, the lack of predictive power does not necessarily imply a lack of effect. In fact, most projects can benefit from an increase in atypical imitation. With everything else equal, if an average project changes its atypicality from zero to the optimal level, the probability of success will increase by almost 20% of the original level by our estimates. Out-of-Sample PredictionPredicting success is a central research topic in the context of crowdfunding. We test the performance of our model in predicting funding success. Table 7 reports the prediction performance measured by accuracy, F1-score, and ROC-area under the curve (AUC), on the projects in the last quarter of 2017. The models are estimated using the data before this quarter.GraphTable 7. Prediction Results for Holdout Sample. AccuracyF1-ScoreROC-AUCNetwork metrics, creator controls, and project controls.799.774.874Creator and project controls.749.714.830Project controls.725.688.798Project controls—only time and category.582.442.552 11 Notes: All numbers are computed on the holdout sample from October 1 to December 31, 2017. The models are trained using data before October 1, 2017. Both the accuracy and F1-score use.5 as the probability threshold when predicting success.The table compares four different model specifications. The top row shows our full model. The second row drops all network metrics. The third row further drops creator-related controls. The last row presents the leanest model that keeps only a time trend, quarter dummies, and category dummies.Across all three prediction measures (accuracy, F1-score, and ROC-AUC), the full model performs substantially better than the models without network metrics, demonstrating the value of the information added by the similarity network. These improvements compare very well with [35] and [34]. In fact, if we use the leanest model in the last row of Table 7 as a baseline, then adding the network metrics improves the out-of-sample prediction more than adding all other project and creator controls (i.e., creator past success rate, creator with past projects, funding goal, text description length, number of images, and video dummy). Managerial ImplicationsThis section derives actionable implications for project creators and platforms, taking advantage of the unique information in our similarity network. We focus on two major decisions of project creators: ( 1) how to set the appropriate funding goal and ( 2) how to improve the content of the project. We devise the two recommendation tools accordingly. The recommendations can be offered by the platform to a creator after the creator submits a new project to the platform (but before the project is posted on the site).The goal recommendation suggests an adjustment (e.g., a 5% increase) to the funding goal set by the creator. The adjustment aims to let the creator collect more funds, and it is based on our models presented in the ""Results"" section. Thus, a unique feature of our recommendation is that we account for the effect of goal overshoot, which benchmarks the new project's goal against the funds historically raised by similar projects. Consequently, our recommendation is customized for individual projects.The content recommendation suggests a prior project for the creator to consider imitating (by revising the submitted project). The objective is to improve the probability of success. The recommendation is based on our models presented in the ""Results"" section, so again, it is customized for individual projects. Funding Goal RecommendationRecall that Kickstarter applies an all-or-nothing policy, whereby project creators collect the raised funds only if the funding is successful (i.e., the raised funds surpass the goal). As a result, setting the correct goal is crucial. Setting the goal too high may cause the creator to end up collecting nothing. Setting the goal too low may lead to insufficient funds collected for the project.Currently, Kickstarter does not advise creators to set goals in a way to maximize their funds collection. Figure 4 shows the current goal-setting page when one submits a project on Kickstarter. The creator first inputs an estimated project budget. Kickstarter then suggests a funding goal, but the suggestion is aimed to simply satisfy the creator-inputted budget after mechanically deducting taxes and fees.Graph: Figure 4. Goal-setting page on Kickstarter.We use our estimated models (the last columns of Table 5 and 6) to suggest a goal adjustment to improve the project's funding outcomes. For each project in our holdout sample, we consider adjustments of the goal ranging from −10% to +10%. There are two reasons why we restrict attention to such conservative adjustments. First, doing so ensures that, in the event of successful funding, the creators will receive funds in the ballpark of what they initially decided was adequate for the project. Second, it alleviates the concern of funding competition among projects. At an aggregate level, there is likely a limit on the total funds available from investors for crowdfunding. A large goal adjustment may benefit an individual project, but if many projects make large and upward goal adjustments, these projects likely will have to compete heavily for the aggregate funds available. As a result, analyzing fundraising under large goal adjustments requires accounting for competitive effects between projects, which is out of the scope of our article.Changing the goal is likely to simultaneously affect the success probability and expected funds to be raised. Improving one of these two aspects may hurt the other aspect. To balance them, we select the goal adjustment for recommendation in the following way. We check two possibilities: ( 1) increasing the success probability by at least 5% with the constraint of not hurting the expected funds by more than 1% and ( 2) increasing the expected funds by at least 5% with the constraint of not hurting the success probability by more than 1%. We first try possibility ( 1) then ( 2). We do not make a recommendation if neither possibility exists.Table 8 presents the results. Out of the 1,903 projects in the holdout sample, we end up recommending goal increases for 155 projects (column 1) and goal decreases for 697 projects (column 2). For the remaining 1,051 projects, we do not recommend a goal adjustment. What drives the direction of our recommendation is the network metric of goal overshoot. To illustrate this driving force, we can compare a project's original goal with the average funds raised by its prior similar projects. Table 8 shows how this comparison turns out, on average. We see that the projects in column 1, on average, had a goal that was too conservative ($2,306 vs. $10,025), whereas the projects in column 2, on average, had a goal that was too ambitious ($28,293 vs. $4,588).GraphTable 8. Recommending Funding Goal Adjustments. Recommend a Goal IncreaseRecommend a Goal DecreaseFunding goal (original)$2,306$28,293Average funds raised by prior similar projects$10,025$4,588Success probability (original).825.118Change in success probability−.4%+7.9%Change in funds raised+6.6%+2.1%Number of projects155697 12 Notes: Goal adjustments are constrained to be within ±10%. The numbers in each column are averaged across the projects belonging to that column. For each project i, the ""average funds raised by prior similar projects"" weigh each prior project j by  wij  .On average, the goal-increasing adjustments bring a 6.6% increase to the expected funds to be raised at the expense of only a.4% loss in success probability. The goal-decreasing adjustments bring a 7.9% increase in success probability as well as a 2.1% increase in the expected funds to be raised—overly high goals hurt both the success probability and expected funds to be raised. These improvements are substantial, especially considering that ( 1) adjusting the funding goal is costless and ( 2) the adjustments are not large—within ±10% of the original goals. Content RecommendationGiven a new project i, we aim to make a suggestion to help the creator to improve the project's content. The way we implement this suggestion is to recommend a prior project j for the creator to imitate. This imitation amounts to an increase in the similarity between i and j, which will then change our predictions for the funding outcomes for i. We want to recommend a prior project j so that the expected funding outcomes of project i can be improved.The key question here is exactly which prior project to recommend. We choose the project such that an increase in the similarity between it and project i will most substantially improve project i's success probability (it gives very similar results if we instead focus on improving the funds raised). Specifically, for each prior project j, we calculate the change in project i's success probability if the WMD between j and i is reduced by 10%. This calculation is made using our model for funding success (last column of Table 5). Note this reduction in WMD generally changes all the network metrics for project i. We recommend a prior project if it leads to the largest increase in success probability among all prior projects and if the relative increase is larger than 5%.Take Figure 1 as an illustration. Suppose that project 6 is the new project. We first try recommending project 1 and calculate how the model-predicted success probability for project 6 will change if the WMD between project 1 and 6 is reduced by 10% (which will result in a stronger link between the two projects). We try each of projects 1–5 in turn and select the one that results in the highest increase in success probability for project 6. Suppose we select and recommend project 5. Then project 6's creator may bring project 6 closer to project 5 by, for example, enriching their motorcycle journey with more interactions with the cultures and people along the journey (for details of each project, see Table 3).Table 9 presents the results. We make recommendations for 887 projects out of the 1,903 projects in the holdout sample. For the remaining projects, we do not make recommendations because the improvement in success probability would be smaller than the 5% threshold. Column 1 displays the statistics of the projects for which we make recommendations, whereas column 2 displays the statistics of the projects for which we do not make recommendations. There are several observations about the original attributes of the projects for which we make recommendations. First, the funds raised by these projects are noticeably lower compared with the other projects ($2,167 vs. $8,309). In addition, they have a substantially lower success probability (.209 vs..633). They also tend to have a far smaller amount of prior similarity (3.3 vs. 6.9).GraphTable 9. Recommending a Prior Project for Imitation. With RecommendationNo RecommendationFunds raised (original)$2,167$8,309Success probability (original).209.633Amount of prior similarity (original)3.36.9Improvement in success probability.055—Improvement in funds raised$240—Number of projects8871,016 13 Notes: The numbers in each column are averaged across the projects belonging to that column.These stark differences give the following intuition. Recall that the success probability has an inverted U-shaped relation with the novelty of the project. The projects with a very small amount of prior similarity are too novel for investors to understand or appreciate. So, by asking the creators to bring these highly novel projects slightly closer to some successful prior projects, we can significantly increase their appeal to investors and thus their probability of success (from.209 to.264).Another reason that our recommendation does not target projects with a large amount of prior similarity is that for these projects, imitating a single prior project can cause only a small marginal effect on the success probability. This observation suggests that we recommend multiple prior projects. However, such recommendations are more complex to design and demand a larger effort from creators. In addition, when such recommendations are adopted by creators, there will be larger changes in the similarity networks. Thus, aggregate effects at the platform level will likely be substantial. These effects are beyond the scope of our models and typically require a structural model to capture (which is similar to the reason why we have restricted ourselves to small adjustments in the goal recommendation). Concluding RemarksHow can we improve crowdfunding project proposals to design winning projects? Few studies have empirically addressed this important and exciting question. This study aims to help answer this question using the concept of combinatorial creativity. First, we extend and apply the concept of combinatorial creativity in crowdfunding, taking advantage of modern machine learning methods to measure a complete similarity pattern between crowdfunding projects on Kickstarter. Second, using this similarity pattern, we significantly improve the out-of-sample prediction of funding success. Third, we derive novel insights on how a project's funding performance is affected by its similarity pattern with prior projects. Our results allow us to provide concrete recommendations for creators to improve their project proposals.Several results emerge from our analysis. First, the similarity network provides an important information source for predicting success. In particular, the historical success of similar projects is a significant predictor for new project success, after we control various project and creator characteristics. Second, there is an inverted U-shaped relationship between novelty and funding performance—being either too novel or too imitative hurts the project. Third, when setting the funding goal, creators should benchmark the goal against the funds raised historically by similar projects. A goal that is either too high or too low compared with this benchmark lowers the funding performance of the project. Fourth, there is an inverted U-shaped relation between the extent of atypical imitation and funding performance. Either too little or too much atypicality harms the funding performance of a project.We take advantage of the similarity network to devise recommendation tools to assist project creators in two critical decisions: ( 1) setting the funding goal and ( 2) developing the project content. Currently, Kickstarter reviews new project proposals (and rejects a portion of them) but does not offer customized help to creators to improve these proposals. Our recommendations can be offered by the platform at this review stage to assist creators, especially less experienced ones, in developing successful projects. Our first recommendation suggests a small adjustment to the funding goal. Our second recommendation suggests a prior project for the creator to imitate. Both suggestions are customized for individual projects, taking into account the effects of prior success, novelty, goal overshoot, and atypicality. For individual projects in our holdout sample, we show that these recommendations can lead to meaningful increases in funding performance. With the use of the external Word2vec by Google, the recommendation tools are easy to implement and update over time on the platform.Although this article focuses on crowdfunding as the empirical context, the developed approach can be applied to other text-based creative settings, such as mobile apps, venture capital, news stories, and blog websites. The first task in these settings is to obtain a repository of text descriptions of the ideas, whether they are apps, start-ups, or articles. Once the text descriptions are obtained, one can adopt the WMD to automate the measurement of similarity between ideas. Next, one can construct a similarity network between these ideas, accounting for both the content similarity and time proximity, as we have done in this article. Then, the network-based metrics we developed, including prior success residual, novelty, and atypicality, can be applied to improve predictions of idea success. The goal overshoot may not be directly applicable in contexts outside crowdfunding, but variants may be used. For example, in the case of mobile apps, one could compute a ""price overshoot"" as the difference between a focal app's price and the average price of prior similar apps.The aforementioned applications of similarity networks allow us to explore interesting questions specific to the domain of application. For example, for user-generated content such as blog articles, does novel or familiar content appeal to consumers, or is there an optimal level of balance between novelty and familiarity? How should writers determine the appropriate length of an article—can prior similar articles provide a useful benchmark? For new app development, can prior prices of similar apps help in setting the price of a new app? How should the developer choose the optimal degree of imitation? Should the app focus on developing a core function or integrating multiple functions delivered by different past apps? In the age of the long tail ([ 2]), it is more exciting than ever to study the similarity pattern among the great number of products as well as the relationship between this similarity pattern and product success.  "
30,"Measuring the Real-Time Stock Market Impact of Firm-Generated Content Firms increasingly follow an ""always on"" philosophy, publishing multiple pieces of firm-generated content (FGC) every day. Current methodologies used in marketing are unfit to unbiasedly capture the impact of FGC disseminated intermittently throughout the day on stock markets characterized by ultra-high-frequency trading. They also neither distinguish between the permanent (i.e., long-term) and temporary (i.e., short-term) price impacts nor identify FGC attributes capable of generating these price impacts. In this study, the authors define price impact as the impact on the variance of stock price. Employing a market microstructure approach to exploit the variance of high-frequency changes in stock price, the authors estimate the permanent and temporary price impacts of the firm-generated Twitter content of S&P 500 information technology firms. The authors find that firm-generated tweets induce both permanent and temporary price impacts, which are linked to tweet attributes of valence and subject matter. Tweets reflecting only valence or subject matter concerning consumer or competitor orientation result in temporary price impacts, whereas those embodying both attributes generate permanent price impacts. Negative-valence tweets about competitors generate the largest permanent price impacts. Building on these findings, the authors offer suggestions to marketing managers regarding the design of intraday FGC.Keywords: real-time marketing; microstructure; high-frequency data; firm-generated content; TwitterWith U.S. firms investing in excess of $37 billion in 2020 ([69]), social media is one of the most pervasive communication channels used by marketers ([ 5]; [36]). The result of this investment is the creation of corporate social media accounts that support firm-generated content (FGC), defined as a firm's communications disseminated through its own online communication tools ([45]). Many firms have adopted an ""always on"" approach in their social media marketing, disseminating multiple pieces of FGC throughout the day. Figure 1 illustrates the high-frequency approach to FGC dissemination using the example of information technology (IT) firms' activity on Twitter. Each piece of FGC is characterized by its attributes (e.g., Figure 1 shows FGC's valence and subject matter as key attributes),[ 5] and the timestamps show the dissemination time for each piece of FGC. Each piece of FGC and its timestamp can be accurately recorded to the second and mapped against the corresponding timestamp of trading activity that takes place at ultra-high frequency, that is, at subsecond intervals (Hasbrouck and Saar 2013). As a result of these high-frequency activities, large volumes of intraday data emerge. For example, an S&P 500 IT firm can issue in excess of 6,000 tweets in a given month, and trading in a single firm's stock often yields well over 10 million trading-related messages (e.g., quotes, cancellations, transactions) during the same interval (see Web Appendix A).Graph: Figure 1. A high-frequency approach to FGC dissemination on the example of IT firms' activity on Twitter.Current marketing methodologies are challenged when analyzing high-frequency data because the trading data, which is used in capturing the impact of FGC on firm value, is characterized by unequal time intervals. Low-frequency event studies using end-of-day price measures and time series analysis, such as standard vector autoregressive (VAR) models, are unable to effectively address these problems (see Web Appendix B). These methods rely on discrete and uniform time intervals that do not align with the time intervals associated with high-frequency trading data, which encapsulate the intraday evolution of firm stock price. Often, these methods aggregate trading data to regular intervals, which can lead to the elimination of upwards of 99% of intraday trading observations in studies employing end-of-day price data (see Web Appendix C). The frequency of FGC as an intraday variable and the effects of other non-FGC events potentially further bias low-frequency analyses that employ standard low-frequency analytical methods. Furthermore, the richness of such an assessment and marketing researchers' understanding of ""always on"" strategies are compromised unless the research identifies both the short- and long-term impacts of this form of marketing ([26]). Consequently, the findings derived from current examinations lack detailed ex post insights on the impact of FGC generated at intraday frequencies, which leaves marketing managers unable to effectively design future intraday marketing resource allocation strategies ([41]).Employing the market microstructure approach, which relies on ultra-high-frequency trading data analysis, we investigate the stock price impact of FGC where price impact is defined as the impact on the variance of stock price. This approach of estimating price impact of FGC as impact on the variance of stock price rather than on changes in the level of stock price is driven by both methodological and theoretical necessity. Although estimating level changes in stock price as a result of an event such as FGC dissemination could be approached from the perspective of computing simple price impact measures, when working with high-frequency data, this approach is inadequate for at least two reasons. First, simple price impacts are misleading when trades in stock markets are serially correlated. Second, in the presence of transient impacts, as is the case in this study (e.g., we capture price impact at second-by-second intervals), simple price impacts rely on getting the timing just right, which is methodologically unfeasible when investigating large data sets, as is the case here. Our methodological approach is in line with market microstructure theory, addresses the previously outlined issues, and is consistent with the microstructure literature ([11]; [65]). Using this approach and assessing S&P 500 IT firms' Twitter activity, we contribute to the marketing literature at three levels.First, by using high-frequency data, this is the first study to document the subminute impact of individual pieces of FGC disseminated during the day; therefore, the insights presented are unlikely to be affected by the confounding effects that the use of end-of-day data is susceptible to. We estimate the price impact of FGC at the second and minute levels by computing the variance of fast-paced (e.g., subsecond-by-subsecond) intraday changes in stock price as they occur in financial markets ([11]; [12]; [42]), thereby demonstrating the instantaneous impact of FGC. We obtain the variance of intraday changes in price through state-space modeling with Kalman filtering. By doing so, we contribute to the emerging stream of marketing research studying ( 1) the impact of FGC on firm financial outcomes ([ 8]; [15]) and ( 2) real-time marketing ([64]), and we respond to research priorities established by the Marketing Science Institute (2018), emphasizing the need to help marketers ""get marketing right"" by providing insights into the instantaneous impact of FGC.Second, using the microstructure perspective, we reveal both the permanent and temporary price impact of FGC as new forms of FGC impact on firm-level performance. In the process, we address [26] call for research capable of identifying both the short- and long-term financial impact of a marketing activity, which has thus far remained difficult to quantify. By being able to distinguish between temporary and permanent price impacts at the fine-grained level of analysis, marketing managers can demonstrate both the short- and long-term impacts of FGC on firm financial performance ([53]). This, in turn, will allow them to overcome short-termism in marketing and improve long-term growth initiatives ([18]; [58]).Finally, to support intraday actionable FGC design, we examine the extent to which key attributes of FGC, including content valence and subject matter, influence the occurrence of permanent and temporary impacts of FGC on price. Although FGC valence and subject matter have been examined by previous research (e.g., [20]; [27]; [36]) and are recognized as key components of marketing excellence ([39]), what constitutes the ""right content"" is largely unknown according to research priorities recently published by the Marketing Science Institute (2020). We show that FGC reflecting only one of the attributes of valence (positive or negative) or subject matter (consumer or competitor orientation) generates temporary price impact, whereas FGC that incorporates both valence and subject matter is associated with permanent price impacts on stock price and thus correlates with long-term firm performance.Using a two-stage least squares (2SLS) estimation framework to examine S&P 500 IT firms' Twitter activity, we find that negative- or positive-valence tweets are consistently linked with a reduction in permanent price impact and an increase in temporary price impact. Similar findings are obtained when examining tweets that only reflect a consumer or competitor subject matter, although the reduction in permanent price impact and increase in temporary price impact they elicit are of smaller magnitudes. These findings indicate that tweets reflecting only valence (positive or negative) or subject matter (consumer or competitor orientation) result in temporary price impacts, which is commonly associated with the incorporation of noise into the price discovery process ([60]). This type of effect has not been studied previously in the marketing literature; however, given that it is a source of uncertainty in the value of firms that can increase a firm's cost of capital ([17]), it demands attention. Employing the market microstructure approach to exploit the variance of high-frequency changes in stock price, this is the first study that reveals tweets' temporary price impacts and identifies tweet attributes that elicit such short-term impacts on price.We further find that tweets that embody both attributes—valence and subject matter—generate permanent price impacts; however, this impact varies according to the type of valence and subject matter. The results evidence the importance of interaction effects between tweet valence and subject matter in generating a higher permanent price impact. The average negative- and positive-valence tweet, when viewed through the lens of a consumer or competitor orientation, generates a permanent price impact, and a competitor-oriented tweet with a negative valence is likely to have the highest permanent price impact. This is a crucial finding from the perspectives of marketing practice and intraday social media marketing strategy design because valence, as a singular attribute, is associated with decreasing permanent price impact. Our research shows that information-rich tweets that include both variance and subject matter can result in permanent price impacts, and it demonstrates investors' ability to act on information contained in FGC at subsecond levels (Hendershott et al. 2011; [65]).To illustrate the relevance and magnitude of these findings, in Figure 2, tweets A and B are characterized by negative and positive valence, respectively, and tweets C and D reflect only consumer and competitor orientation, respectively. In line with our research findings, the permanent price impacts associated with these tweets are more than three standard deviations lower than the average permanent price impact estimates for all the 153,041 tweets in our sample and are therefore below the 10th percentile of the estimates. In contrast, tweets E to H reflect varying combinations of both valence and subject matter (consumer or competitor orientation). Consistent with our findings, we show that these tweets generate permanent price impact estimates ranked above the 90th percentile in our sample of tweet trades' permanent price impact estimates. The temporary and permanent impact estimates for the average tweet are as large as 279 and 187 times, respectively, what we document for the average regular intraday transaction in our sample.Graph: Figure 2. Examples of tweets characterized by valence and subject matter (consumer and competitor orientation). Theoretical Background FGC and High-Frequency Trading DataFirms increasingly use social media because it provides greater reach and can be less costly than traditional channels for FGC dissemination ([45]). FGC is often posted several times a day ([41]) and serves as a valuable source of high-frequency marketing data that can offer insights into the growth potential of a firm ([18]). With the advancement of data collection tools ([79]), marketing researchers can now record each piece of FGC and create large data sets depicting FGC attributes and their dissemination time. Recorded with accuracy to the second, FGC can then be mapped against the corresponding trading activity that takes place at subsecond intervals and used to study its financial impact. However, measuring the impact of FGC sampled at intraday levels requires marketing researchers to be able to utilize high-frequency trading data with observations occurring at unequal time intervals.The market microstructure approach to estimating price impact offers marketing researchers tools to, piece-by-piece, algorithmically link FGC to time-specific trading activity at a fine-grained level of analysis (e.g., subseconds, seconds, minutes). Unlike symmetrical asset pricing models, market microstructure recognizes that a firm's stock price is only informationally efficient to the extent that it reflects all available and relevant information ([21]). A firm's stock price, while reflecting information, is also distorted by noise generated by (temporary) non-information-based factors. Such factors can include trading frictions due to low levels of liquidity, defined as the ability to trade large quantities of a firm's stock quickly with little or no price impact ([ 2]; [28]), or the activity of traders who lack adequate information regarding the value of a stock, known as ""uninformed traders"" in the market microstructure literature ([24]; [47]). Estimating the proportion of stock price driven by information (relevant to the value of a firm) and the proportion driven by noise is a critical aspect of the analyses many studies conduct in the market microstructure literature (see Web Appendix D). However, this holistic view of both temporary and permanent price impacts is often missing from marketing research. The market microstructure approach allows marketing researchers to estimate the price impact of FGC at high frequencies and to identify both types of price impacts. A crucial element in such analysis is knowing the ""event time"" (i.e., timestamp), which refers to the time at which an event occurs, such as the FGC dissemination time. By deploying time series models to estimate changes in the components of price at high-frequency intervals (e.g., seconds) and then linking the FGC timestamp to the components, the instantaneous impact of FGC on firm stock price can be estimated. FGC's Impact on Firm OutcomesExtant research has primarily focused on linking FGC with consumer behavior ([15]; [13]; [36]; [45]; [55]; [72]) and firm performance ([ 8]; [15]; [64]) (see Web Appendix E). With the focus on firm performance, [15], [ 8], and, most recently, [64] show that FGC affects firm value. [15] document an indirect effect of FGC volume on shareholder value measured using abnormal returns and idiosyncratic risks. [ 8] were the first to demonstrate the direct impact of humorous FGC on firm value as measured by abnormal stock market returns. To demonstrate these impacts, they employed an event study that estimated abnormal returns and VAR modeling. However, these methods, as deployed, focus on daily activity, which can result in aggregation bias and misevaluation of FGC's impact on firm value ([64]). Moreover, the richness of such an assessment is compromised because short- and long-term impacts of FGC are not estimated ([26]; [58]). Finally, current marketing methods do not examine FGC attributes at a fine level of granularity (i.e., intraday frequencies), preventing marketing managers from moving beyond a ""throw it on the wall and see what sticks"" strategy ([37], p. 47) in the design and dissemination of intraday FGC ([36]). High-Frequency Approach to FGC AnalysisThe market microstructure approach responds to calls for more powerful methodological approaches that allow marketing researchers to harness the potential of rich data sources and develop insights capable of advancing theory and informing contemporary marketing practice (e.g., [18]; [36]; [49]; [79]). The fine-grained level of analysis available using a market microstructure approach overcomes the limitations of low-frequency methodologies, such as VAR and daily event studies, to study FGC and its impact on firm value (i.e., it estimates the variance in firm stock price following FGC dissemination). Utilizing high-frequency intraday data, it adds richness to the assessment of FGC's financial impacts by distinguishing between permanent and temporary price impacts. Temporary and permanent price impactsTemporary price impacts are short-term impacts that result in momentary changes in the price of a stock before it returns to its pre-event (e.g., pre-FGC) value, and they are often the result of uninformed trader activity (see Web Appendix D). Uninformed trader activity could be driven by several factors; for example, it could be linked to investor uncertainty about the relevance of information ([34]; [38]) or trading friction due to liquidity constraints ([ 2]; [16]). Ignoring temporary price impacts can lead to misunderstanding the total impact of FGC, with prior research suggesting that temporary price impacts result in larger transaction costs ([14]) and firm cost of capital ([17]). In contrast, an event (e.g., FGC) can generate a permanent price impact and result in the price attaining an enduring new value after the event. This occurs when the event provides information that updates informed investor/trader expectations related to a firm's long-term performance ([52]). Importantly, the microstructure approach also supports intraday actionability by assessing how the attributes of information signaled by these events influence temporary and permanent price impacts. A state-space decomposition of firm stock priceConsistent with the market microstructure literature, this study estimates the permanent and temporary price impact of FGC by first conducting a state-space decomposition of firm stock price into its efficient (permanent) and inefficient/noise (temporary) components and then linking the changes in these components to individual pieces of FGC. State-space modeling is a tool for modeling an observed variable as the sum of unobserved variables ([35]), and it is commonly used for the decomposition of price ([11]; [35]; [57]; [65]). Due to its efficiency when applied to ultra-high-frequency data like stock price movements, the state-space modeling approach for decomposing price has significant economic and methodological advantages over other commonly used methods ([31]) such as VAR models.An assumption underlying a standard VAR model is that data are sampled at regular frequencies, as variables at time t are regressed on variables dated at t − 1, t − 2, and so on. However, FGC and intraday trading data are often sampled at unequal time intervals, which suggests there would be many instances of missing variables in a model calibrated on regular time intervals ([63]). The modeling of such data using VAR requires the alignment of variables misaligned in time either downward, by aggregating the data to a lower frequency, or upward, by interpolating the high-frequency data with heuristic rules such as polynomial fillings. Downward alignment eliminates potentially valuable information in the high-frequency data. Data aggregation is problematic ([68]), as it can alter the lag order of autoregressive moving average models ([ 1]), reduce the efficiency of the parameter estimation and forecast ([74]), affect Granger causality and cointegration among component variables ([54]), and induce spurious instantaneous causality ([10]). Upward alignment has also been deemed inefficient and dubious ([61]) because a VAR approach assumes that the model specifies the high-frequency data-generating process. However, interpolation is not based on the multivariate model that generates the data but instead on heuristic rules, which, at a minimum, inevitably incorporate noise into the data and distort it.State-space modeling offers a solution to the irregular frequency challenge inherent in intraday transaction data ([62]). Specifically, the use of state-space modeling with a Kalman filter in maximum likelihood estimation of parameter estimates ensures maximum efficiency in dealing with unequal time intervals or irregular frequency in data. The use of a Kalman filter accounts for changes across periods of analysis with missing observations. This is a critical consideration in the use of state-space modeling for decomposing high-frequency time series because standard approaches do not deal with the ""missing observations"" caused by unequal data intervals. For example, estimating a standard autoregressive framework implies truncation of the lag structure and could potentially discount valuable information in high-frequency data. Using the Kalman filter facilitates the decomposition of any realized change in the time series (e.g., variance in the stock prices), such that the permanent or temporary component at any interval is estimated using all past, present, and future observations in the series. Thus, the purpose of filtering is to ensure that estimates are updated with the introduction of every additional observation ([19]). Heterogeneously informed traders and FGC attributesWith the estimation of FGC's temporary and permanent price impacts, marketing researchers can explore how FGC attributes influence the occurrence of these two types of price impact, which are driven by the existence of heterogeneously informed trading agents in financial markets ([29]; [60]). Thus, how information events, such as FGC, are observed and deciphered vary significantly between the two main groups of agents in financial markets: the informed and uninformed traders/investors (for a discussion on how the activities of informed and uninformed traders drive the asymmetric effects of information events in financial markets, see Web Appendix D). The valence and subject matter of FGC are attributes that should provide information signals to (informed) investors and thus generate a permanent price impact. This is because FGC subject matter (consumer and competitor orientation) relate to a firm's competitive advantage ([46]; [48]), which is not often public and can be difficult to observe because it is embedded in a firm's culture ([23]). The role of valence has also been documented in the literature ([71]; [76]), with the impact of negative valence appearing to be stronger than that of positive valence and thus more commonly associated with permanent price impacts ([75]). There is also reason to expect that FGC valence may interact with FGC subject matter and induce a permanent price impact. The basis for this expectation comes from a branch of signaling theory recognizing that signal recipients combine information signals to make more informed decisions ([ 6]; [73]).Although ample evidence suggests that FGC valence and subject matter may generate a permanent price impact, note that the price impact of FGC cannot occur without trading in financial markets. Trading activity incorporates the information and/or noise content of an event (e.g., FGC) into price. Therefore, because trading agents in financial markets are heterogeneously informed due to how they observe and decipher the information content of events, their trading activities also generate varied price impacts. Specifically, a permanent price impact will arise as a result of trading activity by traders/investors who have been able to correctly decipher the information content of FGC—these are the informed traders. Conversely, the trading activity of those unable to decipher the information content of FGC (i.e., uninformed traders) will only induce temporary price impacts ([24]) because their trading activity is uncorrelated with firm value ([ 4]; [29]). Accordingly, FGC that incorporates valence and subject matter can be associated with both permanent and temporary price impacts simply because of heterogeneously informed trading agents. The trading activity of informed traders thus contributes to the efficient component of price (i.e., permanent price impact), which is driven by information, whereas the activity of uninformed traders incorporates noise (i.e., temporary price impact), which is uncorrelated with firm-relevant information. S&P 500 IT Firms' Use of TwitterWe examine the instantaneous stock market impact of FGC by studying S&P 500 IT firms' activity on Twitter. Twitter is a social media communication channel characterized by ""fast-paced and short-lived information flows"" ([50], p. 177), from which deep insights can be derived if appropriate methods are developed ([79]). In addition, with 92% of firms tweeting multiple times a day ([10]), Twitter FGC is an example of high-frequency intraday marketing data. Finally, the Securities and Exchange Commission's Regulation Fair Disclosure recognizes Twitter FGC as potentially carrying ""useful"" information for investors. For these reasons, Twitter provides a suitable context to study. We study the IT sector because IT firms are often considered early adopters of trends ([ 7]). The IT sector provides a comprehensive sample of firms disseminating multiple pieces of FGC throughout the day (see Web Appendix F). It is a major driver of economic activity, with the leading five IT firms in the United States accounting for more than 22% of the S&P 500 ([30]). Globally, the IT sector is valued at $11.5 trillion, representing over 15.5% of the global gross domestic product ([14]). Finally, the diverse consumer base of IT firms is useful for characterizing the relevance of FGC subject matter (consumer and competitor orientation) and its interaction effects with valence. A review of 10-K filings for each firm shows that 7% of the sample consists of firms marketing solely in B2C markets, 72% solely in B2B markets, and 21% selling in both B2C and B2B markets. Data Set ConstructionWe obtained a sample of tweets using an application programming interface (API) to access Twitter data. In line with previous research ([50]; [77]) and following [16], we employed the API to access tweets from corporate accounts for S&P 500 IT firms. In total, we obtained 153,041 firm-generated tweets from 64 firms, which we then used in our analysis. On average, this is 2,391.2 tweets per firm over our sample period spanning January 2013 to August 2018. It should be noted that these are tweets that fall within the limits of Twitter's API in terms of the maximum number of tweets that can be accessed over a given time period. Among the IT firms we initially selected, we eliminated seven because either they did not have established corporate Twitter accounts or Twitter's API limited access to their data. Firms included in the sample engaged in high-frequency intraday marketing activity. On average, they generated a minimum of 1.07 to a maximum of 37.03 tweets a day, with the total average equaling 4.42 tweets per firm per day (see Web Appendix F), which confirms the appropriateness of the selected sample. Table 1 shows the sample of ten S&P 500 IT firms generating the highest number of tweets per day.GraphTable 1. Twitter Data Sample. S&P 500 IT FirmNumber of TweetsNumber of Tweet DaysMinimum Number of Tweets per DayMaximum Number of Tweets per DayAverage Number of Tweets per DaySingle Tweet Days (%)Number of Tweets ExcludedaNumber of Days ExcludedaRed Hat3,102204215515.1302025DXC Technology2,23915533014.35058CA3,05223216913.09.26027Cognizant3,09429413210.48.653436Oracle3,18733111059.59.852024F5 Networks3,0413552328.5401530Gartner3,2293931858.19.771624FLIR Systems3,2284131307.791.332927ANSYS3,1414251427.371.972220PAYCHEX3,0724281687.161.461030 2 a Excluded due to excessive return volatility.3 Notes: Table 1 reports the frequency statistics for a sample of tweets generated between January 8, 2013, and August 17, 2018, for S&P 500 IT firms with stocks included in the S&P 500 Index. The table shows the ten firms with the highest average number of tweets per day. Web Appendix F provides the full sample of 64 S&P 500 IT firms.We recorded each tweet with a timestamp to the nearest millisecond.[ 6] We then used these timestamps to obtain corresponding ultra-high-frequency stock trading activity data from the Thomson Reuters Tick History v2 database in Datascope for each tweet in the sample (see Table 2). This stock trading data supplemented the Twitter data set. Our data set included data for the trading days between January 2013 and August 2018. After performing data cleaning using the criteria consistent with that of [19] and [32], the stock trading data included 8,177,183,865 instances of trading activity or messages (i.e., quotes, cancellations, and transactions), which includes 520,356,393 transactions and 7,656,827,472 orders.[ 7]GraphTable 2. Trading Data Descriptive Statistics MessagesTransactionsOrdersBefore Cleaning8,182,063,205522,403,1787,659,660,027After Cleaning8,177,183,865520,356,3937,656,827,472Percentage of Data Removed After Data Cleaning.06%.39%.04% 1 Notes: Table 2 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Data cleaning is completed by following Chordia, Roll and Subrahmanyam (2001) and Ibikunle (2015), and each individual message is measured U.S. currency.After excluding days (and tweets generated on these days) with comparatively high levels of price volatility, the descriptive statistics show that the average time between trades is 7.159 seconds and the mean number of tweets per firm during the sample period is 2,377.22. The mean number of tweets per day is 54.03, and the mean number of tweets per day per firm is.844 (see Table 3 for details).GraphTable 3. Descriptive Statistics of Tweet Activity. Mean Number of TweetsMinimum Number of TweetsMaximum Number of TweetsTweets per Day per Firm.844.006.79Tweets per Firm2,377.2230.003,040Tweets per Day54.030.00353.00 4 Notes: Table 3 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Each individual message is measured U.S. currency. Investigating the Permanent and Temporary Price Impact of TweetsTo investigate tweets' permanent and temporary price impacts respectively, we first used state-space modeling to estimate the permanent and temporary components of price at a given time interval using trading observations within that time interval.[ 8] The primary interval of interest was one second; however, we estimated for one minute as well for robustness. Next, we linked these estimates to firms' tweet activity using tweets' timestamps, which were labeled to the second. Thereafter, we estimated the impact of each tweet on the temporary and permanent components of price by estimating the corresponding absolute change in the components following each tweet as the respective temporary and permanent price impacts. The methodological steps are outlined next. Step 1 (model characterization)The first step involved modeling price as the sum of a nonstationary permanent (information-driven) component and a stationary temporary (noise) component.[ 9] In this step, the only relevant observations were the prices of the 520,356,393 transactions obtained from the Thomson Reuters Tick History v2 database. These prices were defined as the prices of stocks at intraday periods and intervals. In its simplest form, the structure of the state-space model for price, a multiple of S stock prices, T intraday periods, and N intervals, is expressed as follows: vs,t,τ=ms,t,τ+is,t,τ, Graph( 1)and ms,t,τ=ms,t,τ−1+us,t,τ, Graph( 2)where vs,t,τ=ln(ps,t,τ), Graph( 3)for s = 1,...,S,  τ   = 1,...,T, and t = 1,...,N;  τ  and t index event and clock times, respectively ([56]); and an event occurs when a transaction is recorded. Thus, T = 520,356,393 and N equals the number of one-second or one-minute intervals during a stock trading day.  ps,t,τ  is the price of stock s at interval t and period  τ  ,  ms,t,τ  is a nonstationary permanent component of the price of stock s at interval t and period  τ  ,  is,t,τ  is a stationary transitory component of the price of stock s at interval t and period  τ  , and  us,t,τ  is an idiosyncratic disturbance error in the permanent price component of stock s at interval t and period  τ  .  is,t,τ  and  us,t,τ  are assumed to be mutually uncorrelated and normally distributed.[10]The model captured in Equations 1–3 is a special case of the general state-space representation. The standard state-space model is formulated for a vector of time series  vt  with a frequency/time interval  t  , and this is given by (for simplicity, we temporarily ignore the stock notation  s  and period  τ  ): vt=Wtδ+Ztmt+it,mt+1=Dtmt+Rtut,t=1,...,N, Graph( 4)where disturbances  it∼N(0,It)  and  ut∼N(0,Ut)  are mutually and serially uncorrelated. The initial state vector  m1∼N(a,P)  is also uncorrelated with the disturbances. The mean vector  a  and variance matrix  P  are usually implied by the dynamic process for  mt  in Equation 4 ([57]). The remaining terms,  Wt,Zt,Dt,Rt,It  and  Ut  , are system matrices and are generally assumed to be fixed for  t=1,...,N  . The elements of these system matrices are usually known; however, some elements that are functions of the fixed parameter vector need to be estimated. Equations 1 and 2 can be represented as the state-space Equation 4 by choosing  vt  as a single time series (this is the stock price series in this study), where  Wt=0  ,  Zt=Dt=Rt=1  ,  It=σ2i  , and  Ut=σ2u  . We note that  σ2i  and  σ2u  vary for each frequency  t  for  t=1,..,N  . Unlike standard variable decomposition approaches, this model naturally deals with irregular frequency/missing observation issues because the Kalman filter is used for its estimation, which is critical in a high-frequency analysis.[11] Step 2 (model outputs)The structure of the model shows that only changes in  us,t,τ  (now reinstating the stock notation  s  and period  τ  ) affect price permanently, and  is,t,τ  is temporary because its effects are transient and hold no significance for long-term firm performance. This is because this model decomposes price into two parts. The first,  us,t,τ  , captures smoothed (constant) changes in price, which are driven by informed trading activity, while the second captures irregular changes in price, which deviate from the smoothed evolution and are therefore driven by uninformed trading activity (noise or friction in the pricing process). By using maximum likelihood (constructed using the Kalman filter), we estimated  σs,t2u  (i.e., permanent component) and  σs,t2i  (i.e., temporary component), where t is equal to either one second or one minute. Specifically, we first partitioned our sample into one-second and one-minute (clock) intervals, and then estimated  σs,t2u  and  σs,t2i  for these intervals by using the prices at different event periods (  τ  ) during the intervals. This suggests that, as in [57], our permanent and temporary components (  σs,t2u  and  σs,t2i  ), as estimated using the state-space model, were time variant (see Table 4 in Menkveld, Koopman, and Lucas [2007, p. 220]). We imposed the time-variant structure to be consistent with the time intervals studied in subsequent multivariate regressions, which is in line with [11], who also compute time-variant permanent and transitory components of price.GraphTable 4. Permanent and Temporary Components of Price: Descriptive Statistics. Price ComponentMeanMedianStandard DeviationMinimumMaximumTemporary price component (σs,t2i).011.008.009.000.297Permanent price component (σs,t2u).055.010.048.000.644 5 Notes: Table 4 reports descriptive statistics for the permanent and temporary impact of tweeting.  σs,t2i  and  σs,t2u  are the respective estimates of the temporary and permanent components of the price for firm stock s at interval t, estimated by maximum likelihood (constructed using the Kalman filter). Step 3 (estimation with the Kalman filter)We used the Kalman filter to evaluate the conditional mean and variances of the state vector  mt  (ignoring the stock notation  s  and period  τ  ) given past observations  Vt−1={v1,...,vt-1}  :  at|t−1=E(mt|Vt−1)  ,  Pt|t−1=var(mt|Vt−1)  ,  t=1,...,N.  To initialize the Kalman filter, we also had  a1|0=a  and  P1|0=P  , where  m1∼N(a,P)  . This initialization only works if  mt  is a stationary process. However, as in our case,  mt  is often not a stationary process because it is obtained from stock price series, which are inherently nonstationary given the rational expectation of economic growth over time. Therefore, ""diffuse initialization"" (i.e., infinite variance distribution; see [44]) is used and estimated by numerically maximizing the log-likelihood. This is evaluated with the Kalman filter due to prediction error decomposition. According to the structure of the state-space model, our estimated outputs,  σs,t2u  and  σs,t2i  , were modeled as variances of permanent and temporary components of price, respectively.  σs,t2u  is a proxy for information reflected in the price (i.e., the permanent component of price), and  σs,t2i  is a proxy for noise reflected in the price (i.e., the temporary component of price). Stock prices should only experience permanent movements because of the arrival of new information; thus, we would expect  σs,t2u  to be higher than  σs,t2i  . The two estimated coefficients are variances; therefore, the coefficient encapsulating information  (σs,t2u)  , which is the primary driver of price from an efficient market perspective, should be larger.  σs,t2i  captures frictions/noise and should therefore have a lower value.[12] Step 4 (linking σ s , t 2 u and σ s , t 2 i to tweets)Our empirical framework required linking an individual intraday tweet to a corresponding trade/transaction with price pt in our sample. We call each tweet-linked trade a ""tweet-trade,"" and t, in this case, corresponds to both trade and time. Accordingly, we designated a trade in the stock of a firm as a ""tweet-trade"" if it was the first trade to occur immediately after a tweet in our sample and if it occurred within 60 seconds of the tweet. For robustness, we varied this threshold but find our inferences to be consistent if the threshold is reduced to 30 and 45 seconds, suggesting that the link between tweets and trading of firms' stock is not merely coincidental. The tweet-trade's time of occurrence allowed us to link a tweet to a corresponding pair of  σs,t2u  and  σs,t2i  , which we estimated for the one-second and one-minute intervals covered by our sample period, including those with no tweet-trades. Thus, each second and minute in our sample has a corresponding set of  σs,t2u  and  σs,t2i  . We could therefore determine the information reflected in the price (i.e., price efficiency) and noise contained in the price at every second or minute. This information allowed us to estimate whether the change in both components was occasioned by the posting of a tweet. Table 4 presents the descriptive statistics for the one-minute intervals, including tweet-trades.  σs,t2u  is higher than  σs,t2i  , which is consistent with our expectation that most of the observed tweets at time t reflect fundamental information rather than frictions or transitory components of price. This is also in line with microstructure literature ([11]; [35]; [57]; [65]). Step 5 (estimating changes in σ s , t 2 u and σ s , t 2 i following a tweet)The next step in our analysis was determining how a tweet/tweet-trade changes the composition of price with regard to  σs,t2u  and  σs,t2i  , which is required in further analysis when examining the impact of tweet valence and subject matter (i.e., consumer and competitor orientation). We linked each tweet-trade to a pair of  σs,t2u  and  σs,t2i  using the tweet-trade timestamps at the second level and then computing 30-second percentage absolute changes for both  σs,t2u  and  σs,t2i  . The changes in  σs,t2u  and  σs,t2i  following a tweet are designated as  Δσs,t2u  (permanent price impact) and  Δσs,t2i  (temporary price impact), respectively (45- and 60-second percentage changes are also computed for robustness)[13]: Δσs,t2u=|σs,t+30s2u−σs,t−12uσs,t−12u|, Graph( 5) Δσs,t2i=|σs,t+30s2i−σs,t−12iσs,t−12i|. Graph( 6)Thereafter, we also constructed  Δσs,t2u  and  Δσs,t2i  for each non-tweet-trade in our sample. Using these measures, we constructed daily ratios of the impact of a non-tweet-trade relative to that of an average tweet-trade in stock s during day d. We then tested the null that the mean daily ratios in stock s equal 1 on average across our sample period by using their standard errors for statistical inference. We expected to reject the null if the tweet-trades, on average, generated a larger or lower price impact than all the trades on an average day.[14] We present the results of the hypothesis testing in Table 5. The ratios employed in the analysis were winsorized at.5 and 99.5 percentiles within each stock. This statistical approach was consistent with prior marketing research ([ 9]), and it allowed us to eliminate outliers or extreme values and improve the chance of obtaining statistically significant estimates. Winsorization was also necessary due to the inherent noisiness of high-frequency trading data used in estimating  Δσs,t2u  and  Δσs,t2i  .GraphTable 5. Ratios of the Price Impact of Tweet-Trades to the Price Impact of Other Trades. Price Impactt60-Second Threshold45-Second Threshold30-Second ThresholdTemporary price impact (Δσs,t2i)279.67***(7.51)230.12***(9.58)222.55***(10.23)Permanent price impact (Δσs,t2u)146.83***(4.33)178.87***(3.21)189.04***(5.17) 6 ***Statistical significance at the.01 level.7 Notes: The t-statistics testing the null that the ratios equal 1 are presented in parentheses.  Δσs,t2i  and  Δσs,t2u  , which correspond to the temporary price impact and permanent price impact for stock s at time/interval t, are obtained from  σs,t2i  and  σs,t2u  (defined in Table 4). The ratios of  Δσs,t2u  and  Δσs,t2i  for each tweet-trade to the  Δσs,t2u  and  Δσs,t2i  for the other trades during an average trading day are then computed. The mean ratios are presented in Table 5 and their corresponding t-statistics are reported in parentheses.The estimates in Table 5 show that, on average, tweet-trades generate larger permanent and temporary intraday price impacts than non-tweet-trades. All the estimates are statistically significant at the.01 level, thus refuting the null hypothesis that there is no difference between the impact generated by tweet-trades and other trades. The price impact of a tweet-trade is 150–300 times larger than that of the average trade. Using the 60-second threshold,  Δσs,t2i  , which corresponds to the temporary price impact generated by the average tweet-trade, we find that the average tweet-trade has 279.67 times the impact of the average trade, suggesting that tweets result in large but momentary movements of price. This finding suggests that FGC generates temporary effects that can induce increases in the cost of trading a firm's stock and the firm's cost of capital ([14]; [17]).  Δσs,t2u  , the permanent impact of the average tweet-trade, is about 146.83 times larger than the average trade's permanent impact, suggesting that FGC can cause investors to update their expectations about a firm's future performance, which in turn leads to price movement. Overall, the analysis indicates that, on average, tweet-trades occurring in the wake of a potentially information-laden tweet substantially impact stock prices both permanently and temporarily relative to non-tweet trading activity.Estimating the effects of tweet-trades within subminute to minute windows addresses methodological issues associated with the occurrence of confounding events. Therefore, to a very high level of accuracy, we can attribute estimated temporary and permanent price impacts to the observed FGC. Given the fine-grained level of analysis, it is highly unlikely that any other relevant event could be driving the effects we capture. The sampling at high-frequency intervals also raises the question of whether investors and other trading agents could digest and act on the contents of tweets within the price impact windows we examine. Addressing this question requires an understanding of the nature of trading in financial markets today, especially in the case of highly traded stocks such as the S&P 500 stocks in our sample. Today's markets are dominated by algorithmic traders (commonly known as ""algos"") capable of digesting and acting on information in FGC (e.g., tweets) within the windows we examine in our analysis. The effects of this speed of activity are evidenced by the findings of [65], who, using S&P 500 stock data, show that information arriving in U.S. markets is exploited within seconds and that this activity is driven by algorithmic trading.Although all the ratios are statistically significant and suggest that FGC influences the permanent and temporary components of price, the obvious question is how economically meaningful tweet-trades are compared with other events that impact stock price. To answer this question, we conducted further analysis to examine the corresponding ratios of other large-impact non-tweet-trades in the same period by computing ratios similar to the ones presented in Table 5. This involved substituting a permanent price impact measure for each tweet-trade with that of other trades generating price impacts corresponding to one standard deviation or more above the daily mean in each stock. The obtained average ratios for the three thresholds are 7.9, 5.2, and 1.3 for the 30-, 45-, and 60-second windows, respectively. The inference drawn from this analysis is that the information content of tweet-trades is several times higher than that of the average non-tweet, high-impact trade. In comparing the temporary price impacts associated with the same trades with those of the tweet-trades, we find that tweet-trade ratios are again several times higher. This suggests that tweet-trades tend to be noisier than other trades associated with a more permanent price impact, and this provides a basis for demonstrating to marketers the significance of the relatively high levels of both permanent and temporary price impacts that can be generated in financial markets with the use of tweets. A robustness comparative analysis based on [22] is consistent with the presented findings (see Web Appendix G). Investigating the Temporary and Permanent Price Impacts of Tweet Valence and Subject MatterTo add intraday actionability, we used  Δσs,t2u  and  Δσs,t2i  , which encapsulate the permanent and temporary impacts of intraday tweets on firm value, as dependent variables to determine how tweet valence and subject matter (consumer and competitor orientation) influence the impact of tweets on stock price. To investigate whether tweet valence and subject matter drive the price impact of tweet-trades, we estimate Equation 7: PriceImpacts,t=αs+βt+γ1consumers,t+γ2competitors,t+γ3consumer*−ves,t+γ4competitor*−ves,t+γ5consumer*+ves,t+γ6competitor*+ves,t+γ7−ves,t+γ8+ves,t+∑k=17φkCk,s,t+ϵs,t, Graph( 7)where  PriceImpacts,t  corresponds to  Δσs,t2u  or  Δσs,t2i  , respectively, for a tweet-trade t in stock s.  αs  and  βt  are stock and time fixed effects. We used the VADER rule-based algorithm ([40]) to determine the valence of the tweets. VADER outperforms other commonly used benchmark methodologies such as Linguistic Inquiry and Word Count, Affective Norms for English Words, and the machine learning algorithm support vector machine in the literature as well as in our robustness tests. We also utilized [66] library and followed their method for measuring the competitor (  competitors,t  ) and consumer  (consumers,t)  orientation for each tweet, which is in line with [ 3] and [78]. Consumer and competitor subject matter are dummy variables that equal 1 when a tweet-trade's content is about consumer and/or competitors. We also studied the interaction effects of these attributes.  competitor*+ves,t  and  competitor*−ves,t  refer to positive-valence tweets related to competitors and negative-valence tweets related to competitors, respectively, for a tweet-trade t in stock s, and  consumer*+ves,t  and  consumer*−ves,t  refer to positive-valence tweets related to consumers and negative-valence tweets related to consumers, respectively, for a tweet-trade t in stock s.To avoid omitted variable bias and to ensure completeness, the model also includes  Ck,s,t  , which reflects a vector of known determinants of price impact based on past research in the market microstructure literature, as well as the natural logarithm of the number of an account's followers at the time of a tweet-trade t's tweet (  #followerss,t  ).  Ck,s,t  includes the natural logarithm of trading volume (  lnvolumes,t  ), the natural logarithm of average trade size (  lntradesizes,t  ), volatility (  volatilitys,t  ), effective spread (  Effectivespreads,t  ), the natural logarithm of a high-frequency trading proxy (  HFTs,t  ), and order imbalance (  OIBs,t  ). We measured trading volume as the dollar volume of transactions executed in stock s prior to a corresponding tweet-trade t. We computed average trade size as the trading volume prior to tweet-trade t divided by the number of transactions just prior to a corresponding tweet-trade t in stock s.  volatilitys,t  is the standard deviation of midpoint dollar price returns from the start of the trading day up to the trade just before the corresponding tweet-trade t in stock s.  Effectivespreads,t  (in basis points) was computed as twice the absolute value of the last trade price less the prevailing price midpoint prior to the corresponding tweet-trade t in stock s divided by the prevailing price midpoint. Price midpoint is the average of the prevailing best bid and ask prices.  HFTs,t  is the ratio of the number of messages (quotes, cancellations, and transactions) to actual transactions from the start of the trading day until prior to a corresponding tweet-trade t in stock s. Finally,  OIBs,t  is the ratio of the difference between the number of sell and buy orders and the average of both from the start of the trading day until prior to a corresponding tweet-trade t in stock s. To eliminate outliers in the data caused by the characteristic noisiness of high-frequency trading data, all variables are winsorized at.5 and 99.5 percentiles within each stock.We estimated Equation 7 using both panel least squares and 2SLS instrumental variable (IV) estimation approaches. Panel-corrected standard errors were computed to obtain heteroskedasticity and autocorrelation robust standard errors. We performed the IV estimation to account for the likelihood of endogeneity due to selection bias caused by a firm's decision regarding whether to use Twitter ([25]). The IV approach we employed was based on approaches adopted by an increasing number of studies in the marketing literature ([80]). For a given firm in our sample of S&P 500 IT firms, our approach involved first identifying the firms in the same two-digit Standard Industrial Classification that had sent a corresponding tweet on the previous or same day as the firm and then estimating the mean value of the potentially endogenous variables (consumer and competitor orientation) for these firms. The mean estimates were employed as an instrument for the firm in question. This variable met the requirements for an instrument because price impacts observed in the other firms' stocks were unlikely to be driven by the focal firm's tweets and, at the same time, tweeting activity has been shown to be correlated for firms in similar industries. In each of the first-stage regressions, we regressed each of the consumer and competitor variables separately on the corresponding IVs and the control variables defined previously for each firm/stock and obtained the F-statistics as tests of the null of weak instruments. The fitted values for each of the measures from the first-stage regressions were then employed as the variables in place of the consumer and competitor orientation variables in the second-stage regressions.The first-stage F-statistics, testing the null of weak instruments, show that our IV model does not suffer weak instrument issues. The test statistic is higher than the threshold of 10 needed for 2SLS inferences to be reliable when instrumenting for endogenous variables ([70]). We also conducted further tests to examine the instruments' relevance and the validity of the overidentifying restrictions in the IV regressions. The Cragg–Donald and Kleibergen–Paap Lagrange multiplier statistics we obtained reject the nulls of weak instruments and underidentification according to the [33] critical values, respectively. Essentially, these test the null hypothesis that the instruments we used have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameters. All the p-values we obtained in the Sargan χ2 test also indicate that we cannot reject the null that the overidentifying restrictions are valid. All the 2SLS estimates for Equation 7 are presented in Table 6, and the results of the panel least squares estimations are presented in Web Appendix H.GraphTable 6. The Relationship Between Permanent and Temporary Price Impact and Tweet Valence and Orientation. VariablesPermanent Price Impact(Δσs,t2u)Temporary Price Impact(Δσs,t2i)Key Findingsconsumers,t−.007(1.06).009**(2.39)Consumer-related tweets are, on average, associated with a larger temporary price impact relative to other tweets.competitors,t−.034**(2.03).026**(2.46)Competitor-related tweets are, on average, associated with a larger temporary price impact and lower permanent price impact relative to other tweets.−ves,t−.063**(2.37).032**(2.46)Tweets with only negative or only positive valence are associated with increasing temporary price impact and decreasing permanent price impact.+ves,t−.108***(3.11).033**(2.20)consumers,t*−ves,t.047**(1.97).072**(2.50)Tweets reflecting both valence and subject matter are associated with increases in both permanent and temporary price impact. The increase in permanent price impact contrasts the decrease in permanent price impact that tweets with only valence or only subject matter are associated with.Except for tweets reflecting negative valence and consumer orientation (consumers,t*−ves,t), permanent price impact is more pronounced than temporary price impact.competitors,t*−ves,t.606***(3.69).011**(2.09)consumers,t*+ves,t.088**(2.10).019**(2.21)competitors,t*+ves,t.220***(4.88).077**(2.43)lnvolumes,t−.039***(-4.51)−.034***(-6.68)Increases in firm stock trading activity are linked with reductions in both permanent and temporary price impacts.lntradesizes,t.089***(6.64).061***(7.25)Larger firm stock trade sizes induce larger permanent and temporary price impacts.volatilitys,t−.123***(−3.62).015**(2.13)Firm stock volatility is linked with increases in temporary price impact and decreases in permanent price impact.Effectivespreads,t.241**(2.66).014**(2.06)Deterioration in firm stock liquidity is associated with increases in permanent and temporary price impacts.lnHFTs,t−.000(−.26)−.021***(−3.83)Algorithmic and high-frequency trading is linked with decreases in temporary price impact. Its effect on permanent price impact is benign.OIBs,t−.371***(−6.89).046**(2.39)Order imbalance is linked with reductions in permanent price impact and increases in temporary price impact.ln#followerss,t−.082**(−2.43).037**(2.43)The number of followers of a firm's twitter account amplifies the propensity for tweets to generate larger temporary price impact and reduce permanent price impact.R2¯.35.49Observations139,997139,997Kleibergen–Paap LM31.32***110.24***Tests the null hypothesis that the employed instruments have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameter.Cragg–Donald79.08***88.66***Tests the same null hypothesis as the Kleibergen–Paap LM test.Sargan's χ2p-value.37.46Tests the null hypothesis that the overidentifying restrictions are valid. 8 **p <.05.9 ***p <.01.10 Notes: Table 6 reports 2SLS estimated coefficients for Equation 7. Standard errors are robust to heteroscedasticity and autocorrelation, coefficients are multiplied by 107, and t-statistics are reported in parentheses. LM = Lagrange multiplier.The results presented in Table 6 show the importance of tweet valence and subject matter in determining the permanent and temporary impacts of tweets on firm value. The existence of permanent and temporary price impacts associated with tweet attributes supports the signal theory perspective ([43]) and shows that investors pay attention to the tweet attributes of valence and subject matter. The estimates of permanent price impact for  γ7  and  γ8  are negative and statistically significant (−.063, p < .05, and −.108, p < .01, respectively). This suggests that tweets displaying only positive or only negative valence are linked to less permanent impacts in stock price. The positive and statistically significant  γ7  and  γ8  estimates of the temporary price impact estimation also indicate that such tweets are linked to increasing temporary price impact (.032, p < .05, and <.033, p < .05, respectively), and they suggest that tweet valence generally contributes more noise to stock price than stock-relevant information. The findings reinforce the role of positive and negative valence FGCs and their impact on firm value ([75]; [76]).With respect to tweet subject matter, only tweets conveying information about competitors generate statistically significant permanent price impacts (−.034, p < .05). Therefore, on average, tweets about a firm's competitors generate lower permanent price impact relative to other tweets. Conversely, the positive and statistically significant estimates for  γ2  for temporary price impact (.026, p < .05) show that these types of tweets are more likely to contribute to the noise component of price; in other words, they generate a larger temporary price impact than other tweets, on average. Thus, tweets conveying competitor orientation appear to result in a lower permanent price impact, suggesting that this form of subject matter is comparatively less impactful and relevant to investors' expectations about a firm's future performance ([48]). Notably, tweets about consumers do not yield any permanent price impact that is statistically different from that of other tweets and, thus, by themselves do not appear to offer a signal capable of causing investors to permanently update their firm performance expectations. Consumer-related tweets, similar to those about competitors, also generate more temporary price impact than other tweets on average, which suggests that their potential for inducing noise in stock price is higher than that of the average tweet in our sample. The  γ1  and  γ2  estimates of the temporary price impact are positive and statistically significant (.009, p < .05, and.026, p < .05, respectively). This finding implies that, as is the case with valence, tweets reflecting only competitor or only consumer orientation generate noise in the price discovery or trading processes and lower permanent price impact.Inferring from information-based market microstructure models ([24]; [47]), the more information about a firm investors observe, the more they become informed about the valuation of the firm. In line with this expectation, the interaction variables we include in Equation 7 should yield positive estimates for the  Δσs,t2u  estimations. As we expected, all the  γ3  ,  γ4  ,  γ5  , and  γ6  estimates of permanent price impact are positive and statistically significant (respectively,.047, p < .05;.606, p < .01;.088, p < .05; and.220, p < .01), even though, as already stated,  γ1  ,  γ2  ,  γ7  , and  γ8  are negative and statistically significant (except for  γ1  ). Thus, increases in both negative and positive valence, when viewed through the lens of subject matter, are linked with increased permanent price impact. These estimates show that tweet valence, when contextualized by subject matter or vice versa, is seen by investors/traders as firm-relevant information. In the context of these findings, the incorporation of valence and subject matter into FGC can yield increases in permanent price impact.Furthermore, the findings suggest that tweets about competitors with a negative valence are likely to have the highest permanent price impacts (.606, p < .001). This finding is crucial from the perspective of marketing practice and intraday social media marketing strategy design because valence and competitor subject matter as singular attributes of FGC are independently associated with decreasing permanent price impact. The findings underscore the view that investors seek additional information (i.e., information beyond what they already have) when making trading decisions ([ 6]; [73]) and that they operate according to classical market microstructure models. For example, [47] and [24] emphasize the crucial importance of information for price discovery in financial markets. This also confirms [51] findings that information from microblogging platforms, such as Twitter, impact investors' decisions.To illustrate the relevance of these findings, Figure 3 presents tweets A and B as examples of FGC characterized by negative and positive valence, respectively, but not containing any subject matter related to competitor or consumer orientation. Consistent with our findings, the permanent price impact estimates for the tweet trades corresponding to both tweets are more than three standard deviations lower than the average permanent price impact estimate and are thus below the tenth percentile of the estimates. The estimates for the negative and positive tweets' tweet-trades are.0017% and.0035%, respectively. In contrast with A and B, tweets C, D, and E reflect varying combinations of both valence and subject matter. Our findings suggest that these tweets should generate significant permanent price impact, and indeed, the permanent price impact estimates for the tweet-trades corresponding to tweets C, D, and E are above the 90th percentile in our sample of tweet-trades' permanent price impact estimates. The estimates are 3.74%, 2.84%, and 1.32% for tweets C, D, and E, respectively.Graph: Figure 3. Examples of tweets generating temporary and permanent price impacts.The effects of the tweet attributes we studied on temporary price impact,  Δσs,t2i  , also deserve attention. The results suggest that the relationship between valence and temporary price impact is generally magnified when combined with subject matter. For example, the  γ7  and  γ8  estimates, which capture the relationship between  Δσs,t2i  on the one hand and  −ves,t  and  +ves,t  on the other, are positive and statistically significant (.032, p < .05, and <.033, p < .05, respectively), while the estimates for  γ3  and  γ6  , which capture the relationship between  Δσs,t2i  on the one hand and  consumers,t*−ves,t  and  competitors,t*+ves,t  on the other, are also positive and statistically significant (.072, p < .05, and <.077, p < .05, respectively). The latter set of estimates is at least two times larger than the former. The overall implication of these positive and statistically significant coefficient estimates related to temporary price impacts is that, although tweets reflecting both valence and subject matter are likely to generate permanent price impact, these attributes may also be associated with increased temporary price impact. Thus, on average, tweets inject noise (uncertainty) into the prices of stocks traded in financial markets.In conclusion, the estimates presented in Table 6 highlight the relevance of tweet attributes for the price discovery process in financial markets and reinforce the importance of studying the multifaceted nature of FGC ([45]). We find that tweets, as with many events observed in relation to trading in financial markets, generate both permanent and temporary price impacts. However, whereas tweets containing singular attributes—either positive or negative valence or either consumer or competitor orientation—readily inject noise into the price discovery process and thus generate temporary price impact, those that include more than one attribute generate permanent price impact and thus generally enhance the efficiency of the price discovery process. DiscussionIn this research, we examine the real-time impact of FGC on the variance of firms' stock price. In the current fast-paced online communication landscape, marketers must understand the financial impact of firms' ""always on"" marketing ([64]). The assessment of FGC's financial impacts, however, is in an early stage ([ 8]; [15]; [64]). This research contributes to this emerging stream of marketing research and addresses multiple calls for new methods that are able to develop real-time insights from online data ([ 5]; [49]; [59]; [79]). By employing the market microstructure approach to study S&P 500 IT firms' Twitter activity, this study contributes to marketing literature and practice. Research ContributionsThis study offers several implications for marketing research. First, aligning with the work by [15], [ 8], and [64], it advances understanding of FGC's financial impact by providing an assessment of FGC's impact on the variance of stock price in real time (i.e., seconds). By employing a market microstructure approach, we show how to algorithmically link individual pieces of FGC to time-specific trading activity at a fine-grained level of analysis. In the process, we demonstrate the limitations of low-frequency methodologies such as daily event studies, which are subject to aggregation bias and may yield biased estimates of the impact of FGC on firms' financial outcomes, while offering an alternative and more robust method of analysis for studying intraday marketing activity. In our examination of the impact of FGC on variance, we fully utilized high-frequency transaction data characterized by unequal time intervals and demonstrate how to retain data that otherwise would have been eliminated in studies that use end-of-day stock price. By doing so, we provide marketing researchers with a new approach that allows them to harness the potential of online data.Second, we distinguish between FGC's temporary and permanent price impacts. Specifically, we show that FGC impacts investor expectations related to a firm's future performance, thus generating permanent price impact, and it also injects uncertainty about a firm's value into the firm's stock price, thus inducing temporary price impact. Our research, therefore, adds a new perspective to the marketing literature stream on the financial impact of FGC. This assessment of FGC's temporary and permanent price impacts adds richness to the examination of marketing's financial impact and enables the quantifying of long- and short-term financial impacts of marketing activity.Finally, this research has implications for the design of intraday marketing strategies. By examining FGC valence and subject matter (consumer and competitor orientation), we advance a growing body of research documenting the complex nature through which marketing signals impact financial markets and firm financial outcomes. We show that, by themselves, FGC valence and subject matter are more prone to injecting uncertainty about a firm's stock price into the market, and thus, they generate temporary price impacts rather than permanently changing investors' and traders' beliefs about firm value. Used together, FGC valence and subject matter both hold statistically significant and economically meaningful relevance for price discovery in financial markets. In other words, they can influence investors' expectations related to firms' future performance and thus result in permanent price impacts. Recent research by [ 6] provides evidence of interactions between marketing signals, and our research shows that the interaction between FGC valence and subject matter can also impact firm stock price. Managerial ImplicationsThus far, firms have struggled to demonstrate financial accountability regarding FGC's impact on firm value ([15]; [45]) or provide evidence of its immediate contribution to their financial outcomes ([53]; [58]). We provide marketing managers with evidence of FGC's impact on variance in firms' stock price. Specifically, we show that tweets can generate both permanent and temporary price impacts. By manipulating tweet attributes, such as valence and subject matter, marketing managers can design Twitter content to generate varying degrees of permanent or temporary impact. From a market quality perspective, firm managers should prefer tweets that generate a permanent price impact, and our research provides some useful indications about how to achieve this outcome. We show that tweets expressing degrees of positive or negative valence regarding either consumers or competitors generate a permanent price impact. We therefore encourage marketing managers to design information-rich tweets that both ( 1) focus on consumers or competitors and ( 2) communicate valence. Our results suggest that firms should utilize valence and subject matter in their tweets if they would like their stock to be more informative with respect to their value. Our analysis suggests that tweets about competitors with a negative valence are likely to have the highest permanent price impacts. Thus, by using permanent price impact as a metric to evaluate the longer-term impact of tweets, social media managers can design campaigns that have a sustainable impact on firm financial outcomes. The design recommendations from this study complement [41] work on social media content scheduling, as well as [64], which addresses real-time social media marketing and provides firms with information regarding which tweets to disseminate during the day for long-term effectiveness. We recognize that not all intraday tweets will, nor should they, have permanent impacts on firms' stock price. Some tweets are aimed at the creation of social media buzz, which is related to the temporary price impacts we examined in this study. Firms can indeed achieve social media buzz by tweeting, as our findings reveal that tweets, in aggregate, mostly generate temporary price impacts. We urge caution, however, because temporary price impacts are linked with larger transaction costs ([14]) and increases in firm cost of capital ([17]). This suggests that the benefits of designing tweets to generate buzz and incorporate information into stock price must be carefully managed. To support marketing managers in their intraday social media strategy design, Table 7 is designed as a set of insights based on our findings.GraphTable 7. Suggested Insights for Marketing Managers. Permanent Price ImpactTemporary Price ImpactResearch FindingsRecommendationExpected OutcomeInteraction EffectsPermanent Price ImpactTemporary Price ImpactResearch FindingsValencePositive valence Positive and negative valence-only FGC contributes to more noise in a firm's stock price.Add subject matter (e.g., competitor orientation such as ""competition,"" ""peer"")Increased permanent price impactPositive valence and competitor orientation Interaction of valence and subject matter increases/generates permanent price impact and amplifies temporary price impact. Permanent price impact is more pronounced than temporary price impact. The financial implication of these outcomes is a reduction in transaction and firm capital costs.Negative valenceAdd subject matter (e.g., consumer orientation such as ""customer,"" ""consumer,"" ""buyer"")Increased permanent price impactNegative valence and consumer orientationSubject MatterConsumer orientationSubject matter-only FGC contributes to the noise component in a firm's stock price.Add valence (e.g., positive valence such as ""help,"" ""solution,"" ""best"")Increased permanent price impactPositive valence and consumer orientationCompetitor orientationAdd valence (e.g., negative valence such as ""attack,"" ""stop,"" ""threat"")Increased permanent price impactNegative valence and competitor orientation 11 = no price impact; = negative impact on stock price component; = positive impact on stock price component; = increased positive impact on stock price component. LimitationsWe conclude by encouraging future research to address the limitations of our empirical study. One potential limitation of our analysis is its focus on firms in the IT sector. We recognize that these findings may not apply to other sectors. Future research could extend our analysis to other sectors to confirm whether similar price impacts hold. Second, the impact of tweets could depend on whether Twitter was the first source through which a firm released an important piece of news. For example, tweets could have been published in response to a competitor's tweet. In some cases, a firm's tweet could lead to a number of successive tweets, in which case the subsequent tweets might not be as impactful as the first. We do not discount the possibility that there could be some carryover or dampening effect in such situations. We note that, if this is the case, it would be highly unlikely for the magnitude of the effects we observe to occur, especially given the granular level of analysis that our market microstructure approach entails. Third, future work could explore high-frequency data generated by firms' use of FGC other than tweets, such as Facebook posts, where it has been reported firms post up to 80 times a day (Hutchinson 2018). It would be interesting to see whether the effect of FGC across social media platforms is consistent or if it varies. In addition to social media, it would be useful to examine firms' use of other online communication tools, such as webpages and blogging platforms. Researchers could also explore various types of FGC, including video content, as well as its characteristics, including emotions ([72]). As [36] show, there is an array of online marketing communication practices, and future research could therefore study the ""echoverse"" at a fine-grained level of analysis. Finally, we note that researchers can apply market microstructure to study user-generated content (UGC) in future research. We welcome future research that addresses the following questions: What is the real-time impact of UGC on firm value? What are the UGC attributes capable of generating permanent and temporary price impacts? Are these attributes the same as for FGC, or do they differ? Our research highlights the importance of interaction effects when examining the impact of FGC attributes on firm value; therefore, investigating the optimal mix of UGC attributes capable of generating temporary and permanent price impacts should be an interesting endeavor. "
31,"Minimum Payments Alter Debt Repayment Strategies Across Multiple Cards U.S. households currently hold $770 billion in credit card debt, often managing repayments across multiple accounts. The authors investigate how minimum payment requirements (i.e., the requirement to allocate at least some money to each account with a balance) alter consumers' allocation strategies across multiple accounts. Across four experiments, they find that minimum payment requirements cause consumers to increase dispersion (i.e., spread their repayments more evenly) across accounts. The authors term this change in strategy ""the dispersion effect of minimum payments"" and provide evidence that it can be costly for consumers. They find that the effect is partially driven by the tendency for consumers to interpret minimum payment requirements as recommendations to pay more than the minimum amount. While the presence of the minimum payment requirement is unlikely to change, the authors propose that marketers and policy makers can influence the effects of minimum payments on dispersion by altering the way that information is displayed to consumers. Specifically, they investigate five distinct information displays and find that choice of display can either exaggerate or minimize dispersion and corresponding costs. They discuss implications for consumers, policy makers, and firms, with a particular focus on ways to improve consumer financial well-being.Keywords: credit cards; choice architecture; debt‌; financial decision making; financial well-beingImagine that you have accumulated debt across several different credit cards and are now allocating your monthly budget toward repaying this debt. As you look at your bills, each debt has a different total balance, interest rate, and minimum payment. How do you decide how much to allocate to each one? You might focus on balance amounts, for example, paying the smallest balance first to feel like you are making progress. Alternatively, you might focus on interest rates, paying the highest-interest card first to minimize total interest paid. Regardless of how you choose to prioritize your debts, you will likely consider the minimum payment amounts across all cards. You might plan to pay at least the minimum to avoid fees, interest rate increases, and credit score implications associated with failing to make the minimum payments. You might even think the presence of the minimum payments suggests you should pay more than the minimum.U.S. households currently hold $770 billion in credit card debt ([13]). Rather than being consolidated into a single monthly payment on one card, the average American household with at least one credit card must manage decisions across an average of four accounts ([ 8]), and 91.7% of revolving balances are held by people with two or more cards ([16]). Relying solely on interest rates and minimum payment requirements can lead consumers to a strategy that would minimize total interest costs. However, it is not clear that consumers use the cost-minimizing strategy and may instead use other strategies such as heuristics based on balance amounts (e.g., [ 3]; [16]).Minimum payment requirements are a central element of the credit card statement and, more broadly, of the financial system surrounding debt repayment. Specifically, credit card companies require borrowers to make a minimum payment each month. Missing these payments is used to classify accounts as in default and leads to late fees and credit score penalties (see [21]). Beyond avoiding penalties, there may be benefits to consumers from paying some amount every month, though paying exactly the minimum on a given credit card tends to reduce that credit card's debt very slowly ([32]).We propose that minimum payments fundamentally alter allocation strategies across several credit card debt accounts. In field data, consumers' repayments are more dispersed (i.e., spread more evenly across accounts) than a strategy that minimizes interest costs or other known heuristic strategies would suggest ([16]; [23]; see also Web Appendix A, Study F). In the current research, we use controlled experiments to isolate the effect of minimum payments on dispersion. We find that the presence of minimum payments on all accounts increases dispersion relative to a control with no minimum present, which we term ""the dispersion effect of minimum payments."" This strategy change tends to redirect payments from the highest-interest cards to lower-interest cards, thus increasing overall interest costs. The dispersion effect occurs in addition to anchoring effects previously associated with minimum payments on single cards (e.g., [35]). We provide evidence that a perceived recommendation to pay more than the minimum drives the effect. Finally, we examine the role that alternative choice architectures (e.g., [33]) can play in altering dispersion and corresponding interest costs in the presence of minimum payments. Impacts of Credit Card Statements on Debt Repayment for a Single CardPolicy makers have used a variety of behavioral science tools to influence consumers' repayments, particularly through regulations affecting the credit card statement. For example, the CARD Act of 2009 required credit card companies to provide additional information to consumers including the total interest costs associated with paying only the minimum payment, the time required to pay off the full credit card balance if paying only the minimum, and the repayment amount required to pay off the full credit card balance in three years. One goal of the CARD Act disclosures was to nudge consumers to make larger repayments, and these small changes to the information environment did affect repayment decisions, though to a somewhat limited extent ([ 1]; [ 2]).The way credit card bills display information can impact consumers' understanding and repayments. [34] find that consumers have inaccurate perceptions of the growth of debts over time due to a misunderstanding of compound interest. The additional information mandated by the CARD Act corrects some but not all of consumers' misunderstanding of the time it takes to repay debt.Focusing on the provision of the repayment amount required to pay off the full credit card balance in three years, [32] finds that the provision of that three-year cost information has the desired effect of increasing repayments above the minimum. However, the information about time required to pay off the full credit card balance if paying only the minimum has an unintended consequence of moving repayments below the three-year repayment amount.The prevalence of online repayments has diminished the likelihood that consumers even see CARD Act disclosures ([ 8]). This context motivated [33] to investigate the way online payment modules with default options are constructed and the consequences of these displays for repayment decisions. They find that active-choice displays, which consist of distinct salient options to pay the minimum and the total debt amount, increase the likelihood that consumers will repay a single card in full.Recent research has also examined how the presence of minimum payments on credit card statements influences the amount allocated toward debt repayment. To understand this question, researchers typically randomly assign participants to a single debt that either has a minimum payment or does not, and they ask participants how much they would allocate toward debt (e.g., [35]). Consumers faced with minimums in this context tend to repay less than those without them (e.g., [20]; [30]; [35]). Subsequent work has shown similar effects in the field, with consumers paying at or just above the minimum when the minimum required payments change (e.g., [24]).One explanation for these results is that consumers anchor on the minimums and, as a result, pay just above the minimum ([30]; [35]). Probing this anchoring effect further, [20] propose that the salient numbers on credit card statements operate as a recommendation or appropriate payment amount, akin to a default amount serving as a recommendation in other contexts (e.g., [25]). In other words, consumers may take the required minimum values as the amount they should pay and adjust their repayments from that implied recommendation.Each of these demonstrations examining the effect of credit card statements on payment decisions investigates the decision of how much money to repay toward a single card (e.g., [20]; [24]; [30]; [35]). If a person only has one outstanding debt account, then making lower payments toward this account will increase overall interest costs. However, when determining the most efficient allocations across multiple accounts, the strategy for minimizing interest costs changes. While it is possible that other fees can drive costs higher in specific situations, we focus on interest as a main driver of costs associated with credit card debt. Assuming a given amount of money is available to repay debt across cards, the strategy that leads to the lowest amount of accrued interest over time is to pay the minimum required amount on each card first and then to use the discretionary funds (i.e., repayments in excess of the minimum payments on all cards) to allocate any remaining money to the debt associated with the highest-interest-rate card. If, after doing so, funds remain, the consumer would repay the debt associated with the second-highest-interest-rate card, and so on. As a result, paying only the minimum on lower-interest-rate cards can be consistent with the cost-minimizing strategy. Studies using only one card cannot determine whether consumers are responding to minimum payments efficiently when considering repayment strategies across a consumers' portfolio of cards. A separate line of research examines the question of whether consumers use the optimal strategy across their portfolio of cards and, more generally, aims to better understand what strategy consumers do use. These articles tend not to examine the relevance of the minimum payment to repayment strategy decisions. Debt Repayment Across Multiple AccountsA variety of studies both in the lab (e.g., [ 3]; [ 5]; [ 6]) and in the field ([16]) have shown that consumers do not use the cost-minimizing strategy to make debt repayment decisions. Using field data from the United Kingdom, [16] argue that consumers are balance matching—that is, they behave as if they are paying in proportion to their account balances.[ 5] For example, a consumer who holds a $1,000 balance on one card and a $500 balance on another would allocate $200 to the first debt and $100 to the second if they had $300 available to repay. The authors argue that this heuristic is consistent with other matching heuristics shown in humans and animals (e.g., probability matching; [18]; [19]). Notably, this heuristic predicts that people should make their largest repayment to the debt with the largest balance.In contrast, both academic researchers and financial advice gurus such as Dave Ramsey have documented positive motivational effects of paying off credit card accounts in full, typically by paying off the debt with the smallest balance. Under this strategy, described as ""debt account aversion,"" consumers repay more money toward their debts with the smallest balances so they can close accounts and feel a sense of progress ([ 3]; [ 5]; [ 6]). For example, sorting multiple tasks from smallest to largest can reduce the time it takes to complete the tasks ([ 7]). Providing further evidence, consumers in a debt repayment program were more likely to remain in the program when they had a debt account closed by being fully repaid, even though the debt repayments were decided by the program ([15]). The motivational benefits of focusing on smaller debt balances can manifest, even in cases where consumers cannot pay debts off in full, as long as consumers make a repayment toward the card with the lowest balance ([23]) or complete a subgoal by repaying a purchase in full ([11]). As a result, there may be ways for consumers to improve their outcomes by deviating from the interest-cost-minimizing strategy.Finally, drawing on literature from investment savings, consumers may rely on the 1/N heuristic as a way to simplify their decision-making process ([ 4]). Under this strategy, consumers would divide their budgets evenly across debt accounts. More recent work shows that consumers may allocate evenly only across considered options ([27]). Field evidence suggests that consumers may also use the 1/N heuristic when choosing how to allocate payments across credit cards ([16]). Current ResearchSubstantial attention has been devoted to documenting how minimum payments affect debt repayment decisions in single-card settings and, separately, to debt repayment decisions across multiple cards. However, an important gap remains in understanding whether and how minimum payments alter debt repayment strategies across multiple cards. In multicard settings, anchoring on the minimums could help consumers if it leads them to reduce their repayments to lower-interest debts while maintaining or increasing payments to higher-interest debts. Alternatively, it could hurt consumers if it leads them to reduce payments to higher-interest-rate cards. This could occur either by reducing overall allocations toward debt repayment or by causing consumers to shift funds from higher- to lower-interest debts. This article examines the effect of minimum payment requirements on repayment strategies when borrowers have multiple credit card debts. Specifically, we examine the relationship between minimum payments and dispersion (i.e., spreading payments across accounts) as well as consequences for interest costs.Our focus on dispersion is motivated by patterns of repayments in field data. The cost-minimizing repayment strategy in any given payment cycle typically requires concentrating repayments on only one or a small number of consumers' highest-interest debts. By contrast, field data suggests that consumers' repayments are too dispersed relative to the cost-minimizing approach ([16]; [23]; see also Web Appendix A, Study F). Given the ubiquity of minimum payments in the context of credit card debt, each of the field investigations described previously examines cases where the minimum payment requirement is present, with no comparison to a no-minimum control. In the current research, we hypothesize that these minimum payment requirements may contribute to the observed dispersion in repayments. H1:  Consumers make payments that are more dispersed across debts when repaying credit card accounts that all have (vs. do not have) minimum payment requirements.More concentrated repayment strategies do not inherently lead consumers to pay lower interest costs, though the cost-minimizing strategy described previously is typically concentrated. Instead, terms of the specific accounts on which consumers choose to concentrate their repayments will affect the amount of interest paid. For example, if consumers' starting point was making the largest payments toward an account that happened to have the lowest interest (e.g., in the case that this was their smallest debt amount), then increasing dispersion would likely reduce interest costs overall by directing money toward higher-interest accounts. While consumers do not have a strong understanding of how interest compounds (e.g., [34]), there is evidence that consumers do attend to interest to some degree ([ 1]). This attention to interest rates suggests that consumers direct repayments toward higher-interest accounts. Some, but potentially insufficient, attention to interest rates is also consistent with results from a pilot study we conducted examining intended repayment strategies (see Web Appendix A, Study G). To the extent that consumers begin by allocating more funds toward the highest-interest-rate cards (vs. lower interest-rate cards), increasing dispersion (e.g., through the introduction of the minimum payment) should shift repayments away from the highest-interest-rate accounts toward those with lower interest rates. Consequently, we predict that minimum payments will increase consumers' total interest costs. Correspondingly, H2a:  The presence of the minimum payment on all cards decreases repayments to the highest-interest-rate debt when compared with a no-minimum control. H2b:  The presence of the minimum payment on all cards increases interest costs when compared with a no-minimum control.While our hypotheses directly address situations with and without minimum payments, having minimum payments on only some accounts may lead to similar patterns.Why might minimum payments lead to an increase in dispersion? In the absence of minimum payments, consumers would likely allocate available funds based on one of the previously mentioned heuristics. Whether focused on interest or amounts, these repayment strategies tend to be concentrated on a small number of accounts. To either minimize interest costs or maximize the motivation for paying off debt ([ 3]; [23]), it is helpful to repay certain accounts in full while repaying small or zero amounts to others. Consistent with this intuition, consumers describing their intended repayment strategies typically include plans to pay all or most of their available funds to a single debt (for a study examining lay beliefs about debt repayment strategies, see Web Appendix A, Study G).Once minimum payments are introduced, however, we propose that consumers will perceive a recommendation to repay more than the minimum toward every account. While the primary focus of the literature on minimum payments has been on anchoring (e.g., [35]), more recent work has emphasized how a variety of numbers presented to customers on credit card statements can be perceived as recommended amounts (e.g., [20]; [26]; [33]). In multiple-card settings, consumers may act in accordance with a recommendation to repay more than the minimum on each account. As a result, accounts that would not have received an allocation at all without a required minimum payment may instead receive an allocation that is even greater than the minimum. This will lead allocations in the minimum-payment condition to be more dispersed. These repayments may appear similar in nature to a 1/N repayment strategy of splitting evenly but will not necessarily be divided into equal amounts. Further, we propose that the motivation to pay something to every card stems at least in part from a perceived recommendation. More formally, H3:  A perceived recommendation to pay more than the minimum to every card mediates the relationship between the presence of the minimum requirement on all cards and payment dispersion.We investigate these hypotheses across four experiments reported in the main text. Seven additional studies reported in the Web Appendix corroborate these key findings and provide evidence of robustness to variations in the experimental design. In Experiment 1, using a nationally representative sample and an incentive-compatible design, we find that participants' allocations in the minimum-payment condition are more dispersed than in the no-minimum-payment control (H1). We also find that the presence of minimum payments leads participants to repay less to the highest-interest-rate debt (H2a) and to incur larger interest costs (H2b). In Experiment 2, we allow consumers to choose their budgets for debt repayment and replicate these findings (H1, H2a, and H2b). Further, consistent with prior literature, we find that minimum payments decrease the budget allocated to debt repayment (e.g., [35]). In Experiment 3, we provide evidence that consumers take minimum payment requirements as recommendations to pay more than that amount toward each card. Further, these beliefs mediate the relationship between the minimum-payment condition and payment dispersion (H1, H2a, H2b, and H3). Finally, in Experiment 4, we aim to identify potential policy interventions by examining how five different information presentations modeled on real-world debt repayment environments influence the dispersion effect. We replicate findings of the dispersion effect of minimum payments (H1) and associated interest costs (H2a, H2b) using the same presentation as in Experiments 1–3. In addition, we find that a presentation modeled on the paper statements currently in use leads to worse outcomes for consumers relative to all other presentations. By contrast, a presentation modeled on newer online interfaces that use active choice alters perceived recommendations and reduces dispersion (H3). The exact materials and data for all studies are available on OSF at https://osf.io/xuf59/.This article makes several contributions. First, we contribute to the literature on how consumers repay credit card debt (e.g., [39]; [ 5]; [16]) by examining the impact of minimum payments in situations where consumers have multiple credit cards. In addition, we provide a deeper understanding of the psychological mechanisms underlying heuristics previously identified using field data (e.g., [16]) by using lab experiments to examine the causal impact of minimum payments on dispersion. Second, we contribute to the literature on consumer inferences from choice architecture ([20]; [25]; [33]) by documenting dispersion and corresponding interest costs resulting from consumer inferences of recommendations from minimum payment requirements. Finally, we contribute to the literature on using choice architecture to improve consumers' financial outcomes ([28]; [38]) by identifying how different choice environments can exaggerate or minimize the excess dispersion and interest costs associated with minimum payment requirements. Consequently, our findings have practical implications for providing credit card companies or third-party financial management apps (e.g., Tally, Mint) additional strategies for helping consumers repay their debts. Experiment 1When making debt repayment decisions, most consumers are choosing how to allocate payments across multiple cards. While it is difficult to find variation in the presence or absence of minimum payments in the field because they are almost universally required, examining repayments in the lab enables us both to assess the causal impacts of minimum payments and simultaneously to gain a better understanding of the underlying psychological drivers of payment strategies. In a nationally representative population, we use a debt management game to test participants' strategic responses to minimum payments. Method ParticipantsFour hundred thirteen participants from a market research panel aggregator operated by CloudResearch completed the study. Of these, 50.7% were female with a median age of 48 years, a modal education level of a ""Some college (no degree),"" and median income of $30,000–$39,999. The sample was selected to be approximately nationally representative of U.S. adults on age, gender, and income. Seventy-nine percent reported having at least one credit card, with a median of two cards per participant. The sample was limited to U.S. participants. Design and procedureParticipants played a three-round debt management game modeled on a task from [ 3] but modified to mimic the average U.S. consumer's debt more closely. Participants were provided with information on six debt accounts including interest rates and amounts. We drew interest rates at random from a CFPB database of national credit card terms and drew debt amounts from a normal distribution designed to add up to $16,000, approximately the amount of credit card debt for the average indebted American household ([12]). In each round, participants received a budget of $3,000 dedicated to debt repayment, and they selected the amount they wanted to allocate to each debt. Their allocations in each round were forced to equal their budget (for participants' view of the task, see Figure 1).Graph: Figure 1. Example participant screen in Experiment 1.Participants were randomly assigned to either a minimum-payment condition or a no-minimum-payment control condition. All participants were instructed that their goal in the task was to reduce their debt as much as possible. After reading the instructions, all participants answered one comprehension check question about their goal in the task. In addition, participants in the minimum-payment condition answered a question about the size of the minimum payment fee. If a participant answered a comprehension check question incorrectly the first time, the question was shown again with the relevant section of the instructions.All participants saw a table indicating the balance and interest rates for each of six credit card accounts. Participants in the minimum-payment condition saw an additional column in the table with the minimum payment amounts. The minimum payments were set at 2% of the total debt amount on all accounts, consistent with the typical range of 1%–4% in the field ([24]). Participants faced a $25 fee for each minimum that they failed to pay, similar to the initial ""safe harbor"" late fee set by the [ 8]. After each round, the task was updated to reflect the decisions participants had made in the previous round. Participants were told that they would be paid a bonus that ranged from $0–$1 based on their performance in the task.[ 6] Results and DiscussionWe excluded participants who failed to answer the comprehension check questions twice (N = 13) and those who allocated more than they owed to any debt in more than one round (N = 26) because these repeated errors suggest inattention to the task. These exclusions did not differ significantly by condition (no minimum: 10% vs. minimum: 9%; z = .33, p= .741). Analyses with all participants are included in the Web Appendix, and results do not substantially change (for robustness to exclusion criteria, see Web Appendix B, Table W2).As a measure of dispersion, we build on the concentration metric described in [23], defining dispersion as one minus concentration: dispersionit=1−[∑k=1Ait(xikt−x¯it)2/(Ait−1)]x¯it×∑k=1Aitxikt, Graphwhere xikt is the allocation made by participant i to debt k in round t,  x¯it  is the mean allocation made by participant i in round t, and Ait is the number of accounts with nonzero balances for participant i in round t. Intuitively, the metric captures the ratio of the variance in repayments to the mean of the repayments, with a normalization to create a measure bounded between zero and one. This measure of dispersion has several advantages: it builds on information-theoretic concepts, is continuous over the range of zero to one, and allows for meaningful comparisons between participants with different numbers of accounts.To account for mechanical differences between conditions on this measure that may bias it in favor of our hypothesis, we adjust the dispersion metric of participants in the minimum-payment condition who use the cost-minimizing strategy to match the dispersion of cost-minimizing repayments in the control. Specifically, participants in the minimum-payment condition who repay using the cost-minimizing strategy in round one are adjusted to have the same dispersion as participants in the control who use the cost-minimizing strategy. In rounds two and three, participants in the minimum-payment condition who repay using the cost-minimizing strategy in all three rounds are adjusted to have the same dispersion as control participants who use the cost-minimizing strategy in all three rounds (for additional details on this adjustment, see Web Appendix C). Because the measure is bounded between zero and one, we analyze dispersion using a fractional regression in a panel over the three rounds, controlling for the round of the task, with heteroskedasticity-robust standard errors clustered at the subject level ([ 9]).Ideally, our measure of dispersion would produce similar results for people who try to implement similar repayment strategies, would be implemented the same way for all participants regardless of condition or strategy used, and would not require additional exclusions. However, many of these factors trade off against one another. The adjusted measure that we present in the main text has the benefit of producing similar results for similar repayment strategies across conditions. In addition, it allows us to interpret allocations of people who do not make the minimum payments, enabling us to include them in the analysis. However, it requires an ad hoc adjustment for those using the cost-minimizing strategy. As an alternative, the unadjusted dispersion measure can be applied uniformly, but it is biased in favor of H1 for some strategies, most importantly the cost-minimizing strategy. In contrast, a version of this metric that focuses on only discretionary repayments in the minimum-payment condition (i.e., by subtracting out the minimum payments in the minimum-payment condition prior to computing the dispersion metric) is substantially biased in the opposite direction for the cost-minimizing strategy as well as for any person repaying more than the minimums. In addition, when looking at only discretionary repayments, it is not clear how to use data from participants in the minimum-payment condition who fail to make the minimum payments, potentially inducing a sampling bias.We present the adjusted dispersion metric throughout, and Web Appendix B, Table W4, reports the unadjusted metric and the discretionary metric excluding people who fail to make the minimum payments. Importantly, while each of these approaches has advantages and disadvantages, they produce qualitatively similar results. This converging evidence suggests that differences in dispersion as a function of the minimum payment requirement are robust to the specific operationalization of dispersion.Connecting dispersion to interest costs, we focus on two measures. First, we examine the amount allocated to the highest-interest debt by condition using a linear regression. This analysis controls for the round of the task and uses heteroskedasticity-robust standard errors clustered at the subject level. Second, to capture the overall costs associated with the repayment decisions in each condition, we examine the natural log of interest paid per round by condition using a linear regression, controlling for the round of the task, with heteroskedasticity-robust standard errors clustered at the subject level. We use the natural log because this measure could be positively skewed depending on the strategies consumers use to repay debts, with many consumers clustered around low amounts but some paying large amounts of interest. We do not include the penalty fees in this measure as there are no penalty fees in the control condition. However, participants who fail to make minimum payments may face larger debts in subsequent rounds as a result of the fees.[ 7] In the following results, we report both the average amount in natural logs, which is the measure for our statistical comparison, as well as the dollar levels, in brackets, for interpretability.Participants in the minimum-payment condition allocated their repayments in a more dispersed way than those in the control condition (Mcontrol = .73 vs. Mmin = .79; B = .31, 95% confidence interval [CI] = [.038,.57], t(373) = 2.24, p = .025), consistent with the hypothesized dispersion effect (see Figure 2). Turning to interest, we find that participants in the minimum-payment condition paid significantly less to the highest-interest-rate debt (Mcontrol = $1,085, Mmin = $777; B = −307.75, 95% CI = [−409.80, −205.70], t(373) = 5.91, p < .001). These repayment patterns translated to significantly higher interest paid per round in the minimum-payment condition (Mmin = 7.00 [$1,128]) than in the control condition (Mcontrol = 6.96 [$1,084]; comparison of natural log amounts: B = .044, 95% CI = [.024,.064], t(373) = 4.36, p < .001).Graph: Figure 2. Main results from Experiment 1.Experiment 1 demonstrates the effect of minimum payments on dispersion and interest across multiple cards. Minimum payments shifted participants' debt repayment strategies when allocating a fixed sum across multiple accounts, leading to higher dispersion. In addition, minimum payments decreased the money allocated to the highest-interest-rate debt and increased interest costs. These results are robust to accounting for financial literacy, number of credit cards, and other demographic controls (see Web Appendix B, Table W3). Experiment 2In Experiment 1, participants were given a fixed budget to allocate across six accounts and were required to allocate their full budget to debt repayment in each round. Consequently, participants could not pay exactly the minimum requirements for all debts or otherwise vary the total amount allocated to debt repayment. Prior literature on debt repayment has identified consumers' tendency to pay at or just above the minimum payment in a single-card setting (e.g., [35]). Further, it has examined effects of paying off accounts in full on motivation to repay, altering decisions of how much money to allocate to debt repayment ([ 3]; [ 5]; [ 7]; [23]). The design of the current study allows us to build on this prior work by giving participants the opportunity to save some of their budget. This design has the additional benefit that it is more consistent with the decision people face in the real world, which requires determining both how much money to allocate to debt repayments and how to distribute this money across debt accounts. Method ParticipantsFour hundred two participants completed the study on Amazon's Mechanical Turk (MTurk). Fifty-four percent of our participants were female, with a median age of 33 years, median income in the range of $50,000–$59,000, and modal education of a bachelor's degree. Eighty-four percent of our participants report having at least one credit card, with a median of two cards per participant. Design and procedureParticipants played a debt game similar to that from Experiment 1. However, in this version of the debt game, participants had the option to allocate money to a savings account if they desired. They were not given information about interest earned in this savings account. We instructed participants to allocate funds as they would in their everyday life. This instruction was included to allow us to gain a realistic picture of consumer behavior and avoid having participants treat this as an optimization problem, which could bias them against using the savings account. Participants were randomly assigned either to a minimum-payment condition or a no-minimum-payment control condition as in Experiment 1. In addition to the measures from the prior experiment, we also examined the amount and likelihood of savings across conditions, as well as how dispersion in prior rounds affected subsequent savings decisions. This analysis allowed for a better understanding of how the dispersion effect relates to literature on concentration and motivation ([23]). Results and DiscussionPrior to examining the data, we excluded participants (N = 9) who allocated more than they owed to any debt in more than one round.[ 8] Consistent with results from Experiment 1, participants in the minimum-payment condition (Mmin = .72) allocated their chosen budgets to debt repayment in a significantly more dispersed fashion than the no-minimum-payment control (Mcontrol = .65; B = .33, 95% CI = [.094,.56], t(391) = 2.74, p = .006). The amount allocated toward the highest-interest debt was significantly lower in the minimum-payment condition than in the control condition (Mcontrol = $1,179, Mmin = $888; B = −291.20, 95%CI = [−403.20, −179.20], t(392) = 5.10, p < .001). These repayment patterns translated to significantly higher interest paid per round in the minimum-payment condition than in the control (Mcontrol = 6.99 [$1,119], Mmin = 7.06 [$1,206]; B = .076, 95% CI = [.042,.111], t(392) = 4.37, p < .001).Prior work (e.g., [35]) suggests that people will allocate less money toward debt repayment when minimum payments are present. We replicate this finding in a multiple-card setting by examining the log of the amount allocated to debt in each round (i.e., the budget of $3,000 less any amount saved). We impute $0 allocated to debt to $1 because the log(0) is undefined, but this occurs in only three observations. Participants in the minimum-payment condition (Mmin = 7.79) allocated significantly less to debt than those in the no-minimum-payment control (Mcontrol = 7.91; B = −.12, 95% CI = [−.215, −.025], t(392) = 2.48, p = .014). Because their overall budget was fixed, this implies participants in the minimum-payment condition saved more. Participants in the minimum-payment condition were also significantly more likely to allocate any money to the savings account (Mcontrol = 47%, Mmin = 62%; B = .61, 95% CI = [.25,.97], t(392) = 3.31, p = .001). Correspondingly, we see increased dispersion in the minimum-payment condition when including the savings account in our dispersion metric (Mcontrol = .66, Mmin = .71; B = .23, 95% CI = [.02,.45], t(392) = 2.10, p = .036).To better understand the relationship between amount allocated to debt repayment and dispersion, we regressed the log of the amount allocated to debt in round t on condition, dispersion in round t − 1, and log of the amount allocated to debt in round t − 1 with a round control. Unsurprisingly, prior-round allocation to debt predicted current-round allocation to debt (B = .751, 95% CI = [.543,.959], t(391) = 7.07, p < .001). In addition, prior-round dispersion was negatively correlated with future allocation to debt (B = −.07, 95% CI = [−.126, −.015], t(391) = 2.51, p = .013), suggesting that participants with less dispersed strategies were more focused on repaying debt. Controlling for these two effects, there was no effect of condition on allocation to debt (B = −.007, 95% CI = [−.026,.013], t(391) = .68, p = .495). This suggests that the differences in dispersion may contribute to the increased use of the savings option and the reduction in money allocated to debt in the minimum-payment condition. The positive relationship between dispersion and savings is consistent with the negative motivational effects of dispersed debt repayments. In other words, more dispersed prior-round repayments were associated with less focus on debt repayment in the subsequent round ([23]).Prior literature on minimum payments in single-card settings finds that the presence of minimum payments reduces the amount repaid toward debt. We replicate that finding and also show that minimum payments lead consumers to allocate their remaining budget in a more dispersed way. Thus, minimum payments induce two separate costs across multiple accounts. First, they decrease the amount of money allocated to debt repayment. This pattern is consistent with an increase in ""co-holding"" in which consumers simultaneously hold money in low-interest bearing savings accounts and high-interest debt accounts ([17]; [36]). Second, they lead borrowers to spread their repayments more evenly across accounts, consistent with the proposed dispersion effect. Our findings further suggest that the more dispersed repayments may themselves be associated with reduced money allocated to debt repayment to the extent that they reduce motivation to repay debt. Experiment 3In the current experiment, we aim to develop a better understanding of why the dispersion effect occurs. Specifically, we examine the inferences consumers draw from the minimum payments. Prior work suggests that minimum payments and other numbers included on debt statements can be perceived as a recommendation for how much to pay ([20]; [33]). Consequently, we test whether consumers in the minimum-payment condition perceive a recommendation to pay more than the minimum amount. Perceiving a recommendation to pay more than the minimum amount on every card could lead consumers to increase allocations from the minimum, corresponding to increased dispersion.In addition, the current experiment includes two new elements to further explore the source and the robustness of the dispersion effect. To examine how much of the effect is driven by the mandatory nature of the required minimum payment, we add a suggested-minimum-payment condition. In this condition, participants are told a suggested amount for each card, which is the same 2% amount as the required minimum payment, but there is no penalty for failing to pay it. We hypothesize that this condition will be perceived more as a recommendation to pay exactly the suggested amount than the required-minimum condition.In addition, we test the robustness of the dispersion effect to the distribution of debt amounts. Because debt amounts tend to be skewed ([23]), we want to examine how a more skewed distribution of debt amounts affects dispersion. We do this by drawing an additional set of debt amounts from a log-normal distribution with similar total debt to the prior studies, leading to a more skewed distribution of debt balances (see Figure 3). The study was preregistered at aspredicted.org.[ 9]Graph: Figure 3. Debt values displayed in the high-skew condition. Method ParticipantsTwelve hundred seven participants completed the study. Forty-six percent were female, with a median age of 37 years, a median income of $50,00–$59,999, and a modal education level of a bachelor's degree. Eighty-six percent of participants reported having at least one credit card, with a median of two credit cards per participant. Design and procedureParticipants completed a debt management game similar to that in Experiment 1. They were randomly assigned to a no-minimum control, a required-minimum-payment, or a suggested-minimum-payment condition. The no-minimum control and required-minimum-payment conditions were the same as in Experiment 1. Participants in the suggested-payment condition saw the same 2% minimum payment as those in the required-minimum condition, but these payments were not required. The instructions read, ""On each page you will see a suggested minimum payment amount. There is no consequence for not making these payments, but they are suggested."" In addition, participants were randomly assigned to see debt amounts drawn from a normal distribution (normal skew) as in previous studies or a log-normal distribution (high skew). Thus, the experiment had a 3 (minimum: control, required, suggested) × 2 (skew: normal vs. high) design.After completing the debt management task, participants responded to questions about recommendations they perceived from the task. The main mediation question was ""The instructions provided a recommendation to pay MORE THAN [X] to every card.""[10] Participants responded on a seven-point scale from ""strongly disagree"" ( 1) to ""strongly agree"" ( 7). In the control condition, X was replaced with ""$0""; in the suggested minimum condition, it was replaced with ""THE SUGGESTED MINIMUM,"" and in the required minimum condition, it was replaced with ""THE REQUIRED MINIMUM."" We maintained capitalization in the survey to emphasize differences across questions. The experiment included two ancillary questions which are reported and discussed in the Web Appendix.Note that our questions focused on recommendations inferred from the instructions, which contrasts somewhat from prior work on defaults as recommendations asking about inferences from policy makers' beliefs (e.g., [25]). Given widely known policy requiring minimum payments on credit cards in the United States, we directed the current question toward the debt game's instructions so that it would be easily interpretable in all conditions, even those (i.e., suggested minimum and control conditions) that deviate from known policy. Results and DiscussionWe excluded participants (N = 5) who failed to answer the comprehension check questions twice. We also excluded participants who allocated more than they owed to any debt in more than one round (N = 49).[11]For our main analyses, in addition to the dummy variables for each round we included in prior studies, we also included a dummy variable for whether the debt amounts were drawn from a high-skew distribution. Examining the two different distributions of debt amounts with an interaction shows qualitatively similar results, with a somewhat larger dispersion effect of minimum payments in the high-skew condition. The Web Appendix contains additional analyses detailing the robustness of our results to different specifications.Payments in the required-minimum condition were significantly more dispersed than those in the no-minimum control (Mcontrol = .58, Mmin = .66; B = .37, 95% CI = [.22,.51], t( 1,152) = 4.88, p < .001). The suggested minimum (Msuggested = .63) fell between the required-minimum and the control, with payments significantly more dispersed than in the control condition (B = .20, 95% CI = [.052,.34], t( 1,152) = −2.66, p = .008) and significantly less dispersed than in the required-minimum condition (B = −.17, 95% CI = [−.32, −.015], t( 1,152) = 2.16, p = .031).Participants allocated less to the highest-interest-rate debt in both the required-minimum condition (Mcontrol = $1,239, Mmin = $1,029; B = −211.81, 95%CI = [−276.71, −146.92], t(1145) = 6.40, p < .001) and the suggested-minimum condition (Msuggested = $1,144; B = −96.33, 95% CI = [−170.11, −22.55], t( 1,152) = 2.56, p = .01) when compared with the control. Those in the required-minimum condition allocated significantly less to the highest-interest-rate debt than those in the suggested-minimum condition did (B = 115.48, 95% CI = [50.13, 180.83], t( 1,152) = 3.46, p < .001).These allocations led participants in the required-minimum and suggested-minimum conditions to pay more interest than those in the control (Mcontrol = 7.20 [$1,424], Mmin = 7.22[$1,439], Msuggested = 7.22 [$1,449]; Bcontrol vs. min = .023, 95% CI = [.012,.033], t( 1,152) = 4.37, p < .001; Bcontrol vs. suggested = .024, 95% CI = [.014,.035], t( 1,152) = 4.60, p < .001). There was no significant difference between the minimum-payment and suggested-minimum conditions (B = .002, 95% CI = [−.009,.012], t( 1,152) = .355, p = .722), see Figure 4. Typically, lower dispersion and more focus on the highest-interest-rate debt correspond to lower interest paid. However, because our measure of interest is a composite of repayments on all accounts, this pattern may be driven by participants in the suggested-minimum condition making less efficient allocations to the non-highest-interest accounts.Graph: Figure 4. Main results from Experiment 3.To better understand the underlying process, we next examine the perceived recommendation to pay more than the minimum as a potential mediator of the differences in dispersion across conditions. As we expected, participants in the required-minimum condition report significantly higher agreement with the statement that there was a recommendation to pay more than the minimum when compared to the control participants (Mmin = 2.80, Mcontrol = 2.08; B = .72, 95% CI = [.47,.97], t( 1,152) = 5.68, p < .001), as do those in the suggested-minimum condition (Msuggested = 2.64; B = .56, 95% CI = [.32,.80], t( 1,152) = 4.56, p < .001).[12] However, there was no significant difference between the required and suggested minimums on this measure (B = -.16, 95% CI = [-.43,.10], t( 1,152) = 1.22, p = .221). Notably, average responses in each condition are below the scale midpoint, suggesting overall levels of disagreement. However, there is substantially more agreement, defined by responses at or above the scale midpoint, in the required-minimum condition (Mmin = 35% vs. Mcontrol = 22%; B = .64, 95% CI = [.32,.96], t( 1,152) = 3.96, p < .001) and the suggested-minimum condition (Msuggested = 32%; B = .50, 95% CI = [.18,.83], t( 1,152) = 3.07, p = .002) than in the control condition.We use the Mediation package in R to conduct mediation analyses that separately compares each of the minimum-payment conditions with the no-minimum-payment control. The package allows us to cluster standard errors at the subject level using a quasi-Bayesian Monte Carlo simulation to estimate the variance ([39]).[13] We use 5,000 samples. Comparing the required-minimum and control conditions, the perceived recommendation to pay more than the minimum partially mediates the difference in dispersion (indirect effect = .0211, 95% CI = [.012,.032], p < .001; direct effect = .064, 95% CI = [.029,.097], p < .001). In a separate model, the perceived recommendation to pay more than the minimum fully mediates the difference between the control and suggested-minimum conditions (indirect effect = .0215, 95% CI = [.012,.033], p < .001; direct effect = .025, 95% CI = [−.008,.059], p = .135). These findings, illustrated in Figure 5, demonstrate that participants in the minimum-payment conditions were more likely to perceive a recommendation in the instructions to pay more than the minimum. Further, a substantial portion of the differences in dispersion between the no-minimum-payment control and the two minimum-payment conditions is explained by these differences in the perceived repayment recommendation.Graph: Figure 5. Condition effects in mediation analysis in Experiment 3.Endorsement of a perceived recommendation to pay more than the minimum accounts for some of the differences in dispersion we observe. Given the low average ratings on the perceived recommendation scale and the fact that the recommendation to pay more than the minimum does not significantly mediate the difference between the required and suggested minimums (indirect effect = .004, 95% CI = [−.002,.011], p = .219; direct effect = .036, 95% CI = [.003,.069], p = .036), this mechanism likely operates in concert with other factors. We=discuss potential additional factors in the ""General Discussion"" section.Results from Experiment 3 suggest an important role for policy makers in designing decision environments. Specifically, while the role of minimum payments is commonly considered to be providing a base level of repayment necessary to avoid defaulting, their presence can be interpreted by consumers as a recommendation to pay more than the minimum. We find that the level of this perceived recommendation varies as a result of the details of the choice environment, for example, carrying greater influence when the minimum payment is required. In our next experiment, we examine how choice environments can exacerbate or reduce the consequences of minimum payments for consumers. Experiment 4Experiments 1–3 documented a new cost to minimum payments—namely, that they lead participants to make more dispersed repayments and increase interest costs incurred. Experiment 3 further showed that the strength of the perceived recommendation to pay more than the minimum amount partially drives this effect. However, minimum payments serve important functions for the financial system, for example, by allowing credit card companies to classify accounts as in default and potentially reducing prices in the market overall. Consequently, in spite of the challenges that they create for consumers, it is unrealistic, from a policy perspective, to eliminate these requirements. The most efficient path for policy makers, firms, and consumer welfare advocates to improve consumer decision making or mitigate the costs of errors may not be through the elimination of the minimum payment requirement. Instead, they should reconsider the design of repayment interfaces. From an implementation perspective, third parties such as financial technology applications interested in enhancing consumer welfare may be most likely to produce a decision aid. Understanding of the psychology underlying debt repayment decisions, together with the role of information displays, can help inform these changes.In the current experiment, we return to the setting from Experiment 1 but introduce four new conditions with required minimum payments that represent possible choice architectures policy makers, marketers, or app designers could use: a high-interest-salience condition, which sorts debts by their interest rates; an active-choice condition ([33]), which provides the minimum payment and full balance as two distinct salient options; a combination of the active-choice and salient-interest conditions; and a standard-paper-statement condition, which attempts to approximate the consumer experience of searching for relevant information in credit card debt statements. Each of these conditions provides insight into the way policy makers can choose to format decision environments to help—but also potentially hinder—consumers. Method ParticipantsOne thousand seven hundred ninety-one participants completed the main study on MTurk. Fifty-one percent of our participants were female, with a median age of 37 years, median income in the range of $60,000–$69,000, and modal education of a bachelor's degree. Eighty-eight percent of our participants reported having at least one credit card, with a median of two cards per participant. Design and procedureParticipants were randomly assigned to one of six conditions. The first was the no-minimum-payment control condition. The other five conditions had minimum payments. The second condition was the same as the standard minimum-payment condition from Experiment 1. We include this condition to examine the size of the baseline dispersion effect in this setting and to have a benchmark of comparison relative to the newly introduced designs. In addition, we introduce four new conditions that attempt to capture the way that different features of choice architecture can alter the impacts of minimum payments. The interest-salience condition aimed to increase the perceived importance of interest rates by ordering the debts by interest rates with the highest-interest-rate debt on top. We expected and found in a pretest that participants in this condition would be more likely to perceive a recommendation to repay according to interest rates (see Web Appendix A, Study H). If consumers underweight the importance of interest and thus allocate less to high-interest rate debts, this condition will decrease interest costs associated with minimum payments.The active-choice condition was modeled on the current format for online repayments, which allows participants to easily select the minimum amount, select the full statement balance, or enter a different value (see Figure 6). Online repayment of credit card debts is growing, with about 10% of cardholders using online portals ([33]). However, the regulations and nudges that appear in paper statements are not visible to the consumer when making repayments in these interfaces. Prior work examining these interfaces suggests that they can highlight paying debt amounts in full. By nudging people to repay in full, we predict that consumers will focus their repayments on a smaller number of debts, reducing dispersion. To the extent that consumers focus on high-interest debts, this choice architecture may help consumers overcome the interest-cost consequences of minimum payments. We expected and found in a pretest that participants did perceive a recommendation to repay accounts in full (see Web Appendix A, Study H). Building on our findings from Experiment 3 (H2), we anticipated that shifting this perceived recommendation from a focus on the minimum payment to the full statement amount would minimize the dispersion effect. The active choice × salient interest condition combined active-choice repayment mode with the debt table sorted by interest rate.Graph: Figure 6. Participant display for the active choice condition in Experiment 4.Finally, we were interested in understanding how each of these highly designed settings that highlighted specific information to consumers compared with a standard paper statement. A key feature of the standard paper credit card statement is that it highlights debt amounts and not interest rates. While debt balances, minimum payments, and the minimum payment warning disclosure are shown on the front page of credit card statements by regulation, interest rates are frequently found on the last page of the statement. As a result, consumers need to exert effort to find the interest information. To approximate the need for information search, we created a table similar to the information table in Experiment 1 except that only amounts and minimum payments were featured on the initial screen. Participants could click a link to see additional information. The information acquired through the link included the interest rate along with the amount consumers would have to pay to pay off the debt in full in three periods (similar to the three-year number reported on credit card statements after the CARD Act of 2009), the previous balance, the last payment, and the amount of interest paid (for the participants' screens, see Figure 7). The goal of this condition was to increase the ecological validity of the task by adding an element of information search, though its impact on dispersion was not clear ex ante. We expected that by highlighting the amounts more than interest, participants would be more likely to perceive a recommendation to repay according to debt amounts and confirmed this intuition in our pretest (see Web Appendix A, Study H). This study was preregistered at aspredicted.org.[14]Graph: Figure 7. Participant display for the standard statement condition in Experiment 4. Results and DiscussionWe excluded participants who failed our attention checks twice (N = 4) and participants who allocated more than their debt amount in more than one round (N = 48).[15] The effects of minimum payments varied across information presentation conditions (see Figure 7). We first examine the effects of the paper statement condition, because this maps most closely onto the most common information presentation for consumers in the field. The paper statement condition was the most costly to consumers. Though it did not substantially affect dispersion relative to the standard-minimum-payment condition (B = .053, 95% CI = [−.14,.24], t( 1,738) = .540, p = .589), it did significantly increase dispersion relative to the no-minimum-payment condition (B = .369, 95% CI = [.186,.552], t( 1,738) = 3.95, p <.001). Further, it reduced the amount allocated to the highest-interest-rate debt (minimum: B = −157.2, 95% CI = [−225.28, −89.13], t( 1,738) = 4.53, p < .001; control: B = −490.33, 95% CI = [−577.81, −402.85], t( 1,738) = 10.99, p < .001) and significantly increased the interest paid per round (minimum: B = .047, 95% CI = [.031,.064], t( 1,738) = 5.65, p < .001; control: B = .067, 95% CI = [.051,.084], t( 1,738) = 8.04, p < .001) when compared with both the standard-minimum-payment condition and the no-minimum-payment condition. Notably, these results could be driven by participants in this condition either failing to seek out additional information or using the information less effectively than participants in the standard-minimum condition.We next turn to the condition that showed the most promise in reducing dispersion and interest costs: the active-choice condition. When compared with the standard-paper-statement condition, the active-choice condition significantly outperforms on all of our measures (dispersion: B = −.28, 95% CI = [−.47., −.10], t( 1,738) = 3.02, p = .002; highest-interest payments: B = 224.74, 95% CI = [157.16, 292.32], t( 1,738) = 6.52, p < .001; log interest paid: B = −.054, 95% CI = [−.071, −.037], t( 1,738) = 6.29, p < .001). Further, when compared with the standard-minimum condition, the active-choice condition significantly reduced dispersion (B = −.231, 95% CI = [−.41, −.055], t( 1,738) = 2.57, p = .01). It also significantly increased the amount allocated to the highest-interest-rate debt (B = 67.53, 95% CI = [3.28, 131.79], t( 1,738) = 2.06, p = .04) but only directionally reduced the amount of interest paid (B = −.007, 95% CI = [−.022,.008], t( 1,738) = .89, p = .373). By introducing an additional perceived recommendation, the recommendation to pay off debts in full, the active-choice condition partially moderated the effect of minimum payments on dispersion.[16] As Figure 8 shows, the other minimum-payment conditions fell somewhere between the active choice and the standard paper statement (for regressions, see Web Appendix B, Table W27).Graph: Figure 8. Main results from Experiment 4.The differences across conditions with minimum payment requirements suggests a role for policy makers and firms in designing consumers' choice environments and their implied recommendations. In particular, the current shrouding of interest rates in paper statements is especially costly for consumers because it leads them to repay less to high-interest debts. These results suggest that highlighting information on the core characteristics of debts, particularly interest rates, can help consumers repay in a less costly way, even in the presence of minimum payments.In addition, the active-choice condition, by presenting multiple default options, can decrease dispersion and increase repayments to the highest-interest debts. This design also outperforms the standard-paper-statement condition on all measures examined. Choice architects can take advantage of this format to help consumers increase consideration of interest rates and possibly provide motivational benefits, as documented in Experiment 2 and in prior literature ([23]). At the same time, credit card companies can profit from higher-interest payments and therefore may have an incentive to maintain current user interfaces. Consequently, making repayment environments more user friendly may require government intervention or additional third parties like financial aggregation apps (e.g., Mint, NerdWallet, Tally). Additional ExperimentsIn addition to the studies reported previously, we conducted five experiments to further explore the dispersion effect of minimum payments, using a variety of alternative designs.[17] We briefly summarize results here and provide complete details in the Web Appendix. The first additional experiment replicated our findings in an MTurk sample using a similar design to Experiment 1 (Web Appendix A, Study A). The second additional experiment aimed to enhance the ecological validity of the experimental design. Because credit card bills tend to arrive at different points throughout the month, we extend our findings to a version of our task in which participants make allocation decisions one at a time (Web Appendix A, Study B). We find consistent patterns using this design, allowing us to rule out the possibility that the effect is driven by participants responding to all debts in the same elicitation. This enables us to generalize the dispersion effect and more closely approximate credit card bill timing from the world.The third additional experiment aimed to address the possibility that a difference in the mathematical sophistication required in the minimum-payment condition versus the control condition drives the dispersion effect. We examined a scenario where all the minimum payments are round numbers ($25). We found a marginally significant increase in dispersion and a significant decrease in the amount paid to high-interest debt, though the log of interest paid per round did not reach significance (Web Appendix A, Study C). These findings suggest that even when the minimum payments are round numbers, they still increase dispersion, though the effect may be somewhat weaker.The fourth additional experiment introduced a condition where the minimum payment amount was filled in by default (Web Appendix A, Study D). Dispersion and interest costs in the default condition fell directionally between the minimum payment and control condition. Thus, the default payment interface appears to have similar outcomes for consumers as the active-choice interface. The default payment interface may operate in a similar way to the active-choice condition, shifting perceptions of recommended payment amounts. However, because participants can also ignore debts with the minimum payments already filled in, it may also reduce dispersion by reducing the number of accounts for which they have to make allocation decisions.Finally, our hypotheses specifically ask about the presence of minimum requirements on all cards. However, there could be situations in which some cards have minimum requirements but others do not. In a one-round version of our task, we found that the dispersion effect of minimum payments persisted when there were minimum payments on some, but not all, accounts (Web Appendix A, Study E). This suggests that the minimum payment requirement can spill over to debts that do not have minimum payments. General DiscussionThis article presents new insights into the influence of minimum payments on consumer debt decisions. First, we document a dispersion effect of minimum payments that leads consumers to spread their discretionary allocations across more accounts. Second, we show that the dispersion effect tends to lead to larger interest costs. Further, we provide evidence that the perceived recommendation to pay more than the minimum partially underlies this effect. Together, our experimental results suggest that minimum payment requirements may contribute to the overdispersion of repayments observed in field data (e.g., [16]).Our findings also document a novel path, beyond anchoring, through which minimum payments can harm consumers by increasing allocation dispersion. Consumers facing minimum payments fundamentally alter their strategic approach to repayments across accounts. Although minimum payments serve an important role in ensuring that debts are not neglected, they may negatively impact consumer financial well-being in part by suggesting a recommendation to pay above the minimum on each account. Importantly, we find that information and allocation environments can influence the impact of the minimum payment on dispersion and interest costs. Alternative information presentations can help consumers reduce interest costs in the presence of minimum payments but can also exacerbate the interest cost consequences for consumers. In fact, consumers in the condition modeled on current paper statements performed the worst, suggesting there are substantial opportunities to improve consumer outcomes through choice architecture.We contribute to the literature on heuristics in debt repayment by studying the role of minimum payments in changing allocations across multiple cards. Previous research suggests that consumers allocate money to the accounts with the lowest amount of debt first ([ 3]; [ 5]; [ 6]). While some participants in our data show a strong focus on the accounts with the lowest amount of debt, most focus on their highest-interest-rate debts but allocate less to those debts in the minimum-payment condition.Recent work has also provided evidence for a balance-matching heuristic, which implies that consumers will make their largest allocation to the account with the largest debt ([16]). The dispersion effect we document is closest in nature to that heuristic. However, because consumers are mostly focused on their highest-interest-rate debts, our results suggest that balance matching may not be an intentional decision strategy. Instead, it may be a consequence of the structure of debt repayment in the world. Specifically, minimum payments and the implied recommendation to pay more than the minimum may play a role in driving consumers to repay as if balance matching in the world, because they increase dispersion.Finally, our participants do not appear to be using a pure 1/N heuristic with equal allocations across all accounts motivated by a desire to diversify ([ 4]). Instead, they seem to be focusing on salient interest costs and, when in the minimum-payment condition, moving repayments from this higher-interest debt to lower-interest ones. For example, in Experiment 1, participants allocate their largest repayment to the highest-interest-rate debt in 46% of rounds in the minimum-payment condition. By contrast, they allocate exactly evenly to all cards in only 12% of rounds. Thus, the effect of minimum payments appears to be a deliberate strategy shift that reflects their desire to act in accordance with the perceived recommendation from the minimum payment as opposed to a shift to 1/N heuristic use that relies on diversification in the absence of more informative cues. Limitations and Future DirectionsOur studies have several limitations that suggest avenues for future work. First, our paradigm includes only three allocation periods. There may be an opportunity to examine the impact of learning and feedback on repayment decisions by extending the paradigm to more rounds. Second, while we have evidence for two distributions of debt amounts, there may be boundary conditions induced by particularly low or high debt amounts. This avenue may be of particular interest given that we find substantially more dispersion in our studies than has been documented in field data ([23]). It may be that the way consumers accrue debt and the dispersion of their debt accounts influences the dispersion of their repayments. There may be additional scope for examining what specific component of the minimum payment leads consumers to perceive a recommendation to repay more than the minimum. For example, different types of minimum payments—such as round numbers or flat, consistent amounts across cards—may alter perceptions of recommendations. We present data on suggested minimum payments as well as a flat minimum of $25 (see Web Appendix A, Study C) that lead to similar levels of dispersion as documented in the previous studies. Finally, it is not clear how moving from an environment with all minimum payments to one in which only some cards have minimums would affect consumers. While our hypotheses concern minimums on all versus no cards, data reported in the Web Appendix suggest that the presence of minimum payments on only a subset of accounts may similarly increase dispersion (Web Appendix A, Study E). Capturing these details will help future work explore the dynamics of debt repayment decisions more thoroughly, with direct implications for policy.In addition, although we have identified perceived recommendations to repay more than the minimum as one mechanism underlying the dispersion effect, other mechanisms are likely to operate in concert. For example, some additional dispersion may come from using more approximate strategies, such as rounding, as a result of increased decision complexity ([16]). Another possibility is that minimum payments may alter the size of the consideration set, which could potentially lead to an increase in the use of 1/N type heuristics ([27]). Further, the default repayment experiment (Web Appendix A, Study D) provides suggestive evidence that increasing the number of active allocation decisions that a participant needs to make may also increase dispersion. Future research should examine these additional mechanisms in greater detail.Although we have been examining effects on average across the population, consumers do not all use the same strategies to make these decisions. Understanding heterogeneity in repayment decisions and how choice architectures can nudge consumers to make better financial decisions is important, particularly in light of recent findings that low-financial-literacy consumers are the most likely to be affected by certain nudges ([28]).Finally, there may be opportunities to bring the insights from the debt repayment heuristics literature to other similar allocation decisions, particularly investing. Our results suggest that highlighting important decision features—for example, fees in an investment context—may help consumers focus on those features. As a result, they may allocate their retirement dollars to higher-yield, lower-cost funds, which will leave them better off than using strategies such as the 1/N heuristic ([ 4]). In a charitable giving context, highlighting ratings such as Charity Navigator's may direct money toward higher-impact charities. In addition, if people simultaneously donate to a popular charity and other less well-known ones, adding a suggested donation to each charity may lead people to spread their donations more evenly. Implications for Marketers and Policy MakersThis article has several important implications for practice. First, the results highlight the importance of considering what inferences consumers will draw from choice architecture. We show that minimum payment requirements lead consumers to infer a recommendation to pay more than the minimum, which causes increased dispersion and higher interest costs. When policy makers revise requirements for credit card statements, our results suggest that it is important to test the inferences consumers draw and how it changes their decision making. Doing so will allow policy makers to produce desired effects while heading off unanticipated consequences.Second, while it is unlikely that the government would mandate the removal of minimum requirements, several decision aids could mitigate the costs associated with the dispersion effect. One possible choice architecture is adopting active-choice decision environments that highlight interest rates. Active-choice environments led to decreased dispersion in Experiment 4. Other work has shown consumers with an active-choice format are more likely to repay debts in full ([33]). This may increase the amount consumers dedicate to debt repayment in the future ([23]).Relative to a statement modeled on current paper statements, our participants tended to focus more on higher-interest-rate debts when the interest information was displayed in a format that facilitated comparison (Experiment 4). Featuring interest rates on the front page of credit card statements may help consumers focus on minimizing interest costs instead of amount-based heuristics. Currently, interest rate information is not shown on the first page of credit card statements, unlike minimum payments and other debt amount information. Making interest more salient could offset some of the costs associated with increased dispersion.Financial technology firms and account aggregators (e.g., Mint, NerdWallet), which consumers give account access to, may be in a good position to aid consumers by aggregating their credit card debt information, including interest rates. By focusing consumers on interest, our data suggest that such a decision aid could provide a buffering effect for consumers, reducing the consequences of dispersed allocations. A more extreme solution already being offered in the marketplace is automated credit debt management. For example, users of the app Tally pay a single sum to the app, which then allocates the lump sum to minimum payments and the consumer's highest-interest-rate debt ([37]). However, it is not clear that consumers are fully aware that they are paying excess interest, which may reduce demand for these products.Importantly, our results suggest another channel by which the number of debt accounts could influence financial well-being. Recently, there has been substantial interest in defining and measuring financial well-being, using both subjective assessments and administrative data (e.g., [10]; [29]; Netemeyer et al. 2018). Our work suggests that, holding both total debt amount and number of accounts constant, the amount of dispersion may serve as a cue to differences in long-term interest costs that a borrower is likely to incur. In addition, reducing the total number of debt accounts may blunt the dispersion effect simply because there are fewer debts to disperse over. This could also lead to lower accumulated interest costs over time, even given an identical debt balance initially. As a result, the total number of active debt accounts and dispersion across these accounts may be useful indicators for measuring and intervening on financial well-being. Drawing firms' attention to these metrics may change the way they help consumers plan, budget, and understand their best options.Finally, our work highlights the importance of considering the consumers' situation and inferences when designing policy. Most consumers have multiple debt accounts, so nudges targeted at increasing repayment on individual accounts may lead consumers to incur additional costs. In this instance, policy makers need to consider the impact of nudges on the full portfolio of debt accounts when they make policy changes. More broadly, applying marketing research to policy requires careful consideration of consumers' decision environment. We recommend that marketers make use of descriptive research not just to motivate their research questions but also in their empirical designs to maximize the likelihood of real-world impact. "
32,"Platform Exploitation: When Service Agents Defect with Customers from Online Service Platforms Online, pure-labor service platforms (e.g., Zeel, Amazon Home Services, Freelancer.com) represent a multibillion-dollar market. An increasing managerial concern in such markets is the opportunistic behavior of service agents who defect with customers off platform for future transactions. Using multiple methods across studies, the authors explain this platform exploitation phenomenon. In Study 1, they utilize a theories-in-use approach to clarify why and when platform exploitation occurs and derive some hypotheses. Study 2 empirically tests these hypotheses using data from a health care platform that connects nurses and patients. The results indicate that high-quality, long-tenured service agents may enhance platform usage, but customers also are more likely to defect with such agents. Platform exploitation also increases with greater customer–agent interaction frequency (i.e., building stronger relationships). This phenomenon decreases agents' platform usage due to capacity constraints caused by serving more customers off platform. These effects are stronger as service price increases (because higher prices equate to more fee savings), as service repetitiveness increases, and as the agent's on-platform customer pool comprises more repeat and more proximal customers. Finally, the authors use two scenario-based experiments to establish some managerial strategies to combat platform exploitation.Keywords: customer defection; disintermediation; hazard model; opportunism; service platforms; sharing economy; theories in useOnline service platforms help customers and service agents connect and thereby facilitate transactions of physical assets (e.g., Airbnb), combinations of assets and labor (e.g., Uber), or pure labor (e.g., TaskRabbit) ([ 4]). Pure-labor platforms are growing in markets for home services (e.g., Handy), medical services (e.g., Heal), and specialized outsourcing (e.g., Freelancer.com) that grant customers access to ""qualified"" (e.g., background-checked) service agents and provide service agents with access to jobs. For both parties, platforms reduce search costs and transaction uncertainty and provide scheduling and payment services ([ 4]), usually in return for a commission on each transaction.But a growing challenge for these service platforms, especially pure-labor platforms, is that a matched agent and customer may knowingly break platform rules and engage in subsequent transactions off the platform to avoid platform fees, a phenomenon we call ""platform exploitation."" Such exploitation reduces vital platform revenue and can even threaten the platform's survival ([23]; [46]). Anecdotal evidence of it abounds. [34] quotes an agent who admits, ""I quit [using the platform] because … once people found me, they would just call me direct."" The chief executive officer of Openbay has lamented, ""A car owner who finds a great mechanic through Openbay may just call the mechanic directly the next time her car needs a repair, rather than book … through [us],"" and Rover's chief executive officer similarly noted, ""A dog owner who books a sitter through Rover.com may just call that sitter directly the next time"" ([28]). In response to these behaviors, many platforms prohibit agents from transacting with customers off platform, suspend the accounts of agents found doing so, and explicitly require that the payment be made via the platform (see Table 1). Platforms often inform customers of the risks of transacting off platform and ask them to report agents who ask for direct payment. To incentivize customers to stay, platforms attempt to provide more value, with assurances (e.g., Amazon's Happiness Guarantee) or enhanced functionality (e.g., Rover's Dog Walking Map).GraphTable 1. Platform Rules to Prevent Exploitive Disintermediation. Platform RuleRule Summary from Terms and ConditionsExample PlatformsExclusive subsequent bookingsAll subsequent appointments between agents and customers must be made through the platform.Zeel, SootheExclusive communication channelAgents must only communicate with users via the platform.FreelancerProhibition of transacting outside the platformNurses are prohibited from transacting with platform patients in private. Penalties of breaching include a warning if suspected and permanent account suspension if confirmed.Our nursing platformProhibition of exchanging contact informationUsers are not allowed to share identifiable personal information with each other.TaskRabbit, Tutor.com, Glamsquad""Referral fee"" chargeA very high ""referral fee"" will be charged for transactions outside the platform (e.g., $1,000 per instance).WagWithholding paymentPayment to agents will be held if any agent activity threatens the platform.Rover 1 Notes: These rules reflect various platforms' terms and conditions. The consequences for violating them include suspended accounts, ceased cooperation, and withheld payments.Despite the prevalence and importance of platform exploitation, as well as existing calls for research ([13]), the phenomenon has prompted few scholarly examinations ([21]; [22]; [46]). No research explicates its exploitative nature or tests strategies for combating it. We examine platform exploitation to shed light on why and when platform exploitation is likely to occur, clarify the relational and transaction dynamics surrounding it, and propose managerial interventions. We address platform exploitation in general but conduct empirical tests with pure-labor platforms, which are especially vulnerable because the services they involve typically require agents and customers to interact personally (e.g., in-home care), involve commission-based fees, allow relationships to form through communication and coordination, and rely on an agent's individual skill.We use multiple methods to achieve these insights. In Study 1, we identify drivers of platform exploitation and uncover the theories-in-use held by platform participants on an in-home health care (patient–nurse) platform ([43]). Findings reveal that platform exploitation exists and is pervasive; they also indicate factors that lead customers and agents to defect. We combine these insights with existing theory to derive hypotheses tested in Study 2.Study 2 uses the same setting to analyze 17,636 platform transactions among 12,523 unique patients (customers) and 2,009 nurses (agents). As customer–agent interaction frequency increases, platform exploitation increases. For platforms, high-quality or long-tenured agents who prompt platform usage can be a double-edged sword; they may also entice customers off the platform. These effects are exacerbated at higher prices and with more repetitive services. Among agents, interaction frequency, agent quality, and tenure also reduce agent return rates due to capacity constraints. That is, as agents take more customers off platform, they have less time for on-platform orders. These effects are magnified for highly priced services. The negative effects also are exacerbated by variables that reflect exploitation opportunities in the agent's pool of recent on-platform customers, such as when more of them are repeat customers or geographically close. In a robustness check, these core findings hold when we test them as predictors of the likelihood that a specific customer–agent dyad will return to the platform.Finally, we use the insights from Studies 1 and 2 to inform scenario-based experiments in a different pure-labor context (dog walking platform) and test the efficacy of two interventions: a sliding-scale fee (financial mechanism) and service–agent community building (nonfinancial mechanism). Both interventions are effective in combating platform exploitation.We extend prior literature by introducing platform exploitation, its driving factors, and a theoretical lens to understand it. These insights can generalize to service platforms that require close contact, communication, and skilled agents. We provide managerial implications related to the conditions in which platform exploitation is most likely and strategies for counteracting it. Literature DisintermediationStudies that examine situations in which buyers and sellers avoid intermediaries and transact directly tend to take a neutral stance toward such disintermediation, focusing more on overall market welfare ([ 5]; [31]). For example, [39] consider how bypassing traditional publishers in the book industry affects product variety and quality. Few studies take the intermediary's perspective.Platform exploitation involves disintermediation, but it is unique due to its opportunistic nature. In using the term ""exploitation,"" we explicitly aim to connote opportunistic misuses of a platform. Traditional disintermediation involves legal choices by customers, producers, or channel members to bypass an intermediary, without violating existing agreements, but with platform exploitation, users knowingly break their agreement with the platform, consistent with Wathne and Heide's (2000) idea of active opportunism (see common platform rules in Table 1). For example, the user agreements for Zeel and Freelancer specify that all customer–agent communications and future transactions must occur on the platform. The agreements also spell out consequences for violating these rules, including account suspension and withheld payments. Despite agreeing to these rules, agents exploit the platform to match with good customers and then take those customers off platform. Acquisition costs mean that platforms must rely on repeat customers and agents for profitability. Platform exploitation thus keeps current and future transaction-based revenue from the platform, hurts customer retention, and reduces agents' availability for on-platform customers, threatening the platform's profits and its very viability.Although managerial thought pieces acknowledge that disintermediation may threaten the survival of service-based platforms ([22]; [46]), they do not investigate the antecedents of disintermediation, discuss the problems it causes, or examine the practice in a platform context. Notably, [21] examine users of a freelance project platform, who begin a discrete project and then complete that project either on or off the platform. Using a randomized control trial study, with treatment and control conditions that are not fully unique, they suggest a role for trust. When a freelancer has higher customer satisfaction scores, the likelihood that customers and freelancers finish the transaction off platform increases. Although these scholars do not simultaneously test all the statistical interactions that are key to their conclusions, their findings align with and help support our view.Overall, the issues surrounding platform exploitation remain poorly understood. First, no study explicates the opportunistic nature of the behavior, the contextual conditions that alter its likelihood, or its long-term consequences for firms. Second, platforms differ widely in their design and service type, the presence and style of reputation systems, and the information shared prior to a transaction. For example, Amazon Home Services does not offer reputation scores or allow for customer–agent communication prior to transactions. Thus, we consider which types of platforms are more vulnerable to exploitation and how it manifests in practice. Third, little is known about interventions platform managers might take. In their conclusion, [21] propose a few strategies but do not test them empirically, a critical gap that we address. Pure-Labor Platforms and Platform ExploitationAlthough service platforms differ in the resources they monetize, most research focuses on asset-based platforms that involve transactions of underutilized physical assets (e.g., [38]). However, pure-labor platforms have unique aspects ([ 4]) that make them susceptible to exploitation. The quality of the customer experience relies solely on the agent, because there is no physical asset quality (e.g., comfortable, clean car) to compensate for concerns about the agent (e.g., erratic Uber driver) ([13]). In turn, customers likely perceive the agent (not the platform) as the source of value and may develop interpersonal trust with agents ([21]). Services also require substantial communication and coordination between parties to clarify work specifications, which enhances the social aspects of these transactions. Agents and customers experience personal vulnerability that requires them to exhibit trust, especially as pure-labor services often occur in private settings (e.g., in-home). Thus, these aspects of pure-labor services can promote interpersonal loyalty.Such enhanced trust and loyalty decrease the need for the platform. Initially, the platform makes matches and reduces search costs and uncertainty (e.g., by providing guarantees), serving as a formal institution that establishes rules to overcome transaction obstacles, as predicted by transaction cost economics ([41]). However, as trust between parties develops, the trust itself can serve as an informal institution ([16]) that effectively overcomes transaction hurdles ([21]). With trust, the need for the formal protections of the platform declines. For the agent, once matched with a desirable customer, the value of the platform for further transactions with that customer diminishes greatly. Payments and scheduling are easy to handle in other ways. Furthermore, agency theory predicts that if the platform fee (∼15%–30% of price) exceeds the value of added benefits ([37]), agents are likely to act opportunistically at the expense of the platform's interests. Yet the agent is the face of the platform to customers, so agents have both motivation and opportunity to pursue their own self-interest. The platform risks becoming a mere customer prospecting tool.Figure 1 summarizes these incentives for agents and customers to defect from the platform and links those motivations to our research questions. Even if we can argue theoretically why platform exploitation is likely, we know little about how and when it manifests in practice. Therefore, with Study 1, we directly solicit insights from practitioners regarding ( 1) the degree to which platform exploitation occurs, ( 2) how and when customer–agent relationships form, and ( 3) which customer and agent characteristics increase the likelihood of platform exploitation.Graph: Figure 1. Theoretical underpinnings of platform exploitation and research questions. Study 1: Exploratory Interviews to Derive Theories in Use ContextThe context of Study 1 is a platform in China that connects patients and nurses for in-home health care services (e.g., injections, infusions). In China, medical services are confined to a few big hospitals in cities, increasing patients' transaction costs (e.g., long waits). Launched in 2015, the platform is one of the largest platforms for in-home nursing services in China, according to the Baidu Mobile Assistant app market. Patients can register for free; the platform reviews patients' medical documents to verify an order and then makes it available to nurses on the platform. A nurse who accepts an order contacts the patient and performs the service in the patient's home. The platform sets the price for each service (e.g., dressing change = ¥139, or ∼$21) and receives a 30% commission on all orders. It has explicit policies that nurses who perform off-platform transactions will be excluded from future platform use. Nurses usually work full-time at medical institutions and use the platform to earn extra money. To register on the platform, nurses must be certified, have three years of experience, and submit to a background check.According to the summary of business models and designs of various pure-labor platforms in Table 2, the focal platform is not unique in its rules and operations. For example, many platforms similarly set prices, take percentage-based commissions, and rely on platform-matched agents to serve customers. We note some variations in whether agent information is revealed to customers before a transaction, but generally, platforms that set pricing tend not to reveal agent information before scheduling the transaction (e.g., Wag, Zeel).GraphTable 2. Business Models of Pure-Labor Service Platforms. Service TypePlatform CompanyRevenue Stream (% of Price Retained by Platform)Who Sets Service Price?Who Matches Agent When Fulfilling Order?Agent Info. Visible to Customer Before Order?Payment by Platform Required?Valuation (in Millions of U.S. Dollars)Home servicesAmazon Home ServicesTransaction-based revenue (15%–20%)PlatformPlatformNoYesN.A.HandyTransaction-based revenue (around 20%)PlatformPlatformNoYes500TaskRabbitTransaction-based revenue (15%)AgentCustomerYesYes125ThumbtackQuote fee from agentAgentCustomerYesNo1,700Work outsourcingFreelancer.comTransaction-based revenue (13%–20%)AgentCustomerYesYes290FiverrTransaction-based revenue (around 20%)AgentCustomerYesYes525Dog careRoverTransaction-based revenue (15%–25%)AgentCustomerYesYes970WagTransaction-based revenue (around 40%)PlatformPlatformNoYes650TutoringWyzantTransaction-based revenue (20%–25%)Platform/agentPlatform/customerYesYes85Tutor.comTransaction-based revenue (N.A.)PlatformPlatform/customerYesYes66MassageSootheTransaction-based revenue (around 30%)PlatformPlatformNoYes272ZeelTransaction-based revenue (N.A.)PlatformPlatformNoYes129Medical careDoctor on DemandTransaction-based revenue (around 25%)PlatformPlatform/customerYesYesN.A.HealTransaction-based revenue (N.A.)PlatformPlatformNoYes228Our nursing platformTransaction-based revenue (30%)PlatformPlatformNoYesN.A.Beauty careGlamsquadTransaction-based revenue (around 40%)PlatformPlatformNoYes123StylebeeTransaction-based revenue (20%–33%)PlatformPlatformNoYesN.A.PsychotherapyTalkspaceSubscription by customerPlatformPlatform/customerYesYes210BabysittingUrbansitterSubscription by agent and customerAgentCustomerYesNo110CaregivingCare.comSubscription by agent and customerAgentCustomerYesNo867 2 Notes: N.A. = not applicable. Most information is based on the platforms' ""Terms & Conditions""; valuation data are from the PrivCo database (in millions of dollars). Theories-in-Use ApproachGiven this nascent area, we begin with a theories-in-use approach ([43]). We interviewed nurses, patients, and platform managers from cities served by the platform. Because our goal was discovery, the sample included participants with varying demographics and experiences with the platform. Following McCracken's (1988) guidance to continue interviews until no further significant insights arise, we interviewed 35 participants: 15 nurses, 15 patients, and 5 platform managers (each was paid $15). The sample details are in Web Appendix Tables W1, W2, and W3. Interviews were conducted in Chinese and ranged from 10 to 30 minutes. As recommended by [43], we began by asking generally how the participant feels about using the platform, probing for pros and cons. After building rapport, we asked nurses (patients) whether they had ever worked with a patient (nurse) initially through the platform and then moved future transactions off platform or if they had heard of others doing so. If they answered affirmatively, we followed up to uncover reasons why they (or others) took the transactions private and when patient–nurse relationships develop. We used a similar protocol for managers but also asked about how pervasive off-platform transactions were, whether they affect the platform, and the actions the platform takes in response. Given the topic's sensitive nature, we assured participants of confidentiality. Interviews were recorded and transcribed, and we used the transcripts to search for emerging themes. The results inform our hypothesis development. Table W4 in the Web Appendix contains summary data. Platform Exploitation and Its Impact on the Company Existence and pervasiveness of platform exploitationPlatform exploitation is pervasive: 12 of 15 nurses (80%) and 10 of 15 patients (67%) had personally transacted off platform with partners they met on the platform (and nearly all had heard of others doing so). This pervasiveness exists despite explicit policies, clearly communicated to nurses and patients, that prohibit off-platform transactions. The platform managers noted that nurses would be ""permanently blocked from taking orders on our platform"" and that customers are told ""do not make a private deal with nurses."" Patients are aware, acknowledging that ""[they] don't want us to transact directly."" As a nurse explained,Of course, the platform doesn't want these cases [transactions in private] to happen. They are trying to prevent it. But this is unavoidable. The nurses are not stupid. They know if they build relationships with the customers, they control the resources. Financial impact on the companyManagers view platform exploitation as a major problem that reduces platform revenue and profit, such that ""every year we are missing three to four times the profit we are making right now"" because the platform constantly recruits new nurses and patients to replace those who leave. Recruiting and on-boarding (e.g., background checks) is expensive. For example, the platform's primary recruiting method for new nurses is to pay a bonus for referrals from current nurses. Managers estimate the combined cost of acquiring a new customer and a new nurse at ¥110–¥150. For a typical order—say, a ¥139 injection—the platform earns 30%, or ¥42. Using injections as an example, for the platform to break even on its acquisition costs, a new nurse and patient must complete three to four injections on the platform. Thus, retention and repeat ordering are critical to the platform's profitability. Spillover effectsPlatform exploitation also creates consequences that are not themselves direct violations of platform policies but are potentially harmful to the platform. First, some potential customers never try the platform. Nurses noted that when they have a good relationship with a patient, the patient often introduces relatives and friends to them directly, bypassing the platform (corroborated by patient interviews). Second, high-quality nurses may use the platform less. These nurses reduce their platform usage when they have ""a stable [off-platform] customer base,"" because they have ""no time to take more orders [on the platform]."" This trend affects the quality of the platform's portfolio of nurses and increases recruiting costs to maintain enough high-quality nurses. Third, platform exploitation might alter the quality of the platform's patients. Nurses have incentives to take ""good-quality customers"" off platform, skimming the best patients and leaving riskier ones ([ 1]). As explained by one nurse,As long as the nurse starts taking orders in private, she'll keep going, not [using] the platform. Unless she feels the risk is particularly high, such as the patient or his family members are particularly mean and the patient's condition is serious … once she has assessed the patient and found that the family is okay and the patient is okay, she will always consider accepting orders in private. Factors that Lead to Platform Exploitation Relational dynamicsVarious relational dynamics and motivations underlie defections from the platform. First, nurses and patients frame affiliations with one another as relational (e.g., nurse: ""It's not like a relationship of serving and being served. It's more like friends""; patient: ""We are like friends""). Service platforms may promote such relationship building because the person-to-person nature of service provision requires extensive communication and coordination. A patient described the process as ""making friends at the beginning."" Relationships grow through multiple interactions across service visits between the same nurse and patient.Second, once built, relationship trust reduces information asymmetry and uncertainties in future transactions (e.g., ""If we trust each other … it's better to contact directly. I feel secure if we know each other""). Using the same partner increases communication efficiency, saving time:When I do infusion[s] … I need to know the specific condition of the patients before I go. But the platform will not offer me very detailed information. If I [work with] patients directly, it's much more convenient since [I] know what they need … specifically.It is ""inconvenien[t] for patients to change nurses every time"" because they have to repeatedly share information. Through private dealings, patients have on-demand access to service, easily reached by ""the phone at [their] disposal."" By ""contact[ing] this nurse directly, I can just tell them what I need, there's no need to use a third party."" Transactional dynamicsOperational and participant characteristics within a transaction also exert effects. First, platform exploitation is heavily driven by the potential for monetary savings. Taking orders privately enables ""patients [to] pay less, and nurses [to] earn more."" As detailed by one nurse, a particular service ""was 130 yuan per time. The patient saves 38 yuan, I earn about 20 to 30 yuan more. It was a long-term and multitime [procedure]."" Higher-priced services are more likely to be conducted off platform, because the savings earned from avoiding the fixed-commission fee are higher. Nurses similarly prefer to take higher-value patients off platform, to avoid competing for them, noting that ""it's not the case you can grab [a customer's] order on the platform every time.""Second, in terms of agent characteristics, patients seek to defect with high-quality nurses, defined by their ""good operation skill"" and ""good serving attitude,"" which are important determinants of trust. Nurses who have used the platform longer also appear more likely to defect, because they ""know more about the processes"" and how the platform truly works.Third, opportunities for platform exploitation depend on the characteristics of the customers the nurse currently serves. For example, agents who work with many customers who are proximal have a customer pool ripe for exploitation; as a patient noted, nurses ""can decide whether they want to work with me based on whether their location is close to my place.""Although platform exploitation is pervasive, nurses and patients also highlight some benefits of remaining on the platform. The platform offers a large pool of nurses, increasing patients' constant access to service. It can help resolve ""customer disputes"" and ""offer protection for both patients and nurses"" in every transaction. Thus, not everyone seeks to leave. Platform exploitation is a strategic decision, based on the situation and any patient–nurse relationship. Platform Exploitation: Predicting Customer Retention and Agent Return RateWe integrate these interview findings with existing theory to derive a set of testable hypotheses about when platform exploitation is most likely. Figure 2 illustrates our conceptual model. Table W5 in the Web Appendix summarizes the representative quotes and theory for each hypothesis.Graph: Figure 2. Conceptual framework: drivers of platform exploitation.The interview data suggest that platform exploitation manifests differently for each party. Customers' primary motivation is to find a good agent with whom to defect, and thereafter, they are unlikely to return to the platform. Thus, we predict factors that might decrease customer retention on the platform. However, the agents engaged in platform exploitation are unlikely to leave completely because the platform provides ongoing opportunities for customer prospecting. Still, agents do not have unlimited service capacity ([ 9]); when an agent takes more customers private (i.e., builds a base of off-platform customers), an increasing amount of that agent's capacity is used, so the agent likely is slower to return to the platform. Thus, we predict factors that decrease agents' rate of returning to the platform. Drivers of Platform Exploitation: Main EffectsWe begin with three main-effect variables, prominent in the interviews: number of prior dyadic transactions, agent quality, and agent tenure. Number of prior dyadic transactionsInterviewees noted that platform exploitation becomes more likely as trust within a customer–agent dyad builds over repeat interactions (e.g., nurse: ""It is hard to establish a good relationship during the first meeting""; patient: ""It's like making friends … after [a few] times""). Service transactions are prone to relationship building because they involve high levels of interpersonal communication ([29]), and repeated transactions provide evidence that reinforces trust ([24]). Greater customer–agent trust also reduces uncertainties, so the platform becomes less necessary for future transactions. Thus, repeated, dyadic transactions between an agent and customer should increase platform exploitation, in the form of reduced customer retention and agent return rates. H1:  As the number of prior platform dyadic transactions increases (number of times an agent transacts with the same customer on the platform), the (a) probability of customer retention and (b) agent return rate to the platform decrease. Agent qualityThe interviewees noted that patients are most likely to defect with high-quality nurses, stressing qualities such as ""good operation skill"" as keys to growing trust, so that patients ""want to contact you directly for services."" This finding is congruent with Gu and Zhu's (2020) arguments, but the trust we investigate arises more from social interaction and direct customer experiences of quality, not an agent's public rating. High-quality agents engender trust, reducing transaction uncertainty and making platform exploitation more likely.Thus, platforms face a paradox. Agent quality drives platform growth ([11]), and platforms can evoke more customer responses by featuring agents in advertisements ([12]). Yet high agent quality may decrease customer retention. The paradox implies a nonlinear, inverted U-shaped relationship between agent quality and retention: bad experiences with low-quality agents drive customers away, and initial increases in quality should increase retention, but after a certain level of agent quality, customers grow more likely to defect with these superior agents. We anticipate similar effects on the agent return rate, such that high-quality agents are more desirable to customers and thus more capable of growing a private customer base, reducing their return rate to the platform. H2:  Inverted U-shaped relationships exist for agent quality relative to both (a) the probability of customer retention and (b) agent return rate, such that greater agent quality increases these outcomes up to a certain point but then reduces them. Agent platform tenureThe length of time an agent has served on the platform (i.e., platform tenure) should increase the likelihood of platform exploitation because experienced agents, as stated by a nurse interviewed for Study 1, ""know more about the processes, and what services they can offer to the customers and what the customer can get."" Their insights into the platform's processes and institutional knowledge of how to use the system enable them to attain personal gains, as well as maximize value for customers. When agents first join the platform, they rely on it to acquire customers, and they may be reluctant to take customers private. Over time, as they meet more customers, identify desirable ones, and learn that the likelihood of exploitation activity being discovered is low, they feel less dependent on the platform and more confident taking customers private. This logic also applies to the agent return rate. As longer-tenured agents gain more off-platform customers, more of their capacity gets taken up, and they return to the platform more slowly. Formally: H3:  As agent platform tenure increases, the (a) probability of customer retention and (b) agent return rate decrease.We next examine several moderators to these main effects that follow from our interviews. We consider four moderators: one relevant to both parties (i.e., transaction price), one for customers (i.e., service repetitiveness), and two for agents (i.e., on-platform customers that are repeat and proximal). Moderating Role of Transaction Price on Customer Retention and Agent Return RateTransaction price enhances platform exploitation motivations for both parties. Patients and nurses both identified avoiding fees as a major benefit of skirting the platform to work with a trusted partner. When fees are a fixed percentage of price, potential savings increase with price. As [23] notes for the cleaning service platform Homejoy, ""The higher the commission, the higher the incentive for cleaners and customers to strike a better deal directly."" However, even though price is a strong motivating factor, we do not think it is sufficient to drive platform exploitation; customers and agents leave the platform only after finding a good match. Thus, higher prices increase the financial incentives and motivations of both parties to move the relationship off platform given a trusting relationship. Thus, we propose price as a moderator of all three main-effect relationships involving customer retention and agent return rates. H4:  The effect of prior dyadic transactions on (a) customer retention and (b) agent return rate intensifies as price increases. H5:  The effect of agent quality on (a) customer retention and (b) agent return rate intensifies as price increases.[ 6] H6:  The effect of agent tenure on (a) customer retention and (b) agent return rate intensifies as price increases. Moderating Role of Service Repetitiveness on Customer RetentionWe define service repetitiveness as the frequency with which customers require a particular service. The Study 1 interviews indicate that moving future transactions with a given agent off platform reduces the hassle of repeating the order process, offers greater service delivery flexibility, and reduces coordination effort. Thus, we expect that greater service repetitiveness increases customer motivation to defect with a good agent and thus will strengthen the negative main effects on customer retention. H7:  The effects of (a) prior dyadic transactions, (b) agent quality, and (c) agent tenure on customer retention intensify with greater service repetitiveness. Moderating Role of an Agent's On-Platform Customer Pool on Agent Return RateThe last two moderators provide insights into when agents might move their platform customers off platform. As agents acquire a larger on-platform pool of customers, they become less dependent on the platform for customer acquisition, and this asymmetric dependence makes opportunistic behavior more likely ([32]). Certain characteristics of the agent's on-platform pool in turn might indicate increased exposure to desirable customers and thus increase exploitation opportunities.The interviews suggest that nurses prefer to work with patients who are geographically close, for efficiency reasons: ""Nurses want to have a stable and regular customer base. If we can meet patients in our area through the app, then we [know] each other."" Patients recognize this desire for geographic proximity, noting that nurses ""can decide whether they want to work with me based on whether their location is close to my place."" The percentage of on-platform customers who live in close geographic proximity to a particular agent represents the potential pool of desirable customers for this agent to take off platform. However, customers might be willing to leave with these motivated agents only if they represent a good match, so we propose it as a moderator. H8:  The effects of (a) prior dyadic transactions, (b) agent quality, and (c) agent tenure on agent return rate intensify as the percentage of an agent's current pool of proximal on-platform customers grows.Finally, assuming good customer–agent matches have been established on the platform, the percentage of repeat on-platform customers within the agent's portfolio represents targets who are particularly susceptible to be taken off platform. Agents who build relationships with repeat customers become less dependent on the platform and more able to act opportunistically, so that ""only after I've been on this platform for a while, I started to offer services to the patients directly,"" whereas ""right at the beginning … you don't have your customer base … not many people know you."" Agents with a higher percentage of repeat customers have a pool of good prospects for off-platform transactions because their prior interactions have helped establish trust ([29]). Trusted agents have more motivation and opportunity to move customers off platform. H9:  The effects of (a) prior dyadic transactions, (b) agent quality, and (c) agent tenure on agent return rate intensify as the percentage of an agent's current pool of repeat on-platform customers grows. Study 2: Analyzing Platform Transaction DataWe analyze transaction data from the platform from Study 1 to test our hypotheses. We observe all transactions on the platform between July 1, 2017 and July 4, 2018, which encompasses 17,636 transactions by 2,009 unique nurses and 12,523 unique patients. In this set, 22.3% of patients place multiple orders (41.9% of observations). There are 15,002 unique patient–nurse dyads, and 1 in every 3.43 transactions involves dyads observed multiple times.We jointly model customer retention and agent return rates, which provides stronger evidence of platform exploitation while also enabling us to address the specific hypotheses for each. If a customer defects after working with a desirable agent but the agent's platform usage remains the same, then platform exploitation likely is not driving defection (e.g., patients might recover faster due to high-quality care). However, if both parties decrease platform usage, the pattern indicates platform exploitation. As a robustness check, we also estimate the probability that each dyad returns, which reflects the joint decision by a customer and an agent to continue their relationship through the platform.There are four unique strengths of our data set. First, customers are financially motivated, because they pay for the service themselves (i.e., no insurance payments). Second, in-home service is an attractive option for customers who are unable to travel and who find the service quality offered by large hospitals in China insufficient ([15]). Third, no significant new competition emerged or public policy changed ([45]) during the study period. Fourth, the rate at which a customer requires service is dictated by the type of service they need, not the agent. The platform mostly deals in routine services needed at regular intervals. Dependent Variables Customer retention, time until a customer's next orderFor each customer, we observe the number of days until the next order by the same customer,  tC  . Variables are right-censored because we do not observe additional orders after the sample ends. For right-censored observations,  tC  equals days from the last order to the end of the sample period. An indicator variable marks censored observations, such that  γ  = 1 if the customer places another order, and 0 otherwise (censored). Agent return rateFor each agent, we observe the number of days between orders they filled,  tA  . We let  ξ  = 1 if we observe the agent fill another order, and 0 otherwise. Accordingly,  tA  is the days from the agent's last order to the end of the sample time frame when  ξ  = 0. Dyad retention, time until the dyad's next orderWe observe the number of days between each dyad's appearance on the platform,  tD  . We let  ϱ  = 1 if we observe the dyad again, and 0 otherwise, and here,  tD  is the days from the last order to the end of the sample when  ϱ  = 0. Independent Variables Prior dyadic transactionsWe operationalize prior transactions as the number of times, prior to the current order, that the customer received service from the agent who fills a current order (  PriorDyadVisits  ). Agent qualityThe platform provides a continuous measure of quality (  AgentQuality  ) for each nurse. The measure is proprietary to the platform and not revealed to customers, which lessens endogeneity concerns because customers cannot rely on this quality metric when making decisions about future orders. According to the platform managers, factors that contribute to  AgentQuality  include the nurse's job title, customer ratings (not visible to customers or agents), years of experience, and number of platform orders filled. This measure captures quality differences because it considers both experience independent of the platform and performance on the platform. It also is managerially relevant and used to evaluate agents. We normalize  AgentQuality  from 0 (worst) to 100 (best). Agent tenureWe use the number of days since the agent joined the platform to operationalize agent tenure (  AgentTenure  ). Moderators PriceWe obtain each order's total price (  Price  ). Prices for each service do not change over the observed time frame, which eliminates endogeneity issues. However, some customer-specific variation in service prices might occur, due to surcharges imposed if agents must travel more than 5 km or provide medical supplies (e.g., bandages). These actual costs do not enable the agent to make extra money. We separately control for medical supply surcharges (  EquipmentCosts  ), which might signal particularly complex or risky services. By controlling for distance and equipment cost,  Price  thus captures the unchanging platform charge and associated fees that could be saved by circumventing the platform. Service repetitivenessWe measure the degree to which a given service is required repeatedly by customers. A service is ""new"" for customers the first time they receive it and ""repeat"" all other times. We operationalize service repetitiveness (  %ServiceRepeat  ) as the percentage of times a service is repeated on other customers by other agents in a given month. We calculate the measure after excluding data from the customer and agent of the focal observation so that it is not influenced by their characteristics. The monthly time frame addresses seasonality in service provision (e.g., infusions are more common during flu season). Agent's current pool of proximal on-platform customersThe current pool of proximal on-platform customers (  %CloseCustomers  ) equals the fraction of customers served by the agent in the last 30 days who are within 5 km of the agent's location. Five kilometers is what the platform considers distant when imposing a travel surcharge; 30 days captures recent opportunities. Agent's current pool of repeat on-platform customersSimilarly, the agent's current pool of repeat on-platform customers (  %RepeatCustomers  ) is the fraction of repeat customers (i.e., repeat divided by total customers) served by the agent in the past 30 days. ControlsTable 3 lists the summary statistics (correlations are in Web Appendix Table W6) for our controls. We include several variables managers use when computing agent quality to address potential nonquality confounds. For example, an agent's years of nursing experience likely relates to their salary at their full-time job, which then may correlate with their desire to use the platform to earn extra money. Controlling for these observables ensures that  AgentQuality  reflects the inputs to platform's quality measure that more strongly relate to overall excellence of the nurse (e.g., customer ratings and job titles). We also include service dummies (24 services), city dummies (188 cities), and time dummies by month.[ 7] The time dummies capture platform-wide shocks in a period.GraphTable 3. Variables Names, Definitions, and Descriptive Statistics. Variable NameDefinitionMSDMinMaxDependent VariablestCNumber of days between orders for the customera131.884120.603.001368.701γDummy = 1 if the customer places another order after current order.290.454.0001.000tANumber of days between orders filled by the same agenta29.72871.416.001368.560ξDummy = 1 if the agent fills an additional order after current order.886.318.0001.000tDNumber of days between times the same dyad is on the platforma154.609118.919.001368.701ϱDummy = 1 if the dyad returns to the platform after current order.149.356.0001.000Key Independent Variables and ModeratorsAgentQualityContinuous agent quality measure given by the platform11.73016.705.000100.000AgentTenureNumber of days since the agent was first observed on the platform406.043196.269.000966.000PriorDyadVisitsNumber of times the customer is observed receiving service from the same agent that fills the current order prior to the current order.4602.079.00043.000PriceTotal order price (¥) including distance and medical supply surcharges308.753381.76470.0004434.000%ServiceRepeatPercentage of time the service is performed as repeat on other customers by other agents in a given month.431.204.0001.000%CloseCustomersFraction of customers served by the agent in the last 30 days within 5 km.519.340.0001.000%RepeatCustomersFraction of repeat customers served by the agent in the last 30 days.052.134.0001.000Additional Control VariablesCustomer characteristics CustomerSameServiceNumber of times prior to the order the customer orders the same service1.0975.129.000102.000 CustomerPriorOrdersNumber of orders by the customer prior to the order1.5915.788.000102.000 TimeOnAppNumber of days since customer was first observed on the platform75.332147.578.000889.000Agent characteristics AgentSameServiceNumber of times prior to the order the agent performs the same service8.55616.367.000121.000 AgentOrderNumNumber of times the agent fills an order, including current order37.70056.0681.000332.000 AgentHomeVisitsTotal number of times agent has performed a home visit on the platform324.585465.472.0002624.000 AgentProExpYears of nursing experience for the agent3.2181.7933.00028.000 PotentiallyStolen  CustomersNumber of customers served by the agent prior to the order that have not been observed returning to the platform23.11535.205.000238.000Order characteristics EquipmentCostsMedical equipment and distance surcharges for the order15.07833.963.000750.000 MultiVisitDummy = 1 if the order included multiple in-home visits.167.373.0001.000 DistanceThe distance in meters between the customer and agent in the order7,056.9179,248.474.00098,218.00 How order was filled  RequestDummy = 1 if the patient requests a nurse and the nurse accepts.185.389.0001.000  ScrambleDummy = 1 if the order is available as first-come, first-serve.762.426.0001.000  RequestScrambleDummy = 1 if the patient requests a nurse who is unavailable after which the order is made available as first-come, first-serve.052.223.0001.000 3 aNumber of days from the last observation for the customer, agent, or dyad to the end of the sample period for censored observations.4 Notes: N = 17,636.We also use dummies to control for how customers and agents connect on the platform. The platform's process for connecting parties mitigates endogeneity concerns. Neither party can select partners on the basis of unobservables that might also affect customer retention or the agent return rate. Agents fill orders on a first-come, first-served basis (76.22%), after reviewing the service type, travel distance, and date/time requested. No other information is available until an agent accepts. It also is difficult and costly to cancel orders. Selection issues thus are addressed by observable factors. Consumers also can request an agent, who accepts (18.54%) or not (5.23%); following a rejection, the process reverts to first-come, first-served.[ 8] Only customers can make requests; agents are unable to pick customers. The dummy that indicates an agent accepts a request captures variance due to customer preference for an agent. ModelAt the customer level, we model the probability that a customer returns after the most recent order, regardless of whether they use the same agent. Customer return to the platform thus represents the ""event"" in hazard-model terminology ([35]). We have multispell data, such that some customers experience the event many times ([ 3]).[ 9] At the agent level, we model how quickly agents return for more orders, regardless of whether they match with different customers. We use a continuous-time hazard approach for both customers and agents because we observe the exact order time, and there is no set order schedule ([14]). Customer sideHazard models assume that all individuals experience the event, so we follow [30] and adapt a multispell hazard model with a cure fraction (i.e., percentage of individuals who will not return; [ 3]). We use the cure fraction to model the probability that a customer returns to the platform.[10] Specifically, for customer i, we observe each order j on the platform;  tijC  is the time between j and j + 1. For the cure fraction, we define the latent variable  Rij  = 1 if customer i returns to the platform after order j. The probability that  Rij  = 1 is  p(Rij=1|xij,μi)  , where  μi  is a customer-specific effect, and  xij  is a vector of covariates that contain characteristics of customer i's jth order.[11]The survival function, or the probability that a customer will not order again at least until time  tijC  , is used to derive the hazard, or the likelihood that a customer orders at time  tijC  , given that this customer has not done so yet. Given customer i's repeat order on the platform, this survival function is  SC(tijC|Rij=1,zij,υi)  , where  υi  is the customer effect and  zij  are covariates. The survival function that includes the cure is SC(tijC|xij,zij,μi,υi)=1−p(Rij=1|xij,μi)+p(Rij=1|xij,μi)SC(tijC|Rij=1,zij,υi). Graph( 1)The first two terms in Equation 1 capture the probability that customer i defects; the last term is the probability that customer i returns to the platform but has not placed another order by time  tijC  . The hazard then is  −∂SC(tijC|xij,zij,μi,υi)∂tijC×1SC(tijC|xij,zij,μi,υi)  ([ 3]), or hC(tijC|xij,zij,μi,υi)=p(Rij=1|xij,μi)fC(tijC|Rij=1,zij,υi)1−p(Rij=1|xij,μi)+p(Rij=1|xij,μi)SC(tijC|Rij=1,zij,υi), Graph( 2)where  fC(tijC|Rij=1,zij,υi)  is the probability density function associated with  hC(tijC|Rij=1,zij,υi)  , which is the hazard conditional on customer i returning to the platform.According to Equations 1 and 2, the log-likelihood for the N customers on the customer side of the problem with right-censoring ([25]) is LLCustomer=ln[∏i=1N∏j=1JihC(tijC|xij,zij,μi,υi)γijSC(tijC|xij,zij,μi,υi)]=∑i=1N∑j=1Jiγij[lnp(Rij=1|xij,μi)+lnfC(tijC|Rij=1,zij,υi)]+(1−γij)lnSC(tijC|xij,zij,μi,υi). Graph( 3) Agent sideThere is not a cure with agents, in that platform exploitation means they slow their return rather than abandon the platform. For agent k, we observe each order m placed on the platform; the time between m and m + 1 is  tkmA  . The survival function and hazard for agent k returning to the platform is  SA(tkmA|wkm,ςk)  and  hA(tkmA|wkm,ςk)  , where  ςk  is an agent-specific effect, and  wkm  is a vector of covariates.[12] The log-likelihood function for all K agents is LLAgent=∑k=1K∑m=1MkξkmlnhA(tkmA|wkm,ςk)+lnSA(tkmA|wkm,ςk). Graph( 4) Functional formsAn Expo-power distribution for  hC(tijC|Rij=1,zij,υi)  allows for a flexible hazard.[13] The probability density function and survival functions, given customer i's return, are ([33]) fC(tijC|Rij=1,zij,υi)=λα(tijC)α−1eϕ(tijC)αeλϕ(1−eϕ(tijC)α) Graph( 5)and SC(tijC|Rij=1,zij,υi)=eλϕ(1−eϕ(tijC)α), Graph( 6)where  λ  > 0,  α  > 0, and  ϕ  are the scale, shape, and location parameters. We use a proportional hazard specification in which  λ=ezij′δ+υi  ([33]) and  δ  is a vector of coefficients. We also assume the agent-side hazard follows an Expo-power distribution with associated parameters  αA  ,  ϕA  , and  λA=ewkm′θ+ςk  , where  θ  is the coefficient vector.We use logit to model the cure fraction for customers ([14]). It is akin to a panel logit, in that we include a customer-specific effect to account for unobserved heterogeneity that affects customer retention. Formally, with  β  as the coefficient vector, p(Rij=1|xij,μi)=exij′β+μi1+exij′β+μi. Graph( 7)Finally, we model customer-specific effects,  υi  and  μi  , and the agent-specific effect,  ςk  , as random effects, where  (υiμiςk)∼MVNormal(000,Σ)  , and  Σ=(συ2ρυμσυσμρυςσυσςρυμσυσμσμ2ρμςσμσςρυςσυσςρμςσμσςσς2)  . Random effects address unobserved heterogeneity at the customer and agent levels in their respective equations; failure to do so in multispell hazard models can lead to biased estimates ([ 3]).[14] We allow customer and agent random effects to correlate with each other through  ρυς  and  ρμς  . A positive  ρμς  indicates that unobserved agent characteristics associated with longer (shorter) times between orders also are associated with a higher (lower) probability the customer defects. Variables on both customer and agent sidesFor the cure fraction, we specify xij′β=β1+β2PriorDyadVisitsij+β3AgentQualityij+β4AgentQualityij2+β5AgentTenureij+β6PriorDyadVisitsij×ln(Priceij)+β7AgentQualityij×ln(Priceij)+β8AgentQualityij2×ln(Priceij)+β9AgentTenureij×ln(Priceij)+β10PriorDyadVisitsij×%ServiceRepeatij+β11AgentQualityij×%ServiceRepeatij+β12AgentQualityij2×%ServiceRepeatij+β13AgentTenureij×%ServiceRepeatij+ControlsijC, Graph( 8)where  ControlsijC  consists of the dummy variables discussed previously and the additional controls in Table 3.[15] However, we do not include  PotentiallyStolenCustomers  in the customer equation, because it is an agent side–specific control, as we discuss subsequently.We include the same variables in the customer-side hazard,  zij′δ  , as the customer cure fraction. It provides a stronger test of our hypotheses because the variables are not constrained to affecting only return probability; otherwise, results could be biased. For example, agent quality's true effect may be to decrease the hazard. If we failed to allow for this effect, it might instead manifest as a negative impact on retention and provide undue evidence of platform exploitation.For the hazard on the agent side, we specify wkm′θ=θ1+θ2PriorDyadVisitskm+θ3AgentQualitykm+θ4AgentQualitykm2+θ5AgentTenurekm+θ6PriorDyadVisitskm×ln(Pricekm)+θ7AgentQualitykm×ln(Pricekm)+θ8AgentQualitykm2×ln(Pricekm)+θ9AgentTenurekm×ln(Pricekm)+θ10PriorDyadVisitskm×%CloseCustomerskm+θ11AgentQualitykm×%CloseCustomerskm+θ12AgentQualitykm2×%CloseCustomerskm+θ13AgentTenurekm×%CloseCustomerskm+θ14PriorDyadVisitskm×%RepeatCustomerskm+θ15AgentQualitykm×%RepeatCustomerskm+θ16AgentQualitykm2×%RepeatCustomerskm+θ17AgentTenurekm×%RepeatCustomerskm+ControlskmA, Graph( 9)where  ControlskmA  includes variables in  ControlsijC  ,  PotentiallyStolenCustomerskm  , and its square. Here,  PotentiallyStolenCustomerskm  is the number of customers served by the agent prior to the order who have yet to return to the platform. We include this variable on the agent side because these customers are potentially in the agent's off-platform customer pool. Linear and quadratic terms capture an agent's initial prospecting opportunity and capacity constraint.We estimate the model by summing Equations 3 and 4, using the functional forms in Equations 5–9, and maximizing with respect to all coefficients, including random effect variances and correlations, using STATA's maximum likelihood estimation command.[16] Variables are centered so main effects are interpreted for the other variables at their means. Study 2: Results of Transaction Data AnalysisTable 4 contains information on orders, customers, and agents by month. Orders fluctuate monthly, with some growth toward the end; the monthly percentage change averages 2.6%. Yet each month, 923.1 new customers on average join the platform. Many new customers, coupled with tenuous growth in the number of orders, suggests that relatively few new customers place multiple orders. In addition, note that, on average, 58.42 new agents join each month. Together, new agents and low growth in orders implies that existing agents take fewer orders. While the percentage of existing agents who return to the platform averages 27.9% per month, it declines initially from a high of 56.9%, stabilizes, and then declines again toward the end to finish at 23.5%. Patterns for both customers and agents are consistent with platform exploitation. The platform-level dynamics also hurt profitability, in that acquisition costs are increasing over time, while existing customers and agents are using the platform less.GraphTable 4. Orders, Customers, and Agents by Month. Month/Year7/178/179/1710/1711/1712/171/182/183/184/185/186/18aAvg.Orders1,6891,3431,3001,3141,4021,4201,5359521,4761,5351,9811,6891,470% Δ in Orders from Prior Month−20.5%−3.21.16.71.38.1−38.055.04.029.1−14.72.6%New Customers8538358598468629069525649519591,3921,098923.1% Δ in New Cust. from Prior Month−2.1%2.9−1.51.95.15.1−40.868.6.845.2−21.15.8%% Orders by Existing Cust.42.526.723.225.224.525.625.329.224.626.621.624.226.6New Agents79746042504638324958759858.42% Existing Agents Using Platform56.932.626.427.025.125.225.518.324.925.123.923.527.9 5 aIncludes the first four days of 7/18.6 Notes: New Agents = agents who fill orders for the first time in the month; Existing Agents = agents who filled orders before the month; New Customers = customers ordering for the first time in the month; Existing Customers = customers who placed orders before the month. Customer-Side ResultsTable 5 contains results. We focus on Model 2a, which includes both main and moderator effects. The main effects of prior dyad visits (  β  = −.0867, p <.05) and agent tenure (  β  = −.0006, p <.01) provide strong support for H1a and H3a. Customers are likely to defect when they repeatedly receive service from the same agent or if they interact with longer-tenured agents.GraphTable 5. Probability of Customer Return and Hazard Estimation Results. Probability of Customer ReturnCustomer Return HazardBase 1aModerators 2aH (sign)SupportBase 1bModerators 2bβsSEβsSEδsSEδsSEPriorDyadVisits−.1227***.0335−.0867**.0377H1a (−)Yes−.0073.0076−.0127.0102AgentQuality−.0070*.0040.0081.0058H2a (+)−.0021.0034.0017.0050AgentQuality2−.0003***.0001H2a (−)Yes−.0001.0001AgentTenure−.0007***.0002−.0006***.0002H3a (−)Yes.0001.0001.0001.0002PriorDyadVisits × ln(Price)−.0956**.0426H4a (−)Yes−.0266.0205AgentQuality × ln(Price).0239***.0054H5a (+)−.0072.0044AgentQuality2 × ln(Price)−.0004***.0001H5a (−)Yes.0002**.0001AgentTenure × ln(Price)−.0007**.0003H6a (−)Yes.0001.0002PriorDyadVisits × %ServiceRepeat−.5294***.1543H7a (−)Yes−.0493.0441AgentQuality × %ServiceRepeat.0333*.0193H7b (+).0236.0155AgentQuality2 × %ServiceRepeat−.0006*.0004H7b (−)Yes−.0006**.0003AgentTenure × %ServiceRepeat−.0014*.0008H7c (−)Yes−.0002.0007ln(Price)−.5815***.0854−.4361***.0933−.4835***.0982−.4761***.0960%ServiceRepeat−.1392.4612.0101.4762.7949**.3607.7768**.3708%CloseCustomers−.1533.0943−.1643*.0944−.0706.0779−.0808.0780%RepeatCustomers.3351.2516.3119.2486−.1442.1466−.1485.1435CustomerSameService−.1870***.0449−.1819***.0450.0048.0089.0061.0088CustomerPriorOrders.3041***.0351.2992***.0336−.0042.0084−.0046.0082TimeOnApp.0007***.0002.0007***.0002−.0007***.0003−.0007***.0002AgentSameService−.0018.0031−.0045.0032.0002.0025.0003.0026AgentOrderNum.0005.0011−.0002.0012.0015*.0009.0010.0009AgentHomeVisits.0002.0002.0002.0002−.0001.0001−.0001.0001AgentProExp−.0059.0142−.0069.0143.0092.0103.0091.0102ln(1 + EquipmentCosts).1560***.0213.1544***.0214.1020***.0199.0956***.0196MultiVisit.2345**.1187.1177.1212.0343.1265.0264.1208ln(1+Distance)−.0096.0150−.0165.0151−.0222*.0115−.0232**.0115Scramblea−.2691***.0881−.2884***.0886−.0215.0583−.0447.0596RequestScramblea−.1309.1302−.1355.1308−.0390.0937−.0605.0939Constant–1.4412***.5062–2.0310***.4748–2.0310***.4748–1.9295***.4757Service, City, and Time Fixed Effects✓✓✓✓ α.7272***.0136.7268***.0137 ϕ−.0028.0046−.0002.0046Random Effects Parameters σμ21.6065***.19901.6875***.2013 συ2.7682***.0839.7891***.0816 ρυμ.1731*.0956.1031.0802.1731*.0956.1031.0802 ρμς.7064***.0499.4552***.0462 ρυς.1689**.0750.0469.0619Log-likelihood–74,564.9–74,373.8–74,564.9–74,373.8Wald (p-value)621.68 (.000)710.12 (.000)621.68 (.000)710.12 (.000)N17,63617,63617,63617,636 7 *Significant at the 10% level.8 **Significant at the 5% level.9 ***Significant at the 1% level.10 aRequest is the base category.We find evidence of an inverted U-shaped effect of agent quality on retention, which supports H2a. High agent quality reduces retention because customers are more likely to go off platform with superior agents. However, low agent quality is likely associated with poor service, which also reduces retention. The coefficient on  AgentQuality2  is negative (  β  = −.0003, p <.01), and the linear term is insignificant, so the effect of agent quality is close to a maximum at variable averages: an average-quality agent is best for retention for average-priced services with average repetitiveness. Deviations from average quality reduce retention. In summary, the main effects of the key independent variables are consistent with platform exploitation and support their hypotheses.Turning to moderators, we expect the impact of prior dyad visits, agent quality, and agent tenure to be more pronounced at higher prices. After finding a good match, both parties have a stronger motivation to move the relationship off platform when the financial incentive is larger. Note that all interactions with  ln(Price)  are significant. The impact of prior dyad visits (  β  = −.0956, p <.05), the inverted U-shaped effect of agent quality (  β  =.0239, p <.01;  β  = −.0004, p <.01), and the effect of agent tenure (  β  = −.0007, p <.05) all are amplified at higher prices, in support of H4a, H5a, and H6a. We illustrate the interaction of  AgentQuality  at different values of  ln(Price)  in Figure 3, revealing the strongest inverted U-shaped effect of agent quality on retention at high prices, whereas it does not hold at the lowest price. The results for low agent quality imply that customer dissatisfaction from working with an inferior agent is amplified at higher prices; for high agent quality, the financial incentive of higher prices increases the likelihood of going off platform. Figures W1A and W1B in the Web Appendix depict the predicted probability of customer return by  PriorDyadVisits  and  AgentTenure  at different values of  ln(Price)  .Graph: Figure 3. Probability of customer return to platform by agent quality for different order prices.We hypothesize that once a customer finds a good agent, they should be more inclined to go off platform when they require repeat services (  %ServiceRepeat  ). The results show that as services become more repetitive, the negative impact of prior dyad visits increases (  β  = −.5294, p <.01; H7a); the inverted U-shaped effect of agent quality is marginally more pronounced (  β  =.0333, p <.10;  β  = −.0006, p <.10; H7b); and the effect of agent tenure is marginally greater (  β  = −.0014, p <.10; H7c). See Figures W2A–W2C in the Web Appendix.In summary, results for the probability of customer return provide strong evidence of platform exploitation. As additional evidence, we consider the customer return hazard. Platform exploitation suggests that our key variables of interest should have limited influence on the time between orders for customers who come back to the platform. With Moderators 2b, we examine the cumulative probability of customer return to the platform by day, following their most recent order (conditional on retention). It represents the (conditional) failure rate in hazard terminology. Agent quality, agent tenure, prior dyad visits, and most of the interactions with price and service repetitiveness have little influence. Rather, the time between orders depends mostly on service characteristics: restricting service dummy effects jointly to 0 in the hazard yields  χ2  = 210.86, with p <.01. Platform exploitation affects retention rather than time between orders for patients. Agent-Side ResultsTable 6 presents the agent-side results. Platform exploitation suggests that the main variables which decrease customer retention will also decrease agent return rate. For the main-effect hypotheses, the impact of prior dyad visits, agent quality, and agent tenure on agent return rate should mirror customer side results. We do not find support for H1b, as the main effect of prior dyadic transactions is insignificant (  θ  =.0046, p >.10). However, the significant effects of agent quality (  θ  =.0380, p <.01) and its square (  θ  = −.0012, p <.01) support H2b; the negative and significant impact of agent tenure (  θ  = −.0010, p <.01) supports H3b. Customers are more likely to defect with high-quality or long-tenured agents; these agents are slower to return to the platform as their private customer base grows.GraphTable 6. Hazard Estimation Results for Agent's Return to the Platform. Nurse Return HazardBase 1cModerators 2cH (sign)SupportθsSEθsSEPriorDyadVisits−.0094**.0046.0046.0068H1b (−)NoAgentQuality−.0060***.0016.0380***.0027H2b (+)AgentQuality2−.0012***.00003H2b (−)YesAgentTenure−.0010***.0001−.0010***.0001H3b (−)YesPriorDyadVisits × ln(Price)−.0014.0100H4b (−)NoAgentQuality × ln(Price).0051***.0016H5b (+)AgentQuality2 × ln(Price)−.0001*.00003H5b (−)YesAgentTenure × ln(Price)−.00014*.00008H6b (−)YesPriorDyadVisits × %CloseCustomers−.0342**.0155H8a (−)YesAgentQuality × %CloseCustomers.0163***.0043H8b (+)AgentQuality2 × %CloseCustomers−.0002***.0001H8b (−)YesAgentTenure × %CloseCustomers−.0002*.0001H8c (−)YesPriorDyadVisits × %RepeatCustomers−.0513***.0189H9a (−)YesAgentQuality × %RepeatCustomers.0016.0097H9b (+)AgentQuality2 × %RepeatCustomers−.0001.0002H9b (−)NoAgentTenure × %RepeatCustomers−.0012***.0004H9c (−)YesPotentiallyStolenCustomers.0094***.0013.0133***.0018PotentiallyStolenCustomers2−.00002***.00001ln(Price)−.2688***.0279−.2062***.0298%ServiceRepeat−.6499***.1264−.5928***.1258%CloseCustomers−.0249.0321.0752*.0438%RepeatCustomers−.4363***.0733−.3369***.0963CustomerSameService.0003.0049.0018.0050CustomerPriorOrders.0025.0045.0009.0046TimeOnApp−.0001**.0001−.0001**.0001AgentSameService−.0001.0008−.0012.0008AgentOrderNum−.0052***.0009−.0056***.0010AgentHomeVisits.0030***.0001.0026***.0001AgentProExp−.0308***.0051−.0151**.0074ln(1 + EquipmentCosts).0370***.0063.0321***.0064MultiVisit.2156***.0385.1483***.0395ln(1+Distance).0355***.0050.0338***.0050Scramblea−.1259***.0266−.1198***.0271RequestScramblea−.1530***.0432−.1515***.0434Constant–2.7685***.2244–2.1736***.2168Service, City, and Time Fixed Effects✓✓ αA.7556***.0053.7573***.0053 ϕA−.0309***.0012−.0311***.0012Random Effects Parameters σς2.5864***.0259.4438***.0262 ρμς.7064***.0499.4552***.0462 ρυς.1689**.0750.0469.0619Log-likelihood–74,564.9–74,373.8Wald (p-value)621.68 (.000)710.12 (.000)N17,63617,636 11 *Significant at the 10% level.12 **Significant at the 5% level.13 ***Significant at the 1% level.14 aRequest is the base category.Similar to the customer side, we hypothesize that the impact of the main variables should be magnified at higher prices. The motivation for both parties to move off platform with a good match is stronger when the financial incentive is larger. We do not find support for H4b, as  ln(Price)  ×  PriorDyadVisits  is insignificant (  θ  = −.0014, p >.10). However, price interactions with agent quality (  θ  =.0051, p <.01;  θ  = −.0001, p <.10) and agent tenure (  θ  = −.00014, p <.10) support H5b and offer marginal support for H6b. At higher prices, high-quality or long-tenured agents are in a better position to move customers off platform. To gauge effect sizes, Figures W3A and W3B in the Web Appendix depict the return rate in the seven days since the last on-platform order by  AgentQuality  and  AgentTenure  at different prices. Although the moderating effect of price on agent quality is statistically significant, it is relatively small. The inverted U-shaped effect is relatively stable, though it shifts down as price increases. In this example, the negative main effect of price on return rate dominates the moderation effect. Results are similar for agent tenure (i.e., significant but relatively small impact).In contrast, characteristics of the agent's on-platform customer pool have greater moderating effects. Agents have more exploitation opportunities when they interact with a relatively larger pool of desirable customers on the platform. These characteristics likely strengthen the impact of the main variables because customers only leave the platform with agents that are a good match.For example, agents prefer working with customers who are geographically close. There are more opportunities to take customers off platform when an agent serves a greater percentage of geographically close customers in their on-platform customer pool. The interactions of  %CloseCustomers  with prior dyad visits (  θ  = −.0342, p <.05), agent quality (  θ  =.0163, p <.01;  θ  = −.0002, p <.01), and agent tenure (  θ  = −.0002, p <.10) are significant (marginal for agent tenure) with expected signs, in support of H8a–H8c. Figures W4A–W4C in the Web Appendix show that the moderating effects are relatively large. As  %CloseCustomers  increases, the impact of  PriorDyadVisits  goes from positive to negative, the inverted U-shaped effect of agent quality is more distinct, and the effect of agent tenure is greater. Agents with a larger percentage of close customers in their on-platform customer pool are better able to leverage exploitation opportunities from more prior dyad visits, higher agent quality, and longer agent tenure.Agents also prefer working with repeat customers. An agent with a larger percentage of repeat customers in their on-platform customer pool has more opportunities to take customers off platform. We find that  %RepeatCustomers  negatively moderates the impact of  PriorDyadVisits  (  θ  = −.0513, p <.01) and  AgentTenure  (  θ  = −.0012, p <.01), in support of H9a and H9c. However, we do not find support for H9b, as the interactions with the agent quality variables are insignificant. Figures W5A and W5B in the Web Appendix show that the significant moderation effects of  %RepeatCustomers  are also relatively large. Agents with a larger percentage of repeat on-platform customers are in a better position to capitalize on prior dyad visit or agent tenure related exploitation opportunities.Lastly, customer- and agent-specific random effects are significant in Tables 5 and 6. We find a positive correlation between agent and customer effects in the cure fraction (  ρμς  =.4552, p <.01), which suggests that customers are less likely to return when they interact with agents who come back to the platform less frequently, which is consistent with platform exploitation. Dyad-Level Robustness CheckOur theorizing generates distinct hypotheses for agents and customers, with their different motivations, in support for our approach of modeling them separately. However, the main-effect hypotheses and price interactions are congruent for both sides. Thus, we model returns at the customer–agent dyad level as a robustness check. Because each dyad contains a specific customer and a specific agent, it is not observed again if the customer defects, so customer-side moderators could influence the dyads. In contrast, agents try to build portfolios of offline customers and are unlikely to leave the platform completely, so we do not rely on the dyadic analysis to provide insights into how quickly agents return to serve additional customers.The dyad model and results appear in Web Appendix, Table W7; they offer broad support for the main-effect hypotheses, price interactions, and most customer-side interactions. The results for the agent-side interactions are mixed, which is not surprising, as we indicated that these moderators are unlikely to influence both parties. In summary, the results are largely robust for dyads. Testing Intervention Strategies with Scenario-Based ExperimentsAfter documenting the existence and nature of platform exploitation, we nextinvestigate ways to potentially reduce it. We conduct two scenario-based experiments using a fictional platform that connects agents with dog-owning customers (DogGo; similar to Rover or Wag). We test a financial intervention that alters the incentive structure and a social intervention that aims to build agent–platform community. We screened respondents from Amazon Mechanical Turk to ensure that offering dog walking services for pay is something they would consider (so that the respondents' mindsets are similar to those held by real platform users). If not, they were eliminated from the study. Financial Intervention: Sliding-Scale Fee (Study 3a)The nurses in Study 1 indicated that reduced fees would decrease their motivation to defect with customers and lead them to pursue more orders on the platform. Thus, we test a sliding-scale fee, such that the platform's commission decreases with more on-platform services. DesignFor the independent variables, we manipulated customer–agent relationship quality (RQ) and the platform's fee structure. We assigned 324 participants randomly to a 2 (RQ: high vs. low) × 2 (fee structure: fixed percentage vs. sliding scale) factorial design. The scenario explained the platform rules and asked the participants to imagine being an agent on the platform (Web Appendix, Figure W6A). In the high-RQ condition, agents read that they had seen the customer several times and were familiar with their expectations. In the low-RQ condition, they instead read that they had not seen the customer before and were unsure of their expectations. Next, in the fixed-fee condition, agents learned that the platform commission is 30% of the transaction price, whereas the sliding-scale condition explained that when agents complete more orders per week, the commission rate per transaction decreases. We measured off-platform transaction intentions by asking respondents how likely they were to suggest future transactions off platform with the customer (seven-point scale; 1 = ""extremely unlikely,"" and 7 = ""extremely likely"").[17] We concluded with manipulation checks. ResultsFor our manipulations, respondents in the high-RQ (vs. low-RQ) condition rated trust with customers higher (Mhigh = 5.7, Mlow = 4.0; t(320) = 10.77, p <.01). The percentage of participants who correctly identified the fee policy was high in both the sliding-scale (P = 94.0%,  χ2  ( 2) = 250.90, p <.01) and fixed-fee (P = 80.9%,  χ2  ( 2) = 177.02, p <.01) conditions.An analysis of variance for off-platform transaction intentions reveals main effects of both RQ (F( 1, 320) = 16.27, p <.01) and fee structure (F( 1, 320) = 6.59, p <.05), as well as an interaction effect (F( 1, 320) = 4.04, p <.05) (Web Appendix, Figure W7A). Planned contrasts show that in the fixed-fee model, the findings match those from our main studies, in that participants indicate greater intentions to take customers off platform in the high-RQ (vs. low-RQ) condition (Mhigh = 4.95, Mlow = 3.66; t(320) = 4.43, p <.01). However, the sliding-scale fee mitigates this RQ effect, such that high-RQ no longer leads to higher off-platform intentions (Mhigh = 3.97, Mlow = 3.54; t(320) = 1.39, p =.17). Overall, the results suggest that a sliding-scale fee is a promising intervention, effective for counterbalancing the effect of high relationship quality. Nonfinancial Intervention: Service Agent Community Building (Study 3b)Financial interventions can be costly, so we also test whether platforms might leverage social mechanisms to decrease platform exploitation. Because organizational commitment can be enhanced through employee community building and socialization ([ 6]), we test the effectiveness of a program that enables platform service agents to build stronger connections with other platform-affiliated agents and with the platform itself. DesignIn this experiment, we manipulated customer–agent relationship quality and the presence of a community program. The 331 participants were randomly assigned to a 2 (RQ: high vs. low) × 2 (community program: yes vs. no) factorial design. The context and procedures are similar to those in the previous experiment, except that we manipulated the presence of a community program, such that participants read about the benefits (community status and offline social events) that could be earned with a certain number of platform orders in the program condition (Web Appendix, Figure W6B) but received no such information in the no-program condition. ResultsThe manipulations were successful; participants in the high-RQ (vs. low-RQ) condition rated their trust with customers higher (Mhigh = 5.4, Mlow = 3.7, t(327) = 10.96, p <.01). The percentage of subjects who correctly identified the presence of a community program was higher in both the community (P = 82.1%,  χ2  ( 2) = 156.01, p <.01) and no-community (P = 78.5%,  χ2  ( 2) = 173.32, p <.01) program conditions.The analysis of variance for off-platform transaction intentions revealed a main effect of RQ (F( 1, 327) = 22.90, p <.01), replicating the results from the first experiment (Web Appendix, Figure W7B). The main effect of the community program significantly reduces off-platform intentions (F( 1, 327) = 5.77, p <.05). The lack of interaction effect (F( 1, 327) =.33, p =.56) indicates that the presence of the community program does not specifically mitigate the effects of high RQ, but that the intervention is effective regardless of the level of RQ (high RQ: Mno community = 4.60, Mcommunity = 3.96; low RQ: Mno community = 3.45, Mcommunity = 3.05). Discussion of ExperimentsThe findings from these two experiments corroborate our theorizing. In a trusting relationship, agents are more likely to move future transactions with that customer off platform, but in low-quality relationships, the parties prefer the relative safety of the platform. We demonstrate this effect using a distinct service setting (dog walking), which suggests the generalizability of our findings from Studies 1 and 2.The experiments also provide insights for how managers might reduce platform exploitation. First, a sliding-scale fee increases the likelihood that agents keep their future transactions on the platform. Such a policy appears most effective for high-quality relationships, for which the other benefits of using the platform are lower. Without an offsetting financial incentive, the agent is motivated to move off platform. Second, a program that builds a sense of community also can reduce the likelihood of platform exploitation, affecting both high- and low-RQ conditions. Regardless of trust levels, community building can enhance agents' loyalty to the platform. General DiscussionGrowing the base of users is critical for platform businesses ([11]), but defection by agents and customers remains an ongoing problem. Little scholarly research has addressed this issue. With this article, we establish the existence and nature of platform exploitation, isolate key drivers, and provide potential remedies. Managerial ImplicationsWe highlight four implications for managers of online service platforms. First, high-quality agents and those with longer platform tenure are more likely to leave. Ironically, platforms need high supply-side quality to keep customers ([13]), but doing so increases the risk of exploitation. Managers of platforms on which high-quality service prompts interpersonal trust must recognize that high-quality agents are double-edged swords in business models based on a fixed percentage-of-price fee. In addition to a sliding-scale fee, managers might segment agents on the basis of quality and tenure and design different incentive strategies to encourage on-platform transactions. Furthermore, subscriptions or flat-fee models might be designed to reduce customer incentives to move off platform, even if an agent seeks to do so.Second, managers should regularly emphasize the benefits of staying on-platform for customers and seek to enhance those benefits; repeat customers are essential for platform profitability. But it would be a mistake to assume that customers appreciate all platform benefits. Existing customers, particularly those who consume high-priced or repetitive services, may need as much attention as new customers. Thus, managers could segment customers according to their consumption patterns and apply different messaging and incentive strategies to these segments (e.g., premium benefits such as guarantees, insurance, special platform features; [28]).Third, high-quality, long-tenured agents who defect and take customers with them are less available for on-platform services. The situation worsens when they interact with susceptible (e.g., repeat, proximal) customer pools. Managers thus might need to reimagine the agent–platform relationship. Initially, they are codependent: the platform relies on agents to fill orders, and agents rely on the platform to access customers. However, agents become less dependent as they meet more customers. To strengthen the relationships, platforms might offer usage incentives or even consider directly employing agents, though our interviews and anecdotal evidence (e.g., Amazon Home Services; [18]) suggest mixed results of direct employment experiments.Fourth, human needs for recognition and social belonging can be leveraged; in the program we tested, offering special status markers and opportunities to socialize with other members of the platform community reduced the likelihood that agents tried to take customers off platform. Perhaps other agent socialization approaches could be effective too, such as encouraging mentor–mentee relationships. Theoretical ImplicationsVery little research has discussed platform exploitation, a gap with important implications for building theory about relationships between platforms and their ecosystem members (e.g., service agents) ([13]). Previous platform and two-sided market research take a perspective that largely emphasizes the symbiotic relationship of the two sides—for example, how ecosystem members and platforms help each other ([20]; [26]). However, our research introduces a perspective that stresses the friction between the two. Through theories-in-use and secondary data analysis, we show how platform-dependent service agents can knowingly break platform rules and retain customers for themselves. Our results suggest that platforms experience financial harm from exploitation. Further, while previous research emphasizes how the two sides of a platform market are reliant on each other, our results suggest that platform exploitation may decrease service agents' dependence on the platform.Our research also contributes to platform literature by examining pure-labor platforms and adding nuances to findings in extant research. Most of the platform research focuses on physical products, product sharing (e.g., Airbnb), or product-based service (e.g., Uber) platforms. For example, studies of product transactions indicate that high supply-side quality enhances platform growth ([20]). We clarify that in a service setting, quality helps ensure early attraction and retention but also increases the likelihood of defection. Scholars might test other findings generated in product contexts to see if new insights emerge in service contexts.Finally, in studies of the negative consequences of close customer–agent relationships ([ 8]; [17]) and employee turnover ([ 7]), the focus is often employees of traditional firms, rather than agents with substantial options to behave opportunistically. This distinction is important. Online service markets are two-sided platforms, so managers need to build a customer base by increasing supply-side quality, but when they do so, they also risk increasing platform exploitation. We reveal the dark side of customer relationships in a context where these effects might not be readily apparent. Limitations and Further ResearchFurther research might address limitations of our study. The study context is a typical service platform (see Table 2), but managers should take care in extrapolating findings across settings. Our results likely generalize to platforms that offer services rendered in close proximity, involve commission-based pricing, rely on the agent's skill, and involve customer–agent coordination. It would be helpful to test the robustness of our findings in other contexts, such as home services (e.g., TaskRabbit) or beauty care (e.g., Stylebee). Furthermore, we consider conditions that make platform exploitation more severe and use two experiments to test mechanisms to combat it. Managers also might leverage other moderators, which further research can identify and test, perhaps in field experiments that analyze whether directly employing agents or altering fee structures reduce defection in reality. Finally, even as pure-labor platforms grow more common, we lack detailed insights into the challenges of managing the dual agent–platform relationship (cooperative and competitive) and the long-term implications of platform exploitation. Thus, service platform research offers many promising avenues for scholars. "
33,"Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" Campaign The unprecedented number of product recalls in recent years and subsequent low consumer recall compliance raise questions about the role of regulatory agencies in ensuring safety. In this study, the authors develop a conceptual framework to test the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The empirical context is the launch of a nationwide DMC by the U.S. automobile industry's regulator. The analysis utilizes recall completion data from 296 product recalls active both before and after the DMC's launch. The results show that the DMC improves consumer recall compliance. In the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC. Regarding boundary conditions, the study finds that the DMC is more effective for recall campaigns with greater media coverage and for those with older recalled products. However, the DMC's effect weakens as the time needed to repair a defective component increases. The findings should help regulators make compelling cases for greater resource allocation toward digital initiatives to improve recall compliance.Keywords: digital marketing campaign; product recalls; public policy; regulationProduct recalls and consumer safety are regulated by government agencies in several countries, including the United States, Canada, Germany, the United Kingdom, and Japan. While issuing a recall notice to inform consumers of a potential issue is critical, far too many defective products remain unremedied long after their recall notifications have been sent to consumers. For instance, J.D. Power reports that consumer recall compliance in the U.S. automobile industry is quite low, with approximately 30%–50% of vehicles on the road at any point in time having an unrepaired safety problem (J.D. [54]). Relatedly, according to a report by the Consumer Product Safety Commission (CPSC), only 10% of children's products recalled in 2012 were successfully corrected, replaced, or returned ([44]). For example, since the 2016 recall of millions of IKEA Malm dressers, only 1% of consumers, at best, have had the unstable furniture removed and been issued a refund (Consumers [19]).The continued use of defective products by consumers is a serious public health concern and raises the specter of injuries and fatalities. The sustained string of casualties in 2018 due to faulty Takata airbags, a safety problem for which a recall was issued in late 2014, underscores the perils of low consumer recall compliance. Unsurprisingly, regulatory agencies have been subjected to intense scrutiny and even rebuke for not taking adequate measures to improve consumer recall compliance. For example, an audit report released in 2018 by the U.S. Department of Transportation faults the National Highway Traffic Safety Administration (NHTSA) for a lack of proper oversight of the recall completion process ([52]).While numerous factors likely contribute to poor consumer recall compliance, low consumer awareness is often noted as a key issue responsible for inaction. In fact, the NHTSA's former Associate Administrator for Enforcement stated that a lack of public knowledge is ""the single greatest weakness"" in successfully addressing product recalls ([31], p. 232). Similarly, NHTSA focus group interviews conducted to understand the reasons for low recall compliance found that while over 70% of consumers preferred electronic recall notifications, only 7.4% reported receiving any. Notably, 90% of the respondents mentioned that recall notifications received electronically had a greater chance of being noticed ([28]). In the past, regulatory agencies such as the CPSC have conducted targeted national campaigns to raise public awareness, support industry compliance, and improve safety in specific consumer product categories ([20]). The issue of low consumer recall awareness is so pervasive that in 2014, the CPSC tried to crowdsource solutions, announcing a contest for application developers to create tools to inform the public of consumer product recalls ([26]).Although regulators introduce such initiatives to raise consumer awareness and reduce accidents, empirical evidence to support or refute these expectations is inconsistent. A potential concern is that campaigns by regulatory agencies might not be effective policy instruments if consumers view them as symbolic acts by government officials and are unresponsive to these efforts ([23]; [32]). In addition, prior research has found that consumers could develop reactance to regulation-related public awareness campaigns because they view such efforts as infringements on their personal freedoms ([15]; [47]). This contention is supported by popular press: a 2018 survey of over 1,500 U.S. consumers revealed that almost two-thirds of them did not believe that government-initiated product recall programs had much to do with increasing consumer safety. Instead, many consumers viewed the recall programs as government exercises in ""red tape"" ([62]).The study's objective is to investigate the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The manuscript makes two contributions to marketing literature. First, this study is the only one that we are aware of that examines whether a DMC initiated by a regulator improves consumer recall compliance. The product recall literature has predominantly focused on the stock market and sales consequences of recalls (e.g., [13]; [29]), the ability to learn from and prevent future recalls ([33]; [37]), the drivers and consequences of recall timing decisions ([25]), and how recalls can impact marketing effectiveness ([16]; [65]). Little attention has been devoted to the consumer compliance process. A couple of studies have also investigated how recall attributes, including vehicle country of origin, vehicle age, and the publicity around recalls, influence compliance ([34]; [58]). However, the impact of a regulator's efforts on consumer compliance has, to our knowledge, received virtually no attention. Our study fills this research void and offers valuable insights to policy makers.Specifically, we examine the effectiveness of a DMC initiated by a regulatory agency. To tackle the problem of low consumer recall compliance, the NHTSA, the agency responsible for regulating consumer safety in the U.S. automobile industry, launched a full-coverage, nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. The DMC featured paid search and online display advertisements that provided consumers with links to check for open recalls and access pertinent recall remedy information online. We exploit this setting to formally test the effectiveness of a regulator-initiated DMC to improve consumer recall compliance. The empirical analysis utilizes recall completion data pertaining to 296 product recalls active both before and after the DMC's launch. Our econometric analyses account for various potential confounds such as the DMC's possible endogenous nature, time trends in recall completion, recall campaign attributes, and unobserved vehicle make and temporal characteristics. The results show that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what was to be expected without the DMC. This improvement in consumer recall compliance should, in turn, lead to potentially fewer vehicle crashes, casualties, and lower economic costs.Second, we identify important boundary conditions for the DMC's effectiveness. We find that the DMC is more effective at increasing consumer compliance for recalls with greater media coverage. Although media coverage of recalls could be detrimental to the impacted brand's financial health, our finding implies that it plays a critical role in aiding the DMC to improve compliance. We also find that the DMC is more effective at increasing compliance for older (as opposed to newer) recalled products. This finding is critical for regulators who struggle to reach owners of older products using conventional communication methods. However, the DMC's impact on compliance is lower for recalls in which the defective component takes a longer time to repair. This suggests that the DMC may be unable to fully counteract the barrier of time-related inconvenience for consumers. Collectively, our findings enable regulators to make compelling cases to receive more resources for digital marketing initiatives in the future. Literature Review and Conceptual FrameworkRegulatory agencies introduce interventions with the goal of promoting and protecting consumer welfare. Government interventions supporting public welfare are comprised of pecuniary interventions (e.g., imposing higher taxes/penalties, offering financial incentives) as well as nonpecuniary interventions (e.g., focused on educating, building awareness) ([45]). Prior research has investigated the efficacy of interventions targeted at decreasing the consumption of unhealthy products, stimulating consumer spending, and promoting preventive health screenings ([18]; [59]; [66]). The effectiveness of interventions is often assessed by examining the extent to which consumers comply with or respond to the proposed initiatives.Prior research provides equivocal evidence regarding the effectiveness of pecuniary interventions. For example, although imposing higher taxes has been found to lower purchases of bottled water, reduce purchases of drinks with sugar additives, and decrease smoking prevalence, their associated unintended consequences may drive consumers toward more dangerous products ([ 6]; [18]; [67]). Relatedly, the ""Click It or Ticket"" campaign, intended to improve seat belt usage in vehicles and promote consumer safety by ticketing transgressions, has been shown to be successful ([61]). However, programs that offer consumers incentives to stimulate sales through initiatives such as ""Cash for Clunkers"" ([48]) or drive consumer spending through tax rebates ([59]) have been found to be generally ineffective.Likewise, evidence from prior research assessing the efficacy of nonpecuniary interventions is also inconsistent. For example, the impact of the Nutrition Labeling and Education Act (NLEA) in altering consumers' behaviors is somewhat nebulous. [49] found that the NLEA significantly increased consumers' nutrition information processing and comprehension, although information-sensitive consumers did not report increased use of nutrition information following its implementation. Relatedly, [ 3] did not find any change in consumers' consumption-related search and recall of nutrition information after the NLEA's enactment. [55] summarizes the effects of government agency-initiated public information campaigns across various contexts and even documents consumer responses that are opposite to those intended in some situations.The mixed evidence for the efficacy of regulator-initiated interventions (those pecuniary and nonpecuniary) documented in prior research is not satisfactory from a knowledge advancement perspective and raises important questions about the potential effectiveness of a regulator-initiated DMC. More specifically, is a regulator-initiated DMC effective in improving consumer recall compliance? Under what conditions is a DMC more or less effective?To answer these questions, we develop a conceptual framework (depicted in Figure 1) by drawing on insights from both the health belief model (HBM) and health warning streams of research.[ 6] With conceptual origins in the public health domain, the HBM contends that for individuals to take action to prevent a detrimental outcome, they need ( 1) a cue or trigger (internal and/or external) to overcome their avoidance tendencies, ( 2) to believe that they are susceptible to its risk, ( 3) to perceive that the occurrence of the outcome would have negative consequences for them, and ( 4) to believe that taking preventative action would lead to benefits outweighing the barriers of cost, convenience, pain, and embarrassment ([11]; [36]; [56]). Drawing on these insights, our framework examines the roles of ( 1) awareness cues, ( 2) the perceived susceptibility to risk, ( 3) the perceived threat of noncompliance, and ( 4) the perceived time inconvenience of compliance in eliciting consumer recall compliance following a regulator-initiated DMC.Graph: Figure 1. A conceptual model of the impact of a regulator-initiated digital marketing campaign on consumer recall compliance.Our framework also draws on health warning streams of research, specifically, prior work on consumer responses to persuasive public health appeals. This stream of research suggests that increasing the frequency of health warnings might desensitize consumers to the potential hazard, resulting in either inaction or actions that are opposite to the intended warning ([55]; [63]). In our context, government regulators, retailers, manufacturers, and consumer experts are concerned that product recalls have become so frequent in various industries, including food, consumer products, and automobiles, that consumers might suffer from ""recall fatigue"" ([39]). Therefore, our framework also considers the impact of multiple concurrent health warnings on a consumer's compliance with a regulator's request.Given that our goal is to investigate a DMC's effectiveness, we conceptualize a regulator-initiated DMC as the primary awareness cue intended to improve consumer recall compliance. The ability of the DMC to improve compliance depends on the extent to which consumer recall awareness is generated and the extent to which consumers are motivated to comply. Accordingly, we expect that the DMC's impact on compliance is contingent on four factors identified in the HBM and the presence of multiple concurrent health warnings for consumers. We propose that a DMC's effectiveness is moderated by ( 1) media coverage, ( 2) the age of the recalled product, ( 3) component hazard, ( 4) time for repair, and ( 5) concurrent recall campaigns.A DMC improves consumer recall compliance by providing relevant information to consumers impacted by recalls. These consumers could be unaware that their products have been recalled but are actively searching for recall-related information, benefiting from a DMC's paid search advertisements that make them aware of existing recalls and direct them to relevant information. Other consumers may be exposed to a DMC's online display advertisements and then cued to check if their products have been recalled on the regulator's webpage.In addition to a DMC providing the necessary information to aid consumers in complying with a recall request, media coverage of recalls could heighten a DMC's effectiveness by sensitizing consumers to the fact that their products may have defects and thereby influencing them to comply. Accordingly, we include media coverage (akin to another awareness cue, a factor in the HBM) as a moderator of a DMC's impact on consumer recall compliance.Because owners of older (vs. newer) products are less likely to receive information about recalls directly from their products' manufacturers, they are likely cognizant of their greater susceptibility to recall risk and more responsive to a DMC that provides relevant information. Accordingly, we include the age of the recalled product (akin to the perceived susceptibility to risk, a factor in the HBM) as a moderator of a DMC's impact on compliance.Following a DMC, a consumer learns about the hazard of a defective component, the time it will take to repair it, and the existence of concurrent recall campaigns for the product after being made aware that it has been recalled. As such, these factors are pertinent in explaining a consumer's motivation to comply with a recall request after being made aware of it through a DMC. Accordingly, we include component hazard (akin to the perceived threat of noncompliance, a factor in the HBM), time for repair (akin to the perceived time inconvenience of compliance, a factor in the HBM), and the presence of concurrent recall campaigns (akin to multiple concurrent health warnings) as moderators of the DMC's impact on consumer compliance ([36]; [55]; [63]). Research HypothesesPrior research suggests that marketing campaigns can be effective at improving individuals' compliance-related behaviors. For example, [66] showed that the introduction of a mass media campaign was associated with increased consumer information search, displayed by high click-through rates to a website and more referrals to medical centers. Relatedly, [69] found that patients were more likely to obtain a first prescription of a treatment following a direct-to-consumer advertising campaign designed to educate them and provide resources about health-related decisions. The positive impact of advertising initiatives by firms may even spill over to the category level, as [68] noted that advertising by a brand increased consumer drug therapy compliance of its rival brands' products. However, the positive effects of awareness campaigns documented in prior research are primarily the result of initiatives by firms, and not by regulators. One notable exception is work by [32], which found a positive effect from a government-initiated proenvironmental demarketing campaign to reduce water consumption. However, the campaign was more effective on the majority ethnic/religious group than on the minority groups.There are also reasons to believe that a marketing campaign may not be as effective if initiated by a regulator. For example, [53] found that fewer than half of the antismoking advertisements they showed to adolescents increased their nonsmoking intentions. Similarly, research examining the National Youth Anti-Drug Media Campaign in the United States reported no effects from the initiative ([35]). Although consumers may be skeptical about government-initiated awareness campaigns related to health or safety issues, the preponderance of evidence for these campaigns' positive effects leads us to our baseline hypothesis that a DMC will improve consumer recall compliance. H1:  A regulator-initiated DMC improves consumer recall compliance. The Moderating Role of Media CoverageThe effectiveness of a regulator-initiated DMC is likely to vary depending on whether a recall received media coverage. Mass media outlets are a unique voice in the marketplace that provide consumers with information that is different from that of firms or governmental agencies ([14]). Extant research argues that separate media channels could have a synergistic impact on the effectiveness of multichannel marketing campaigns ([12]; [50]). Importantly, in addition to the repetition of the message, the variation of the specificity of the message also likely affects consumers. Although both media coverage and a regulator-initiated DMC generate awareness about a recall, they also vary in the information they provide. In our context, a regulator-initiated DMC provides the specific information consumers need to determine whether they are personally affected by a recall and how to address it, whereas media coverage increases consumer awareness of a recall. We thus argue that consumers are more likely to comply with a recall notice if the information provided by a DMC is complemented by awareness generated through media coverage ([41]). This argument is also consistent with research suggesting that advertising is more effective when there is more associated publicity following a product-harm crisis ([16]; [21]). Drawing on these arguments, we hypothesize: H2:  Media coverage positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for recall campaigns with greater media coverage. The Moderating Role of Age of Recalled ProductConsumers are, in general, more informed of product-related issues (e.g., defects, failures) when they interact with original equipment manufacturers (OEMs) during their products' warranty periods. As products age and their warranties expire, consumers' interactions with OEMs tend to be less frequent. For example, [58] propose that owners of older products are less likely to schedule routine product maintenance. In addition, the likelihood that product ownership could change hands increases as products age, reducing the probability that notifications sent from the OEM will reach the product's present owner ([ 2]; [27]). These decreased interactions with OEMs leave consumers who own older products at greater risk of being unaware that their products are the subject of active recalls. A regulator-initiated DMC can fill this communication gap for owners of older products, allowing them to receive recall-related information through paid search and online display ads. For these reasons, we expect consumers with older products, who likely perceive themselves as more susceptible to recall-related risk, to utilize a DMC's information to a greater extent than consumers owning newer products. Accordingly, we hypothesize that a DMC's impact on consumer recall compliance will be stronger for older (vs. newer) recalled products. H3:  The age of the recalled product positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger when the recalled product is older. The Moderating Role of Component HazardThe impact of a DMC on consumer recall compliance is also likely to vary depending on consumers' perceptions of the threat of not complying with a recall request. For example, [40] finds that a consumer's perception of threat is the most influential determinant of whether they seek medical care services. Similarly, consumers are more likely to comply with treatment regimens when they perceive that noncompliance would threaten their well-being ([11]). Relatedly, vehicle owners are more likely to repair products when the defective component's hazard is higher. [34] provide evidence that consumers remedy more severe defects at higher rates than less severe defects. In general, after a DMC makes consumers aware of a recall and the defective component involved, consumers should be more motivated to comply with a recall request if they perceive a high threat of noncompliance. If a defective component's perceived hazard is low, consumers are less likely to be motivated to comply with a recall request. Conversely, if the perceived hazard of a component is high, consumers are more likely to be motivated to comply. Therefore: H4:  Component hazard positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for high hazard components. The Moderating Role of Time for RepairThe effectiveness of a DMC also depends on the extent to which consumers perceive recall compliance to be convenient. Time-related costs have been shown to be a significant barrier to achieving desired levels of compliance from individuals following a request ([10]; [36]). In addition to providing other relevant information, a DMC informs consumers of the time needed to repair their products. Prior research has found that consumers perceive compliance to be more convenient if the time it takes to complete a task is shorter. For example, [ 4] find that consumers who view recycling as more convenient are more motivated to recycle; consumers weigh the costs of sacrificing time to separate recyclable items and then dispose of them if they are under the threshold of inconvenience. Similarly, [11] find time costs to be better predictors of compliance than monetary costs. Likewise, we argue that while a DMC's value lies in creating awareness that a defective component needs repair, a consumer's motivation to comply with a recall following the DMC will be lower if the amount of time required to repair the product is greater. Therefore, we hypothesize the following: H5:  Time for repair negatively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for components requiring more time to be repaired. The Moderating Role of Concurrent Recall CampaignsThe impact of a DMC on consumer recall compliance is likely to vary depending on whether there are concurrent recall campaigns for the same product. The rationale for examining concurrent recalls stems from research suggesting that repeated exposure to product warnings might result in consumers getting habituated to or even developing psychological reactance toward warning messages ([30]; [63]). Relatedly, some consumers may be skeptical of multiple instances of health-related product warnings from government information campaigns ([55]). In such situations, consumers might ignore safety warnings as they begin to perceive messages by regulatory agencies seeking compliance as less credible or even coercive. The detrimental effect of warning messages is particularly damaging for frequent users of the products that receive them ([30]). Accordingly, after a DMC makes consumers aware of recalls that have affected them, its impact on recall compliance is likely weaker for consumers facing concurrent recalls for different product issues relative to consumers facing a single recall. If the DMC makes consumers aware that their products are involved in concurrent recalls, they could become desensitized to recalls, leading to recall requests being ignored. Therefore, the impact of a DMC on recall compliance will be weaker for recalls including products involved in concurrent recall campaigns. H6:  Concurrent recall campaigns negatively moderate the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for products involved in concurrent recall campaigns. Research MethodologyThe setting for this study is the automobile industry in the United States. The NHTSA regulates product recalls in the U.S. automobile industry, and several of the characteristics of this setting make it an attractive one to explore our research objectives. For instance, safety in this industry is highly regulated, and manufacturers are mandated to report progress on product recall campaign completion rates periodically. As a result, longitudinal data on recall completion is publicly available, thereby facilitating a systematic investigation of recall compliance over time. The ""Safe Cars Save Lives"" CampaignIn 2014, the U.S. automobile industry experienced the largest product recall in its history. Several automobile manufacturers notified the NHTSA that they were conducting recalls to address a possible safety defect involving Takata airbag inflators. The concern was that the defective airbags were at risk of rupturing violently following a collision, hurling fiery shrapnel into drivers and passengers. Reports indicated that in addition to fatalities, hundreds of drivers had been injured. The scandal's scope escalated sharply in the following months, leading to a record-breaking 63 million vehicles recalled in 2014 and 51 million in 2015.Against this backdrop, the NHTSA launched a nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. Even though the DMC was motivated by the Takata airbag inflator issue, the campaign was a part of the agency's effort to improve consumer compliance amongst all recall campaigns. Lamenting low recall completion, the U.S. Transportation Secretary noted that informed consumers were ""allies"" in improving recall compliance ([24]). The DMC was a full-coverage initiative that sought to push consumers to use the NHTSA's recall lookup webpage to check for open recalls and then fix defective vehicles quickly.The DMC utilized paid search and online display advertisements to influence recall compliance. A key part of the campaign was the use of Google AdWords to target consumers searching for recall-related information online through keywords such as ""Vehicle Recalls,"" ""Check for Recalls,"" and ""Is My Vehicle Recalled,"" among others. A primary goal of sponsored advertisements is to guide consumers searching for information to the right location ([22]). In addition, the NHTSA used online display ads on media platforms such as Facebook that also directed consumers to the NHTSA's recall lookup webpage where they could receive recall-related information. A sample online display advertisement from the DMC is presented in Figure WA1 of the Web Appendix. Between January of 2016 and March of 2017, the NHTSA spent approximately $1 million on sponsored advertisements on Google, Facebook, and other platforms ([28]).To gain a preliminary understanding of the DMC's effectiveness, we collected data on the paid search traffic to the NHTSA's recall lookup webpage using SEMrush Analytics, a leading vendor providing a competitive research service on online marketing and advertising. Specifically, we collected data on the total number of paid search visits to the NHTSA's recall lookup webpage for every calendar quarter-year in our data period. Figure 2 depicts the paid search traffic to the recall lookup webpage before and after the DMC. The mean quarterly recall-related paid search traffic in the post-DMC period (quarter 1 [Q1] of 2016 onward) is 48,770 visits compared with 650 quarterly visits in the pre-DMC period.Graph: Figure 2. Model-free evidence of automobile recall–related paid search traffic to the regulator's recall lookup webpage over time. Data Sources and MeasuresThe unit of analysis for the study is the recall campaign-calendar quarter-year. The NHTSA maintains a database of every vehicle safety recall campaign issued from 1966 onward. A typical recall notice provides information on the vehicle make and models affected, the number of vehicles recalled, the nature of the defect, and the date of the recall's announcement.The data on the cumulative number of vehicles fixed, the measure for consumer recall compliance, was sourced from the NHTSA's database. The agency mandates that manufacturers provide quarterly updates on the number of vehicles affected by a recall and the number of affected vehicles fixed. These updates allow the NHTSA to monitor recall completion on an ongoing basis. Specifically, manufacturers report the number of vehicles fixed in each calendar quarter-year, and these reports are aggregated by unique recall identification numbers. We also collected data from the NHTSA on individual recall characteristics such as the ages of the recalled vehicle models, the defective component in the vehicles, the estimated time needed to complete the repairs, and if multiple recall notifications were sent to consumers. In addition, we gathered data from LexisNexis on the number of press articles about each recall campaign to capture the media coverage of a recall. Furthermore, we collected make-level advertising and sales data from Kantar's ad$pender database and Ward's Automotive Yearbook, respectively. Table 1 presents the variables and data sources for the study.GraphTable 1. Data Sources and Operationalization. Variable NameOperationalizationData Source(s)Role of VariableConsumer Recall ComplianceThe cumulative number of vehicles fixed in a recall campaign.NHTSADependent variableTime from RecallThe number of quarters elapsed from the time a recall campaign began until the calendar quarter directly preceding the start of the DMC.NHTSAExplanatory variableTransition to DMCCoded as 1 if the observation is during or after the first quarter of 2016 (during which the DMC was active), and 0 otherwise.NHTSAExplanatory variableTime Since DMC StartThe number of quarters elapsed since the NHTSA's DMC began.NHTSAExplanatory variableMedia CoverageThe number of media mentions for an individual recall campaign beginning from the day of the recall's announcement until three months afterward.LexisNexisModerator variableAge of Recalled ProductThe age, in years, of the oldest model recalled in a recall campaign at each respective recall completion report's calendar quarter-year.NHTSAModerator variableComponent HazardA binary measure of the hazard of the defective component identified in a recall campaign. Components range from 0 (low hazard) to 1 (high hazard).NHTSAModerator variableTime for RepairThe estimated time, in hours, needed for the dealer to complete the recommended repair.NHTSAModerator variableConcurrent Recall CampaignsA binary measure coded as 1 for recall campaigns including vehicles involved in concurrent recall campaigns, and 0 otherwise.NHTSAModerator variableAdvertising IntensityMake advertising expenses, in dollars, normalized by make sales, in units.Kantar ad$pender; Ward's Automotive YearbookControl variableMultiple Recall NotificationsA binary measure coded as 1 if the recalling firm sent an additional recall notification to consumers affected by a recall campaign above and beyond the mandatory notification required by the NHTSA, and 0 otherwise.NHTSAControl variableRecall SizeThe total number of vehicles affected by a recall.NHTSAControl variable Because the goal of the study is to examine the effectiveness of the DMC launched in January 2016, we only included recall campaigns in the sample if they were active both before and after the DMC began and had recall completion data available. Accordingly, we did not include recalls that ended before the DMC began or were announced after the DMC began. The final sample includes recall completion data from 296 recall campaigns in the U.S. automobile industry from 2015 to 2017, totaling 1,809 recall campaign-calendar quarter-year observations. Dependent measureWe operationalize Consumer Recall Compliance, the dependent variable, as the cumulative number of vehicles fixed for individual recall campaigns on a quarterly basis. For example, if a recall campaign had 10,000 vehicles fixed in its first calendar quarter-year, the dependent variable would equal 10,000. If an additional 4,000 vehicles were remedied in the subsequent quarter, the dependent variable would equal 14,000. Given that the DMC was an ongoing effort by the NHTSA, the cumulative number of vehicles fixed measure allows us to examine the DMC's effectiveness over time. Measures for moderator variablesWe operationalize Media Coverage as the count of the number of articles in leading news publications that covered a recall campaign during the three months following its announcement. For example, if a recall began on January 1, 2015, and four news articles were published covering the recall from the date of its announcement to March 31, 2015, the Media Coverage variable for that recall campaign would equal 4.We operationalize the Age of Recalled Product variable as the number of years elapsed between the manufacturing year of the oldest vehicle model in a recall campaign and the calendar quarter-year in which we observe its cumulative number of vehicles fixed. For example, consider a recall announced in January 2015 featuring two vehicle models manufactured in January 2013 and September 2014. Age of Recalled Product during the first quarter of the recall campaign would be coded as two years, since the January 2013 model is the oldest in the recall campaign. In April 2015 (the next quarter), the variable would equal 2.25 years.To operationalize Component Hazard, we used the NHTSA's database to identify the defective component in each recall campaign and past literature to determine the component's hazard rating. Consumers learn of the nature of their vehicle's defect in recall notification letters sent by manufacturers. Extant literature has classified recalls into two categories on the basis of the hazard or severity of the issue ([ 1], [ 2]; [34]; [57]; [58]). Consistent with previous research, we operationalize Component Hazard as a dummy variable by classifying defective components related to the driving functionality of a vehicle (i.e., those that could result in a loss of vehicle control due to acceleration, steering or braking, frame corrosion, fire, or repeated stalling) as ""high hazard"" (coded as 1). All other components not directly related to the drivability of a vehicle are classified as ""low hazard"" (coded as 0). A nonexhaustive list of examples of components and their hazard ratings is reported in Table WA1 of the Web Appendix.We operationalize the Time for Repair variable in terms of the estimated time (in hours) needed to repair the defective component provided in recall notification letters.To operationalize Concurrent Recall Campaigns, we examined active recalls to determine if vehicles of the same make-model-year whose dates of production overlapped were involved in multiple simultaneous recalls during our observation window. For example, we used the NHTSA's database to check if 2015 Toyota Camry vehicles experienced multiple active recalls. We operationalize Concurrent Recall Campaigns as a dummy variable by coding recall campaigns including vehicles involved in concurrent recall campaigns as 1 and all other recall campaigns that did not include vehicles involved in concurrent recalls as 0. Measures for control variablesIn addition to the DMC, make-level advertising by firms could impact how aware consumers are of active recalls and thereby improve their compliance. Accordingly, we include the make-level Advertising Intensity variable in our specification to control for firm efforts to improve make awareness. We use advertising expenses data from Kantar's ad$pender database at the make level and normalize it by make sales in units using Ward's Automotive Yearbook to account for automaker efforts at improving make awareness.Although all automakers are mandated to notify consumers about a product recall, in some cases, they issue more than one notification for the same recall. Given that multiple notifications could improve recall completion, we include the dummy variable Multiple Recall Notifications to control for this possibility. The variable takes a value of 1 if a manufacturer notifies consumers affected by a recall more than once and 0 otherwise. Furthermore, we control for the size of a recall, as the total number of vehicles impacted by a recall campaign could positively impact the cumulative number of vehicles fixed. We assemble data on the Recall Size variable from the NHTSA's quarterly recall completion reports. Finally, we control for unobserved automobile make and temporal (i.e., calendar quarter) characteristics with the Make and Quarter dummy variables, respectively. Model SpecificationWe ground our empirical specification in the interrupted time series analysis (ITSA) framework, a technique commonly used to evaluate the effectiveness of universally implemented policies. ITSA is a quasiexperimental design typically utilized to assess the longitudinal effects of interventions with observational data where full randomization is not possible. ITSA is implemented using a regression model that includes up to three types of time-related covariates: ( 1) Time, ( 2) Transition, and ( 3) Time Since Intervention. The interpretation of the covariates is sensitive to their coding and the presence or absence of the other covariates; their inclusion in the model is theory driven. We employ an absolute coding approach, which allows the impact of an intervention to be interpreted in absolute terms (i.e., relative to zero) ([ 8]). The key identifying assumption of ITSA is that in the absence of an intervention, the slope in the outcome will remain unchanged in the postintervention period ([ 8]; [38]; [60]).For a model that includes all three covariates and utilizes absolute coding, Time from Recall is operationalized as the number of calendar quarters elapsed from the time a recall began until the calendar quarter preceding the start of the DMC. The coefficient of Time from Recall captures the slope in the outcome before the DMC begins. Transition to DMC is coded as 1 for all observations during the DMC (Q1 of 2016 and onward) and 0 for observations before the DMC (Q1–Q4 of 2015). In the presence of Time from Recall, the coefficient for Transition to DMC captures the absolute change in the outcome relative to zero in the first time period the intervention is active (i.e., immediately after it is introduced), and its effect remains relevant in the remainder of the observation window thereafter; it is not limited to the first quarter after the DMC. Time Since DMC Start is operationalized as the number of quarters elapsed since the NHTSA's DMC began in Q1 of 2016; it is coded as 0 for all observations before Q2 of 2016. In the presence of Time from Recall and Transition to DMC, the coefficient of Time Since DMC Start captures the slope of the outcome following the DMC relative to zero. Table 2 illustrates the coding of the three ITSA time-related variables for a recall campaign beginning in Q1 of 2015 and ending in Q2 of 2017.GraphTable 2. Illustration of Absolute Coding of Time-Related Variables in the ITSA Framework. QuarterYearTime from RecallTransition to DMCTime Since DMC Start12015000220151003201520042015300120163102201631132016312420163131201731422017315 Consistent with model testing procedures outlined in previous research ([ 8]), we examine whether there are quadratic trends in the dependent variable. Prior research on prescription drug use compliance has found quadratic patterns in some instances ([11]). We compare the model fit for the specification in Equation 1 with and without quadratic pre- and post-DMC slopes. The inclusion of Time Squared and Time Since Intervention Squared decreases the Akaike information criterion for the model from 41,791 to 41,751, indicating better model fit. Accordingly, we augment the specification by including the Time Squared and Time Since Intervention Squared variables.In our context, we argue that Consumer Recall Compliance is a function of Time from Recall, Time from Recall Squared, Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared. The Time from Recall and Time from Recall Squared variables capture the quarterly pre-DMC slope in recall compliance. Transition to DMC captures the absolute change, relative to zero, in recall compliance immediately after the DMC was introduced in Q1 of 2016. In our context, the impact of Transition to DMC is reflected over the remainder of the observation window and is not just limited to the first quarter after the DMC. Time Since DMC Start and Time Since DMC Start Squared capture the post-DMC slope in recall compliance (i.e., the impact of the DMC following its introduction).In summary, the empirical model controls for various recall campaign-level and time-specific factors that could impact consumer recall compliance. We also account for automobile make- and calendar-quarter-specific unobserved heterogeneity using fixed effects. The empirical specification to test the main effect of the DMC on consumer recall compliance is as follows: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9–63Makei+β64–66Quartert+εit, Graph( 1)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. Results DescriptivesTable 3 presents the descriptive statistics for the variables. The mean media coverage in our sample is.44 articles. The mean age of the oldest vehicle in each recall is 4.09 years, with vehicles ranging from less than a month old to 19.06 years old. The mean estimated time needed to repair a defective component is 1.88 hours, with a few requiring as many as 13 hours. The mean hazard rating of the components involved in recalls is.19. While there is a wide range for the advertising intensity of different vehicle makes, the mean is.98. That is, approximately $1 is spent on advertising for every unit sold. In the full interaction model sample, the mean number of calendar quarters for a recall campaign is 6.04 (min = 3, max = 10, SD = .70). The mean number of calendar quarters in the pre-DMC period for a recall campaign in the full interaction model sample is 2.47 (min = 1, max = 4, SD = 1.17), whereas in the post-DMC period it is 3.57 calendar quarters (min = 1, max = 8, SD = 1.32).GraphTable 3. Descriptive Statistics. Variable NameMeanSDMinMax# of ObservationsConsumer Recall Compliance38,503105,58601,397,6121,809Media Coverage.441.250101,809Age of Recalled Product (in years)4.093.51.0419.061,655Component Hazard.19.39011,809Time for Repair (in hours)1.882.110131,809Concurrent Recall Campaigns.35.48011,809Advertising Intensity (in $/unit sales).989.510208.861,785Multiple Recall Notifications.03.17011,809Recall Size77,366196,46811,814,2841,809  Results for the Main Effect of the DMC on Consumer Recall ComplianceThe results of the DMC's main effect on Consumer Recall Compliance are reported in Model 1 of Table 4. We note that the results reported in this analysis pertain to 296 recall campaigns involving issues unrelated to airbag inflators. We exclude airbag inflator–related recall campaigns from the sample to avoid a potential endogeneity issue arising from the NHTSA's unobserved efforts to improve the recall completion of airbag inflator–related recalls specifically. The discussion of endogeneity and the analyses appear in Table WA2 of the Web Appendix.GraphTable 4. Results for the Main Effect of the DMC on Consumer Recall Compliance. Model 1Main-Effects ModelDependent Variable: Consumer Recall ComplianceEstimate (SE)Hypothesis TestedIntercept−12,560.21(27,360.13)Time from Recall14,172.90***(3,674.81)Time from Recall Squared−2,453.12**(1,168.53)Transition to DMC11,112.42**H1 supported (+)(4,393.39)Time Since DMC Start−1,959.86H1 supported (+)(2,894.73)Time Since DMC Start Squared1,748.90***H1 supported (+)(606.74)Advertising Intensity−1.32(209.64)Multiple Recall Notifications9,011.68(20,151.71)Recall Size.43***(.02)Make Fixed EffectsYesQuarter Fixed EffectsYes N (sample size)1,785R-Squared Within.09R-Squared Between.74  1 *p < .10.2 **p < .05.3 ***p < .01.The results in Model 1 of Table 4 reveal that the coefficients for Advertising Intensity and Multiple Recall Notifications are not significant (p > .65). As we expected, the coefficient for Recall Size is positive (.43, p < .01). The coefficient of Time from Recall Squared is negative (2,453.12, p < .05). To understand the linear effect of Time from Recall, we estimate a model specification identical to Equation 1 except that the quadratic terms are dropped, as the average linear effect of a variable is not interpretable in the presence of its quadratic effect ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time from Recall is positive (8,725.57, p < .01). These results collectively suggest that consumer recall compliance increased at a decreasing rate prior to the start of the DMC.The coefficient for Transition to DMC is positive (11,112.42, p < .05), indicating that, immediately following the DMC's introduction, the cumulative number of vehicles fixed increased in absolute terms in Q1 of 2016. In our context, the impact of Transition to DMC associated with Q1 of 2016 remains relevant in the remainder of the observation window thereafter and is not just limited to the first quarter after the DMC. The interpretation of Transition to DMC is contingent on the presence of Time from Recall in the model.In the presence of Time from Recall, Time from Recall Squared, and Transition to DMC, Time Since DMC Start and Time Since DMC Start Squared represent the slope of consumer recall compliance in the post-DMC period. The results in Model 1 of Table 4 indicate that the coefficient for Time Since DMC Start Squared is positive (1,748.90, p < .01). As before, to understand the linear trend in Time Since DMC Start, we estimate Equation 1 without the quadratic terms in the model ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time Since DMC Start is positive ( 6,036.62, _I_p_i_ < .01). These results collectively suggest that following the DMC's introduction, consumer recall compliance increases at an increasing rate (i.e., a positive accelerating curve), in contrast to the pre-DMC trend. H1 is supported.The visual plots in Figure 3 provide a representation of the DMC's effectiveness for an average-sized recall using the estimates provided in Model 1 of Table 4. In Figure 3, the solid line (""Number of vehicles fixed with the DMC"") denotes the DMC's impact on recall compliance for a recall campaign that begins in Q1 of 2015 and ends in Q4 of 2016. The solid line shows that recall compliance increased at a decreasing rate before the start of the DMC. The DMC begins in Q1 of 2016 after four quarters have elapsed, represented by the vertical dotted line. Between four and five quarters elapsed, the solid line captures the impact of the DMC in Q1 of 2016. Following Q1 of 2016, the solid line captures both the positive linear and positive quadratic trends in consumer recall compliance. Thus, Figure 3 reveals that the DMC interrupted the negative accelerating curve in consumer recall compliance in the pre-DMC period and resulted in a positive acceleration in the cumulative number of vehicles fixed in the post-DMC period.Graph: Figure 3. A visual representation of the DMC's effect on consumer recall compliance.To estimate consumer recall compliance in the absence of the DMC, we set the post-DMC time-related variables Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared to zero in Equation 1. In the pre-DMC period, the dotted line with circular dots (""Number of vehicles fixed without the DMC"") is overlapped by the solid line, as both depict the pre-DMC slope. The dashed line (""Predicted number of vehicles fixed without the DMC"") depicts the predicted trend in recall compliance in the post-DMC period without the DMC. In line with the negative quadratic trend in compliance before the DMC, the model predicts that the cumulative number of vehicles fixed in Q1 of 2016 would be lower than in Q4 of 2015. However, because the cumulative number of vehicles fixed over time cannot decrease, using the model-based results to predict compliance without the DMC in the post-DMC period would overstate the DMC's positive effect and lead to erroneous conclusions. A reasonable prediction in our context is that compliance would level off from Q4 of 2015 onward without the DMC. Accordingly, the predicted trajectory of consumer recall compliance without the DMC in Figure 3 in the post-DMC period is depicted as a flat line.To quantify the DMC's positive impact, we compute the difference in the predicted cumulative number of vehicles fixed with and without the DMC for a recall campaign after eight calendar quarters have elapsed using the coefficient estimates in Model 1 of Table 4 (represented by the solid line) and the dashed line in Figure 3. We find that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what would be expected without the DMC. Testing for Moderators of the DMC's EffectivenessTo test the moderating effects (H2–H6), we estimate the following specification: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9MediaCoveragei+β10AgeofRecalledProductit+β11ComponentHazardi+β12TimeforRepairi+β13ConcurrentRecallCampaignsi+β14TransitiontoDMCt×MediaCoveragei+β15TransitiontoDMCt×AgeofRecalledProductit+β16TransitiontoDMCt×ComponentHazardi+β17TransitiontoDMCt×TimeforRepairi+β18TransitiontoDMCt×ConcurrentRecallCampaignsi+β19TimeSinceDMCStartt×MediaCoveragei+β20TimeSinceDMCStartt×AgeofRecalledProductit+β21TimeSinceDMCStartt×ComponentHazardi+β22TimeSinceDMCStartt×TimeforRepairi+β23TimeSinceDMCStartt×ConcurrentRecallCampaignsi+β24–78Makei+β79–81Quartert+εit, Graph( 2)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. There are two points worth noting about this specification. First, Equation 2 is identical to the main-effect specification in Equation 1, aside from the addition of the moderators and interaction terms. Second, the interaction terms between Transition to DMC and the moderators test for boundary conditions of the DMC's effectiveness at the moment it is introduced in Q1 of 2016. The interaction terms between Time Since DMC Start and the moderators test for the DMC's boundary conditions following its introduction from Q2 of 2016 onward.[ 7]The results for the moderating effects are reported in Model 1 of Table 5. None of the interaction terms in Model 1 have variance inflation factors (VIFs) above 5.17. The VIFs for Model 1 are reported in Table WA3 of the Web Appendix. While the lower-order terms of the time-related variables in Model 1 of Table 5 have VIFs above 10, our interest is only in interpreting the coefficients of the interaction terms. Furthermore, the interpretation of an interaction term is not affected by multicollinearity, as this multicollinearity affects neither the interaction term's coefficient nor its standard error ([42], p. 399). The same is true for the coefficients and standard errors of higher-order polynomials, which are essentially interactions of the lower-order terms ([17]).GraphTable 5. Results for the Effect of the DMC on Consumer Recall Compliance: Interaction Models. Model 1Model 2Model 3Model 4Dependent Variable: Consumer Recall ComplianceInteraction ModelInteraction Model Without Make Fixed EffectsInteraction Model with Only Linear Slope TermsInteraction Model with Only Linear Slope Terms and Without Make Fixed EffectsEstimate (SE)Hypothesis TestedEstimate (SE)Estimate (SE)Estimate (SE)Intercept–6,809.36–3,814.06–5,232.021,761.74(20,975.05)(6,009.95)(20,958.88)(5,769.30)Time from Recall14,838.04*** 13,896.02***8,929.75***8,562.00***(3,697.30)(3,603.27)(1,545.46)(1,467.90)Time from Recall Squared–2,165.48* –1,988.51*(1,194.20)(1,178.13)Transition to DMC–2,315.27 –1,157.12–3,212.90–2,432.85(6,250.76)(6,123.64)(5,763.04)(5,668.35)Time Since DMC Start1,243.44 652.762,195.532,076.94(3,492.59)(3,445.95)(2,136.19)(2,113.41)Time Since DMC Start Squared201.56 314.61(677.47)(666.82)Advertising Intensity–25.31 48.54–49.5031.68(211.92)(164.90)(211.58)(164.66)Multiple Recall Notifications6,328.66 5,699.317,362.646,778.29(15,073.09)(13,263.56)(15,078.72)(13,250.10)Recall Size.51*** .51***.51***.51***(.01)(.01)(.01)(.01)Media Coverage–2,105.46 –2,162.93–2,263.14–2,291.67(1,614.43)(1,546.53)(1,612.49)(1,544.60)Age of Recalled Producta–5,434.18*** –5,373.96***–5,438.46***–5,400.57***(938.50)(809.64)(939.49)(809.57)Component Hazard3,789.08 2,573.144,034.832,797.70(7,013.30)(6,388.82)(7,018.76)(6,388.20)Time for Repairb–518.81 274.31–571.21262.58(1,700.56)(1,242.39)(1,702.20)(1,242.49)Concurrent Recall Campaigns4,380.81 4,753.414,539.604,777.33(6,334.25)(5,499.19)(6,340.05)(5,499.64)Transition to DMC × Media Coverage3,993.22**H2 supported (+)4,076.06**4,288.84**4,309.28**(1,770.60)(1,758.61)(1,748.75)(1,737.81)Transition to DMC × Age of Recalled Producta2,171.51***H3 supported (+)2,111.26***2,083.69***2,018.78***(684.75)(680.06)(681.70)(677.03)Transition to DMC × Component Hazard–1,538.73H4 not supported (+)–925.12–1,336.34–807.79(5,546.05)(5,506.80)(5,536.56)(5,497.76)Transition to DMC × Time for Repairb538.61H5 partially supportedc (−)468.71556.37490.69(1,077.57)(1,071.98)(1,077.58)(1,072.11)Transition to DMC × Concurrent Recall Campaigns3,498.99H6 not supported (−)3,363.493,602.483,495.35(4,782.83)(4,759.03)(4,780.47)(4,757.08)Time Since DMC Start × Media Coverage1,499.02***H2 supported (+)1,455.20***1,519.93***1,497.27***(516.48)(511.32)(501.47)(497.09)Time Since DMC Start × Age of Recalled Producta884.17***H3 supported (+)878.30***899.60***900.10***(246.13)(244.12)(241.40)(239.48)Time Since DMC Start × Component Hazard–3,080.78H4 not supported (+)–3,350.67–3,062.90–3,293.08(2,334.58)(2,308.70)(2,324.63)(2,298.18)Time Since DMC Start × Time for Repairb–830.94*H5 partially supportedc (−)–785.30*–841.29*–799.98*(458.47)(454.04)(458.16)(453.75)Time Since DMC Start × Concurrent Recall Campaigns2,156.78H6 not supported (−)2,478.382,112.022,390.51(1,986.83)(1,971.75)(1,976.25)(1,960.38)Make Fixed EffectsYesNoYesNoQuarter Fixed EffectsYesYesYesYesN (sample size)1,6371,6371,6371,637R-Squared Within.07.08.07.08R-Squared Between.87.87.87.87 4 *p <.10.5 **p <.05.6 ***p <.01.7 a In years.8 b In hours.9 c The interaction between Transition to DMC and Time for Repair is not significant. However, the interaction between Time Since DMC Start and Time for Repair is marginally significant (p =.07). Therefore, H5 is partially supported.10 Notes: The full set of results, with VIFs for the variables, is reported in Table WA3 of the Web Appendix.We find that the coefficient of the interaction between the Transition to DMC and Media Coverage variables is positive (3,993.22, p < .05). This finding suggests that at the moment the DMC was introduced in Q1 of 2016, its impact on consumer recall compliance was stronger for recall campaigns with greater media coverage. We also find that the coefficient of the interaction between Time Since DMC Start and Media Coverage is positive (1,499.02, p < .01), implying that Media Coverage continues to positively moderate the DMC–consumer recall compliance relationship from Q2 of 2016 onward. H2 is thus supported.We use visual plots in Figure 4, Panel A, to provide an intuitive understanding of interactions with Media Coverage. We set ""low"" (0) and ""high"" ( 1) values for Media Coverage to visually represent its impact on the DMC's effectiveness for a recall campaign of average size that spans from Q1 of 2015 until Q4 of 2016. As Panel A shows, the DMC's impact is stronger when media coverage for a recall is high relative to low.Graph: Figure 4. The moderating roles of media coverage, age of recalled product, and time for repair during the DMC.We also find that the coefficient of the interaction between Transition to DMC and Age of Recalled Product is positive (2,171.51, p < .01). That is, the DMC's impact on consumer recall compliance is stronger for older (vs. newer) recalled vehicles at the moment it is introduced in Q1 of 2016. We also find that the coefficient for the interaction between Time Since DMC Start and Age of Recalled Product is positive (884.17, p < .01), which suggests that the DMC also has a stronger impact on consumer recall compliance for older vehicles from Q2 of 2016 onward. H3 is supported. We set ""low"" (25th percentile) and ""high"" (75th percentile) values of the Age of Recalled Product variable in Figure 4, Panel B, which demonstrates that the DMC's effectiveness is stronger for older vehicles (i.e., for larger values of the variable) both at the moment of the DMC's introduction and thereafter.Inconsistent with H4, we find that neither the Transition to DMC × Component Hazard nor the Time Since DMC Start × Component Hazard interaction terms are significant (p > .78 and p > .18, respectively). Perhaps this insignificant finding implies that consumers are unable to actually discern the hazard of the defective components cited in recall notification letters.We find that the interaction between Transition to DMC and Time for Repair is not significant (p > .61). However, the coefficient for the interaction between Time Since DMC Start and Time for Repair is negative (830.94, p = .07) and marginally significant. Thus, following the DMC's introduction from Q2 of 2016 until Q4 of 2017, the DMC is less effective at improving consumer recall compliance as the time needed to repair a defective component increases. H5 is partially supported. Using ""low"" (25th percentile) and ""high"" (75th percentile) values for Time for Repair in Figure 4, Panel C, we demonstrate visually that following the DMC's introduction, the DMC's effectiveness is weaker when a defective component requires more versus less time to repair.Finally, we find that neither the Transition to DMC × Concurrent Recall Campaigns nor the Time Since DMC Start × Concurrent Recall Campaigns interaction variables are significant (p > .46 and p > .27, respectively). H6 is not supported. The coefficients for the Advertising Intensity and Multiple Recall Notifications variables are also not significant (p > .67), while the coefficient for the Recall Size variable is positive (.51, p < .01).The moderator results are robust to several alternate model specifications. We estimate three specifications similar to Model 1 in Table 5, removing automobile make–specific fixed effects and/or Time from Recall Squared and Time Since DMC Start Squared. The results of the alternate specifications are reported in Models 2, 3, and 4 of Table 5. Across each model, the estimates of the interaction terms are largely similar to Model 1, and the conclusions regarding the impact of the moderators on the DMC's effectiveness remain the same. DiscussionThis study addresses the important question of whether a regulator-initiated DMC can improve consumer recall compliance. Further, the study aims to understand the conditions under which a DMC is more or less effective. We were able to exploit a full-coverage national DMC launched in January of 2016 by the NHTSA, the regulator in the U.S. automobile industry, to answer these research questions. The study offers several implications for researchers and policy makers. Research ImplicationsThe question of whether awareness campaigns are successful in eliciting the required compliance-related behaviors from consumers has been investigated in various empirical settings (e.g., [32]; [66]). The fact that the awareness campaign in our study is introduced by a governmental agency and that it is its first major effort to improve recall compliance using digital media raises questions about its potential efficacy. We find that a regulator-initiated DMC is effective at improving consumer recall compliance, as reflected by the additional number of vehicles fixed above what was to be expected without it. This finding underscores the critical importance of regulators utilizing digital means to provide recall-related information to elicit greater compliance from consumers.The moderator analyses reveal that the DMC's effectiveness varies across recall campaigns. Specifically, the DMC's impact on consumer recall compliance is stronger for recall campaigns with greater media coverage. This finding is similar in spirit to research that has documented the moderating impact of negative publicity on postcrisis advertising and consumers' brand share and category purchases ([16]). Although brands aim to avoid negative publicity of their recalled products because of its adverse effects on sales, media outlets play an important functional role in reinforcing a regulator's efforts to improve consumer recall compliance. In our context, this effect is likely driven by the varying specificity of information provided by the regulator and the media.We also find that the DMC is more effective when the recalled products are older. Prior research has found that the utility of paid search campaigns is greater in contexts where consumers have low levels of familiarity with the firms' brands or products ([ 7]). Along similar lines, we find that the DMC is more effective when the recalled products are older. Consumers of older products may not be in contact with their products' OEMs as their warranties expire and thereby be less familiar with safety-related developments for their vehicles. A DMC can provide resources for these consumers who are more likely to seek out recall-related information digitally.Finally, we find that the DMC is less effective at improving consumer recall compliance for recall campaigns containing products with defects that require more (vs. less) time to repair. Previous work examining tax compliance similarly finds that time-related costs are significant barriers to compliance ([10]; [64]). Although our study finds that the DMC is effective at providing the necessary information to improve compliance, the inconvenience that higher repair times pose to consumers is still a significant impediment to compliance. Future research should examine whether regulators can improve compliance for recalls that require long repair times by using persuasive appeals. Public Policy ImplicationsOur study's findings also offer actionable insights for policy makers. In recent years, multiple audit reports submitted to U.S. Congress have questioned the adequacy of the NHTSA's oversight processes in managing consumer recall compliance ([52]). The overarching concern is that the agency has failed to carefully review safety issues, hold automakers accountable, collect safety data, or adequately train its staff, resulting in significant safety concerns being overlooked. Some industry experts go a step further and lament that product recalls may increase the number of crashes on the road, as the extra driving needed to remedy the issues heightens the probability of accidents ([43]). Our analysis reveals that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC.The improvement in consumer compliance as a result of the DMC is economically meaningful because it could reduce the number of vehicle accidents and the economic costs associated with vehicle crashes. Research has shown that, on average, a 1% improvement in recall compliance in the automobile industry lowers the number of vehicle accidents in the next three years by.46% (Bae and Benítez-Silva [ 1]). The average economic costs (e.g., fatalities, nonfatal injuries, damaged vehicles) associated with a motor vehicle accident are about $14,000 ([ 9]). Therefore, by improving consumer recall compliance, the DMC reduces the number of automobile accidents on the road and lowers the total economic costs associated with accidents. The last few years have witnessed an average of 953 product recalls in the U.S. automobile industry, suggesting that the total annual economic impact of improved consumer recall compliance across all recall campaigns is not trivial.Further, our study implies that a lack of available relevant information is a significant contributing factor to low consumer recall compliance. For years, the NHTSA has tried to mandate electronic recall notices to counteract the issue of low consumer recall awareness ([46]). The regulatory agency's foray into the digital domain with ""Safe Cars Save Lives"" has proven successful, suggesting that improving consumer awareness of recall-related information is a crucial step in improving consumer recall compliance. Even though the automobile industry receives considerable media attention (e.g., GM's faulty ignition issue, Takata's airbag failures), getting consumers to pay attention to recall notifications is challenging.While extant work notes that manufacturers view greater media coverage of a recall as damaging to their brands' financial health ([ 5]), we find that media coverage helps improve safety outcomes by increasing the effectiveness of the DMC. Furthermore, we find that the DMC's impact on consumer recall compliance is stronger when recalled vehicles are older. Issues of low compliance are particularly prevalent among owners of these older products. According to J.D. [54], just 44% of vehicles manufactured between 2003 and 2007 had their defects remedied, drastically below recall completion percentages of 73% for vehicles of model years 2013–2017. Federal and industry leaders have cited improving compliance among owners of older vehicles as one of four key topic areas to address moving forward ([51]). The DMC's effectiveness on consumers owning older products further suggests that regulators' use of digital tools to facilitate consumer access to relevant information could improve compliance.Finally, our findings caution regulators to be mindful of perceived time inconvenience as a serious impediment to consumer recall compliance. While the DMC is effective at improving compliance, its impact is lower for recall campaigns with defective components that require longer to repair. Interestingly, consumers often do not cite the time needed to complete the repair as the most important factor in deciding whether to remedy safety defects ([28]). Yet, our findings suggest that perceived time inconvenience is a serious obstacle to achieving consumer compliance. The findings should enable regulatory agencies to make more compelling cases for financial resources to be devoted to DMCs aiming to improve compliance. Limitations and Future DirectionsThe findings of the study are subject to some limitations. First, the study is limited to the U.S. automobile industry and a single DMC, implying that caution is warranted in generalizing our findings to other settings. If systematic recall completion data were available in other contexts, the conceptual and methodological framework employed in this study could be useful in deriving empirical generalizations about the effectiveness of DMCs in other settings. Second, data on the NHTSA's total advertising expenditure on this campaign were not available. As such, the DMC's positive effect should not be interpreted as consumers' responsiveness to advertising dollars. Third, our study was unable to examine if a recall notification message's content impacts compliance because the mandated recall notification letters sent from manufacturers to consumers in our setting were standard, lacking variation. If regulators use assertive versus nonassertive language or fear- versus health- versus norm-based persuasion appeals in their advertisements or messaging, understanding which types improve compliance would be valuable. "
34,"Sales and Self: The Noneconomic Value of Selling the Fruits of One's Labor A core assumption across many disciplines is that producers enter market exchange relationships for economic reasons. This research examines an overlooked factor; namely, the socioemotional benefits of selling the fruits of one's labor. Specifically, the authors find that individuals selling their products interpret sales as a signal from the market that serves as a source of self-validation, thus increasing their happiness above and beyond any monetary rewards from those sales. This effect highlights an information asymmetry that is opposite to what is found in traditional signaling theory. That is, the authors find that customers have information about product quality that they signal to the producer, validating the producer's skill level. Furthermore, the sales-as-signal effect is moderated by characteristics of the purchase transaction that determine the signal strength of sales: The effect is attenuated when product choice does not reflect a deliberate decision and is amplified when buyers incur higher monetary costs. In addition, sales have a stronger effect on happiness than alternative, nonmonetary forms of market signals such as likes. Finally, the sales-as-signal effect is more pronounced when individuals sell their self-made (vs. other-made) products and affects individuals' happiness beyond the happiness gained from producing.Keywords: self-production; signaling; selling; happiness; self-validationDigital platforms such as Etsy or Amazon Handmade have made it easy for individuals to sell their self-made products to other individuals. Commercial activities that were previously limited to economically marginal contexts such as flea markets have become big business. Although each individual producer's commercial activity might be small, the sheer number of such producers adds an important new source of competitive pressure for traditional firms in many industries, including clothing, food, and home furnishings. For example, in 2020, Etsy had around 4.4 million sellers and 82 million buyers, leading to a total transaction value of around $10 billion ([39]).Existing research has investigated the psychological and behavioral consequences of engaging in self-production. In short, people like the fruits of their labor and value products they produce themselves more than comparable products made by others ([17]; [31]). The higher valuation of self-made products stems from the sense of accomplishment, pride, and competence that individuals experience when they successfully self-design or assemble a product ([ 9]; [29]). Prior research has focused on individuals engaging in self-production with the objective of consuming the product themselves or giving it as a gift ([30]), but it provides little insight into the increasingly common situation in which individuals make products with the objective of selling them to ""the market""; that is, to unknown others.A common assumption in marketing, economics, and entrepreneurship is that producers participate in market exchanges for economic reasons ([18]; [36]). Despite this disciplinary emphasis on economic motives, it seems possible that individuals produce and subsequently sell products also for noneconomic reasons. Research in organizational behavior ([19]), economics ([ 2]), and entrepreneurship ([37]) has drawn attention to the socioemotional motivations of producing, but little attention has been paid specifically to the noneconomic benefits of selling the fruits of one's labor. Drawing on survey data from the field as well as a series of experiments, we document a sales-as-signal effect: Selling their self-made products increases individual producers' happiness above and beyond any monetary implications from these sales. This effect occurs because individual producers interpret the number of products sold as a market signal that validates their skills and competencies as producers.Our research makes several contributions. First, in demonstrating the socioemotional consequences of selling one's creations, we introduce a novel perspective on the value of sales. We propose that above and beyond their importance as a source of monetary income, sales can also have socioemotional value by providing a source of self-validation. Economic theory generally conceptualizes supply-side agents as profit-maximizers, such that ""managers of a firm make those choices that maximize the sum of current and future profits"" ([15], p. 769). Consequently, one would expect that the value individual producers derive from selling their products is a function of the money they make from these sales. However, we demonstrate that the value individual producers gain from sales cannot be solely defined in economic terms. Instead, individual producers also gain considerable happiness via feelings of self-validation from selling their products. We thus provide evidence for the importance of the noneconomic value of participating in market exchanges.Second, our findings contribute to the literature examining individuals' valuation of their self-made products ([17]; [30]; [31]). Whereas existing research has shown that individual producers feel competent and proud from successfully designing or assembling a product, our work examines the psychological consequences of selling self-made products and not of merely producing those products. We demonstrate that having actual buyers purchase one's self-made products functions as an external confirmation that the individual producer is competent and capable of creating a high-quality, marketable product, which fuels one's happiness beyond the happiness derived from production.Third, we offer a novel perspective on the role of signals in market exchanges by conceptualizing sales as a signal from the market. Research in economics, marketing, and management has typically conceptualized marketplace signals as actions taken by sellers to convey information about unobservable product qualities to buyers ([ 5]; [ 8]; [23]). Thus, signals are traditionally sent by sellers and interpreted by buyers. In contrast, we propose that sellers interpret the act of purchasing a product as a signal from the buyer that validates the seller's skills and competencies as producer. Furthermore, whereas signals are usually conceptualized as intentional actions meant to benefit the sender ([ 8]), we propose that sending this type of signal to the seller is often incidental to buyers' core motivation to buy, which lies in their consumption goals, and that the signal does not directly benefit the sender but rather the receiver of the signal. Moreover, traditional signaling research assumes that sellers know the quality of their product and that buyers are uncertain. Our work indicates that sellers are, to some extent, uncertain about their own product and thus about their competencies as a producer. Our work indicates that buyers have information about product quality that reduces sellers' uncertainty. Therefore, we propose that individual producers interpret sales as a signal from their buyers that serves as a source of self-validation.Fourth, our findings broaden the discussion on the societal role of market exchanges, a topic of intense interest among marketing scholars and practitioners ([ 7]). We add to this discussion on how marketing can help create a better world by drawing attention to the way selling might provide a positive source of meaning and happiness for individuals. Just like the social costs of marketing have often been underestimated, we argue that some important benefits have been neglected as well. Specifically, successfully marketing their products is a source of self-validation and happiness for producers. Theory Benefits of Selling Self-Made ProductsWhat benefits do individuals derive from selling their self-made products? Why do they continue to populate online marketplace platforms? The principal and most obvious benefit that people derive from selling their products is money. But can monetary incentives alone explain the increasing popularity of online marketplaces? We propose that learning a customer bought their products increases individual producers' happiness above and beyond the monetary implications of these sales.Specifically, we argue that sales validate one's skills and competencies as a producer. We found preliminary support for this notion in an exploratory qualitative study conducted as part of a master's thesis supervised by one of the authors ([42]). In ten in-depth interviews with Etsy sellers and nonobtrusive observations of Etsy's online discussion forums (see Web Appendix A [WA-A]), several informants highlighted that selling their products makes them happy; for example, ""creating something that I like and others like enough to spend their hard-earned money on, is bliss"" (Informant #7 from Etsy forum), and ""it's so flattering when people choose to buy your creations"" (#61, forum). The narratives suggest that the happiness derived from selling is not necessarily rooted in economic reasons but in one's perceived self-worth as a producer; for example, ""Etsy allows me to rediscover my worth"" (#31, forum). Describing a producer friend, one of our informants (#4, interview) stated, ""for Jeani, I think it is the fact that she is making something someone thinks is worthy of buying. You know, paying some money for and it's like an accolade of her creative talent."" Interestingly, the narratives suggest that the increased self-worth derived from selling motivates people to continue producing their own products; for example, ""a sale...usually motivates me to make more, since it makes me feel as though my items are appreciated."" In summary, our preliminary qualitative insights point to the possibility that sales make individual producers happy not only because of the monetary gains but also because sales more fundamentally validate their skills and competencies as a producer. The Sales-as-Signal EffectWe argue that sales function as a signal from the marketplace that boosts individual sellers' self-validation. Signaling theory was developed in information economics to study market interactions under conditions of information asymmetry between sellers and buyers ([38]). It generally assumes that sellers are aware of the quality of their goods but buyers are not. To distinguish low-quality sellers from high-quality sellers, buyers must detect and interpret the signals sent by sellers. Prices, advertising, brands, and different types of firm actions can constitute signals ([ 8]; [22]; [23]). Moreover, signaling theory and its applications in marketing usually presume that the signal originates from a seller and is received by a buyer.[ 7] In our research, we propose that sales constitute a signal that is sent by buyers (with or without buyers' intention to actually signal something) and that validates the seller's competencies as a producer. Thus, in this context, there is an information asymmetry in the opposite direction from traditional signaling theory: It is the buyer who has information that reduces the seller's uncertainty (about the seller's skills and competencies about the producer).Feelings of competence, which often result from others' validation of one's skills, are a central motivation for people to engage in creative tasks ([ 9]) and greatly affect how satisfied individuals are with their work ([33]). Crucially, feeling competent is a fundamental psychological need among humans, and its fulfillment strongly determines individuals' intrinsic motivation, life satisfaction, and mental health ([34]; [41]). [11], p. 231) even argue that the need to feel competent ""must be satisfied for long-term psychological health."" We thus propose that being validated as a competent producer through sales increases individual producers' happiness.In summary, we predict a sales-as-signal effect: Sales increase individual producers' happiness, even when controlling for the effect of monetary gains. This is because producers interpret sales as a positive signal from the market that validates them as competent producers. Formally: H1:  Sales increase individual producers' happiness above and beyond the monetary rewards from these sales (i.e., the sales-as-signal effect). H2:  The sales-as-signal effect is mediated by feelings of self-validation as a producer. Moderators of the Sales-as-Signal EffectThe strength of a signal is determined by the extent to which receivers interpret the signal as credible ([ 5]). Signals are perceived as more credible the more they are able to provide information about products' unobservable quality ([ 6]; [ 8]) and the higher the costs and associated risk in sending a signal ([ 3]; [23]). Accordingly, we predict that the strength of the sales signal will depend on at least two characteristics of the purchase transaction: ( 1) the extent to which the product choice reflects a deliberate decision and ( 2) the monetary cost involved in purchasing the product. We decided to focus on these two moderators because they provide an internally valid test of our proposed underlying process (self-validation) and provide actionable implications for the management of online marketplaces. Deliberateness of the product choiceSales should more credibly inform individual producers about their competencies the more the sales are a direct consequence of the quality of individual producers' products ([ 6]; [ 8]). Thus, the sales-as-signal effect should be stronger the more the buyer is seen as making a deliberate decision to acquire the product. That is, sales should be self-validating when the buyer intentionally chooses a producer's product, but much less so when the product was selected in a way that does not reflect the buyer's preference for a specific product. Examples of the latter include a chef's choice item on a menu, a surprise wine box subscription, the specific vegetables offered by a community-supported agriculture farm co-op, a ""mystery car"" car rental option, a sneak preview at a movie theatre, or a sweepstakes in which it is not clear in advance which participant will receive which prize. We hypothesize that, above and beyond the monetary rewards from sales, individual producers will feel greater self-validation, and thus happiness, the more the purchase appears to be the result of a buyer's deliberate decision. H3:  The sales-as-signal effect is stronger when the product purchase reflects a more (vs. less) deliberate decision. Cost of the purchaseIf signal credibility is a function of signal cost ([ 3]; [23]), varying the cost of buying a product should alter the strength of the sales-as-signal effect. The higher the costs involved in purchasing a product, the more credibly the sales signal should inform individual producers about their competencies as a producer—even if the higher costs do not translate into higher monetary rewards for the individual producer (such as when the buyer bears higher shipping costs). We hypothesize that individual producers will feel higher levels of self-validation, and thus happiness, when the buyer incurs higher monetary costs, even when the higher monetary costs do not lead to higher monetary income for the individual producer. H4:  The sales-as-signal effect is stronger when buyers incur higher (vs. lower) monetary costs. Sales Versus Other Forms of Market SignalsSales are not the only signals consumers might send. In addition to buying products, consumers may also signal quality through noneconomic signals, such as writing a review or liking a product or company on social media. One might argue that such noneconomic signals might have a stronger self-validating effect than sales because the noneconomic signals are sent intentionally (vs. being a usually unintentional by-product of the decision to buy). However, we propose that sales would be a more credible signal because they may be seen as more informative about the product's unobservable quality for several reasons.First, other forms of signals such as online reviews have been criticized for being unable to reveal a product's actual quality ([12]), and public displays of support for a cause on social media (e.g., Facebook likes) are often unreliable indicators of one's willingness to support the cause when doing so is costly ([26]). Thus, noneconomic signals such as likes may be seen as ""cheap talk."" In contrast, customers should decide to purchase a product only if they really deem the product to be of high, or at least sufficiently high, quality ([14]). Second, selling products may evoke the specific norms that are associated with an exchange domain rather than a relational domain. In an exchange domain, the normative signal of value may be sales rather than more relational signals such as likes.[ 8] In this domain, having buyers purchase one's product might be the ""ultimate"" form of appreciation of an individual's competencies as a producer. Thus, sales may more credibly inform individual producers about their skills and competencies compared with other common forms of market signals such as likes, even when the cost to the customer of sales and likes are kept the same. We hypothesize that individual producers will feel greater self-validation, and thus happiness, from sales than from noneconomic signals, even when sales do not lead to higher monetary rewards to the seller than noneconomic signals (and even when monetary cost to the customer is kept constant). H5:  Sales increase individual producers' happiness more than noneconomic signals above and beyond the monetary rewards from those sales to the producer (and above and beyond the monetary cost of sending those signals). Happiness from Selling Self-Made Versus Other-Made ProductsWe further propose that individuals derive greater happiness from sales when selling self-made products as opposed to selling products made by someone else. Our hypothesis is that the effect of sales of self-produced goods on happiness is, to an important extent, driven by validation of the producer's skills and competencies as a producer. Of course, selling self-made products might also validate individual producers' skills as a seller; that is, successful sales may be interpreted as a signal that the seller of self-produced products is a competent marketer rather than a competent producer. However, unlike selling one's self-made products, selling products made by others cannot validate one's skill as a producer. Thus, the effect of sales on happiness should be larger for self-produced than for other-produced products. H6:  The sales-as-signal effect is stronger among individuals selling self- (vs. other-) made products. Overview of StudiesWe tested our propositions in eight studies (N = 4,970). Study 1 and a supplementary study provide an initial exploration of our main hypothesis that sales increase individual producers' happiness above and beyond the monetary rewards from these sales (H1). In Study 1, we surveyed actual producers selling their self-made products (e.g., on Etsy). We find a positive relationship between sales and individual producers' happiness, controlling for the monetary implications of sales. In a supplementary study, we replicated this finding experimentally using a recall task among a sample of actual producers.In Studies 2–6, we provide further evidence for the core sales-as-signal effect, explore the underlying mechanism, examine boundary conditions, and investigate the incremental effects of sales among different samples of producers. Studies 2 and 3 show that the sales-as-signal effect is driven by feelings of self-validation (H2). These studies also show that the strength of the sales-as-signal effect depends on ( 1) the extent to which the product choice reflects a deliberate decision (H3) and ( 2) the monetary cost involved for the consumer in purchasing the product (H4). Study 4 and a supplementary study demonstrate that sales increase individual producers' happiness more than receiving likes (H5). Study 5 shows that the effect of sales on happiness is stronger when individuals sell self-made products than when they sell products made by others (H6). Finally, Study 6 shows that the sales-as-signal effect is different from the mere happiness individuals gain from producing. Study 6 also shows that trying to sell one's self-produced products can backfire when low sales become a signal of low competency. Study 1Study 1 provides an initial exploration of our core prediction that selling their products increases producers' happiness over and above any monetary rewards from those sales. We do so through a field survey of actual producers selling their self-made products online. MethodTo engage this special and difficult-to-recruit population (individual producers), we worked with the administrators of eight Facebook groups of producers of handmade goods to promote our survey. As an incentive to participate, each participant received a $5 Amazon voucher (see WA-B1 for detailed study materials). After agreeing to a data protection disclaimer, we told participants that the aim of the study was to gather knowledge about their life as producers of handmade goods. To increase the comparability of the responses, we asked participants to think about the last four weeks when answering the questions.The survey first captured our dependent variable, participants' satisfaction and happiness with their life as a producer of handmade products, which we measured with two items (r = .61; see survey scales in Table 1). We measured our key independent variable, one's current sales (i.e., the number of products sold), using two variables. First, we assessed the sales volume by asking participants to indicate the total number of items sold in the past four weeks. Second, we assessed sales growth by capturing how many items a given producer sold in the last four weeks compared to the average number of items sold per month in the last six months. The comparative nature of the measure is important because it is the within-person variance that most strongly predicts one's happiness at any given point in time ([35]).GraphTable 1. Survey Scales (Study 1). VariableMeasuresHappiness""If you think about the past four weeks, how satisfied are you with your life as a producer of handmade products?"" (1 = ""extremely dissatisfied,"" and 7 = ""extremely satisfied"") and ""if you think about the past four weeks, how happy are you with your life as a producer of handmade products?"" (1 = ""extremely unhappy,"" and 7 = ""extremely happy"")Sales volume""Over the past four weeks combined, how many handmade items did you sell?""Sales growth""Now, please compare the number of items that you sold in the past four weeks with the average number of items you sold per month in the past six months. Would you say that you sold more or fewer items in the past four weeks compared to the months before?"" (1 = ""much fewer,"" and 7 = ""much more"")Revenue""What was the total revenue on these items in the past four weeks? That is, how much money did you make by selling these items in the past four weeks?""Profit""How much profit did you make in the past four weeks by selling these items? That is, after subtracting all costs, how much money were you able to keep in your pockets?""Future profit expectations""If you think about the near future, how do you think your profits from selling your handmade products will develop?"" (1 = ""decrease a lot,"" and 7 = ""increase a lot"")Socioeconomic status""I don't think I'll have to worry about money too much in the future,"" ""I don't need to worry too much about paying my bills,"" and ""I have enough money to buy things I want"" (1 = ""totally disagree,"" and 7 = ""totally agree"") We included the following control variables to empirically isolate the sales-as-signal effect from a series of alternative explanations. We captured the direct monetary implications of sales by asking for the respective revenue and profit (in USD) in the said time period.[ 9] Although these measures are important for assessing our hypothesis (i.e., to control for any monetary implications of selling), they do not account for any future profit expectations. For example, one could argue that a positive sales trend in the current period might be (perceived to be) diagnostic of future sales and thus profit developments. Thus, a given happiness level at a given point in time might be due to future profit expectations based on the comparative number of items sold. To account for this alternative explanation, we asked participants how they expected their future profits to develop.GraphTable 2. Ordinary Least Squares Regressions on Happiness (Study 1). (1a)(1b)(2a)(2b)(3a)(3b)(4a)(4b)Ln(sales volume).06**(.03).05**(.03).07***(.02).06***(.02)Sales growth.34***(.02).34***(.02).19***(.03).19***(.03)Ln(revenue)−.01(.02).002(.02).01(.01).01(.02)Ln(profit)−.002(.02).01(.02).01(.01).02(.02)Future profit expectations.23***(.03).23***(.03).15***(.03).15***(.03)Ln(experience).10**(.05).10**(.05).13***(.05).13***(.05)Main job.07(.09).06(.09).10(.09).10(.09)Proportionate household income−.004**(.002)−.004**(.002)−.003*(.002)−.004**(.002)Socioeconomic status.28***(.03).28***(.03).21***(.03).21***(.03)Age.01(.004).005(.004).01(.004).01(.004)Gender−.07(.08)−.08(.08)−.02(.08)−.02(.08)Education controlsNoNoYesYesNoNoYesYesProduct category controlsYesYesYesYesYesYesYesYesObservations828828828828828828828828R2.07.07.32.32.25.25.35.35 1 Notes: Unstandardized regression coefficients. Standard errors in parentheses. Education controls: bachelor's degree is baseline. Product controls: jewelry is baseline. Main job: 0 = side job, 1 = main job. Gender: 0 = female, 1 = male.2 *p <.10.3 **p <.05.4 ***p <.01.In addition, we captured a series of control variables with regard to the business type and the producer. We captured the product domain(s) by asking what type(s) of products they sell (see WA-B2). We further asked participants about their experience as a producer (how long they have been selling their products [in months]), whether selling these products is their main or side job (0 = ""side job,"" and 1 = ""main job""), and how much the income from selling these products contributes to their total household income (in %). To assess each individual producer's socioeconomic status, we used a three-item scale (α = .82; [20]). Finally, participants indicated their gender (0 = ""female,"" and 1 = ""male""), age (in years), country of residence (0 = ""United States,"" and 1 = ""other""), relationship status (0 = ""single,"" 1 = ""committed relationship,"" 2 = ""married,"" and 3 = ""widowed""), and highest degree of education (see WA-B3). No further measures were taken. Sample DescriptionThe sample consisted of 828 individual producers (Mage = 35.22 years, SD = 8.43, 61.0% female; 90.6% U.S. residents). We successfully recruited a diverse sample of producers. Participants differed widely in their experience selling their products (M = 56.89 months, SD = 40.59, range: 0 to 360.00), revenue and profit (revenue: Mdn = US$4,100.00, M = US$44,904.99, SD = US$81,115.39, range: US$0 to US$520,000.00; profit: Mdn = US$2,000.00, M = US$14,545.44, SD = US$27,234.03, range: US$0 to US$350,000.00), percentage of household income from selling (M = 54.94%, SD = 34.51%, range: 0% to 100%), and types of products produced (see WA-B4). Because several of our open-ended measures were highly skewed (sales volume, revenue, profit, and experience), we log-transformed those variables (for descriptive statistics and interconstruct correlations, see WA-B5). ResultsWe tested the sales-as-signal effect by estimating a series of ordinary least squares (OLS) regression models accounting for different sets of control variables. Columns 1a and 1b in Table 2 report the results of regression models in which one's happiness as a producer, our dependent variable, is regressed on sales volume while controlling for product type(s) and for the economic implications of sales (in terms of either revenue or profit). We find that sales volume is indeed positively and significantly related to one's happiness, controlling for economic implications (Column 1a: b = .06, SE = .03, p = .02; Column 1b: b = .05, SE = .03, p = .04). This effect persists, and even gets a bit stronger, after adding all further control variables (see Column 2a: b = .07, SE = .02, p <.01; Column 2b: b = .06, SE = .02, p <.01).We ran the same models using sales growth as the independent variable, with consistent results. Columns 3a and 3b in Table 2 show that sales growth is positively and significantly related to producers' happiness when controlling for product domain(s) and producers' revenue (b = .34, SE = .02, p <.01) or profit (b = .34, SE = .02, p <.01). The effect remains positive and significant when adding all further control variables (see Column 4a: b = .19, SE = .03, p <.01; Column 4b: b = .19, SE = .03, p <.01). DiscussionUsing a survey of actual producers, we find real-world evidence for the sales-as-signal effect (H1): individual producers draw happiness from selling their self-made products above and beyond the money they make from these sales.In a supplementary study (see Study 1S in WA-C), we examined whether the observed relationship between sales and happiness, controlling for monetary considerations, holds in a more controlled setting with random assignment. We tested this using actual producers from the Australian marketplace madeit.com.au (n = 169) by making them recall a period of time in which they sold more (vs. fewer) products than normal and asking them about their happiness as a producer at that point in time. The results of this experiment replicate the main finding of Study 1. Consistent with our theorizing, we find that individual producers are happier at periods of time in which they sell more (vs. fewer) products even when controlling for the profit they made from these sales. In addition, the effect is robust to another potential happiness driver: future profit expectations. Study 2Study 2 aims to provide a behavioral test of causality for the effect of sales on happiness through feelings of self-validation (H2). We did this by conducting a two-stage behavioral experiment in which we first asked participants to actually produce their own products and then manipulated at a later stage whether their products were sold.Another aim of Study 2 was to investigate how the extent of deliberateness of product choice moderates the sales-as-signal effect. We theorize that sales credibly inform individuals about their skills and competencies as producers to the extent that they provide information about the quality of the producers' products (H3). When sales do not reflect a deliberate decision, for example when a product is chosen at random, they are less informative about the buyer's product quality perceptions and thus the sales-as-signal effect should be reduced. MethodWe invited 1,700 American workers from Amazon Mechanical Turk (MTurk) in two waves[10] to participate in a two-stage study. We recruited participants interested and skilled in drawing. The experiment employed a 2 (sales: product sold vs. product not sold) × 2 (choice: deliberate vs. random) between-participants design. We informed all participants that, to enrich life in times of crisis, a researcher was delegated by the university's program director to organize an exhibition of comics that symbolize the defeat of the coronavirus (SARS-CoV-2). We told participants that their task would be to draw a picture of a superhero conquering the coronavirus, that all drawings would be shown to potential customers (i.e., faculty and administrative staff at the university) who could purchase one drawing for $1.50, and that all purchased drawings would be exhibited. To ensure that participants only submitted their original work, we told participants to sign their hand-drawn picture with their MTurk ID. Finally, we told participants that they would receive a second survey about one week later that would inform them about whether their drawing was sold. Importantly, to keep the monetary rewards constant, we truthfully told participants that all participants would receive $1.50 as a compensation for their work—irrespective of whether their drawing was sold. We received 417 valid drawings (see Figure 1 for a selection of drawings; invalid submissions included blank submissions, pictures that were unrelated to the task, and unsigned submissions).Graph: Figure 1. Examples of drawings of superheroes defeating the coronavirus (Study 2).Approximately one week later, we invited workers who had submitted valid drawings to participate in the second part of this study, which included our manipulations. A total of 347 workers accepted this invitation (Mage = 33.39 years, SD = 11.15, 53.0% female). We first reminded all participants that the researcher showed all drawings to potential customers (i.e., faculty and administrative staff at the university), who could purchase one drawing for $1.50. In addition, participants were reminded that all artists would receive $1.50 as compensation for their work, irrespective of whether their drawing was sold. Then, participants either read that each customer who decided to purchase a drawing selected the one they wanted (deliberate choice) or that each customer who decided to purchase a drawing received one selected at random (random choice). In addition, participants read that their specific drawing either was or was not sold (see WA-D for study materials). Participants then indicated their level of happiness as a producer on two seven-point items (""extremely unhappy/extremely happy,"" ""extremely dissatisfied/extremely satisfied""; r = .87) and their feelings of self-validation on three seven-point items (""not at all skilled/extremely skilled,"" ""not at all competent/extremely competent,"" and ""not at all talented/extremely talented""; α = .95).[11] Finally, participants completed two attention checks (""was your drawing sold?""; ""yes, my drawing was sold/no, my drawing was not sold""; ""how did the purchase of drawings happen?""; based on customers' deliberate/random choice [the 'random selector' picked the drawing]/I cannot remember)[12] and indicated their age and gender. Results HappinessA 2 × 2 ANOVA on happiness reveals a significant main effect of sales (Msold = 6.03, SD = 1.47 vs. Mnot_sold = 3.67, SD = 1.27; F( 1, 343) = 263.27, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.81, SD = 1.91 vs. Mrandom = 4.93, SD = 1.70; F( 1, 343) = .65, p = .42). Importantly, we also obtained the expected significant interaction effect (F( 1, 343) = 11.20, p = .001; see Figure 2, Panel A). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on happiness was significantly stronger (Msold = 6.21, SD = 1.17 vs. Mnot_sold = 3.37, SD = 1.38; F( 1, 343) = 194.37, p <.001) than when the product choice was random (Msold = 5.84, SD = 1.34 vs. Mnot_sold = 3.98, SD = 1.50; F( 1, 343) = 81.75, p <.001).Graph: Figure 2. Happiness and self-validation as a function of product choice and sales (Study 3).In addition, we find that participants whose products were sold were happier when the focal decision was deliberate versus random (Mdeliberate = 6.21, SD = 1.17 vs. Mrandom = 5.84, SD = 1.34; F( 1, 343) = 8.50, p = .004). Interestingly, for participants who learned that their products were not sold, happiness was marginally higher when the focal decision was random rather than deliberate (Mdeliberate = 3.37, SD = 1.38 vs. Mrandom = 3.98, SD = 1.50; F( 1, 343) = 3.28, p = .07). Self-validationA 2 × 2 analysis of variance (ANOVA) on self-validation reveals a significant main effect of sales (Msold = 4.97, SD = 1.32 vs. Mnot_sold = 3.44, SD = 1.55; F( 1, 343) = 99.45, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.17, SD = 1.71 vs. Mrandom = 4.26, SD = 1.53; F( 1, 343) = .34, p = .56). Importantly, we also obtained a significant interaction effect (F( 1, 343) = 12.23, p = .001; see Figure 2, Panel B). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on self-validation was significantly stronger (Msold = 5.19, SD = 1.29 vs. Mnot_sold = 3.14, SD = 1.46; F( 1, 343) = 92.04, p <.001) than when the product choice was random (Msold = 4.74, SD = 1.31 vs. Mnot_sold = 3.76, SD = 1.60; F( 1, 343) = 20.67, p <.001).In addition, we find evidence that participants whose products were sold reported higher feelings of self-validation when the product choice was deliberate (Mdeliberate = 5.19, SD = 1.29 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 8.18, p = .004), whereas participants whose products were not sold reported higher feelings of self-validation when the product choice was random (Mdeliberate = 3.14, SD = 1.46 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 4.32, p = .04). Although not the focus of our theorizing, the latter difference further validates our signaling framework, as it indicates that a negative sales signal is more detrimental to feelings of self-validation when it is more easily interpreted as a reflection of one's competencies. Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000) with sales (0 = not sold, 1 = sold) as the independent variable, product choice (0 = deliberate, 1 = random) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = −.48, SE = .16, CI95% = [−.81, −.19]). Supporting our prediction, the effect of sales on happiness through feelings of self-validation was significantly stronger when the product choice was deliberate (b = .93, SE = .15, CI95% = [.66, 1.23]) versus random (b = .45, SE = .11, CI95% = [.23,.67]). DiscussionUsing a multiwave experimental paradigm involving actual production, Study 2 provides causal evidence in support of our primary prediction that, above and beyond the monetary reward, sales increase producers' happiness via elevated feelings of self-validation (H2). In addition, the results are consistent with our theorizing that sales have a stronger impact on individual producers' self-validation when product choice is more deliberate (H3).Although significantly smaller (as hypothesized), we also find a residual effect of sales on happiness and, to a lesser extent, self-validation when products were sold but selected at random. This finding is beyond the scope of our hypotheses, so we can only speculate about why even a random sale might make producers happy. One possibility is a process identified by Marx (1844/1993; see also [43]) in his Comments on James Mill. Marx's discussion of what it is like to produce as a human being (rather than being a cog in a machine), suggests that producing something that is used and enjoyed by another person provides important enjoyment of life and, to some extent, affirms the producer's unique competency as a person. Thus, the mere fact that another person has acquired one's product, even if the exact product choice was made at random, may provide some basic sense of self-validation and happiness in turn. Study 3Study 3 investigates another moderator of the sales-as-signal effect: the monetary cost involved in purchasing the product. We theorize that sales credibly inform individuals about their skills and competencies as a producer to the extent that the acquisition of the product is costly to the customer (H4). In this study, we manipulated the monetary cost of sales by varying the shipping cost that a buyer needed to pay to acquire the product. We predicted that, although the financial gain from the sale is constant (the buyer bears the shipping costs), individual producers would be happier when the buyer accepts paying higher (vs. lower) shipping costs. As in the previous study, we again tested whether the increase in happiness can be explained by feelings of self-validation (H2). MethodWith the help of hobbii.com, an online shop that sells knitting kits and supplies, we recruited 1,230 recreational knitters (Mage = 53.38 years, SD = 12.26, 99.4% female). The company promoted a link to our study in their weekly newsletter, which was received by German-speaking customers (and which yielded a response of N = 818) as well as customers from the United States (N = 412). As an incentive to participate, we raffled ten gift cards to the company's online shop worth $30 each.The experiment employed a between-participants design with three conditions (sales: baseline vs. higher cost vs. lower cost). We asked all participants to imagine marketing their self-made knitted accessories on an online platform. Specifically, as we ran this study in June and participants came from Europe and the United States, we asked participants to assume they currently produce and sell summer beanies (i.e., beanies that are made from thin, lightweight material). Next, participants read that they received an email from a customer in New Zealand asking about winter beanies. Participants then read that they were able to offer their self-made winter beanies for $30.00. Finally, participants either read that the customer from New Zealand did not respond to this offer (baseline), decided to purchase the beanie for $30.00 plus $20.90 shipping costs (higher cost), or decided to purchase the beanie for $30.00 plus $2.90 shipping costs (lower cost; for study materials, see WA-E1).Participants then indicated their level of happiness (r = .87) and feelings of self-validation (α = .93) on the same scales as in Study 2. To account for alternative mechanisms, participants indicated how much profit they made with the customer from New Zealand (1 = ""none,"" and 7 = ""a lot"") and how they thought their profit from selling products would develop in the near future (1 = ""decrease a lot,"" and 7 = ""increase a lot""). Participants further completed the following attention check: ""Did the customer from New Zealand buy your beanie?"" (yes/no). Participants in the lower cost and higher cost conditions additionally completed the following attention check: ""How much did it cost to ship the beanie to New Zealand?"" ($2.90/$20.90).[13] Finally, participants indicated their gender and age.[14] Results HappinessA one-way ANOVA with happiness as the dependent variable produces a significant effect (F( 2, 1,227) = 406.52, p <.001). Follow-up contrasts reveal that, compared to the baseline condition in which the customer from New Zealand did not purchase the product (M = 3.13, SD = 1.10), participants were happier when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.90, SD = 1.38; t( 1,227) = 27.93, p <.001; lower shipping cost: M = 5.03, SD = 1.72; t( 1,227) = 19.03, p <.001). More importantly, participants reported significantly higher levels of happiness when the buyer paid higher versus lower shipping costs (t( 1,227) = 8.79, p <.001). Self-validationA one-way ANOVA with feelings of self-validation as the dependent variable produces a significant effect (F( 2, 1,227) = 160.52, p <.001). Follow-up contrasts reveal that, compared with the baseline condition (M = 4.09, SD = 1.27), participants reported greater self-validation when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.46, SD = .98; t( 1,227) = 17.43, p <.001; lower shipping cost: M = 5.07, SD = 1.12; t( 1,227) = 12.38, p <.001). In support of our theorizing, we further find significantly higher feelings of self-validation in the case of higher (vs. lower) shipping costs (t( 1,227) = 4.98, p <.001). MediationWe conducted mediation analyses ([21], Model 4, n = 5,000) with our multicategorical independent variable, happiness as the dependent variable, and self-validation as the mediator. We find positive and significant indirect effects on happiness through self-validation when comparing ( 1) the higher shipping cost condition with the baseline condition (b = .78, SE = .07, CI95% = [.65,.91]), ( 2) the lower shipping cost condition with the baseline condition (b = .55, SE = .06, CI95% = [.45,.67]), and ( 3) the higher shipping cost condition with the lower shipping cost condition (b = .22, SE = .05, CI95% = [.14,.31]). These results are robust to the inclusion of current profits and future profit expectations as covariates (for detailed results, see WA-E2). DiscussionStudy 3 further corroborates our theorizing by showing that the extent to which sales provide self-validation, over and above monetary outcomes for the producer, depends on the monetary cost of the sales signal. In particular, participants reported greater self-validation and thus happiness when the buyer accepted to pay higher (vs. lower) shipping costs (H4). Study 4Study 4 compares the effect of sales with that of a noneconomic signal. We focus on comparing sales with likes, which are arguably the most common form of market signals on electronic platforms such as Etsy. Comparing the effects of sales and likes is also important from a practical point of view because the seller dashboards of prominent online platforms tend to display sales and likes (or related forms of noneconomic signals such as favorites) concurrently, raising the question of how these different forms of signals impact individual producers' happiness. We hypothesize that individual producers experience greater self-validation and thus more happiness from sales than from likes, even above and beyond the monetary rewards from these sales (H5). We tested this by conducting another two-stage behavioral experiment in which we first asked participants to actually produce products and then manipulated at a later stage whether their products were either acquired or liked by customers. To test whether the happiness advantage of sales over likes goes beyond their cost difference to the customer, Study 4 kept cost to the customer (along with the monetary outcomes for the producer) constant across signals. MethodWe invited 1,000 American workers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. The experiment employed a 2 (signal: sales vs. likes) × 2 (number: high vs. low) between-participants design. All participants were told that their task would be to create a positive slogan for a ""post-COVID"" event that would be taking place at our university. We told participants that we would print each slogan on a poster and exhibit each poster at the event like a gallery exhibition. Furthermore, participants were told that all guests of the event would pay an entrance fee of $1.50 to cover costs and that, in return, each guest would receive a token. In the sales condition, we told participants that guests could use this token to purchase a poster of their choice and that each poster could only be purchased once. In the likes condition, we told participants that guests could use this token to like a poster of their choice by pinning the token on the poster and that each poster could only be liked once. Next, we asked all participants (those in both the sales and likes conditions) to create their slogan by finishing the following sentence: ""When Corona is over....""Two weeks later, we invited those who had submitted valid slogans (N = 1,000) to participate in the second part of the study, yielding 843 participants (Mage = 34.70 years, SD = 12.63, 48.2% female). A chi-squared test revealed that participation in the second part of the study did not depend on whether participants were assigned to the sales or the likes condition in the first part (χ2( 1) = .11, p = .75). First, we thanked all participants for submitting their slogan and reminded them that all slogans were printed on posters exhibited at the ""post-COVID"" event, at which guests could either purchase (sales condition) or like (likes condition) a poster of their choice. In addition, we reminded participants that each poster could only be purchased/liked once. Next, participants in the sales condition were either told that one (high sales) or no (low sales) customer(s) bought the poster with their slogan on it, while participants in the likes condition were either told that that one (high likes) or no (low likes) customer(s) liked the poster with their slogan on it. Next, all participants indicated their level of happiness (r = .91) and their feelings of self-validation (α = .96) on the same scales as in the previous studies. Finally, participants responded to an attention check (""how many guests [purchased/liked] the poster with your slogan on it?""; ""no (0) guest [bought/liked] the poster with my slogan on it/one ( 1) guest [bought/liked] the poster with my slogan on it"")[15] and indicated their gender and age (for study materials, see WA-F). Results HappinessA 2 × 2 ANOVA on happiness produces significant main effects of signal (Msales = 4.43, SD = 1.64 vs. Mlikes = 4.21, SD = 1.60; F( 1, 839) = 7.28, p = .007) and number (Mhigh = 5.37, SD = 1.27 vs. Mlow = 3.28, SD = 1.20; F( 1, 839) = 610.80, p <.001). More importantly, we obtained the predicted signal by number interaction (F( 1, 839) = 9.23, p = .002; see Figure 3, Panel A). Planned contrasts reveal that the effect of the sales signal was significantly stronger (Mhigh = 5.61, SD = 1.17 vs. Mlow = 3.27, SD = 1.11; F( 1, 839) = 386.45, p <.001) than the effect of the likes signal (Mhigh = 5.13, SD = 1.32 vs. Mlow = 3.30, SD = 1.29; F( 1, 839) = 234.11, p <.001). In addition, we find that participants whose poster was sold were happier than participants whose poster was liked (F( 1, 839) = 16.36, p <.001). We detected no such differences between participants whose poster was not sold versus not liked (F( 1, 839) = .06, p = .81).Graph: Figure 3. Happiness and self-validation as a function of signal and value (Study 4). Self-validationA similar 2 × 2 ANOVA on feelings of self-validation produces a significant main effect of signal (Msales = 3.62, SD = 1.48 vs. Mlikes = 3.41, SD = 1.50; F( 1, 839) = 5.44, p = .02) and a significant main effect of number (Mhigh = 4.05, SD = 1.40 vs. Mlow = 3.00, SD = 1.38; F( 1, 839) = 121.20, p <.001). This main effect was qualified by a significant interaction effect (F( 1, 839) = 7.36, p = .007; see Figure 3, Panel B), demonstrating that the effect of the sales signal was significantly stronger (Mhigh = 4.29, SD = 1.29 vs. Mlow = 2.98, SD = 1.37; F( 1, 839) = 94.48, p <.001) than the effect of the likes signal (Mhigh = 3.81, SD = 1.47 vs. Mlow = 3.02, SD = 1.39; F( 1, 839) = 34.29, p <.001). In addition, participants whose poster was sold felt more validated than participants whose poster was liked (F( 1, 839) = 12.66, p <.001). Feelings of self-validation did not differ between participants whose poster was not sold versus not liked (F( 1, 839) = .07, p = .79). Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000 bootstraps) with number (0 = low, 1 = high) as the independent variable, signal (0 = likes, 1 = sales) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = .25, SE = .09, CI95% = [.07,.44]). As expected, the mediating effect through self-validation on happiness was stronger when participants' posters were sold versus not sold (b = .63, SE = .08, CI95% = [.48,.78]) than when participants' posters were liked versus not liked (b = .38, SE = .07, CI95% = [.25,.51]). DiscussionStudy 4 compared the effects of sales and likes regarding their impact on individual producers' feelings of self-validation and happiness as a producer (H5). In support of our theorizing, we find that sales produce stronger effects on happiness and self-validation than likes, even when the associated monetary costs were kept constant between conditions. In a supplementary study (see Study 4S in WA-G), we tested the effects of sales and likes when presenting information about sales and likes simultaneously (mimicking the situation on online platforms such as Etsy) among a sample of recreational knitters (n = 161). Holding the monetary rewards to the producer (but not the costs to the customer) from sales and likes constant, we again find that sales make individual producers happier than likes and that this effect is driven by feelings of self-validation. Study 5Study 5 tests whether the magnitude of the sales-as-signal effect depends on whether individuals sell their self-made products or products that were made by someone else. We tested this by varying sales of products that were self-produced versus produced by someone else. Although selling more versus fewer products should increase individuals' happiness in both situations, we predict an incremental increase in happiness from selling self-made products (H6). MethodParticipants included 1,008 U.S. consumers recruited from MTurk (Mage = 39.81 years, SD = 12.41, 47.6% female). The experiment employed a 2 (sales: high vs. low) × 2 (product: self-made vs. other-made) between-participants design. All participants imagined selling muffins at a local food market. The muffins were either made by themselves (self-made condition) or by someone else (other-made condition). To keep the monetary rewards constant across conditions, we told participants that the organizers of the food market receive all sales revenue from people selling products at the market for the first time (which is how the food market finances itself). We further informed participants that there are no other costs involved in being able to sell products at the food market. Thus, across conditions, the monetary reward from potential sales was zero. Below this description, participants responded to an attention check verifying they understood that revenue from their first-time sales would go to the food market. Next, participants either read that they made (self-made condition) or received (other-made condition) a total of 50 muffins to sell and that they sold either 36 (high sales condition) or six (low sales condition) of them at the food market.After reading this information, participants indicated their level of happiness on the same scale as in the previous studies (r = .92). Next, participants completed two more attention checks (""which of the following statements is correct?""; ""at the food market, I sold muffins that were made by someone else/at the food market, I sold muffins that I made myself""; ""how many muffins were sold at the food market?""; 6/36).[16] Finally, participants indicated their gender and age (for complete study materials, see WA-H). ResultsA 2 × 2 ANOVA on happiness reveals the expected significant main effect of sales (Mhigh_sales = 4.53, SD = 1.54 vs. Mlow_sales = 2.59, SD = 1.49; F( 1, 1,004) = 418.73, p <.001) and a nonsignificant main effect of product (Mself-made = 3.58, SD = 1.83 vs. Mother-made = 3.55, SD = 1.78; F( 1, 1,004) = .18, p = .67). Importantly, we also obtained a significant interaction effect (F( 1, 1,004) = 19.65, p <.001; see Figure 4). As hypothesized, planned contrasts reveal that the effect of sales on happiness was significantly stronger when participants sold their self-made muffins (Mhigh_sales = 4.77, SD = 1.42 vs. Mlow_sales = 2.41, SD = 1.37; F( 1, 1,004) = 310.54, p <.001) than when participants sold muffins made by someone else (Mhigh_sales = 4.31, SD = 1.62 vs. Mlow_sales = 2.79, SD = 1.59; F( 1, 1,004) = 128.21, p <.001).Graph: Figure 4. Happiness as a function of product and sales (Study 5).Importantly, we find that participants who sold more muffins were happier when they made the muffins themselves versus someone else making the muffins (Mself-made = 4.77, SD = 1.42 vs. Mother-made = 4.31, SD = 1.62; F( 1, 1,004) = 11.85, p = .001). In contrast, participants who sold fewer muffins were happier when the muffins were made by someone else versus by themselves (Mself-made = 2.41, SD = 1.37 vs. Mother-made = 2.79, SD = 1.59; F( 1, 1,004) = 8.00, p = .005). DiscussionThe results of Study 5 complement the previous studies by demonstrating that sales have a stronger effect on individuals' happiness when individuals sell their self-made products than when they sell products that were made by someone else (H6). This finding provides additional evidence that the effect of selling self-produced products on happiness is driven by the effect of sales on self-validation as a competent producer, and it rules out the alternative explanation that the effect is driven solely by the effect of sales on self-validation as a competent seller or marketer. Study 6Study 6 further tests whether selling their self-made products has any incremental effects on producers' happiness compared with merely producing products. Because sales function as a credible signal regarding producers' skills and competencies, we expect that sales affect individual producers' self-validation and thus happiness beyond individual producers' self-validation and happiness derived from production. We tested this with another two-stage behavioral experiment in which we again asked participants to produce their own products and manipulated at a later stage whether their products were sold. In addition, we added a control condition in which participants' products were not offered for sale, and we assessed participants' feelings of self-validation and happiness derived from both production and selling. Doing so allowed us to assess any increase in self-validation and happiness as a result of a successful sale or any backfiring effect in the case of no sales, compared with the baseline of ""not going to the market"" to begin with. MethodWe invited 500 U.S. consumers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. In the study description, all participants were informed that we would ask them to generate a slogan followed by a short survey and that they would receive a second survey in about two weeks. The experiment employed a 2 (stage: production vs. selling) × 3 (sales: control vs. product sold vs. product not sold) mixed design with stage as the within-subjects factor and sales as the between-subjects factor. In the production stage, we told all participants that their task would be to create a positive slogan one could print on a T-shirt that describes what they will do when the COVID crisis is over. Participants in the sales conditions were additionally told that all slogans from this study would be offered to our university community and that, if a given slogan finds a customer, that slogan will be printed on a T-shirt for that customer. We further informed participants in the sales conditions that each slogan will only be printed on a T-shirt once and that the price the customer will pay for the T-shirt will equal the cost incurred in having it produced. We also told participants in the sales conditions that the second survey would inform them about whether the T-shirt with their slogan had been purchased by a customer. Next, we asked all participants (those in the control and in the two sales conditions) to create their slogan by finishing the following sentence: ""When Corona is over...."" After creating their slogan, all participants indicated their level of happiness (r = .78) and their feelings of self-validation (α = .96) on the same scales used in the previous studies. Finally, we reminded all participants that they would receive a second survey in about two weeks.Two weeks later, we invited participants who had submitted valid slogans (N = 499) to participate in the second part of the study. A total of 384 participants accepted this invitation (Mage = 31.56, SD = 10.51, 45.6% female).[17] First, we thanked all participants for submitting their T-shirt slogan two weeks earlier. Participants in the sales conditions were additionally reminded that all slogans were offered to our university community and that the T-shirt with their slogan on it could only be purchased once. Next, we informed participants in the sales conditions that their T-shirt was either sold or not. Finally, all participants indicated their level of happiness (r = .85) and their feelings of self-validation (α = .97) on the same scales as in the production stage, as well as their gender and age (for study materials, see WA-I). Results HappinessA 2 × 3 mixed ANOVA on happiness with stage (production vs. selling) as the within-subject factor and sales (control vs. product sold vs. product not sold) as the between-subjects factor reveals the expected significant stage by sales interaction (F( 2, 381) = 127.47, p <.001; see Figure 5, Panel A). Participants' happiness did not differ across the sales conditions in the production stage (Mcontrol = 5.19, SD = 1.12 vs. Msold = 5.17, SD = 1.21 vs. Mnot_sold = 5.39, SD = 1.15; F( 2, 381) = 1.34, p = .26). In contrast, participants' happiness significantly differed across the sales conditions in the selling stage (F( 2, 381) = 157.63, p <.001). As expected, participants were happier when a T-shirt with their slogan on it was sold (M = 5.87, SD = 1.02) than when a T-shirt with their slogan on it was not sold (M = 3.37, SD = 1.17; t(381) = 17.47, p <.001). Compared with the control condition (M = 4.97, SD = 1.21), participants were happier when a T-shirt with their slogan on it was sold (t(381) = 6.26, p <.001) and less happy when a T-shirt with their slogan on it was not sold (t(381) = −11.40, p <.001).Graph: Figure 5. Happiness (left) and self-validation (right) as a function of stage and sales (Study 6).Comparing participants' happiness across the two stages reveals that, in the product sold condition, participants were happier in the selling stage (M = 5.87, SD = 1.02) than in the production stage (M = 5.17, SD = 1.21; F( 1, 381) = 30.93, p <.001). In the product not sold condition, participants were less happy in the selling stage (M = 3.37, SD = 1.17) than in the production stage (M = 5.39, SD = 1.15; F( 1, 381) = 279.11, p <.001). In the control condition, participants were marginally less happy in the selling stage (M = 4.97, SD = 1.21) than in the production stage (M = 5.19, SD = 1.12; F( 1, 381) = 3.38, p = .07). The main effect of stage was also significant (Mselling = 4.72, SD = 1.53 vs. Mproduction = 5.25, SD = 1.58; F( 1, 381) = 53.90, p <.001). Self-validationA similar 2 × 3 mixed ANOVA on self-validation reveals a significant stage by sales interaction (F( 2, 381) = 28.33, p <.001; see Figure 5, Panel B). In the production stage, participants' feelings of self-validation did not differ across the sales conditions (Mcontrol = 3.88, SD = 1.36 vs. Msold = 3.85, SD = 1.32 vs. Mnot_sold = 3.92, SD = 1.52; F( 2, 381) = .09, p = .91). In contrast, participants' feelings of self-validation significantly differed across the sales conditions in the selling stage (F( 2, 381) = 23.71, p <.001). As expected, participants felt more validated when a T-shirt with their slogan on it was sold (M = 4.35, SD = 1.27) than when a T-shirt with their slogan on it was not sold (M = 3.14, SD = 1.61; t(381) = 6.80, p <.001). Compared to the control condition (M = 3.89, SD = 1.35), participants felt more validated when a T-shirt with their slogan on it was sold (t(381) = 2.56, p = .01) and less validated when a T-shirt with their slogan on it was not sold (t(381) = −4.31, p <.001).In the product sold condition, participants felt more validated in the selling stage (M = 4.35, SD = 1.27) than in the production stage (M = 3.85, SD = 1.32; F( 1, 381) = 16.47, p <.001). In the product not sold condition, participants felt less validated in the selling stage (M = 3.14, SD = 1.61) than in the production stage (M = 3.92, SD = 1.52; F( 1, 381) = 42.19, p <.001). In the control condition, participants' feelings of self-validation did not differ between the selling stage (M = 3.89, SD = 1.35) and the production stage (M = 3.88, SD = 1.36; F( 1, 381) = .01, p = .92). The main effect of stage was not significant (F( 1, 381) = 1.61, p = .21). MediationWe conducted two mediation analyses ([21]; Model 4, n = 5,000 bootstraps) to test whether feelings of self-validation can explain the effect of our sales manipulation on happiness. We first looked at participants' ratings of happiness and self-validation obtained in the selling stage. We thus entered sales as the independent variable, self-validation in the selling stage as the mediator, and happiness in the selling stage as the dependent variable into the regression. This analysis produced positive indirect effects when comparing the product sold with the product not sold condition (b = .45, SE = .09, CI95% = [.29,.64]) and the control condition (b = .17, SE = .07, CI95% = [.05,.31]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.28, SE = .08, CI95% = [−.45, −.14]).We next examined whether relative differences in happiness between the selling stage and the production stage can be explained by relative differences in self-validation between the selling stage and the production stage. To do so, we calculated difference scores between happiness and self-validation ratings in the selling stage and the production stage. The respective regression analyses ([21]; Model 4, n = 5,000 bootstraps) produced positive indirect effects when comparing the product sold with the product not sold condition (b = .55, SE = .10, CI95% = [.36,.76]) and the control condition (b = .21, SE = .08, CI95% = [.07,.37]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.34, SE = .08, CI95% = [−.51, −.19]). DiscussionBy including a preselling baseline measure as well as a no-selling control condition, the results of Study 6 confirm there is a positive incremental effect of selling self-produced products on happiness. They also show, however, that marketing self-produced products can have a downside as well. Specifically, there is a happiness penalty to pay when products fail to sell, as the absence of sales leads to negative self-validation; that is, failing to sell products sends a negative signal about one's skills and competencies as a producer, reducing happiness. Thus, deciding to offer their self-made products for sale can enhance but also diminish individuals' happiness. General DiscussionThis research investigates the socioemotional benefits of selling self-made products. Eight studies provide evidence for a sales-as-signal effect: individual producers derive happiness from selling their products above and beyond the money they make from these sales (all studies). This is because sales validate their skills and competencies as a producer (Studies 2–4 and 6). In addition, Study 2 shows that the sales-as-signal effect is more pronounced when the choice mechanism that precedes the purchase is more versus less deliberate. Study 3 demonstrates that the increase in happiness from sales is higher when the buyer incurs higher (vs. lower) monetary costs in purchasing the product, even when this higher cost does not translate into higher financial return for the individual producer. Study 4 demonstrates that individual producers gain more happiness from sales than from receiving likes. Finally, Studies 5 and 6 show that individuals derive greater happiness from high sales of self-made products than from high sales of products that were made by someone else (Study 5) and that individuals show increases in happiness from pre- to post-selling but show decreases in happiness after (vs. before) failing to sell (Study 6). The studies (N = 4,970) span a variety of methodological approaches, designs, and procedures, and they feature different participant populations across three continents and producer communities. Contributions to TheoryOur research makes a number of theoretical contributions. First, previous research has extensively studied the psychological and behavioral consequences of engaging in self-production (e.g., [17]; [31]). This line of research has, however, focused on studying production for oneself or for gift giving. Our work goes a step further and examines a context in which individuals produce for the market; that is, with the aim of selling their creations to others. Our contribution thus lies in shedding light on the psychological consequences of participating in market exchanges. Our findings suggest that sales provide individual producers with a sense of self-validation and, in turn, happiness. We find that selling self-made products affects individuals' happiness beyond the happiness derived from producing the products.Second, we introduce a new perspective on the value of sales. Most models of producer behavior assume that producers are driven solely by a profit-maximization motive ([18]; [36]). Our findings caution against taking a reductionist view of sales by demonstrating that selling one's self-made products can also have important socioemotional value, as sales provide individual producers with self-validation regarding their skills and competencies as a producer. Thus, models of producer behavior may benefit from including the self-validation motive documented in this research.Third, in developing our theory, we conceptualized sales as a signal from buyers. This approach is different from the process investigated by existing research on signaling in marketing and management that conceptualizes signals as actions taken by sellers, who have low uncertainty about the product, to reduce uncertainty about product qualities for buyers ([ 5]; [ 8]; [23]). In our research, we find that sales constitute a signal that is sent by buyers and received by sellers. Moreover, our work suggests that sellers, even of self-produced products, actually do have uncertainty about their own products' qualities that is reduced by quality information from buyers. Thus, the traditional roles about who has information and who is uncertain are to some extent reversed. There may be less information asymmetry than is often assumed, and quality information may be exchanged in both directions. Furthermore, signals are traditionally conceptualized as intentional actions performed by the sender that in turn benefit the sender ([ 8]). Our account, in contrast, suggests that sending a signal can be a more or less incidental by-product (of a purchase) rather than an intentional signal and that the signal benefits the receiver of the signal (i.e., individual producer) by increasing self-validation. In addition, we demonstrate that the strength of a signal depends not only on the signal's costliness but also on its diagnosticity. For example, even when likes and sales are equally costly for the customer, sales may be seen as a more reliable indicator of product quality and producer skill. Likewise, a buyer's deliberate choice to purchase an item makes the sale of that item more diagnostic of producer skill than a sale that involves a buyer randomly picking a specific product. Thus, we believe that the present work offers a novel perspective on signaling.Finally, our research informs the ongoing debate within the marketing discipline regarding the implications of marketing for society ([ 7]). The backdrop of this debate involves widespread concern about the pernicious aspects of a consumer society ([24]), the negative environmental externalities of market exchanges ([25]), and the potentially exploitative nature of common marketing practices ([40]). We find that marketing one's products can actually have an important positive effect by providing individual producers with a significant happiness benefit through self-validation. We thus demonstrate how studying a classic marketing topic from a ""better world"" perspective can yield novel insights about marketing's potential to improve people's well-being. Moreover, marketing scholars studying the implications of marketing on society frequently focus on activities undertaken by marketers at large corporations, and more research is needed to uncover the societal impact of marketing beyond these contexts ([ 7]). Answering this call, we study the socioemotional benefits of individual producers participating in market exchanges. Our work also demonstrates the value of taking a behavioral approach to the study of supply-side behaviors. Despite calls for behavioral marketing research to broaden its focus beyond consumers ([27]; [44]), virtually all research by behavioral marketing scholars currently focuses on consumer behavior. We provide an example of how researchers can tackle important marketing phenomena on the supply side with a behavioral lens. Practical ImplicationsIn addition to these theoretical contributions, our work also provides several practical implications, especially for online marketplaces that focus on producers selling self-made products. First, the finding that sales can increase individual producers' happiness could be leveraged in, for example, the recruitment of prospective sellers by highlighting the socioemotional benefits of selling their products (e.g., ""Be a pro. Sell your tote bags on Etsy!""). Online marketplaces could also highlight socioemotional benefits to existing sellers to maintain motivation and retention. This could be done, for example, by stressing that customers recognize sellers' expertise by paying good money to buy their products (e.g., ""They voted with their wallets to tell you you're a pro"").In addition, our findings yield recommendations for the design of seller dashboards. Study 4 demonstrates that knowing how many people bought their products makes individual producers happier than knowing how many people liked their products. Study 3 suggests that showing how much customers paid might increase motivation, and thus production volume per seller and seller retention, beyond the amount of money the seller made. Therefore, we recommend designing seller dashboards so that the number of people who have made purchases and the average amount paid by customers (including shipping and other fees) are more prominent, rather than highlighting likes or aggregate revenue as is common in such dashboards.Finally, Study 2 shows that sales make individual producers happier when they know that buyers deliberately chose their products (vs. being chosen at random). This finding could be leveraged, for example, by encouraging buyers to leave a comment indicating why they chose that seller's product over other available product choices or by highlighting that customers decided to buy the focal producer's product despite having many other options. Future Research Opportunities Self-validation as a sellerWe hope that our findings will motivate other researchers to explore the under-researched topic of individual producers' motivations, beyond monetary considerations. Specifically, we identify several opportunities for future research. Our findings indicate that the sales-as-signal effect is strongly driven by individual producers' gain in self-validation as a competent producer (e.g., in Studies 2, 4, and 6 sales increased individual producers' happiness even when their self-made products were sold by someone else). This focus on self-validation as a producer was motivated by the fact that sellers on platforms such as Etsy typically spend more than half of their time designing and making their products and only about 10% of their time with marketing and promotion activities ([16]). However, to successfully sell their products, many individual producers not only engage in skilled production activities but also in skilled promotion- or selling-related activities, such as taking pictures of their offerings, pricing their products, maintaining their online appearance, and engaging in advertising on social media platforms. Therefore, sales might, at least for some people and in some contexts, also be an important source of happiness by validating individual producers' skills as a promoter or seller. Study 5 indeed shows that sales also increase the happiness of individuals selling products made by someone else above and beyond the monetary rewards from sales—though to a lesser extent compared to individuals selling their self-made products. Empirically examining the self-validation processes involved in selling would be a worthwhile future research direction. Likewise, while the entrepreneurship literature documents that financial success is related to entrepreneurs' satisfaction ([13]; [32]), future research could investigate whether entrepreneurial success (e.g., firm growth, increase in funding) has a causal effect on entrepreneurs' happiness beyond the related financial gains. In sum, our work provides a fruitful foundation for investigating the socioemotional benefits of participating in market exchanges across business contexts. Moderators of the sales-as-signal effect Characteristics of the purchase transactionOur studies investigated two characteristics of the purchase transaction—the deliberateness of the product choice and the monetary cost of buying—that moderate the strength of the sales signal. However, other important moderators of the sales-as-signal effect likely exist, and future research should expand the nomological network in which the sales-as-signal effect is situated. For example, the strength of the sales signal might depend on the expertise of the buyer. Compared to novices, experts are more capable of assessing products' quality ([ 1]). Having one's products bought by an expert in the respective product category should thus more credibly inform individual producers about their competencies as a producer than having one's products bought by a novice. Similarly, the number of buyers might affect how strongly individual producers perceive sales to correspond to the quality of their products. Would 40 buyers purchasing one product each provide more self-validation than four buyers purchasing ten items each?Besides the direct monetary cost associated with buying a product (Study 3), the sales-as-signal effect might also depend on the relative cost that buyers incur when purchasing a product. We would expect the effect of sales to be stronger when a buyer has a smaller (vs. larger) budget to spend. Likewise, relative price might also play a role, as the same cost might be a stronger signal of expertise when that cost is high compared to other products in the category than when that same cost is low compared to other products in the category. Finally, although customers accepting a high price should increase the strength of the sales signal, individual producers accepting higher prices might reach a point where they feel that customers are being treated unfairly (such as when customers have to pay extraordinarily high shipping costs) or that the prices might cause customers to refrain from making repeat purchases. Do concerns about fairness and relationship-building reduce the happiness individual producers gain from selling products at exorbitant prices? Characteristics of the individual producerOne could also argue that the strength of the sales-as-signal effect depends on certain characteristics of individual producers. We used the data obtained in Study 1 to test whether the effect of sales on happiness is moderated by any of the captured control variables. Only one statistically significant interaction emerged for both measures of our independent variable (i.e., sales volume and sales growth): a positive interaction effect between sales and producers' socioeconomic status (ps <.01; see WA-J1). This suggests that the effect of sales on happiness is stronger for producers that have a higher socioeconomic status—in other words, producers who feel financially secure. We also used the data obtained in Study 1S to examine whether the effect of sales on happiness is moderated by any of the captured control variables. Moderation analyses produce nonsignificant interaction effects between ( 1) sales and producers' experience and ( 2) sales and socioeconomic status (ps >.20; see WA-J2). Thus, although the moderation analyses of Study 1 suggest that the sales-as-signal effect is stronger for producers that have a relatively higher (vs. lower) socioeconomic status, we did not observe such an interaction effect in Study 1S. Therefore, future research might more closely look at the role socioeconomic status plays in the sales-as-signal effect.The strength of the sales-as-signal effect might also depend on individual producers' own evaluations of their products—that is, how competent individuals feel in producing their products might alter how happy they feel as a result of selling those products. We used the data of Study 6 to test this. A moderation analysis produces a nonsignificant interaction between sales (higher vs. lower) and self-validation from production on happiness from sales (p = .98; see WA-J3). This suggests that the positive effect of sales on happiness does not depend on producers' presales perception of their products. We encourage future researchers to look deeper into these, and possibly other, potential moderators of the sales-as-signal effect. ConclusionTraditional producer behavior models assume that producers' behavior is driven by the monetary rewards from selling their wares and that there is an information asymmetry that favors producers. In other words, these models assume producers have information about the quality of their products that (prospective) customers do not, leading producers to signal product quality to customers. This research shows that producers derive happiness from selling their wares that goes above and beyond any monetary rewards. It shows that there is also an information asymmetry in the other direction: Customers possess information about the skill level of the producer that they signal by purchasing the producer's products. This provides a feeling of self-validation to the producer, increasing their happiness. To understand the behavior of producers, it is critical to broaden our scope from purely monetary to self-validation benefits and from assuming a one-directional flow of quality information to a two-directional flow, with at least some quality signals being sent from customers to producers. "
35,"Shedding Light on the Dark Side of Firm Lobbying: A Customer Perspective Firms spend a substantial amount on lobbying—devoting financial resources on teams of lobbyists to further their interests among regulatory stakeholders. Previous research acknowledges that lobbying positively influences firm value, but no studies have examined the parallel effects for customers. Building on the attention-based view (ABV) of the firm, the authors examine these customer effects. Findings reveal that lobbying negatively affects customer satisfaction such that the positive relationship between lobbying and firm value is mediated by losses to customer satisfaction. These findings suggest a dark side of lobbying and challenge current thinking. However, several customer-focused moderators attenuate the negative effect of lobbying on customer satisfaction, predicted by ABV theory, including the chief executive officer's background (marketing vs. other functional area) and the firm's strategic use of resources (advertising spending, research-and-development spending, or lobbying for product market issues). These moderators ensure consistency between lobbying and customer priorities or direct firm attention toward customers even while firms continue to lobby. Finally, the authors verify that lobbying reduces the firm's customer focus by measuring this focus directly using text analysis of firm communications with shareholders. Collectively, the research provides managerial implications for navigating both lobbying activities and customer priorities, and public policy implications for lobbying disclosure requirements.Keywords: attention-based view; corporate political activity; customer satisfaction; lobbying; regulationLobbying, defined as ""expending resources in an attempt to sway government officials to make decisions beneficial to the lobbying firm"" ([74], p. 1138), is a primary means for firms to manage their regulatory environment and attain strong returns. Accordingly, lobbying expenditures have increased by more than 130% since 1998 ([15]), and many large firms (e.g., Ford, Cisco, Facebook, Delta) maintain their own government affairs divisions, which retain dozens of lobbyists to represent their interests (opensecrets.org). The strong accounting and financial market returns to lobbying ([89]), estimated by some at 22,000% ([ 3]), can even exceed returns to product market investments such as research and development (R&D) ([11]). Similarly, recent findings reveal that $325 million in lobbying investments by Fortune 100 firms accounted for $338 billion in federal contracts in return ([ 7]).Lobbying is the most common form of corporate political activity in the United States ([36]). Through lobbying, firms aim to minimize threats and exploit opportunities in their regulatory environment. As regulatory capture theory explains, firms derive competitive advantages from benefits such as subsidies, monopolistic or favorable competitive conditions (e.g., barriers to entry, access to new markets), protective tariffs, and fixed prices ([87]). Because these competitive benefits do not hinge on the firm's ability to satisfy customers, regulatory capture theory hints that lobbying could shift firm attention away from customer priorities ([22]). Anecdotal evidence supports this argument. For example, Oracle lobbies extensively on technology policy matters, to such an extent that some observers have criticized its lack of focus on customers. A former government official accused Oracle of ""using government as a weapon to delay, annoy, and extract value from other entities"" rather than attending to the marketplace or its customers ([42]). Despite these arguments, the effects of firm lobbying on customer outcomes remain largely unknown.Regulatory capture theory alludes that lobbying may adversely affect customer outcomes, but no research in marketing directly examines this relationship. Moreover, extant theory does not explain why a focus on the regulatory environment reduces the firm's customer focus. Therefore, our first research objective is to investigate the heretofore unexamined effect of firm lobbying on customer satisfaction—a critical customer outcome that affects firm value. We draw from the attention-based view (ABV) of the firm ([63]), which argues firms have limited attention available to devote to distinct strategic priorities ([50]; [63], [64]; [65]). Because lobbying can produce direct firm advantages by appealing to the regulatory environment, firms may be inclined to focus on specific activities, imperatives, and stakeholders in that environment, rather than on customers, which should lead to diminished customer satisfaction. In line with this reasoning, previous research highlights that firms struggle to maintain focus on multiple distinct priorities, such as when they partner with competitors versus channel members in interfirm alliances ([75]) or attempt to both grow revenues and cut costs ([77]).Yet the high returns to lobbying make it unlikely that firms will halt this practice. To this end, our second research objective is to identify strategic levers firms can use to minimize the negative effects of their lobbying on customer satisfaction. The ABV of the firm also informs our choice of which strategic levers to study. The theory posits that firm behavior is an outcome of the distribution of decision makers' attention. Decision makers' attention, in turn, is informed by personal values, unique firm resources, and rule configurations ([63], [64]; [66]). Accordingly, we predict that four moderators might effectively channel firm attention toward customers: chief executive officer (CEO) background (marketing vs. other functional area), the firm's spend on advertising and R&D, and lobbying for product market issues (rather than for non–product market issues). These four moderators work as aligning and/or counterbalancing mechanisms. Aligning mechanisms ensure consistency between lobbying and customer priorities. Influential firm decision makers can direct attention and shape focus by aligning the firm's strategic priorities with their own. Counterbalancing mechanisms work to offset firm attention on one strategic priority by redirecting focus to another. We expect that various counterbalancing mechanisms can direct firm attention toward customers even while firms continue to lobby. These attention mechanisms should attenuate negative effects of lobbying on customer satisfaction.[ 6]Finally, our third research objective is to uncover the mechanism underlying the negative relationship between lobbying and customer satisfaction. In developing our hypotheses, we posit that lobbying reduces customer satisfaction by decreasing the firm's focus on customers. With additional analyses performed on a subset of the data, we confirm this prediction. In support of the ABV, we uncover a loss of customer focus using shareholder communications, a text-based measure that captures firm emphasis on customers as conveyed in shareholder earnings calls.The tests of our hypotheses rely on an unbalanced panel of 758 observations involving 87 publicly traded firms during the period 2000–2014. We find a significant, negative effect of lobbying on customer satisfaction, providing novel evidence of the dark side of firm lobbying. We also replicate previous findings of a positive effect of lobbying on firm value but identify a negative counteracting effect when we account for customer satisfaction. This insight challenges economic and finance literature that suggests largely positive effects of lobbying on firm value (e.g., [17]; [46]). Consistent with our expectations, we also show that the CEO's background, advertising spend, R&D spend, and product market lobbying each positively moderate the lobbying–customer satisfaction relationship. Finally, a decrease in customer focus helps explain the negative lobbying–customer satisfaction link.Our findings contribute to marketing theory and practice in three important ways. First, we extend the ABV to reveal that otherwise-beneficial firm actions (lobbying) can simultaneously harm customer outcomes. This view helps augment shortcomings in extant theoretical frameworks (i.e., regulatory capture) for explaining how limits in firm attention reduce the necessary focus on customers, with detrimental effects for satisfaction and firm value. In addition to challenging extant research, identifying this dark side of lobbying represents a warning to firms to be wary of losing customer focus. Second, we offer solutions in the form of a set of theoretically informed, managerially relevant moderators that can align or counterbalance firm attention to customers and thereby attenuate the negative effects of lobbying on customer satisfaction. Third, we detail how the negative effects emerge, by showcasing a key pathway leading to a loss of customer focus conveyed by firms' shareholder communications. Considering firms' increasing strategic attention to lobbying, this research offers timely implications for marketing theory and practice.We begin by developing the conceptual framework, which blends regulatory capture theory with the ABV of the firm. After we explain our empirical approach, we report the focal study findings. We also present a series of additional analyses. Finally, we conclude with theoretical implications and insights for marketing managers and policy makers. Firm Lobbying and Regulatory CaptureAs the list in Table 1 reveals, concepts highlighted in regulatory capture theory ([22]; [32]; [87]) underpin studies of the relationship between lobbying and firm value, most of which identify positive firm outcomes of lobbying. We draw on these foundations to suggest a positive effect of lobbying on firm value, consistent with previous findings. However, to address our research objectives, we expand our conceptual framework to account for limits to firm attention (Figure 1). We expect that firm lobbying activities decrease the firm's focus on customers, in line with arguments from the ABV ([49]; [63]; [65]). The ABV also informs our investigation of moderators of the lobbying–customer satisfaction relationship.Graph: Figure 1. Customer focus and the dark side of lobbying: conceptual framework.GraphTable 1. Selected Lobbying Research: Past Outcomes and Moderators Studied. StudyFirm OutcomesCustomer OutcomesModeratorsKee, Olarreaga, and Silva (2004)Market access, tariffsDe Figueiredo and Silverman (2006)EarmarksAlexander, Mazza, and Scholz (2009)Money repatriatedRichter, Samphantharak, and Timmons (2009)Effective tax rateYu and Yu (2011)Fraud lawsuitsDuchin and Sosyura (2012)Approval for Troubled Asset Relief Program bailout fundsHill et al. (2013)Firm valuePolitical action committee contributionsBorisov, Goldman, and Gupta (2016)Abnormal returnsFirm unethical behaviorChen, Parsley, and Yang (2015)Net income, stock market returnsGao and Huang (2016)Returns, trading volumeKang (2016)Policy enactment, lobbying returnsUnsal, Hassan, and Zirek (2016)Firm valueCEO political affiliationRidge, Ingram, and Hill (2017)Government contractsConnectednessFidrmuc, Roosenboom, and Zhang (2018)M&A review outcomesRegulator riskMartin et al. (2018)Firm value, systematic and idiosyncratic riskR&D and advertising stockRayfield and Unsal (2018)Product approvalProduct recallDiestre, Barber, and Santalo (2019)Timing of safety alertDrug-related side effectLambert (2019)Regulatory enforcement, firm performanceCurrent studyFirm valueCustomer satisfactionCEO marketing background, advertising spend, R&D spend, product market lobbying As noted previously, lobbying has a positive effect on firm value ([12]; [17]; [46]; [60]), which helps explain its growing practice. In particular, lobbying can result in regulatory capture ([87]) or allow the firm to dominate decisions about its regulatory environment ([32]). With regulatory capture, a firm attains disproportionate influence over a regulatory system designed to constrain and temper their behavior, which produces firm-specific benefits ([22]). For example, regulatory capture might create policy advantages or allow firms to establish monopoly power ([56]). The diverse outcomes of regulatory capture might assist firms directly, without benefiting customers, such as reduced regulatory oversight, lower tax rates, preferred government subsidies, or entry into restricted markets (Table 1). It is important to note that financial market responses to lobbying do not necessarily hinge on policy changes. Successful lobbying might preserve a favorable status quo or foster relationships without producing any other immediate outcomes ([29]). As [52] shows, even if policy changes are rare, a firm that lobbies still achieves a positive return on its investment, perhaps because financial markets use lobbying as a signal of firm influence. Accordingly, we hypothesize the following: H1:  Firm lobbying relates positively to firm value. ABV of the Firm: Lobbying and Customer FocusRegulatory capture creates competitive advantages for the firm that do not depend on satisfying customers. Therefore, lobbying could make customer-focused efforts seem less necessary ([22]; [23]), though we lack any clear explanation for how this shift occurs or what can be done to attenuate its effect. By turning to the ABV ([49]; [63]; [65]), we seek to address this gap (Figure 1). This theory stipulates that firm actions, adaptations, and performance outcomes result from the distribution of attention—defined as ""the noticing, encoding, interpreting, and focusing of time and effort by organizational decision-makers"" on behalf of the firm ([63], p. 189)—to various strategic activities ([64]). Firm decision makers confront myriad demands on their time and attention ([18]), and their bounded rationality and information processing constraints limit the activities or strategic imperatives to which they can attend ([21]; [66]; [81]). Consistent with the ABV, prior research suggests that firms and their decision makers generally cannot pursue two strategically opposed foci effectively, such as revenue and cost emphases ([77]), allying with competitors and channel members ([75]), exploration and exploitation strategies ([ 6]), meeting stock market expectations and innovating ([90]), developing international and domestic market knowledge bases (Sapienza, De Clercq, and Sandberg 2005), or pursuing growth through organic and merger and acquisition approaches (Yu, Engleman, and Van de Ven 2005).We extend this thinking to consider firm lobbying and examine the effect on customer satisfaction. This important performance metric is a function of customer expectations, perceived quality, and perceived value ([ 5]; [35]), and it usually requires an intentional firm focus on customers. When that focus decreases, satisfaction is likely to suffer. Lobbying, or appealing to regulators for direct benefits, does not hinge on appeasing customers ([22]). Because firm attention to both lobbying and customers implies that its focus is spread across diverse priorities (i.e., regulators and customers), ABV suggests that the firm cannot adequately focus on both. We predict that when firms lobby, their attention to customers decreases for four reasons.First, customer-focused activities such as R&D spending have more uncertain outcomes and relatively lower returns than lobbying ([11]). Therefore, firms might prioritize lobbying to attain direct advantages. Second, lobbying generally entails rent seeking from existing assets, by protecting them and expanding the returns from them to the greatest extent possible, whereas a focus on customers typically implies that firms create new value for or with customers, which is inherently more difficult. Third, lobbying and a customer focus involve conceptually distinct stakeholders and environments: legislators/regulators and customers, respectively. In the latter case, product market competition is intense, as firms jockey for position in heterogeneous customers' consideration sets. Competition in the regulatory sphere instead is less intense because the relatively fewer legislative audiences tend to be more homogeneous, and relationships can be established more quickly ([29]). Furthermore, firms (in contrast to other stakeholders such as special interest groups) have more resources and coordination ability to appeal to legislators ([59]; [67]). Fourth, firms must deploy different resources, skills, and actions to succeed in these disparate stakeholder environments. Because there are fewer, more well-defined, and more accessible regulatory and special interest group stakeholders, firms' resources, skills, and actions can be deployed more effectively than in broad and diverse customer environments. Accordingly, we hypothesize the following: H2:  Firm lobbying relates negatively to customer satisfaction. Mediating Role of Customer Satisfaction in the Lobbying–Firm Value RelationshipAs noted previously, lobbying research reveals positive effects on firm value (Table 1). We examine how lobbying affects firm value when accounting for the mediating role of customer satisfaction (Figure 1). Marketing research highlights the positive influence of customer satisfaction on firm value ([ 4]), perhaps because customer satisfaction increases cash flows ([43]; [61]) and reduces future cash flow volatility ([34]; [43]). Because we expect lobbying to relate negatively to customer satisfaction (H2) but positively to firm value (H1), we predict a competing, mediating role ([96]) of customer satisfaction in the lobbying–firm value relationship. That is, a loss of customer satisfaction is a cost of lobbying, and we believe that negative relationship will detract from the firm value achieved by using lobbying. Lobbying relates positively to firm value, but the negative relationship between lobbying and customer satisfaction should have a counteracting effect. We therefore hypothesize the following: H3:  Customer satisfaction negatively mediates the positive relationship between lobbying and firm value. Customer-Focused ModeratorsIf lobbying produces direct, positive effects on firm value, firms are unlikely to temper the practice, as suggested by real-world examples of its increasing use. Therefore, to address the risk of negative counteracting effects through customer satisfaction, we propose moderators that should attenuate these negative effects. The lobbying–firm value path is well established (Table 1), so we concentrate here on the unexplored relationship of lobbying with customer satisfaction.According to the ABV, firm behavior is an outcome of the distribution and regulation of decision maker attention and the firm-level attentional structures that support this focus ([63], [64]; [66]). In proposing the ABV, [63], p. 188) established two foundational premises: ( 1) influential firm decision makers determine the firm's attention priorities and shape its focus and ( 2) the firm's attendance to its situation and context depends on ""resource and rule configurations"" by which the firm allocates resources and channels attention. If lobbying diverts firm attention away from customers, these premises offer direction for refocusing attention back to customers.Drawing from [63] two premises, we propose that the negative effect of lobbying on customer satisfaction can be attenuated through two attention-directing mechanisms. First, aligning mechanisms ensure consistency between lobbying and customer priorities. Influential firm decision makers can direct attention and shape focus by aligning the firm's strategic priorities with their own. In particular, a CEO with a marketing background can likely better align lobbying with firm attention to customers and customer imperatives than a CEO who does not have a marketing background. Second, counterbalancing mechanisms work to offset firm attention on one strategic priority by redirecting focus to another. We expect that various counterbalancing mechanisms can direct firm attention toward customers even while firms continue to lobby. Firm spending on critical priorities emphasizes their importance, especially when used together with firm efforts that seemingly work to achieve different goals, such as lobbying. Specifically, we propose greater advertising spending and R&D spending, allocations that primarily concern customers, counterbalance firm attention to lobbying, thereby offsetting the negative effects of lobbying on customer satisfaction.Finally, previous research has not distinguished lobbying for non–product market issues, such as taxes and workplace safety, from product market issues, such as product specifications and patents (see Web Appendix A for a complete list of non–product market and product market issues for which firms can lobby). Consistent with ABV logic, we expect that lobbying for non–product market issues directs attention away from customers, whereas lobbying for product market issues orients firm attention toward customers. We distinguish between these two types of lobbying to posit that customer satisfaction is less adversely affected when firms counterbalance general lobbying efforts with a focus on product market issues. In summary, the ABV leads us to derive four moderators that focus firm attention on customers to offset negative effects, even while lobbying. We discuss each in greater detail in the following subsections. CEO backgroundIn the ABV theoretical tradition, CEOs emerge as ""the most critical players"" in directing the firm's focus because they choose how the firm should channel its attention and which relevant priorities it pursues ([63], p. 197; [93]), even among diverging priorities such as lobbying and customers. Indeed, attention-based perspectives suggest that firm strategies reflect the CEO's values and vision ([19]; [86]) and that the CEO can integrate firm attention or align diverse divisions toward a shared focus on specific priorities ([50]). As we explain, and supported by [63] foundational premises, CEOs shift firm attention to different imperatives through an aligning mechanism. Therefore, CEO background, which is defined as the functional knowledge and skills that CEOs develop throughout their educational and career experiences ([78]), should inform firm strategic priorities and attention because CEOs' background allows them to align firm focus accordingly.We expect that the negative relationship between lobbying and customer satisfaction is moderated by the firm's CEO background, such that this relationship is less negative for firms with CEOs who have a marketing background, as opposed to firms with CEOs who have other types of functional expertise. A CEO with a marketing background understands the need to monitor customer expectations and create customer value ([13]), and thereby can more likely align disparate firm efforts, such as those focused on customers and regulators, to enhance the synergy between them. Greater CEO attention to customer priorities and alignment of otherwise disparate firm activities can create a shared, customer-centric vision throughout the firm. Taken together, these efforts should lessen the negative effect of lobbying on customer satisfaction. That is, we expect that this lessened negative effect will occur for firms that have CEOs with a marketing background, as opposed to firms that have CEOs with other functional expertise. We hypothesize: H4:  CEO background moderates the negative effect of lobbying on customer satisfaction, such that the effect is less negative in firms with CEOs with a marketing background. Advertising spend and R&D spendThe negative relationship between lobbying and customer satisfaction may be attenuated among firms that counterbalance lobbying with advertising spend and R&D spend. In their study of strategic attention and firm performance, [31] show that if firms with widely dispersed attention increase their R&D spending, they counterbalance negative effects of attention dispersion. Further, existing research finds that spending on R&D serves as a powerful signal to employees, customers, and other stakeholders that the firm prioritizes customer value creation and maintains a fundamental focus on innovation-related activities ([19]; [72]).We expect that the negative relationship between lobbying and customer satisfaction will be moderated by the firm's R&D spend. Foundational premises of the ABV state that resource allocation to particular priorities signal their importance to internal stakeholders and channel their attention toward these priorities ([63]; Sapienza, De Clercq, and Sandberg 2005). Firms with higher R&D spend signal greater importance of customer priorities to internal stakeholders as compared with firms that spend less on R&D. Consequently, firms that spend more on R&D experience a stronger counterbalancing effect that attenuates the customer focus loss associated with lobbying. In complement to its attention counterbalancing role, R&D spend may be deployed with lobbying to create value for customers. For example, when R&D spending and lobbying are used in concert, firms can gain access to new markets, thereby providing customers with more and varied product options. Similarly, lobbying may help the firm introduce more innovations, especially if they are subject to regulatory hurdles such as in the drug and medical devices sector (Rayfield and Unsal 2018). Thus, we expect R&D spend to moderate the lobbying–customer satisfaction relationship, lessening its negative effect.We expect a similar moderating relationship for advertising spend, such that the negative relationship between lobbying and customer satisfaction will be moderated by the firm's advertising spending. Studies have shown that greater spending on advertising signals superior product quality, highlights customer value, and influences stakeholders' perceptions of the firm and its priorities ([38]; [41]; [45]). Typically, advertising activities are customer focused ([ 9]). Therefore, advertising spend together with firm lobbying works in a counterbalancing way: firms that spend more on advertising experience a stronger counterbalancing effect to attenuate the customer focus loss associated with lobbying. In complement to this attention counterbalancing role, when advertising spend and lobbying are integrated, they can lead to increased reach and saliency of a firm's advertising activities. For example, pharmaceutical firms deployed lobbying to help expand their available advertising options to highly profitable direct-to-consumer formats ([76]). Thus, we expect advertising spend to moderate the lobbying–customer satisfaction relationship, lessening its negative effect. For these reasons, we predict the following: H5a:  Advertising spend moderates the negative effect of lobbying on customer satisfaction., such that the effect is less negative in firms with greater advertising spend. H5b:  R&D spend moderates the negative effect of lobbying on customer satisfaction, such that the effect is less negative in firms with greater R&D spend. Lobbying for product market issuesWe integrate the ABV perspective and lobbying research to suggest that the target of a firm's lobbying efforts can attenuate the negative effect of lobbying on customer satisfaction. That is, firms lobby for both non–product market issues (e.g., taxes, beneficial federal budget allocations) and product market issues (e.g., advertising, patents, product safety issues). Lobbying for product market issues should lessen the negative effect of lobbying on customer satisfaction, as product market issues have an inherent customer focus. By lobbying for issues relevant to the product market, the firm conveys its strategic prioritization of customers to its employees, customers, regulators, and other stakeholders. Likewise, lobbying that is focused on product market imperatives counterbalances lobbying that is focused on direct firm benefits, such as those stipulated in regulatory capture.Lobbying to address product market issues, relative to other issues, should offset otherwise negative customer satisfaction effects from lobbying generally. This occurs by directing firm attention to customer priorities and channeling that attention across functional divisions (i.e., regulatory and marketing firm functions), thus attenuating the customer focus loss associated with lobbying. Conversely, lobbying for non–product market issues does not require attention to customers and resembles the direct path to firm value as stated in regulatory capture theory. Even if such lobbying secures other firm benefits, those efforts are unlikely to produce benefits for customers. We expect firm lobbying for product market issues to moderate the negative effect of lobbying on customer satisfaction; that is, the greater the proportion of lobbying for product market issues relative to total lobbying efforts, the more likely this negative effect is lessened. We predict the following: H6:  Product market lobbying moderates the negative effect of lobbying on customer satisfaction, such that the effect is less negative in firms with a greater proportion of lobbying for product market issues. MethodTo test our hypothesized relationships, we collected secondary data from a variety of sources. We describe the data, sources, and variable construction approaches next. The sample includes firms of various sizes from a broad range of industries, tracked over time, to provide a thorough test of our research questions. Sample and DataOur initial sampling frame comprises all firms for which we can obtain customer satisfaction data from the American Customer Satisfaction Index (ACSI; theacsi.org), which provides scores for approximately 200 Fortune 500 firms across multiple industries and is commonly used in marketing strategy research (e.g., [35]; [71]). When a firm had multiple brands for which satisfaction scores are reported, we took the average as our measure of firm-level satisfaction.[ 7] We also obtained industry-level customer satisfaction scores from the ACSI database and then matched firms from this database to their entries in COMPUSTAT. Consistent with our focus and prior research, we dropped firms that failed to report both advertising and R&D expenditures ([71]).[ 8] We manually checked the firms' annual 10K statements if they reported zero R&D expenditures.[ 9] The overlap between ACSI and COMPUSTAT, when accounting for advertising spend and R&D spend, produced a final sample of 87 firms, consistent with prior research that adopts similar approaches ([43]; [71]).Next, we combined this data set with lobbying data obtained from opensecrets.org, the Senate Office of Public Records (SOPR), and followthemoney.org. If none of these databases provided information, we set the lobbying expenditures for the firm to zero dollars. The Lobbying Disclosure Act mandates that firms report all lobbying expenditures above $5,000 per quarter; a failure to do so incurs penalties (lobbyingdisclosure.house.gov). Thus, we are confident that no firm lobbying is omitted from our data set. Lobbying data include expenditures and the issues on which firms lobby (see also Web Appendix A). To cull the CEO background data, we used disparate sources such as Securities Exchange Commission annual reports, Bloomberg, BoardEx, LexisNexis, popular media, and industry reports. We use publicly available data from Regdata ([ 2]) to identify whether an industry is regulated. Furthermore, to control for the potential effects of customer awareness of firm lobbying (i.e., lobbying visibility) that might affect satisfaction, we count relevant news articles using a Factiva search for Associated Press articles on firm lobbying. Table 2 details the variables, operationalizations, references, and data sources.GraphTable 2. Variables, Measures, and Data Sources. ConstructVariable NotationDescriptions (Measures)Representative PapersData SourcesDependent VariablesCustomer satisfactionCust_SatisCustomer satisfaction index (1–100)Fornell et al. (2016)ACSIFirm valueTobins_qTobin's qMartin et al. (2018)COMPUSTATIndependent VariableLobbyingLobby_SpendLobbying expenditure (federal and state)/ AssetsHill et al. (2013)Opensecrets.org, SOPR, followthemoney.orgCustomer-Focused ModeratorsCEO marketing backgroundMkt_CEODummy = 1 if CEO has predominant marketing experienceSaboo et al. (2017)Annual Reports, Bloomberg, LexisNexis, BoardExAdvertising spendAD_SpendAdvertising expenditure/AssetsTuli and Bharadwaj (2009); Gruca and Rego (2005)COMPUSTATR&D spendRD_SpendR&D expenditure/AssetsTuli and Bharadwaj (2009); Gruca and Rego (2005)COMPUSTATProduct market lobbyingProduct_LobbyNumber of product market issues/Total number of issues lobbied forNovel to this researchSOPR, Opensecrets.orgFirm-Specific Control VariablesLeverageLeverageLong-term debt-to-assetTuli and Bharadwaj (2009); Jindal and McAlister (2015)COMPUSTATProfitROAReturn on assets: Ratio of net income to total assetsRego, Morgan, and Fornell (2013)COMPUSTATFirm sizeSizeEmployee count (in 1,000s)Otto, Szymanski, and Varadarajan (2020)COMPUSTATLobbying visibilityVisibilityNumber of articles (annual) that discuss focal firm lobbyingNovel to this researchFactivaIndustry-Specific Control VariablesIndustry growth rateIndustry_GrowthChange in industry sales in primary two-digit SIC industry/total industry saleRego, Morgan, and Fornell (2013); Tuli and Bharadwaj (2009); Gruca and Rego (2005)COMPUSTATIndustry concentrationIndustry_ConcHerfindahl–Hirschman index (0–1)Rego, Morgan, and Fornell (2013); Tuli and Bharadwaj (2009); Gruca and Rego (2005)COMPUSTATRegulated industryIndustry_RegulatedDummy = 1 if firms operate in a highly regulated industryMartin et al. (2018)REGDATAIndustry customer satisfactionInd_Cust_SatisAverage customer satisfaction score of industryNovel to this researchACSI 1 Notes: ACSI = American Customer Satisfaction Index; SOPR = Senate Office of Public Records; SIC = Standard Industrial Classification. Variable ConstructionCustomer satisfaction, advertising spend, and R&D spend are widely used variables, so we do not detail their construction here, beyond the information provided in Table 2. Instead, we focus on the variables that require additional explanation or coding or are unique to our research. First, we calculate Tobin's q using the [20] measure, which is common to marketing research (e.g., [60]). Second, lobbying is the sum of firms' federal- and state-level lobbying spend, obtained from the SOPR, opensecrets.org (both of which report federal data), and followthemoney.org (which reports state data). Consistent with prior research (e.g., [55]; [88]) and to account for size effects, we scale lobbying spend by assets to create our lobbying measure.Third, firms are not required to report the specific dollar amounts allocated to each lobbying issue. However, they do list the issues for which they lobby (Web Appendix A). We use this information to construct our measure of product market lobbying, according to the coding method described by [16] and [83]. Specifically, two coders independently coded all non–product market issues, defined as ""activities that do not directly influence the company's ability to create, deliver, or communicate products or services to customers."" A kappa value of.80 indicates high interrater reliability. Any disagreements were resolved by a member of the research team. After we identified non–product market issues, we summed their occurrences in any given year and subtracted that value from the total number of lobbying issues reported by a firm for a given year to arrive at the count of product market issues. We then scaled this count by the total number of lobbying issues reported by the firm in that year to create our measure of product market lobbying.Fourth, we use Securities Exchange Commission annual reports, Bloomberg, LexisNexis, popular media, and industry reports to construct our CEO background variable. This dummy variable takes a value of 1 if the CEO has dominant marketing experience and 0 if s/he has any other functional background ([78]). Like our coding of product market lobbying, the coding of CEO marketing expertise was independently verified by two coders and validated by a member of the research team. Kappa values, again, exceed.80, providing evidence of interrater reliability. Data DescriptionTable 3 features summary statistics and correlations for the study variables. Among the 155 unique CEOs in our data set, 22% had a marketing background. Their average tenure was 4.66 years. Lobbying expenditures are less than advertising and R&D expenditures, yet firms spend more money lobbying Congress than taxpayers spend to operate the legislative branch ([29]). Each member of Congress is the focus of about $3.7 million annually in attempted influence and persuasion by U.S. firms ([ 7]).GraphTable 3. Summary Statistics and Correlations. ConstructMSD12345678910111213141.Firm value1.871.481.002.Customer satisfaction78.945.09.161.003.Lobbying.00.00.11−.071.004.CEO marketing background.22.42.00.06−.061.005.Advertising spend.04.05.20.21.32−.041.006.R&D spend.02.03.27.03.08−.12.031.007.Product market lobbying.51.31.18.04.28.02.04.121.008.Firm size164.01271.93−.09−.18−.14.12−.19−.18−.031.009.Leverage.24.16.05.06−.05−.03.07−.23.10−.021.0010.Profit.16.09.51.08−.02.00.28.00.09−.06.091.0011.Industry concentration.11.10−.16−.22−.08−.00−.07−.36−.11.31.05−.101.0012.Industry growth rate.04.08.07.05.05.02.02.00.00−.01−.02.03−.061.0013.Regulated industry.10.30−.02−.34.02−.06−.12−.18.12−.03.17.10.32−.051.0014.Lobbying visibility.742.72.10−.25.05−.01−.11.13.09−.02−.15.11−.11.01.121.0015.Industry customer satisfaction78.084.44.03.76.00.11.24.02.03−.06.01.06−.27.08−.34−.18 2 Notes: Correlations at.06 or greater (absolute value) are significant at p < .10.We checked for multicollinearity before proceeding to the identification strategy and model specification. The mean variance inflation factor is 2.06, and the highest individual value is 4.32. In addition, to rule out multicollinearity concerns for the interaction terms (with r > .70), we residual-centered the interaction of lobbying with product market lobbying. Residual centering has been shown to reduce multicollinearity between an interaction term and its first-order effect term, to provide stable and unbiased results ([58]), and has been used in recent literature (e.g., [24]; [51]). Identification Strategy and Model SpecificationPrior to specifying our models, we conducted panel Granger causality tests to examine whether lobbying Granger-causes customer satisfaction or vice versa. They reveal that lobbying Granger-causes customer satisfaction (χ2 = 5.04, p < .10) and not the reverse. Next, we examine independent variable stationarity (lobbying) with panel unit root tests. A lack of stationarity dictates how the variables enter the model. The Fischer-type Phillips–Perron test rejects the null hypothesis that the variables contain unit roots (p < .01). We conclude the variable is mean-stationary and specify it in terms of levels. Model SpecificationTo test H1–H3, we specify two equations[10] for any firm i operating in a primary two-digit Standard Industrial Classification (SIC) code industry j at time t[11]: Tobins_qi,j,t=α1,0+α1,1Lobby_Spendi,j,t−1+α1,2Cust_Satisi,j,t−1+α1,3Mkt_CEOi,j,t−1+α1,4AD_Spendi,j,t−1+α1,5RD_Spendi,j,t−1+α1,6Product_Lobbyi,j,t−1+ω1Firm_Controlsi,j,t−1+ϑ1Industry_Controlsj,t−1+∑K=1t−1δ1,KTIMEK+ε1,i,t, Graph( 1)and Cust_Satisi,j,t=α2,0+α2,1Lobby_Spendi,j,t−1+α2,2Mkt_CEOi,j,t−1+α2,3AD_Spendi,j,t−1+α2,4RD_Spendi,j,t−1+α2,5Product_Lobbyi,j,t−1+ω2Firm_Controlsi,j,t−1+ϑ2Industry_Controlsj,t−1+∑K=1t−1δ2,KTIMEK+ε2,i,t, Graph( 2)where Tobins_q is Tobin's q, Cust_Satis is customer satisfaction, Lobby_Spend is lobbying spend, Mkt_CEO is CEO marketing background, AD_Spend is advertising spend, RD_Spend is R&D spend, and Product_Lobby is the firm's product market lobbying. With Firm_Controls, we create a vector of firm-specific control variables (including lobbying visibility) for each firm i, whereas Industry_Controls is a vector of industry-specific control variables for firm i operating in the primary two-digit SIC industry j (see Table 2). In Equation 2, we include average industry-level customer satisfaction (Ind_Cust_Satis) as an industry-specific control variable to rule out any covariations in industry customer satisfaction and lobbying that may influence our results. We include time dummies for each observation year to account for macroeconomic factors that influence all firms. Finally,  ε1,i,t  and  ε2,i,t  are the idiosyncratic error terms.Next, we specify the following equation to test H4–H6, with the same firm- and industry-specific control variables as in Equation 2: Cust_Satisi,j,t=α3,0+α3,1Lobby_Spendi,j,t−1+α3,2Mkt_CEOi,j,t−1+α3,3AD_Spendi,j,t−1+α3,4RD_Spendi,j,t−1+α3,5Product_Lobbyi,j,t−1+α3,6Lobby_Spendi,j,t−1×Mkt_CEOi,j,t−1+α3,7Lobby_Spendi,j,t−1×AD_Spendi,j,t−1+α3,8Lobby_Spendi,j,t−1×RD_Spendi,j,t−1+α3,9Lobby_Spendi,j,t−1×Product_Lobbyi,j,t−1+ω3Firm_Controlsi,j,t−1+ϑ3Industry_Controlsj,t−1+∑K=1t−1δ3,KTIMEK+ε3,i,t. Graph( 3) Model IdentificationThe set of independent variables identified in the previous subsection cover important firm and industry factors that could influence customer satisfaction and Tobin's q. However, for credible identification of the effects it is necessary to consider the potential endogeneity that could arise due to simultaneity and omitted variables ([40]).Although we use lagged independent variables in Equations 1–3 to account for reverse causality (e.g., [71]), the analyses still might suffer from endogeneity bias due to omitted variables. To alleviate this concern, we include time fixed effects in all our focal equations. Consistent with unobserved effect models ([40]), year fixed effects help control for the omitted variables, and including the average industry-level customer satisfaction (Ind_Cust_Satis) as an industry-specific control variable helps rule out covariation in industry customer satisfaction and lobbying that may influence our results.Although these efforts reduce concerns about omitted variables, we cannot theoretically claim that lobbying is uncorrelated with the error term in Equation 2 ([ 8]). Thus, we specify a fourth equation with lobbying as the dependent variable, the exogenous variables from Equations 1 and 2 as independent variables, and two variables that meet the criteria for preserving the rank and order conditions of the system of equations. In detail, among industry-based excluded variables, the first instrument that meets the exclusion restriction criterion is average industry lobbying (total industry lobbying expenditures, excluding focal firm lobbying expenditures, divided by number of entities lobbying in that industry as given in opensecrets.org), as an instrument for firm-specific lobbying. Because there are multiple firms in an industry, it is unlikely that average industry lobbying correlates with firm-level omitted variables that influence a focal firm's customer satisfaction. This variable also meets the relevance condition, because peer firm behavior can have a normative effect in that peer firms generally face similar market conditions.The second instrument is the total number of lobbyists operating in an industry. The industry supply of lobbyists should normatively influence lobbying spend. The greater the total industry expenditure on lobbying, the more lobbyists are likely to be operating. It also is reasonable to assume that the cost of lobbying is lower for firms that operate in industries with more lobbyists. This information, available to all firms within an industry, is unlikely to correlate with unobserved firm-level variables that affect a focal firm's customer satisfaction, so it meets the exclusion criteria. Accordingly, we specify the following equation and add it to our system of equations: Lobby_Spendi,j,t=α4,0+α4,1Industry_Lobbyj,t−1+α4,2Num_Lobbyistj,t−1+α4,3Mkt_CEOi,j,t−1+α4,4AD_Spendi,j,t−1+α4,5RD_Spendi,j,t−1+α4,6Product_Lobbyi,j,t−1+ω4Firm_Controlsi,j,t−1+ϑ4Industry_Controlsj,t−1+∑K=1t−1δ4,KTIMEK+ε4,i,t, Graph( 4)where Industry_Lobby and Num_Lobbyist are the average lobbying expenditure and number of lobbyists, respectively, for firm i operating in the primary two-digit SIC code industry j. The control variables are as described in Equation 2.Before discussing our results, we note that the results of Hansen's J test reveal that the instruments are valid (p > .10). The Kleibergen–Paap test also shows that our instruments are relevant (p < .00), increasing our confidence in the use of these variables as instruments. Likewise, we examine the instrument effects on firm lobbying (Table 4, Column A). Industry lobbying has a negative, nonsignificant effect on firm lobbying (α4,1 = −.01; p > .10), and industry lobbyist supply has a negative, significant effect on lobbying (α4,2 = −.00; p < .01).[12]GraphTable 4. Effect of Lobbying on Customer Satisfaction and Customer Focus Moderation. Main Effects Model (H1–H3)Interaction Effects Model (H4–H6)Construct(A)Lobbying(B)Tobin's q(C)Customer Satisfaction(D)Lobbying(E)Tobin's q(F)Customer SatisfactionLobbying (H1, H2)4.70***(1.67)−8.37***(1.99)4.52**(1.77)−12.20***(2.30)Customer satisfaction (H3).03**(.01).03**(.01)CEO marketing background.01(.01).08(.11)−.37(.27).01(.01).08(.11)−1.11***(.33)Advertising spend1.02***(.36)−1.68(2.92)7.15*(4.08)1.03***(.36)−1.54(2.97)2.87(4.09)R&D spend.27(.23)12.26***(2.93)4.81(5.30).27(.23)12.26***(2.92)−.73(6.02)Product market lobbying.10***(.02)−.03(.24)1.15**(.48).10***(.02)−.00(.25)1.22**(.47)Lobbying × CEO marketing background (H4)12.33***(3.18)Lobbying × Advertising spend (H5a)19.51***(4.95)Lobbying × R&D spend (H5b)57.60*(32.99)Lobbying × Product market lobbying (H6)8.98***(3.31)Firm size−.00**(.00).00*(.00)−.00***(.00)−.00**(.00).00(.00)−.00***(.00)Leverage−.07(.06)1.00**(.44)1.44(1.09)−.07(.06).99**(.43)1.24(1.01)Profit−.33***(.12)8.18***(1.19)−1.34(1.96)−.33***(.12)8.12***(1.19)−.05(2.05)Industry concentration−.22***(.06)1.02**(.47)1.66(1.47)−.22***(.06)1.02**(.47)1.22(1.47)Industry growth rate.06(.06).93*(.50).99(1.76).06(.06).94*(.50).86(1.71)Regulated industry.03**(.01)−.05(.12)−2.16***(.49).03**(.01)−.04(.12)−2.37***(.49)Lobbying visibility−.00(.00).02(.02)−.14**(.06)−.00(.00).02(.02)−.13**(.06)Industry customer satisfaction−.01***(.00).74***(.04)−.01***(.00).73***(.04)Industry lobbyists−.00***(.00)−.00***(.00)Industry lobbying spend−.01(.04)−.00(.04)Year fixed effectsYesYesYesYesYesYesConstant.61***(.12)−2.44**(1.19)20.92***(3.10).61***(.13)−2.61**(1.22)21.84***(3.08)Log-likelihood−2,593.35−2,582.10Akaike information criterion5,360.715,346.19Observations758758 3 *p < .10, **p < .05, ***p < .01.4 Notes: Robust standard errors are in parentheses. We scale lobbying and all its interaction terms by 103 for visual consistency. EstimationWe estimate two systems of equations. The first (Equations 1, 2, and 4) tests H1–H3, whereas the second system (Equations 1, 3, and 4) tests H4–H6. We have multiple equations in which the errors across them can be correlated, so we estimate the equations jointly using a structural equation model approach with correlated errors. Joint estimation across multiple equations yields more efficient estimates ([92]), accounts for endogeneity due to common omitted variable bias (Drukker 2014), and has been used to test mediation, moderation, and moderated mediation relationships in the presence of endogenous regressors (e.g., [91]).[13] Although H4–H6 focus on the simple moderation of the lobbying–customer satisfaction relationship, with a system of equations estimation approach, we can examine moderated mediation of the lobbying–Tobin's q relationship as well. Results Model-Free EvidencePrior to conducting the formal analyses for hypotheses testing, we describe our data using model-free evidence. By comparing mean customer satisfaction values across firms with high and low lobbying levels, this evidence reveals that lobbying is negatively associated with customer satisfaction. Customer satisfaction scores are 79.55 for low-lobbying-level firms as compared with 78.36 for high-lobbying-level firms, using a median split. When comparing the top and bottom quartile (decile), customer satisfaction is 78.45 (78.71) for low-lobbying-level firms and 77.91 (77.68) for high-lobbying-level firms.Model-free evidence also shows that higher customer satisfaction is associated with each of our four moderators, including a CEO with a marketing background (in firms with above-median customer satisfaction, 26% have a marketing CEO, whereas this number is 18% for firms with below-median customer satisfaction), higher advertising spend (.51 for above-median customer satisfaction firms and.32 for below-median firms), higher R&D spend (.25 for above-median customer satisfaction firms vs..22 for below-median firms), and a greater proportion of product market lobbying (.54 for above-median customer satisfaction firms and.49 for below-median firms). We hold lobbying constant at a high level and compare customer satisfaction scores of firms that score high on the moderating variables with the scores of firms that score low on these variables. For firms with a marketing CEO, customer satisfaction scores are 80.61, versus 77.80 for firms with a CEO with a different functional area background. High advertising spend produces a customer satisfaction score of 80.01, whereas low advertising spend is 76.45. High R&D spend is 79.73, versus 76.91 for low R&D spend. Finally, customer satisfaction is 76.46 for firms that lobby for product market issues relative to 75.05 for firms that lobby for non–product market issues. Taken together, model-free evidence supports key predictions outlined in our hypotheses, which we test in the following subsection. Hypothesized ResultsTable 4 provides the results of our empirical analyses and hypotheses tests. Recall that Table 4, Column A, displays results of our instrument tests. Column B contains the results of our test of H1. Lobbying has a significant, positive effect on Tobin's q (α1,1 = 4.70; p < .01), in support of H1 and validation of past findings. That is, we confirm a direct effect of lobbying on firm value, even when accounting for customer satisfaction. We also find a positive association between Tobin's q and customer satisfaction (α1,2 = .03; p < .05), R&D spend (α1,5 = 12.26; p < .01), and profit (ω = 8.18; p < .01). The results from Equation 2, shown in Table 4, Column C, indicate a significant negative effect of lobbying on customer satisfaction (α2,1 = −8.37; p < .01), in support of H2. Notably, the negative effect of lobbying on customer satisfaction occurs independent of our lobbying visibility control (ω = −.14; p < .05), explained previously in our sample and data section and described in Table 2. That is, our results establish a negative effect of firm lobbying on customer satisfaction, regardless of whether customers are aware of firm lobbying. Among other controls, we find that operating in a highly regulated industry lowers customer satisfaction (  ϑ   = −2.16; p < .01). Customer Satisfaction MediationWe use the path modeling framework by [96] to test for mediation. As detailed in Table 4, lobbying has a significant, negative effect on customer satisfaction, which has a significant, positive effect on Tobin's q. The mediation test of the indirect path from lobbying to Tobin's q through customer satisfaction (lobbying → customer satisfaction → Tobin's q) is significant. It is the product of the lobbying to customer satisfaction path (lobbying → customer satisfaction) and the customer satisfaction to Tobin's q path (customer satisfaction → Tobin's q). The indirect effect of lobbying through customer satisfaction on Tobin's q is negative and significant (β = −.22, 95% confidence interval [CI] = [−.40, −.04]). Customer satisfaction partially and negatively mediates (competitive mediation) the effect of lobbying on Tobin's q, as we predicted in H3. The total effect (sum of direct and indirect effects) of lobbying on Tobin's q (β = 4.49, 95% CI = [1.23, 7.74]) is smaller than its direct effect (β = 4.70, 95% CI = [1.43, 7.98]), which indicates competitive mediation ([96]), such that the direct and indirect effects are in opposite directions. The direct benefits of lobbying are larger than previously identified when we consider the negative counteracting effect of customer satisfaction. The lobbying–customer satisfaction path accounts for 4.90% of the total effect of lobbying on Tobin's q. Customer-Focused Moderation CEO backgroundThe results in Table 4, Column F, show that a CEO's marketing background significantly and positively moderates the lobbying–customer satisfaction relationship (α3,6 = 12.33; p < .01), in support of H4. The negative effect of lobbying on customer satisfaction decreases, from significantly negative (β = −6.14, 95% CI = [−11.89, −.40]) to positive, though nonsignificant, when a firm has a CEO with a marketing background (β = 6.19, 95% CI = [−1.52, 13.89]). This result is consistent with our expectation that a marketing-focused CEO can align the firm's focus with customers, which lessens the negative effect of lobbying on customer satisfaction. Advertising spend and R&D spendTable 4, Column F, also shows that firm advertising spend and R&D spend each positively and significantly moderate the lobbying–customer satisfaction relationship. As we predicted in H5a, the interaction involving advertising spend is positive and significant (α3,7 = 19.51; p < .01), such that the negative effect of lobbying on customer satisfaction decreases with greater advertising spend, and this effect is significantly negative at low (−1 SD) advertising spend (β = −5.43, 90% CI = [−10.26, −.60]) but is nonsignificant at higher (+1 SD) advertising spend (β = −3.22, 90% CI = [−7.93, 1.48]). Similarly, we find a positive and significant interaction between lobbying and R&D spend (α3,8 = 57.60; p <.10), in support of H5b. The negative effect of lobbying on customer satisfaction decreases with an increase in R&D spend, and this effect changes from significantly negative at lower (−1 SD) R&D spend (β = −6.51, 95% CI = [−12.19, −.83]) to nonsignificant at higher (+1 SD) R&D spend (β = −2.14, 95% CI = [−8.76, 4.48]). These results identify two important counterbalancing levers; firms can significantly lessen the negative effect of lobbying on customer satisfaction through advertising spend and R&D spend. Product market lobbyingProduct market lobbying significantly and positively moderates the lobbying–customer satisfaction relationship (α3,9 = 8.98; p < .01; Table 4, Column F), in support of H6. The negative effect of lobbying on customer satisfaction decreases with an increase in product market lobbying, and this effect changes from being significantly negative at lower (−1 SD) product market lobbying levels (β = −7.16, 95% CI = [−11.60, −2.72]) to nonsignificant at higher (+1 SD) levels (β = −1.49, 95% CI = [−8.75, 5.77]). We thus confirm our assertion that product market lobbying lessens the otherwise negative effect of lobbying generally on customer satisfaction. Additional Moderated Mediation AnalysesAlthough we did not hypothesize moderating effects of CEO marketing background, advertising spend, R&D spend, or product market lobbying for the indirect effect of lobbying on Tobin's q through customer satisfaction, we conduct additional moderated mediation analyses to estimate these conditional indirect effects. We find that lobbying has a significant, conditional, indirect negative effect on Tobin's q through customer satisfaction, but only in the absence of a marketing CEO (β = −.17, 95% CI = [−.33, −.02]). The effect becomes positive, although nonsignificant, with a marketing CEO present (β = .17, 95% CI = [−.13,.48]). The conditional indirect effect also holds only at lower (−1 SD) advertising spend (β = −.15, 90% CI = [−.28, −.02]) and becomes nonsignificant at higher (+1 SD) advertising spend (β = −.09, 90% CI = [−.20,.02]); it similarly persists only at lower (−1 SD) R&D spend (β = −.18, 95% CI = [−.36, −.01]) and becomes nonsignificant at higher (+1 SD) R&D spend (β = −.06, 95% CI = [−.22,.10]). Finally, lobbying has a significant conditional indirect effect on Tobin's q through customer satisfaction at lower (−1 SD) product market lobbying levels (β = −.20, 95% CI = [−.35, −.05]), but this effect is nonsignificant at higher product market lobbying levels (+1 SD) (β = −.04, 95% CI = [−.23,.15]). That is, customer-focused variables positively moderate the negative effect of lobbying on customer satisfaction and the negative indirect effect of lobbying on Tobin's q through customer satisfaction. Put differently, when each of the moderators is at a low level, the conditional indirect effect of lobbying on Tobin's q through customer satisfaction is negative. When each moderator is at a high level, there is no indirect effect. These findings highlight that firms can use these strategic levers to neutralize the negative indirect effect of lobbying on firm value. Mediation Evidence of Customer Focus LossTo further shed light on the relationship between lobbying and customer satisfaction, we provide empirical evidence regarding the loss of customer focus due to lobbying, as predicted by ABV theory. Specifically, we employ an accepted proxy for a firm's customer focus using shareholder communications. Such communications (e.g., letters to shareholders, annual reports, conference calls) provide a clear and immediate measure of the firm's priorities (e.g., [74]). These communications are scrutinized by many stakeholders, and therefore, firms are intentional about what they share. Further, these communications should reflect firm priorities and key areas of strategic focus ([31]). For example, prior research has linked such communications with the firm's innovation priorities and outcomes ([93]).For this study, we use earnings conference call transcripts to create a measure of a firm's customer focus. Firms voluntarily disclose large volumes of information during earnings calls (e.g., [14]). The calls also include question-and-answer sessions that capture information beyond the firm's prepared remarks. Finally, noting their frequency, we believe these transcripts offer good potential for accurately capturing firm attention to (or away from) important priorities. In line with existing literature (e.g., [10]), we create a count of customer-focused words (using the dictionary created by [93]; Web Appendix B) in quarterly earnings call transcripts, as a percentage of the total number of words, and then take the mean value over four quarters in a fiscal year.The resulting data set refers to earnings conference call transcripts for 75 firms. The mean customer focus value is.78%, and it ranges from.35% to 1.35%. We test whether this measure of customer focus mediates the lobbying–customer satisfaction relationship using the [69] Model 4 PROCESS macro with 10,000 iterations. Consistent with our expectations, lobbying has a negative effect on customer focus (β = −.34, 95% CI = [−.43, −.25]), which has a positive effect on customer satisfaction (β = 1.43, 90% CI = [.18, 2.69]). Customer focus significantly and negatively mediates the lobbying–customer satisfaction relationship (β = −.49, 90% CI = [−.94, −.05]), highlighting the pathway for this negative effect. Robustness Checks Counterfactual AnalysisWe use a counterfactual analysis to estimate the effect of high lobbying on customer satisfaction and its indirect effect on firm value via customer satisfaction. Because we only observe firms in their actual high-frequency or low-frequency lobbying states, we identify their counterfactual matches (i.e., firms similar to them on other variables but with different lobbying) using the nearest-neighbor matching procedure (see Web Appendix C). Using this approach, we find that lobbying has a direct negative effect on customer satisfaction and an indirect negative effect on Tobin's q via customer satisfaction. The results of this counterfactual analysis are consistent with the results of our focal study models. Parallel Mediation Through Market ShareAlong with providing evidence of competitive mediation in the lobbying–firm value relationship through customer satisfaction, we follow prior research guidelines that advise exploring alternative mediation explanations ([96]). According to regulatory capture theory, lobbying can positively affect firm value through other routes, such as when lobbying protects the industry status quo and allows dominant firms to gain power, hinder competitive market entries, and lower competition ([87]). In this case, lobbying should enhance firm value through an increase in its market share.We empirically evaluate this parallel mediation. While simultaneously accounting for the lobbying → customer satisfaction → firm value relationship, we examine whether lobbying → market share → firm value indicates positive mediation. The formal test uses [69] Model 4 PROCESS macro with 10,000 iterations. We find that market share partially mediates the relationship between lobbying and firm value. Lobbying has a positive effect on market share (β = .03, 90% CI = [.00,.06]), which in turn has a positive effect on firm value (β = 1.32, 95% CI = [.50, 2.15]). Thus, market share significantly and positively mediates the lobbying–firm value relationship (β =.04, 90% CI = [.00,.09]). We acknowledge there may be other routes (e.g., taxes paid, contracts) but limit our approach to providing evidence of one additional path through which lobbying influences firm value. Brand Equity as an Alternative Customer OutcomeWe examine the effect of lobbying on an alternative customer outcome: brand equity. This variable refers to the outcomes and preferences that accrue to a branded option compared with those that accrue to a similar, nonbranded alternative ([ 1]). Brand equity captures awareness, familiarity, and brand associations, so it drives both new customer acquisition and customer retention ([85]). To test for this effect, we adapt a sales-based brand equity measure ([ 1]; [25]) that reflects the revenue difference between branded and nonbranded alternatives. In our firm-level data, we lack measures of customer satisfaction or sales for nonbranded alternatives, so we proxy for their sales by taking the median of two-digit SIC sales. Thus, our measure of sales-based brand equity is the difference in firm revenue relative to industry median revenue ([44]). The analysis reveals that lobbying has a negative effect on brand equity (β = −17.94; p < .01), thus confirming our findings for another customer outcome (Web Appendix D, Column A). Model Misspecification TestsWith a sequence of robustness checks, we ensure the validity of the focal relationship between lobbying and customer satisfaction. Web Appendix D presents complete reporting of these tests, which include analyzing a larger data set of ACSI firms by setting advertising spend and R&D spend to 0 if they are not reported (Column B), constructing a data set of firms not included in our sample due to missing observations for our covariates (Column C), analyzing all data to include firms that do not report both advertising expenditures and R&D expenditures (Column D), testing the effect of lobbying on the difference between a firm's customer satisfaction value and average industry customer satisfaction value to account for industry effects in a different way than controlling for them (Column E), and scaling firm lobbying expenditures by the sum of its advertising expenditures and R&D expenditures to capture the relative strategic emphasis (Column F). Our focal results remain consistent in all cases, further strengthening the confidence in our findings. General Discussion and ImplicationsConsistent with regulatory capture theory, lobbying is a positive driver of firm performance, and companies are likely to continue using it. However, our findings also reveal that costs of firm lobbying become apparent when accounting for customer effects. Specifically, we augment regulatory capture theory by building arguments using the ABV of the firm to explain how firm lobbying negatively affects customer satisfaction. To our knowledge, this investigation is the first to consider the prevalent, growing practice of firm lobbying in relation to customer outcomes. We advance research in marketing by showing that firm lobbying has a worrisome dark side: it reduces customer satisfaction, a critical customer performance outcome that is foundational to marketing theory and practice.We also draw from the ABV perspective to suggest a set of moderators (CEO background, advertising spend, R&D spend, and product market lobbying), each of which lessens the negative lobbying–customer satisfaction relationship by preventing customer focus loss. We describe how these moderators, through aligning and counterbalancing means, orient the firm's focus to customers and minimize the negative effects of lobbying on satisfaction. Finally, by testing for customer focus loss through shareholder communications, we provide empirical evidence that the negative effect of lobbying on customer satisfaction is driven by a decrease in a firm's attention to its customers. These findings provide insights into how lobbying hurts customer outcomes. To the best of our knowledge, they offer the first empirical evidence that lobbying reduces the firm's customer focus and thereby the customer satisfaction it achieves. Managerial ImplicationsThese findings have important takeaways for managers. Existing research on firm lobbying has not considered customer effects, which is surprising given the critical role of customers for firm growth and survival ([84]). Our research shows that firm lobbying strongly and negatively affects customer satisfaction, and we offer some preliminary evidence that it may negatively affect brand equity–related measures as well. Moreover, we find that lobbying erodes customer satisfaction at a faster rate than advertising spending can build it. Although raw lobbying spend is less than advertising spend, the negative effect of lobbying (β = −8.37) is greater than the influence of advertising spend (β = 7.15) on customer satisfaction (Table 4, Column C). Our mediation analysis (H3) further suggests that if it is not accounted for, the indirect negative effect of lobbying on Tobin's q through customer satisfaction can negate the benefits of a  $  1,000,000 increase in R&D spend, and the positive effects it would have on Tobin's q, with an increase of only  $  55,727 in lobbying spend.Fortunately, our findings reveal managerially relevant strategic levers firms can use to neutralize the negative indirect effect of lobbying on firm value. Taken together, moderation findings suggest that instead of keeping government affairs separate from marketing functions, the two should work together, through aligning and/or counterbalancing means, to enable firms to achieve the highest returns on their lobbying efforts. Indeed, our findings reveal useful synergies that can be derived from these firm attention–directing mechanisms.As one example, we show that advertising spend and R&D spend counterbalance lobbying spend by reorienting collective firm attention to customers. These moderation findings also suggest that advertising spend and R&D spend serve as important signals of firm focus on customers to internal and external stakeholders and can combine with lobbying to produce firm benefits. Examples we cite from pharmaceuticals and medical device manufacturers highlight how some firms that spend considerably on lobbying use tools from the marketing environment to their advantage. The counterbalancing role of advertising spend and R&D spend is an additional benefit to the already well-known advantages that these expenditures produce for firms.To the best of our knowledge, our study also is the first investigation to differentiate issues for which a firm can lobby in our model. We find that lobbying for product market issues, relative to non–product market issues, weakens the negative effect of lobbying spend on customer satisfaction. This finding suggests that not all lobbying activities are the same when it comes to their impact on customer satisfaction. Second, although this study just scratches the surface on how lobbying can affect customer outcomes, this moderating relationship suggests that some lobbying issues can reorient firm attention to customers. For example, Apple spent $6.65 million on lobbying in 2020 (opensecrets.org). Of those resources, although some were devoted to non–product market issues, some were devoted to product market issues, such as their music streaming division. Like Spotify, Pandora, and others, Apple lobbies for issues that allow the firm to provide customers greater access to different musical genres, artists, and albums. Although controversial among individual musicians and publishers, customers are the ultimate beneficiaries of this lobbying, via greater access and lower fees.Additionally, we find that the negative effect of lobbying on customer satisfaction is weaker for firms with CEOs that have a marketing background, which we argue is because these CEOs direct collective attention to customers and align firm lobbying activities to be customer focused as well. This finding has major corporate governance ramifications. By effectively aligning two disparate environments (i.e., customer environment and regulatory environment), a marketing CEO can produce important, positive effects for the firm. These novel benefits provide another compelling reason to boards of directors engaged in top management recruiting for hiring a CEO with a marketing background. Theoretical ContributionsA critical shortcoming of prior research that examines customer satisfaction antecedents is that most studies only note a firm's product market strategies. Satisfaction is a function of customer expectations, perceived quality, and perceived value ([ 5]; [35]), but firm lobbying can significantly influence all these dimensions. With this initial empirical evidence, grounded in compelling theory, we propose that a firm's non–product market strategy (i.e., lobbying) can significantly influence product market performance. Additional non–product market activities and their role on customer outcomes warrant investigation and theoretical refinement to the broad customer satisfaction literature. For example, investigating firm attention to corporate social responsibility initiatives, relative to customer outcomes, also may identify surprising and unintended insights for customer satisfaction theories.The ABV of the firm is still gaining momentum in marketing theoretical development. We provide one framework for how the ABV can be used in complement with extant theories for refined insights about firm behavior. We contribute to regulatory capture perspectives by identifying a critical shortcoming of this theory regarding firm attention to customers. Our theory development also orients firm lobbying behavior squarely in the marketing literature, representing a novel contribution. Foundational theoretical premises of the ABV suggest additional theoretical applications and extensions in marketing. For example, the manner in which a firm attends to various issues, stakeholders, and environments can have important implications for marketing intelligence dissemination and organizational learning (e.g., [39]). The ABV theoretical emphasis on creating attention structures and channeling that attention suggests that marketing theories about information flows may be ripe for integration. Finally, additional marketing outcomes beyond customer satisfaction may benefit from analysis through an ABV lens. Innovation theories, in particular, may be advanced by incorporating the nature of firm focus. Attention distribution may influence the extent to which key decision makers are able to produce radical versus incremental innovation possibilities for the firm, among other outcomes.Theoretical premises of the ABV also may shed new light on research findings that reveal the perils of a dual firm focus. Different strategic foci have the potential to pull attention away from benefiting customers. As our findings on lobbying show, it can lead to undesirable customer and firm outcomes. Additionally, in the face of negative effects, it may be useful to determine whether firm attention was spread too thin, or senior managers struggled with strategically opposed priorities. We offer evidence in support of the ABV premises that a firm's attention is even more constrained and limited than the expenditures it can dedicate to various initiatives. Through an ABV lens, we suggest that explicit consideration of firms' limited attention capacity and the influential role of aligning and counterbalancing forces may help clarify prior findings. Our investigation refines theory around these two different mechanisms, building from [63] foundational premises. Although ABV literature has proposed moderators that attenuate negative effects from loss of focus (e.g., [31]), our study is the first to show that different forces can redirect firm attention by using distinct yet complementary means. Future research should continue to disentangle aligning and counterbalancing mechanisms regarding firm attention and the ability to maintain desirable sources of focus.Finally, in our empirical findings, lobbying is manifest in negative outcomes; customers experience reduced satisfaction regardless of their knowledge of firm lobbying behavior. Yet customers seek out information about such firm activities, and firms also increasingly communicate with customers about politically motivated behaviors ([47]; [62]; [80]). Consumer-side theory, related to perceptions and evaluations of firm attention diversion away from customer priorities, remains underdeveloped. Customers disapprove of strong business–government relationships, but why is that true? Although regulatory capture theory highlights the risks of government–business interaction, it is not clear that this theory's premises about firms' undue influence translates to customer perceptions and evaluations. Theoretical grounding of customers' strong negative reactions to lobbying and the larger family of firm political influence strategies is critical and needed. With the rapid pace of many technological advancements, business–government interactions become increasingly relevant to customers, their consumption experience, and their overall well-being. Theoretical advancement must work to further import these interactions into marketing scholarship. Public Policy ImplicationsThe potential anticompetitive effects of lobbying, coupled with an erosion of customer satisfaction, suggest public policy implications of our results. Public sentiment suggests a growing distaste for lobbying and close ties between business and government, but few efforts have been made to curb the practice. Our findings suggest that greater limits may be warranted in some areas to promote positive customer outcomes. Although counterintuitive, greater lobbying limits may work to benefit firms, by redirecting focus to customers and by improving the quality of the firm's long-term customer outcomes.Regardless of whether greater limits on lobbying are imposed, our study supports the need for continued disclosure mandates. The Lobbying Disclosure Act gives customers, special interest groups, advocates, and researchers more information about the role of lobbying in modern business practice. Although this reporting necessarily creates a burden for firm compliance, it may have the unexpected benefit of showcasing when firms lobby for the customer's interests, as in the music streaming example cited previously. When firms lobby for issues that benefit customers, mandatory disclosures can visibly signal a customer focus. Firms could use these disclosures as evidence in support of customer-focused lobbying.However, there is also compelling evidence that the information contained in lobbying disclosure reports does not go far enough in either detail or metrics. In 2018, shareholder resolutions asking for greater transparency of lobbying activities were presented to 50 prominent U.S. companies ([82]). Currently, firms are required to disclose lobbying expenditures quarterly, but the information is very basic, and reporting can be inconsistent. Identifying specific dollar values devoted to any given issue would further understanding of lobbying's performance outcomes. Greater detail about the direction of firm lobbying (i.e., in support of or against an issue) also would further research objectives and give customers and other stakeholders greater insight into a firm's position on important issues. Indeed, our results show that lobbying issues matter, and clearer communication about them could benefit multiple stakeholders. Added scrutiny may further shift firm attention to customer-focused issues. Further Research and ConclusionSeveral additional research questions arise from our findings. First, we provide initial evidence that lobbying can lower customer satisfaction, and continued research could examine other firm political behaviors and their effects on customer outcomes. For example, studying customer awareness of lobbying might provide added nuance and reveal a customer-side path that parallels our firm-side focus. Second, although we focus on how lobbying affects customer satisfaction, we provide preliminary evidence that lobbying can influence other customer metrics such as brand equity. Future studies should examine the role of lobbying on customer metrics such as brand equity using additional measures. Third, firms frequently lobby to achieve specific goals. We know of no other studies of differential effects based on the issues being lobbied, so additional research is needed to disentangle the effect of specific lobbying issues, beyond just product market versus non–product market focus, on customer outcomes and firm performance. Perhaps product market lobbying issues lead to different outcomes according to individual areas of emphasis. Alternatively, lobbying effects seemingly might be weaker if the issues are further removed from a firm's focal business domain (e.g., lobbying for environmental issues by a software company, lobbying for guns by an arts organization). Fourth, further research could explore antecedents of firm attention. For example, lobbying might increase market share, to the extent that it even might produce a monopoly. Firms that gain market share through lobbying also might exhibit greater hubris. Thus, in addition to their diverted attention, managers of these firms may have excessive confidence, which could lead them to discount customer priorities.Firm lobbying has a worrisome dark side when accounting for customer effects. Although our research advances important findings about lobbying outcomes on customer satisfaction, we are only just beginning to realize the many ripple effects from political influence strategies on firm performance and the broader competitive market environment. We hope marketing researchers continue to investigate how marketing strategies and political strategies interface on a variety of firm and societal outcomes.  "
36,"The Impact of Corporate Social Responsibility on Brand Sales: An Accountability Perspective Consumers are increasingly mindful of corporate social responsibility (CSR) when making purchase and consumption decisions, but evidence of the impact of CSR initiatives on actual purchase decisions is lacking. This article introduces a novel brand accountability–based framework of consumer response to CSR initiatives, which categorizes CSR efforts as ""corrective,"" ""compensating,"" or ""cultivating goodwill."" Leveraging a database of CSR press releases by leading consumer packaged goods brands, the authors examine the effect of the different types of CSR announcements on brand sales. The findings suggest that CSR initiatives that genuinely aim to reduce a brand's negative externalities (""corrective"" and ""compensating"") lift sales, whereas CSR actions focused on philanthropy (""cultivating goodwill"") can hurt sales. The authors propose two moderators—CSR reputation and CSR focus on environmental or social causes—and a mechanism for these effects, which they examine under controlled experimental settings. The experimental results show that, conditional on CSR reputation, consumers perceive varying degrees of sincerity in the different CSR types and that sincerity mediates the effect of CSR type on purchase intentions. Overall, the results suggest that consumers are more inclined to reward firms that directly reduce the negative by-products of their own business practices than to be impressed by public goodwill gestures.Keywords: corporate social responsibility; sustainability; CSR reputation; brand sincerity; environmental initiatives; social initiativesCorporate social responsibility (CSR)—defined as discretionary business practices and contributions of corporate resources intended to improve societal well-being ([51]) —is increasingly present in consumer consciousness. As more consumers support brands that contribute to the greater societal good, companies have incentives to engage in some form of CSR ([21]; [44]). Effective CSR can enhance corporate perceptions, differentiate products, and reduce the impact of public relations miscues ([18]; [49]; [62]). However, despite a stream of research that has documented various positive effects of CSR, a causal link between a firm's CSR activities and actual consumer purchase decisions has not been established. Thus, one goal of this research is to provide evidence of the effect of CSR initiatives on brand sales.Another goal of this article is to provide a categorization of CSR and an examination of the contingent effects of different types of CSR on brand sales. CSR typically spans a wide array of potential activities, including philanthropic community support, environmental initiatives, diversity promotion, employee support, changes to products and supply chains, and corporate governance issues. These activities have been classified in extant literature into categories such as philanthropic versus business practice ([42]), environmental versus product-focused ([47]), or proactive versus reactive ([80]) CSR. The breadth of these categorization schemes, however, can complicate both the study and the efficient managerial deployment of CSR initiatives. We aim to provide structure to this variety by using an important, but understudied, characteristic of CSR: the extent to which CSR addresses a brand's liability and thereby demonstrates accountability in consumers' minds.Anecdotal evidence suggests that consumers care about brands being accountable for their actions. For instance, hoping to better understand what types of CSR activities consumers would prefer, Coca-Cola recently tested a battery of potential initiatives using a series of consumer focus groups. These initiatives ranged from social to environmental, and from the purely philanthropic (women's economic empowerment) to the seemingly apologetic (helping address obesity). In the end, the initiative that most clearly addressed and preempted the brand's own potential negative social and environmental impact (reduced water consumption) elicited the most favorable consumer response ([19]). Likewise, a recent public survey about CSR found that a majority of respondents preferred that firms adopt business operations aimed at minimizing their own societal and environmental harm ([21]). The degree to which a brand's CSR efforts actually address any of its own negative externalities (i.e., harmful effects on society and the environment) may thus help predict consumer response and guide the management of CSR decisions ([40]).Using accountability as a basis to address negative externalities, we distinguish between three types of CSR engagement: correcting for the potential negative societal or environmental impact of a brand's business operations by making changes to those operations, compensating for the negative impact of a brand's business operations without making changes to those operations, and cultivating goodwill[ 6] through prosocial acts that are not directly related to any negative impact of a brand's business operations. We argue that this conceptualization captures important and fundamentally distinct CSR-related concerns and expectations among consumers and covers the vast majority of CSR activities. Using this typology, we develop a framework and conduct one of the first large-scale examinations of the effect of different types of CSR on brand sales. We extend this framework and complement the secondary data analysis with experimental evidence that both replicates these results and explores the mechanism underlying the differential effect of CSR type on consumer response.Our work contributes to the literature in three important ways. First, we provide a novel typology of CSR based on the underexplored concept of firm accountability, whereby firms take responsibility for the consequences of their operations ([28]). We develop our framework based on this typology in a consumer-centric manner, drawing on sociopsychological theory, invoking the role of responsibility and restitution in attitude change ([16]). This typology encourages greater integrity in the practice of CSR by highlighting the alignment of societal and business interests. It also provides more actionable managerial insights because it directly links CSR initiatives to firm actions and is more granular than previous categorizations (e.g., dual categorizations such as CSR focused on primary vs. secondary stakeholders, CSR focused on business practice vs. philanthropy).Second, to our knowledge, this article represents one of the first attempts at leveraging field data to offer direct empirical support for the existence of an economically significant effect of CSR on brand sales (see Table 1). Although prior work has drawn valuable insights from CSR case studies, work that involves actual purchase behavior has been rare (for exceptions, see [ 4]] and [46]]). Findings from experimental studies suggest that CSR may lead to greater purchase intent and increased brand loyalty ([26]), though a few behavioral studies also suggest that under certain conditions CSR can lead to negative attitudinal outcomes (e.g., [31]; [74]). Moreover, an attitude–behavior gap caused by social desirability bias may exist, particularly in contexts that involve social and ethical issues ([ 7]; [70]). Our focus on brand sales offers the benefit of performing a real-world, decision-based examination of how CSR shapes actual consumer response. Our findings are further nuanced by the inclusion of two moderators of the relationship between types of CSR initiatives and brand sales: ( 1) the role of the CSR reputation of the firm and ( 2) the environmental versus social focus of CSR efforts.GraphTable 1. Representative CSR Literature: Measurement, Type, and Classification of CSR Initiatives in Extant Research. ResearchExperimental DataSurvey DataSecondary Data (Firm Level)Secondary Data (Brand Level)EffectDependent VariableType of CSR AnalyzedClassification of CSRAilawadi et al. (2014)✓Positive (negative) for behavioral loyalty for CSR (not) related to customer's direct exchange with the firmAttitude, behavioral loyaltyEnvironmental and social responsibilityNoneAnselmsson and Johansson (2007)✓PositiveAttitude/purchase intentProduct, social, and environmental responsibilityNoneBecker-Olsen, Cudmore, and Hill (2006)✓Positive (negative) for purchase intent for high- (low-) fit CSRAttitude/purchase intentEnvironmental and social responsibilityNoneBhardwaj et al. (2018)✓PositiveAttitude/purchase intentSocial responsibilityCompany-ability-relevant CSR (positively impacting performance) and company-ability-irrelevant CSR (no impact on performance)Buell and Kalkanci (2021)✓PositiveBookstore sales from a field experimentEnvironmental and social responsibilityInternally or externally focused on the value chainChernev and Blair (2015)✓PositiveProduct evaluationsPhilanthropyNoneDu, Bhattacharya, and Sen (2007)✓PositiveLoyalty, consumer advocacy for the brandEnvironmental and social responsibilityNoneGroza, Pronschinske, and Walker (2011)✓PositiveAttitude, purchase intentEnvironmental responsibilityReactive versus proactive CSRHomburg, Stierl, and Bornemann (2013)✓PositiveCustomer loyaltyCorporate CSR measureBusiness practice CSR engagement and philanthropic CSR engagementInoue, Funk, and McDonald (2017)✓✓PositiveAttendance at football gamesSurvey-based perceived CSRNoneKang, Germann, and Grewal (2016)✓Positive (no effect) when CSR is a good management (penance) mechanismFirm performance (Tobin's q)Corporate CSR measureNoneLuchs et al. (2010)✓Positive (negative) when gentleness (strength) product attributes are valuedConsumer preferenceSustainabilityNoneLuo and Bhattacharya (2006)✓Positive; negative for firms with low innovativeness capabilityStock return, firm performance (Tobin's q)Corporate CSR measureNoneMishra and Modi (2016)✓Positive for stock returns; negative for risk (community CSR, n.s.)Stock returns, Idiosyncratic riskCorporate CSR measureNoneNewman, Gorlin, and Dhar (2014)✓Positive effect greater for unintentional (vs. intentional) product changesPurchase intentEnvironmental responsibilityNoneSen and Bhattacharya (2001)✓PositiveCompany evaluation/purchase intentionCorporate CSR measureNoneServaes and Tamayo (2013)✓Positive (negative/n.s.) for firms with high (low) customer awareness; effects reversed for firms with poor reputations as corporate citizensFirm performance (Tobin's q)Corporate CSR measureNoneWagner et al. (2009)✓NegativeAttitudeCorporate CSR measureReactive versus proactive CSR, abstract versus concrete CSR policy, inoculation strategy or notYoon, Gurhan-Canli, and Schwarz (2006)✓Positive (negative) when consumers learn about low-benefit-salience CSR through a neutral (company) sourceCompany evaluationsCorporate CSR measureHigh- versus low-benefit-salience CSROur study✓✓Positive (negative) when firms take (do not take) accountability for negative externalities(1) Brand sales, (2) purchase intentionsCSR measure encompassing product, social, and environmental responsibility as well as philanthropyAccountability based: corrective, compensating, and cultivating CSR Third, we use laboratory experiments to provide process evidence regarding the perceived sincerity of brand motives. Perceived sincerity serves as a mechanism that underlies the changes in consumer purchase behavior associated with CSR initiatives ([ 8]). Specifically, we examine the effects of CSR type on consumer purchase intention and the mediating role of perceived brand sincerity. Results from experiments largely support our findings obtained with brand sales data for corrective and compensating CSR and show that perceived brand sincerity mediates the effect of CSR type on purchase intention and CSR reputation moderates the mediation chain. Our framework is depicted in Figure 1.Graph: Figure 1. Conceptual framework of the effect of CSR initiative on purchase intentions and sales.To investigate the effect of CSR on brand sales, we collect CSR press releases issued by a comprehensive set of prominent consumer packaged goods (CPG) brands, documented in the CSRwire database as well as on brand websites between the years 2002–2011. These data contain the announcement date as well as the textual content of all CSR announcements made by these brands in this time window. We then collect detailed sales data from the Information Resources Inc. (IRI) consumer panel data set for the brands that announced CSR initiatives as well as a set of close substitute brands that did not engage in CSR. After merging the two databases, our sample includes a total of 55 brands that announced CSR initiatives and 194 brands that did not, spanning 21 CPG product categories.[ 7] Our CSR event list contains 80 actual CSR initiatives (27 corrective actions, 19 compensating actions, and 34 cultivating goodwill actions) that were announced by the corporate parents of the 55 focal brands.We specify an empirical model estimated on the sales of the focal brands as well as those of peer brands from the relevant product categories, measured one year before and one year after the focal brands implemented CSR events. The results from our empirical analyses indicate that the type of CSR effort undertaken has distinct implications for brands engaging in CSR. While, on average, corrective and compensating CSR actions provide a boost to the sales of participating brands, cultivating CSR actions lead to a slight drop in sales. This negative effect of cultivating goodwill actions is in line with the behavioral literature that has documented, under certain conditions, a reduction in purchase intentions for firms that engage in CSR ([ 8]; [74]). Cultivating goodwill may reduce sales because it detracts resources that could be used to support the brand's primary stakeholders, such as customers and channel partners, and redirects them to external constituencies that may not respond by purchasing the brand's products. At the same time, and consistent with these findings, the results obtained from our experiments suggest that cultivating goodwill CSR actions are viewed as less sincere than the other two types of CSR.In summary, findings from our analysis of brand sales, in conjunction with results from laboratory experiments, suggest that taking an accountability-based view of CSR may offer useful insights to managers looking to enhance the consumer impact of CSR actions. We next present our conceptual framework and hypotheses, followed by the description of the data, methods, and results for the secondary data empirical analysis. We conclude with a description of the experiments, followed by a discussion of implications from our research. Conceptual FrameworkThe literature focused on the impact of CSR is vast and has evolved primarily in two separate streams: one focused on the financial consequences of corporate CSR (e.g., [49]; [59]) and one focused on how CSR impacts antecedents to consumer purchase behavior, including consumer attitudes and purchase intentions (e.g., [26]; [56]; [74]). Most studies show a positive effect of CSR, though some authors identify conditions under which CSR has null or negative effects (Table 1).There is less consistency in the type of CSR analyzed. CSR, whether measured at the brand or corporate level, encompasses actions that can pertain not only to products, employees, or business partners but also to the community or special groups of stakeholders as well as more general environmental or philanthropical initiatives ([64]). For instance, [42] distinguish between business practice CSR, which targets the firm's primary stakeholders, and philanthropic CSR engagement, which targets the firm's secondary stakeholders. In turn, [47] examine product-focused CSR actions and environmentally focused CSR actions. Other researchers differentiate between proactive CSR, in which firms engage in CSR before consumers receive potentially negative firm information, and reactive CSR, in which firms conduct CSR to protect their image after reports of an irresponsible action ([37]; [80]). Perhaps because of the large variety of CSR initiatives that firms can undertake, very few articles, as illustrated in Table 1, attempt to comparatively assess the effect of different types of CSR.An underresearched aspect of CSR that is under increasing public scrutiny is the extent to which the costs of a firm's quotidian operations are passed on to the general population. Such costs include, for instance, waste, pollution, or use of labor from developing countries with weak labor laws ([40]). A categorization scheme that focuses on the firm's responsibility for varied externalized costs would help managers choose the appropriate type of CSR action from among a cornucopia of available options. This is all the more critical as managers face increased scrutiny of their firms' externalities from an environmental and social perspective.The most useful categorization would be one that is actionable, in the sense that it both readily translates to specific actions and wields distinct effects on metrics helpful to managers. The extant literature is lacking on this latter dimension as well. Research on corporate CSR is often conducted using complex CSR indices aggregated at the corporate level (e.g., [58]), while behavioral studies typically leverage metrics with lower external validity, such as laboratory participants' evaluations of fictional CSR initiatives (e.g., [31]). We intend to tackle both shortcomings in the literature by proposing a categorization based on the notion of firm accountability and by examining the impact of this categorization on brand sales. We do so after carefully surveying the literature and noticing that in the few instances where a negative effect of CSR was documented, it was because consumers did not find the CSR actions to be meaningful. For instance, [ 8] show that companies hoping that their sales would improve as a result of their efforts to combat homelessness or domestic violence find instead that purchase intentions for the companies' products decrease. The authors attribute this effect to consumers being skeptical that the firms sincerely want to make a positive change.Accountability in CSR represents firms' acknowledgment that their business operations create negative externalities, which may include pollution, waste generation, or downstream consumer health issues. Although these externalities may vary, they do constitute a liability that consumers may expect firms to correct by taking specific actions ([ 5]).First, to directly reduce its negative impact on society or the environment, a firm may adopt changes to its business operations. Examples include product or packaging modification, responsible ingredient sourcing, ethical labor practices, or expansions to the existing product line to cater to consumers at the bottom of the pyramid. Second, a firm may choose to make philanthropic or service contributions aimed at offsetting its negative externalities without changing its business operations (e.g., donations to a cause benefiting stakeholders negatively affected by the brand, cleanup efforts, in-kind donations). Finally, a firm may engage in philanthropic activities unassociated with its negative externalities. Such activities may be intended to engender consumer goodwill (e.g., public relations campaigns, scholarships, endowments). Drawing from these three possibilities, we propose a typology of corrective, compensating, and cultivating goodwill CSR. Corrective, Compensating, and Cultivating Goodwill CSR ActivitiesCorrective CSR is a form of CSR whereby a brand attempts to minimize its negative impact on society or the environment via actual changes to its products or business operations. For example, a bottled water brand may reduce the amount of plastic used in its bottles, or a retail brand may work on providing more favorable working conditions for its labor force. We argue that explicit changes to a brand's products/operations targeting reduced societal harm represent, in consumers' minds, an acceptance of accountability along with restorative action ([30]). Corrective CSR actions share similarities with Porter's ""Shared Value"" concept wherein companies find business opportunities in social problems ([71]). Corrective CSR, however, has the distinct goal of minimizing one's harm to society, rather than the deliberate search for a business opportunity within an existing social problem (generally not of the firm's making).In contrast, compensating CSR involves initiatives whereby a brand addresses its negative externalities ""indirectly"" (i.e., no actual changes to its products or business practices occur). Compensating CSR initiatives thus represent an implicit acceptance of accountability with attempted redress (e.g., charitable giving, volunteering). For example, a bottled water brand may donate money to plastic recycling programs. While corrective and compensating CSR actions are similar in their implicit acceptance of firm accountability for the negative externality, a key difference is that in the latter case there is no direct restitution offered in the form of actual product or business practice changes. Nonetheless, research in interpersonal relationships demonstrates that an apology without restitution is more effective than no apology at all ([16]).Finally, when engaging in cultivating goodwill CSR, brands do not address their negative externalities but, instead, offer support for one of an endless variety of unrelated good causes. In this case, brands make no strides toward the acknowledgment of responsibility for any negative externality. Many philanthropic efforts may fall into this category. For example, a bottled water brand may donate money to literacy programs. Although the benefiting cause may be worthwhile, consumers may see these CSR initiatives as a failure by firms to acknowledge any liability arising from their operations. We expect that consumers may view such initiatives as insincere and potentially as a waste of corporate resources ([53]).The three types of CSR that we study can target both primary and secondary stakeholders and encompass both business practice and philanthropic CSR engagement ([42]). At the same time, there are clear theoretical differences between the three types of CSR; for instance, corrective and compensating initiatives are rooted in separate strategies outlined in the theory of image restoration ([ 9]). Categorizing a CSR initiative into one of the three types involves answering two questions: First, does the CSR initiative address the brand's own social or environmental harm by making changes directly to the company's business operations (i.e., product, supply/distribution network, labor practices)? If so, it is a corrective action. If the initiative does not, then the next question is, does the CSR initiative address a social or environmental harm for which the brand's business operations are perceived as bearing responsibility? If so, then it is a compensating action. However, if it is addressing a social or environmental issue for which the brand bears no clear responsibility and involves no changes to its product nor business operations, then it is a cultivating goodwill action. The Effect of Corrective, Compensating, and Cultivating Goodwill CSR on Brand SalesConsumers often assess a brand and its actions as they would other members of society ([ 3]). As with interpersonal relationships, consumers often evaluate brands positively if they conform to accepted behavioral norms and negatively if they violate these norms. Irresponsible brand behavior toward society or the environment represents one form of social norm violation that consumers are likely to disapprove of ([ 1]; [47]).Just as consumers can punish brands for violations of social norms through negative evaluations, attitude, or behavior toward a brand, they can also forgive brands that take accountability for the harm that they may cause. The psychology literature explains the link between accountability and forgiveness. Apology, which incorporates an acknowledgment of violated norms, particularly if coupled with restitution, which involves restorative action and remediation, has been shown to promote forgiveness ([16]). In the same vein, consumers are more likely to favorably evaluate brands that acknowledge their own shortcomings and perform restorative actions. The literature on brand crises also provides support for this assertion. Restorative actions taken during brand crises that involve both an acknowledgment of the problem and plans for remedial actions can be effective at repairing brand attitudes ([30]). Moreover, recent research suggests that consumers understand if the CSR efforts of the firm are focused on its own value chain, and they are more likely to purchase from such firms than from their peers whose CSR is external to the value chain ([15]).These arguments suggest that corrective CSR actions, which convey the highest level of accountability to the firm stakeholders, will be received most positively by consumers. Likewise, there is also some level of implicit accountability in compensating CSR, though not as direct as that advanced by corrective actions. While compensating CSR may provide a weaker form of restitutive action than corrective CSR, we still expect compensating CSR to increase sales.In contrast, consumers may view cultivating goodwill CSR as disingenuous or wasteful even if the cause supported by the firm is worthwhile. They may perceive this type of initiative as an attempt to ""check a box,"" and thereby as an insincere approach to CSR that fails to acknowledge the potentially negative consequences that the firm's products or operations may have on society. Prior literature has found some support for this assertion, as CSR initiatives can lead to reduced purchase intent or other negative attributions when consumers believe that the initiatives come at the expense of investments that could improve corporate abilities (e.g., [74]). In addition to consumers, retailers are also important stakeholders who may view CSR initiatives as redirecting resources that could have been used to more directly support an increase in sales, such as price or display promotions. Moreover, both consumers and retailers may believe that corporate philanthropy is driven more by managers' desire to enhance their personal reputations than by stakeholders' interests ([79]). In summary, stakeholders may feel, at the margin, disenfranchised by initiatives meant to cultivate the goodwill of unrelated groups, resulting in a negative impact on sales. Formally, we expect the following: H1:  Corrective and compensating (cultivating goodwill) CSR initiatives have an overall positive (negative) effect on consumer purchase intentions and, consequently, on brand sales. The Moderating Effect of Firms' CSR Reputation and CSR Focus on the Relationship Between CSR…Extant CSR literature can guide us in establishing boundary conditions for the effects hypothesized in H1. We focus on two factors that emerge from the literature as likely to have a role in determining the effectiveness of CSR initiatives. First, firms' CSR reputation is particularly important in shaping consumers' reactions to a firm's CSR activities. In addition to establishing expertise and increasing the credibility of CSR initiatives, firms' CSR reputation can influence product evaluations and, in instances of product harm, can temper consumers' negative evaluations of the brand ([50]). Second, the focus of CSR (environmental or social) is one of the fundamental characteristics of CSR highlighted in the literature ([22]). While initiatives in both domains have been found to have a positive impact (e.g., [ 6]; [26]), their relative contribution to the success of a CSR initiative has not been clearly established. In addition, by focusing on the interaction between CSR focus and type, we provide managers with a 2 × 3 matrix of possible CSR initiatives that can help them adopt a CSR outreach that is most appropriate for their firm. CSR reputationThe expectation disconfirmation paradigm suggests that consumers' responses to CSR initiatives will be contingent on their assessment of a firm's CSR reputation, defined as stakeholders' assessment of the past performance and success of the firm's CSR activities ([63]). Favorable firm reputations can influence the actions of firms' stakeholders, including consumers ([33]). Moreover, brand activities that are congruent with prior CSR reputation are less likely to change consumer brand perceptions and may have little effect on consumer response ([68]). Consequently, we expect the effects of all three types of CSR initiatives on sales, whether positive or negative, to be smaller in magnitude for firms with high CSR reputations, as these initiatives confirm what consumers already believe.Conversely, CSR actions from firms with lower CSR reputations may come as a surprise to consumers. When firm actions are inconsistent with existing knowledge, consumers engage in deeper processing of the new information, which may make them question the sincerity of brands' motives for engaging in these efforts ([85]). Consumers may be particularly suspicious of the sincerity of lower-reputation brands that engage in cultivating CSR actions. Such actions may be perceived as perfunctory (e.g., [ 4]), which can magnify their negative effect on sales. In contrast, corrective and compensating CSR actions, which invoke a certain level of accountability, may provide a positive disconfirmation of consumers' initial perceptions of low-CSR-reputation firms and further enhance their brand sales. Indeed, research on brand crises and service failures suggests that demonstrating accountability after negative incidents is particularly effective at improving a consumer's brand perceptions ([30]). We thus expect that, all else being equal, CSR initiatives announced by brands with more favorable CSR reputations are relatively less likely to impact brand sales than initiatives from brands with less favorable CSR reputations. H2:  CSR reputation mitigates the effect of CSR initiatives on sales as follows: Higher CSR reputation reduces the positive effect of corrective and compensating CSR initiatives on brand sales. Higher CSR reputation reduces the negative effect of cultivating CSR initiatives on brand sales. CSR focusOur proposed typology describes the actions taken by a brand, which may allow it to take accountability for its negative externalities. However, a brand may go about corrective, compensating, and cultivating CSR in a myriad of ways. One of the most frequently discussed dimensions of CSR is the domain in which CSR is implemented: environmental or social (e.g., [ 4]; [ 8]; [26]). Prior literature has not offered a clear comparison between the efficacies of CSR in these two domains, but it would be helpful for managers to know whether one of the two demonstrates accountability for the brand's negative externalities more effectively, particularly in conjunction with the three types of CSR.We argue that environmentally focused CSR initiatives will have an enhanced effect on brand sales for two reasons. First, consumers tend to place greater relative importance on environmental concerns than on social issues ([67]). As the media regularly highlights the liability of firm operations for harm done to the environment, consumers have become increasingly aware of these issues. For instance, the 2017 Carbon Majors Report found that a mere 100 companies generate 70% of global greenhouse gas emissions, and this received significant media coverage ([24]). As a result, firms are finding that it is increasingly necessary to address such liabilities ([23]). Academics agree; for instance, [52], writing on the occasion of the 75th anniversary of the Journal of Marketing, centers his article on the growing importance of the environmental imperative to marketing theory and practice. Thus, environmental CSR is important to stakeholders, relatively objective, and typically noncontroversial.Second, while social CSR initiatives have the potential to create a favorable image among subsets of stakeholders, these initiatives are perceived as being less focused, less verifiable, and thus more prone to agency costs ([64]). Moreover, recent research shows that consumers and shareholders do not always agree with the direction of social CSR, with some viewing such actions as an alienating form of activism ([17]; [45]; [11]). Therefore: H3:  Having an environmental rather than a social focus in a CSR initiative enhances the positive effect of corrective and compensating CSR initiatives and mitigates the negative effect of cultivating CSR initiatives. The Mediating Effect of Brand Sincerity on the Relationship Between CSR and Consumer BehaviorIn this subsection, we propose a mechanism for the predicted effects and explore the mediating role of perceived sincerity, a process we test subsequently in controlled laboratory experiments. Perceived sincerity is the extent to which consumers perceive a brand as caring and genuine in its actions ([41]). Greater perceived sincerity in CSR can lead to higher brand evaluations, purchase intent, and brand loyalty ([ 5]; [85]). In the domain of service failure, demonstrating accountability and taking reparative action are viewed as sincere gestures needed to improve customer satisfaction and repurchase intention ([77]). In our typology, corrective and compensating CSR actions signal a brand's willingness to take responsibility for its impact on society and the environment. As discussed previously, this may entail making changes to products, the supply chain, or manufacturing operations, or contributing time, money, or other resources. Such efforts directly acknowledge fault, may be costly and difficult to implement, and are thus unlikely to be taken lightly by consumers.In contrast, research suggests that when a brand does not sufficiently redress the harm caused by its actions, consumers are likely to perceive its CSR actions as insincere ([ 5]). In the absence of an acknowledgment of accountability, consumers may discount the good deeds associated with cultivating CSR activities or even be cynical of them. The CSR activities may backfire, leading to negative evaluations of the company and reduced purchase intentions or behavior. We thus predict that corrective and (to a lesser degree) compensating actions will be perceived as relatively more sincere than cultivating actions, and that this greater perceived sincerity will result in a more favorable consumer response, mediating the effect of CSR on purchase intentions.We also expect that brand CSR reputation will moderate the aforementioned mediation chain. In line with our previous arguments, brands with higher CSR reputations are likely to be viewed as simply fulfilling expectations by engaging in CSR and acting relatively sincerely regardless of CSR type ([34]). Thus, attitudes toward high-CSR-reputation brands are ultimately less likely to be affected when these firms engage in any new CSR activities. In contrast, when brands with weaker reputations engage in CSR, consumers are likely to think more deeply regarding their motives, leading to greater relative differences in perceived sincerity across CSR types ([85]). Thus, for lower-reputation brands in particular, we expect corrective and compensating CSR to be seen as more sincere than cultivating CSR, a difference that should influence purchase intentions accordingly. More formally: H4a:  Perceptions of brand sincerity mediate the effect of CSR initiatives on purchase intentions. H4b:  CSR reputation mitigates the mediation mechanism that indirectly links CSR initiatives to purchase intentions through perceptions of brand sincerity. Empirical Examination of the Impact of CSR Initiatives on Brand SalesWe use two approaches to test our hypotheses. First, we use a regression model estimated with panel data with fixed brand effects to examine the impact of CSR initiatives on brand sales. Second, we use experiments to document the process that underlies these effects and to show that brand sincerity mediates the effects of CSR actions on intentions of purchase. We begin by presenting the data, our method, and the results of the brand sales model, and we follow with a summary of the experiments that demonstrate the mediating role of brand sincerity. Data Sources to Analyze the Impact of CSR on Brand Sales and Sample ConstructionWe leverage two main sources of data to examine the impact of CSR on brand sales: ( 1) 3BL CSRwire service (CSRwire.com), to extract the CSR announcements, and ( 2) the IRI data set, to obtain brand sales before and after these announcements. CSRwire contains a searchable CSR news archive of more than 20,000 news items including corporate- and brand-level CSR-related press releases, CSR reports, and other event announcements dating back to 1999. Through CSRwire, companies disseminate CSR information to a diverse global audience via a myriad of websites and portals including Google, Reuters, LexisNexis, and Bloomberg ([23] Corporation 2009; [36]). Data from CSRwire have been previously used to study the impact of CSR (e.g., [27]; [35]).We obtain data on brand sales from the IRI academic data set ([13]] provide a detailed description of the data set). The IRI data set comprises weekly aggregate store-level product sales as well as consumer panel data for 30 CPG categories. The data set provides a rich time series of sales information at the Universal Product Code level for various brands and across markets (designated market areas [DMAs]). A vast body of research has employed the IRI data set to study the impact of marketing actions on brand sales (e.g., [ 5]; [14]). We begin by tracking CSR initiative announcements from brands in the IRI data set in the time period of our data, from 2001 to 2011. We first record the date of the CSR press releases drawn from CSRwire and the CSR/sustainability initiative press announcements from brand websites in this time period. Our CSR announcements were made between January 2002 and December 2011 and are listed in Web Appendix A. Archival searches for these announcements revealed that about 95% of them were prominently featured and discussed in major local and national newspaper outlets on the same date as the one reported on CSRwire. This suggests that there was a reasonable level of awareness for the events in our sample; at the same time, having events with lower coverage would work against the effects we hypothesize, making our tests more conservative.Our analysis sample includes 80 CSR initiatives across 55 brands, 21 product categories, and 48 DMAs. The classification of CSR announcements as corrective, compensating, or cultivating CSR was done by a panel of independent judges (N = 378), who each categorized a small random subset of these announcements by applying our definitions to the text of the press releases, with a high degree of interrater reliability (intraclass correlation coefficient = .80).For each brand, we use weekly brand sales aggregated (across stores) at the DMA level as our outcome of interest. We extract sales information from the IRI data set for the brands that have announced a CSR initiative for the 12-month period before and the 12-month period after the CSR announcement. We also wanted to obtain data on a set of appropriate control brands. Among the brands in the IRI database that belong to the same category as the focal brands, we kept all brands that, in descending order of market share, made up for 70% of the focal brand's market share. From this control group of brands, we exclude the ones that announced a CSR initiative in the year before and the year after the focal brand announced a CSR action.[ 8] Thus, our control group size ranges from 3 brands (in the facial tissue category) to 18 brands (in the cereal category), with an average size of 5.15 control brands (across all categories).Next, for each of the 80 CSR announcements, we choose an observation window of 104 weeks of weekly sales activities (52 pre- and 52 postannouncement weeks) for both focal and control brands in the product category. While 52 weeks is sufficient time for the sales effect of brands' CSR announcements to have manifested, the focus on a relatively tight window helps mitigate the influence of unobserved time-varying drivers of sales changes for both focal and control brands. We find that, on average, 65% of brands that form our control group also ended up announcing CSR initiatives at a later date (i.e., at least 12 months after our postannouncement observation window ends). This pattern is perhaps intuitive and, to some extent, also showcases the increasing extent to which the relatively prominent CPG brands that are part of our data set opt into engaging in CSR.Our framework includes two moderators: CSR reputation and CSR focus. The CSR focus on social versus environmental issues can be easily categorized from the text of each announcement. The focus of CSR is distinct from CSR type, and our sample includes observations for each combination of focus and type. Specifically, for environmentally focused CSR, we observe 14 corrective, 13 compensating, and 8 cultivating initiatives, and for socially focused CSR we observe 13 corrective, 6 compensating, and 26 cultivating actions.To measure CSR reputation, we return to CSRwire and construct an index based on the recorded total number of instances over the one-year pre-CSR announcement window during which each of the brands in our analysis sample either ( 1) relayed sustainability-related information—but not new CSR efforts/engagements—on CSRwire (e.g., ""Seventh Generation Releases Annual Corporate Consciousness Report"") or ( 2) was featured in a sustainability-related report showcased by its corporate parent on CSRwire (e.g., ""Kimberly-Clark Receives Perfect Score on 2011 Corporate Equality Index,"" ""Miller Coors Launches Corporate Responsibility Web Site""). We find that this index offers sufficient variability across the focal brands in our sample, having a mean of.98 announcements and a standard deviation of 1.87.Our research design exploits two useful sources of variation: ( 1) while some brands within a product category announce CSR initiatives, others do not, and ( 2) CSR announcements in our data are spread over a wide time horizon (vs. being clustered over a narrow time window). The variation in ( 1) helps us account for possible differences between brands that announce CSR initiatives and ones that do not. Alternatively, the sizable spread offered by ( 2) helps us partially mitigate the influence of broader macroeconomic trends (such as the recession of 2008) that may have otherwise played a role in influencing brand sales pre- and postintervention over a few specific years ([76]; [78]).We use a host of other data sources to construct control variables and instruments to account for endogeneity. For each of the brands in our data set, we collect information on ( 1) product prices (from IRI); ( 2) whether the product is on display—categorized into ""none,"" ""minor,"" or ""major"" displays (from IRI); ( 3) distribution intensity (number of stores carrying the brand, from IRI); ( 4) monthly advertising spending (from Kantar Media's Ad$pender database); and ( 5) press coverage (from RavenPack). To construct the press coverage control variable, we identify the corporate parent of each brand and download all the press releases of this corporate parent available in RavenPack for the same period for which we collect brand sales data. As RavenPack provides a sentiment score for each press release, we separate them into positively and negatively valenced announcements. Further, we include both the count of positively and negatively valenced announcements to proxy not only for the extent of press coverage during our sample period but also for the sentiment that underlies that coverage (for a description of RavenPack and the sentiment scores associated with the press releases, see [81]]). Finally, we use multiple data sources to construct instruments that account for ( 1) the endogeneity of the type of the CSR decision and ( 2) the endogeneity of the marketing instruments used as controls in our main brand sales model. We describe the instruments and their operationalization next. Identification StrategyWe first describe how we address the potential endogeneity associated with the choice of the type of CSR, followed by a description of our controls for the endogeneity of the marketing-mix instruments used in the sales model. In Web Appendix B, we present the results of an additional robustness step that assesses the potential importance of unobserved confounders in explaining our effects, by following the approach proposed by [69]. Accounting for the endogeneity of the type of CSR initiativeTo accurately assess the impact of CSR initiatives on brand sales, we need to control for the endogeneity of the type of CSR initiative undertaken by brands. Specifically, brands choose which type of CSR initiative to implement, and this choice could be driven by unobservable characteristics, leading to biased estimates for the effects of CSR on sales. To account for this choice, we estimate a multinomial logit model where the dependent variable has four levels—one for each type of CSR initiative and one for the choice to not do any CSR. The dependent variable takes a value of 0 for both the brands that did not announce CSR initiatives at all, as well as for focal CSR-announcing brands but only during the weeks preceding the CSR announcement. In the post-CSR announcement window for the focal brands, the dependent variable is coded as a categorical variable designating the type of CSR action undertaken (corrective, compensating, or cultivating). We use this model to obtain a set of three generalized inverse Mills ratios that will be included in the brand sales model as controls for this particular type of endogeneity. This follows the approach outlined in Wooldridge (1995) and Bourguignon, Fournier, and Gurgand (2007), which has been used in marketing applications by [25]; [32], and [43].To estimate this model in a manner that does not exclusively rely on the functional form of the chosen selection equation, we need exclusion restrictions, in the form of one or more variables that significantly impact the choice of conducting CSR but do not directly impact sales. We identified three such variables: the Product Responsibility Score (PR_Score) and the Innovation Score (Innovation Score) from Refinitiv's EIKON database as well as a variable that denotes the proportion of new products introduced by the brand that contain CSR claims, but in product categories other than the focal one (Prop_CSR claims) from Product Analytics. We describe in Table 2 the construction of these variables and explain their validity.GraphTable 2. Description of Instruments Used to Control for the Endogeneity of the Type of CSR Action. InstrumentDefinitionInstrument RelevanceExclusion RestrictionDataProduct Responsibility Score (PR_Score)The Product Responsibility Score is a weighted combination of scores that captures the extent to which a company has structures and processes in place dedicated to producing quality goods and services, ensuring the customer's health and safety, and protecting customers' data privacy. This variable is different from CSR reputation: the policies of a firm with high PR score are not necessarily visible to the public; alternatively, a firm with high CSR reputation could owe this reputation to philanthropic efforts or to CSR initiatives not directly related to policies that impact consumers This is a firm-level variable, and data vary annually.PR_Score captures firms' CSR emphasis on ensuring that as little harm as possible is done to consumers, it should be positively correlated with the firms' propensity to conduct corrective actions.PR_Score is calculated at the corporate level and refers to the existence of corporate-level policies and processes that are meant to reinforce a positioning focused on responsibility. The products associated with this type of positioning may be niche or may elicit a price premium, but their sales are not necessarily higher than those of more conventional alternatives, as many consumers continue to prefer the latter (e.g., Wilcox et al. 2009).Refinitiv's EIKON. The EIKON database includes ESG scores for over 9,000 global firms, which are computed from a variety of public sources including annual reports, company websites, news sources, nongovernmental organization websites, and others.Innovation Score (Innovation Score)The Innovation Score within the Environmental pillar in EIKON reflects the brand's inclination to use new environmental technologies and processes or to manufacture ecodesigned products. This is a firm-level variable, and data vary annually.Firms that score high on this pillar are focused on innovation and, therefore, are less likely to be focused on their old, existing products. Moreover, because their new products are already likely to be more sustainable and incorporate more responsible practices, these firms are more likely to engage in cultivating CSR than in the other two types.Innovation Score captures mostly corporate processes that reinforce a manufacturing and positioning strategy focused on new environmental technologies. As previously argued, such products do not necessarily surpass conventional alternatives in sales, but are rather more likely to target unique consumer segments.Proportion of new products introduced by the brand that contain CSR claims, in product categories other than the focal one (Prop_CSR claims)For each brand in our sample, we obtain all new products introduced in the two years before each CSR initiative was introduced, from which we exclude the products introduced in the categories we study in our article—we call this resulting measure ""NP."" We then classify the package claims for these products into CSR (NPCSR claims) and non-CSR-related (NPother claims). We use this classification to compute NPCSR claims/NP, which we use as an instrument for the propensity to engage in CSR initiatives. This is a brand-level variable computed using two years of data preceding each CSR initiative.This variable reflects a brand's commitment to incorporate CSR practices in its products and should be positively associated with the general propensity of the firm to engage in CSR but negatively associated with corrective CSR actions because firms may have less remaining to correct for, or less that can easily be corrected.Because the variable was constructed using new products from all categories in which brands operate, except for the focal one, it should not directly impact brand sales in the focal category, ensuring that the exclusion \restriction is verified.GlobalData Product Launch Analytics database, a database that provides extensive information on CPG products (e.g., Moorman et al. 2012). The utility of choosing a CSR initiative of type j by brand i at time t is given by Uijt = Vijt + εijt, where Vijt is a deterministic component and εijt is a random error. Using the multinomial logit model and assuming that the random error is independently and identically Gumbel-distributed, the probability that the CSR initiative of type j is chosen by brand i at time t is given by Pijt=exp(Vijt)1+∑j=13exp(Vijt), Graph( 1)where Vijt = α0j + α1jPR_Scoreit + α2jInnovation_Scoreit + α3jProp_CSRclaimsit + α4jAdvertisingit + α5jBrand_assetit, j = 1, 2, 3 refers to the three types of CSR (corrective, compensate, and cultivating goodwill), i refers to the brand, and t to the month of measurement, which spans 12 months before a CSR initiative was announced and 12 months after. Brand_asset is the Brand Asset Valuator Y&R overall measure of brand equity (measured at the annual level), and Advertising denotes brand-level advertising expenditures (measured at the monthly level). Thus, for each brand that has undertaken a CSR initiative and for each peer brand in its product category we have 24 months of advertising data and at least two years of brand asset and CSR score data (contingent on each brand reporting sales in each particular DMA), resulting in an unbalanced panel over which the model is estimated. Using the choice probabilities predicted from Equation 1, we compute a set of three generalized inverse Mills ratios (one for each CSR type j) of the form[ 9]  IMRijt=3×ln(Pijt)+∑j′≠j[Pij′t×ln(Pij′t)1−Pij′t]  to include in the outcome equation governing the sales response of CSR (which we discuss subsequently). Accounting for the endogeneity of marketing instruments used as control variablesIn addition to accounting for the endogeneity of the decision to implement a CSR initiative, we also account for the endogeneity of the marketing-mix variables included as controls in the model of brand sales. To do so, we use a two-stage least squares approach. Specifically, we specify an additional equation for each marketing-mix variable and model these variables as a function of all fixed effects and exogenous variables from the sales equation and an instrument for the brand's marketing-mix variable. We follow [78] in using as instruments weighted averages of the marketing mix of brands that do not have products in the same narrow product category but belong to the same industry. The marketing-mix variable of the focal brand is likely to be correlated with that of these brands, because the same underlying cost structures apply and may lead to similar movements in these variables. We use this approach for advertising, display, and distribution intensity. We use a different set of instruments to account for the endogeneity of product prices. For prices, we use measures that commonly govern the factor costs of production/packaging in the CPG industry, such as the producer price indices for plastic (North American Industry Classification System code 326160) and wood pulp (North American Industry Classification System code 322110), gathered from the Bureau of Labor Statistics website. Estimating the Effect of CSR Initiatives on Brand SalesWe specify the following model to estimate the effect of CSR on brand sales: lnSalesidt=∑j=13β1jCSR_PostAnnounceijt+∑j=13β2jCSR_PostAnnounceijt×CSRRepit+∑j=13β3jCSR_PostAnnounceijt×CSRFocusijt+β4CSRRepit+γ1lnPriceidt+γ2lnAdvertisingit+γ3lnDistribit+γ4lnDisplayidt+γ5lnPositivePressit+γ6lnNegativePressit+γ7lnSalesidt−1+θ1id+θ2t+∑j=13δjIMRijdt+εidt. Graph( 2)The model is estimated at the brand (i  ∈1…N  ), DMA (d  ∈1…D  ), and week (t  ∈1…W  ) levels. For each brand that introduced a CSR initiative and all the peer brands from the same product category, the sample includes data for 52 weeks before the date of that brand's CSR initiative announcement and 52 weeks postannouncement. The term CSR_PostAnnounceijt takes a value of 1 in the 52 postannouncement weeks t if/after brand i made a CSR announcement of type j and 0 otherwise. CSR_Focusijt takes the value 1 if the CSR initiative has an environmental focus and 0 if the focus is social. For each of the N brands and D DMAs in our data, we include separate fixed effects (θ1id) to account for heterogeneous brand preferences at the local market level. Including brand × DMA fixed effects obviates the need to separately account for whether brand i implemented CSR (i.e., via a dummy variable for having implemented a CSR initiative) and whether the CSR was environmentally or socially focused. We also include week fixed effects θ2t to control for seasonality, which obviates the need to separately account for a common main effect for the postannouncement period in the data. The term lnPriceit is the logarithm of price for brand i, lnAdvertisingit reflects the logarithm of advertising spending for brand i, lnDistribit captures the logarithm of the number of stores carrying brand i, and lnDisplayit reflects the logarithm of brand i's in-store display intensity, all in week t.[10] Following the two-stage least squares approach, we replace all marketing-mix controls with their predicted values from the respective first-stage equation used to account for their plausibly endogenous nature.The terms lnPositive Pressit and lnNegative Pressit are, respectively, the logarithm of the number of positively and negatively valenced mentions of brand i in the news. The lagged value of log sales of brand i in week t (lnSalesidt − 1) is also included to account for the carryover effect of marketing events on brand i. The IMR measures are the inverse Mills ratios incorporated to account for the endogeneity of brands' CSR choices. We compute bootstrapped cluster-robust standard errors using 50 replications to account for any within-unit serial correlation and sampling error inherent in the predicted probabilities generated from Equation 1 used in the computation of the IMR measures and of the endogenous marketing-mix controls.The terms β1j, β2j, and β3j denote our three coefficients of interest. The coefficients β1j capture the main effects of CSR of type j on brand sales, while β2j and β3j capture the moderating role of CSR reputation and CSR focus, respectively. They correspond to the effect that different types of CSR efforts announced by brands have on their sales, after controlling for the influence of heterogeneity in consumers' brand preferences, changes in brands' marketing-mix strategies, and seasonality. Next, we discuss the results from these models. Results Determinants of firms' choice of type of CSR effortWe start by presenting the results of the auxiliary equations used to account for endogeneity and selection, and we then present the main model results. Descriptive statistics for the variables used in Equations 1 and 2 are presented in Tables 3 and 4, respectively. We do not observe any concerning correlations that could suggest multicollinearity.GraphTable 3. Correlation Matrix and Summary Statistics of Determinants of CSR Type. PR ScoreAd ExpenditureProp CSR ClaimsInnovation ScoreBrand AssetMeanSDPR Score13.35.96Ad expenditure ($M).1411.673.52Prop CSR claims−.02−.0311.04.77Innovation Score.33−.05−.0812.491.15Brand asset.03.11.16.29173.5022.01 GraphTable 4. Correlation Matrix and Summary Statistics of Determinants of Brand Sales. CSR ReputationCSR FocusLog of PriceLog of Ad ExpenditureLog of Display IntensityLog of Distribution IntensityLog of Positive PressLog of Negative PressLog of Salest−1MeanSDCSR reputation1.301.11CSR focus.171.08.27Log of price.08.0611.56.48Log of ad expenditure.10.10.0518.206.69Log of display intensity.04.06−.02.161−5.733.67Log of distribution intensity−.01−.02−.09.24.1517.02.45Log of positive press−.15−.12.022.16.01.1419.10.71Log of negative press−.02−.11−.12.18.09.25−.1615.883.55Log of salest − 1.04.02−.31.23.38.36.08.0716.511.76 The results from the multinomial logit model are presented in Table 5 and suggest that the instruments for the CSR initiatives have strong explanatory power for the propensity of firms to conduct CSR, confirming their validity. The first instrument, PR_Score, is positively associated with the propensity to conduct corrective CSR, in line with this type of CSR being focused on the firm's existing products (β = .031, p < .01). Interestingly, a high PR_Score is also positively associated with cultivating CSR (β = .371, p < .01) but negatively associated with firms' propensity to engage in compensating CSR efforts (β = −.149, p < .01). We expect that the latter result is due to the singular focus that firms with high PR_Score place on improving their previously introduced products, rather than compensating for other negative externalities. The direction of the Innovation Score instrument, which refers to the extent to which the firm's processes and new products incorporate sustainable technologies, is as expected. We find a negative association for the two types of CSR focused on addressing the negative externalities associated with the firm's existing products and operations, which are already likely to be designed using high-CSR standards (corrective: β = −.073, p < .01; compensating: β = −.188, p < .01). In contrast, the association with cultivating CSR is positive (β = .618, p < .01). The third instrument, PropCSR claims, is also negatively related to corrective CSR (β = −.609, p < .01), consistent with the argument that firms whose new products already include CSR claims are less likely to need to engage in corrective efforts. At the same time, Prop_CSR claims is positively related to compensating CSR (β = .112, p < .01) and marginally positively related to cultivating CSR (β = .012, p < .10). Finally, we find that brand equity is positively related to the propensity to conduct corrective and compensating CSR, while advertising expenditures are positively related to all three types of CSR, suggesting that strong brands view CSR as an additional avenue to maintain their brand equity.GraphTable 5. Determinants of the Type of CSR Initiative. DV = CSR Type ChoiceCorrectiveCompensatingCultivatingEst.SEEst.SEEst.SEPR Score.031**.006−.149**.005.371**.013Ad expenditure.180**.012.776**.009.113**.009Prop CSR claims−.609**.007.112**.006.012†.007Innovation Score−.073**.005−.188**.006.618**.007Brand asset.008**.000.004**.000.001.000N1,196,057Log-pseudolikelihood−456,251.55 1 *p < .05. **p < .01. †p < .1.2 Notes: Heteroskedasticity-robust SEs are reported alongside estimates. The effect of CSR on brand salesTo determine the effect of CSR initiatives on sales, we use a regression model estimated with panel data with fixed brand effects. We first conduct augmented Dickey–Fuller tests to assess whether the brand sales series is stationary or possesses a unit root. We also conducted Perron tests, which extend the Dickey–Fuller methodology to structural breaks in the model. Both augmented Dickey–Fuller (p <.01) and Perron (p <.01) tests of the null hypothesis of all panels with a unit root are significant, suggesting that the series are stationary. In addition, our analyses are conducted using 104 weekly sales observations, alleviating concerns of ""dynamic panel bias"" salient in studies utilizing few time periods ([48], p.13; [73], p. 103).We present our model results in Table 6, where we include the results from various specifications, including with and without controlling for the endogeneity of the marketing-mix variables. In all the specifications presented, we account for the endogeneity of firms' CSR choices as described previously. To begin, column 1 shows the results from a model that includes main effects but without any marketing-mix controls. Column 2 shows estimates from a model that includes interactions with CSR reputation and CSR focus, but again without marketing-mix controls. In column 3, we control for firms' marketing-mix strategies without (yet) correcting for their plausibly endogenous nature. Column 4 presents results from models that employ instruments to address the endogeneity in the marketing-mix controls and where the continuous moderator CSR reputation is standardized.GraphTable 6. Effect of CSR Initiatives on Brand Sales. Main Effects, Excluding Marketing-Mix ControlsFull Model, Excluding Marketing-Mix ControlsMarketing-Mix Controls Included (Without Their Endogeneity Correction)Marketing-Mix Controls Included (with Endogeneity Correction), CSR Rep StandardizedDV = Log (Brand Sales)(1)Est.SE(2) Est.SE(3)Est.SE(4)Est.SECorrective.020**.002.012**.005.016**.005.010*.005Compensating.011**.003.029*.011.020†.013.030*.014Cultivating−.012**.003−.013*.006−.036**.009−.035**.008Interactions with CSR FocusIncremental effect for environmental—Corrective.039***.011.029*.013.045**.009Incremental effect for environmental—Compensating−.013.013.001.014−.006.007Incremental effect for environmental—Cultivating goodwill.024*.011.057**.015.051**.015Interactions with CSR ReputationCorrective × CSR Rep−.007*.003−.011**.004−.012**.001Compensating × CSR Rep−.007*.002−.011**.002−.011**.002Cultivating × CSR Rep−.005.003−.008*.003−.004.003Control VariablesLog of price.007.555−.142.742Log of ad expenditure.004**.000−.001.005Log of display.987**.113.641**.093Log of distribution intensity.359**.084.015.270Log of positive press75.96**5.43966.88**9.811Log of negative press−68.26**4.952−59.47**9.615Lag of log (brand sales).375**.001.375**.005.343**.029.334**.037Endogeneity CorrectionIMR—Compensating−.035**.005−.036**.0062.346**.1732.099**.266IMR—Corrective.101**.004.101**.007−.865**.065−.746**.129IMR—Cultivating.078**.003.078**.0041.437**.109−1.276**.176N1,157,5301,157,5301,141,0221,141,022 3 *p < .05. **p < .01. †p < .1.4 Notes: DV = dependent variable. Brand × DMA fixed effects and week fixed effects are included in all specifications. Bootstrapped heteroskedasticity-cluster-robust SEs are reported. Main effects of CSR focus of an announcement and the brand's CSR reputation are not identified separately from the brand × DMA fixed effects. Employs one-week lags for the DV. Results are similar with four- or eight-week lags.Our results reveal that the effect of CSR on brand sales varies, and in material ways, with the type of CSR action undertaken by brands. Consistent with H1, the direction of the change in brand sales is positive for corrective and compensating CSR announcements and negative for cultivating CSR announcements. Because our dependent variable is specified in logarithms, we can compute the percentage change in sales for the focal brand on account of CSR as (exp(βit) − 1). The change in sales for brands engaging in corrective (compensating) CSR appears to be in the order of 1.0% (3.05%), whereas for cultivating CSR it is around −3.45%. The resulting equilibrium sales levels over the long term are about  1/(1−γ7)   = 1/(1 −.33) = 1.5 times the size of the short-term sales changes (i.e., approximately 1.5%, 4.6%, and −5.2%, on average, for corrective, compensating, and cultivating CSR, respectively).The estimates of the coefficients corresponding to these controls are all in line with expectation—for example, product price and advertising are respectively negatively and positively related to sales. The auxiliary equations that link the marketing-mix variables to the instruments that help us account for the endogeneity of these marketing-mix variables yield results that are all in the expected direction and are significant at p < .01. We also formally verified the strength of the instruments employed to rule out weak identification concerns. The Sanderson–Windmeijer multivariate F-statistics linked to each of our endogenous marketing-mix variables range between 23.88 for price and 61.48 for distribution intensity (the p-value associated with each case was <.001). Finally, the coefficients corresponding to the inverse Mills ratios for corrective, compensating, and cultivating CSR actions are significantly different from zero, highlighting the importance of accounting for such an endogenous influence on our estimates. The moderating effect of firms' CSR reputation and CSR focus on the relationship between CSR…In terms of the moderators, we find that higher CSR reputation brands experience a lower increase in brand sales compared with lower-scoring brands for corrective (β = −.012, p <.01) and compensating (β = −.011, p < .01) CSR actions. This is in line with our arguments about consumers expecting less from brands with low CSR reputations and therefore being more pleasantly surprised when they undertake CSR initiatives. In contrast, for cultivating CSR action, we do not find a significant moderating influence of CSR reputation. Thus, H2a is supported, whereas H2b is not.The focus of CSR on environmental (vs. social) issues also impacts the effect of CSR on brand sales. We find that the incremental effect of environmental CSR focus (β = .05, p <.01) appears to help reduce the negative effects of cultivating CSR on sales. Similarly, environmentally focused corrective CSR actions appear to contribute a positive boost to sales (β = .045, p <.01), while socially focused corrective CSR actions also have a significant but slightly more modest positive effect on brand sales (β = .014, p <.01). In contrast, brands that announce environmentally focused compensating CSR actions experience benefits that are statistically indistinguishable from those announcing socially focused compensating CSR actions. We provide a more detailed discussion of these effects next and an illustration in the Web Appendix D.While similar, two important differences between corrective and compensating CSR arise in the results. First, the effect of compensating CSR does not differ significantly between environmental or social initiatives. Second, there is a more pronounced moderating effect of a brand's CSR reputation on corrective CSR. Finally, while, on average, cultivating CSR initiatives result in a slight decrease in sales, this effect was not significantly moderated by CSR reputation. Rather, the effect on cultivating CSR was more pronounced among social initiatives.In summary, the results are consistent with the effects hypothesized in H3 for corrective and cultivating CSR, but an environmentally focused compensating CSR effort does not appear to further enhance the effect of this type of CSR on sales. Experimental Evidence of the Impact of CSR Initiatives on Purchase Intentions and an Examinat…Next, we briefly examine our predictions under experimental settings, with the primary purpose of understanding the process underlying the effects observed in the model. Specifically, the experiments serve to replicate the pattern of results observed in the model, moderated by CSR reputation (H1 and H2), while also demonstrating the mediating effect of perceived brand sincerity (H4a and H4b). To reiterate, H4 predicts that the different effects of CSR actions on brand sales postulated in the model are attributable in part to differences in consumers' inferences regarding the sincerity of the brand's actions. Consumer inferences regarding a brand's motives for CSR actions are known to influence consumer responses to those actions ([26]), as consumers are reluctant to reward CSR when they distrust the company's motivations ([18]). We expect that perceptions of sincerity will be relatively high across CSR types for high-CSR-reputation brands and generally lower for brands with lower CSR reputations ([85]). However, when a relatively lower-reputation brand demonstrates greater accountability through corrective actions, we predict that consumers will be relatively less skeptical of ulterior motives and, thus, more likely to attribute these actions to the character of the brand ([85]). In contrast, actions that recognize a problem without solving it (compensating), or do not relate to or address the brand's negative externalities (cultivating), are likely to be met with greater skepticism due to the increased salience of possible ulterior, selfish motives ([31]). Thus, although we predict that the main effects of CSR type on sales will be mediated by perceived sincerity (H4a), we also predict a moderated mediation effect (H4b). Specifically, we expect that CSR reputation will moderate the effect of CSR action on perceptions of brand motive sincerity, which will, in turn, affect purchase intentions. StudiesWe conducted three studies using similar study designs and representing different products, CSR initiatives (social and environmental), stimuli, and participant pools (total N = 507; for details, see Table 7). The three studies are each individually reported in detail in Web Appendix B. Here, however, in the interest of parsimony, we summarize all experimental results in the form of a single-paper meta-analysis, specifically utilizing an independent participant data (IPD) meta-analysis to allow for the test for mediating effects ([72]).[11] This enables us to provide a concise summary of our experimental results and process evidence based on all of the available data, while providing greater generalizability (i.e., a lower risk of idiosyncratic stimuli effects).GraphTable 7. Road Map of Experimental Studies That Show the Effect of CSR on Purchase Intentions. Study 1Study 2Study 3Distinguishing FeaturePackaging Stimuli/Laboratory StudyAlternative Process Variables/Retail ContextTests Alternative Process Variables and Individual DifferencesCSR domainSocial (health)Social (labor)EnvironmentalDVPurchase intentionsPurchase intentionsPurchase intentionsProduct categoryCold cerealCoffee (chain)Bottled waterDesign3 × 23 × 33 × 2ParticipantsStudentsOnline panelOnline panelInteractionsp < .001p < .001p = .05N181176150 5 Notes: DV = dependent variable. Meta-Analysis Study designAll three studies employed the same base 2 (brand CSR reputation: favorable vs. unfavorable) × 3 (CSR type: corrective vs. compensating vs. cultivating) study design. To begin each study, participants (N = 507; 41.6% female; average age = 29.9 years) were given background information regarding a CPG brand in a particular category. All participants were told that these were actual brands but were either marketed exclusively in another country (Study 1; cold cereal) or were not identified for privacy reasons (Studies 2 and 3; coffee and bottled water, respectively). These brands were then described as being either relatively socially responsible (positive-CSR-reputation condition) or socially irresponsible (negative-CSR-reputation condition) compared with peer brands within their categories.[12]After reading this background information about the focal brand, participants rated the likelihood that they would consider purchasing the brand (purchase intent) using a seven-point scale anchored on ""not likely at all"" ( 1) and ""very likely"" ( 7). Following this introduction and initial measurement, participants then read a description of a recent CSR initiative announced by the brand. This initiative represented an action that reduced the brand's own negative social or environmental impact (corrective CSR), addressed the brand's impact without actually reducing it (compensating CSR), or was an unrelated philanthropic gesture (cultivating CSR). After exposure to the CSR initiative, participants were then asked a second time about their purchase intentions. In addition, participants responded to measures about their perceptions of the brand's motives for the CSR initiative. Specifically, participants completed two seven-point items about how ""sincere"" and ""genuine"" they believed the brand's interest in the cause to be, and Study 3 included an additional item asking how much the brand ""truly cares"" about the initiative ([85]). Participants then rated the subjective fit of the CSR initiative with the brand ([10]) as an alternative process measure. Finally, participants provided demographic information including age and gender before the studies concluded. Thus, while stimuli details and CSR contexts differed to better generalize results, the basic study designs were highly consistent, enabling a very straightforward single-paper (IPD) meta-analysis (for a summary of results, see Table 8).GraphTable 8. Results from a Meta-Analysis of Three Experimental Studies: Main Effects, Moderating Effects of CSR Reputation, and Indirect (Mediating) Effects of Perceived Sincerity for Each CSR Type in the Experimental Data. Main Effects of CSR Type on Purchase IntentaModerating Effect of CSR Reputation on the Relationship Between CSR Type and Purchase IntentaMain Effects of CSR Type on Perceived Brand SinceritybRelative Indirect Effect of CSR Type on Purchase Intent Mediated by Perceived SinceritycIndex of Moderated MediationdCorrect.869, p <.05−1.13, p <.001.210, p <.01(Reference category)(Reference category)Compensate.582, p <.05−.275, p = .12−.024, n.s.−.0473, p <.05−.075, p <.05Cultivate.275, p <.05.124, n.s.−.280, p <.01−.0992, p <.05−.063, p <.1Omnibusp <.001p <.001p <.001—— 6 aMarginal means (column 1) and parameter estimates (column 2) from ANOVA results on purchase intent.7 bMarginal means from ANOVA results on perceived sincerity.8 cIndirect effects resulting from PROCESS Model 4, controlling for CSR reputation ([38]). The indirect effect represents the product of ( 1) the effect of CSR type on perceived sincerity and ( 2) the effect of perceived sincerity on purchase intentions.9 dIndirect effects resulting from PROCESS Model 8 ([38]). The indirect effect represents the product of ( 1) the effect of CSR type on perceived sincerity, moderated by CSR reputation, and ( 2) the effect of perceived sincerity on purchase intentions. AnalysisThe primary dependent variable of interest was the change (Δ) in purchase intentions from before and after the CSR information was presented to participants. Individual analyses of variance (ANOVAs) interacting CSR reputation and CSR type as factors were significant for all three studies (see Table 8). For the meta-analysis, we use an aggregated data set of all observations enabling our test for process (PROCESS Model 8; [38]), using indicator coding and controlling for study-level effects ([39]; [55]; [72]). Main effectsConsistent with the model, the studies individually and collectively show a significant interaction between CSR type and CSR reputation (F( 2, 442) = 13.67, p <.001), with significant main effects for CSR reputation (F( 1, 442) = 17.94, p <.001) and CSR type (F( 2, 442) = 11.88, p <.001). Corrective CSR again produced the most positive consumer response (M = .869, SE = .087, p <.05), followed by compensating CSR (M = .582, SE = .089, p <.05). Cultivating CSR again proved the least effective (M = .275, SE = .086, p <.05; see Figure 2 and Table 8). All contrasts between CSR types were significant. Although cultivating CSR was again significantly less effective than the other two CSR types, the net effect of cultivating CSR across the studies was positive overall, which itself was not consistent with model results. We expect that this was due to simple anchoring effects given the laboratory setting and study procedures. It is also relevant that while the effect was positive within the high-CSR-reputation condition, the effects of cultivating CSR were not significant within the low-CSR-reputation condition (M = .213, SE = .115, n.s.). Thus, the overall pattern of results was largely consistent with the pattern obtained from our empirical analysis using brand sales. The exception was cultivating CSR, which, while obtaining lower evaluation than the other two types, was nevertheless not negatively viewed.Graph: Figure 2. Results from a meta-analysis of three experimental studies: change in purchase intentions across CSR types and CSR reputation conditions. ModerationThe moderating effect of CSR reputation was also consistent with model results, such that a high CSR reputation attenuated the positive effects of corrective and compensating CSR but improved the effect of cultivating CSR on purchase intentions (see Figure 2). Overall, the main-effect differences between CSR types were primarily driven by differences in the unfavorable-CSR-reputation conditions. Thus, H2 is supported in the studies, while H1 is partially supported. The pattern of results largely mimics those found with the brand sales model. MediationThe effects of CSR type on purchase intentions were mediated by perceived sincerity (H4a). An ANOVA on perceived brand sincerity[13] showed main effects of CSR reputation (F( 1, 442) = 104.31, p <.001) and of CSR type (F( 2, 442) = 11.94, p <.001) with a marginally significant moderating interaction (F( 2, 442) = 2.43, p = .089). Not surprisingly, brands with better CSR reputations were viewed as having more sincere motives for their CSR actions (M = .411) than brands with lower reputations (M = −.446). More importantly, among CSR types, corrective CSR actions were perceived as the most sincere (M = 2.55, t(150) = 3.50, p <.001), followed by compensating CSR actions (M = −.035, t(142) = −.414, p = .680) and cultivating CSR actions, which was viewed as significantly insincere (M = −.313, t(156) = −3.79, p <.001).[14] In turn, perceived sincerity positively affected purchase intentions (β = .203, t = 3.52, p <.001). The mediating effects were significant, as perceived sincerity mediated the effects of compensating CSR (a1 × b1 = −.0473, 95% confidence interval [CI] = [−.1084, −.0044], p <.05) and cultivating CSR (a2 × b1 = −.0992, 95% CI = [−.1875, −.0311], p <.05) relative to corrective CSR, in support of H4a. Moderated mediationBeyond the mediation of the main effects, we also found evidence for moderated mediation (H4b). The moderating effect of CSR reputation on perceived sincerity was similar to that observed with purchase intentions, such that differences in perceived sincerity were more pronounced within brands of lower CSR reputations (F( 2, 442) = 11.21, p <.001) than brands with higher CSR reputations (F( 2, 442) = 2.86, p = .056). In a test of the entire model, a moderated mediation analysis (PROCESS Model 8) returned significant indices of moderated mediation through perceived sincerity for both compensating CSR (a1 × b1 = −.075, 95% CI = [−.1921, −.0032], p <.05) and cultivating CSR (a2 × b1 = −.063, 90% CI = [−.1493, −.0014], p <.10), relative to the corrective CSR condition. The latter moderated mediation effect was only marginally significant, as the indirect effects for cultivating actions by brands with favorable and unfavorable CSR reputations were both negative and significant (favorable: a2 × b1 = −.1194, 95% CI = [−.2364, −.0046], p <.05; unfavorable: a2 × b1 = −.0562, 95% CI = [−.1294, −.0046], p <.05). Overall, the moderated mediation results predominantly support H4b. Alternative mediatorPerceived fit of the CSR initiative with the brand, which was tested as an alternative mediator, did mediate the main effects of CSR type on purchase intentions (compensating: a1 × b1 = −.0331, 95% CI = [−.0743, −.0079], p <.05; cultivating: a2 × b1 = −.0753, 95% CI = [−.1538, −.0172], p <.05). However, perceived fit was not significant in the moderated-mediation model (PROCESS Model 8), as the index of moderated mediation was not significant for either compensating (a1 × b1 = −.0378, 90% CI = [−.0912,.0049], p >.10) or cultivating (a2 × b1 = .0315, 90% CI = [−.0164,.0919], p >.10) CSR actions, relative to corrective CSR. Thus, while perceived fit may thus help explain the main effects of CSR on sales response, fit does not appear to fully explain the effects observed in the model and studies. DiscussionThe results from three laboratory experiments lend support to the results documented with secondary data, but more importantly, they provide process evidence for the underlying effect. As was the case in the brand sales model, participants rewarded corrective CSR actions with increased purchase intentions. The moderating effect of brand CSR reputation also shows a similar pattern, attenuating the positive effects for both corrective and compensating CSR, and improving purchase intention outcomes for cultivating CSR. These effects were driven in part by subjects' inferences regarding the sincerity of the brand's motives behind the CSR initiatives. Overall, the results of these studies are consistent with our findings from the sales data, with the exception of cultivating CSR not being negatively viewed by participants. Finally, while we did not explicitly manipulate the environmental versus social focus of CSR action in these studies, we note that the CSR actions used in Studies 1 and 2 are socially focused, the CSR actions used in Study 3 are environmentally focused, and the direction of the effects across these two sets of studies is consistent with the moderating effect hypothesized in H3. Discussion and ConclusionThis article proposes a typology of CSR activities that is based on demonstrating accountability for the impact that brands have on consumers and the environment. Using both brand-level sales data as well as data from lab experiments, we show that consumers respond positively to brands that undertake corrective and compensating actions, but not to those that engage in cultivating goodwill actions. We also show that the effects of these actions on sales and purchase intentions are mitigated by high CSR reputation. Further, we show that an environmental CSR focus, relative to a social one, strengthens the positive effect of corrective CSR and weakens the negative effect of cultivating CSR. Finally, we find that perceptions of brand sincerity mediate the effect of CSR actions on purchase intentions: corrective and compensating CSR, which suggest a higher desire to correct brand liabilities, are perceived as more sincere and increase purchase intentions, whereas cultivating goodwill CSR is viewed as less sincere and does not appear to lead to any changes in purchase intentions. Theoretical ImplicationsThe CSR literature is sizable and already includes classifications of CSR activities into business practice versus philanthropic, reactive versus proactive, or environmental versus social. However, such categorizations are broad and do not provide a direct link to what would be the most suitable type of CSR for each firm. By focusing on CSR activities that address the negative externalities associated with firms' operations, our typology establishes this link, while also offering an umbrella large enough to encompass the full spectrum of CSR efforts typically undertaken by firms. Our work connects the CSR stream of literature with the one on brand harm crises and with image restoration theory. This work suggests an underlying mechanism for the success of corrective and compensating CSR initiatives, which starts with firms acknowledging their externalized costs and selecting appropriate compensatory CSR actions and results in firms enjoying positive consumer outcomes including increased perceptions of firm sincerity and stronger purchase behavior. In contrast, our results suggest that cultivating goodwill is a special type of CSR and that stakeholders' reactions to it require further scholarly and practitioner inquiry. This finding is in line with recent research that has documented negative employee outcomes to cash donations to a nonprofit, which can be classified as cultivating CSR ([54]).Moreover, our framework goes beyond this categorization and also allows for a comparison of two broad classes of CSR—environmentally versus socially focused—which can both materialize within each of the three types of CSR included in our categorization ([22]). Thus, assessing the intersection of environmental and social focus with our typology deepens our understanding of CSR as a complex and multidimensional construct. In line with research finding that consumers view environmental issues as relatively more important than social ones ([67]), our results suggest that an environmental focus further increases the positive effect of corrective CSR initiatives on brand sales, while it mitigates the negative effect of cultivating CSR. The intriguing finding that an environmental focus does not appear to help compensating CSR initiatives could be due to a heightened awareness of the cost that the firm's operations have on the environment, which may not be adequately addressed by firms' compensation efforts.We also highlight the moderating effect of CSR reputation, as prior work finds that consumers may react differently to the same CSR actions depending on whether the perception is that the actions are isolated endeavors or typical of the brand ([26]). CSR reputation is especially informative for consumers given the potential for greenwashing in the CSR space. We add to the literature by outlining some of the reasons for the differences in consumer reactions to the same CSR actions. High CSR reputation provides a ceiling effect for potential rewards associated with such initiatives, while lower-CSR-reputation brands' corrective and compensating CSR engagement leads to increased perceptions of brand sincerity and ultimately brand sales. Moreover, our research adds a new facet to the stream of literature that has examined CSR reputation in the context of tempering negative evaluations associated with brand harm crises ([26]; [50]). The CSR typology proposed in this article—by focusing on firms taking accountability for ""everyday"" harm, rather than reacting to distinct crises—expands the role of CSR as a mitigating factor of a broader set of negative externalities associated with the activities of firms.Our theory also allows us to reconcile some of the negative effects of CSR previously documented in the literature, such as that of profit-oriented CSR in [ 8], activities focused on community support in [ 4], and agency costs in [79]. Our accountability-based framework offers overarching insight into these negative effects. We propose that these negative effects are more likely to occur when the CSR actions appear to be disconnected from the brand's own footprint, are not perceived as sincere and do not suggest that the firm wants to counteract some of the negative externalities associated with its operations. Consumers may find CSR actions aimed at cultivating goodwill to be less sincere. They may attribute this lack of sincerity to profit orientation, support of issues that they do not agree with, or engagement in corporate philanthropy that enhances managers' personal reputations. The experiments that we present in our article not only establish brand sincerity as a mediator of the relationship between CSR and purchase intentions in three very different scenarios but also highlight the fact that cultivating CSR, perceived as being the least sincere of the three types of initiative, fails to sway consumers toward the brand. Managerial ImplicationsOur results also carry implications for managers. With the caveat that our (observational) data are not particularly suited for enabling strong normative claims about the type of CSR actions firms should undertake, we propose that one of the key managerial takeaways of our study is that brands can benefit from emphasizing the accountability of their CSR efforts, particularly if these efforts address environmental issues. Furthermore, our results suggest that managers should reconsider engaging in CSR actions that cannot be clearly linked to the brand's perceived negative externalities on either society or the environment. This is critical, as consumers are becoming ever more aware of the potentially harmful effects of brands' business operations on both societal and environmental dimensions. Brands in the CPG category, in particular, are increasingly being taken to task on these issues. For example, Coca-Cola was named top plastic polluter for the third year in a row, beating out other top polluters Pepsi and Nestlé. Coca-Cola was accused of making zero progress on plastic waste reduction, with its beverage bottles found littered on beaches, rivers, and parks ([61]). At the same time, Coca-Cola recently celebrated its having awarded over $73 million in college scholarships over the last 25 years ([20]). Our results suggest that, as a general guiding philosophy, brands should strive to ""clean up their own mess"" before engaging in general charitable efforts that may otherwise be seen as insincere efforts aimed squarely at garnering consumer goodwill. While college scholarship donations are a worthy endeavor, our study suggests that a focus on plastic waste reduction may resonate more strongly with Coca-Cola's consumers. It is thus not surprising that in response to being named ""top plastic polluter,"" Coca-Cola has highlighted its commitment to recycling every one of its beverage bottles by 2030 ([61]).In addition to helping managers better understand the consequences of various CSR activities, our findings also suggest what aspects of these initiatives have to be clearly communicated to the public. Accountability and efforts to reduce negative externalities should be highlighted in companies' press releases about CSR initiatives, as these appear to lead to positive consumer outcomes. LimitationsThere may be unaccounted factors that could impact the effect of CSR actions on brand sales. For instance, the extent of resource investment (in terms of both financial resources and effort) into the CSR rollout process may play a role in influencing sales returns and the success of the CSR initiatives. However, information on brand investments into CSR is proprietary, hard to quantify, or both. We partially account for the support given to CSR actions in two ways—( 1) by including time-varying advertising spending at the brand level as a covariate in our regressions and ( 2) by making the brand's identity constant in our experimental analyses—but there could be other strategic considerations that drive CSR that we were not able to incorporate in our analysis. Future ResearchOur research represents a first step in better understanding the effects of CSR initiatives on brand sales. We have examined brand sales up to a year after an initiative was announced, but CSR activities may also strengthen brand loyalty and satisfaction, which could in turn lead to strong longer-term outcomes. Identifying additional boundary conditions for the effects we present in this article can also help managers make better CSR choices. Likewise, although we test and find evidence for one proposed mediator, we recognize that this process may involve further unexplored nuance. For instance, might CSR affect other outcome variables (e.g., consumer identification with the brand, product attractiveness, strength of partnership with retailers) that help mediate and explain the differences in sales? Systematically understanding when and why consumers respond in particular ways to these CSR types may be a promising area of future inquiry.An interesting question that stems from our findings is the sequencing of the types of CSR at the firm level. While our results indicate that cultivating initiatives lead to negative outcomes, it is plausible that this effect could be softened or even reversed if a firm has already established a strong history of accountability, having already corrected and compensated for its negative externalities through CSR initiatives conducted in the past. With more companies engaging in CSR, researchers may soon have access to data that allow them to expand on our research to examine the effects of a firm's rich history of CSR actions on brand sales, beyond merely controlling for corporate CSR reputation. Moreover, might it be wise for firms to deploy multiple CSR strategies at once? Could conducting cultivating and corrective CSR at the same time counteract the negative effects of cultivating on sales, or could it confuse consumers and hurt the brand's CSR reputation? Concurrent CSR actions may increase awareness and be more likely to be noticed by investors ([83]), but estimating the direction of their net effect may not be straightforward.Finally, while our justification for the negative effects of cultivating initiatives is rooted in theory (lower accountability) and supported by data from experiments showing that consumers view brands that engage in this type of CSR as less sincere, the question of why firms pursue such activities, beyond the obvious tax benefits, remains open. Fully answering this question is beyond the scope of this article, but one possibility is that firms may target other stakeholders, such as employees or shareholders, with these efforts. For instance, it could be that employee engagement in some cultivating actions may lead to positive employee outcomes, such as higher productivity and retention, which may help offset some of the negative effects on sales. Moreover, cultivating CSR is directed at a set of stakeholders that is in many cases distinct from the consumer base. Consequently, it would be valuable to take a closer look at which external group of stakeholders can be targeted with cultivating CSR and how these actions should be framed and communicated so that consumers view them more favorably. ConclusionWe show that brand decisions to engage in CSR present both opportunities and challenges. By reducing their negative impact or footprint first, brands appear to win consumers' approval to the greatest degree. However, stepping beyond its natural purview may be met with cynicism if a brand has not yet met a certain standard for its own behavior. For brands with already sterling reputations, the prospect of further impressing consumers can be more challenging, though doing good outside their footprint in the form of philanthropic efforts becomes an option. These results provide practical guidance for managers making decisions about their own CSR. Overall, it is both encouraging and promising to note that business, consumer, social, and environmental interests can align in the form of businesses genuinely reducing their adverse impact for global betterment. "
37,"The Influence of Social Norms on Consumer Behavior: A Meta-Analysis Social norms shape consumer behavior. However, it is not clear under what circumstances social norms are more versus less effective in doing so. This gap is addressed through an interdisciplinary meta-analysis examining the impact of social norms on consumer behavior across a wide array of contexts involving the purchase, consumption, use, and disposal of products and services, including socially approved (e.g., fruit consumption, donations) and disapproved (e.g., smoking, gambling) behaviors. Drawing from reactance theory and based on a cross-disciplinary data set of 250 effect sizes from research spanning 1978–2019 representing 112,478 respondents from 22 countries, the authors examine the effects of five categories of moderators of the effectiveness of social norms on consumer behavior: ( 1) target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors, and ( 5) methodological characteristics. The findings suggest that while the effect of social norms on approved behavior is stable across time and cultures, their effect on disapproved behavior has grown over time and is stronger in survival and traditional cultures. Communications identifying specific organizations or close group members enhance compliance with social norms, as does the presence of monetary costs. The authors leverage their findings to offer managerial implications and a future research agenda for the field.Keywords: cultural influence; meta-analysis; reactance; social approval; social influence; social marketing; social norms marketing; social normSocial norms shape consumer behavior. Defined as ""rules and standards that are understood by members of a group, and that guide and/or constrain social behavior without the force of laws"" ([26], p. 152), social norms influence various forms of everyday consumption, including food choices ([87]), responses to new products ([51]), and loyalty ([63]). For example, signs in a hotel stating that other hotel guests reuse their towels increase towel reuse ([38]). Social norms are often leveraged by marketers and policy makers to encourage various socially approved behaviors, such as conserving energy ([95], [96]), complying with product recalls ([85]), and making tax payments (Cabinet Office UK [22]). They are also used to discourage socially disapproved behaviors, such as polluting the environment ([109]) and smoking or excessive alcohol or drug use ([108]).The academic literature examining social norms has produced conflicting findings ([61]; [95], [96]). Some studies report large-scale favorable results for using social norms to curb socially disapproved behaviors ([21]). [90], for example, report a significant reduction (13%) in the prevalence of impaired driving among students. However, some campaigns encouraging socially approved behaviors have backfired. For example, [95], [96]) find that social norms for energy preservation can increase energy consumption. These mixed findings suggest contingent effects of social norms on behavior. A second reason for mixed findings is that some research studies actual behavior, while other research examines behavioral intentions. A final reason for mixed findings may lie in the fact that the country context introduces cultural factors into the study of norms that are important to their impact.Our article looks across a wide range of research on social norms across behaviors, time, and cultures to resolve these conflicting findings and to synthesize the extant literature on social norms. Specifically, we investigate the effects of social norms on actual consumer behavior and identify moderators of these effects, using reactance theory as a theoretical lens ([18]; [92]). We contend that the effectiveness of social norms varies with the level of consumer reactance they trigger ([18]); norms that are less likely to trigger reactance are more likely to be effective.We conduct a meta-analysis that examines the effects of five categories of moderators of the effectiveness of social norms on consumer behavior, matching central factors that may induce reactance. Specifically, we examine how the relationship between social norms and behaviors depends on ( 1) social approval or disapproval of behavior and other target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors (e.g., culture, time), and ( 5) methodological characteristics (e.g., type of sample, study).We collected 250 effect sizes from 136 articles published between 1978 and 2019 across different fields (e.g., marketing, psychology, health, environmental studies), representing 112,478 respondents from 22 countries. In conducting this research, we encountered several meta-analyses related to social norms. However, most prior meta-analyses focus on a single behavior, such as condom usage ([98]), or else investigate limited set of communication factors, such as whether the norm is descriptive or injunctive ([76]; [91]). Moreover, most include consideration of behavioral intentions rather than actual behavior, which is our focus. Finally, some prior meta-analysis focus on studies that use a specific theoretical framework, such as the theory of planned behavior ([ 2]; [70]), which limits their generalizability.We aim to go beyond these insights by investigating critical moderators that have not been addressed in prior research. We not only study the new moderator of socially approved versus disapproved behavior but also examine new and managerially actionable moderators, such as target behaviors, communication factors, and consumer costs. Importantly, this study investigates behaviors (observed or reported) rather than intentions and covers consumer behaviors across domains, regardless of the theoretical framework used in primary studies. With this comprehensive approach, we establish that social norms have significant impacts on behavior, but the effect varies systematically according to the influence of a wide range of moderators.This research makes several contributions across domains. First, we go beyond previous meta-analyses and contribute to theories of reactance and social influence by uncovering previously overlooked moderators and establishing several new empirical generalizations. Second, for social norms marketing ([38]; [110]), we specify the effects of social norms for a broad spectrum of consumer behaviors and detail how practitioners and government officials can utilize actionable moderators, such as using appropriate communication elements for certain behaviors, countries, and consumers. This should improve their success rate which has been mixed to date.Third, we contribute to the literature on cross-cultural marketing ([88]; [93]; [106]) by establishing how cultural differences can determine the effects of social norms on both socially approved and disapproved behaviors. Finally, we develop a comprehensive research agenda, based on insights from our meta-analysis. Theoretical Background Social NormsSocial norms are a shared understanding among members of a society about which behaviors are permitted, forbidden, or obligatory ([29]). They result from exposure to and observations of others' behavior and act as a ""social proof,"" whereby consumers follow the actions or opinions of others, in the belief that ""If everyone is doing it, it must be a sensible thing to do"" ([25], p. 1015). Social norms serve as decision shortcuts for choosing how to behave in a given situation ([24]).One of their distinctive features is that social norms are shared, which implies the existence of some group through which they spread ([26]). Humans maintain social harmony by complying with the social order and developing coping strategies to ""fit in"" ([66]) or ""copy the successful"" ([46]). Consequently, humans have an almost automatic propensity to learn social norms ([81]). Yet, this propensity does not necessarily result in compliance with them ([84]).The reason is that unlike laws, social norms are informal—they regulate behaviors without formal enforcement ([43]), so consumers have the freedom to follow or violate social norms ([26]). Accordingly, their impact on behavior stems from two evolutionary desires: ( 1) for social acceptance or affiliation and ( 2) for avoiding negative social outcomes such as social exclusion ([ 8]; [66]). As people are free to comply, and are inclined to do so, understanding why they do not comply is key for identifying systematic differences in reactions to social norms. Social Norms and ReactanceDespite consumers' natural inclination to comply with social norms, research consistently shows that attempts at influence that cite social norms can evoke psychological reactance ([18]; [92]). Reactance stems from individuals cherishing their autonomy and freedom of choice. As [19], p. 420) explain, ""For a given individual at a given time, there is a set of behaviors in which he believes he is free to engage. Any reduction or threat of reduction in that set of free behaviors arouses a motivational state, 'reactance,' which is directed toward reestablishment of the lost or threatened freedom."" If consumers believe their freedom to engage in a specific behavior is threatened, this evokes reactance, which enhances the attractiveness of the threatened behavior.Reactance theory is useful for approaching the vast, heterogeneous literature on social norms because it provides a broad theoretical lens for investigating the influence of diverse factors, including behavioral, communication, consumer, and environmental factors ([92]). Thus, we build on the key antecedents of reactance: consumer expectations of freedom and the extent of the freedom threat ([92]) to understand the drivers of systematic differences in the effects of social norms on behavior. Expectations of freedomConsumers do not perceive all their behaviors as freedoms ([20]), so reactance is contingent on an expectation that the person can freely choose among different behavioral alternatives ([27]). Thus, consumers likely exhibit reactance to social norms that appear to undermine their freedom ([19]). If they lack expectations of freedom in the first place, social norms should trigger less reactance ([92]). Several studies affirm that consumer reactance to attempts to influence decreases if an experimental manipulation lowers their perceptions of choice freedom ([36]; [58]). Freedom threatA social influence attempt that implies that someone is trying to reduce freedom represents a threat ([18]). This threat of social norms is exacerbated if the norms exert greater pressure for change ([20]). The threat level tends to reflect the way a social norm is communicated ([92]), so more forceful messages prompt more reactance ([59]; [111]). For example, research suggests reduced compliance with messages that advocate teetotalling rather than limited drinking ([ 9]). Yet, freedom threats may also stem from barriers to performing a behavior, such as consumer costs. When costs are a barrier to free choice, the aroused reactance is directed at maintaining the threatened behavior and therefore increasing its desirability ([27]). Conceptual FrameworkBuilding on reactance theory, we identify different groups of moderators driving behavioral compliance with social norms, as shown in Figure 1. These moderators include target behavior characteristics, communication factors, consumer costs, and environmental factors.Graph: Figure 1. Conceptual framework: Factors that influence the effect of social norms on behavior. Target Behavior CharacteristicsThe effects of social norms may vary across behaviors because characteristics inherent to the behavior influence perceptions of the freedom to perform it and threat to that freedom. Social approval/disapproval of behaviorsSocieties have developed social reinforcement mechanisms that encourage some behaviors and discourage others ([48]; [72]). We define socially approved behaviors as being explicitly encouraged by society (e.g., recycling, volunteering), socially acceptable (e.g., carpooling), and/or perceived as appropriate by society (e.g., using condoms). We define socially disapproved behaviors as being explicitly discouraged (e.g., smoking), socially unacceptable (e.g., littering), and/or perceived as inappropriate (e.g., binge drinking). A socially approved behavior evokes positive reinforcement via social outcomes, such as inclusion, acceptance, and affiliation ([24]). A socially disapproved behavior instead induces negative reinforcement via social consequences, such as social exclusion, alienation, or ridicule ([66]). Social approval versus disapproval of behaviors is thus a crucial factor that has implications for consumers' expectations of their freedoms to perform them.Performing a socially disapproved behavior is potentially more damaging to society as a whole than failing to perform an approved behavior ([47]). Thus, to maintain social order, societies tend to be more punitive of disapproved behaviors ([33]; [46]). In contrast, not adopting an approved behavior is less harshly punished and sometimes can even bring positive benefits, such as elevation in inferred social status ([ 8]). Thus, consumers are less likely to perceive social norms regulating socially disapproved behaviors as limitations to their freedoms, which diminishes reactance and increases compliance with social norms discouraging these behaviors. Yet, social norms targeting socially approved behaviors are seen as freedom limitations, causing more reactance and reduced compliance. Thus, we expect social norms pertaining to socially disapproved behaviors to be more effective than those pertaining to socially approved behaviors (H1). Existing versus new behaviorsExisting behaviors are already performed by consumers, at least sometimes, in contrast to entirely new behaviors. Consumers already have exercised their freedom to perform the existing behaviors, so they may feel less threatened when encouraging existing behaviors and their reactance to social norms that target existing behaviors may be relatively low. In contrast, targeting a new behavior may represent a stronger threat to freedom and, thus, induce reactance and decrease compliance. Consistently, for example, compliance with hand-washing advice has been higher than compliance with mask-wearing advice during COVID-19 and other infection outbreaks ([32]; [103]). Hand washing is an existing behavior and engrained into daily routines, whereas mask wearing was new for most consumers and generated more reactance. Therefore, we expect social norms pertaining to existing behaviors to be more effective than those pertaining to new behaviors (H2). Hedonic versus utilitarian behaviorsHedonic behaviors are those driven by pleasure-related goals and are evaluated primarily on the benefits related to enjoyment, taste, aesthetics, and symbolic meaning. Utilitarian behaviors, instead, are driven by functionality goals and are performed and evaluated primarily on the basis of functional, instrumental, and practical benefits ([23]). Social norms can pertain to both types, including utilitarian behaviors such as banking (e.g., ""Most millennials use online banking"") and hedonic ones such as buying cosmetics (e.g., ""12 makeup bag must-haves""). But their effectiveness is not clear a priori. On the one hand, reactance to social norms might be higher for hedonic behaviors because consumers have a stronger desire to perform those behaviors as part of their sense of freedom ([79]). Consumers can leverage social norms to justify a desirable behavior for themselves and enhance their perceptions of freedom to perform it. For example, the justification that ""everyone's doing it"" is common for hedonic behaviors ([39]) and can increase perceived freedom for engaging in these behaviors. On the other hand, indulging in a hedonic behavior often prompts a sense of guilt, making it harder to justify ([75]; [83]), which may reduce consumers' perception of freedom. With these opposing predictions, we treat the effects of social norms on hedonic versus utilitarian behaviors as an empirical question. Behaviors benefiting other peopleSome behaviors benefit other people directly (e.g., donating to charity), whereas others have indirect benefits (e.g., recycling). Social norms stem from group considerations, so consumers' willingness to enact their freedom may decrease if they realize that others will be negatively affected by their social norm violations ([101]). Correspondingly, behaviors that have negative implications for others yield lower reactance levels ([34]). We therefore expect that when others benefit from the behavior, this will enhance the effect of social norms on that behavior (H3). Public versus private behaviorsWe define public behaviors as those that are performed in public or can be observed by others (e.g., using public transport), in contrast to private behaviors (e.g., reducing energy consumption at home). Private behaviors are not subject to others' scrutiny, so consumers' perception of freedom threat to perform them may be relatively low, which should decrease reactance ([92]). This argument would imply that compliance with social norms regulating private (vs. public) behaviors should be higher. Yet, for public behaviors, reactance may also be reduced but for a different reason. Specifically, consumers are often concerned with how others perceive them ([65]), which reduces their willingness to enact their freedom and, in turn, reduces reactance. This would suggest higher compliance with social norms regulating public (vs. private) behaviors. Given these two opposing predictions, we refrain from making a directional hypothesis about this variable. Communication FactorsThe use of social norms can trigger reactance because communication factors influence the perceived threat to behavioral freedom ([ 9]; [92]). We consider several communication factors, such as how the norm is formulated as well as whether it benefits an organization, references specific groups, and includes explicit sanctions or rewards. Norm formulationSocial norms can be formulated as descriptive or injunctive. Descriptive norms describe typical behaviors of some relevant group and signal which behaviors are most popular ([25]; [95]). Injunctive norms instead prescribe certain behaviors and indicate what the target consumer should or should not do. For example, a list of ""bestsellers"" represents descriptive norms, but ""ten must-read books"" lists communicate injunctive norms. As injunctive norms convey explicit demands, which consumers likely perceive as forceful threats to their freedom, they should generate more reactance than descriptive norms ([69]; [109]). Consumers exposed to descriptive norms instead may come up with reasons for the behavior of the majority and adjust their own behavior accordingly, without much reactance ([95]). Therefore, we expect a stronger impact of descriptive (vs. injunctive) social norms on behaviors (H4). Organization-benefiting normsWe define social norm communications that reveal a specific entity, such as a firm or government body, which would benefit from compliance with the social norm, as organization-benefiting social norms. For example, ""my friends subscribed to the university's gym program"" would benefit the gym if the target consumer complied with this behavior. While specific entity matters, overall, social norms that refer to organizations tend to be more concrete and specific because they activate situational factors (i.e., where and when the norm applies) ([ 1]). Such specificity and concreteness diminish the general threat to freedom for consumers by limiting it to the particular situation, which lowers their reactance ([38]). We thus expect organization-benefiting social norms to be more effective than those that do not mention organizations (H5). Close group membersCommunications about social norms often specify close group members—that is, people who are genetically related (e.g., family) or similar (e.g., close friends)—rather than refer to an abstract group (e.g., fellow citizens, people). Evolutionary predictions of social cooperation highlight kinship mechanisms. Namely, a request that activates a kin care motive reduces reactance and promotes compliance without expectations of reciprocation ([41]; [46]). The closer the relationship is, the less reactance consumers are likely to experience, enhancing norm compliance ([79]; [99]). Thus, we expect social norms referring to a close group member to be more effective than social norms referring to abstract or distant groups (H6). Authority figuresCommunications around social norms often refer to authority figures, or individuals who can exercise power over others, formally or informally (e.g., superiors, experts, government officials, teachers), to enhance compliance. [78] famous studies show that formal orders from an authority figure (real or perceived) increase obedience. Yet, because social norms are informal, being required to do something by an authoritative source may make the threat to freedom more salient and trigger reactance ([ 4]). For example, expert recommendations may lead to reactance and diminish compliance ([36]). For these reasons, we expect social norms referring to authority figures to be less effective than those that do not refer to authority figures (H7). Explicit sanctions and rewardsThe sanctions and rewards associated with noncompliance and compliance with social norms might be either implicit, meaning they are indirect and left for consumers to infer, or explicit, meaning they are clearly stated. If sanctions and rewards are explicit, they might diminish behavioral compliance because they make the persuasive nature of the social norm message salient ([86]) and threaten freedom expressly ([58]). Both aspects increase the perceived threat to freedom to perform the behaviors and reactance ([ 4]). Thus, we expect social norms that specify potential sanctions (for failing to comply) (H8) or potential rewards (for complying) (H9) to be less effective than social norms that do not make those consequences explicit. Consumer CostsThe costs incurred to perform a behavior can create barriers. For example, social norms may direct consumers to buy an electric car, which is considerably more expensive than a regular car. We believe such costs will increase consumer reactance ([92]). However, the direction of the cost effect is not clear a priori (see [27]). On the one hand, a high cost may signal the desirability or status of the behavior, thereby motivating compliance. On the other hand, a high cost may dissuade consumers from attempting the behavior. Given these two opposing forces, we refrain from making directional hypotheses about costs, including costs associated with effort, money, and time. EffortWe define ""effort costs"" as the amount of physical or mental resources consumers must invest to comply with social norms. Some behaviors require more effort (e.g., exercising), others less (e.g., not littering). Social norms that require more effort demand greater behavioral change. They may either increase compliance by increasing the attractiveness of the effortful behavior or decrease compliance by decreasing the attractiveness of the effortful behavior (by derogating it because of reduced attainability) ([27]). Which of these two forces is stronger is an empirical question. Monetary costsConsumers may incur additional monetary costs to comply with social norms. For example, buying organic rather than conventional food requires more monetary resources. However, reusing a hotel towel does not result in monetary costs. Monetary costs constitute a direct barrier to free choice because consumers must sacrifice extra resources to comply. When social norms regulate costly behaviors, the monetary costs may imply a greater threat to the freedom to engage in this behavior, enhancing the attractiveness of this option and increasing compliance with such social norms ([27]). Yet, monetary costs may also emphasize the unattainability of the option, which would reduce compliance. Thus, we treat the effect of monetary costs on compliance with social norms as an empirical question. Temporal costsCompliance with social norms may require long-term (e.g., adhering to a healthy eating program) or temporary (e.g., reusing hotel towels) commitment. The temporal costs barrier is greater for behaviors with long-term commitments because these social norms impose more behavioral constraints than those that require only temporary commitments. Thus, on the one hand, consumers may also have stronger resistance to losing an option with potential longer-term consequences ([58]), which would increase compliance with social norms regulating longer-term behaviors. On the other hand, perceived unattainability of behaviors is also greater if they persist, now and into the future, rather than if they involve a single instance, which could decrease compliance with social norms involving longer-term commitment. Thus, we treat the effect of temporal costs on compliance with social norms as an empirical question. Environmental FactorsConsumers form freedom expectations through socialization in a specific cultural environment at a particular time ([80]). Culture shapes expectations by providing a logic for acting both housed in members' knowledge and beliefs and observed in members' behaviors ([102]). In some cultures, the range of approved behaviors is wide, and behavioral transgressions of social norms are tolerated. Other cultures allow a narrower range of behaviors and exhibit lower tolerance for deviations from social norms ([74]; [106]). Consumer reactance and compliance to social norms should thus differ systematically across cultures ([94]).To account for cultural differences, we adopt Inglehart's cultural framework ([52]) with two bipolar dimensions: traditional versus secular-rational and survival versus self-expression values. These dimensions have clear implications for reactance to social norms because they influence tolerance for transgressions (traditional–secular-relational) and the range of approved behaviors (survival–self-expression). Moreover, these dimensions are measured regularly, which enables us to account for cultural dynamics ([104]).[ 6] Traditional versus secular-rationalThis dimension contrasts traditional societies, in which religion is very important, and secular-rational societies, in which it is not ([52]). Traditional societies also emphasize deference to authority, absolute standards, cultural protectionism, and national pride, and they generally exhibit less tolerance for transgressions of social norms. Secular-rational societies reflect opposing values. We expect the effect of social norms on behavior to be stronger in cultures closer to the traditional (vs. the secular-rational) pole (H10) because they effectively restrict consumers' awareness and expectations of freedom, which should decrease reactance. Survival versus self-expressionThis dimension reflects transitions from industrial to postindustrial societies ([52]). Survival societies emphasize economic and physical security and familiar norms to maximize the predictability of others' behaviors, which results in a relatively narrow range of behaviors that may be perceived as freedoms. Consumers in survival societies have low expectations of personal freedoms and identify less freedom to be threatened ([54]). In contrast, self-expression values emphasize variety, imagination, and tolerance of outgroups. As societies move toward self-expression, people generally become freer to make choices for themselves ([106]), which enhances their reactance to social norms and decreases compliance. In cultures that value self-expression, noncompliance with social norms may even signal the person's freedom to be unique, which is valued by consumers of these societies ([40]; [106]). In contrast, in survival cultures, violation of social norms is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. Thus, we expect the effect of social norms on behavior to be stronger in cultures close to the survival (vs. the self-expression) pole (H11). TimeThe human propensity to comply with social norms has resulted from evolutionary processes ([41]). Therefore, the effect of social norms on behaviors should be stable throughout the short time (in evolutionary terms) marketers have been using them as a persuasion strategy. Yet research into conformity to social pressures also indicates some changes over time, including studies that document that conformity in the United States has declined ([16]), increased ([60]), or fluctuated ([62]) due to changes in social media and the cohesiveness of society, among other things. Thus, the effectiveness of social norms over time is an empirical question. Moral freedomCultures also vary in moral freedom, which reflects the extent to which people make their own moral choices rather than being influenced by state intervention ([ 3]). We expect the effect of social norms on behavior to be stronger in countries lacking moral freedom (H12), because of the lower expectations of freedom in those countries. Interaction Effects The interaction of social approval/disapproval and the environmental factorsThus far, our discussion has focused on main effects. However, consumers do not learn social norms in isolation; instead, they become aware of freedoms to perform certain behaviors through socialization in a particular culture and by observing different behaviors over time ([80]). To the extent that different societies shape consumer awareness of social norms, we expect the effects of the behavior being socially approved versus disapproved to be moderated by environmental characteristics (i.e., culture and time).Specifically, with respect to the traditional versus secular-rational cultural dimension, we note that participation in a world religion makes punishments for socially disapproved behaviors more salient to people ([47]). In Christianity, seven of the Ten Commandments start with the phrase ""you shall not."" In Judaism, of 613 mitzvot in the Torah, 365 (60%) forbid bad behaviors. Islam explicitly specifies an extended list of behaviors that are haram, or forbidden ([71]). Thus, in traditional cultures, where religion is more important, reactance to social norms that target socially disapproved (vs. approved) behaviors might be lower, because many of these behaviors already have been forbidden by religions. Thus, we expect the stronger effect of social norms on behaviors in traditional cultures to be especially pertinent for socially disapproved (vs. approved) behaviors (H13).With respect to the survival versus self-expression cultural dimension, engaging in socially disapproved behaviors in survival cultures is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. In contrast, in societies leaning toward the self-expression pole, engaging in disapproved behaviors would be more tolerated ([52]). Thus, we expect that the effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors (H14).As to the effect of time, we expect that social media might enhance the effects of social norms by exposing consumers to more regular reinforcements pertaining to a wider range of socially approved and disapproved behaviors ([10]). Social media enables consumers to share content and feedback in real time, much of which remains available indefinitely and can be tracked by other parties ([45]). Exposure to norm violations in such settings triggers exhibitions of moral outrage, as manifested in the notion of a ""cancel culture,"" whereby social media users shame and punish perpetrators of bad behaviors, signaling that such behaviors are not tolerated ([30]). These developments imply that, over time, engaging in socially disapproved behaviors is stigmatized more severely than not engaging in socially approved behaviors ([33]), which reduces expected freedom to perform socially disapproved behaviors. Thus, we expect a stronger, more positive impact of social norms on socially disapproved (vs. approved) behaviors over time (H15). Other interactionsIn addition to the aforementioned hypothesized interaction effects, given the importance of the fundamental distinction between social norms regulating socially approved versus disapproved behaviors, we also explore additional interactions. Specifically, we investigate the interaction between social approval/disapproval and the target behavior characteristics, communication factors, and consumer costs. When these interactions are significant, we return to them in the discussion and highlight theoretical and managerial insights. Methodological CharacteristicsSystematic differences in the methodologies used by studies may cause variation in the reported effects ([14]). We control for ( 1) type of data ([quasi]experiment vs. other), ( 2) whether a sample involves students or regular consumers, ( 3) whether participants were exposed to (vs. indicated their perceptions of) social norms, and ( 4) whether participants' behavior was self-reported (vs. observed). Finally, to account for publication bias, which arises when the effect sizes in published studies are not representative of the entire population of effect sizes ([17]), we control for ( 5) the association between the strength and the precision of the effect sizes ([100]). More details follow. Methodology SampleTo identify relevant studies of the impact of social norms on consumer behaviors, we retrieved references from Google Scholar, Online Contents National, PsycINFO, and the Web of Science up to March 2019. We searched for keywords such as ""norm,"" ""social norms,"" and ""social pressure"" (for the full list of keywords, see Web Appendix A). We also checked the websites of the Social Science Research Network, the National Social Norms Resource Center, and Higher Education Center for Alcohol and Other Drug Abuse and Violence Prevention for relevant studies. We posted requests for unpublished manuscripts and working papers on the online academic platform ELMAR. Finally, we examined all cross-references from applicable documents. The procedure resulted in articles from five research domains: psychology (35.2%), health (34.4%), marketing (10.4%), food and nutrition (10.8%), and the environment (9.2%).Our dependent variable is the strength of the relationship between social norms and consumer behavior in the studies, which constitutes their observed effect sizes. We selected Pearson's product-moment correlation coefficient to measure effect sizes, because most studies operationalize both social norms and the target behavior as continuous variables. The consumer behaviors investigated in these eligible studies refer to the purchase, consumption, use, or disposal of products, services, material objects, or consumption experiences (e.g., buying organic products, subscribing to a gym, adopting mobile banking, donating). We exclude studies that focus on ( 1) aggregate entities (e.g., countries, societies) rather than individual consumers; ( 2) behaviors unrelated to consumption, such as social perceptions or interpersonal relations (e.g., stereotypes); ( 3) criminal behaviors, because the influence of the law would be confounded with the influence of social norms; and ( 4) consumers with impaired autonomy, such as workers making job-related decisions who must follow organizational policy, patients who rely extensively on others to make medical decisions ([77]), or people whose addictions limit their decision-making ability ([64]). Furthermore, to be included an eligible study must ( 1) examine actual behaviors, reported or observed (rather than intentions); ( 2) contain enough information to calculate the correlations between social norms and behaviors; and ( 3) support computations of the unconfounded effects of social norms. To illustrate ( 3), we excluded studies that collapsed the impacts of social norms and marketing promotion (e.g., [112]) or injunctive and descriptive social norms (e.g., [56]).The final sample thus consists of 252 effect sizes extracted from 137 articles, comprising 177 studies over the period 1978–2019. Web Appendix B lists the articles, effect sizes, and moderator values. The sample sizes of the primary studies range from 28 to 44,108 (median = 269), so that they produce a total of 112,929 unique respondents from 22 countries. Three of the 252 effect sizes have studentized residuals that are greater than 2.57 ([107]); two of them (r = −.19, n = 353; r = .71, n = 451) are influential, in that they lie outside the prediction interval (i.e., range of plausible values for any individual effect size) and cannot be explained by small sample sizes ([17]). We remove them from the subsequent analyses, leaving 250 effect sizes from 136 articles, based on 112,478 unique respondents. (As we detail in Web Appendix C, the primary studies provide an explanation for the extreme values of these outliers; the results are robust for including them in the analysis.) Variable CodingTwo independent coders (blind to the hypotheses) coded the moderators and cataloged the technical information (e.g., sample size). The intercoder agreement was 94.8%, and any disagreements were resolved through discussion. Effect sizesWe retrieved zero-order correlations, measuring the association between social norms and the target behavior, from the studies' correlation matrices or else converted statistics (e.g., F-value, t-value, p-value, χ2) into r (see [17]; [67]). If partial correlations were available, we also retrieved them from the studies. (The results are robust whether we use partial or zero-order correlations as measures of effect sizes.) We transformed the correlations into Fisher's z-scores ([17]) to satisfy the assumptions of normal distributions and known sampling variance of the effect sizes to estimate the model (for details, see the ""Model"" section).[ 7] In turn, we estimate the meta-analytic regression model with Fisher's z-scores as the dependent variable. We obtain the mean effect sizes, confidence intervals, predicted values, and plots by back-transforming Fisher's z-scores into correlation coefficients to facilitate interpretation (for details, see Web Appendix D). For robustness, we perform the analyses also by using the correlation coefficients. The effect sizes are coded such that a positive sign indicates a positive change in behavior (i.e., increase in socially approved or decrease in disapproved behaviors).[ 8] Moderators Table 1 shows the coding scheme for all the moderators. We mean-centered all continuous moderators and all dummy variables involved in interactions ([89]). We retrieve scores for the cultural dimensions from the World Values Survey ([53]) for each effect size, using the country and year of publication of each study. For the time variable, the code reflects the year of publication.[ 9] The precision of the effect size estimate is measured as the inverse of its standard deviation ([100]). If a publication bias is present, retrieved small sample studies are more likely to yield stronger effect sizes than those that are not retrieved, which implies a negative relationship between precision and effect size. By controlling for precision, the effects can be estimated more accurately ([35]).GraphTable 1. Coding Scheme for the Moderators of Social Norms–Behavior Effects. VariableCodeTarget Behavior CharacteristicsSocial approvalDummy = 1 if the behavior is socially approved (i.e., discussed positively by the authors) and 0 if the behavior is socially disapproved (i.e., noted as problematic by the authors). Mean-centered.ExistingDummy = 1 if the behavior exists (i.e., consumers already engage in it at least sometimes) and 0 if the behavior is new (i.e., consumers have not adopted the behavior yet).HedonicDummy = 1 if the behavior is hedonic (i.e., driven by pleasure-related goals) and 0 if the behavior is utilitarian (i.e., driven by functionality-related goals).Benefits to other peopleDummy = 1 if the behavior brings about social benefits and 0 otherwise.Public behaviorDummy = 1 if the behavior is public (i.e., is visible to others) and 0 if the behavior is private (i.e., is invisible to others).Communication FactorsSocial norm formulationDummy = 1 if the social norm is descriptive (i.e., describes behaviors of others) and 0 if the social norm is injunctive (i.e., suggests what should be done).Organization-benefitingDummy = 1 if the social norm benefits a specific organization and 0 otherwise.Close groupDummy = 1 if the norm refers to a person close to the individual and 0 otherwise.Authority figureDummy = 1 if the norm refers to a person in a position of authority and 0 otherwise.Explicit sanctionsDummy = 1 if the negative consequences of not abiding by the norms are made explicit and 0 otherwise.Explicit rewardsDummy = 1 if the positive consequences of abiding by the norm are made explicit and 0 otherwise.Consumer CostsEffortDummy = 1 if complying with the social norm entails much physical or mental effort and 0 if compliance entails little effort.Monetary costsDummy = 1 if complying with the social norm entails additional monetary costs and 0 otherwise.Temporal costsDummy = 1 if complying with the social norm entails a long-term investment and 0 if it entails a temporary investment.Environmental FactorsTraditional–Secular-rationalContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.Survival–Self-expressionContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.TimeContinuous: year of publication of the paper from which the effect sizes are extracted minus 2. Mean-centered.Moral freedomContinuous: World Index of Moral Freedom (i.e., extent to which individuals make their own moral choices rather than being influenced by state intervention; Álvarez, Kotera, and Pina 2020). Mean-centered.Methodological ControlsType of dataDummy = 1 if the study is an experiment or a quasiexperiment and 0 otherwise.SampleDummy = 1 when a student sample was used and 0 otherwise.Effect size precisionContinuous: inverse of the standard error of the effect sizes (Fisher's z-transformed). Mean-centered.Behavior operationalizationDummy = 1 when participants self-report the behavior and 0 when the behavior is observed.Social norm operationalizationDummy = 1 when participants are exposed to the social norms and 0 when the social norms are perceived. 1 Notes: We control for the operationalizations of behaviors and social norms in a robustness check (see Web Appendix G). Meta-Analytic Model and EstimationTo test the conceptual framework in Figure 1, the model should account for the structure of the data, because the effect sizes are nested within samples that are nested within articles, which could lead to correlated errors. We specify a mixed-effects meta-regression model using a multilevel parameterization ([105]) in which ( 1) observed effect sizes are assumed to be a normally distributed random sample from the population of true effect sizes; and ( 2) the variance distribution of true effect sizes can be explained by random effects at the effect size, sample, and article levels, to account for data nesting, and by the fixed effects of the moderators. Thus, the effect size i extracted from sample j in article u is modeled as follows:Effect Sizeiju = β0 + β1 Social approvaliju + β2 Existing behavioriju + β3 Hedoniciju + β4 Benefit peopleiju + β5 Public behavioriju + β6 Norm formulationiju + β7 Organization-benefitingiju + β8 Close groupiju + β9 Authority figureiju + β10 Explicit sanctionsju + β11 Explicit rewardsiju + β12 Effortiju + β13 Monetary costsiju + b14 Temporal costsiju + β15 Traditional–secular-rationalu + β16 Survival–self-expressionu + β17 Timeu + β18 Moral freedomu + β19 Social approvaliju × Traditional–secular-rationalu + β20 Social approvaliju × Survival–self-expressionu + β21 Social approvaliju × Timeu + β22 Type datau + β23 Sampleju + β24 Effect size precisioniju + δu + εi + γju + φiju, where δu ∼  N(0,  σδ2  ) is a random effect that reflects the variance among articles, εi ∼ N(0,  σϵ2  ) is the sampling variance of the observed effect sizes, γju ∼  N(0,  σγ2  ) is a random effect estimating the variance across samples nested within articles, φiju ∼  N(0,  σφ2  ) is a random effect that indicates the variance among effect sizes nested within samples and within articles, β0 is the intercept, and β1–24 are the parameter estimates for the moderators defined in Table 1. We perform all the analyses with the Metafor package for R ([107]). ResultsWe first present the grand mean effect size and the distribution of individual effect sizes. Next, we present the results of the meta-regressions testing for the moderators. To provide evidence without the multilevel specification, Model 1 does not account for the nested data structure. The hypothesized moderators (H1–H15) are then tested with Model 2, which accounts for the nested structure. We note that the findings of Model 1 versus Model 2 are very similar (see Table 3), despite their different approaches to nesting. Drawing on Model 2, Table 3 also presents predicted effect sizes for each level of the categorical moderators with all the other moderators set at their sample average ([15]) as well as simple mean correlations. The predicted values and simple mean correlations do not differ substantially, suggesting a good balance of moderator conditions across studies ([57]). Model 3 adopts an iterative approach to identify additional significant interactions between socially approved versus disapproved behaviors and target behavior characteristics, consumer costs, and communication factors, and we discuss its results where relevant. Grand Mean Effect Size and Distribution of Effect SizesOverall, social norms have a positive, small to medium impact on behaviors (i.e., the grand mean effect size is positive and significant, with  r¯   = .254 and a 95% confidence interval [CI95%] ranging from.232 to.277; [28]). The Q-statistic, which represents the total weighted deviation of each individual effect size from the mean, is significant (Q = 4,360, p < .001). Most observed effect size variance thus is systematic rather than due to sampling error and can be explained by moderators ([17]). Other heterogeneity indicators (Tau2 and I2) lead to the same conclusion (see Web Appendix E).The distribution of individual effect sizes, shown in Figure 2, reveals that they range from r = −.22 to r = .63 (M = .249, Mdn = .240, SD = .152), and 67% of them fall within a.10–.40 interval. Multicollinearity is not a concern. The largest bivariate correlation, r = .54, is between monetary costs and socially approved behaviors. The variance inflation factors are 3.73 or less for all variables. Table 2 provides the correlation matrix and descriptive statistics.Graph: Figure 2. Social norm–behavior effect size frequency distribution (k = 250).GraphTable 2. Bivariate Correlations and Descriptive Statistics for the Social Norm–Behavior Effect and Moderators. MeanSD12345678910111213141516171819202122 1. Effect size (r).25.1521 2. Social approval.79−.1211 3. Existing.26.004.0321 4. Hedonic.32.141−.525.0681 5. Benefits to people.32.095−.237−.074.0131 6. Public behavior.52.066.050.023.127−.1051 7. Norm formulation.30.287−.152−.056.150.118.0351 8. Org. benefiting.52.319−.028−.087.161−.208.038.1401 9. Close group.66.182−.207.005.112.125−.030.009−.014110. Authority figure.10−.120.048−.040−.121.078−.092−.137−.145−.004111. Explicit sanctions.15.014−.038.155.078−.127.029−.057.084.141−.018112. Explicit rewards.41−.078.106.064−.093−.162−.005−.076.085.109.116.101113. Effort.30−.215−.019−.150−.155−.169−.026−.129−.218.034.003.000.092114. Monetary costs.57−.005.540−.007−.429−.490−.006−.175.107−.058−.050.080.201.185115. Temporal costs.69−.038−.159−.039.073−.434.027.008.182.027−.138.005.135.389.150116. Trad.–Sec-rational.12.406.090.156−.193−.110−.074.007.104.067−.062.022.135−.111.129.109.049117. Survival–Self-exp..80.330−.054.202.034−.200−.035.034−.082.001−.106−.058−.016−.066.160.051−.040.134118. Time (year)20049.219.164.026.087.009−.101.266.324.049−.231.071−.052−.073.025.106.133.207−.018119. Moral freedom74.918.71−.096.108.130−.072−.016−.041.007.030−.085.036.024.018.110−.012−.004.154.436.059120. Type of data.16−.065−.194−.249−.022.333.122−.133−.060.094−.126−.165−.009−.270−.211−.086−.240−.027−.195.006121. Student sample.41.034−.346.170.285−.057.167.048.098−.016.046−.048.053−.137.173.081−.253−.070.084−.041−.044122. Effect size precision20.4719.77.144.029−.183.117−.023.155.168−.033−.103−.031−.011−.063−.091−.101.055.130−.009.169.076−.166.2291 2 Notes: Boldfaced correlations are significant at α = .05. For the dummy variables, only the mean is provided as a descriptive statistic. Target Behaviors Characteristics Social approval/disapprovalContrary to H1, social norms are not more effective for disapproved (vs. approved) behavior (b = −.002, p = .957). Although the insignificant main effect suggests that social norms are equally effective for approved and disapproved behaviors overall, this variable is involved in several interactions with other moderators, as we discuss next. Existing versus newThe effect of existing (vs. new) behavior is not significant (b = .017, p = .514); thus, H2 is not supported. However, according to Model 3, this variable interacts with the social approval of behavior at marginal significance (b = .138, p = .072). Figure 3, Panel A, shows that when targeting socially approved behaviors, social norms tend to be more effective in encouraging existing (predicted  r¯   = .253) versus new (predicted  r¯   = .217) behaviors. For socially disapproved behaviors, the opposite pattern emerges, as social norms tend to be more effective for discouraging new (predicted  r¯   = .298) versus existing (predicted  r¯   = .207) behaviors.Graph: Figure 3. Social norm–behavior effect sizes for socially approved and disapproved behaviors as a function of focal moderators. Hedonic versus utilitarianThe impact of social norms does not differ across hedonic and utilitarian behaviors (b = .016, p = .599). This finding is not surprising given the two opposing forces (desire and guilt) driving hedonic behaviors. Benefits to peopleSocial norms are more effective when behaviors benefit others (predicted  r¯   = .289) than when they do not (predicted  r¯   = .229, b = .064, p = .035), in support of H3. Public behaviorThe effect of public behavior is not significant (b = .013, p = .500), suggesting that social norms are equally effective for public and private behaviors. Communication Factors Norm formulationIn support of H4, descriptive norms have stronger effects on behavior (predicted  r¯   = .305) than injunctive norms (predicted  r¯   = .223, b = .088, p < .001). Further, the interaction between norm formulation and socially approved behaviors in Model 3 is also significant (b = −.110, p = .018). Figure 3, Panel B, shows that descriptive (vs. injunctive) norms are more effective when targeting disapproved behaviors (predicted  r¯ descriptive = .337 vs. predicted  r¯ injunctive = .183, Δ = .154) than when targeting approved behaviors (predicted  r¯ descriptive = .281 vs. predicted  r¯ injunctive = .228, Δ = .053). Thus, descriptive social norms that describe how the majority behaves are especially effective in curbing disapproved behaviors. Organization-benefitingIn support of H5, social norms are more effective when they benefit an organization (predicted  r¯   = .279) than otherwise (predicted  r¯   = .214, b = .069, p = .004). Close groupIn support of H6, social norms are more effective if they refer to a close group member (predicted  r¯   = .266) versus an abstract group (predicted  r¯   = .213, b = .057, p = .008). Authority figureThe effectiveness of social norms is not impeded by references to an authority figure (b = −.012, p = .729), in contrast to H7. Sanctions and rewardsThe effect of social norms does not depend on the presence of explicit sanctions (b = −.010, p = .767), disconfirming H8. However, social norms with explicit mentions of rewards (predicted  r¯   = .208) are marginally less effective than those where rewards are not mentioned (predicted  r¯   = .255, b = −.050, p = .078), in line with H9. Consumer Costs EffortThe amount of effort required to comply with social norms does not have a significant effect on compliance (b = −.031, p = .255). This finding is consistent with the idea that there are two opposing forces behind this effect that counteract each other. Monetary costsSocial norms exert stronger effects on behavior when compliance entails monetary costs (predicted  r¯   = .276) than when it does not (predicted  r¯  =.211, b = .069, p = .017). This finding is consistent with the idea that barriers such as monetary costs can make the behavior more desirable for consumers (e.g., via status signaling; [27]). Temporal costsSocial norms seem to be equally effective for behaviors requiring a long-term or a temporary investment, as temporal costs are nonsignificant (b = −.011, p = .706). Environmental Factors Traditional–secular-rationalThe traditional–secular-rational cultural dimension does not have a main effect on the effectiveness of social norm (b = −.017, p = .643), which fails to support H10.[10] However, we find some support for H13 because the interaction with the social approval of behaviors is positive and marginally significant (b = .211, p = .104). Figure 3, Panel C, shows that the impact of social norms on socially disapproved behaviors is somewhat weaker in more secular-rational cultures, whereas their impact on socially approved behaviors remains stable across the traditional–secular-rational dimension. Survival–self-expressionThe effect of this cultural dimension is negative, as we expected, but it is not significant (b = −.027, p = .448), which does not support H11. However, in support of H14, its interaction with the social approval of behaviors is positive and significant (b = .180, p = .033). Thus, consistent with our expectations, social norms are less effective for socially disapproved behaviors in cultures closer to the self-expression pole, whereas their effectiveness for socially approved behaviors is stable across the survival–self-expression dimension (Figure 3, Panel D). TimeThe effect of time is not significant (b = .001, p = .956), but its interaction with the social approval of behaviors is negative and significant (b = −.006, p = .036), in support of H15. Social norms have become more effective over time at curbing socially disapproved behaviors, while their influence on socially approved behaviors remains stable (Figure 3, Panel E). Moral freedomThe national level of moral freedom lowers behavioral compliance at a marginal level of significance (b = −.002, p = .094), in support of H12. Methodological CharacteristicsThe studies in our meta-analysis yield the same results regardless of the type of data (b = −.045, p = .189) and whether they rely on student samples (b = −.001, p = .986). We obtain similar results if we control for whether the studies operationalize social norms using exposure or perception (note that we exclude this control variable in the main model because of multicollinearity but address it in supplementary analyses). Precision has a positive, marginally significant effect (b = .001, p = .082); therefore, effect sizes greater than the mean might be missing, which suggests that publication bias is not an issue. Robustness ChecksTo confirm that our results are robust, we first perform a series of diagnostic tests (Web Appendices E and F) to rule out publication bias; they reconfirm the positive influence of effect size precision in the meta-regression model. Next, we performed analyses based on 16 alternative model specifications. In Tables A1 and A2 in Web Appendix G, we present the results when we adopt alternative methodological choices: ( 1) including theoretical moderators only; ( 2) adding two demographic controls—the primary study participants' mean age and the percentage of men; ( 3) accounting for effect sizes coming from marketing journals; ( 4) estimating the meta-regression parameters with t-tests instead of z-values; ( 5) estimating the model including the two outliers (k = 252); ( 6) using the raw effect sizes—r and the variance of r—rather than Fisher's z transforms; ( 7) relying on partial instead of the zero-order correlation, if both measures could be retrieved from the primary study; and ( 8) controlling for the operationalization of the behavior (self-reported vs. observed) and for the operationalization of social norms (exposed vs. perceived) instead of controlling for the type of data. Across these methodological choices, the results remain consistent with the findings of the main models (Table 3).GraphTable 3. Meta-Regression Results. Model 1:Model 2:Model 3:Predicted ValuesSimple Mean CorrelationsPredictorsb (SE)b (SE)b (SE)r¯[CI95%]r¯[CI95%]Intercept.097 (.047)*.104 (.049)*.127 (.048)**——Social disapproval (k = 53)———.250 [.181,.317].299 [.248,.348]Social approval (k = 197).009 (.040)−.002 (.042).015 (.044).248 [.225,.271].250 [.230,.269]New behavior (k = 84)———.236 [.196,.276].263 [.221,.303]Existing behavior (k = 186).016 (.025).017 (.026).010 (.027).253 [.228,.277].259 [.237,.280]Utilitarian behavior (k = 170)———.244 [.216,.270].244 [.222,.265]Hedonic behavior (k = 80)−.012 (.028).016 (.030).001 (.031).258 [.216,.300].295 [.257,.331]Benefits people: no (k = 171)———.229 [.200,.258].251 [.228,.274]Benefits people: yes (k = 79).064 (.029)*.064 (.030)*.066 (.032)*.289 [.249,.328].279 [.244,.314]Private behavior (k = 120)———.242 [.214,.270].250 [.224,.275]Public behavior (k = 130).016 (.019).013 (.019).015 (.019).254 [.228,.280].269 [.241,.297]Norm formulation: injunctive (k = 175)———.223 [.200,.247].229 [.208,.251]Norm formulation: descriptive (k = 75).086 (.022)***.088 (.021)***.080 (.021)***.305 [.272,.338].329 [.293,.363]Organization-benefiting: no (k = 120)———.214 [.183,.245].208 [.181,.235]Organization-benefiting: yes (k = 130).077 (.022)***.069 (.024)**.078 (.025)**.279 [.250,.308].304 [.280.329]Close group: no (k = 85)———.213 [.179,.246].215 [.185,.244]Close group: yes (k = 165).066 (.021)**.057 (.022)**.051 (.022)*.266 [.242,.290].283 [.259,.306]Authority figure: no (k = 224)———.249 [.228,.271].266 [.246,.286]Authority figure: yes (k = 26)−.001 (.033)−.012 (.034)−.004 (.034).238 [.179,.297].202 [.149,.253]Explicit sanctions: no (k = 227)———.249 [.228,.270].259 [.239,.279]Explicit sanctions: yes (k = 23)−.019 (.032)−.010 (.035).001 (.035).239 [.177,.300].266 [.204,.325]Explicit rewards: no (k = 213)———.255 [.234,.277].265 [.244,.286]Explicit rewards: yes (k = 37)−.056 (.027)*−.050 (.028)†−.041 (.029).208 [.158,.257].229 [.184,.272]Effort: small (k = 174)———.257 [.231,.282].281 [.259,.303]Effort: large (k = 76)−.034 (.026)−.031 (.027)−.044 (.028).228 [.187,.268].210 [.174,.246]Monetary costs: no (k = 107)———.211 [.173,.249].262 [.230,.294]Monetary costs: yes (k = 143).071 (.027)**.069 (.029)*.072 (.029)*.276 [.246,.305].259 [.235,.282]Temporal costs: temporary (k = 78)———.256 [.211,.299].267 [.237,.297]Temporal costs: long-term (k = 172)−.012 (.028)−.011 (.029)−.010 (.031).245 [.220,.270].256 [.232,.281]Traditional–Secular-rational−.014 (.035)−.017 (.038)−.031 (.039)——Survival–Self-expression−.026 (.033)−.027 (.036)−.021 (.037)——Time−.001 (.001).001 (.001).001 (.001)——Moral freedom−.002 (.001)†−.002 (.001)†−.003 (.001)*——Social approval × Traditional–Secular-rational.175 (.121).211 (.130)†.298 (.145)*——Social approval × Survival–Self-expression.159 (.079)*.180 (.084)*.206 (.087)*——Social approval × Time−.005 (.003)*−.006 (.003)*−.008 (.004)*——Social approval × Existing behavior——.138 (.077)†——Social approval × Norm formulation——−.110 (.046)*——Type of data: other (k = 210)———.255 [.233,.277].265 [.244,.286]Type of data: (quasi)experiment (k = 40)−.048 (.033)−.045 (.034)−.014 (.036).213 [.155,.270].229 [.182,.274]Sample: nonstudent (k = 148)———.249 [.222,.275].254 [.230,.278]Sample: student (k = 102)−.001 (.023)−.001 (.024).011 (.025).248 [.214,.282].269 [.237,.301]Effect size precision.001 (.0005)†.001 (.0005)†.001 (.0006)†——Pseudo R230%29%30%Variances componentsTau2 = .016;σϵ2 = .001σδ2 = .0001;σϵ2 = .001;σγ2 = .004;σφ2 = .012σδ2 = .0006;σϵ2 = .001;σγ2 = .005;σφ2 = .011 3 *p < .05.4 **p < .01.5 ***p < .001.6 †p < .10.7 Notes: There is no regression coefficient for the reference category of the moderators. The pseudo R2 is the amount of between-effect size variance explained by the moderators. In Model 1, the nested structure of the data is not modeled and Tau2 represents the residual between-effect size variance. In Models 2 and 3, the nested structure of the data is accounted for and the variance components are the following:  σδ2   = variance across articles,  σϵ2  = sampling error variance,  σγ2   = variance between samples within articles, and  σφ2   = variance between effect sizes within samples within articles. Predicted values are based on Model 2 parameter estimates, with the levels of all the other moderators set at the sample average. The slight discrepancy between the predicted values for the categories of the moderators (e.g., monetary costs: no = .211; yes = .276, Δ = .065) and the corresponding regression coefficient (b = .069 > .065) arises because the parameter estimates are based on Fisher's z-scores, whereas the predicted values are expressed as correlation coefficients. The simple mean correlations are obtained using a random-effect model with restricted maximum likelihood as a between-effect size variance estimator. Predicted values and simple mean correlations are not provided for the continuous moderators, because their effect is linear (see Figure 3).In Table A3 (Web Appendix G), we also rule out the effects of four alternative moderators: ( 9) whether the target behavior entails environmental benefits, (10) whether the target behavior has social and physical consequences for the individual, and (11) whether the norm features a promotion or prevention frame. None of these alternative variables is significant.Finally, Table A4 (Web Appendix G) presents the results when we include additional country controls based on (12) country-level gross domestic product and population density, as well as (13) Hofstede's (vs. Inglehart's) cultural dimensions. The results again remain similar to those from the main models. Further, to account for a potential cultural invariance bias, we control for (14) the language spoken in the country of the data collection and (15) whether the study was conducted in an Asian country; (16) we also model the nesting of articles within countries. The stable regression coefficients suggest that cultural invariance does not affect the results. Across all 16 alternative models, the magnitudes, signs, and significance of the parameter estimates are consistent and aligned with our main findings, which strengthens our conclusions and affirms the robustness of the effects obtained in the main model. SummaryBy meta-analyzing the extant empirical evidence, our research provides new evidence for the effects of social norms on consumer behavior. On average, social norms significantly influence behavior and their effect (  r¯   = .254) is small to medium in size ([28]). Importantly, several moderators can explain substantial variation among these effects. Table 4 summarizes our findings. We next detail the theoretical and practical implications of our study.GraphTable 4. Hypotheses and Empirical Questions. Moderator and Predictions (Hypotheses) or Competing Explanations (Empirical Questions)Reactance ExplanationEvidenceTarget Behavior CharacteristicsSocial approval (vs. disapproval) (H1): Social norms for socially disapproved behaviors are more effective than those pertaining to approved behaviors.Expectations of freedom are lower for socially disapproved vs. socially approved behaviors×Existing (vs. new) (H2): Social norms are more effective when targeting existing (vs. new) behaviors.Freedom threat is lower for existing than for new behaviors×Hedonic (vs. utilitarian) (Empirical question): Greater desire or more guilt for hedonic benefits may either increase or decrease freedom threat from social normsN.A.n.s.Benefits to other people (H3): The presence of benefits to other people of a behavior enhances the effect of social norms on that behavior.Expectations of freedom are lower for behaviors that are beneficial to others✓Public behavior (Empirical question): Freedom threat from social norms may be lower for private behaviors due to a lack of scrutiny from others, or it may be lower for public behaviors due to concerns about perceptions from others.N.A.n.s.Communication FactorsNorm formulation (H4): Descriptive social norms have a stronger impact on behaviors than injunctive norms.Descriptive norms imply less freedom threat than injunctive norms✓Organization-benefiting (H5): Organization-benefiting social norms are more effective than social norms that do not mention organizations.Organization-benefiting social norms imply less freedom threat than social norms that do not benefit organizations✓Close group members (H6): Social norms referring to a close group member are more effective than social norms referring to abstract or distant groups.Close group members imply less freedom threat than abstract or distant groups✓Authority figures (H7): Social norms referring to authority figures are less effective than social norms that do not refer to them.Authority figures imply more freedom threat×Explicit sanctions (H8): Social norms that specify potential sanctions are less effective than social norms that do not make these consequences explicit.Explicit sanctions and rewards imply more freedom threat×Explicit rewards (H9): Social norms that specify potential rewards are less effective than social norms that do not make these consequences explicit.✓Consumer Costs Effort, monetary costs, and temporal costs (Empirical questions): The presence of costs barriers arouses reactance, which either makes the behaviors more attractive, or dissuades consumers from attempting the behaviors.Costs create barriers to free choice and increase reactanceMonetary costs lift social norm effect; n.s. for effort and temporal costsEnvironmental FactorsTraditional–Secular-rational (H10): The effect of social norms on behavior is stronger in cultures closer to the traditional (vs. secular-rational) pole.Expectations of freedom are lower in cultures toward the traditional pole and toward the survival pole×Survival–Self-expression (H11): The effect of social norms on behavior is stronger in cultures near the survival pole versus the self-expression pole.×Time (Empirical question): Evolutionary forces driving social norms effectiveness should be stable over time but fluctuations of conformism over time is observed.N.A.n.s.Moral freedom (H12): The effect of social norms on behavior is stronger in countries lacking moral freedom.Expectations of freedom are lower in low moral freedom countries✓InteractionsTraditional–Secular-rational × Approved behavior (H13): The stronger effect of social norms on behaviors in traditional cultures is driven by socially disapproved (vs. socially approved) behaviors.Expectations of freedom for socially disapproved behaviors are especially low in traditional and survival cultures✓Survival–Self-expression × Approved behavior (H14): The effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors.✓Time × Approved behavior (H15): There is a more positive impact of social norms for socially disapproved than for socially approved behaviors over time.Expectations of freedom for socially disapproved behaviors decrease over time✓Existing (vs. new) × Approved behavior (Exploratory interaction)N.A.Social norms are effective for existing, approved behaviorNorm formulation × Approved behavior (Exploratory interaction)N.A.Descriptive norms are effective for disapproved behavior 8 Notes: N.A. = not applicable; n.s. = not significant; x = not supported. Theoretical ImplicationsOur results go beyond previous meta-analyses by uncovering previously overlooked boundary conditions of the effects of social norms on consumer behavior using reactance theory as our theoretical lens. While our meta-analysis provides important insights about whether and how social norms can influence behavior, any meta-analysis is limited to the factors included in available primary studies. Those gaps also open avenues for future research, as discussed next. The Role of Social Approval/Disapproval on Social Norms Key insightsWe reveal that the effects of social norms on behavior differ systematically for behaviors that are socially approved versus disapproved in the presence of certain environmental factors. We find that the effects of social norms on socially disapproved behaviors have increased over time and are particularly strong in survival (vs. secular-rational) and traditional (vs. self-expression) cultures. In contrast, the effects of social norms on socially approved behaviors are more stable across cultures and time. Future researchThe critical difference in social norm effectiveness for regulating socially approved versus disapproved behaviors establishes the need to investigate drivers of this difference. In the process of adopting socially approved behaviors, consumers might glean benefits beyond the direct consequences of complying with norms. For example, does compliance promote positive emotions, due to an enhanced sense of belonging, acceptance, or well-being, independent of the benefits of adopting the behavior? And do consumers suffer negative emotions (e.g., guilt) when they engage in socially disapproved behaviors, and do social norms reinforce these emotions? Another issue is how legal mandates affect behaviors, both when they contradict social norms of disapproved behaviors (e.g., using cell phones while driving) and when they reinforce approved behaviors (e.g., driving below the speed limit). Implications for Social Influence Literature Key insightsWe shed new light on several unexplored moderators of the effects of social norms on consumer behavior and thereby contribute to the literature on social influence ([24]; [109]; [110]). For example, social norms have stronger effects on behaviors that benefit others, but they are weaker for already existing socially disapproved behaviors. Social norms are equally effective for private and public behaviors, for hedonic and utilitarian behaviors, for behaviors requiring much versus little effort, and for behaviors requiring long-term versus temporary commitment. Importantly, monetary costs do not deter consumers; on the contrary, they make them more likely to comply with social norms regulating costly behaviors, in line with the reactance theory explanation that barriers enhance behaviors' desirability. Future researchThe intriguing finding that private behaviors are just as likely to be influenced by social norms as public ones suggests new research avenues. Perhaps consumers overestimate the extent to which they are monitored by others because of the spotlight effect ([37]). If so, social norms could be effective in nonsocial settings, where they traditionally have been perceived as less influential. Further, finding that social norms are equally effective for hedonic and utilitarian behaviors highlights the need to clarify the roles of desire and guilt as underlying processes. Teasing out these effects could also help enhance communication strategies (e.g., downplaying or emphasizing desire and guilt). Yet, the lack of effect of some behavioral characteristics might also be due to selection; for example, researchers may be inclined study situations in which they expect social norms to matter. Thus, testing the effects of social norms in situations where they a priori seem less relevant is insightful. Studying the effectiveness of social norms when behavioral autonomy is impaired (e.g., employees making decisions on behalf of firms) is also intriguing. Finally, research could address behaviors consumers adopt to distinguish themselves from the group (e.g., to enhance authenticity; [82]) and the leadership behaviors they display to encourage others to go against current social norms (e.g., boycotting, brand sabotaging; [55]). Implications for Marketing Communications Literature Key insightsWe contribute to marketing communication research by identifying communication strategies that enhance the effectiveness of social norms in several ways. First, several commonly used social norm communication factors appear to be ineffective, such as referring to authority figures or specifying explicit sanctions; the same is true for rewards, which even seem to hinder social norms' effectiveness. Second, formulating norms as descriptive (vs. injunctive), organization-benefiting, and/or referring to close others enhance the effect of social norms on behaviors. Further, our exploratory results suggest that injunctive norms are especially weak when targeting disapproved behaviors, which is consistent with reactance theory. To discourage socially disapproved behaviors, injunctive norms tend to be proscriptive (""you should not"") rather than prescriptive (""you should""). Proscription represents a greater threat to freedom than prescription, so it prompts more reactance to injunctive norms ([11]). Instead, socially disapproved behaviors can be better curbed by descriptive norms.The finding that organization-benefiting social norms enhance compliance contributes to the emerging stream of research that examines how situational factors influence the effectiveness of social norms ([ 1]; [38]). While which type of organization benefits should matter as well, on a broader level, our results suggest that organization-benefiting social norms, because they are situation-specific, enhance compliance. Future researchMore research is needed for boundary conditions for the effectiveness of different communication strategies. For example, there are multiple avenues for future research on the impacts of sanctions and rewards. First, reward size and reward type (e.g., material vs. nonmaterial; [73]) might be important. Second, rewards or sanctions may not exert effects past a ceiling level of the impact of social norms when many consumers are ""uninfluenceable."" Third, research should identify ways to account for consumer needs and boost perceptions of the benefits of following and the costs of not following social norms without triggering reactance.Boundary conditions under which injunctive norms are more effective than descriptive ones (e.g., the level of ambiguity, the source of the norm: ""Why fight city hall?"") should be addressed. Further, because norms are group-specific, research could explore how group exclusivity should be communicated. Social norms linked to homogeneous exclusive groups (""Harvard students recycle"") might evoke less reactance because they distinguish group members from outsiders while also satisfying the need to belong ([13]). The effect of group exclusivity on behavior might be stronger if the communication contrasts ingroup versus outgroup norms (""Harvard students, unlike [vs. similar to] MIT students, recycle""). Relatedly, communicating the size of the group that shares social norms could also enhance social norms' influence ([46]).Finally, most of the primary studies in our meta-analysis predated social media, but our results suggest that social media may have disrupted the influence of social norms on behavior. Online interactions enable consumers to develop friendships with people they have never met in real life, so further research might investigate how social norms evolve on social media. For example, how does the immediate, often permanent feedback available through social media shape the effects of social norms on online behaviors? Consumers might not just comply with social norms but also try to become agents of influence by spreading the norm further on social media. Future research should investigate which factors facilitate such efforts ([68]), and the role of social norms in interpersonal relationships in general. Finally, future research should address the interaction between social norms and marketing-mix instruments, particularly promotion. Implications for Social Reactance Literature Key insightsOur results inform discussions on reactance ([18]; [92]) and the role of intrinsic versus extrinsic behavioral motivations ([31]). The nonsignificant effect of public behavior implies that consumers follow social norms even if their behavior is not observable to others. Further, while effort may impede social norms' effectiveness, the monetary costs of behavior enhance it, suggesting that consumer effort and temporal costs are a greater barrier to compliance than monetary costs. These results, together with the finding that explicit sanctions and rewards do not help, suggest that complying with social norms may be a more intrinsically motivated activity than previously believed. Future researchFollow-up research should investigate other barriers that may incite reactance and ways to circumvent them. Noting our finding that monetary costs enhance social norm compliance, we call for research that specifies the boundary conditions of this effect. More research is needed to clarify what makes costly behavior attractive to consumers. For example, costly behaviors may help signal status ([42]).Research should address the relative explanatory power of alternative mechanisms parallel to reactance that might influence the effect of social norms on behaviors. For example, the literature identifies other potential mechanisms, such as self-efficacy ([ 5]), sense of belonging ([ 6]), internalization ([97]), and justification of behaviors ([39]). Contributions to PracticeThe findings offer insights for marketers and public policy makers by identifying effective (and some commonly used but ineffective) strategies for enhancing the impact of social norms on consumer behavior. In contrast to conventional wisdom ([ 7]), our results suggest that the influence of social norms can prompt private acceptance. Thus, marketers and policy makers can leverage social norms to encourage both private and public behaviors. Communication StrategiesThe content of the communication should feature descriptive rather than injunctive forms of social norms (i.e., describe what [most] people actually do rather than what they should do[11]). Further, we recommend that marketers should avoid specifying explicit sanctions and rewards associated with social norms. Instead, strategies that highlight benefits to others or consumer freedom (e.g., a communication with a postscript ""The choice is yours""; [12]) may mitigate reactance and thus be more effective for inducing the target behavior.Practitioners might worry about highlighting a specific organization when communicating about social norms, but our results suggest that referring to a specific firm, governmental body, or nongovernmental organization can make communications about social norms more influential. Social norms are also more powerful when they cite people who are perceived as close to the target consumers. Thus, practitioners should target social norm communication toward nano-influencers on social media, with their smaller, more engaged audiences. In contrast, our results indicate that references to authority figures when using social norms do not affect consumer behavior.In communicating norms, marketers can acknowledge the monetary costs associated with the targeted behaviors. Although monetary costs are a financial barrier, they seem to also increase the desirability of the behavior, so social norms can be particularly effective for promoting costly behaviors like donations or buying (more expensive) organic food. Further, social norms are equally effective irrespective of required effort, and the time investment in complying. Cultural Differences Between Countries Socially approved versus disapproved behaviorsThe impact of social norms on socially disapproved behaviors varies significantly depending on the country of implementation, but it is more stable for socially approved behaviors. Social norms have stronger influences on socially approved than disapproved behaviors in secular-rational and self-expression cultures. These findings have important public health implications when group behavior is essential. To encourage mask wearing in most Western countries, for example, public officials should communicate that wearing a mask is a socially approved behavior that close others adopt. In most survival countries, the communications should highlight that not wearing a mask is socially disapproved. Cultural profilesTo specify the net effect of culture at the country level, we estimate the impact of social norms in countries with different cultural profiles. We calculate the predicted effect sizes for socially disapproved and approved behaviors in Figure 4 for eight countries that represent different regions of the world. We use descriptive norms as a base category and country scores on Inglehart dimensions from the latest wave of the World Values Survey.Graph: Figure 4. Predicted current effect sizes (Pearson's product moment correlations) of descriptive norms for socially approved and disapproved behaviors, by global regions.In Scandinavian countries such as Denmark (eight effect sizes), which score high on secular-rational values (85th percentile) and high on self-expression values (98th percentile), the mean effect size for socially approved behaviors is  r¯   = .365 (CI95% = [.282,.442]), whereas it is not significant for disapproved behaviors (  r¯   = .176, CI95% = [−.077,.407]). Campaigns using social norms thus may be effective for encouraging healthy eating, for example, but are likely not the best choice to curb excess drinking in such countries.In Western European (e.g., France) and Commonwealth (e.g., Canada, United Kingdom) countries with medium to high scores on both dimensions (but lower than Scandinavia), social norms are equally effective regardless of the social approval of behavior. For example, in Australia (20 effect sizes), a country with average secular-rational values (55th percentile) and high self-expression values (96th percentile), the impact of social norms on approved behaviors (  r¯   = .344, CI95% = [.273,.411]) is the same as the impact on disapproved behaviors (  r¯   = .355, CI95% = [.249,.452], Wald-type test = −.175, p = .431).In contrast, in the United States (104 effect sizes), which is more traditional (43rd percentile for secular-rational) and has high self-expression values (85th percentile), the effect of social norms on socially disapproved behaviors (  r¯   = .460, CI95% = [.374,.538]) is stronger than their impact on approved behaviors (  r¯   = .330, CI95% = [.262,.395], Wald-type test = −2.406, p = .008).Interestingly, we find the same pattern in Southern Europe (e.g., Italy) and China, even though these areas represent relatively high secular-rational (65th percentile) and low self-expression values (28th percentile). In these regions, social norms' effectiveness is greater for disapproved behaviors (  r¯   = .530, CI95% = [.397,.642]) than for approved behaviors (  r¯   = .319, CI95% = [.213,.417], Wald-type test = −2.611, p = .005).Finally, in countries with strong traditional and survival values, such as most African and Muslim-majority countries, social norms' impact on disapproved behaviors is much stronger than on approved behaviors. Consider Ethiopia (25th and 30th percentiles), where the mean effect size for disapproved behaviors (  r¯   = .660, CI95% = [.455,.799]) is much greater than that for approved behaviors (  r¯   = .297, CI95% = [.188,.398]). In these countries, social norms are especially effective for discouraging disapproved behaviors. ConclusionThis extensive meta-analysis shows that social norms significantly impact behavior and uncovers novel contingencies of this effect. We hope the proposed research agenda, which reflects our comprehensive investigation of the extant literature, sparks additional research in the fascinating ways in which social norms shape (or do not shape) consumer behavior.  "
38,"The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer Mindsets Despite the ubiquity of pets in consumers' lives, scant research has examined how exposure to them (e.g., recalling past interactions with dogs and cats, viewing ads featuring a dog or a cat) influences consumer behavior. The authors demonstrate that exposure to dogs (cats) reminds consumers of the stereotypical temperaments and behaviors of the pet species, which activates a promotion- (prevention-) focused motivational mindset among consumers. Using secondary data, Study 1 shows that people in states with a higher percentage of dog (cat) owners Google more promotion- (prevention-) focused words and report a higher COVID-19 transmission rate. Using multiple products, Studies 2 and 3 demonstrate that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Study 4 shows that pet stereotypicality moderates the proposed effect such that the relationship between pet exposure and regulatory orientations persists to the extent consumers are reminded of the stereotypical temperaments and behaviors of the pet species. Studies 5–7 examine the role of regulatory fit and evince that exposure to dogs (cats) leads to more favorable responses toward advertising messages featuring promotion- (prevention-) focused appeals.Keywords: pets; regulatory orientation; advertising; COVID-19Pets are prevalent and play important roles in consumers' daily lives ([ 2]; Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008; Serpell and Paul 2011). According to the survey of the American Pet Products Association ([ 4]), 68% of U.S. households, or 84.6 million homes, own a pet. Dogs and cats are the most popular pets, with 48% of U.S. households (60 million homes) owning at least one dog and 37% of U.S. households (47 million homes) owning at least one cat. Pet adoption rates have climbed significantly, with about one in five households having acquired a dog or cat since the outbreak of the COVID-19 pandemic (American Society for the Prevention of Cruelty to Animals [ASPCA] 2021). Pets also frequently appear in popular culture, mass media, and marketing communications. For example, Target uses a dog as its brand mascot, Microsoft features dogs in its 2020 holiday commercial to inspire people to find joy, and Wells Fargo uses a cat in its commercial to advertise its suspicious card activity alert services.Despite the significance of pets in people's lives and in mass media, popular culture, and marketing communications, scant research has examined how pets may influence consumers' judgments, decisions, and behaviors. Existing research on pet–human relationships largely revolves around examining how owning a pet influences the owner's pet-related judgments and behaviors. For example, attesting to a strong tie between owners and their pets ([ 2]; Cavanaugh, Leonard, and Scammon 2008), this stream of literature suggests that pets provide not only companionship but also a sense of safety and belongingness for their owners ([79]). Pet owners have significantly greater physical and psychological well-being than non-pet owners ([ 2]) and are more likely to endorse causes protecting animal rights (Kidd, Kidd, and Zasloff 1995).Research that examines how pets may influence consumer behavior beyond the immediate context of pet ownership is lacking, however. Such knowledge would provide novel and important insights to marketers and allow them to develop marketing strategies based on pet exposure situations. For example, marketers might choose to recommend more fitting products or services or craft appropriate communication messages to effectively target consumers depending on the type of pets to which they are exposed. Consider the following scenario: A newly opened massage center is pondering the language to use in direct mail to potential customers and whether to focus on how its therapies help reduce fatigue and stress or how its therapies promote metabolism and energy levels. Which strategy might be more effective if the company features a cat (dog) figure in its advertisement? This article provides a theory-based answer to this question.In our research, we focus on the effects of exposure to pets (e.g., recalling consumers' past interactions with dogs or cats, viewing ads featuring a dog or a cat) and examine how such experience influences consumers' judgments and decision making through the lens of regulatory focus theory. Drawing from literature on human–animal relationships, research on regulatory orientation, and work on mindset, we suggest that exposure to pets will remind consumers of the stereotypical personality traits, temperaments, and behaviors of the pets and thus will evoke different regulatory mindsets among consumers. Specifically, we predict and find that exposure to dogs fosters a more promotion-focused motivational mindset whereas exposure to cats activates a more prevention-focused motivational mindset. We further identify pet stereotypicality as a moderator for our findings, such that our results on the relationship between pet exposure and regulatory orientations persist only when consumers are reminded of the stereotypical temperaments and behaviors of the pet species, and that our main proposed effects dissipate when consumers are reminded of information inconsistent with the pet stereotypes. Moreover, we show that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Theoretical Framework Regulatory Orientation and Its Social OriginRegulatory focus theory suggests that consumer judgments, decisions, and behaviors are motivated by two regulatory orientations: promotion and prevention focus (Higgins 1997; Lee, Aaker, and Gardner 2000; Pham and Avnet 2004; Wang and Lee 2006). A promotion focus in self-regulation reflects consumers' motivations to attain growth and nurturance in an effort to align their actual selves with their ideal selves (achieving accomplishments and fulfilling aspirations; Higgins 1987, [28]). Promotion-focused consumers are characterized by an eagerness regulatory system during behavioral regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). For example, they are sensitive to the presence or absence of positive outcomes (gains and successes; Higgins 1997), concerned about reducing errors of omission (Croe and Higgins 1997), and more risk seeking when processing information and rendering decisions (Zhou and Pham 2004). By contrast, self-regulation with a prevention focus reflects consumers' motivations to attain safety and security in an attempt to bring their actual selves into alignment with their ""ought"" selves (fulfilling duties and obligations; Higgins 1987, [28]). Thus, prevention-focused consumers are more vigilant and cautious during behavior regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). In this system, consumers are sensitive to the presence or absence of negative outcomes (losses and failures; Higgins 1997), concerned about reducing errors of commission (Crowe and Higgins 1997), and more risk averse (Zhou and Pham 2004) when processing information and making decisions.More germane to our research is the finding that social influences play a pivotal role in shaping people's regulatory orientations. For example, interactions with childhood caretakers and parents' parenting style can influence the formulation of consumers' regulatory orientations during the socialization process (Crowe and Higgins 1997; Higgins 1996). Social exclusion causes a shift toward prevention motivation (Park and Baumeister 2015), whereas making choices for other people instigates a promotion focus ([59]). In addition, distinct social relationships can activate alternative regulatory orientations, such that reminders of friends activate a promotion focus while reminders of family members engender a prevention focus ([22]). Similarly, positive role models induce a promotion focus, whereas negative role models instigate a prevention focus (Lockwood, Jordan, and Kunda 2002).Given pets' prevalence in consumers' daily lives, we posit that consumers' interactions with pets are also an important part of socialization that can influence their regulatory orientations. These socialization activities can involve direct or indirect interactions with pets (e.g., observing pets' interactions with other people). Indeed, research on human–animal relationships evinces that pets play an important part in people's socialization process, influencing the development of various cognitive and social abilities (e.g., worldviews, empathy; [ 2]; Myers 1999; Purewal et al. 2017). Exposure to Pets and Regulatory OrientationsAs we have mentioned, dogs and cats are two primary types of pets ([ 4]). Despite within-species breed differences, research on animal behavior has identified systematic cross-species differences between domesticated dogs and cats ([11], [12]; Jardim-Messeder et al. 2017). This stream of research suggests that a promotion-oriented eagerness system better captures dogs' temperaments and behavioral characteristics, whereas a prevention-focused cautious system better describes cats' temperaments and behavioral characteristics. For example, on a temperament level, dogs tend to be open and expressive, while cats are elusive and cautious ([11], [12]; Potter and Mills 2015). Consistent with the promotion orientation's receptivity to change (Boldero and Higgins 2011; [43]), dogs (vs. cats) cope better with and adapt quicker to changes in the environment, such as moving into a new house or having a new person in the household ([11], [12]; Langenfeld 2020). In line with the prevention orientation's preference for the status quo ([10]; Chernev 2004), cats (vs. dogs) appear more concerned with the protection their owners provide and the consistency and stability of their social and physical surroundings ([11], [12]). Similarly, consistent with the eagerness prediction of a promotion regulatory system ([15]), dogs are more responsive to rewards (e.g., food, praise, petting) than cats and thus are easier to train ([49]).When interacting with human beings and other pets, dogs are more eager to please their owners and socialize with other dogs, whereas cats are more cautious, suspicious, boundary setting, and anxious when surrounded by unfamiliar people or other cats ([11], [12]; Potter and Mills 2015). Indeed, research has shown that dogs are more attentive and responsive to human's social cues (e.g., gestures) than cats ([49]; Wynne, Udell, and Lord 2008). Dogs' eagerness can be exemplified by the spike in oxytocin (a hormone mammals release when they feel love or affection for someone) when their owners are around ([53]). A study conducted by scientists at BBC shows that dogs produce five times more oxytocin than cats upon seeing their owners ([21]).We further predict that through repeated socialization episodes with pets (through either direct or indirect interaction with pets), the traits and motivational characteristics of dogs (cats) are gradually associated with a promotion-focused (prevention-focused) eagerness (cautiousness) system. These learned associations are brought to mind and thus accessible when consumers interact with pets or encounter stimuli featuring pets (e.g., ads) in their daily lives. To confirm the associations of dogs and cats with promotion and prevention orientations, we conducted a pilot study, which found that participants indeed associated promotion-focused words with dogs and prevention-focused words with cats (for details of the pilot study, see Web Appendix A). HypothesesDrawing from research on motivational mindset, which we review next, we further predict that exposure to dogs (cats) or stimuli featuring them (e.g., ads) will remind consumers of the temperaments and behaviors of the dogs (cats), which will in turn activate a promotion-focused (prevention-focused) mindset among consumers and guide their subsequent judgment and decision making. Mindset reflects ""the activation and use of a procedure that is stored in memory as part of declarative knowledge"" ([76], p. 110). That is, engaging in a particular operation when pursuing a goal in a prior task may give rise to a mindset (e.g., a promotion-focused mindset) that remains accessible in consumers' memory and, in turn, guides their pursuit of a different goal in a subsequent, unrelated context.A growing body of literature has found considerable evidence of the role of mindset across a wide range of information-processing activities, from comprehension, to judgment, to decision making (Ma and Roese 2014; Wyer and Xu 2010; Xu et al. 2020). In some situations, mindsets involve cognitive procedures induced by engaging in a prior task that spills over to influence a subsequent, unrelated context. For example, Xu et al. (2020) show that managers during election years are more likely to adopt a comparative mindset due to the omnipresence of comparative political advertisements. Accordingly, they spend more money on their managerial decisions because the comparative mindset accentuates ""which option to spend money on"" and forgoes the ""whether or not to spend"" consideration. More germane to our theorizing, mindsets may also be based on motivation ([76]), such that the motivational mindset induced by pursuing a goal in a prior task will guide consumers' subsequent behavior in an unrelated context (e.g., pursuit of a different goal). For example, Wyer and Xu (2010) assert that the promotion (prevention) regulatory mindset can be induced procedurally by, for example, making salient participants' desire to achieve their ideal (ought) self. When activated, the promotion (prevention) regulatory mindset produces a cross-domain effect, making consumers, for example, more likely to approach positive (avoid negative) consequences in their decision making.Drawing on this stream of literature, we posit that exposure to pets (e.g., recalling an interaction with a pet, viewing ads featuring a pet as the spokescharacter) in a prior task may render different regulatory mindsets salient. Specifically, because the stereotypical personality traits, temperaments, and behaviors of dogs (cats) brought to mind by the pet exposure are associated with eagerness (cautiousness) strategies commonly employed by a promotion (prevention) orientation, consumers' different regulatory orientations (promotion vs. prevention) will be activated. When evoked, these motivational regulatory mindsets will carry over to influence consumers' subsequent, unrelated judgments and decision making, rendering them more eager (cautious) during behavioral regulation, leading them to pursue promotion- (prevention-) focused goals such as growth and advancement (safety and stability), and making them more risk seeking (more risk averse) in decision making. Thus,H1: Pet exposure activates different regulatory motivational mindsets among consumers, such that (a) exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) activates a more promotion-focused mindset and (b) exposure to cats or cat-featuring (vs. dogs or dog-featuring) stimuli activates a more prevention-focused mindset.A key premise of our theorizing that exposure to pets activates different regulatory mindsets is that such exposure will remind consumers of the stereotypical temperaments and behavioral characteristics of dogs (cats), giving rise to a promotion-focused (prevention-focused) mindset. In other words, through repeated socialization, consumers have developed preestablished mental connections between dogs' (cats') typical temperaments and behaviors and the promotion (prevention) focus, and exposure to pets or pet-featuring stimuli can render these stereotypical associations accessible, thus activating the corresponding regulatory-focus mindset among consumers. Prior research has shown that established mental associations are likely to be temporarily weakened, nullified, or even reversed when presented with information inconsistent with the original associations. For example, Gorn, Jiang, and Johar (2008) reversed the association between baby-faceness and unintentionality by presenting counterassociation information about a baby-faced person intentionally harming others. Thus, if our reasoning that mental associations between dogs (cats) and promotion- (prevention-) focused mindsets is right, our proposed effects should persist to the extent consumers are reminded of the stereotypical behaviors and temperaments of the pet species; when consumers are reminded of pet information inconsistent with the stereotypes of the species (e.g., dogs [cats] unlike a stereotypical dog [cat]), we are likely to show that our results are attenuated. More formally,H2: Pet stereotypicality moderates the effect of pet exposure on regulatory mindsets, such that the effect dissipates when consumers are exposed to pets that are inconsistent with the stereotypes of the species.We expect that the impact of pet exposure on consumers' motivational mindsets will carry over to influence downstream variables, including ad evaluation, purchase intention, and real purchasing behavior, even in pet-unrelated consumption contexts. We anticipate that the effects of pet exposure on these variables will stem from the activation of a regulatory mindset and regulatory fit. Regulatory fit occurs when the regulatory strategies individuals employ during goal pursuit are compatible with their regulatory orientations (Higgins 2000; Hong and Lee 2007); it usually results in favorable effects on downstream consumer responses, such as enhanced value of the product ([ 7]), brand attitudes ([37]), self-regulation (Hong and Lee 2007), and decision making (Zhou and Pham 2004).Therefore, in accordance with this literature, we anticipate that consumers who are exposed to dogs or dog-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring promotion-focused claims. By contrast, consumers who are exposed to cats or cat-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring prevention-focused claims. Thus,H3: There is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product, such that (a) when exposed to ads with promotion-focused claims, exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) leads consumers to form more favorable product evaluations, and (b) when exposed to ads with prevention-focused claims, exposure to cats or cat-featuring stimuli (vs. dogs or dog-featuring stimuli) leads consumers to form more favorable product evaluations.H4: Regulatory fit mediates the interaction between pet exposure and ads' regulatory focus proposed in H3 on product evaluations. Overview of StudiesStudies 1 and 2 provide initial evidence for our prediction by showing that long-term exposure to dogs (cats) is associated with a promotion (prevention) focus. Specifically, using secondary data gathered from the American Veterinary Medical Association, Google Trends, and Centers for Disease Control and Prevention (CDC), Study 1 finds that people in states with a higher percentage of dog (cat) owners search more promotion- (prevention-) focused words (Study 1a) and show a higher COVID-19 transmission rate (Study 1b). Study 2 shows that dog (vs. cat) owners are more likely to invest in stocks and are less likely to invest in mutual funds in financial decision making. Studies 3a–3d establish the basic effect that exposure to dogs (cats) activates a promotion- (prevention-) focused motivational mindset by employing multiple experimental manipulations of pet exposure and different measures of regulatory orientation in both pet-related and pet-unrelated contexts, including incentive-compatible choices. Study 4 explores a moderating effect for our findings, showing that our hypothesized effects will dissipate when consumers are exposed to pet information inconsistent with the stereotypes of the pet species. Study 5 examines the downstream effects of pet exposure on consumers' incentive-compatible behaviors, showing that consumers exposed to dogs (cats) bid higher for products framed with a promotion (prevention) focus. Studies 6 and 7 provide additional support for our theorizing by examining the mediation effect of regulatory fit. Table 1 in Web Appendix B provides a summary of all studies.GraphTable 1. Study 1a: Results from the Multiple Regression Model. btpPet ownership indexa.362.62.012Median household income−.15−.63.534Per capita GDP−.25−1.26.214Political orientationb−.05−.29.774 1 a Higher scores indicate more dog (vs. cat) owners.2 b 1 = Democratic, 2 = Republican.3 Notes: Dependent variable = Regulatory-focus index: high scores indicate more promotion- (vs. prevention-) focused. Study 1: Pet Ownership and Regulatory Orientations—Evidence from Google Trends and COVID-19 C...Relying on secondary data and operationalizing pet exposure as pet ownership, Study 1 aims to provide preliminary evidence for our prediction that exposure to dogs and cats is associated with different regulatory mindsets. We collected aggregated state-level data on pet owner statistics, public interest in promotion- versus prevention-oriented behaviors, and per capita COVID-19 cases during the pandemic. We expect that at the state level, having a relatively higher dog-owning (cat-owning) population will be related to more search interests in promotion-oriented (prevention-oriented) behaviors in general (Study 1a) and more per capita COVID-19 cases during the pandemic (Study 1b). Study 1a: Pet Ownership and Search Interests in Regulatory BehaviorsFor pet ownership, we obtained the latest (2016) state-level pet ownership data set (n = 49) from the U.S. Pet Ownership and Demographics Sourcebook released by the [ 1]. This data set provides the most complete data on pet population demographics, covering the 48 U.S. continental states and the District of Columbia (excluding Alaska and Hawaii). For each state, we divided the percentage of dog-owning households by the percentage of cat-owning households to obtain a pet-owning index, with a higher score indicating more dog-owning (vs. cat-owning) households in the state.To obtain a proxy for citizens' public interest in regulatory-oriented behaviors in each state, we examined the search interest scores data from Google Trends (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017). Google is the most often-used internet search engine in the United States (accounting for 88% of the market share; Schultheiß and Lewandowski 2021), and Google Trends counts how often a particular search term is entered relative to the total search volume across various geographic regions. After a search term, period, and interested geographic area are entered, Google Trends displays how often that search term appears on Google in that geographic area and in that period relative to the total search volume on a standardized scale ranging from 0 (lowest search volume) to 100 (highest search volume). Given its viable role in monitoring public interests, Google Trends has become an increasingly used data source for research in psychology ([46]), political sciences (Mellon 2013; Weeks and Southwell 2010), and marketing (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017).To build the state-level regulatory orientation index, we first selected ten representative promotion-focused words (i.e., ""growth,"" ""gain,"" ""achievement,"" ""aspiration,"" ""pleasure,"" ""proud,"" ""hope,"" ""earn,"" ""win,"" and ""spontaneous"") and ten representative prevention-focused words (i.e., ""privacy,"" ""safety,"" ""loss,"" ""prevention,"" ""pain,"" ""stable,"" ""saving,"" ""frugal,"" ""rules,"" and ""risky""), in line with literature on regulatory focus (Higgins 1997, [29], [30]; Scholer, Cornwell, and Higgins 2019). We then obtained search interest scores of these words on Google Trends from January 1, 2016, to December 31, 2020, across the 48 continental states plus the District of Columbia. For each state, we calculated the average search interest score for the ten promotion-focused words (α = .85) and the ten prevention-focused words (α = .74). Finally, we built a regulatory orientation index for each state by dividing the promotion search interest score by the prevention search interest score (i.e., higher numbers indicate a higher promotion focus).To demonstrate the ecological validity of our findings, we also controlled for state-level microeconomic influence (income), macroeconomic influence (gross domestic product [GDP]), and political orientations. Specifically, we included the (state-level) covariates median household income in 2016 (U.S. Bureau of the Census 2017), per capita GDP in 2016 (U.S. Bureau of Economic Analysis 2019), and political orientation based on the 2016 presidential election results (The New York Times 2017).We conducted a multiple linear regression with the pet ownership index as the independent variable, regulatory-orientation index as the dependent variable, and median household income, per capita GDP, and political orientation as covariates. Table 1 shows the results. The results reveal that our regression model was significant (F( 4, 44) = 4.86, p = .002), suggesting that the independent variables significantly explained the variance in regulatory orientation. More importantly, after controlling for the covariates, the pet ownership index (b = .36, t = 2.62, p = .012) significantly predicted the regulatory-orientation index, showing that at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion-oriented (prevention-oriented) behaviors in general (for additional analyses, see Web Appendix C). Study 1b: Pet Ownership and Per Capita COVID-19 Cases During the PandemicStudy 1b uses the same pet-ownership data from Study 1a but focuses on per capita COVID-19 cases (CDC 2020) as a proxy for regulation-related behaviors. Considering the findings that promotion- (prevention-) focused people are more risk seeking (risk averse; Zhou and Pham 2004), we expect that dog owners will have an increased probability to engage in promotion-focused, relatively risky behaviors that may result in COVID-19 transmission (e.g., more willing to dine in restaurants, letting their guard down when following social distancing); by contrast, cat owners will have an increased probability to be more cautious and engage in less risky, prevention behaviors (e.g., behaving extra cautiously, practicing social distancing, wearing face masks). Accordingly, we predict that states with more dog (cat) owners will report a higher (lower) number of per capita COVID-19 cases.To obtain a state-level proxy for regulatory-oriented behavior, we examined each state's COVID-19 cases per 100,000 people reported to the CDC from January 21, 2020 (the earliest available date) to November 1, 2020 (the date Study 1b was conducted). As of November 1, 2020, the 48 continental states and the District of Columbia had reported 2,819 COVID-19 cases per 100,000 people, on average, with Vermont being the lowest (348 per 100,000) and North Dakota the highest ( 6,054 per 100,000).We performed a linear regression on COVID-19 cases (per 100,000), with the pet ownership index as the independent variable, and controlled for the same covariates as in Study 1a. The results reveal that our regression model was significant (F( 4, 44) = 8.93, p <.001), suggesting that the independent variables significantly explained the variance in COVID-19 cases. As Table 2 shows, the pet ownership index was significantly related to an increase in COVID-19 cases per 100,000 people (b = .38, t = 3.15, p = .003), suggesting that, at the state level, a relatively higher dog-owning (cat-owning) population was associated with more reported per capita COVID-19 cases. Consistent with this finding, our ancillary analyses (for detailed analyses and results, see Web Appendix D) also suggest that the pet ownership index (higher scores indicating more dog [vs. cat] owners) significantly increased search interests (per Google Trends during the same period as the COVID-19 data) in promotion-focused behaviors, such as dining in, but significantly reduced search interest in prevention-focused behaviors, such as face mask and social distancing.GraphTable 2. Study 1b: Results from the Multiple Regression Model. btpPet ownership indexa.383.15.003Median household income.09.56.579Per capita GDP.13.97.336Political orientationb.533.71.001 4 a Higher scores indicate more dog (vs. cat) owners.5 b 1 = Democratic, 2 = Republican.6 Notes: Dependent variable = number of COVID-19 cases per 100,000 people. DiscussionUsing aggregated state-level data across different data sources, Study 1 provides support for our prediction of a significant association between long-term pet exposure and people's regulatory orientations, such that dog (cat) exposure is associated with a promotion (prevention) focus. Specifically, we find that, at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion- (prevention-) focused behaviors in general (Study 1a) and more reported per capita COVID-19 cases (Study 1b). In subsequent studies, we use individual-level data to provide additional support for our prediction. Study 2: Pet Ownership and the Choice Between Stocks and Mutual FundsStudy 2 also operationalizes pet exposure as pet ownership and examines whether consumers' pet-owning situations are associated with different regulatory mindsets. Unlike Study 1, which used aggregate, state-level data, Study 2 relies on individual-level pet ownership data. In addition, we used an established measure of regulatory orientation ([65]), which involved participants in a financial decision-making task choosing between two investment options: stock (a proxy for promotion focus) and mutual fund (a proxy for prevention focus).We recruited 145 pet owners from Amazon Mechanical Turk (MTurk) (Mage = 35.3 years; 53.1% female; 53% dog owners and 47% cat owners). We recruited only participants who own dogs only or cats only; owners of both dogs and cats were excluded (for the screening criteria, see Web Appendix E). We asked participants to partake in a financial decision-making task, which served as our measure of regulatory orientation (Zhou and Pham 2004). We first gave them basic definitions of stocks and mutual funds and told them that stock investments were typically associated with a higher level of risk, whereas mutual fund investments were typically associated with a lower level of risk and therefore more conservative. Next, we asked participants to imagine that they had $2,000 and were considering investing in two assets: a stock and a mutual fund. Afterward, we asked them to indicate which asset they would invest in if they could choose only one asset and then to indicate the amount of money they would invest. Given that prior research has shown that the activation of a promotion- (vs. prevention-) focused mindset entails greater risk taking (vs. risk aversion; Zhou and Pham 2004), we expected that dog (vs. cat) owners would be more willing to take risks in their financial investments and choose the stock option. After the financial decision-making task, participants completed measures of their mood using PANAS (Positive Affect Negative Affect Schedule; Watson, Clark, and Tellegen 1988) and a few demographic measures, including their age, gender, ethnicity, and income level (Web Appendix F presents the measures used). ResultsA logistic regression showed a significant effect of pet ownership (cat owners = 0, dog owners = 1) on investment choice, such that dog owners (36.4%) were more likely to choose to invest in the riskier stock option than cat owners (20.6%; b = .79, SE = .38, χ2 = 4.73, p = .039, Exp (B) = 2.20). Similarly, a one-way analysis of variance (ANOVA) revealed a significant effect of pet ownership on money allocations. As we expected, dog owners allocated more money to the stock option (Mdog = $796.10, SD = $524.45) than cat owners (Mcat = $603.69, SD = $474.90; F( 1, 143) = 5.31, p = .023,  ηp2   = .04).To rule out possible alternative explanations that participants' mood, gender, age, ethnicity, or income level accounted for our findings, we controlled for these variables simultaneously. Our results for both choice (χ2 = 5.47, p = .019) and money allocations (F = 5.74, p = .018) remained significant even after we controlled for these covariates. DiscussionStudy 2's findings show that dog (vs. cat) owners were more likely to take risks in their financial decisions, showing more preference for stock investment. Importantly, incorporating the demographic variables age, ethnicity, gender, and income as covariates did not change the results. Taken together, using pet ownership as an operationalization, Studies 1 and 2 provide initial support for our prediction that exposure to pets is associated with different regulatory mindsets. However, despite the extra steps taken, such as controlling for demographic variables (e.g., income) to rule out alternative explanations, Studies 1 and 2 were correlational in nature. To provide stronger causal evidence for our prediction, in the subsequent studies, we manipulate exposure to pets in various ways. Study 3: Effects of Pet Exposure on Consumers' Regulatory OrientationsThe purpose of Study 3 is twofold. First, the study aims to establish causality between pet exposure and the formation of regulatory motivation mindsets by using multiple manipulations of pet exposure. Second, the study operationalizes regulatory orientations in various ways and across different (pet-related and pet-unrelated) contexts.In a pet-related domain, Study 3a shows that participants exposed to dogs (vs. cats) will be more likely to prefer a pet toothpaste ad with promotion- (vs. prevention-) focused benefits. Studies 3b–3d test the effect in pet-unrelated domains. Consistent with prior research showing that the activation of a promotion- (prevention) focused mindset entails greater risk taking (risk aversion) (Zhou and Pham 2004), participants who are exposed to dogs (vs. cats) will be more willing to take risks to participate in a lottery (an incentive-compatible behavior; Study 3b) and in their financial investment decisions (Study 3c). In a health product context, Study 3d demonstrates that participants who are exposed to dogs (vs. cats) will be more likely to prefer a vitamin product with promotion- (vs. prevention-) focused benefits.In this and subsequent studies, participants completed mood measures and demographic measures, including pet ownership, gender, age, income level, and ethnicity, at the end of study. Incorporating these variables as covariates did not influence our results (for the exact measures used, see Web Appendix F; for results pertaining to the impact of pet ownership across studies, see Web Appendix G), and thus we do not discuss them further. Study 3aIn Study 3a, we examine our prediction in the context of pet-related decisions: pet toothpaste choice. One hundred eighty-three participants recruited from MTurk completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, for a small financial compensation (Mage = 37.4 years; 54.6% female). To manipulate pet exposure (dog vs. cat), under the cover story that we wanted to examine consumers' day-to-day experiences, participants were asked to recall and write down a past experience in which they interacted with a dog or cat (for details of the recall instructions, see Web Appendix H).Afterward, participants were told that a pet toothpaste brand was testing advertisements for its new product and needed their opinions on two ad versions (adapted from Wang and Lee [2006]). Corresponding to their assigned pet exposure condition, participants in the dog (cat) condition viewed dog (cat) toothpaste ads. Ad A, which featured a promotion-focused claim, read, ""Our product helps your dog [cat] freshen breath and strengthen tooth enamel!"" Ad B, which emphasized the prevention-focused benefits of the product, read, ""Our product helps your dog [cat] prevent gingivitis and fight plaque buildup!"" A separate pretest confirmed that Ad A (B) was indeed perceived as more promotion (prevention) focused (Web Appendix H).After viewing the two ads, participants indicated their preference for one of the two ads on three seven-point scales (1 = ""definitely/for sure/certainly Ad A,"" and 7 = ""definitely/for sure/certainly Ad B""). We created a preference index by averaging participants' responses to the three items (α = .99), with higher scores indicating a preference for Ad B, the prevention-focused version.As we expected, a one-way ANOVA revealed a significant effect of exposure to pets on ad preference. Specifically, participants in the dog condition indicated a stronger preference for the promotion-focused ad (Mdog = 4.10, SD = 2.06) than those in the cat condition (Mcat = 4.85, SD = 1.83; F( 1, 181) = 6.78, p = .010,  ηp2   = .04). Study 3bStudy 3b aims to examine our prediction using an incentive-compatible behavior in a pet-unrelated domain. One hundred eighty MTurk workers completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, in exchange for a small financial compensation (Mage = 39.5 years; 60% female). We told participants that the study was about people's general knowledge about pets and their past experiences with pets. We randomly assigned them to one of the two conditions (dog vs. cat). Participants first answered five quiz questions about dogs (cats; see Web Appendix I) and then recalled a past experience interacting with a dog (cat) and wrote it down (following the same instructions as in Study 3a).We next told participants that they could participate in a lottery and explained the options they had as follows. If they chose not to participate in the lottery, they would still get paid the initial amount ($.40) as described in the study, so there was nothing to lose. If they chose to participate in the lottery, they had a 50% chance to receive a bonus ($.20) in addition to the base pay; however, they also had a 50% chance to lose half the base pay ($.20). A separate pretest confirmed that the lottery participation (nonparticipation) option was indeed perceived as more promotion- (prevention-) focused (Web Appendix I).Because the promotion (prevention) focus prompts people to focus more on gains (losses) and thus be more risk seeking and open to change (risk averse and status quo oriented) (Liberman et al. 1999; Zhou and Pham 2004), we expected participants who were exposed to dogs to be more likely to participate in the lottery than their counterparts who were exposed to cats. A logistic regression showed a significant effect of pet exposure (cat = 0, dog = 1) on lottery participation, such that participants in the dog condition showed a higher likelihood to take part in the lottery (63.4%) than participants in the cat condition (44.8%; b = .76, SE = .31, χ2 = 6.20, p = .013, Exp (B) = 2.14). Study 3cTwo hundred twenty-five MTurk workers completed Study 3c in exchange for a small financial compensation (Mage = 38 years; 49% female). The study featured the same two-cell (pet exposure: dog vs. cat) between-subjects design and manipulated pet exposure by asking participants to view a series of four print ads, one per screen, that featured either dogs or cats as the spokescharacter (see Web Appendix J) and to provide their thoughts and feelings after viewing the ads. We then measured participants' regulatory orientation using the same financial decision-making task ([65]) as in Study 2.A logistic regression showed a marginally significant effect of exposure to pets (cat = 0, dog = 1) on investment choice, such that participants in the dog condition were more likely to choose to invest in the riskier stock option (29.8%) than participants in the cat condition (18.9%; b = .60, SE = .32, χ2 = 3.57, p = .059, Exp(B) = 1.82). Similarly, a one-way ANOVA revealed a significant effect of exposure to pets on money allocation. Participants in the dog condition allocated more money to the stock option (Mdog = $790.35, SD = 514) than those in the cat condition (Mcat = $613.06, SD = 429; F( 1, 223) = 7.86, p = .005,  ηp2   = .03). Study 3dOne hundred fifty-seven MTurk workers completed Study 3d in exchange for a small financial compensation (Mage = 42 years; 61% female). Study 3d employed the same two-cell (pet exposure: dog vs. cat) between-subjects design. To manipulate pet exposure, participants watched a short video featuring either dogs or cats. Both videos had the same theme—pets ""shopping"" around in a store (see Web Appendix K).After participants watched the video, we presented them with a choice scenario. Specifically, we asked them to imagine that they were buying vitamins and that two brands were available (Zhou and Pham 2004). Brand A was rich in vitamin C and iron and could promote high energy. Brand B was rich in antioxidants and could reduce the risk of cancer and heart disease. A separate pretest confirmed that Brand A (Brand B) was perceived as more promotion- (prevention-) focused (see Web Appendix K).After viewing the two brands, participants then indicated their preference for one of the two brands on three seven-point scales (1 = ""definitely/certainly/for sure Brand A,"" and 7 = ""definitely/certainly/for sure Brand B""). We created a preference index by averaging and reverse coding participants' responses to the three items (α = .99; a higher rating indicating a stronger preference for Brand A, the promotion-focused brand).A one-way ANOVA revealed a significant effect of exposure to pets on brand preference. As expected, participants in the dog condition indicated a stronger preference for the promotion-focused brand (Mdog = 3.96, SD = 2.34) than those in the cat condition (Mcat = 3.19, SD = 2.02; F( 1, 155) = 4.93, p = .028,  ηp2   = .03). DiscussionUsing a variety of methods to manipulate exposure to pets (i.e., pet knowledge, viewing print ads featuring pets, watching a short pet video, and recalling the experience of interacting with a pet), Studies 3a–3d provide converging support for H1 and show that exposure to dogs can lead to behaviors consistent with a promotion-focused mindset, whereas exposure to cats can prompt behavior patterns more aligned with a prevention-focused mindset. Specifically, in Study 3a, consumers preferred the ad with promotion-focused (prevention-focused) benefits for a dog (cat) toothpaste product. In Studies 3b–3d, we extended this finding to pet-unrelated domains. In Studies 3b and 3c, consumers exposed to dogs (vs. cats) were more willing to take risks in their decisions. In Study 3d, exposure to dogs (vs. cats) prompted consumers to prefer a vitamin brand with promotion-focused benefits. These findings provide converging support for our basic prediction that in both pet-related and pet-unrelated contexts, exposure to dogs can activate more of a promotion-focused mindset, whereas exposure to cats can activate more of a prevention-focused mindset.Importantly, in Studies 3a–3d, we found no systematic differences between the dog- and cat-exposure conditions in terms of mood, age, gender, ethnicity, income, and pet ownership. In addition, including these variables as control variables does not change our results anyway; thus, we do not discuss these variables further. Although some stimuli used in these studies may not be completely balanced (e.g., the pet pictures used in Study 3c may differ on certain dimensions, and the energy-boosting benefits of the vitamin seem less consequential than the cancer-risk-reducing benefits of the product in Study 3d), these studies taken together show convergent evidence for our main hypothesis and suggest that the proposed effect is robust across different contexts. Study 4: Moderating Role of Pet StereotypicalityThe primary purpose of Study 4 is to examine the moderating effect of pet stereotypicality on the activation of regulatory-focus mindsets (H2). We predict that making nonstereotypical information (i.e., pets that do not possess the stereotypical characteristics of their species) available to consumers will attenuate the effect of pet exposure on regulatory-focus mindsets. MethodThree hundred eighty MTurk participants (57% female; Mage = 40.3 years) completed the study for a small monetary compensation. Study 4 featured a 2 (exposure to pets: dog vs. cat) × 2 (pet stereotypicality: stereotypical vs. nonstereotypical) between-subjects factorial design.We used a recall task similar to Study 3a. Specifically, we told participants that we were interested in consumers' experience with a pet. Participants then read that some dogs (cats) possess stereotypical characteristics of a dog (cat) and some of them do not. To manipulate pet stereotypicality, in the stereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that reminds them of the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior like a stereotypical dog [cat]). In the nonstereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that does not have the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior unlike a stereotypical dog [cat]). (See Web Appendix L for the detailed manipulation).After the recall task, participants completed the financial decision scenario used in Study 2 and Study 3c. Specifically, participants imagined they had $2,000 and considered investing in two assets: a stock and a mutual fund. They indicated their preference between the two options on a nine-point scale (1 = ""stock,"" and 9 = ""mutual fund""; reverse-coded with a higher rating indicating a stronger preference for the promotion-focused option [i.e., stock]) and then indicated the amount of money they would invest. Results PreferenceA 2 × 2 ANOVA revealed a significant two-way interaction of pet exposure with pet stereotypicality on investment preference (F( 1, 376) = 8.11, p = .005,  ηp2   = .021). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were replicated. That is, participants in the dog condition demonstrated higher preference for the stock (Mdog = 4.21, SD = 2.69) than those in the cat condition (Mcat = 3.06, SD = 2.14; F( 1, 376) = 11.72, p <.001,  ηp2   = .03). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that participant did not show preferences for the stock (Mdog = 3.40, SD = 2.47; Mcat = 3.66, SD = 2.36; F( 1, 376) = .53, p = .466). Money allocation to the stockA 2 × 2 ANOVA on money allocation to the stock also revealed a significant two-way interaction (F( 1, 376) = 4.72, p = .031,  ηp2   = .012). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were again replicated such that participants in the dog condition allocated more money to the stock option (Mdog = $765.82, SD = 523.89) than those in the cat condition (Mcat = $623.39, SD = 466.25; F( 1, 376) = 4.20, p = .041,  ηp2   = .011). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that the amount of money allocated to the stock option was not statistically different between the dog and cat conditions (Mdog = $606.80, SD = $477.79; Mcat = $688.04, SD = $527.19; F( 1, 376) = 1.14, p = .286). Thus, the results support H2. DiscussionProviding support for our theorizing that associations triggered by pet exposure evoke different regulatory motivational mindsets, Study 4 shows that information related to pet stereotypicality moderates the effect of pet exposure on the activation of regulatory-focus mindsets. Specifically, Study 4 demonstrates that exposing participants to pet information consistent with their stereotype replicated the findings in the previous studies; by contrast, exposure to pets inconsistent with their stereotype nullified the effect of pet exposure on the activation of regulatory-focus mindsets.Having established the basic effect of pet exposure on consumers' regulatory motivational mindsets, in subsequent studies we aim to further examine the downstream effects of pet exposure on consumer behavior, including product evaluations, purchase intentions, and real incentive-compatible behaviors. Specifically, as we predict in H3, which is based on the regulatory fit between pet exposure and ad frames, because exposure to dogs (cats) activates a promotion (prevention) regulatory mindset among consumers, they should form more favorable evaluations and show more purchase intentions of products framed with promotion-focused (prevention-focused) benefits. Study 5: Pet Exposure, Product Feature Frames, and Bidding BehaviorStudy 5 aims to provide evidence for H3, which predicts that there is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product by using incentive-compatible behaviors. The study also uses a different ad to further augment the robustness of our findings. Two hundred eighty-three undergraduate students from a large midwestern U.S. university participated in the study for partial course credit (45.9% female; Mage = 20.5 years). Study 5 employed a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subject factorial design.Similar to the previous studies, to manipulate pet exposure, under the cover story that we wanted to understand consumers' day-to-day experiences, we first asked participants to recall a past experience in which they interacted with a dog or a cat and to write it down. We then told participants that they would read a message from a local massage center. We varied the message to accentuate either a promotion or a prevention focus (see Web Appendix M). The promotion-focused message emphasized that massages performed by therapists help people increase metabolism, boost immunity, and build a rejuvenated body. The prevention-focused message indicated that massages performed by therapists help soothe body aches, relieve tensions, and reduce stress from school and work. We conducted a separate pretest to confirm the success of our regulatory focus manipulation (see Web Appendix M).Next, we told participants that the local massage center would offer $50 gift cards to several survey participants. They were asked to bid on the gift cards and were told that the top bidders would be contacted later and offered the gift cards at the bidding price (though later the top bidders received the gift cards for free). Participants were then instructed to write down the dollar amount they were willing to bid on a $50 gift card.A 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 279) = 8.91, p = .003,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog = $20.31, SD = $14.57) than those in the cat condition (Mcat = $14.98, SD = $13.20; F( 1, 279) = 4.77, p = .030,  ηp2   = .017). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $20.51, SD = $15.35) than those in the dog condition (Mdog = $15.67, SD = $13.68; F( 1, 279) = 4.14, p = .043,  ηp2   = .02). DiscussionUsing a behavioral study with an incentive-compatible measure, Study 5 confirmed the robustness of our findings that exposure to pets activates different regulatory mindsets among consumers. After viewing the promotion-focused version of an ad promoting a local massage center, participants who recalled exposure to a dog placed higher bids on the gift card; by contrast, after viewing the prevention-focused version, participants who recalled exposure to a cat placed higher bids. These results provide support for H3. Study 6: Pet Exposure and Product Frames Induce Regulatory Fit and PersuasionThe goal of Study 6 is threefold. First, Study 6 aims to augment robustness for H3 by conceptually replicating the findings of Study 5 using a different context (bidding for a product). Second, Study 6 aims to test H4, which predicts that regulatory fit will mediate the interaction of exposure to pets with ads' regulatory focus on consumer behavior. Third, it uses a new method to manipulate exposure to pets, such that dogs (cats) are directly incorporated into the stimuli as an integral part of the ad message. MethodTwo hundred sixty-four undergraduate students from a large southeastern U.S. university participated in the study in exchange for course credit (Mage = 20.2 years; 52% female). Study 6 featured a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subjects factorial design.The experimental procedure was similar to Study 5 except that a new ad with a new product (sneaker) was employed and that pets (dogs or cats) were referenced in the ad. We told participants that they would review a message from a sneaker brand. Dependent on the assigned condition, participants next viewed one of the four versions of the ad (adapted from [22]]). The promotion-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with eagerness. Our sneakers feature H-Ergy synthetic material, which improves breathability of the shoes and promotes strong support for your feet."" The prevention-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with caution. Our sneakers feature N-Ergy synthetic material, which is anti-skid and reduces the possibility of foot pain."" We conducted a separate pretest to confirm the success of our regulatory focus manipulation and the believability of the stimuli.Two hundred sixty undergraduate students (Mage = 20.3 years; 56% female) were randomly assigned to one of the four conditions. Participants first indicated the extent to which the advertised sneakers had benefits that could help people attain something positive and the extent to which the advertised sneakers had benefits that could help people avoid something negative (adapted from Mogilner, Aaker, and Pennington [2008]; 1 = ""strongly disagree,"" and 9 = ""strongly agree""). Participants then rated the extent to which the message was reasonable/appropriate/believable as an ad (1 = ""strongly disagree,"" and 9 = ""strongly agree""; α = .91; averaged to form a believability index). The ANOVA on manipulation check measures revealed only main effects, such that the promotion-focused message (Mpromotion = 7.53, SD = 1.19) was deemed as having benefits that helped people attain something positive to a greater extent than the prevention-focused message (Mprevention = 6.71, SD = 1.51; F( 1, 256) = 23.76, p <.001,  ηp2   = .09), and that the prevention-focused message (Mprevention = 6.46, SD = 1.93) was perceived as having benefits that helped people avoid something negative to a greater extent than the promotion-focused message (Mpromotion = 5.56, SD = 2.51; F( 1, 256) = 10.58, p = .001,  ηp2   = .04). A one-sample t-test on the believability index revealed a significant difference against the mid-point of the scale (5; M = 5.40; t(259) = 3.10, p = .002; d = .19) such that participants perceived the ad message they viewed as believable. There was no significant difference on the believability index across conditions (F =��.03, n.s.).Next, after revealing that the suggested retail price for the sneakers was $50, we asked participants to bid on the advertised sneakers and told them that the top bidders would be offered the sneakers based on the price they bid. After entering the bidding amount, participants then responded to regulatory fit measures (Lee and Aaker 2004) on two nine-point scales (""It was easy to process the message"" and ""It was difficult to understand the message (reverse coded)""; r = .84) ResultsA 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 260) = 8.38, p = .004,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog= $33.74, SD = $11.81) than those in the cat condition (Mcat = $28.23, SD = $15.61; F( 1, 260) = 5.49, p = .020,  ηp2   = .02). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $32.45, SD = $13.61) than those in the dog condition (Mdog = $28.40, SD = $12.66; F( 1, 260) = 3.05, p = .082,  ηp2   = .012).A 2 × 2 ANOVA on regulatory fit revealed only a significant interaction between exposure to pets and regulatory focus (F( 1, 260) = 9.80, p = .002,  ηp2   = .036). Specifically, planned contrasts showed that for the promotion-focused ad, the dog version elicited higher regulatory fit (Mdog = 5.21, SD = 2.53) than the cat version (Mcat = 4.19, SD = 2.37; F( 1, 260) = 5.37, p = .021,  ηp2   = .02). By contrast, for the prevention-focused ad, the cat version elicited higher regulatory fit (Mcat = 5.15, SD = 2.40) than the dog version (Mdog = 4.24, SD = 2.62; F( 1, 260) = 4.45, p = .036,  ηp2   = .017).To test the mediation prediction in H4, we conducted a moderated mediation analysis using 5,000 bootstrapped samples (PROCESS Model 8; [24]), with exposure to pets as the independent variable, regulatory fit as the mediator, regulatory focus as the moderator, and bidding amount as the dependent variable. The index of moderated mediation was significant (b = 1.68, 95% confidence interval [CI]: [.30, 3.70]). Specifically, for the promotion-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was positive and significant (b = .89, 95% CI: [.05, 2.11]). By contrast, for the prevention-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was negative and significant (b = −.80, 95% CI: [−2.05, −.003]). Thus, the data support H4. DiscussionUsing a new method (featuring a dog/cat as an integral part of the ad) to manipulate exposure to pets, we provide further evidence for our theorizing of an interactive effect between exposure to pets and regulatory focus of an ad on consumer responses. For sneaker ads featuring promotion-focused (prevention-focused) claims, consumers exposed to dogs (cats) formed higher bidding amount than those exposed to cats (dogs). Importantly, the findings of Study 6 also provide support for H4, such that the influence of exposure to pets on bidding amount was mediated by the regulatory fit between the activated regulatory mindset and the regulatory focus of the ad claim. We also conducted an additional study to conceptually replicate this study in the financial decision-making context (for details, see Web Appendix P). Study 7: Pet Exposure and Product Frames Induce Regulatory Fit and ChoiceStudy 7 has two objectives. First, it aims to lend additional support to H3 and H4 using a within-subject design. Second, to augment the robustness of our findings, we employ a different product category (toothpaste) and a new manipulation of pet exposure (pet pictures). MethodTwo hundred thirty-seven undergraduate students from a large southeastern U.S. university completed the study in exchange for partial course credit (Mage = 20 years; 45% female). The study featured a 2 (pet exposure: dog vs. cat) × 2 (products' regulatory focus: promotion- vs. prevention-focused) mixed ANOVA design, with pet exposure a between-subjects factor and products' regulatory focus a within-subjects factor.Participants were randomly assigned to one of two conditions (pet exposure: dog vs. cat). We told participants that an online calendar company was interested in people's feedback on several dog (cat) pictures that it planned to incorporate into a dog- (cat-) themed calendar. Participants then were shown a series of dog (cat) pictures, one on each screen (order counterbalanced). Afterward, participants were again shown all the pictures they had seen on one screen and were instructed to pick one of the pets to imagine interacting with. (For the stimuli used, see Web Appendix N.)Next, in an ostensibly different task, we presented participants with the descriptions of two toothpaste products (Wang and Lee 2006): Toothpaste A had strong promotion but weak prevention product claims, and Toothpaste B had strong prevention but weak promotion product claims (see Web Appendix O). We counterbalanced the order of the two toothpaste products across all participants. We then asked participants to evaluate each of the two products on four nine-point scales (1 = ""dislike very much/very unfavorable/very unattractive/very bad,"" and 9 = ""like very much/very favorable/very attractive/very good"").Afterward, we presented participants with all the strong feature claims and asked them to evaluate each of the features on a nine-point scale (1 = ""not at all attractive,"" and 9 = ""very attractive""). The features were evaluated as generic features of the product category, rather than as the features of a specific brand, and served as measures of regulatory fit (Wang and Lee 2006). That is, if participants relied more on (promotion or prevention) features that fit their regulatory orientations in their evaluation, they should find strong feature claims consistent with their regulatory orientations more attractive than claims that do not fit their orientations. ResultsWe averaged participants' evaluations of the toothpaste products on the four items to form a brand attitude index for each product (αA = .94, αB = .95). We expected that participants assigned to the dog exposure condition would evaluate Toothpaste A (with the strong promotion claims) more favorably than those assigned to the cat exposure condition. By contrast, participants assigned to the cat exposure condition would evaluate Toothpaste B (with the strong prevention claims) more favorably than those assigned to the dog exposure condition. A mixed ANOVA with toothpaste attitudes as the within-subject variable and pet exposure as the between-subjects variable first revealed a significant main effect of toothpaste attitudes (F( 1, 235) = 4.47, p = .036,  ηp2   = .02), such that, overall, participants evaluated Toothpaste A (with the strong promotion claims; M = 6.94) more positively than Toothpaste B (with the strong prevention claims; M = 6.61). Importantly, the predicted interaction also emerged (F( 1, 235) = 29.76, p <.001,  ηp2   = .11). We found that participants who were exposed to dogs evaluated Toothpaste A more positively (M = 7.41, SD = 1.25) than Toothpaste B (M = 6.29, SD = 1.74, t(235) = 5.74, p <.001; d = .52). By contrast, participants who were exposed to cats formed more positive attitudes toward Toothpaste B (M = 6.94, SD = 1.63) than Toothpaste A (M = 6.45, SD = 1.73, t(235) = −2.21, p = .029; d = –.21).We then analyzed participants' feature attractiveness ratings, which served as our regulatory fit measure. Because both feature type and toothpaste were measured within subject, we calculated a relative feature attractiveness index by dividing participants' attractiveness ratings of promotion features by their ratings of prevention features. With the index as the dependent variable, a one-way ANOVA revealed that participants in the dog exposure condition (Mdog = 1.27, SD = .83) perceived the promotion features as more attractive than participants in the cat exposure condition (Mcat = 1.09, SD = .35; F( 1, 235) = 4.66, p = .032,  ηp2   = .02).To further investigate the mediating role of regulatory fit, we conducted mediation analyses to examine whether perceived attractiveness of the product features mediated the effect of pet exposure on product evaluation. Given the within-subjects design, we used MEMORE macro developed by Montoya and Hayes (2017). With 5,000 bootstrapped samples, the analysis revealed a significant indirect effect of feature attractiveness (b = .41, SE = .09, 95% CI: [.23,.59]), indicating that regulatory fit mediated the effect of pet exposure on product evaluation. DiscussionUsing a within-subject design and a new product category for the dependent variable, Study 7 conceptually replicates the previous studies. Moreover, it provides additional support for H3 in that exposure to pets interacted with an ad's regulatory focus to influence consumer responses and for H4 in that such an effect is driven by regulatory fit. General DiscussionAcross 11 studies, employing different methods (secondary data, lab experiments, and real behavior), different operationalizations of pet exposure (pet ownership; viewing pet-featuring pictures, videos, or ads; and recalled experience with pets), and different measures and contexts of regulatory focus (e.g., financial, service, consumer products), we find converging evidence that pet exposure influences consumer judgments and behaviors through regulatory focus and regulatory fit. Specifically, we show that pet exposure fosters divergent regulatory orientations among consumers, such that exposure to dogs activates a promotion-focused motivational mindset while exposure to cats activates a prevention-focused motivational mindset (Studies 1–3). In Study 4, we further show that this effect is moderated by pet stereotypicality, such that the effects of pet exposure on regulatory mindsets dissipate when consumers are reminded of a pet that is inconsistent with the stereotypes of the species. These regulatory mindsets, when activated by pet exposure, carry over to influence downstream pet-unrelated consumer judgments, purchase decisions, and behaviors through regulatory fit (Studies 5–7), even in pet-unrelated consumption contexts. Theoretical ContributionsOur research contributes to the literature in several ways. First, it recognizes pets as an important source of social influence on consumers' judgments and behaviors (Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008). Prior research on marketplace social influence has examined the contexts of consumer-to-consumer and marketer-to-consumer interactions (e.g., Argo, White, and Dahl 2006; Chan and Sengupta 2010; Duclos, Wan, and Jiang 2013; Lee and Shrum 2012; Mead et al. 2011; White and Argo 2011; White and Dahl 2006, [75]). As examples of consumer-to-consumer interactions, Duclos, Wan, and Jiang (2013) show that social exclusion prompts riskier financial decisions because interpersonal rejection heightens the instrumentality of money to obtain benefits in life; Lee and Shrum (2012) find that social exclusion can lead to conspicuous consumption or prosocial behavior. As an example of marketer-to-consumer interactions, Chan and Sengupta (2010) find that consumers who receive insincere flattery from marketers still form favorable implicit evaluations of the marketer, but their explicit evaluations of the marketer are negative. Beyond these human-to-human contexts, our research suggests that the context of pet–human interactions as a source of social influence can similarly affect consumers' motivations, judgments, and behaviors.Second, our research diverges from extant literature on human–animal relationships by going beyond the immediate context of pet ownership and investigating how pet exposure affects the way people render subsequent pet-unrelated judgments and decisions. Prior research on the effects of human–animal relationships has mainly focused on the immediate context of pets and their owners, such as pet owners' health and psychological well-being ([ 2]), their awareness and protection of animal rights (Kidd, Kidd, and Zasloff 1995), and their relationship satisfaction with pets (Cavanaugh, Leonard, and Scammon 2008). Going beyond this immediate context between pets and their owners, we posit and find that exposure to pets or pet-featuring stimuli fosters divergent regulatory orientations, which in turn influence downstream pet-unrelated judgments and decisions in the consumption domain. We hope that this research will stimulate further research to gain a more nuanced understanding of the impact of human–animal relationships.Third, prior research identifies several antecedent variables of regulatory focus, such as one's cultural background (Lee, Aaker, and Gardner 2000), impulse purchase ([65]), and choosing for oneself versus others ([59]). Most of these factors, however, are intrapersonal. Only recently have researchers begun exploring the interpersonal drivers of regulatory focus (e.g., social exclusion [Park and Baumeister 2015], reminders of friends vs. family members [Fei, You, and Yang 2020]). Contributing to this stream of research, we uncover a novel social source of regulatory orientation: exposure to pets. We find that dog exposure is associated with a promotion orientation, whereas cat exposure is associated with a prevention orientation. Additional research is necessary to further explore the social and interpersonal impact on people's regulatory orientation. Practical ImplicationsOur findings also offer novel implications to marketers. First, marketers should consider crafting their advertising messages differently or recommending different products and services when they target consumers depending on their pet exposure situations. For example, to enhance the effectiveness of their advertising appeals or communication messages, marketers should emphasize promotion-focused goals such as gains and nongains if they are targeting dog owners or after consumers are exposed to dogs or dog-featuring stimuli (e.g., after just watching an ad about dogs). Conversely, they should focus on prevention-focused goals such as losses and nonlosses if they are pursuing cat owners or after consumers who are exposed to cats or cat-featuring stimuli. Importantly, our findings show that this advice holds even when the advertised product or service has nothing to do with pets or pet products.Second, our findings offer important insights into how to incorporate pets into marketing communications. Dogs and cats frequently appear in advertisements and marketing campaigns. For example, Subaru has been running the ""Dog Tested, Dog Approved"" campaign since 2009 (Subaru 2020), and Sainsbury's ""Mog the Cat"" campaign raised a great amount of attention during the 2015 Christmas season ([25]). One consideration factor, according to our findings, is the type of products or services being advertised. For products or services mainly perceived as promotion-focused (e.g., stock investment, sports cars), featuring dogs in the ad is likely to increase the ad's persuasiveness. For products or services deemed more prevention-focused (e.g., mutual fund investment, insurance), featuring cats may increase the ad's appeal. According to the findings of the pet stereotypicality study, a caveat is that marketers should ensure that stereotypical pet temperaments are made salient in the message (e.g., the eagerness [cautiousness] aspect of the dog [cat] should be highlighted). Otherwise, the intended effects of featuring pets in the ad may not be achieved.Third, pet-related marketing strategies are especially relevant in today's big-data era, in which marketers likely know more about consumers, including which types of pets they own or interact with. For example, marketers could obtain consumers' pet exposure information from the pet-related products purchased, the YouTube pet videos watched, or the Instagram pet photos posted and use this data to decide what type of marketing information to highlight. Alternatively, census information can provide marketers with an aggregated level of information about pets, as certain states or zip codes may have a higher concentration of dog (cat) owners. Marketers could use this information to determine the design of regional marketing campaigns.Lastly, our findings that pets and pet ownership are potentially related to COVID-19 transmission rate and prevention behaviors could shed new light on policies related to prevention of COVID-19 and potentially other infectious diseases. For example, policy makers in states with more dog owners could design more customized educational programs and materials related to the diseases. Alternatively, when designing ads to prevent the transmission of COVID-19 and other infectious diseases, cats could be incorporated as a spokesperson and/or their temperaments can be referenced in the message to enhance the effectiveness of the ad. Future Research DirectionsThis research indicates that pets can exert a strong social influence on consumers. Future research could examine the strength of the influence of pets versus people on consumer behavior. On the one hand, one may argue that pets' influence on consumers is weaker than other human influences, because people are more influenced by similar others ([ 3]; Bandura 2002). On the other hand, it is also possible that pets may occasionally exert a stronger influence, especially considering that many pet owners admit that they prefer spending time with their pets over other people ([56]).Another possible research direction is to examine other differences between dog and cat exposure. For example, exposure to dogs may be more likely to lead to conspicuous consumption than cat exposure. This follows because many people likely perceive dogs (vs. cats) as less seclusive and more social. Along similar lines, research could examine the different effects of dog versus cat stimuli. For example, using a dog (cat) as a spokescharacter can increase a brand's perceived excitement. Although our research focuses on examining the effect of pet exposure on consumers' regulatory orientations, future research could investigate the reverse relationship of whether promotion- (prevention-) focused consumers are more likely to adopt a dog (cat) or prefer dog- (cat-) related stimuli.Research could also examine additional moderators for our findings. One example is pet anthropomorphism, or people's tendency to assign human characteristics to pets ([ 2]). One prediction is that our findings that dog (cat) exposure evokes a promotion (prevention) orientation would be more salient among consumers who engage in pet anthropomorphism, because these consumers are more likely to be influenced by the pets. Another possible moderator is the role of culture. We conducted our studies primarily with American participants, and the United States is a culture in which the majority of people treat their pets as friends and family members (Sanders and Hirschman 1996). In many other cultures however, pets are not elevated to the same level, and thus consumers may treat their pets as possessions or servants ([ 9]). Future research could examine whether our identified findings still hold in such cultures. "
39,"The Platformization of Brands Digital platforms that aggregate products and services, such as Google Shopping or Amazon, have emerged as powerful intermediaries to brand offerings, challenging traditional product brands that have largely lost direct access to consumers. As a countermeasure, several long-established brands have built their own flagship platforms to resume control and foster consumer loyalty. For example, sports brands such as Nike, Adidas, or Asics launched tracking and training platforms that allow for ongoing versatile interactions among participants beyond product purchase. The authors analyze these emerging platform offerings, whose potential brands struggle to exploit, and provide guidance for brands that aim to platformize their business. This guidance comprises the conceptualization of digital platforms as places of consumer crowdsourcing (i.e., consumers drawing value from platform participants such as the brand, other consumers, or third-party businesses) and crowdsending (i.e., consumers providing value to platform participants) of products, services, and content along with a well-defined framework that brands can apply to assemble different types of flagship platforms. Evaluating the consequences of crowdsourcing and crowdsending for consumer–platform relationships, the authors derive a typology of archetypical relationship states and develop a set of propositions to help offline-born product brands thrive through platformization.Keywords: digital platforms; platform assemblage; building blocks; relationship states; product brands; digital transformation; relationship marketingDigitization promised established product brands nothing short of their emancipation from the traditional retailing value chain. Online channels made cutting out intermediaries and establishing direct consumer access and relationships both simple and inexpensive ([25]), and many brands willingly embraced this opportunity. However, when new digital aggregators of products and services emerged—in particular, online marketplaces and search engines—they rapidly usurped the interface to consumers, forcing many offline-born product brands back to second rank. For example, Amazon, Google Shopping, and JD.com have become important access points for products from household tools to sports equipment (which we use as a recurring example), leaving brands such as Adidas, Nike, or Asics to resume their places as suppliers to this new group of digital intermediaries.These intermediaries, which we call ""brand aggregation platforms,"" differ from conventional retailers by relying on a platform business model, where they provide the infrastructure and governance to enable commercial transactions between external suppliers and consumers of branded products while not offering these products themselves ([16]; [77]). For example, searching for Adidas running shoes on Google brings up Google Shopping pages from numerous retailers as well as Adidas's own shop. Google aggregates these offerings while the consumer transacts directly with the brand or an online retailer. As a result, these platforms provide value by granting users access to a vast variety of products and services, enabling them to organize their consumption around a few powerful interfaces ([64]).This reintermediation through brand aggregation platforms arguably leaves many brands worse off than in the pre–digital retailing era because these platforms diminish brand differentiation and foster price competition by featuring many similar or even identical offerings at different prices from competing suppliers ([25]). For example, a search for sports jerseys on Zalando, a former online apparel retailer turned brand aggregation platform, provides an exhaustive overview of products from brand-owned shops and myriad retailers.As a countermeasure to this development, some established product brands have started to venture into the platform business themselves ([20]), either by extending their operations organically (e.g., Nike Run Club) or by acquiring existing platforms (e.g., Adidas Runtastic, Asics Runkeeper). This ""platformization"" of brands creates offerings that transcend the specific product brand by including third-party complementary products, services, and content to occupy the broader category space and address consumer needs more holistically ([89]). For example, Nike features events, expert guidance, exclusive products, motivational music playlists, and even personal training as part of its Run Club and Training Club platforms. These emerging offerings, which we call ""brand flagship platforms,"" may be a potent means for product brands to fend off brand aggregation platforms and establish a direct interface to consumers.However, brands have a hard time building competitive flagship platforms. While retailers have long embraced platformization (e.g., Amazon, Zalando, Douglas) as a natural evolution of their consumer-focused aggregator approach, product brands lack cross-product expertise, treat the platform as yet another sales channel, or fear the inclusion of competitor offerings, to name just a few impediments. Thus, the platformization of brands is still in its infancy, though some encouraging examples have emerged in such markets as athletics (e.g., Nike, Garmin), do it yourself (DIY; e.g., Bosch), mobility (e.g., BMW and Daimler), nutrition (e.g., Maggi), or gaming (e.g., Epic). For example, Bosch's DIY & Garden allows consumers to register their tools, engage with a versatile community, and receive advice for DIY projects. In mobility, BMW and Daimler have extended their Share Now platform to include ride hailing; renting scooters, cars, and bikes; and finding parking and charging spots. And in nutrition, Maggi offers online cooking classes and events, inspirational content, and recipes that link directly to supermarkets. In other industries, including business-to-business industries, brands gradually acknowledge the potential of building their own digital platforms, but many have difficulty getting started ([19]; [89]).We address this difficulty by proposing paths for brands to build brand flagship platforms. To this end, we develop a novel conceptualization of digital platforms as places of consumer crowdsourcing and crowdsending, which lie at the core of platformized value creation and which the brand can foster or restrict to shape platform interactions. Consumer crowdsourcing in this context denotes the consumer's assignment of a task (e.g., finding the perfect running shoe, learning how to train for a marathon) to a network of diverse platform participants (i.e., the brand, other consumers, or third-party businesses) that supply products, services, and content, allowing them to draw value from these participants ([33]). Consumer crowdsending denotes the consumer's own contribution of products, services, and content, allowing them to provide value to these platform participants.Drawing on this conceptualization, this article provides brands with a step-by-step guide to venture into platform business. First, we put forth five key goals that consumers pursue when using digital platforms, enhancing the understanding of why platform offerings are often preferred over retail or brand offerings. These insights help design flagship platforms that align consumer and brand goals. Second, we develop a clear framework that disassembles the platform concept into building blocks that facilitate consumer crowdsourcing and crowdsending. Depending on the assemblage of these building blocks, different platform types emerge that may foster or impede the achievement of consumer and brand goals. Third, we propose the intensities of consumer crowdsourcing and crowdsending as typology dimensions that give rise to distinct consumer–platform relationship states. The typology allows brands to evaluate how their selected assemblage affects consumers' use of and attachment to the brand flagship platform. A set of propositions details the benefits and risks of the emergent relationship states. Finally, we offer several suggestions to rethink managerial practices for brands to master the platform transition.Our analysis liaises with an existing stream of literature on (digital) platforms and two-sided markets, extending related work in several directions. Above all, prior studies focus on businesses that are ""born"" as platforms or retailers that expand on their natural aggregation function through platformization. However, how product brands—especially those originating from a nondigital, nonplatformized marketspace—build and use their own platform has been neglected. As a notable exception, [64] raise the idea that platformized brands may pose a threat to traditional retailers in their attempt to reclaim direct consumer access. Yet they neither flesh out the nature of those brand platforms nor analyze how brands can successfully transition to that stage. Furthermore, literature predominantly examines platforms that facilitate commercial exchange. [57] develop a typology of peer-to-peer markets, classifying platforms into discrete types that offer distinct benefits. However, they do not focus on value creation beyond buyer–seller transactions, nor does their model accommodate the possibility that versatile platform assemblages may mix and match functionalities to offer combined benefits. Finally, this study is the first to examine the relational consequences of platformization. While prior work stresses platforms' ability to lower transaction costs, we show that brands can use flagship platforms to build intimate, mutually beneficial relationships based on high levels of self-relevance and affective commitment. Brand Flagship PlatformsResearch and practice have developed a multifaceted understanding of (digital) platforms. However, broad applicability of the platform concept and the myriad platform types and markets that emerged also add complexity, especially for traditional brands, which were largely founded before the advent of internet technologies. To develop the concept of brand flagship platforms, we first briefly define digital platforms in general. We then introduce brand flagship platforms—a specific type of digital platform—by highlighting the opportunities they present to counter and differentiate from brand aggregation platforms. Platform DefinitionPlatforms provide the infrastructure and governance to facilitate interactions between autonomous agents ([16]; [54]; [77]). That is, platforms link agents on two or more market sides that determine the conditions of their interaction directly (i.e., what to exchange, how to exchange it), maintaining residual control rights over their assets ([29]). If Nike or a designated retailer offers the brand's merchandise to consumers on Amazon or an online comparison site, the seller can typically set prices, control the items and quantities offered, and decide sales promotions and delivery conditions. Platforms therefore assume a mediator role, aiming to provide optimal matches between agents on both market sides often through digital technologies ([57]).Platform interactions can range from the commercial selling of products and services (e.g., [57]; [63]) to engaging in forums or social communities (e.g., [67]), to posting and consuming media content on video platforms (e.g., [63]). Accordingly, examples are abundant, and these include social media (e.g., Instagram, TikTok), knowledge exchange forums (e.g., StackOverflow, Quora), communities (e.g., Reddit, brand communities), video sharing platforms (e.g., YouTube, Vimeo), advertising platforms (e.g., Google AdWords), service platforms (e.g., Uber, Airbnb), hardware–software platforms (e.g., game consoles), and software–software platforms (e.g., operating systems, app store). The starting point of this analysis is the digital reintermediation of established brands by brand aggregation platforms (e.g., Amazon, Google Shopping, Booking.com), which emerged as powerful interfaces to consumers' product search and purchase activities. Brand flagship platforms may offer a way out of this renewed dependence for product brands that aim to reestablish direct consumer access and foster brand loyalty. We describe these two types of platforms next. Brand Flagship Platforms as a Counter to Brand Aggregation PlatformsResearch has developed a detailed understanding of brand aggregation platforms (e.g., [29]; [54]), on which the interaction between market sides is, at its core, a commercial exchange between buyers and sellers of branded products. Brand aggregation platforms thus serve as intermediaries to discrete transactions that lower search costs and efficiently match customers with product or service offerings ([38]). The transactions are discrete in that purchases do not trigger an ongoing feedback process (other than, for example, dialogues on social media) and customers typically return to enter a new purchase cycle ([71]). For instance, a consumer who needs running shoes might compare different models on JD.com or Wish, purchase on the platform, and return when they need a new pair or other equipment.A set of distinct features of brand aggregation platforms (see Table 1) poses numerous challenges for product brands, of which we highlight a few. First, their broad, cross-category scope induces consumers to increasingly start their purchase journeys on brand aggregation platforms, often searching for product classes rather than specific brands ([82]). This development puts the platform brand (e.g., Amazon) in the center, threatening to degrade product brands to ""white-label back offices"" ([ 9], p. 13). Second, contributing to this degradation, brands have difficulty differentiating from competitors because aggregation platforms standardize product presentation, limit the use of branding elements, and encourage simple comparison on a few key features (e.g., price, rating, delivery). Third, product competition occurs not only between brands (e.g., Adidas vs. Asics) but also within brands, as different sellers will offer the identical product at different prices. This aspect represents a strong benefit for customers, making the platform itself agnostic to legitimate brand interests (e.g., preventing price erosion and brand-building efforts) because its primary goal is to provide optimal matches.GraphTable 1. Properties of Brand Aggregation and Brand Flagship Platforms. PropertyBrand Aggregation PlatformBrand Flagship PlatformDefinitionPlatform-brand-owned digital platform that mediates discrete transactions between buyers and commercial sellers of branded productsProduct-brand-owned digital platform that mediates versatile interactions between participants within the brand-related category spaceKey goals and activitiesFacilitating direct selling of products and servicesMultiple goals and activities; for example, creating brand awareness, strengthening consumer relationships, building community, fostering consumer learning, selling products and servicesCommercial target functionFacilitating transactions; largely agnostic to product brandingDirectly or indirectly driving brand sales and fostering brand loyaltyCategory scopeBroad: cross-category generalistDeep: within-category specialistActivity scopeNarrow: Product-focused aggregator of commercial offerings with ancillary servicesBroad: consumer-focused, versatile, and multisided network of products, services, and contentOwnerOnline-born platform brandPlatformized retailerOffline-born product brandRole of platform ownerMediator of commercial transactionsMediator and supplier of products, services, and contentRoles of platform participantsLargely well defined (buyers and sellers)Loosely defined (participants can assume many different roles)Role of product brandCompetes for limited platform space and addresses similar consumption needs as competing offeringsOrchestrates value-creating activities on the platformCompetitionEncourages within-brand and between-brand competitionAims to avoid within-brand and between-brand competition; focus on complementsInventoryInternal: none or comparatively limitedExternal: extensive assortment of (competing) third-party offeringsInternal: offers own-branded productsPossibly external: offers complements, possibly direct competitor productsUser experienceStandardized (category-agnostic); aims for simple product comparisonIndividualized (category-specific); aims for optimized consumer category experienceExamplesAmazon Marketplace, Google Shopping, Wish, Idealo, JD.com, Alibaba, ZalandoNike Training Club, Adidas Runtastic, Asics Runkeeper, Bosch DIY & GardenRelated researchTwo-sided markets (e.g., Hagiu and Wright 2015; Reinartz, Wiegand, and Imschloss 2019; Rochet and Tirole 2003)Community building and cocreation (e.g., Kozinets, Hemetsberger, and Schau 2008; Ramaswamy and Ozcan 2018; Siebert et al. 2020) Despite the widespread success of brand aggregation platforms, their focus on facilitating transactions across a broad product range also makes them vulnerable to competitive attacks. They typically lack the resources, expertise, reputation, and infrastructure to individualize customer experiences with respect to any single category and thus fail to occupy a specific category space ([64]). By contrast, product brands can approach value creation through platformization from a more specialized angle (e.g., Nike's Run Club and Training Club platforms are built around athletics).Critically, brand flagship platforms, as mediators of versatile interactions between participants, are destined to become much more than an own-brand sales channel. They offer vast opportunities to cocreate value within the brand's category space through a plethora of commercial and noncommercial activities ([62]). Versatile interactions may include anything from buying products to providing and consuming content (e.g., product reviews, creative videos, educative podcasts) or services (e.g., participating in brand- or community-organized events). For example, participants on Bosch's DIY & Garden can access and upload home projects, discuss tools and techniques in forums, receive expert advice, participate in DIY challenges, collect points and badges, and engage in many other activities. The platform thus provides a rich array of products, services, and content around consumers' idiosyncratic DIY needs, offering them a potentially superior, more specialized yet more comprehensive experience than brand aggregation platforms.Owing to their versatility, brand flagship platforms may pursue numerous goals such as increasing brand awareness and loyalty, offering complementary products and services, or triggering consumer feedback processes. Although the platform may include direct selling of brand inventory, this need not be its sole or central purpose. Runtastic started out as a tracking application and, despite its acquisition by Adidas in 2015, has preserved and enhanced these features, which remain at the core of the platform's value proposition. Table 1 summarizes our conceptions of brand aggregation and brand flagship platforms.Assembling brand flagship platforms is not trivial, as they require strategic and operational choices that may substantially enhance or limit their value to consumers. In both practice and theory, brand platformization is still evolving, and guidance on how to approach the transformation is scarce. In response, we next present a novel conceptual framework to systemize the assemblage and management of brand flagship platforms. A Decision–Process–Outcome Framework of Brand Flagship PlatformsOur framework is rooted in the idea for platforms as communities that leverage the wisdom and addressability of the crowd—that is, ""an undefined (and generally large) network"" of actors ([46], p. 346). Brands can use this framework as a conceptual basis to map the possible paths to their own flagship platform. We detail the underlying concepts and their relationships and tie them to brands' platform assemblage decisions and consumer relationship outcomes. Digital Platforms as Places of Crowdsourcing and CrowdsendingDigital platforms address consumer goals by relegating them to the open community, or the crowd ([33]; [74]). Underlying the platform concept is the idea that standing in a crowd and shouting out why you are there is often more beneficial than reaching out to agents in this crowd individually ([ 1]; [33]). The crowd's assets and capabilities collectively exceed those of any subset of agents, making scarce resources more abundant and increasing the number of unique offerings ([34], p. 186). Furthermore, the search for solutions in crowds become less resource-intensive because one market side (e.g., the consumer) articulates its goal to all participants on the other side (e.g., suppliers) simultaneously. As a result, drawing from the crowd is likely to increase effectiveness and efficiency of problem solving ([74]; [83][83]).Crowds have been shown to play an important part in new idea generation and selection ([24]), product development ([ 4]), and solving of micro tasks ([26]). However, most studies take a managerial perspective, focusing on firms' decision to relegate business problems to consumers ([87]). We extend this notion to the digital platform context, where the ""shouting out"" of platform participants' goals translates to, for example, consumers browsing through content, using search queries, or sorting and filtering categories.We view these activities as forms of consumer crowdsourcing—that is, consumers' open assignment of a task to a network of people or other entities ([87]). On platforms, task assignment pertains not only to consumers but to all platform participants, including third-party businesses and product brands, which assume similar positions as contributors to the whole. Consumers harness the power of this crowd—rather than individual retailers or brands—whose product, service, and content offerings are bundled and made digestible through technology. Notably, it is irrelevant if the consumer decides to source from one platform participant only (e.g., follows one training plan posted by a single person), as this choice is the outcome of the crowdsourcing activity, akin to the winner of a crowdsourcing competition. The consumer selects the offering that suits them best out of all crowd-supplied offerings, like strolling through the weekly market to fill their bag.On digital platforms, recipients can also add to crowd solutions themselves by, for example, rating products; engaging in discussions; and uploading pictures, videos, or music playlists. We term such behavior consumer ""crowdsending,"" which we define as consumers' activity of contributing to the network through the supply of products, services, or content. Engaging in crowdsending may, for example, strengthen consumers' social identity and status ([40]), bestow a sense of purpose and belonging ([73]), and allow them to gain (monetary) rewards ([44]). Akin to crowdsourcing, crowdsending on digital platforms supersedes piecemeal activities by ensuring that the input resonates with a large and engaged crowd of recipients, who in turn use the crowdsent material for further value creation, dramatically extending the scope of these activities compared with a nonplatform environment.[ 6] From Flagship Platform Assemblage to Consumer–Platform RelationshipsBuilding on these ideas, we structure our conceptual development along a decision–process–outcome framework. Assembling infrastructure and governance mechanisms represent upstream decisions the brand makes. The resulting platform assemblage, as the totality of infrastructure and governance mechanisms, determines which crowdsourcing and -sending activities the platform facilitates and how these activities are designed.Consumer crowdsourcing and crowdsending represent value-creating processes that aim to fulfill consumer goals. While brands may foster or restrict specific processes through their assemblage decisions (Kozinets, Ferreira, and Chimenti 2021), they manifest only in the interplay with consumers. That is, a platform's capacity to affect (through crowdsourcing) or to be affected (through crowdsending) needs to be met with the capacity to affect or be affected on the part of the consumer ([21]). Accordingly, which platform offerings are exploited hinges on each consumer's idiosyncratic engagement with the platform's crowdsourcing and -sending capacities, which they mix and match to create their own solution ([22]). For example, learning how to consistently lose weight requires the exploration of parts of the flagship platform that differ from those required when training for a marathon.[ 7]Finally, crowdsourcing and -sending experiences on the brand flagship platform have important downstream consequences (outcome) for the development and maintenance of consumer relationships, which ultimately define the long-term success of the brand's platform business. As we show in a subsequent section, different relationship states manifest from consumer crowdsourcing and -sending, which can range from transaction-oriented, discrete exchanges to ongoing interactions that lead to a profound embeddedness of the platform in consumers' lives ([13]; [90]). Brands need to assess the level of alignment between the relationship states they seek with their flagship platform (i.e., their brand goals) and the achieved outcome and dynamically reassemble platform parts to spur new crowdsourcing and -sending processes or redirect existing ones.Figure 1 summarizes this framework.Graph: Figure 1. Decision–process–outcome framework for the creation of brand flagship platforms. Notes: The different shapes/colors indicate whether a topic area relates to decisions, processes, or outcomes. Platform building blocks are a subset of the decision-related topic areas and consumer-platform relationship states are a subset of the outcome-related topic areas.The following sections tie together the depicted processes of crowdsourcing and crowdsending with brands' platform assemblage activities (decision), on the one hand, and with the emerging consumer–platform relationships (outcome), on the other hand. Decision: Consumer Goals and Platform Building BlocksWe posit that the crowd, that is, the collective of platform participants, is particularly good at addressing important consumer goals, which renders digital platforms a superior alternative to retailers and direct interaction. The question thus arises as to when crowdsourcing and -sending activities become particularly rewarding for consumers. We have synthesized prior research from major marketing journals (see Web Appendix A) and industry observations to derive five key goals that consumers pursue when sourcing from or sending to the crowd: commercial exchange, social exchange, self-improvement, epistemic empowerment, and creative empowerment. We put forth these goals, summarized in Table 2 and explained next, because they follow directly from the interpretation of platforms as crowd-based providers of value.GraphTable 2. Key Consumer Goals and Platform Building Blocks. ConsumerPlatformNature of the Consumer GoalDefinition of the Consumer GoalLiterature ExampleBuilding BlockRelated FeaturesLiterature ExampleCommercial exchangeFinding the best- matching offering or exchange partnerPerren and Kozinets (2018)Transaction blockProduct/service marketplace, complaint handlingHagiu and Wright (2015)Social exchangeEngaging in social interactionSchau, Muñiz, and Arnould (2009)Community blockCommunity forums, social sharingRamaswamy and Ozcan (2016)Self-improvementCompeting and comparing to live up to one's full potentialRamaswamy and Ozcan (2018)Benchmarking blockTracking, measurement, benchmarkingLabrecque et al. (2013)Epistemic empowermentSpreading and gaining knowledge to make informed decisionsKozinets, Ferreira, and Chimenti (2021)Guidance blockPeer-to-peer route planning, ""how-to"" videos, customer feedbackKozinets, Ferreira, and Chimenti (2021)Creative empowermentInspiring or being inspired by something new or curiousAlbuquerque et al. (2012)Inspiration blockDecoration videos, tools for exploration and experimentationFüller, Matzler, and Hoppe (2008) Crowdsourcing may pertain to efficiently finding an offer for products or services from the network of suppliers ([29]), but also extends to, for instance, gathering knowledge or inspiration about products, services, or activities. Likewise, crowdsending may pertain to, for example, the goals of engaging in social interaction (e.g., [67]), self-actualization ([72]), or self-expression by providing information or inspiration ([47]). Consumers strive to become active contributors on the platform and are rewarded by the crowd's engagement with their contributions ([46]). Naturally, consumers may pursue more than one goal with the same platform (and the same platform feature), and platforms may serve different goals for different consumers.Digital platforms cater to these important goals in the form of five building blocks, depicted on the right-hand side of Table 2. Importantly, following straight from the consumer-goal perspective, these building blocks do not map exactly to technical platform features. For instance, the sharing of a training exercise could reflect a social desire (engaging with the crowd) but also serve as guidance, inspiration, or a way of benchmarking for users. Thus, the same technical implementation may reflect several building blocks and thus cater to several consumer goals. We explain each block and corresponding goal in the following. Commercial Exchange—The Transaction BlockCatering to the goal of commercial exchange with a transaction block focuses on providing matches between demand and supply to ensure that consumers find the offering that best suits their needs within the assortment of products or services ([29]; [85]). Although facilitating commercial exchange is not unique to digital platforms, the introduction of virtually endless shelf space coupled with advanced matching algorithms and rapid scalability on both market sides (buyers and sellers) through network effects has laid the foundation for platforms providing superior customer value relative to traditional pipeline businesses (Van Alstyne, Parker, and Choudary 2016). Digital platforms allow for almost frictionless participation, provide higher price and quality transparency, and allow buyers and sellers to find the perfect exchange partner easily and quickly, thereby vastly diminishing transaction costs ([38]).For many commercial platforms (including brand aggregation platforms) the exchange block is at the core of their operations, with additional infrastructure assuming supporting functions (e.g., Amazon Marketplace, eBay, Uber). However, some platforms integrate an exchange block as part of a more balanced approach so that the commercial-exchange focus does not dominate other platform benefits and thereby prevent further user growth. Social Exchange—The Community BlockSocial exchange relates to the goal of engaging in interactions with other platform participants such as consumers, third parties, or employees. These interactions are often part of an ongoing and evolving dialogue between participants aimed at advancing an array of social practices ([67]). Research on cocreation, brand communities, and website use shows that these interactions lead to several social benefits for consumers (e.g., [51]). They create a sense of belonging, identity, and even friendship ([86]), and consumers enjoy the status, reputation, and esteem they acquire within a community of peers as well as their expression of a unique self-image, giving them a sense of self-efficacy ([32]; [51]).Consumers may achieve these goals on the platform through a variety of means, such as by sharing experiences and personal beliefs ([48]). Depending on which features are part of the platform assemblage and how well they are integrated, broadly positioned brand flagship platforms are able to cater to social exchange goals much better than individual brand communities, which are limited by their commercial and brand focus, relatively small group of active participants, and lack of diversity. Self-Improvement—The Benchmarking BlockConsumers have the fundamental urge to live up to their full potential, ""the desire to become more and more what one is, to become everything that one is capable of becoming"" ([50], p. 383). Prior literature has incorporated individual aspects of self-improvement in terms of acquiring excellence ([32]). We extend this concept to include self-respect and accomplishment ([32]; [86]). These aspects underlie the quantification and competition trends that many digital platforms address, especially in the contexts of fitness, health, nutrition, and sports ([66]). We posit two points of reference that are most relevant for the consumer in terms of benchmarking and drawing motivation to self-improve ([43]; [84]): the platform crowd and the consumer's own past performance. This development is fueled by the widespread use of connected devices such as mobile phones and wearables that can track and quantify consumers' activities, workouts, sleep quality, heart rate, and more (James, Deane, and Wallace 2019). Digital platforms often combine self-tracking and measurement technology with crowd-based motivational features and gamified experiences such as user rankings, virtual badges, and opportunities to share accomplishments ([47]). Epistemic Empowerment—The Guidance BlockConsumer empowerment is defined as ""the strengthening of consumers' abilities, rights, or authority to consume or otherwise fulfil their objectives as a marketplace actor"" (Kozinets, Ferreira, and Chimenti 2021, p. 7). Digital platforms support epistemic consumer empowerment through information on the products and services they offer—but also do more. Some platforms focus on participants' exchange of information, as in the case of Skillshare, where thousands of teachers can upload educational videos, or Komoot, where participants can record hiking or biking routes and recommend them to others. Other platforms build a strong guidance block around their main commercial operations by allowing participants to share instructional videos, blog posts, or structured product feedback and evaluation ([45]).Guidance blocks are typically standardized, tailored to a specific purpose, one-directional, and aim to provide clear and concise results compared with the more open and socially oriented community block. Owing to the quantity and diversity of content available through crowd-based guidance blocks, epistemic empowerment of participants on all market sides easily supersedes what retailers or single brands can offer. Creative Empowerment—The Inspiration BlockWith creative empowerment, we denote satisfying consumers' curiosity and longing for new experiences ([70]; [76]). Whereas epistemic empowerment refers to objective and functional knowledge such as that for comparing product alternatives for purchase, creative empowerment refers to knowledge and stimulation that inspires consumers ([15]). Consumers can use digital platforms to gain or provide elements of creative empowerment by consuming or creating content (including entertaining content) and products. Platforms such as Pinterest and Wattpad leverage the crowd to provide a richer set of ideas for consumers seeking inspiration and a large and engaged group of potential recipients for consumers sharing their ideas. Nike's Run Club platform induces consumers to inspire and be inspired by the crowd through music playlists or workout videos that they can add for others to use in their own routine. Assembling the Building BlocksThe five building blocks can be (re)assembled to represent different types of platforms observable in the market (Figure 2). Thinking about the heterogeneous platform landscape as different assemblages of these building blocks presents a powerful tool for brands in their creation of their own flagship platforms because it helps them select the optimal assemblage to achieve specific brand goals. We suggest that the existence and the relative scope of each building block jointly determine which platform type emerges. For example, an emphasis on the transaction block tends to produce marketplaces that cater primarily to the goal of commercial exchange. A strong benchmarking block yields tracking and coaching platforms that cater primarily to the goal of self-improvement. Other platforms combine blocks to fulfill several consumer goals at the same time and allow each consumer to pursue their own goal choosing a specific consumer–platform subassemblage ([21]; [31]). For instance, Nike's Run Club offers tracking and ranking features to support self-improvement and sharing features to socially engage with other participants. While the consumer's idiosyncratic crowdsourcing and -sending activities yield individual subassemblages, the brand sets the general frame through its assemblage of building blocks.Graph: Figure 2. Using the building blocks to assemble different platform types.Our conceptualization of digital platforms as places of consumer crowdsourcing and crowdsending and corresponding assemblages of building blocks moves the focus of analysis from the platform architecture, which most prior studies address, to the consumer goals that platforms aim to fulfill. It also reduces the complexity and ambivalence of the platform concept reflected in prior literature. This view implies that digital platforms are revolutionary not only because of their ability to scale quickly and leverage network effects ([65]) but also because of their versatility and integration ability: they create greater value for consumers by allowing them to draw from and provide to the crowd using assemblages of building blocks that facilitate idiosyncratic goal pursuit. This recent development is important for brands because it allows them to counter the intermediation through brand aggregation platforms by evading direct competition on their home turf and instead establish a direct interface with consumers, developing relationships that go beyond discrete transactions. Next, we discuss what these relationships look like and how crowdsourcing and -sending activities affect their emergence. Outcome: Consumer–Platform RelationshipsThe relationships that emerge on flagship platforms reflect focal outcomes of brands' platformization efforts because they determine how brands can leverage their platforms strategically. By ""relationship,"" we denote the possible states that consumers may enter in their interactions with brand flagship platforms and that vary in terms of their self-relevance, commitment, and durability (e.g., [23]). As consumers interact with platform participants through crowdsourcing and -sending activities, the nature of these interactions shapes the relationship states that emerge ([36]).The following subsections develop this effect in two steps. In the first step, we establish the general link between the consumer processes of crowdsourcing and -sending, on the one hand, and the consumer–platform relationship, on the other. To this end, we argue that the nature of interactions on brand flagship platforms varies by the intensity of consumer crowdsourcing and -sending, and that this intensity increases as consumers use the platform to pursue higher-level goals. We then show that this intensity in turn affects consumer–platform relationships on a spectrum from discrete, exchange-based transactions to versatile, ongoing interactions. More intense crowdsourcing and crowdsending move the needle toward the versatile end of the spectrum, that is, increasingly profound relationships that culminate in a deep embeddedness of the platform in consumers' lives ([13]; [90]).In the second step, we detail this general link by separating out crowdsourcing and crowdsending intensities to constitute key dimensions of a typology of archetypical relationship states. This step allows us to enter a more fine-grained discussion of consumer–platform relationships that helps brands analyze specific outcomes of their platform assemblage efforts. We first describe the typology and then develop propositions that clarify which relationship state brands should foster on their flagship platform to realize particular brand goals, as well as which risks are involved. Consumer Crowdsourcing and -Sending Intensities as Relationship DeterminantsConsumers' crowdsourcing and -sending activities on brand flagship platforms can vary greatly in terms of their intensity—that is, how extensive and/or intimate they are ([ 5]; [27]). Drawing from prior literature, we refer to extensiveness as the frequency, duration, and variety of activities, whereas intimacy in crowdsourcing (crowdsending) denotes how sensitive or personal the consumed (disclosed) information is or the degree to which participants are influenced by (influence) other platform participants with respect to their attitudes and behaviors ([ 5]; [ 6]). Intensity is therefore a continuous dimension of crowdsourcing and -sending that increases with growing extensiveness and intimacy. For example, rating a running shoe constitutes lower-intensity crowdsending than writing a review, which in turn reflects a lower crowdsending intensity than sharing a series of videos that explain different running postures.When does intense consumer crowdsourcing and crowdsending occur on brand flagship platforms? Brands provide the infrastructure that enables consumers to engage in crowdsourcing and -sending through the platform's assemblage of building blocks and governance mechanisms. However, intense interactions only occur if consumers are also motivated to draw from and contribute to the crowd. As a key factor of this motivation, we put forth whether and how comprehensively the platform addresses consumers' higher-level goals and thus allows the platform to transcend discrete, product-centric transactions, as we explain next.According to means-end theory, consumer goals are hierarchically organized (Huffman, Ratneshwar, and Mick 2003; [58]). On the highest level, consumers formulate abstract goals that describe why they perform certain actions in the pursuit of their personal values and ideal self-identity, such as following a healthy and active lifestyle ([58]). Midlevel goals constitute projects with concrete targets such as being able to run ten kilometers in under 50 minutes ([58]). On the lowest level, consumers define actions and behaviors that allow them to achieve their superordinate goals, including purchase decisions such as buying a particular pair of running shoes ([13]; [58]).Brand flagship platforms that address higher-level goals should be more central to consumers' sense of self ([ 8]; [13]) and thus increase the platform's self-relevance—that is, the alignment of the platform with consumers' self-concept and self-image ([14]). As self-relevance grows, consumers' satisfaction, trust, and commitment tend to increase ([41]) along with their willingness to engage in more intense interactions. A platform with an exchange-based, product-centric focus serves lower-level goals (Huffman, Ratneshwar, and Mick 2003), implying low self-relevance. In contrast, flagship platforms with a higher-level goal focus should achieve greater self-relevance to consumers and motivate them to engage in more intense crowdsourcing and -sending.[11] findings on the car-sharing provider Zipcar show how fostering intense crowdsourcing and -sending may fail when higher-level consumer goals are not addressed. Zipcar tried to create more intense interactions by introducing a community block to the platform. However, the platform lacked self-relevance, as consumers used the platform mainly for utilitarian motives. Thus, they remained highly focused on the lower-level consumption-related goals, did not engage in intense crowdsourcing or -sending, and even actively rejected the community features. By contrast, Adidas Runtastic and Nike Run Club explicitly and consistently address higher-level, self-relevant consumer goals pertaining to self-improvement and an active lifestyle, which foster intimate crowdsourcing (e.g., following advice) as well as crowdsending (e.g., contributing to the community) activities. P1:  Brand flagship platforms that address higher-level rather than lower-level consumer goals (a) are more self-relevant to consumers and, therefore, achieve (b) higher crowdsourcing and (c) higher crowdsending intensities.By motivating consumers to engage in intense crowdsourcing and -sending, a platform's locus of value creation shifts from ephemeral value-in-exchange toward ongoing value-in-use ([62]; [82]). Accordingly, product-centric exchanges evolve to project-centric interactions, as consumers use the platform to pursue discrete (e.g., running a marathon, remodeling one's garden) or even continuous projects based on their higher-level goals (e.g., living an active and healthy life, living in a comfortable and safe home). These projects are inherently long-term, involving multiple consumer crowdsourcing and -sending activities on different levels. The platform thus offers a crowd-based, integrated solution that goes far beyond selling products or services ([22]; [80]).To illustrate, brand aggregation platforms typically revolve around discrete transactions, including buying and rating products, or answering product-related questions. These activities may occur repeatedly but remain superficial and short-lived, lacking intimacy and extensiveness. Platforms such as Google Shopping provide outstanding transactional value, but crowdsourcing and -sending beyond that are limited and often discouraged or even prevented ([18]). In contrast, a brand flagship platform like Adidas Runtastic supports extensive and intimate crowdsourcing and -sending. Consumers upload self-created content, publicize their performance, or follow workout routines from peers. As the value-creation process is inherently interactional, greater intensity in these activities leads to greater and perpetuated value for consumers ([62]).[ 8]The extensiveness and intimacy of intense crowdsourcing and -sending and the interdependence that follows from the joint, project-centric value creation elevate consumer–platform relationships from being ephemeral and exchange-focused to becoming more committed and durable ([23]; [56]). As a result, brand flagship platforms can transcend the traditional customer journey stages and achieve an increasingly deep embeddedness in consumers' lives ([13]; [90]) with increasing intensities of crowdsourcing and crowdsending. P2:  Brand flagship platforms that support more intense rather than less intense crowdsourcing and crowdsending foster consumer relationships that are (a) less exchange-focused, (b) more committed, and (c) more durable. A Typology of Relationship States on Brand Flagship PlatformsThe general assertion that crowdsourcing and crowdsending intensities are directly related to relationship outcomes can be further detailed by separating out the activities and examining their distinct effects on consumer–platform relationships. Accordingly, we propose consumer crowdsourcing and -sending intensities as the two core dimensions that determine the emergence of archetypal relationship states on brand flagship platforms (Figure 3). We briefly describe these states and illustrate their differences using our athletics example before discussing their implications for product brands as platform owners.Graph: Figure 3. Consumer–platform relationships and brand outcomes. The ad hoc relationshipWhen consumer crowdsourcing and -sending intensities are low, value creation is typically transaction- or product-centric, addressing consumers' lower-level goals. Value creation may occur through purchase-related activities (e.g., commercial exchange, product reviews) or functional extensions to the core product (e.g., ""smart"" features such as reporting the wear on a running shoe). Importantly, the crowd is not uninvolved in the platform's value creation, as consumers may still crowdsource from and crowdsend to the platform by, for example, consulting or providing product ratings or responding to product-related questions ([10]). However, these activities remain superficial because they lack extensiveness and intimacy. As a result, self-relevance is low, and the emerging relationship is ephemeral and exchange-focused. This outcome is not necessarily negative, because consumers do not always seek highly committed relationships but value instrumentality in low self-relevance consumption contexts ([12]). The capitalizing relationshipWhen consumer crowdsourcing intensity is high but crowdsending intensity is low, the consumer primarily capitalizes unidirectionally on value provided by the brand flagship platform while not being highly engaged in the creation of value themselves. For example, the consumer may follow a workout routine or extend their knowledge base by watching instructional coaching videos. But as crowdsending intensity is low, the consumer does not share their progress with the crowd or engage in community discussions. Intense crowdsourcing implies that the platform addresses self-relevant higher-level consumer goals, creating a willingness to integrate the platform assemblage into their own self ([ 8]). The platform becomes part of the consumer and their identity, endowing them with new capacities that result in self-expansion experiences ([31]). However, as consumer crowdsending intensity is low, the platform misses out on the input that ""lurking"" consumers fail to provide, neither creating value for the consumer by actively contributing to the crowd, nor being able to collect data from these crowdsending activities to further improve crowdsourcing opportunities down the line ([68]). The catalyzing relationshipWhen consumer crowdsourcing intensity is low but crowdsending intensity high, the brand flagship platform catalyzes consumers' value creation, providing a canvas on which they can express and affirm their self ([ 7]; [79]). For example, consumers may share and annotate their favorite running routes or design sportswear.[ 9] This intense crowdsending implies that the platform addresses self-relevant higher-level goals and leads to self-extension experiences ([13]) as the consumer integrates part of their self into the platform ([31]). As a result, the platform assemblage gains new capacities from the participant. Thus, in contrast to the relationship states with low crowdsending intensity, consumers are more deeply integrated into the value cocreation process ([61]).Nonetheless, value creation in these relationships is innately truncated. The consumer self-extends into the platform while their self remains stuck in the status quo, owing to a lack of self-expansion. Thus, the consumer does not gain new capacities such as skills or knowledge which limits the relationship to what the consumer is instead of what they could be. The nurturing partnershipWhen consumers engage in intense crowdsourcing and crowdsending, a nurturing partnership between consumer and platform emerges. Contrary to other relationship states, value creation is virtually unrestricted, as both parties can grow mutually. When the consumer extends their self into the platform (crowdsending), the platform's capacities expand, allowing it to provide value through which the consumer self-expands (crowdsourcing). For example, while the consumer engages in crowdsourcing by following a series of video tutorials and live events, they may also engage in crowdsending by contributing to discussions, sharing their training diary, and creating workout playlists. These activities may create consumer value as others engage with the consumer's input by upvoting their content, cheering them on, or inviting them to challenges. Furthermore, the input can also spawn new opportunities for crowdsourcing, for example by others offering advice on the consumer's newly acquired skills or the platform improving and personalizing their crowdsourcing experience, reaching ever higher levels of value drawn from the crowd. With these ongoing interactions, the platform and consumer can keep nurturing each other's capacities through the interplay of extending into and being expanded by the respective other. Mutual value creation and codependence in a nurturing partnership occur to perpetuate the relationship and its intense interactions ([49]), bearing the potential for the platform to become a close companion on the consumer's path toward their higher-level goals. Aligning Consumer–Platform Relationships with Brand GoalsBrands can use the depicted typology to assemble and dynamically revise their flagship platform to foster the desired relationship state with consumers. Which relationship state is desirable depends on the goal(s) a brand pursues, as not every state lends itself well to each brand goal ([11]). Thus, brands need to identify and prioritize their goals and select building blocks and governance mechanisms accordingly. We first present brands' main goals when venturing into platform business and subsequently discuss suitable relationship states to achieve specific goals, as well as the pitfalls that may arise.Combining insights from academic literature, the business press, and a practitioner workshop (see Web Appendix B), we identified five primary goals that brands pursue when creating their own flagship platform. First, brands expect the platform to support sales of their core offering, either by selling on the platform directly as an additional distribution channel or by indirectly generating leads through recommendations and ads. This activity may also include after-sales services and customer care. Second, brands may aim to extend their core offering by sourcing functional enhancements through the platform, such as connected software applications or inspiration for alternative or improved product use. Third, brands aim to extend their core offering through complementary products and services, typically reaching into adjacent product categories. To stick with our sports example, a brand that sells running equipment might use the platform to offer sports nutrition, personal coaching, active travel booking, and a sports events ticketing service with the help of third-party suppliers. Fourth, brands aim to increase brand equity and deepen consumer relationships through frequent interactions and ways to nudge engagement behavior (e.g., by facilitating social interaction or cocreation activities). For example, a platform may allow runners to co-design their own sports gear, thereby increasing engagement with the core offering and the brand ([59]). Fifth, brand flagship platforms promise access to vast amounts of data that brands can use to better understand consumers and competitors as well as complement providers.In ad hoc relationships, the transaction block often is core to interactions. If other building blocks exist, they either serve the exchange (e.g., community features for product-related questions) or are limited to functional product enhancements (e.g., after-sales service, tracking features). Means for intense crowdsourcing or -sending are usually not provided, yielding an instrumental consumption experience with low self-relevance for consumers and a focus on utilitarian benefits rather than intimate relationships ([11]). As interactions are limited, brand flagship platforms can employ a restrictive governance style in which the brand retains control over interactions ([18]).Despite its lack of self-relevance, the platform can still achieve high consumer loyalty if consumers seek instrumental consumption experiences and the platform provides substantial utilitarian benefits ([11]; [41]), for example, through efficiency gains from a broad range of offerings or high matchmaking quality ([38]; [57]). This loyalty built on utilitarian benefits such as convenience or cost savings constitutes what we term ""cold loyalty."" If a flagship platform fails to deliver on these utilitarian benefits or if a competitor delivers greater utilitarian benefits, consumers are likely to abandon the ad hoc relationship quickly. P3:  If the brand's main objective is to support its core offering (through direct selling and functional enhancements), it is optimal to foster ad hoc relationships on the brand flagship platform. P4:  Lower consumer crowdsourcing and crowdsending intensities on the brand flagship platform (a) foster instrumental consumption experiences and, therefore, (b) increase the risk of consumers switching to platforms that offer superior utilitarian benefits.In capitalizing relationships, platforms offer opportunities for intense crowdsourcing under higher-level consumer goals. These goals spur concrete needs that the platform may address to increase self-relevance and foster intense crowdsourcing. On an athletics platform, for example, the higher-level goal of an active and healthy lifestyle may create needs for adhering to a training schedule, learning about nutrition, or participating in sports events. The more comprehensively a platform covers the many needs associated with a higher-level goal—that is, the further it expands into the broader category space—the more self-relevant the platform can become and the more intensely consumers can engage in crowdsourcing. Thus, capitalizing relationships allow—and usually require—brands to extend their core operations into a variety of adjacent fields connected to higher-level goals.The low degree of consumer crowdsending intensity implies that consumers are not deeply involved in the value creation such that value is usually provided by the brand itself or by third parties. In addition, it allows for a stricter governance style through which brands can retain control over the value-creation process and, by extension, monetization and customer satisfaction ([18]). Typically, the guidance block is most pronounced as consumers focus on expanding their selves rather than engaging with others. Discrete interactions in capitalizing relationships thus become part of a longer journey to self-expansion, during which the consumer builds knowledge and skills. The result is a ""warm loyalty"" that goes far beyond rational motives and that features a high level of self-relevance, affective commitment, and attachment ([41]).Despite consumers' strong attachment, this relationship state faces risks of destabilization ([21]). On the one hand, as the platform extends further into a category space supported through third-party complementors, the brand risks being diluted as its positioning is stretched across the category space and it vies with third parties for consumers' attention (Swaminathan et al. 2020). On the other hand, consumers may disengage from the platform if it lacks alignment with their higher-level goals ([52]). This response may result from the lack of intense crowdsending, which challenges the brand's ability to evaluate the alignment. As consumers are not deeply involved in the value creation, they cannot steer the platform toward their own higher-level goals. P5:  If the brand's main objective is an extension of its offering into the broader category space, it is optimal to foster capitalizing relationships on the brand flagship platform. P6:  Higher consumer crowdsourcing intensity on the brand flagship platform increases the risk of brand dilution. P7:  Lower consumer crowdsending intensity on the brand flagship platform increases the risk of misalignment with consumer goals.In catalyzing relationships, consumers engage in intense crowdsending through which they self-extend into the platform and cocreate its value. The platform thus becomes a canvas that consumers can project their selves and higher-level goals onto. It enables this activity through its assemblage of building blocks, typically by providing pronounced community and inspiration blocks, and a relaxed governance style. The resulting cocreation experiences are associated with a variety of psychological benefits that increase involvement, attachment, and engagement of consumers, fostering another form of ""warm loyalty"" ([17]).Given the low degree of consumer crowdsourcing, interactions typically center on the brand's core products or a few key platform functionalities. Thus, the brand focus is stronger in these relationships, decreasing the risk of brand dilution compared with states with high crowdsourcing intensity. Accordingly, brands can leverage catalyzing relationships to increase consumers' engagement with the brand's core products. Relationship states with high crowdsending intensity also allow brands to gather intelligence from the information shared and value provided by consumers, such as for product development ([35]).While intense crowdsending requires a relaxed governance style that enables consumers' creative empowerment ([57]), a catalyzing relationship may degrade when consumers exploit these conditions ([52]). For example, consumers may hijack the platform by taking it in a direction that is not aligned with the product brand's values, such as by engaging in antibrand behavior (e.g., [55]) or hostile behavior against others (e.g., stalking, bullying). Accordingly, product brands may be tempted to inhibit the expressive capacities granted to consumers. However, this reaction may cause the platform to become unresponsive to consumers' intense crowdsending ([52]), thereby reverting to an ad hoc relationship.Because the low crowdsourcing intensity increases consumers' focus on the brand and its core offerings, the potential to expand into the broader category space, as in capitalizing relationships, is limited. This limitation bears the risk that consumers feel constrained by the brand corset, and the platform may attract (only) product and brand enthusiasts rather than a more diverse crowd. Take Lego Ideas as an example, which allows consumers to create and share own designs, but which is firmly tied to the core product of Lego bricks and its buyers. Thus, an open and inspiring interest community may revert to a more specialized (brand) community ([28]). This issue may further aggravate as in-group members tend to seclude from out-group members ([78]), which could result in the latter leaving the platform or refraining from joining it in the first place. The focus on the brand's core offering and its existing customers may also render the platform less attractive for third-party businesses and reduce positive network effects ([53]). The platform crowd may thus become smaller and less diverse. P8:  If the brand's main objective is to strengthen engagement with its core offering, it is optimal to foster catalyzing relationships on the brand flagship platform. P9:  Higher consumer crowdsending intensity on the brand flagship platform increases the risk of platform hijacking. P10:  Lower consumer crowdsourcing intensity increases the risk of attracting a (a) smaller and (b) less diverse crowd.In nurturing partnerships, platforms enable high-intensity crowdsourcing and -sending by assembling a variety of building blocks and by extending value creation into the category space spurred by consumers' higher-level goals, thereby fostering self-relevance. The resulting combination of simultaneous self-expansions and self-extension experiences has profound implications for the relationship. The platform becomes an integral part in the consumer's identity project by being not only a resource to be used ([ 7]) but also an actual partner cocreating the consumer's self just as much as the consumer cocreates the platform offering. In this way, brands may achieve what was beyond their reach in traditional nonplatformized business models: a profound embeddedness in consumers' identity and lives ([ 7]; [13]).Take a user of an athletics platform who is spurred by the higher-level goal of living a healthy and active life. By following a chosen training schedule and participating in weekly challenges (crowdsourcing), their self expands, and they increasingly identify as an active and healthy person. By sharing their performance and achievements, forming a running group, and giving recommendations to others (crowdsending), the user then also proclaims and exerts their new, expanded self. This means that by extending their expanded self into the platform, the consumer is affirming and reinforcing it ([79]). Therefore, self-expansion combined with self-extension is likely to be more profound and lasting than self-expansion alone. As the platform becomes part of the consumer and vice versa, the relationship can achieve a new level of intimacy, attachment, and perpetuation of value (co)creation, leading to what we term ""hot loyalty."" Thus, brands can harness nurturing partnerships to build consumer relationships that are highly self-relevant, committed, and durable and that benefit multiple downstream brand goals relating to the extension of the offering, sales, engagement, and intelligence.However, nurturing partnerships also combine the risks of high-intensity crowdsourcing and -sending in terms of brand dilution and platform hijacking. In addition, highly self-relevant relationships have been shown to elicit strong adverse consumer reactions when a critical incident occurs, even provoking antibrand behavior such as spreading negative word of mouth ([41]). Thus, if goals in nurturing partnerships should become misaligned or perceptions of exploitation emerge, consumers' ""hot loyalty"" may boil over, causing a sudden and lasting dissolution of the relationship.Nurturing partnerships also tend to be demanding and costly to maintain because brands need to manage third-party businesses as well as high-intensity consumer crowdsourcing and -sending. Their interactions may create new challenges such as complementors defecting with consumers ([88]), and consequently, the value captured from the platform—monetary or otherwise—may be insufficient to justify its continuation. A case in point is Under Armour, which recently discontinued parts of its brand flagship platform Connected Fitness to focus on a more concentrated set of functionalities ([61]). P11:  If a product brand's main objective is to establish consumer relationships that are highly (a) self-relevant, (b) committed, and (c) durable, it is optimal to foster nurturing partnerships on the brand flagship platform. P12:  Higher consumer crowdsourcing and crowdsending intensities on the brand flagship platform increase the risk of (a) conflicts causing a lasting relationship dissolution and (b) the incurrence of high operational costs.Our discussion of relationship states clarifies that although it may seem intuitively desirable for brands to reap the full benefits of intense consumer crowdsourcing and -sending activities, aiming for nurturing partnerships poses specific risks and requires high conviction to overturn the traditional product-centric business model. This strategy is not ideal for all brands, and not every product category is suitable to elicit intense crowdsourcing and -sending. We discuss these aspects next and propose directions of necessary managerial rethinking for brands aiming to create their own flagship platform. Implications and Research Agenda Theoretical ImplicationsBrand flagship platforms are flexible vessels that harness the latest evolutions in platform-based business models. Drawing on our analysis of these platforms, we offer three sets of extensions to conceptual thinking. First, we extend the notion of sourcing from consumer crowds, as prevalent in the crowdsourcing and community literature streams ([30]; [46]), to a platform context, where consumers crowdsource value from a pool of offerings supplied by other consumers, third-party businesses, and the platform owner. We integrate this notion, with its flipside of crowdsending, to an overarching theory of consumer value creation in platform-based business models.Second, our approach spans the boundaries between different types of digital platforms discussed in literature and observable in the marketplace. Our assemblage model generalizes across diverse platforms such as exchange markets or online communities and accommodates a wide range of consumer benefits derived from platform offerings. In extension to the typology developed in [57], which classifies exchange markets into discrete types (forums, matchmakers, enabler, and hubs), the assemblage model suggests that brand flagship platforms may unify several or even all these types under one roof, while also integrating other, noncommercial interactions. Our conceptualization thereby also expands the usage of assemblage theory to study consumption and market phenomena (e.g., [60]) by applying it to the context of digital platforms.Third, we introduce a relational perspective to the theory of platform intermediation. Prior research emphasizes transaction cost advantages and efficiency gains of digital platforms based on network effects ([42]). Our analysis reveals that this rational view, which culminates in relationships of ""cold loyalty"" and the pursuit of immediate consumption goals, paints an incomplete picture. Consumer–platform relationships may evolve toward ongoing and versatile interactions that target consumers' higher-level goals and elicit self-expansion and self-extension experiences. While product brands are in a good position to build these kinds of relationships, the mechanisms carved out in this article generally also hold for other platform owners across myriad industries. Managerial ImplicationsThe platformization of the digital space has put substantial pressure on traditional product brands as competition intensifies and the role of the brand diminishes ([25]). However, our analysis shows that the poison can also be the antidote: product brands can carefully assemble their own flagship platforms to loosen aggregation platforms' grip and reclaim direct consumer access—that is, if decision makers successfully rethink long-established viewpoints and behaviors, which we discuss next. Rethinking markets: From products to category spacesWith brand flagship platforms, product brands can expand beyond their core offering and into the broader category space. Operating a flagship platform, especially with ample consumer crowdsourcing opportunities, will thus shift brands' locus of value creation from selling (somewhat standardized) merchandise to offering a virtually endless variety of project solutions that consumers can mix and match to integrate in their discrete or ongoing goal pursuit.Three steps are important to manage this shift: First, brands must decide on the ground they want to cover. While consumers define the boundaries of the category space, brands fix how far and how fast they want to move away from their home turf. Following literature on brand extensions and resource-based advantage, we suggest that brands address those higher-level goals that fit their positioning and resource configuration. A conservative approach would be to advance in concentric circles by gradually expanding the brand's core offering (e.g., an athletics brand may initially provide guided runs instead of leaping into nutrition). This approach would also give the brand time to build the required infrastructure and additional capabilities, while bearing the risk of competitors occupying larger parts of the category space more quickly through bolder expansion strategies.Second, brands need to decide which brand goal(s) to pursue and align their platform assemblage accordingly. Not any goal can be achieved with any combination of building blocks, and forcing brand goals on a specific assemblage will likely have dire consequences. For example, aggressive selling on platforms with a strong benchmarking block forces the source of value for consumers to reduce or even collapse. Furthermore, if brands do not consider how each building block creates crowd-based value, their goal attainment on the platform is inhibited, and consumers may look for alternatives. For example, to achieve a strong brand presence, product brands may be inclined to stack building blocks closely around their core business. In this regard, turning a versatile platform community into a narrow brand community could be just as detrimental as curating inspirational content to ban competitors. Such excessive control would limit the platform's ability to expand into the broader category space.Third, the brand needs to decide what parts of the offering it intends to ""make,"" ""buy"" (from third-party businesses), or ""earn"" (from consumers). While own production is the default for product brands, occupying entire category spaces implies that offerings will increasingly lie outside of their core business. To expand effectively and efficiently, managers must integrate third-party businesses and stimulate supply from consumers. Again, brands should not be afraid to give up (some) control, as creating large parts of the offering in-house is costly and leads the crowdsourcing and -sending concepts ad absurdum. Rethinking measurement: From brand performance to interaction qualityJust as focus needs to shift away from the core offering, key performance indicators (KPIs) need to evolve beyond product sales. Instead, the value created and appropriated through crowdsourcing and -sending should be measured holistically, with a broader system of KPIs. This system should capture at least five facets of value: ( 1) value to the brand from its products, ( 2) value to the brand from intermediation, ( 3) value to third parties, ( 4) value to consumers from crowdsourcing activities, and ( 5) value to consumers from crowdsending activities.Brand flagship platforms are unique in that they are built around a product-based business core and owned by a brand that ultimately aims to strengthen this core ([89]). Thus, a suitable measurement system would augment rather than replace traditional product KPIs such as sales and market share. Additional indicators that reflect value to the brand from intermediation may comprise direct earnings from operating the platform (commissions, subscription fees, ad revenues) and extend to engagement metrics such as feedback and word-of-mouth activities by third-party businesses and consumers. Brand managers could also learn from Netflix or YouTube, which largely focus on time spent with their services as a key success measure ([69]). Finally, brands should integrate measures of interaction quality through short, automated surveys, reciprocal feedback systems (such as those used by Airbnb or Uber), and tracking of interactions such as the rate of completion of content or service consumption (e.g., training session dropout).To measure value to third parties and consumers, individual-level or segment indicators can inform effective lifetime management. Brands need to track, for example, customer growth; new product, service, and content listings by third-party businesses; consumer usage and ratings of platform offerings; and platform access frequency and recency. One idea to structure all these measurement activities would be to classify them into platform building blocks such that brands can better identify white spots in their offering.With the expansion of the brand's role, optimization objectives expand as well. Brands need to steer away from their sales maximization logic and toward keeping all platform participants happy (Van Alstyne, Parker, and Choudary 2016). Accordingly, managers may have to balance conflicting interests between different value objectives (e.g., Do I integrate an open feedback system to increase crowdsending intensity or would that counter my core brand objectives?). This stretch is uniquely difficult for traditional brands owing to the double burden of being offline born and having to entertain a profitable product business ""on the side."" Our relationship typology can point managers in the right direction, but brands need to be aware that they cannot have their cake and eat it too: brand flagship platforms are doomed to fail if immediate value to the core products always trumps all other measures. Rethinking resources: From manufacturing to orchestratingContrary to common wisdom, brands aiming to build their own flagship platform need to commit to substantial investments and organizational restructuring. Manufacturing brands must, for example, develop market research expertise to elicit consumers' higher-level goals, set up a partner management to integrate third-party solutions, and develop user experience and engagement skills. Contrary to pure players, brands must juggle both industrial production and digital intermediation, which require different skillsets and infrastructure.This latter point pertains also to resources that leverage the exciting dynamics for which flagship platforms provide an ideal playground. Owing to the shift from sequential purchase journeys (characterized by so-called ""loyalty loops"" that describe a predictable cyclical purchase pattern) to an ever-evolving and highly individual platform experience, brands have the unique chance to establish so-called ""involvement spirals"" ([71], p. 45). Perpetuating consumer crowdsourcing and/or crowdsending activities can create unpredictable experiences at every interaction and thereby motivate ""increasing experiential involvement over time"" ([71], p. 46). To this end, brand managers who are used to sequential product life cycles will need to provide the resources, staff power, and organizational setup to adopt an agile approach.Despite these challenges, product brands possess numerous existing assets and capabilities that level the seemingly skewed playing field. Contrary to pure platform players, brands can often rely on an engaged customer base, strong industry ties, expertise in their category, and an assortment of fresh and enticing products to offer. Managers ought to capitalize on these resources, for example, by using their brand power and partnerships to negotiate exclusive supply deals, to jumpstart crowdsourcing and -sending, and to propel strong network effects (Van Alstyne, Parker, and Choudary 2016). While brand aggregation platforms may be unsurmountable in their ability to realize efficiency gains, their Achilles' heel is the lack of tailored consumer experiences and credible expertise within a specific category space. And this is exactly where brand flagship platforms excel. Avenues for Future ResearchTo derive future research avenues in the emerging field of brand platformization, we build on open issues that arise from our theorizing and classify these using a survey of 72 marketing managers in the consumer goods industry. We asked participants to evaluate important topic areas relating to key platform decisions, processes, and outcomes. Managers rated each area in terms of its novelty and current relevance for the success of their brand (for details, see Web Appendix C). We then mapped their answers onto a two-by-two matrix (Figure 4).Graph: Figure 4. Managerial novelty and current relevance of topic areas (manager survey). Notes: The different shapes/colors indicate whether a topic area relates to decisions, processes, or outcomes. Platform building blocks are a subset of the decision-related topic areas and consumer-platform relationship states are a subset of the outcome-related topic areas.In niche areas, researchers must identify specific market conditions under which topics are important and interesting to craft a contribution. Evergreens promise profound impact, as they represent lasting conundrums that practice and research have not or not sufficiently tackled thus far. In emerging areas, research opportunities are plenty, and although results may not be of immediate practical relevance, they can shape future business practices that put brands at the forefront of innovation. New key challenges promise researchers strong and immediate impact due to their high degree of novelty and managerial relevance. We add this classification to our extensive research agenda presented in Table 3. Next, we highlight a few key challenges and emerging areas that may inspire future inquiries, structured along our decision–process–outcome framework.GraphTable 3. Research Agenda. Topic AreaResearch QuestionsClassificationDecision-RelatedIndividual building blocksHow can brands incorporate different types, sources, and presentation modes of inspirational content? How should brands elicit and leverage consumer inspiration? And to what degree does inspirational content foster more intense crowdsending?How can brands mitigate possible negative platform experiences when consumers are not progressing or even regressing as part of self-tracking through the benchmarking block?How can brands provide guidance on the platform while mitigating the risk of harmful outcomes (e.g., eating disorders from meal recommendations, injuries from a mismatched workout plan)?How can consumers be steered from a brand community to an interest community?Emerging area, new key challenge, evergreenAssembling building blocksHow do building blocks interact with each other? How can products, services, and content be made searchable and engaging so that building blocks become better connected and provide networked value?How do existing brand associations and attitudes affect the optimal assemblage of building blocks?Which building blocks are required to elicit a specific relationship state and which are optional?New key challengeThird-party integrationHow do insights from brand extensions, cobranding, and coopetition transfer to value-creation extensions through third-party integration?Which parts of the value creation should brands ""make"" versus ""buy""?Which circumstances (e.g., brand strength, market concentration) promote opening up to competitors?How much competition between suppliers, and between suppliers and the brand, is optimal?How to identify suitable third-party partners? Should brands enter exclusive contracts only? And how can the brand ensure that brand values and standards are reflected by third parties?Niche areaConsumer integrationHow relevant is the size and makeup of the existing customer base for brands that platformize?Which parts of the value creation should brands ""make"" versus ""earn""?How can brands acquire new consumers beyond its existing customer base?Which consumer characteristics determine their willingness to engage in intense crowdsending? How can these consumers be identified and targeted?How does compensation of consumer-created value affect the platform and its consumer relationships?Emerging areaBrandingDoes the joint creation of consumer experiences on flagship platforms diminish brands' importance and dilute their image? And does the shift away from product-centricity harm especially traditional manufacturing brands? Is this development moderated by the intensity of crowdsourcing and -sending increasing the relevance of the brand in high intensity contexts?How does our knowledge from brand extensions and cobranding transfer to platformized brands? Specifically, how is brand positioning in this context being impacted?In which direction and how far can brands expand into a category space without harming their reputation? And how can brands mitigate this through careful platform rebranding?How prominently should the brand be featured in interactions with consumers, among consumers, and between consumers and third parties?New key challengeProcess-RelatedConsumer governanceHow much empowerment versus restriction is optimal under which relationship state? How does this vary by building block?Which individual consumer–supplier relationships occur as part of the platforms subassemblages? How do they affect the platform at large?How do brand flagship platforms affect collectives (e.g., families) rather than individual consumers? Should the platform account for collectives in their value-creation process? Should they focus on providing solutions for individuals or collectives?How open does the platform need to be to engage consumers but not hurt its own brand goals?Emerging areaThird-party governanceHow much openness versus restriction is optimal under which relationship state? How does this vary by building block?How should platforms be shielded against ""hostile takeovers"" or third parties acquiring new customers through the platform and then defecting with them? Is the threat of hostile takeovers less pronounced in more intimate consumer–platform relationships?How should the platform brand mitigate adverse effects through service failures by third-party brands?Emerging areaFostering consumer interactionsHow can game design inform the creation of mechanisms to foster interactions? How effective are gamified mechanisms in fostering consumer interactions depending on the relationship state?How can consumers be transformed from passive crowdsourcers to active crowdsenders? Which crowdsending activities are more and less rewarding for consumers? Are there consumer segments motivated by distinct factors that increase their engagement in crowdsending?Which boundaries should the platform obey to avoid feelings of intrusiveness or exploitation?What detrimental consequences could fostering consumer crowdsourcing and -sending activities have on consumer behavior and well-being?New key challengeOutcome-RelatedIndividual relationship statesWhich brands specifically are able to evolve to providing nurturing partnerships? And how far can brands venture into the broader category space before they are stretched too thin?How far can a platform advance the relationship before consumers respond with reactance? How to identify consumers that are receptive to a nurturing partnership? And which consumer characteristics predict this receptiveness?How can brands intervene to elicit the desired relationship state? Which brand factors (e.g., brand strength, positioning) influence which relationship state(s) manifest?Can brands develop a market to evolve instrumental ad hoc relationships toward relational states?Emerging area, evergreen, niche areaRelational outcome measurementHow can we measure relational outcomes and the role of the platform in consumers' goal pursuit?How do we need to extend existing brand attitude scales?How can we use behavioral big data to infer consumers' goal pursuit and relationship state?How can brands measure and attribute customer engagement value?EvergreenMonetizationHow can platforms monetize relationships without scaring off consumers?How does monetization affect relationships with consumers?When should brands employ subscription models over pay per use?What is the monetary value of the data and insights generated through the platform?How do consumer privacy rules play out? How should privacy be balanced with monetization goals? How does this vary by relationship state?EvergreenBusiness model transformationDoes the conceptualization extend to born-digital brands? Are they born with a platform mindset or do they revert to a pipeline approach?Which brand, company, and market characteristics determine a successful brand platformization?Which brands should platformize, and to what degree? And which brands should assume a platform complementor role?How to transform: with a ""big bang"" or gradually over time? And what are contingency factors?Evergreen  Decision-related avenuesHow to assemble elements of consumer inspiration garners strong managerial interest but is only sparsely researched ([15]). Further studies could examine different types (e.g., integrated narratives vs. open sequences, professional vs. improvised), sources (e.g., consumers, designers, product brands), and presentation modes (e.g., music, picture, video, text) of inspiring content and study how to optimally integrate these in the platform assemblage. Research might also expand our conceptualization of building blocks by exploring whether and how the weaving together of building blocks gives rise to positive or negative interactions. For example, creating a strong inspiration block alone may elicit less intense crowdsourcing and -sending than if paired with community features that allow for collaborative idea development. Lastly, managers view the role of branding on flagship platforms as a key challenge to address. Important questions that arise are whether the joint creation of consumer experience on flagship platforms diminishes brands' importance and dilutes their image, and whether the shift away from product-centricity could harm traditional manufacturing brands in particular. However, increasing crowdsourcing and -sending intensities could also reinforce brands' importance, as they may become the connective tissue that embodies and communicates the platform's higher-level goal. These perspectives complement the work by Swaminathan et al. (2020), who argue that the importance of brands in access-based, instrumental consumption contexts—characteristic of the ad hoc relationships we describe—may diminish. Research could examine these issues, using relationship states as relevant moderators to branding outcomes. Process-related avenuesWith platformization being uncharted territory for most brands, several issues emerge that concern the governance of flagship platforms. For example, how fast and to what degree should the platform open up to consumers and third-party businesses? And how does the optimal degree of openness vary by building block? Governance issues may also pertain to the individual subassemblages that emerge on the platform. That is, while our discussion focused on consumer–platform interactions, it would be fruitful to explore the individual subassemblages emerging from consumer–supplier interactions and their consequences for the platform. For example, consumers may build close relationships with third parties that may even culminate in defection from the platform ([88]). Thus, future research should explore how brands can monitor consumer–supplier relationships and deploy governance mechanisms that prevent such exploitative behaviors. Researchers should also consider the moderating role of relationship states, as exploitative behaviors may be more common in the ad hoc consumer–platform relationships studied by [88] but less likely in intimate consumer–platform relationships. Future studies might also extend our conceptualization beyond the individual consumer to collectives such as families and groups of friends as relevant actors on the platform ([60]). Increasing embeddedness in consumers' lives makes the platform more likely to affect their immediate social environment, raising the question of how interactions and interventions may affect such collectives rather than individuals.Fostering consumer interactions reflects a key challenge that researchers can explore on different levels. For one, it would be important to garner insights about coherent incentive systems, for example, based on gamification approaches, to spur consumer crowdsourcing and -sending. Gamified mechanisms may prove especially useful for intimate relationship states in which monetary incentives are ineffective or even detrimental as consumers tend to apply less exchange-focused norms ([ 2]). In addition, gamified mechanisms can create unpredictable experiences that dynamically adapt to consumers' evolving skill levels and keep them engaged over extended periods of time ([71]), making them well suited for more intimate relationship states. Thus, future research could analyze the effectiveness of gamified mechanisms conditional on relationship states. The optimal degree of brand intermediation of these efforts also warrants further investigation. Outcome-related avenuesThe emerging interest in catalyzing relationships and nurturing partnerships suggests that platform thinking in terms of ""buying"" and ""earning"" products, services, and content is still new to most managers. Accordingly, many important questions have yet to be answered. For example, which kinds of brands will be able to create nurturing partnerships, and how far can brands venture into the broader category space before they are stretched too thin? And in turn, up to which point do consumers allow brand flagship platforms to conquer their daily lives and cocreate their identities before responding with reactance to regain sovereignty? Such reactance is one of many (de)stabilizing processes that may cause relationship states to change over time. While we describe some of these processes, scholars could analyze in detail which specific interventions can transform a relationship into an (un)desired state.Finally, although our discussion focuses on offline-born product brands, future businesses will be born into a digital world. Research should explore how our conceptualization extends to these brands and whether they internalize platformized thinking, seek their salvation in the role of platform complementors, or even revert to the more traditional linear approach. Because the digital platformization of industries is here to stay, brands will be required to take a stance. "
40,"The Roar of the Crowd: How Interaction Ritual Chains Create Social Atmospheres Atmospheres are experiences of place involving transformations of consumers' behaviors and emotions. Existing marketing research reveals how atmospheric stimuli, service performances, and ritual place-making enhance place experiences and create value for firms. Yet it remains unclear how shared experiences of atmosphere emerge and intensify among groups of people during collective live events. Accordingly, this article uses sociological interaction ritual theory to conceptualize ""social atmospheres"": rapidly changing qualities of place created when a shared focus aligns consumers' emotions and behavior, resulting in lively expressions of collective effervescence. With data from an ethnography of an English Premier League football stadium, the authors identify a four-stage process of creating atmospheres in interaction ritual chains. This framework goes beyond conventional retail and servicescape design by demonstrating that social atmospheres are mobile and cocreated between firms and consumers before, during, and after a main event. The study also reveals how interaction rituals can be disrupted and offers insight as to how firms can balance key tensions in creating social atmospheres as a means to enhance customer experiences, customer loyalty, and communal place attachments.Keywords: atmospheres; emotions; events; place; rituals; sport; emotional energy; entrainment‌Atmospheres are experiences of place involving transformations of consumers' behaviors and emotions. When marketers get atmospheres right, firms benefit from enhanced customer experiences, loyalty, and place attachment ([ 9]). Given the value of atmospheres, it is fitting that research explains how firms can create atmospheric effects using collages of material objects ([ 4]; [44]; [73]). These stimulate consumers' senses, emotions, and behaviors in retail sites ([ 7]; [49]; [52]; [72]). Likewise, in themed stores, material stimuli create spectacular places that prime play and pleasure ([ 8]; [69]).Marketing research also explains that experiences of place are socially constructed during interactions between staff and customers ([ 3]). Staff can enhance customer experiences of place by educating consumers about meanings of locations and by performing cultural scripts to elicit emotional responses ([ 3]; [22]; [62]; [69]). When service performances are enacted alongside atmospheric stimuli provided by material design, firms can transform places into ""a separate and self-contained world"" ([47], p. 662). For example, [23] show how opulent design and staff performances create the intimidating atmosphere of luxury stores.Prior research has explained how firms can generate atmospheric effects through material and social constructions of place. Yet in contexts as diverse as nightclubs, festivals, and themed stores, consumers share atmospheric experiences in groups ([ 3]; [32]; [45]; [69]). Despite acknowledging the value of ""boundary open"" feelings of togetherness among consumers ([ 3]), marketing research has yet to explain specifically how group experiences of atmospheres are created. Thus, our primary research question asks, what are the social processes that create atmospheres?Clues as to how these atmospheric interactions are created can be gathered from studies that show consumers cocreating place experiences ([ 3]; [65]). At historical reenactments, festivals such as Burning Man, and brandfests, cohorts of consumers act out consumption rituals ([ 5]; [45]; [53]). Over time, repetitions of these ritual processes can imprint spaces with a convivial ""spirit of place,"" an identity that guides consumers' interpretations, emotions, and behaviors ([10]; [37]). Once instilled, these socially constructed qualities can endure, creating lasting place attachments ([ 9]; [67]).In contrast to these enduring qualities constructed over time, however, social experiences of atmospheres can emerge and dissipate more rapidly ([14]; [38]). In places such as stadiums, nightclubs, religious sites, and street parades, for example, atmospheres often intensify and climax in pleasurable outbursts of shared emotion and behavior ([32]; [39]; [45]). As a valued aspect of collective live events, it is important to understand more about how ritual interactions create these shared intensifications of emotion and behavior that consumers value during atmospheric experiences of place.This is especially the case given that consumer groups are often heterogeneous in nature ([53]; [77]), and misaligned values and behaviors can inhibit rituals, potentially spoiling atmospheres ([37]; [68]). Indeed, prior studies have recognized the necessity for managers to balance divergent expectations and behaviors in places cocreated by multiple actors including firms and consumers ([10]; [47]). To enable firms to benefit from creating social atmospheres and avoid failures of atmosphere, therefore, we ask two further research questions: How do atmospheres fail, and how can the social dimensions of atmosphere be managed?To investigate how social atmospheres are created, how they fail, and how they can be managed, we next explain our theoretical lens and research procedures. We then present our findings, which reveal a four-stage process of creating ""social atmospheres"" in interaction ritual chains, and how social atmospheres can fail. Our discussion explains how these findings extend theories of material atmospherics, servicescapes, and ritual place-making by showing that social atmospheres are cocreated by firms and consumers in multiple sites before, during, and after a main event. We extend interaction ritual theory by explaining how atmospheres can be mobile and how learning enables social atmospheres. Finally, we offer advice for managers on how to create social atmospheres that enhance customer experiences, customer loyalty, and place attachment. Interaction Ritual ChainsTo understand the social processes by which atmospheres are created and how atmospheres can be managed, [15] theory of interaction ritual chains enables us to explain how shared emotions and behavior are created during collective live events. Interaction ritual chains are a ""mechanism of change"" ([15], p. 43) that aligns attention, behaviors, and emotions among group members. Interaction rituals begin when a symbolic object or action gathers a shared focus of attention, transforming individuals into ritual participants and transforming the atmosphere as a result. For example, when a conductor taps their baton on the music stand, the sounds of conversations and musicians warming up are replaced with an anticipatory silence.If a group shares a common focus, then behaviors and emotions tend to align among ritual participants due to physiological predispositions to imitate others ([38]). To describe these alignments of behaviors and emotions, [15] provides the concept of ""entrainment"": the stimulation of common emotional responses is labeled ""emotional entrainment""; the stimulation of common movements and vocalizations are labeled ""behavioral entrainment."" Consider the coordinated actions among a crowd at a tennis match as they follow the ball back and forth, for example, and how shared excitement intensifies the atmosphere.[15] also enables us to explain how atmospheres climax. When crowds of entrained people generate expressions of their shared emotions, such as common vocalizations, gestures, and movements, Collins explains that ritual participants become aware that they are ""doing the same thing"" and ""thinking the same thing"" (2004, p. 33). During these instances, interaction rituals reach fever pitch. Here, Collins follows sociologist Émile [25], p. 424), who coined the term ""collective effervescence""' to describe moments during rituals when participants ""become hyper excited, the passions more intense, the sensations more powerful.""Social atmospheres encompass shared focus, entrainment, and collective effervescence, but it is particularly the latter that produces ""emotional energy,"" a pleasurable feeling of group membership that motivates participants to repeat rituals ([15]). Importantly, the symbols that groups use during expressions of collective effervescence become representative of these pleasures. Collins suggests symbols used in interaction rituals can act like ""batteries,"" storing emotional energy. As such, these charged symbols remind people of prior experiences and can be used as focal points to start future interaction rituals, linking rituals together in ""chains.""Notwithstanding the enabling aspects of Collins's theory, it is not clear how group dynamics impact focus, entrainment, and collective effervescence ([28]). Although entrainment is stimulated by physiological mirroring, for instance, further investigation of how groups moderate alignments of behavior and emotion is warranted in light of our interest in community heterogeneity. This is particularly so in instances where consumers seek different emotional experiences at the same event ([19]; [37]).Furthermore, [15] analyzes discrete interaction rituals that are enacted in particular places. Yet consumer research shows that place experiences can be anticipated by preparatory activities ([10]; [32]), and that consumers move from place to place during consumption experiences ([41]; [75]). Illustrating both possibilities, [11], p. 213) explain how preparations enacted by groups outside a stadium leave people ""primed"" and ""psyched"" for the game to follow. Despite these observations, it is not clear how group activities in one place might impact atmospheres in another. It is with these foregoing questions in mind that we describe our context for this investigation. Research Context: Anfield StadiumTo understand how social interactions influence atmospheres and how these can be managed, we focused on the context of English Premier League (EPL) football stadiums. EPL stadiums are an ideal context to investigate atmospheres because these sites are associated with active and passionate supporters who cocreate the visual, sonic, and emotional qualities of these places. The EPL acknowledges that consumers ""create an atmosphere that sets us apart from other leagues and competitions"" ([63]). Moreover, industry leaders agree that these atmospheres generate commercial value for the EPL ([26]; [56]).In keeping with interpretive marketing research studies of place ([22]; [47]; [69]), we focused on an exemplar case: Anfield, home of Liverpool Football Club (LFC), one of the wealthiest football clubs globally, and part of the Boston-based Fenway Sports Group. Anfield is considered one of the EPL's ""most atmospheric stadiums"" ([36]). Inside Anfield is the ""The Kop"" stand; this is where the ritual singing common in English football originated in the early 1960s, when fans began to sing the popular hit, ""You'll Never Walk Alone"" (from Rodgers and Hammerstein's Carousel). Ever since, the Kop has remained a bastion of impassioned vocal support ([42]), and this song has become the official club anthem.Notwithstanding Anfield's reputation as an atmospheric place, EPL atmospheres are not always lively. Particularly in recent years, some consider that atmospheres have become flat and lifeless ([50]). Moreover, EPL stadiums represent contexts that require a variety of interventions to manage atmospheres. In the 1980s, two stadium disasters involving Liverpool supporters highlighted the risks of highly emotional crowd contexts: Hillsborough, where overcrowding led to 96 deaths, and Heysel, where crowd disorder led to 39 fatalities. These events motivated safety alterations at all EPL stadiums, ensuring that supporters remain seated and subject to security supervision ([43]). These aspects of our context provide opportunities to understand more about how atmospheres fail and how firms attempt to manage atmospheres. Methods and Data AnalysisWe investigated how Anfield's atmosphere is created during fieldwork spanning seven years. Our battery of ethnographic techniques encompassed participant observation, interviews, as well as archival and online data collection ([ 3]; [46]; [69]). The data we collected capture supporters' experiences and interactions within Anfield, as well as in the spaces surrounding Anfield before and after matches ([76]). Our data set is summarized in Web Appendix 1 and comprises field notes, 60 in-depth interviews, and 4 audiencing interviews. Together these activities generated 410 pages of single-spaced text, 610 newspaper articles, 53 photographs, and 17.5 hours of video recordings. Participant observationTo build a ""thick description"" of atmospheric experiences at Anfield, participant observation ([30]; [41]) occurred before and after matches during two consecutive EPL seasons, from November 2012 to August 2013 and from November 2013 to August 2014. In November 2012, the first author rented an apartment close to Anfield, and access to participants was facilitated by a local supporter who acted as a gatekeeper. This access enabled observations in supporters' homes, during social gatherings, on public transport to games, in pubs, and on the streets outside Anfield. In 2013 and 2014, the first author continued these procedures and attended 24 LFC matches: 18 at Anfield, and 6 at other EPL stadiums. Access to Anfield enabled the description of microinteractions between individuals as well as crowd-level expressions ([31]). To record these firsthand experiences of atmospheres, digitized field notes were compiled from observations, photographs of supporters' visual displays, and video footage recorded by the first author. In-depth interviewsAccess gained during the 2012–2013 season allowed for 60 in-depth interviews between November 2012 and March 2021. Interviews sought insight into supporters' life histories, the meanings they associate with Anfield, and their experiences of atmospheres. EPL stadiums provide an ideal instance of collective events characterized by consumer heterogeneity. To reflect this, we purposively sampled devoted supporters with local ties to LFC as well as casual supporters and tourists. Table 1 shows our sample, which represents a typical Anfield crowd by age, gender, and commitment level. To enhance insight into the creation and management of atmosphere we also sampled police, stadium architects, crowd safety experts, journalists, and sports-marketing consultants. Interviews began with grand-tour questions ([54]) about experiences of atmospheres at Anfield, as well as supporters' pre- and postmatch activities. Interviews lasted between 45 minutes and 3 hours; all were recorded and transcribed.GraphTable 1. Participants. Participant TypePseudonymApproximate Age (Years)CommitmentInterview TypeSupportersAmanda (F)40sCasualDepthJune (F)50sDevotedDepthMark (M)50sDevotedDepthMartin (M)30sCasualDepthTim (M)20sCasualDepthTommy (M)50sDevotedDepthAdam (M)20sDevotedDepthDavid (M)50sCasualDepthBrian (M)60sDevotedDepthAnne (F)60sDevotedDepthGraham (M)50sDevotedDepthChris (M)20sDevotedDepthAdam (M)20sDevotedDepthSeb (M)30sCasualDepthBill (M)30sTouristDepthPaul (M)30sCasualDepthGary (M)60sDevotedDepthThomas (M)40sCasualDepthAnthony (M)30sDevotedDepthMichaela (F)20sDevotedDepthJames (M)30sDevotedDepthBarry (M)TeensDevotedDepthKeith (M)40sDevotedDepthAnne (F)60sDevotedDepthJoel (M)20sDevotedDepthJon (M)50sCasualDepthPeter (M)30sCasualDepthTony (M)70sDevotedDepthMick (M)40sDevotedDepthPeter (M)50sCasualDepthSteve (M)30sCasualDepthClive (M)50sCasualDepthSam (M)40sDevotedDepthJames (M)20sDevotedDepthLee (M)TeensDevotedDepthCraig (M)30sDevotedDepthDuncan (M)40sCasualDepthBrian (M)50sDevotedDepthSteve (M)20sDevotedDepthAndrew (M)20sDevotedDepthAndy (M)30sDevotedDepthDavid (M)40sCasualDepthJoe (M)20sDevotedDepthMike (M)30sCasualDepthJenny (F)20sTouristDepthSebastian (M)30sTouristDepthBen (M)20sTouristDepthJay (M)30sDevotedAudiencingGareth (M)30sDevotedAudiencingNev (M)50sDevotedAudiencingDan (M)30sDevotedAudiencingPoliceJim (M)40s—DepthReg (M)40s—DepthStadium architectsBrian (M)50s—DepthJohn (M)50s—DepthDaniel (M)50s—DepthJoe (M)30s—DepthCrowd safety supervisorsSteve (M)60s—DepthKeith (M)50s—DepthSports marketing consultantsTim (M)50s—DepthGeorge (M)40s—DepthJournalistsStuart (M)40s—DepthRory (M)30s—Depth 1 Notes: M = male; F = female. Audiencing interviewsEmbodied practices and multisensory experiences of place can be difficult for respondents to recall or describe ([38]). ""Audiencing"" is a video elicitation technique that uses recorded footage of participants interacting with the environment to evoke discussion of significant and specific sensations, emotions, and actions ([55]). Accordingly, we used this technique to explore how atmosphere is created in Anfield as well as supporters' multisensory experiences of atmosphere and to interpret the role of material aspects of place during these experiences. Four such interviews took place. Participants were shown video footage recorded by the first author of a recent match at which they were present. This procedure lasted between one and two hours. These interviews were recorded and transcribed. Archival and online dataData collection included newspaper coverage as a means to understand wider media representations of atmosphere, supporter behavior, and commercial/social/political issues related to EPL football stadiums. Using the LexisNexis archive ([24]), we collated documents referring to ""atmosphere"" in EPL contexts between September 2012 and August 2019 in national newspapers as well as a popular local newspaper, The Liverpool Echo. Finally, we monitored official LFC web platforms as well as social media where LFC supporters post content related to the club ([75]). Data analysisOpen coding began with authors independently identifying common patterns and sequential moments among interview cases, field notes, and photographs ([74]). These procedures guided our analysis toward a focus on understanding the social processes that create effervescent atmospheres as well as what causes atmospheres to fail. As research progressed, our analysis identified place-based and processual occurrences; key actors, places, and material objects; effects on consumers' emotions and behaviors; and aggregate social effects within crowds. Our data set enabled us to identify interconnected ritual stages before, during, and after supporters' visits to Anfield. Moreover, given that consumers return to Anfield and surrounding places week after week, these repetitions enabled us to identify a cyclical process of creating social atmospheres. Interpretations were triangulated across methods and authors, and member-checks were used to solve coding disputes ([78]). The final stages of analysis involved tacking between emergent themes and existing knowledge of atmospheres and experiential place-making to refine and extend theory ([12]). Web Appendix 3 presents the final coding. FindingsOur findings are organized to explain the process of creating social atmospheres through an interaction ritual chain that comprises four interlinked stages. Figure 1 illustrates how this process begins with preparations in private places, moves to activation in a service ecosystem, climaxes in an event space, and concludes with recovery in the service ecosystem after an event. Linking these stages are symbolic resources that enable consumers to transfer social atmospheres from one place to another, gathering focus, entrainment, and collective effervescence among increasingly large crowds on the way. Having unpacked this process, we explain how atmospheres can fail when shared focus is distracted or entrained groups are disaggregated.Graph: Figure 1. Creating social atmosphere in the interaction ritual chain. Preparation for AtmosphereAtmospheric preparations are consumer-led activities prior to events in which symbolic resources and behavioral expectations are created to enable atmospheres. These preparations begin in relatively private places such as homes and involve families and friendship groups as well as entrepreneurial consumers. We reveal three modes of preparation: learning about cultural expectations to participate, making symbolic resources, and rehearsing entrained behaviors. Learning to participateLearning to participate refers to experienced consumers teaching newer consumers to contribute to atmospheres. A sense of seriousness helps motivate participation in interaction rituals ([31]); Dan's first visit to Anfield as a boy was prefaced by ""a serious chat with my Dad, where he explained how special Anfield is."" Team affiliations among sports consumers are often inherited through families, where emotional and behavioral expectations are modeled. For example, Anthony describes how he taught his son about what going to Anfield means and the practices that supporters share there:I think the most important thing is teaching the younger ones what it means to be a Red. You have to let them know that football is a shared experience, and you go to the match for the craic [fun and enjoyment], the camaraderie, and emotion. When I took our youngest, both me and my Dad told him about the Kop and what it's like to be in the Kop, what it means.... Sam had grown up singing the songs Michaela and I would sing around the house when he was really little, so when we thought he was old enough to come with us, he could join in.Preparations teach consumers about the spirit of place ([69]), and the spirit of place at Anfield identifies expectation to participate in fun, friendship, and shared emotions, all of which encompass a moral responsibility to enact the rituals and traditions that identify and unite the LFC community ([59]; [65]). Making symbolic resourcesMaking symbolic resources involves the preparation of visual and aural symbols that represent consumer identity during interaction rituals. Dedicated supporter Chris and his friends create flags with motifs and slogans that represent the club: ""We meet up on Wednesday nights and plan for the next match, or start thinking about bigger, more complicated designs for a particular occasion."" Chris explains that these objects are meaningful for supporters: ""Some of them are dead historical and were made for a particular moment in time, something like Hillsborough."" Figure 2 shows these flags with visual motifs recalling Hillsborough as well as major cup victories: so valued are these resources that when they finally wear out after years of use and repairs, they are carefully replaced with a new version.Graph: Figure 2. Commemorative flags symbolizing Hillsborough and league victories.In addition to these visual symbols, supporters prepare aural symbols in the form of songs and chants. Dan explains that ""music and football culture are closely entwined,"" and in 2012 he organized ""Boss,"" an open mic night ""for everyone to come together, to sing ... and build camaraderie."" These events are opportunities for supporters to generate new songs. Dan explains that ""songs that are created in the moment in events such as these spill into the ground, one moment you're playing around with your mates and then next weekend there's thousands of people singing it.... You feel dead proud, knowing that that's something you've created."" LFC advertises these events, showing that firms can coproduce symbolic resources by partnering with ""passionate entrepreneurs"" like Dan ([34]). Rehearsing entrained behaviorsRehearsing entrained behaviors occurs before an event and involves individual consumers getting ready to participate in social atmospheres. Much like a sports team warms up before a game, supporters rehearse before matches. James explains that ""you've got to get in the mood beforehand."" Nev describes how he prepares himself for games by ""whistling and humming songs as I'm getting ready. My son likes to put on some YouTube videos, or some of those emotional montages that the television companies like to produce."" Online resources enable consumers to rehearse aural symbols at home, long before events begin. Both official LFC web media and supporter-run websites provide videos of excited crowds that inspire rehearsal. Given that not all supporters have family ties through which behavioral expectations are prepared, Andy, another passionate entrepreneur, explains that he founded the website ""The Anfield Wrap"" to ""help fans across the world to learn how not only to experience and take in the atmosphere, but to contribute."" To facilitate this outcome, Andy uses footage from inside the stadium that ""captures the spectacle we supporters put on, the sights and sounds that we produce."" Through these preparation activities, individual supporters familiarize themselves with songs and behaviors, such that the next stage of the ritual chain is made ready. Activating the AtmosphereWe define atmospheric activation as the creation of social atmospheres among smaller groups prior to a main event. This section first explains how groups of consumers activate shared focus and entrainment in a service ecosystem outside a main event space. Then, we explain that the social atmosphere intensifies when entrained groups gather into a larger crowd as they move toward an event. Finally, we highlight how entry into an event space can activate memories of prior atmospheric experiences through firm-generated objects. Activating focus and entrainmentActivating focus and entrainment transforms individual consumers into small groups that begin to create a social atmosphere. This occurs in the many hotels and pubs around Anfield stadium, a service ecosystem where supporters gather several hours before games. LFC devotee Gary's favorite pub is The Sandon, known as the birthplace of LFC in 1892. Gary returns here week after week to enjoy the same experience: ""It's always a good time in the Sandon.... You know the same faces will be there."" Studies of servicescapes note how consumers enjoy a collage of visual and sonic stimuli, which encourage sociality in small groups, much like the separate areas of ESPN Zone, a now-defunct sports-themed restaurant ([69]). In contrast, The Sandon is a bland, austere space. Yet as supporters arrive, the Sandon's managers allow them to pin their homemade flags to the walls. This act of ""place-marking"" transforms space into place and initiates dialogue among consumers ([79]).The staff at the Sandon also help supporters move tables and chairs to clear a stage for the next moment in the interaction ritual. [15], p. 119) explains that certain individuals embody ""symbolic resources"" that enable them to entrain the behaviors of those around them. Gary fulfils this role: as the Sandon reaches capacity, he moves to the center of the room. Smaller groups of families and friends halt their conversations and share a focus. Field notes from January 18th, 2014, recall how Gary sings the first line of a song to the tune of Boney M.'s ""Brown Girl in the Ring"": ""Poetry in motion,"" to which everyone responds, ""Tra-la-la-la-la!""Poetry in motion/Tra-la-la-la-la![repeat three times]We're the best football team in the land/Yes we are!We are Liverpool/Tra-la-la-la-la![repeat three times]We're the best football team in the land!/Yes we are!The call-and-response style enables supporters to begin entraining their behavior, breathing, and gestures, and physiological research suggests that even heartbeats can align among people who sing together ([58]). By sharing a focus and initiating behavioral entrainment, supporters start to build a social atmosphere hours before the game. Like other pregame rituals ([10]), these activities leave supporters like Gary pumped up with ""emotional energy"" ([15]): ""It's the best pub to get you up for the game, it's noisy and no other pub can prepare you better to stand and sing your heart out on the Kop."" Intensification through collective mobilityAt some stage, consumers leave the service ecosystem and enter public spaces. Here, social atmospheres intensify as crowds gather and move toward an event space together. An hour before the game begins, supporters leave the pubs and join what some call ""the pilgrimage,"" a procession through the streets toward Anfield. Stuart describes this as a ""sea of people who descend at the same time on the most cherished symbol of the club."" Sports and music venues are often likened to religious sites ([10]; [69]). Mark explains ""there is nothing like being part of a crowd heading to the ground. Even though you don't know those around you, you feel like you're part of a gang, a religious group."" Much like the shells carried by pilgrims on the Camino de Santiago ([41]), supporters' flags act as a mobile focus, helping gather people together along the way. Outbursts of laughter and singing light up the street like brushfires as groups of LFC supporters are united in the streets that lead toward the stadium.Social atmospheres intensify when smaller groups are united into larger crowds. Like other appropriations of public space ([10]), thousands of supporters gather outside Anfield stadium to take part in what they call the ""Welcoming Committee"" that greets the LFC team bus as it arrives. Chris explains that this greeting was organized by ""Spion Kop 1906,"" a group of devoted supporters that wanted ""to do something outside the stadium to get people up for the match."" Because of the presence of the team on board, and its proximity to the crowd, the bus is treated as a sacred object ([ 6]). As such, the bus provides another shared focus that gathers groups into an increasingly dense, entrained crowd. As the bus moves through this crowd, the streets are filled with pyrotechnic flares, smoke bombs, and flags.The crowd grows.... Police remind people to not block the road. People climb on walls, encroach in people's gardens, mount scaffolding, and get on each other's shoulders. They sing, chant, fly banners and set off red smoke bombs. When the Liverpool bus arrives, it's impossible to see where the crowd starts or ends. As the bus creeps up Anfield Road at a snail's pace, more smoke bombs are set off, beer is thrown, and the crowd surrounds the vehicle, hitting it in support. (Field notes, February 10, 2014)Figure 3 shows the chaotic scene of revelry that supporters create. The visual display of red and white flags and smoke is accompanied by the smell of beer and diesel fumes, the feeling of bodies pressed together, and the sounds of chanting, singing, and fists striking the bus. Members of this crowd become entrained to the excitement: the release of one flare or smoke bomb hastens the next; as one person chants, hundreds of others join in, producing outbursts of collective effervescence ([25]). This social atmosphere transforms the streets outside the stadium. As one tourist recalled, ""The atmosphere before the game was crazy. I loved it.""Graph: Figure 3. Mobile symbols at the ""Welcoming Committee."" Activating prior emotional experiencesEntering event spaces can activate strong feelings among consumers due to objects that stimulate memories and emotions ([37]; [69]). At Anfield, thousands of supporters file through the wrought iron ""Paisley Gates,"" the iconic threshold of the stadium; they encounter statues of legendary players and the memorial shrine that commemorates the supporters killed at Hillsborough. Here many supporters stop to light a candle or stand in silent prayer. All of these objects symbolize memories of prior social atmospheres, and for Michaela these memories make Anfield special:It could have been any one of us, it could have been our sons and daughters [and] the reason why so many people were adamant that we shouldn't build a new stadium is because of what happened at Anfield after Hillsborough: that mass outpouring of grief and the mourning that occurred here.As much as place attachments can be associated with excitement, as with the Welcoming Committee, so too can they be associated with collective experiences of suffering and pain ([37]). In either case, [25], p. 220) explains that experiences of collective effervescence can instill a place with ""exceptionally intense forces."" For supporters like Michaela, memories of prior social atmospheres at Anfield enhance the attachment she feels toward this place ([ 9]; [20]). In other words, the material objects within event spaces can activate memories of prior social atmospheres, and this motivates consumers' loyalty to return to the sites where these experiences occurred. Atmospheric ClimaxRitual climaxes mark the culmination of collective live events when lively social atmospheres are shared among large groups. First, we show how firms deliberately orchestrate a ""formal climax"" by triggering focus, entrainment, and collective effervescence to unite crowds. Following a formal climax, we reveal that ""natural climaxes"" can follow, where social atmospheres are created by consumers responding spontaneously to events, without leadership or control by firms. Formal climaxA formal climax is a deliberately orchestrated moment of focus, entrainment, and collective effervescence that unites large groups. While other supporters are enjoying the Welcoming Committee or paying their respects at the Hillsborough Memorial, groups of devoted supporters expertly knot their homemade flags to poles and stanchions. As with similar activities in The Sandon, this place-marking ([79]) transforms the blank canvas of Anfield's interior into a place that boosts Steve's feelings of identity as an LFC supporter: ""When you walk in and see them flying, you feel together.... You're there with your own people."" At this stage, 50,000 people chat among themselves, waiting for the club anthem, ""You'll Never Walk Alone."" Having marked the beginning of games for more than 50 years, this song is Anfield's most iconic ""sound-mark"" ([64]), and its singing is a ""formal ritual,"" a ""recognized apparatus of ceremonial procedures"" ([16], p. 50). This ceremony gathers focus and initiates entrainment among supporters who stand and begin to sing in unison:Many close their eyes. Some have their arms wrapped around one another. Others extend their arms, as if they are at church.... Meanwhile, a gargantuan banner is passed over our heads, from left to right across the Kop, covering thousands of supporters. (Field notes, December 7, 2013)Even when the music stops, the crowd continues singing, entrained to one rhythm. At the same time as consumers are cocreating Anfield's soundscape, an enormous banner dubbed ""the surfer"" is passed above supporters' heads from one side of the Kop to the other. The movement seems to symbolize the transformation of individuals into a unified crowd (see Figure 4). In one audiencing interview, devoted supporter Nev reflected on this multisensory experience as one of visceral connection to other supporters:Graph: Figure 4. The formal ritual of ""The Surfer"" passing over supporters.It sends a tingle down your neck, and makes your hair stand on end.... It feels like you have some deep connection with those around you and they might be strangers you know. We're doing this together, like, I'm expressing myself to you, and you are open to me. Where else can you get that connection?Nev witnesses how this formal ritual breaks down the self–other divide, allowing for empathic experiences of shared emotions even among strangers ([32]; [37]). [15] explains that collective effervescence is facilitated when groups recognize that they are sharing emotions and behaviors. Firms play an important role in enabling these moments by providing event spaces that facilitate sensory experiences of others. Joe, a stadium architect, explains that the physical qualities of Anfield enable atmospheres because supporters can see and hear one another, ""It's tight, compact, you feel hemmed in together, and the sight lines allow everyone to see what is happening on the pitch and to respond together. It doesn't take too long for sound to travel."" As well as being pleasurable, this ""heightened intersubjectivity"" ([15], p. 35) among supporters becomes vital as the game begins. Natural climaxNatural climaxes are pleasurable outpourings of shared emotion in which crowds of entrained consumers generate social atmospheres spontaneously, without leadership or orchestration from firms. As much as sport is a form of play ([10]), English football has also been described as an ""audience-oriented conflict,"" where opposing supporters mimic intergroup violence ([17], p. 202). As such, most supporters do not wish to passively observe this conflict. Rather, they become a chorus that comments on the drama taking place on the field. During an audiencing interview, Jay described how this connection between the game and supporters produces an ""incredible atmosphere"":Some games you're on your feet for the whole 90 minutes, and you can come out of the stadium and your head is banging, your mouth dry. It's really draining afterwards, you know.... That Chelsea game, everyone was involved, an incredible atmosphere, whistling nonstop, roaring and celebrating every challenge instantly.Dan explains how he feels compelled to participate: ""If there's someone giving it their all, you can't just let them do it by themselves. You want to get stuck in."" It is noteworthy that the chorus chimes in without leadership. Thousands whistle and boo when the opposition are in possession of the ball; they chant rhythmically to propel their team forward or to humiliate their opponents. [15] uses the term ""natural rituals"" to describe events where entrained participants improvise according to emerging conditions. Fittingly, Joel describes the experience of being entrained with others as a ""natural"" reaction: ""You're on the same wave-length. It's natural, you know, like I know what the others around me are feeling and thinking.""Photojournalist Stuart notes how entrained crowds appear to share emotions: ""Everyone has the same look on their faces, that sort of clear connection of everyone being on the same page, it's like magic."" For [15], it is when crowds become aware of their common experiences that exaggerated changes in a social atmosphere are likely to occur, such as when collective tension and anxiety transform into collective effervescence following a goal:A Liverpool midfielder misplaces a pass, giving the ball to the opposition. The ground is starting to feel tense. Supporters sense that their team is vulnerable, they are rubbing their faces and biting their nails as they watch players fumble easy passes.... The team appear equally nervous, as each of their mistakes are met with an exasperated moan from the crowd. [Then] Liverpool score and the crowd erupts. The noise deafens. Arms and fists flail around as supporters rush to celebrate with each other. Bodies compress: supporters fall on top of one another. I am hauled into the row in front. Supporters scramble down the stairs that lead to the pitch. It is temporary chaos. (Field notes, January 28, 2014)Collective effervescence is a pleasurable experience for supporters like James, who considers that crowds where ""everyone is on the same page are a different animal. At the end of the 90 minutes, your head is banging, and your throat is raw. You know you've been in that crowd situation, an electric atmosphere."" James' account of atmosphere as ""electric"" recalls Durkheim's ([25], p. 217) description of collective effervescence in crowds, where ""a sort of electricity is generated from their closeness and quickly launches them to an extraordinary height of exaltation."" Atmospheric RecoveryIn the stage of atmospheric recovery, shared emotions and memories of atmosphere are stored in resources that make them available for future use. We first show how objects used within the stadium are charged up with emotions and memories, and how these objects will be recirculated by consumers in future interaction rituals. We then show how firms circulate representations of social atmospheres in social media, which helps kindle desires to repeat interaction rituals. Emotionally charged objects[15] explains that objects associated with experiences of collective effervescence become charged up like batteries, storing the emotional energy of an event. The large flags produced by Chris and his friends embody these feelings, for example, and are lovingly kept at the club for this reason. However, if LFC wins, supporters emerge from Anfield twirling red and white scarves above their heads and singing joyously. For tourist Ben, club merchandise helps him participate in a social atmosphere: ""Every club I've been to watch, I've bought something ... to demonstrate that you want to fit in, and to contribute, to participate."" Beyond fitting in, however, the symbolic resources that fans use in the stadium get charged up and become objects that inspire them to return. For example, Peter explains that through repeated use, his red and white scarf has become a ""lucky charm. I've had it for decades, it's dead old, it's tatty. I don't bring it with me to the match every game, only the special, important ones, where the team really needs to win. It's become a bit of a joke, when I turn up at Anfield and those around me are like 'Oh, we're going to win this afternoon, are we?'"" Remembering atmosphereRemembering atmosphere involves the use of digital merchandise to recall experiences and store emotional energy in ways that inspire future interaction rituals. Once the game is over, supporters filter away from the stadium, and the crowd dissipates into smaller groups. Although some groups move on to Liverpool's bustling bar and nightclub scene, Dan explains that many supporters return to the same service-ecosystem venues that they had frequented with family and friends before the game: ""We used to go straight to into town, but now we wait for the traffic to die down, meet my Dad and his mates in the [pub], you know, we have a quiet drink and reflect on the match.""As part of these reflections, many seek out match highlights on their mobile phones. LFC's marketing team and websites like the Anfield Wrap share video footage of the crowd on social media. Jenny likes how this content ""focuses on us, the fans, fan culture, the passion and the singing. They posted one before the Sunderland match.... Footage of fans celebrating wildly at Cardiff the previous week."" Video media reminds supporters of the emotional energy felt during an experience of social atmosphere ([17]). In this way, atmospheric recovery links to the preparation stage of the next event, inspiring customer loyalty by encouraging repetitions of the ritual chain. Indeed, digital merchandise motivates Jenny to repeat the ritual chain next week: ""I sort of build it up in my head, sing things in my head, watch videos of the crowd, and just get into the mood."" James too explains that the songs sung in the stadium and recirculated in social media ""are stuck in your head, rattling around in there."" How Do Atmospheres Fail?Social atmospheres are created through interaction ritual chains in which shared focus and entrainment make possible expressions of collective effervescence. Social atmospheres can fail, however, when shared focus is distracted, or when entrained groups of consumers are disaggregated. We find that these instances can be caused by consumers as well as by firms, particularly during the climax stage of interaction ritual chains. Distracting Shared FocusA shared focus prefigures entrainment and collective effervescence ([15]); it follows that social atmospheres are inhibited when shared focus is distracted. We reveal three means by which shared focus is distracted: first, when consumers have not prepared to focus; second, when firms remove symbolic objects from place; and third, when firms unwittingly distract from consumer-led atmospheres with spectacular atmospheric stimuli. Unprepared consumersUnprepared consumers enter an event space without having learned about the behavioral expectations to participate or the symbols and objects that require shared focus. In such cases, consumers are unable to participate in creating social atmospheres. Anthony calls attention to the heterogeneous nature of EPL consumers, noting ""how different supporters are, their mentalities, their mannerisms."" Group heterogeneity can lead to misalignments of meanings and practices ([37]; [76]), and although devoted supporters have often been prepared to contribute to social atmospheres from a young age, Brian complains that some casual supporters and tourists enter the stadiumcompletely alien to the culture that has been nurtured in the Kop and in Anfield over decades.... It leaves a bitter taste for everyone when that culture and feeling is dying in front of your eyes, especially if we take that social experience of togetherness as one of those things that keeps you returning to the match. You get more people now playing on their phones, not paying any attention to what's happening. Some even are on their phones during ""You'll Never Walk Alone"".... They're just totally disconnected.If consumers do not encounter the preparation stage of an interaction ritual chain, then they may not have learned about expected behaviors or what symbols require focus. By expressing ""too casual an attitude toward the focus of attention"" ([ 6], p. 11), consumers can appear irreverent during the formal rituals intended to unite crowds. Brian explains that if tourists fail to join a collective focus, this detracts from the atmosphere and the cultural experience of shared emotions that motivates supporters to repeat interaction rituals and maintain loyalty to Anfield. Removal of symbolic objectsSocial atmospheres can be disrupted when firms physically alter event spaces ([51]), particularly by removing the symbolic and emotionally charged objects that serve as a shared focus during interaction rituals. When the EPL's popularity surged in the 1990s, many clubs renovated old stadiums or built new ones ([75]). Although improving safety and revenue, these alterations often removed material features of place associated with prior experiences of atmospheres. Amanda complains that these ""cultural aspects of the stadium need to be respected. Clubs and architects need to understand and respect the community and the club. Arsenal's move to the 'Emirates Stadium'—a corporate name already—was undermined even more because they delayed moving Highbury's Clock into the stadium. It made it feel soulless."" Recalling Michaela's place attachments detailed previously, we interpret the removal of these objects of shared focus as inhibiting those memories that help motivate place attachments and associated desires to repeat social atmospheres.The removal of symbolic objects also occurs with respect to the mobile symbols that supporters use to gather shared focus among larger groups when activating social atmospheres. For reasons of cost, EPL clubs often outsource security staff, yet Chris explained that these staff can disrupt the visual display supporters create during formal rituals before the game: ""They just don't get it, the flags and banners on the Kop. Most people understand that it's all part and parcel of what we do, our culture, but some stewards don't."" Staff who remove these mobile symbols can undermine consumers' place-marking activities that enable feelings of shared identity. Spectacular stimuliSpectacular stimuli are material features of an event space that distract from consumer-led social atmospheres, thereby inhibiting shared focus. As with many experiential places, Anfield's managers provide entertainment and ""wow factor"" ([69]) with atmospheric stimuli including lights, music, and pyrotechnics. Yet Mark complains that this attempt to ""pump in the atmosphere just feels wrong."" [11] warn that spectacular environments can reduce consumer participation. In line with this view, Andy explains that spectacular stimuli distract from the shared focus created by supporters themselves: ""You're a consumer. You passively just take it in, keep yourself to yourself. There's no opportunity for having a sing, cracking a joke, actually creating part of the atmosphere, there's none of that. It feels artificial."" In contrast to literature that explains how firms can use spectacular stimuli to enhance place experiences ([ 9]; [69]), Andy warns that these well-intentioned atmospheric interventions change supporters' roles and divert attention from the place-marking and sound-marking activities consumers cocreate in the stadium. Flags and songs, for example, are symbols of group identity that enable shared focus among supporters and are central to the sensory experience of Anfield. Yet if firm-produced atmospheric stimuli distract from these resources, then place atmospheres can feel artificial. Disaggregating Entrained CrowdsSocial atmospheres are intensified through emotional and behavioral entrainment that gathers among larger groups. Disaggregation refers to the opposite of this process, where entrained crowds are separated into smaller groups or individual consumers: this can inhibit atmosphere and cause disappointment for some consumers. In this subsection, we describe how disaggregation is caused by individualized consumers, unactivated consumers, and crowd pacification. Individualized consumersIndividualized consumers refers to the outcome of ticket allocation practices that disaggregate entrained crowds, resulting in passive, lifeless social atmospheres. We have explained that groups of consumers activate entrained behaviors and emotions in the service ecosystem around the stadium before moving to Anfield, transferring a social atmosphere with them. Yet when supporters reach the stadium, these entrained groups can be split up ([75]). Particularly because of the way that tickets are allocated, Tony complains that although many supporters have activated an atmosphere before a game, on entering the stadium they are broken up such that ""you no longer get consistent pockets of support, about 150 or 200 people who you could rely on to get the atmosphere going in different bits of the ground."" When groups that have activated an atmosphere before an event are fragmented as they enter the event space, the social atmosphere is inhibited. Rather than enjoying the experience of being within an entrained crowd that has previously activated a social atmosphere, Gareth describes how this situation results in an individualized experience of shame:Me and my mates may be together before the match thinking we'll get the atmosphere going, but because of how tickets are allocated and sold, we may be spread all over Anfield.... You've got to be brave to start the chanting when nobody you know is with you. There's nothing more embarrassing than raising your voice and you're a lone voice in a crowd of 100 people, it's dead shameful, head in your hands sort of stuff. Unactivated consumersUnactivated consumers describes an aspect of consumer heterogeneity in which not all consumers are ready to align their emotions and behavior in a crowd. For example, many tourists enter Anfield without activating entrained behaviors before a game such that they are not ready to sing and chant with others. Indeed, Gareth's response indicates that in addition to being separated from those he has activated a social atmosphere with before the game, once in the stadium he may be seated with people who are content to watch the game passively. In such cases, the social atmosphere can feel flat. In response, devoted consumers within the stadium such as members of Spion Kop 1906 may try to rectify the situation by activating passive consumers ""on the job,"" inviting them to join in with easy chants:[Spion Kop] start a chant so simple, so rhythmic that even those not versed in Liverpool's hymn sheet could join in. The row behind me put more passion into it this time. It's an open invitation for the rest of the ground to join in. People sat close by cooperate, but take a while to adjust to the welcoming slower pace of the chant. The passion peters out. One supporter has had enough. ""Are youse gonna give us a hand or what? Why are youse here?!"" he shouts at the crowd. Some people in front turn around and roll their eyes, but the majority pretend not to hear him. (Field notes, January 18, 2014).As these field notes show, attempts to activate entrained behavior can fail. Reflecting on instances where supporters are not able to get ""on the same page,"" Michaela explains that ""arguments have broken out in the crowd before."" This can sour an atmosphere for Barry, who complains that sitting among unentrained groups of supporters ""puts me in a foul mood, and I spend the match complaining. I'd rather watch it with my mates in the pub."" Groups can harbor tensions when expectations and emotions are not aligned ([37]; [75]; [76]), leading audiences to dissipate ([61]). This problem motivates Barry's ""place detachment"" ([ 9]); where he used to loyally attend games at Anfield, he now prefers to watch LFC games on television in the pub with friends. Crowd pacificationCrowd pacification refers to deliberate attempts to inhibit entrained behaviors and emotions. Following the accident at Hillsborough, EPL stadiums were modified to improve crowd safety. Where crowds once stood packed together in terraces, new regulations demanded that they be seated and supervised by stadium staff and police ([43]). Despite boasting some of the lowest incidents of crowd disorder in Europe, however, supporters such as Jon complain that these security interventions inhibit the creation of social atmospheres:Let me show you. We're sitting down now.... I've got my hands in my lap, my shoulders and chest are all tight, and I'm stuck facing in one direction. How am I meant to express myself to others here? What can I do? I can clap, yes. Perhaps shout too, but it won't be loud as I've not got space to have a good old bellow. Without being aware of it, seats change your relationship to the match and others around you. You feel unconnected. Now, standing up [Jon gets up out of his seat]. Look at the difference now. I can move my arms. I can turn around. I can chat to the people behind me. I can take deep breaths and sing! You just feel completely different. You feel connected, able to respond to events on the pitch.Jon explains that being seated can limit entrained behaviors such as singing and gesturing. Moreover, because seats direct attention toward the field rather than to other members of the crowd, the possibility for crowds to recognize their shared emotions and behaviors is also limited. This hinders the natural rituals that lead to collective effervescence ([15]). Moreover, when supporters ignore these strictures, such as in our description of celebrations following a goal, they risk being ejected by security guards. This problem has motivated some supporters to campaign to restore standing areas within EPL stadiums. Others, however, admit that they have stopped attending EPL games, preferring lower-league matches where standing is still permitted and where social atmospheres meet their expectations. DiscussionSocial atmospheres are shared experiences of place involving transformations in consumers' emotions and behavior. Our study explains the processes by which these transformations are cocreated during a four-stage interaction ritual chain that spans several interconnected places. We next discuss three theoretical contributions of these findings before recommending how firms can manage social atmospheres in ways that enhance place experiences, customer loyalty, and communal place attachments. Finally, we conclude by considering boundary conditions, the transferability of our findings to other contexts, and opportunities for future research. Theoretical ContributionsOur study makes three theoretical contributions. By explaining the process of creating social atmospheres, we contribute to marketing research in retail atmospherics and servicescapes. By discussing how consumer heterogeneity affects interaction ritual chains, we contribute to theory of interaction ritual chains. Finally, by uncovering how social atmospheres are created in a variety of places, we contribute to research on ritual place-making and interaction ritual chain theory. The process of atmosphereStudies of retail environments show how firms create atmospheric experiences of place using multisensory stimuli ([ 4]; [73]) and that collages of these cues enhance place experiences by providing spectacles that prime playful behaviors ([47]; [69]). When service staff and consumers enact mythic and ideological meanings, experiences of place are further enhanced ([ 3]; [ 8]; [22]; [23]). In particular, ritual events imprint places with lasting identities ([10]; [37]). These social constructions of place are described as occurring in ""long and slow processes"" ([ 9], p. 892) and engender enduring qualities of place ([67]).Our work extends literature on ritual place-making by theorizing more rapidly changing qualities of place created during collective live events in which emotions and behaviors intensify and climax within groups ([ 3]; [32]; [39]; [45]). We conceptualize ""social atmospheres"" as a means to explain the process by which individuals are aggregated into crowds by shared focus, and how shared emotions and behaviors can intensify in pleasurable outbursts of collective effervescence. This pleasurable aspect of interaction rituals enhances place experiences and motivates loyalty as desire to repeat social atmosphere. In addition to how communal meanings and practices can drive loyalty ([53]; [65]), we show that social atmospheres also drive loyalty because emotional experiences can help strengthen place attachments ([ 9]).Moreover, although social atmospheres are conceptually distinct from a spirit of place ([10]), we find that locations with an established spirit of place offer ideal sites for interaction rituals. Our findings indicate that learning about the spirit of place facilitates participation in the cocreation of a social atmosphere. Furthermore, social atmospheres themselves help reconstruct a spirit of place: [25], p. 220) explains that repeated experiences of collective effervescence instill a place with ""exceptionally intense forces."" In short, in addition to the domesticity of ""hestial"" places ([10]; [69]), our study contributes an understanding of how an energetic spirit of place can be constructed through the creation of exciting social atmospheres. Heterogeneity and Interaction RitualsConsumer heterogeneity is a common feature of communal consumption events ([77]). Because heterogeneous expectations, motivations, and resource use can lead audiences to dissipate ([37]; [61]), this common dimension of group dynamics is an important consideration in understanding how interaction ritual chains create atmospheres. Despite this, [15] tends to gloss over heterogeneity among interaction ritual participants ([28]). For this reason, our study extends Collins's ([15], [16]) interaction ritual theory by considering interaction ritual chains as social events involving heterogeneous groups of participants that include both consumers and managerial stakeholders.More specifically, we challenge the notion that entrainment is an automatic result of people being together in a place. [15], p. xix) considers that under these conditions a natural physiological mirroring enables ""emotions in one individual's body to become stimulated in the other person's body."" Sociology has long discussed crowds as social configurations where individuals appear to think and act as one ([13]; [48]), and prior consumer research has described these effects as ""affective contagion,"" where emotions spread via affective cues such as facial expressions, gestures, and touch ([37]; [38]). Yet our findings indicate that when devoted long-time consumers mix with first-time tourists who are unwilling or unable to participate in entrained behavior, social atmospheres can suffer.As a corrective to [15] view of entrainment occurring automatically through physiological mirroring, we explain that this aspect of interaction rituals also depends on participants becoming ""prepared"" and ""activated."" In other words, shared emotions and behaviors are not entirely ""natural""; rather, these effects depend on consumers' learning emotional scripts, procedures, and rules ([37]; [65]). Beyond the kinds of procedures and rules elaborated in prior work, our study shows that learning to participate in entrained behaviors can necessitate opportunities to learn coordinated behaviors such as singing. Consequently, our work extends Collins by explaining that, rather like a rudimentary knowledge of dance steps enables people to move among others on a dance floor, acquired abilities to synchronize group behavior can facilitate social atmospheres. Extended atmospheresThird, our study clarifies how consumer interactions in one place can impact atmospheres in another. Atmospheric effects are commonly associated with bounded retail and servicescape locations ([ 4]; [44]; [69]; [73]). [15] too analyzes interaction rituals occurring in discrete contexts. In contrast to a view of places as bounded ""containers,"" however, studies show that consumers prepare for place experiences in different locations and move from place to place as events progress ([10]; [41]; [75]). By tracing interaction rituals across multiple sites, we explain that the intense social atmospheres consumers enjoy at events do not begin spontaneously. Rather, at concerts, sports fixtures, carnivals, and festivals, atmospheres are often activated prior to, and outside of, the places where these events occur.This contribution aligns with theorizations of place as connected to other places ([14]). Specifically, we view social atmospheres as implicating paths that lead toward and away from places ([18]). In terms of pathways that lead toward a place, social atmospheres are activated before the climax of a ""main event"" among smaller gatherings of consumers, much like the tailgating events described by [10]. In terms of the paths that lead away from a place, we find that a stage of recovery sees consumers calming and breaking back into smaller groups who frequent a wider service ecosystem ([ 1]) close to an event place, where they plan for repetitions of the social atmosphere.Finally, with respect to the possibility for atmospheres in one place to move into another place, an important contribution of our study is to explain how different stages and contexts of the interaction ritual chain are linked. Our findings show that symbolic resources such as flags and songs are a constant presence throughout the interaction ritual chain. A unique implication of our study is that these symbolic resources can become mobile, and the transfer of these mobile resources from place to place connects the stages of an interaction ritual chain. Prepared in private places before being transferred through the service ecosystem and into an event space, visual and aural symbols provide a mobile focus that helps gather larger crowds, thereby facilitating the intensification of a social atmosphere as an interaction ritual chain progresses. How to Manage Interaction Ritual ChainsManagers play important roles in facilitating ritual interactions ([53]; [60]). To help firms benefit from social atmospheres, this subsection identifies and illustrates how a variety of managerial stakeholders—from small businesses to managers of large event spaces—can facilitate each stage of the interaction ritual chain (for a summary, see Table 2). Our insights go beyond existing managerial advice by illustrating best practice at different stages of the interaction ritual chain before, during, and after a main event. Finally, we explain how tensions in the creation of social atmospheres can be managed.GraphTable 2. Managerial Recommendations. Stage in the Interaction Ritual ChainGroup SizeSpaceFirm's Managerial RolesRoles of Other StakeholdersInstances in Prior Research on Collective Live EventsPreparations: Consumer-led activities before events that provide resources to later enable atmospheres.IntimatePrivateEducate consumers through marketing communications that stress behavioral expectations.Family and friendship groups set behavioral expectations.Festivals promote unique ethos through marketing communications (Kozinets 2002; Flinn and Frew 2014)Promote the cocreation of focal symbolic resources.Passionate entrepreneurs and brand devotees help make symbolic resources.Nightclubbers prepare costumers for the event (Goulding et al. 2009)Cocreate events with passionate entrepreneurs.TEDx events training (Fidelman 2012)Activation: Creation of social atmosphere through focus and entrainment among small groups of consumers before a main event.Small-scaleEcosystemProvide consumers with a visitors' guide that mythologizes local landmarks and places.Businesses in service ecosystem help welcome and encourage consumers to activate.Tailgating activities prior to American football games (Bradford and Sherry 2018)Promote venues in the ecosystem in which activations occur.Brand devotees lead activations.Preparty events (Goulding et al. 2009)Security services provide correct safety provisions for activations to occur.Pre–Mardi Gras parades (De Jong 2015)Climaxes: The culmination of collective live events when collective effervescence is shared among crowds.CrowdEvent spaceStage formal climaxes by triggering focus, entrainment, and collective effervescence.Brand devotees lead natural climax.End of festival rituals (Anderton 2019; Kozinets 2002)Enable natural atmospheres through spaces design to enable entrainment without distraction.Casual consumers follow the lead offered by brand devotees.Music concerts and events (Goulding et al. 2009)Security services facilitate climax rituals through developing specialized knowledge.Worship in megachurches (Wagner 2019)Live sports (Holt 1995)Recovery: Shared emotions and memories of atmosphere are stored in resources that inspire repetitions of rituals.IntimateEcosystemProvide merchandise postevent.Businesses in the ecosystem welcome and encourage consumers to recover together.Digital merchandise produced out of music events (Anderton 2019)Circulate video footage of consumers enjoying collective effervescence.Distributing recordings of climaxes in megachurches (Wagner 2019)  Enabling preparationsPreparations facilitate social atmospheres by teaching consumers about behavioral and emotional expectations before an event. Our findings illuminate opportunities for firms to facilitate preparations. Firms can play important roles in educating consumers about cultural meanings and behavioral expectations using media communications and social networks ([65]) and by partnering with ""passionate entrepreneurs,"" consumers who translate their knowledge of a community into offerings that support shared passions ([34]). For example, TED events are characterized by a highly charged elitist atmosphere. To create this experience, the TED organization provides guidelines to enthusiasts who arrange local TEDx events ([27]). These guidelines prepare speakers on how to ""TED,"" covering topics such as creating material and rehearsing speakers' body language to elicit focus from the audience. These preparations also encourage consumers to become enthused and entrained when they attend, all of which benefits the TED brand by enhancing consumer experiences. Enabling activationActivations facilitate atmosphere by providing opportunities for smaller groups to warm up before a main event. To enable activation, we recommend that firms create partnerships with businesses that operate in the broader ecosystem around a main event site, because these are places where consumers often gather before an event, as is the case with the bikers on their way to Sydney's Gay and Lesbian Mardi Gras ([21]). Riding in formation creates a shared focus and entrained behavior in small groups ([66]). Thereafter, bikers gather with others into larger unified crowds as they travel toward the carnival. Oftentimes these periods of mobility are as important as the event itself, and for this reason we recommend managers facilitate consumer ""pilgrimages"" to events by providing information about potential activities, venues, and points of interest along the way. For example, at both Rio de Janeiro's Carnival and New Orleans's Mardi Gras, consumers are guided toward the many events and activities on offer before the main parade. However, given that emotions and behaviors intensify among crowds in public spaces, event managers will benefit from dialogue with public authorities as well as police and security services. Enabling climaxRitual climaxes provide pleasurable moments of open-boundary communication with others ([ 3]; [15]). This can enhance customer experiences and drive loyalty, as consumers seek to repeat rituals to reexperience collective effervescence. To facilitate climaxes, managers can first orchestrate formal rituals that focus and entrain smaller groups of consumers into larger audiences and crowds. Consider megachurches such as the enormously successful Hillsong. Much like musicians at concerts and festivals can unite huge crowds by having them clap and sing together, charismatic preachers begin events with formalized activities intended to focus and entrain a congregations' behaviors and emotions ([15]; [80]). Firms have important roles to play in providing spaces that foster these aspects of interaction rituals. For instance, megachurches benefit from architectural designs that facilitate consumers' awareness of their aligned emotions and behaviors. By enabling recognition of entrainment among a congregation, megachurches also facilitate natural rituals in which groups begin to express themselves more spontaneously. Preachers encourage worshippers to vocalize, or stand with hands raised, for example. When congregations can see, touch, and hear one another during these moments ([80]), their shared actions are more likely to climax in experiences of collective effervescence. Enabling recoveryThe recovery stage is when shared emotions and memories of atmosphere are stored in the symbolic resources that consumers have carried through interaction ritual chains. We have shown how merchandise associated with experiences of collective effervescence can store feelings of emotional energy. Such objects are used by consumers in the preparation and activation stages of future events and can therefore be useful drivers of loyalty. In addition, we recommend that firms create digital merchandise. By circulating footage of crowds enjoying a ritual climax, firms facilitate conversations between consumers about their shared emotional experiences. For example, music events such as Glastonbury and Coachella distribute media that focus on consumers as much as musicians to advertise climactic moments where an atmosphere becomes ""electric"" ([29]). By showcasing crowds enjoying shared focus, entrainment, and collective effervescence, digital merchandise can enhance loyalty by inspiring consumers' desires to repeat interaction rituals. Moreover, digital merchandise connects the recovery stage of one event to the preparation stages of future events by educating consumers about expectations such as dress and emotional conduct ([ 2]; [35]).Despite opportunities for firms to facilitate interaction ritual chains, we recognize that social atmospheres can fail. Next, therefore, we provide recommendations to avoid failures by navigating three common tensions identified in our analysis. Spectacle versus shared focusCreating social atmospheres requires different management techniques compared with those used to manage environments intended to offer spectacular experiences, such as ESPN Zone ([47]; [69]), American Girl Place ([22]), or Disney World ([40]). Studies of these places note how managers assemble dazzling arrays of objects and ""zoned"" activities that provide myriad points of focus for smaller groups of consumers ([62]). In contrast, managers who want to enable social atmospheres should minimize distractions from a common focus of attention necessary to initiate entrainment and collective effervescence. For this reason, pumping in spectacular stimuli such as upbeat music, lights, and fireworks ([11]) can harm social atmospheres by distracting the crowd's attention. To facilitate shared focus in crowds, we recommend that firms use symbols that have previously been ""charged"" with emotions and group significance during prior interaction rituals ([15]). For instance, the Boston Red Sox have played Neil Diamond's ""Sweet Caroline"" during games for 20 years, providing a focus to prime a giant sing-along and a pleasurable social atmosphere ([71]). Managers should be aware that meaningless symbols are less likely to create shared focus. For example, when the Sydney Swans football club tried to activate similar conviviality using ""Sweet Caroline,"" consumers complained the song ""has nothing to do with the club"" ([33]). Insiders versus outsidersPrevious research notes how heterogeneity among consumers can cause group tensions, particularly when ""newcomers"" mix face-to-face with established ""insiders"" ([66]; [77]). When music festivals grow in popularity, for example, a common complaint is that newcomers detract from the atmosphere ([ 2]). We have witnessed this tension between brand devotees and casual consumers, some of whom fail to join in with a shared focus and entrainment, leading to failures in social atmosphere. Managers can play a key role in solving this problem. A common strategy is to provide access to differentiated areas of the event space so as to separate consumer segments according to their expectations and behaviors, such as when season ticket holders are seated together. On the one hand, this possibility can help preserve groups of prepared and activated consumers who want to create social atmospheres. On the other hand, by separating devoted insiders from outsiders, these interventions may exacerbate differences between groups. Reinforcing differences in expectations and willingness to contribute to social atmospheres may prevent more experienced consumers from helping neophytes to join in ([65]). Brandfests offer a useful precedent here: when newer consumers mix with seasoned consumers, both groups benefit, as outsiders are able to learn and feel welcomed, while insiders often ""relish the recognition and status"" afforded them ([53], p. 42). Effervescence versus controlAt collective live events, managers must inevitably assess myriad risks and implement safety precautions ([57]). How can the possibility to create collective effervescence be balanced against the necessity to manage risks and disorder? First, educate security and service staff to understand consumers' interaction ritual chains. Firms can avoid disrupting the entrainment and collective effervescence that contribute to social atmosphere by making staff aware of the places where consumers may have been before they arrive at an event, or the kinds of group behavior that they may have developed on the way there. Equally, service and security staff should be aware that collective effervescence may involve unusual behavior and unpredictable outbursts of emotion. For example, security at music festivals can misinterpret collective effervescence in a ""mosh pit"" as violent disorder, yet for consumers, this behavior is expected and enjoyed and, in most cases, is not harmful ([70]). These outcomes can benefit managers of security companies willing to build specialized knowledge of interaction rituals. Having witnessed ritual events time and again, security and service staff can assume roles of trusted facilitators, whose experience enables them to allow consumers to produce social atmospheres and to intervene only when necessary. ConclusionOur study explains the processes by which social atmospheres are created during interaction ritual chains intended to create the kinds of lively social atmosphere that typify many collective live events. Our analysis is transferable to other contexts that share characteristics encompassed in the following boundary conditions. First, our study is applicable to events at which the collective effervescence emerges as moments of positive affect. Future research could examine how interaction rituals produce atmospheres of grief, serenity, or outrage, which may deviate from the process we have described.Second, our study is applicable to social atmospheres in both small contexts such as pubs and bars and larger, more organized live events such as sports and music events, religious ceremonies, festivals, and carnivals. A benefit of our theorization is to recognize how smaller businesses (e.g., pubs, cafes) are interconnected to larger organizations (e.g., football clubs) or places (e.g., stadiums). We note how smaller businesses that become aware of their place in an interaction ritual chain can facilitate activation and recovery activities; future research could examine how other kinds of small businesses in the ecosystem can benefit from their roles in these chains. Future research could also examine mobile social atmospheres in larger ecosystems, such as extended city-based events like the Edinburgh Fringe Festival or the Olympics.Third, our theory is especially relevant to recurring events, as the cyclical nature of interaction ritual chains means that the recovery stage feeds into the preparation stage. Related to this, our analysis suggests that social atmospheres depend on the presence of consumer groups who have become used to contributing to atmosphere through repeated participation in interaction ritual chains. Thus, our insights are less relevant for one-off live events that are never repeated. Future research could investigate how social atmospheres can be developed at first-time or one-off events, which may differ from the process we have outlined.Finally, our insights are most relevant in face-to-face contexts in which communal interactions occur. Our theory has less relevance in more individualized retail experiences for which the findings of conventional atmospherics research remain relevant. However, as a result of face-to-face interactions being limited during 2020 and beyond, the role of technology in facilitating social interactions is changing. For [15], face-to-face contact is critical for interaction rituals, and for this reason, he claims that interaction rituals cannot occur in digital environments. Nevertheless, rapid developments in virtual reality technologies may enable access to digitally mediated social atmospheres as a way of enhancing experiential places such as museums and historical and educational sites or allowing consumers to experience collective live events remotely. These possibilities present exciting opportunities for future research and practice, not least because people need connections with others and suffer without them. Marketing has important roles to play in facilitating connections and social systems, and we hope that this study inspires managerial practice that enables social atmospheres to flourish.  "
41,"Virtual Reality in New Product Development: Insights from Prelaunch Sales Forecasting for Durables This investigation examines how consumer durable goods producers can leverage virtual reality for new product development. First, the authors develop a prelaunch sales forecasting approach with two key features: virtual reality and an extended macro-flow model. To assess its effectiveness, the authors collect data from 631 potential buyers of two real-world innovations. The results reveal that the new approach yields highly accurate prelaunch forecasts across the two field studies: compared with the actual sales data tracked after the product launches, the prediction errors for the aggregated first-year sales are only 1.9% (Study 1a, original prelaunch sales forecast),.0% (Study 1b, forecast with actual advertisement spending), and 20.0% (Study 1b, original prelaunch forecast). Moreover, the average mean absolute percentage error for the monthly sales is only 23% across both studies. Second, to understand the mechanisms of virtual reality, the authors conduct a controlled laboratory experiment. The findings reveal that virtual reality fosters behavioral consistency between participants' information search, preferences, and buying behavior. Moreover, virtual reality enhances participants' perceptions related to presence and vividness, but not their perceptions related to alternative theoretical perspectives. Finally, the authors provide recommendations for when and how managers can use virtual reality in new product development.Keywords: innovative durables; new product development; sales forecasting; virtual realityIntroducing a new consumer durable product to the market is an important but risky endeavor ([31]). Research thus far provides rich insights regarding various aspects of new product development (NPD) that help firms manage these risks. For instance, investigators have examined customer integration during NPD ([14]) and drivers of new product success ([35]). However, knowledge on how to leverage new technologies such as virtual reality to improve NPD is scarce, despite their high potential ([31]) and several recent calls for such research ([50]; [70]).The paucity of research on virtual reality use in NPD is especially surprising in a consumer durables context, as virtual reality technologies have improved significantly in terms of their visualization and automated-tracking capabilities, and they can now elicit and track extensive consumer experiences ([16]). Leveraging these technological improvements could help durables producers perform their NPD tasks. For instance, because the market success of durables may strongly depend on consumers' experiences ([48]), better simulation and capture of these experiences could improve prelaunch product assessments ([37]) and help firms decrease the high failure rates of durables, which range between 40% and 90% ([26]; [42]). In addition, evidence indicates that extensive experiences can be created and assessed in virtual reality, even for products that do not yet exist as firms only need a virtual blueprint or 3D model of the new product ([36]; [62]). Thus, durables producers could get consumer insights earlier in the NPD process by relying on virtual reality, resulting in advantages such as better aligned production and commercialization plans or cost reductions ([11]; [51]).For these reasons, we examine how consumer durable goods producers can utilize virtual reality—a simulated environment that allows the consumer to interact with it ([49])—to improve their NPD. We focus on virtual reality for prelaunch sales forecasting and then discuss implications for other NPD tasks. In particular, we examine the following: RQ1:  (How) does virtual reality improve prelaunch sales forecasting? RQ2:  Why do virtual reality simulations (vs. traditional studio tests with real products) lead to advantages in forecasting?To examine RQ1, we developed a new prelaunch sales forecasting approach that employs virtual reality, together with an extended macro-flow model. We tested the new approach in two field studies (Studies 1a and 1b) with two real-world innovations that our collaborating companies actually introduced. In total, a representative sample of 631 potential buyers participated in the field studies. Using one year of actual sales data provided by GfK, a leading European market research agency, we validated our prelaunch sales forecasts after the launch of the two innovative durables. The results show that the new approach achieves high prelaunch forecasting accuracy with errors for the aggregated first-year sales forecasts of 1.9% (Study 1a),.0% (Study 1b, forecast with actual advertisement spending), and 20.0% (Study 1b, original prelaunch forecast) as well as a mean absolute percentage error (MAPE) of 18.6% (Study 1a), 26.5% (Study 1b, forecast with actual advertisement spending), and 44.4% (Study 1b, original prelaunch forecast) for the monthly sales forecasts across the two studies. In addition, a comparison of our forecasts with the strongest benchmark model demonstrates that this new forecasting approach improves monthly forecasting accuracy by over 30%. In a supplemental analysis, we furthermore approximated that a substantial part of this improvement can be attributed to the virtual reality key feature of the approach.To address RQ2, because we could not determine from the field studies why virtual reality contributes to sales forecasting accuracy, we conducted a laboratory experiment (Study 2, n = 210). Participants were randomly assigned to one of three conditions: ( 1) lab virtual reality, ( 2) online virtual reality, or ( 3) a studio test with real products. The findings reveal that the forecasting advantages of virtual reality emerged because virtual reality participants on average behaved more consistently in terms of their information search, preferences, and purchase behavior within the simulation than the studio test participants. The results also indicate that these differences can be explained by increased presence (i.e., the extent to which participants feel they are actually in the simulation) and vividness (i.e., the extent to which participants feel that the simulation is detailed and easy to imagine), but not through alternative theoretical lenses, such as decision uncertainty and convenience.Overall, our approach and findings advance the understanding of whether and how virtual reality can improve sales forecasting (RQ1: Studies 1a and 1b) and of the reasons why these advantages arise (RQ2: Study 2). In concluding, we discuss the studies' implications for future research—including a road map for future marketing studies on better prognoses, diagnostics, and customer linking through virtual reality ([19])—and nonacademic audiences—including an action plan for the use of virtual reality in NPD. RQ 1 : (How) Does Virtual Reality Improve Prelaunch Sales Forecasting? ApproachResearchers have focused on forecasting models for new durables that are applied close to launch ([ 7]; [25]). However, firms need a precise sales forecast early in NPD to improve their investment decisions ([51]) and to align their production, marketing, and distribution plans ([11]). Although knowledge is lacking on the prelaunch sales forecasting of durables, sophisticated models for consumables exist that achieve high forecasting accuracy by using a trial-and-repeat logic ([22]; [63]). However, these models are not applicable to durables, which are purchased infrequently and are therefore not subject to trial-and-repeat purchases ([29]). We address this research gap and develop a new prelaunch sales forecasting approach for consumer durables with two key features: virtual reality and an extended macro-flow model. Key Feature: Virtual RealityThe use of a virtual reality simulation for the new approach was motivated by virtual reality's visualization and automated-tracking capabilities. We explain these two capabilities and then discuss the virtual reality types that leverage these capabilities to a varying extent (Table 1).GraphTable 1. Differences and Similarities of Physical Product Experience, Virtual Reality, and Multimedia Approaches. CapabilityPhysical Product ExperienceVirtual RealityMultimedia ApproachesStudio Test with Real Products in a Shelf(e.g.,Castro, Morales, and Nowlis 2013; Basis for Benchmarkin Study 2)Lab Virtual Reality(This Investigation:Studies 1a, 1b, 2)Online Virtual Reality(This Investigation:Studies 1a, 1b, 2)Information Acceleration(Urban et al. 1997; Basis for Benchmark in Studies 1a and 1b)Virtual Shelf Simulation(Burke 1996)Web-Animated Prototypes(Dahan and Srinivasan 2000)VisualizationSimulation scopeMediumFocused on a shelf due to limited studio space with many different products and detailed information on productsVery HighComprehensive environment (e.g., entire local retail store) with many different products and detailed information on productsVery HighComprehensive environment (e.g., entire local retail store) with many different products and detailed information on productsMediumSections of environment with new product and one competitor product with detailed information on the two productsLowFocused on a virtual shelf with different products and some information on productsLowWebpage showing different productsSimilarity to realityMediumStanding in front of a shelf with real productsVery High3D view of the environment and products from 360 degrees and first-person perspective based on 360-degree pictures of actual settingHigh2D view of the environment and products from 360 degrees and first-person perspective based on 360-degree pictures of actual settingMedium2D view of sections of environment and two products from different angles based on pictures and/or videos of actual settingLow2D view of programmed virtual shelf incl. products from different anglesLow2D view of animated products viewedImmersionMediumSenses with regard to product interactions:Use of own eyes for seeing the shelf with the products and hands for interactionsVery HighSenses with regard to product and environment interactions:Use of a head-mounted display to see the whole store and products, use of and controllers for immersing user's motions and interactionsHighSenses with regard to product and environment interactions:Use of a computer to see the whole store and products, use of keyboard and mouse for immersing user's interactionsMediumSenses with regard to product and sections of environment interactions:Use of a computer to see sections of environment and two products, use of keyboard and mouse for interactionsMediumFocused on senses with regard to product interactions:Use of a computer to see the shelf and different products, use of keyboard and mouse for interactionsLowFocused on seeing the product:Use of a computer to see different animated productsTracking of behaviorInteractivityHigh Interactivity, Tracking Through ObservationWalk through store excerpt (i.e., shelf)Explore different products placed on shelfPick products up from shelf and turn them to see them from all anglesPut products in basketHigh Interactivity, Automated TrackingWalk through storeExplore different products placed on shelfPick products up from shelf and turn them to see them from all anglesPut products in basketGo to checkout and purchase any productHigh Interactivity, Automated TrackingWalk through storeExplore different products within the storePick products up from shelf and turn them to see them from all anglesPut products in basketGo to checkout and purchase any productMedium Interactivity, Tracking Was Not ImplementedChange to predefined positions to view the product from different anglesAsk predefined questions regarding the productMedium Interactivity, Automated TrackingExplore different products placed on shelfPick products up from shelf and turn them to see them from all anglesPut products in basket for purchaseLow Interactivity, No TrackingReplay the animation/video of the product  Visualization capabilityBy ""visualization capability,"" we refer to the ability to simulate new products, customer touchpoints, and environments in a comprehensive, realistic, and engaging manner. Drawing on previous work ([16]; [49]; [52]; [68]), we theorized that the visualization capability would be valuable for forecasting: if consumers are immersed into the simulation and the simulation is comprehensive and similar to reality, they might be more likely to act throughout the simulation as they would in a real durable purchasing situation. As Table 1 shows, virtual reality has a higher visualization capability than previous multimedia approaches or studio tests used for prelaunch forecasting. This advantage can be attributed to virtual reality's simulation scope, similarity to reality, and immersion.First, virtual reality can span a very high simulation scope. This attribute is due to its ability to simulate all facets of purchase journeys with many different products and detailed depictions of various information sources and shopping environments, such as online shops or local retail stores ([28]; [45]). In building on these insights, we expected that, compared with previous multimedia approaches and studio tests, virtual reality would motivate more realistic consumer behavior due to the increased liveliness of the simulation ([52]; [61]). This anticipation is because, to reduce costs and complexity, prior approaches relied on more focused and less detailed simulations, such as displaying the new product on a shelf alongside only a few competitors (Table 1).Second, virtual reality can achieve a (very) high similarity to reality. This attribute is due to its ability to showcase the simulated shopping environment and products from 360 degrees and first-person perspective based on 360-degree pictures of the actual setting. Thus, in line with previous research in related fields ([28]), we expected that virtual reality would increase similarity to reality compared with previous multimedia approaches and physical product experiences, such as studio tests (Table 1). This anticipation is because previous approaches relied on either third-person perspectives via pictures, image galleries, and infomercials or on simulating parts of reality (e.g., a shelf with physical products or only the products) in a less genuine manner.Third, virtual reality simulations can achieve a (very) high immersion of consumers. This attribute exists because, using additional virtual reality equipment (e.g., head-mounted displays, motion controllers), it can deeply transport consumers into a simulation by stimulating their senses intensively ([66]). In contrast, previous approaches would either immerse fewer senses or focus on consumers' immersion related to the product (but not the environment) (Table 1). Thus, we expected that virtual reality (vs. previous approaches) would motivate more realistic behavior by further increasing consumers' perceived transportation into the simulation. Automated-tracking capabilityAutomated-tracking capability is the ability to directly collect data of consumers' interactions with the simulation. The recorded data can encompass the type of interaction performed by the consumer (e.g., entering a simulated environment, operating a product in the use environment) as well as the duration of each interaction. We theorized that an automated-tracking capability would be valuable for sales forecasting: prelaunch sales forecasting models could integrate such behavioral data as inputs, which might increase forecasting accuracy ([52]; [67]).Virtual reality has an automated-tracking capability advantage over previous approaches, which relied on observations to capture consumers' actions and focused on collecting data on interactions with the products (Table 1). In contrast, due to sophisticated computer-based simulation technology and motion-tracking sensors, virtual reality offers additional options to interact with both the simulated environment and the products ([16]). Moreover, because these interactions occur digitally, they can be directly tracked ([16]; [28]). Virtual reality typesVirtual reality simulations essentially fall into two main categories: lab virtual reality and online virtual reality. Lab virtual reality requires data gathering in a central location, as it uses additional equipment for viewing and interacting with the simulated environment (e.g., head-mounted displays, sensory input devices, power walls). This equipment allows lab virtual realities to offer many highly intuitive interaction possibilities ([16]). For instance, head-mounted displays automatically react to the user's head movements and change the perspective and picture. Moreover, hand movements can be detected by motion-tracking sensors or virtual reality controllers and mimic and perform these actions within the simulation. Thus, using gesture control, consumers within a lab virtual reality can move around freely in the simulation, pick up items from shelves, and operate or buy products (e.g., through hand movements).In contrast, an online virtual reality can be accessed from anywhere via the internet as it is displayed on a computer and does not need additional equipment. Owing to sophisticated graphics and 360-degree views from the first-person perspective, online virtual realities can realistically depict products and environments at comparatively low costs ([ 8]). However, compared with lab virtual realities, online virtual realities stimulate fewer senses and provide less intuitive interaction possibilities (e.g., clicking on a mouse vs. motion-tracking) while still offering high degrees of interactivity ([16]). For instance, in online virtual reality, consumers can freely navigate in any direction within the simulated environment by clicking in that direction, and the position and perspective changes automatically. Participants can select different products with the cursor, pick a product up by clicking on it, and then view it from 360 degrees, put it back on the shelf, or buy it as they would in a real store.Thus, as Table 1 shows, lab virtual realities and online virtual realities are very similar in terms of their automated-tracking capability but differ with respect to their visualization capability. Both lab and online virtual realities can achieve similarly high levels of simulation scope and interactivity, but lab virtual realities are likely to create more immersive and realistic environments, due to the usage of additional virtual reality equipment. Figure 1 provides visuals and screenshots that illustrate both virtual reality types. See the subsection ""Studies 1a and 1b: Research Setting, Data, and Analyses"" for a depiction on how we use both virtual reality types in field studies.Graph: Figure 1. Visuals and screenshots of virtual reality environments.Notes: *Due to confidentiality agreements, we are not allowed to show the real products and their actual functionalities. Therefore, we show placeholders instead. Key Feature: Extended Macro-Flow ModelMacro-flow models forecast the sales of new products over time before their market launch ([69]). In contrast, alternative forecasting models either do not allow for a prediction over time (e.g., conjoint analysis) or cannot be applied prior to the launch (e.g., diffusion models). To derive a sales forecast over time, macro-flow models predict the flows of potential buyers from one state of the purchase journey to the next. To make these predictions prelaunch, they require a purchase journey simulation ([68]). Macro-flow models consist of three elements: behavioral states, the flows between these states, and the determinants of these flows (Figure 2).Graph: Figure 2. Behavioral states, flows, and determinants in the extended macro-flow model.Notes: The behavioral states shown on the left are simplified. For a detailed depiction, see Figure W1.1 in Web Appendix. Behavioral statesBehavioral states are the attitudes and actions through which consumers flow in their purchase journey ([69]). As Figure 2 shows, we extended previous macro-flow models in the awareness, information search, and purchase decision state. For creating awareness, previous macro-flow models considered word of mouth (WOM) and advertisements ([69]; [68]). To adjust for the presence of the internet (e.g., social media, blogs) and its impact on consumer behavior ([71]), we added awareness via third-party information. For information search, previous macro-flow models included a showroom visit, a test drive ([68]), or a consumer magazine and recommendations from others ([67]). To capture consumers' information search more broadly ([45]), our extended macro-flow model includes five frequently used information sources: a local retail store, an online shop, the producer's website, an online magazine, and a use environment. Finally, previous macro-flow models captured the purchase decision state in terms of buying either the new product or a competitor product, thus forcing participants to make a decision. In contrast, to account for the possibility that consumers delay their purchase or refrain from buying ([29]; [65]), we added a no-purchase option. Flows between the behavioral statesFlows between the states are defined as the proportion of consumers transitioning from one behavioral state to another ([58]; [69]). All flows build on logit and negative exponential models ([33]). We focus the description below on the new and updated flows but summarize all equations in Table 2.GraphTable 2. Overview of Flows in the Extended Macro-Flow Model. FlowRationaleSourcea(1) Pt(Aware|WOM) = α1 1 −exp −β1∑r = 1tYt−rt −rThe probability of becoming aware of the new product via WOM of previous buyers depends on the number of previous buyers and the susceptibility to WOM.Urban, Hauser, and Roberts (1990)(2) Pt(Aware|Ads) = α2 1−exp −β2×AtThe probability of becoming aware of the new product via advertisements depends on the advertisement spending and the susceptibility to advertisements.Urban, Hauser, and Roberts (1990)(3) Pt(Aware|Third) = α3 1−exp −β3×Bt, where Bt = 2−Time since announcement (in years)Number of similar products; Bt≥0The probability of becoming aware of the new product via third-party information depends on the time since announcement of the new product and the number of similar products.New(4) Pt(Aware) = Pt(Aware|WOM) + Pt(Aware|Ads) + Pt(Aware|Third), where Pt(Aware) ≤1The overall probability of becoming aware depends on the awareness via WOM, ads, and third-party information and has an upper boundary of 100%.bNew(5) P(Information Search) = 1−exp−β4∑s = 1s = nactionss ×times×visitss The intensity of information search depends on the actions, time, and visits spent on each product at each information source.New(6) P(Preference) = Viβ∑k = 1mVkβ; Vk>0, Vi>0The preference for the new product depends on the points given to the new product compared with the points given to the other products in the relevant set during the preference measurement.New, based on Luce (1959)(7) P(Behavior) = P(Virtual Reality) + P(Preference)2.The probability of buying the new product depends on the preference for it and the buying behavior during the virtual reality simulation.New(8)  PtBuy New Product = Pt(Aware)×1 + Pi(Information Search)−  Pj(Information Search)×P(Behavior)The overall purchase probability depends on the awareness, information search, and the probability of buying the new product.New, based on Urban, Hauser, and Roberts (1990)(9) SalestNew Product = meanPtBuy New Product×MarketSizetThe sales of the new product depend on the overall purchase probability for it and the market size of the product category.Urban, Hauser, and Roberts (1990) 1 a For differences and similarities across macro-flow models, see Web Appendix W1.2 b From a theoretical point of view, the equation of awareness needs to subtract consumers, who become aware through two or more ways. To simplify the equation, we omitted this subtraction because even if the consumer becomes aware through multiple ways, the maximum awareness will still be 100%, which is why we introduced the upper boundary of 100%.The first new flow is the probability of becoming aware via third-party information (Equation 3, Table 2). We model this flow,  Pt(Aware|Third)  , dependent of the amount of buzz for the product. Specifically, buzz factor Bt computes as two years[ 5] minus the time since the announcement divided by the number of similar products, which accounts for decreasing relevance of buzz over time ([71]) and a decrease in buzz as the number of similar products grow ([37]). Integrating this extension with the two components of previous macro-flow models (Equation 4, Table 2) allows for calculation of the overall probability of awareness,  Pt(Aware)  .[ 6]The next updated flow is from awareness to information search (Equation 5, Table 2). As the time and effort in the information search allocated to particular products indicates purchase interest in those products ([ 9]), we model the flow from awareness to information search, P(Information Search), for each consumer for each product depending on the number of actions, visits, and time spent at each information source.The next new flow is from information search to the purchase decision. This flow, P(Behavior), is calculated as the mean of two components (Equation 7, Table 2): preferences and the observed virtual purchases. We model the preference of each participant, P(Preference), by asking participants to divide a fixed number of points according to their preferences between the products they would consider buying ([46]; [47]).[ 7] We observe each participant's virtual purchase decision, P(Virtual Reality), in the simulation and assign different purchasing probability values to it (i.e., 0 if no product or a competitor is bought,.2 if the product is on the wish list, and.7 if the product is bought), guided by previous findings on conversion schemes for purchase intentions ([13]; [41]). We calculated several sensitivity checks for these choices (Web Appendix W2) and found that forecasting accuracy only slightly decreases if alternative choices based on competing rationales are made.We calculate the overall purchase probability for the new product  PtBuy New Product  by combining all states through multiplication following the logic of macro-flow modeling (Equation 8, Table 2). However, we also adjust the overall purchase probability of each participant depending on their intensity of information search for the new product  Pi(Information Search)  relative to the strongest competitors  Pj(Information Search)  as in the end of the purchase decision consumers commonly select between the top two products ([43]). Finally, to calculate the sales of the new durable at time (t), we multiply the mean of the purchase probability of all participants for the new durable  PtBuy New Product  with the market size  MarketSizet  of the product category (Equation 9, Table 2). Determinants of flowsThe determinants of flows are the independent variables that drive the flows between the behavioral states of the macro-flow model ([69]). We used the same determinants as [69] for the adopted flows and new determinants for all newly introduced or updated flows (see Figure 2). Studies 1a and 1b: Research Setting, Data, and AnalysesWe implemented our new forecasting approach in two large-scale field studies (Studies 1a and 1b) collaborating with two different durables' producers planning to launch new products. For data collection, we also collaborated with GfK to enlist participants from GfK's representative retail panels. Following standard market research procedures, we employed a screener to identify a representative sample of consumers in the market willing to buy a product in this product category (Web Appendix W3).For both field studies, the purchase journey simulation started by creating awareness, as awareness is the necessary precondition for consumers' subsequent purchase journey behaviors, and because the new durable had not been announced to the market ([ 3]). We created awareness by showing all participants actual marketing stimuli provided by the company launching the new product. Next, participants searched for information on the products in the purchase journey simulation. The simulation ended when participants bought any of the displayed products or they ended the simulation without making a purchase ([20]). For both field studies, we captured a virtual purchase decision because an actual preordering of the new products was not yet possible. After the simulation, participants allocated a fixed number of points among the products in a preference measurement ([47]). Finally, we conducted a survey with questions on additional input parameters (e.g., susceptibility to WOM) for the extended macro-flow model.The purchase journey was simulated in virtual reality. Because our collaboration partners required accurate and cost-effective results, we used both types of virtual reality: a lab virtual reality (via a head-mounted display with virtual reality controllers and motion-tracking sensors for interactions) and an online virtual reality (via a computer screen with mouse and keyboard for interactions). Both types showed the same products in exactly the same environments (i.e., local retail store, online shop, producers' webpages, online magazine, and use environment) and had exactly the same study flow (i.e., awareness creation, free information search, and the purchase decision). In addition, both types tracked identical individual-level behavioral data (Table 3): the type and number of actions per product at each information source, the time per action and product at each information source, and the visits per information source and product.GraphTable 3. Data Automatically Tracked in the Lab and Online Virtual Reality Simulations. Information SourceVariables Automatically Tracked in Virtual Reality EnvironmentsActions(Number per Product)Time(Per Action and Product)Visits(Per Information Source and Product)Local retail storeProduct picked from shelfProduct viewed (in 360 degrees)Product returned to shelfProduct put in the basketProduct removed from basketProduct purchasedTotal time in local retail storeTotal time per productTime of viewing each product (in 360 degrees)Number of visits at local retail storeNumber of visits per productOnline shopProduct selected for detail viewProduct viewed (in 360 degrees)Returned to overview pageSearch bar usedFilters usedSorting usedProduct put on wish listProduct removed from wish listProduct put in the basketProduct removed from basketProduct purchasedTotal time in online shopTotal time per productTime of viewing each product (in 360 degrees)Number of visits at online shopNumber of visits per productProducers' webpagesProduct selected for detail viewProduct viewed (in 360 degrees)Product put in the basketProduct removed from basketProduct purchasedTotal time on producers' webpagesTotal time on each producer's webpageTotal time per productNumber of visits at each producer's webpageNumber of visits per productOnline magazineViewed product reviewTotal time on online magazineTotal time per productNumber of visits at online magazineNumber of visits per productUse environmentProduct picked up/selectedProduct operatedProduct deselectedTotal time in use environmentTotal time per operated productNumber of visits at use environmentNumber of visits per product Study 1a forecasts the sales of a new kitchen appliance. The market for this appliance is extremely competitive even though the product is highly priced. Thus, we agreed with the collaborating producer to include 18 products in the simulation to ensure appropriate market coverage. In total, 305 potential buyers participated in Study 1a, with 80 in the lab and 225 in the online virtual reality. Participants' average age was 45 years, with 45% women. Study 1b forecasts the sales of a highly innovative gardening tool. Because of the tool's novelty, we decided with the collaborating producer that the simulation would include nine competitor products with varying prices, from the product category most similar to the new tool's area of application. Study 1b had 326 participants with an average age of 48 years and 50% women. In total, 101 participants completed the lab and 225 participants the online virtual reality simulation. We used the formulas summarized in Table 2 to calculate the forecasts in both studies. We report details on estimated flows in Web Appendix W4. Studies 1a and 1b: Aggregated-Level ResultsFigure 3 presents the sales forecasts calculated prior to launch for Studies 1a and 1b. We organize the aggregated results according to their focus on external validity (i.e., forecasts vs. actual sales and additional external validations) and internal validity (i.e., benchmark macro-flow model, other benchmarks, model variations, and comparison of virtual reality types) assessments of our new virtual reality forecasting approach.Graph: Figure 3. Studies 1a and 1b: sales forecasts compared with actual sales after launch. Forecasts versus actual salesTo assess the accuracy of our new forecasting approach, we compared the prelaunch sales forecasts with the actual monthly sales of the respective new durable (i.e., aggregated number of units sold in one calendar month), as recorded by GfK through its point-of-sales scanning. To assess first-year model performance, we summed the monthly sales over the first 12 months after launch.The results show that the prelaunch sales forecast (vs. actual sales data) in Study 1a achieves high forecasting accuracy with a MAPE for the monthly sales forecast of 18.6% and a forecasting error of only 1.9% for aggregated first-year sales. For Study 1b, the MAPE for the monthly sales forecast is 44.4%, and the forecast deviates from aggregated first-year sales by 20.0%, which is less accurate, mainly owing to deviations in months 9–12 (Figure 3). As our collaboration partner pointed out, these deviations were caused by the company's decision to increase ad spending after initial sales were below expectations. To reflect the company's decision in a recalculated sales forecast, we used its actual ad spending for months 9–12 as input (Equation 2 in Table 2). The forecast with the actual ad spending for months 9–12 is also shown in Figure 3 and achieves a high forecasting accuracy (i.e., 26.5% MAPE over 12 months and.0% forecasting error for aggregated first-year sales). Additional external validationsBecause, in both studies, the sales forecasting accuracy varies in different periods, we analyzed when and why deviations occur. We identified three factors that caused such deviations: external influences (e.g., unusual weather conditions that affected sales of gardening tools, as in Study 1b), problems during the product launch (e.g., supply chain issues, as in Study 1a, where sales started two weeks later than planned), and changes in marketing plans (e.g., an increase in ad spending, as in Study 1b). As outlined in Web Appendix W5, aggregating sales for longer time spans (e.g., quarterly), relying on confidence intervals, and calculating an updated forecast can remedy these deviations.In addition, we assessed longer-term predictions, the speed of adoption, and early uses of our new sales forecasting approach (Web Appendix W6). The results show that the approach can be used for longer-term predictions, as we demonstrate by validating a forecast for 22 months with actual sales data from Study 1a. The results reveal that this longer-term forecast is also highly accurate, with a MAPE between 14% and 23% for the monthly sales. Moreover, the cumulative sales forecast can be used to assess the speed of adoption and identify the takeoff ([24]). In addition, the results show how the approach can be used early in NPD if a virtual blueprint of the new product exists: either by calculating confidence intervals, as in the real-world application, or by making scenario-based predictions. Benchmark macro-flow modelTo validate our new sales forecasting approach's extended macro-flow model, we benchmarked it with its strongest precursor ""information acceleration"" ([67]). To ensure comparability, the benchmark macro-flow model uses the same behavioral states, flows, and determinants from management judgment, survey data, and virtual reality simulations as our new sales forecasting approach. However, we changed one component: the data used for estimating the purchase probability. Instead of using preferences and virtual purchases, we use the percentage of people who intended to buy the new product, as measured in the survey after the purchase journey simulation. This choice permits a prediction more in line with information acceleration and previous macro-flow models ([69]; [67]).The sales forecast derived from the benchmark macro-flow model is also depicted in Figure 3 and has an average MAPE over the first 12 months after introduction of 49.6% in Study 1a and 74.4% in Study 1b (forecast with actual ad spending: 67.2%). It is, on average, more than 30% less accurate than the sales forecasts derived from the extended macro-flow model (Study 1a: 18.6%; Study 1b: 44.4% [forecast with actual ad spending: 26.5%]). This comparison of the extended macro-flow model with the benchmark macro-flow model, which still uses virtual reality and all other amendments of the macro-flow model, provides strong evidence of the new approach's incremental contribution to the literature. However, we cannot assess from these results the incremental value of either the other amendments to the macro-flow model (for the value of different extensions, see the ""Model Variations"" subsection) or the use of virtual reality (for the value of virtual reality for prelaunch sales forecasting, see the ""Supplemental Analysis"" section). Other benchmarksFor further benchmarking the sales forecast calculated using the extended macro-flow model, we also compare it with conjoint analysis, other approaches to sales forecasting that require different data such as diffusion and utility models, and purchase intention conversion schemes (Web Appendix W7). The results reveal that the new approach overall achieves the highest forecasting accuracy. However, other forecasting approaches, such as conjoint analysis or surveys on purchase intentions, are generally associated with fewer data-gathering efforts and therefore provide alternatives for firms that have time or money constraints or need different or more high-level insights (e.g., estimating willingness to pay or making go/no-go decisions). Model variationsWe validated the extensions of the macro-flow model using model variations, which take out one new component at a time. Importantly, we could take out only the amendments we made to the macro-flow model as they are not shown as a stimulus in the simulation. Assessing the relative impact of amendments related to the simulation (e.g., use of virtual reality, the no-buy option, or the search for information) is not possible because participants' experiences in the simulation cannot be undone. To assess the impact of using virtual reality we ran a supplemental analysis, which we discuss after Study 2.Comparing the model variations (Table 4), we conclude that the greatest impact on forecasting accuracy due to macro-flow model extensions comes from the combination of virtual purchases with preferences in the purchase decision stage (Equation 7, Table 2; average accuracy increases across Studies 1a and 1b by 54.0%, with 15.8% coming from virtual reality data on the purchase decision and 38.2% from the preference measurement; see Variants 5 and 6 in Table 4). Adding awareness via third-party information (Equation 3, Table 2) leads to an average accuracy increase across the two studies by 29.8% (see Variant 1 in Table 4), whereas the contribution to forecasting accuracy is lower for the extensions of previous macro-flow models in the information search states (Equation 5, Table 2; average accuracy increase across the two studies by 3.2%; see Variants 2–4 in Table 4).GraphTable 4. Impact of Macro-Flow Model Extensions. ModelAwarenessInformation SearchPurchase DecisionAccuracyAccuracy IncreaseAdsWOMThirdActionsTimeVisitsVirtual PurchasesPreferencesMonthly Forecast Study 1aMonthly Forecast Study 1bMean Studies 1a and 1bMean Studies 1a and 1bExtended macro-flow model✓✓✓✓✓✓✓✓18.6%44.4%31.5%Basis for comparisonVariant 1✓✓✓✓✓✓✓40.4%82.1%61.3%+29.8%Accuracy Increase Due to Extensions in the Awareness Stage+29.8%Variant 2✓✓✓✓✓✓✓20.2%44.6%32.4%+.9%Variant 3✓✓✓✓✓✓✓23.7%43.9%33.8%+2.3%Variant 4✓✓✓✓✓✓✓18.6%44.5%31.5%+/−.0%Accuracy Increase Due to Extensions in the Information Search Stage+3.2%Variant 5✓✓✓✓✓✓✓22.7%72.0%47.3%+15.8%Variant 6✓✓✓✓✓✓✓88.4%51.0%69.7%+38.2%Accuracy Increase Due to Extensions in the Purchase Decision Stage+54.0% 3 Notes: The checkmark (✓) indicates that the model component is included. With this comparison, we investigate the incremental value of the extensions we made to the macro-flow model. However, it is impossible to extract the incremental value of virtual reality for the new forecasting approach from this table, because all depicted variants are computed based on an entire virtual reality consumer purchase journey and market environment simulation. For the value of virtual reality, see Table 6.Notably, findings showed that the preferences and the virtual purchases complement one another (Variants 5 and 6 in Table 4). Specifically, although observing a participant's virtual purchase (i.e., yes/no decision between purchasing vs. not purchasing) may be more representative of their choice, observing a nuanced preference may help incorporate some of the participant's uncertainty when making this choice. These rationales are also reflected in the results. First, the results reveal the highest forecasting accuracy across Studies 1a and 1b when virtual purchases and preferences are used together, confirming the intuition that combining the approaches allows for leveraging the advantages of both and increases forecasting accuracy ([27]). Second, the results show that while preferences have a higher incremental value in Study 1a (expensive product from a high-involvement product category), the incremental value of including the virtual purchases is higher in Study 1b (lower-cost product from a low-involvement product category). This is in line with studies showing that uncertainty can lead to increased selection of the no-choice option when products are associated with either high involvement or high financial risk ([ 6]; [30]). Comparison of virtual reality typesFinally, we compared the lab and online virtual realities. To assess whether the differences in the similarity to reality and immersion between the two (Table 1) lead to differences in sales forecasting accuracy, we compared aggregate sales forecasts derived solely on the basis of the lab participants and the online participants. In line with our expectations, the results reveal that in both studies the forecasting accuracy is higher for the lab (vs. online) virtual reality and increases on average by 6%. Studies 1a and 1b: Individual-Level Results Individual-level purchasesTo offer nuanced insight, we predicted individual purchase behavior within the virtual reality simulation from each participant's information search behavior and preferences. For this purpose, we used Equations 5 and 6 and an adapted version of Equation 8, without awareness and using only preferences. The results reveal that across both studies, the lab (vs. online) virtual reality condition, on average, predicts participants' purchase behavior within the simulation 6.5% more accurately. For Study 1b (Study 1a), we correctly predicted the purchase decision of 72.6% (54.1%) of the lab and only 62.4% (51.2%) of the online participants. The percentage of correct predictions is higher in Study 1b because Study 1a covers more products (Study 1b: 10 products; Study 1a: 18 products). In line with these aggregated findings, individual behavior prediction results show that, on average, lab (vs. online) virtual reality participants' behavior in the simulation was more consistent with actual buyer behavior. Behavioral consistencyBehavioral consistency refers to the extent to which consumers move through their purchase journey (i.e., from searching information to building preferences and making the purchase decision) in a coherent manner ([64]). We focus on behavioral consistency, because actual purchase behavior for durables tends to be a result of—and therefore, consistent with—consumers' information search behavior and preferences ([32]; [38]; [40]; [45]). Understanding differences in behavioral consistency between the virtual reality simulation types could therefore provide first insights regarding the drivers of the prediction accuracy differences. More precisely, in integrating insights from research on durable purchasing behavior ([32]; [45]) and virtual reality in other fields ([16]), we expect that lab virtual reality would motivate higher levels of behavioral consistency than online virtual reality—for instance, because it creates high immersion and comprehensive, realistic experiences (Table 1).To operationalize behavioral consistency, we developed a score reflecting these rationales: we counted the occurrence of consistent behavior throughout the simulation and the preference measurement. The score increases by a count of 1 when consistent behavior occurs (e.g., if a participant assigns a preference to a product that they have previously viewed, or if a participant purchases any product for which they previously assigned a preference). Therefore, the minimum score is 0, indicating no consistent behavior at all (i.e., the participant only assigned preferences to products that they did not view and purchased a product to which they assigned a preference of 0). The maximum consistency score is the number of products plus 1. This maximum score can be reached if a participant assigns a preference to each product, views each product, and purchases one.[ 8] Confirming our expectations, the results reveal significant differences between lab and online virtual reality for Studies 1a and 1b, with lab participants behaving on average more consistently than online participants (Study 1a: F( 1, 303) = 60.45, p =.000; MlabVR = 2.60 vs. MonlineVR =.93; Study 1b: F( 1, 323) = 27.82, p =.000, MlabVR = 2.08 vs. MonlineVR = 1.33). RQ 2 : Why Do Virtual Reality Simulations (vs. Traditional Studio Tests with Real Products) L... ApproachThe results of Studies 1a and 1b show highly beneficial consequences of using virtual reality technologies for prelaunch sales forecasting. Moreover, the comparison between lab and online virtual reality participants hints at increased behavioral consistency between information search, preferences, and virtual reality buying behavior as a reason behind their superior forecasting performance. However, the two field studies can neither disentangle the underlying mechanisms of these benefits nor conclusively quantify the specific contribution of using virtual reality for forecasting accuracy. To address this void, we conducted Study 2 to examine why virtual reality simulations lead to forecasting advantages by comparing a lab versus an online virtual reality purchase journey simulation versus a traditional studio test with real products (RQ2). Finally, we use Study 2 data and results to approximate the impact of virtual reality within the new prelaunch sales forecasting approach. Virtual Reality MechanismsPrior work in other contexts provides several clues why virtual reality might lead to advantages compared with previous visualization approaches. We organize these clues into two theoretical perspectives: presence and vividness. In addition, we consider alternative lenses (i.e., convenience and decision uncertainty), which we explain in the ""Results"" section. The focus on presence and vividness is in line with our conceptualization of virtual reality for prelaunch sales forecasting (Table 1). We argue that both lab and online virtual realities differ from traditional studio tests with real products in terms of simulation scope, similarity to reality, and immersion, whereas lab and online virtual realities differ only in their similarity to reality and immersion. Presence perspective""Presence"" refers to the phenomenon of the participant feeling as if they are actually in the simulation ([ 8]). Consequently, presence has an internal component (i.e., immersion—the degree to which senses are stimulated via interactions with the simulation) and an external component (i.e., realism of environment—the resemblance of the simulated environment of a real environment) ([ 2]; [16]; [49]; [59]).Studies indicate that virtual reality may enhance consumers' presence. For instance, virtual reality simulations provide visual connections and realistic experiences ([28]) or dynamic presentation formats that create feelings that the user is actually experiencing the product ([59]). We therefore expect that lab and online virtual reality (vs. a traditional studio test with real products) leads to advantages in prelaunch sales forecasting. In addition, because lab virtual reality uses additional equipment that may facilitate both participant immersion and an environment that further resembles an actual environment (Table 1), we expect that individuals in a lab (vs. online) virtual reality would experience higher presence. Owing to higher presence, consumers would better imagine the simulated situation and thus behave in virtual reality much as they would in the real world ([16]; [52]). Vividness perspectiveVividness denotes the extent to which consumers feel that a simulation is lively and detailed as well as easy to imagine and remember ([44]). According to vividness theory ([55]), lively simulations enhance consumers' imagination, which reduces the perceived effort to process the displayed information ([49]) and increases their involvement ([28]).In addition, studies show that interacting with a virtual object increases vividness and produces more images in the participant's mind than static pictures or text ([61]). Building on these insights, we expect that virtual reality simulations (vs. traditional studio tests with real products) will elicit higher vividness, which manifests in more consistent and realistic behavior. Because both virtual reality types can generate a very high simulation scope, we do not expect differences in vividness between them (Table 1). Study 2: Aim, Design, and MeasuresStudy 2 is a between-subjects experiment (n = 210) in which we compare a purchase journey simulation between lab virtual reality, online virtual reality, and a studio test with real products. We randomly assigned participants to one of the three conditions, resulting in 70 participants per condition. All three groups were asked to assess the same ten gardening tools in the simulated local retail store as presented in Study 1b, which was a do-it-yourself (DIY) store selling hardware and tools for home improvement (for the lab and online virtual reality visuals, see Figure 1). The choice to use an excerpt from Study 1b is in line with our goal for Study 2 to be comparable to Studies 1a and 1b but to offer more control.All information on the products was identical between the three conditions. However, in contrast to the virtual reality conditions with an entire DIY store with the ten gardening tools (very high scope of the simulation), participants in the studio test saw only a shelf with all ten gardening tools (i.e., medium simulation scope). This more focused visualization follows state-of-the art market research practice emphasizing the need for a focused, but realistic, depiction of the new product and its key competitors on a shelf as they would be in-store, but within a neutral and quiet environment to avoid any distraction. Compared with virtual reality simulations with 360-degree depictions of the environments and products, standing in front of a shelf with real products may create only partial similarity to reality. Moreover, the studio test is likely to create medium immersion, as it only stimulates senses with regard to product interactions (vs. stimulating senses with regard to product and environment interactions). Yet, in all three groups, participants could interact with the products and the shelf in a similar manner: they walked to the shelf, could pick up any of the ten products from the shelf as often as they liked, hold each one for as long as they liked, turn it to assess it from 360 degrees, return it to the shelf, or put it in their shopping basket for purchase. As in Studies 1a and 1b, participants could decide whether to assess all of the products or only some of them.Before the simulation, participants were asked questions regarding demographics and general characteristics. Afterward, we conducted a preference measurement and a survey asking for participants' assessment of the products and the simulation. Using confirmatory factor analysis, we assessed reliability and validity for each measure. Overall, the scales exhibit sufficient psychometric properties: for all constructs, the values for item reliabilities, composite reliability, and average variance extracted surpass the recommended thresholds ([ 1]; [39]). Moreover, the analysis of [23] criterion indicates that discriminant validity exists for all constructs. Table 5 provides an overview of the variables and their measurement.GraphTable 5. Study 2: Overview of Variables. VariableItems (Seven-Point Likert Scale)ILIRCRAVEPresence PerspectivePresenceI felt like I'd just been to a real store..93.88.93.83I forgot my actual surroundings during the simulation..80.64It was like I was really visiting a store..95.96ImmersionThe simulation immersed my senses..80.67.86.68I was able to do everything in the simulation that would have been possible in a real store..73.54The simulation activates the same senses as a real store visit..85.83Realism of environment (Cho, Shen, and Wilson 2014)The visual depiction of the store seemed real..96.92.98.95The visual depiction of the store was realistic..98.97The store shown during the simulation looked real..97.94Vividness PerspectiveVividness(Keller and Block 1997)How would you rate the depiction in the simulation?Not vivid vs. vivid.69.48.83.63Not concrete vs. concrete.82.79Not easy to picture vs. easy to picture.77.62Convenience PerspectiveConvenience(Donthu and Gilliland 1996)The simulation was convenient..82.69.88.72The simulation made it easy to gather information for making a purchase decision..87.85The simulation helped me to quickly gather information..77.60Usefulness(Davis 1989)I found the simulation useful..86.79.90.75The simulation made the purchase decision easier..81.68The simulation enhanced my effectiveness in making a purchase decision..86.77Ease of use(Davis 1989)I found the simulation easy to use..88.82.91.77The use of the simulation was clear and understandable..85.75Learning how to operate the simulation was easy..85.75Decision Uncertainty PerspectiveDecision uncertainty (Rosbergen, Pieters, and Wedel 1997)I am uncertain of my choice..86.75.93.82I did not know if I made the right purchase..90.84I felt a bit at a loss in choosing one of the products..91.88Additional Variables of InterestRealism of product (Cho, Shen, and Wilson 2014)The visual depiction of the products seemed real..95.94.91.77The visual depiction of the products was realistic..94.89I could experience the products as in real life..69.47Task involvement (Pham 1996)I was very motivated to reach an accurate evaluation of the products..77.61.86.68I did put much effort into the evaluation of the products..84.81It was important for me to examine the products carefully..77.61Manipulation ChecksRisk aversion(Donthu and Gilliland 1996)I prefer not to take risks when purchasing anything..82.73.84.65In general, I avoid risky things..83.78I would rather be safe than sorry..66.43Technology experience (Beatty and Smith 1987)Compared to the average person,......I am highly knowledgeable regarding new technologies..90.81.95.87...I regard myself as very experienced in new technologies..95.94...I know a lot about new technologies..92.85Need for touch(Peck and Childers 2003)I rather trust products that can be touched before purchase..89.85.91.77I feel more comfortable purchasing a product after physically examining it..89.83I feel more confident making a purchase after touching a product..79.63 4 Notes: IL = item loading; IR = item reliability; CR = composite reliability; AVE = average variance extracted. Study 2: Analyses and Results Overview and manipulation checksWe used analysis of variance (ANOVA) to analyze the data obtained from the controlled laboratory experiment and summarize the results in Figure 4, Panels A–I. The findings reveal that the randomization of participants worked, as the groups did not differ with respect to gender (F( 2, 207) =.11, p =.894) and other traits, such as risk aversion (F( 2, 207) =.04, p =.961), technology experience (F( 2, 207) = 1.05, p =.350), and need for touch (F( 2, 207) = 1.40, p =.248).Graph: Figure 4. Study 2: ANOVA results for virtual reality mechanisms.*p <.1.**p <.05.***p <.01.aDirect measure of theoretical perspectives.Notes: Error bars = ±1 SEs. Behavioral consistencyBuilding on Studies 1a and 1b, using the individual-level information search, preference, and purchase behavior data from virtual reality, we assessed behavioral consistency. The results reveal significant differences in behavioral consistency between virtual reality and non–virtual reality participants (Figure 4, Panel A). Specifically, confirming the results of Studies 1a and 1b, we find that behavioral consistency is highest for the lab virtual reality, followed by online virtual reality and the studio test. Therefore, in line with our expectations, virtual reality participants show behaviors in the simulation that are closer to actual consumer behavior than studio test participants. Presence and vividnessThe ANOVA results provide support for the presence and vividness perspectives (Figure 4, Panels B and E). The findings show great differences between the groups for the direct measures of presence and vividness. The findings also show significant group differences for the constructs associated with the presence perspective (Figure 4, Panels C and D): immersion and realism of the environment.[ 9] Overall, the results confirm the rationales for why the virtual reality advantage occurs: lab virtual realities, due to the additional equipment use, generate higher presence than online virtual realities, whereas both virtual realities have advantages over a traditional studio test due to their higher presence and vividness. Alternative theoretical perspectivesAs Table 5 shows, we measured further constructs to rule out alternative explanations. For example, research on virtual reality suggests that it may facilitate consumers' convenience (i.e., the extent to which consumers perceive the simulation as easy to use and useful) by expediting their access to relevant decision parameters or enhancing the possibilities to explore the products relative to visualization modes not relying on new technologies ([28]; [34]; [49]). Moreover, virtual reality simulations could reduce participants' decision uncertainty (i.e., the extent of difficulty in making a purchase decision) by collecting more information and better integrating information than visualization modes that do not use virtual reality ([ 5]; [49]). However, the complexity of a virtual reality simulation, as well as usage troubles (e.g., dizziness, fatigue), could diminish potential convenience and decision uncertainty reduction advantages ([53]). In line with these rationales, the results do not reveal differences in the direct measures of decision uncertainty (Figure 4, Panel I) and convenience (Figure 4, Panel F), or for constructs associated with convenience (Figure 4, Panels G and H). Additional variables of interestTo ensure that the virtual product representations in the lab and online virtual realities were displayed adequately, we assessed product realism (i.e., the extent to which the depicted product resembles the real product). We expected no differences, as the virtual product representations were created from 360-degree pictures taken of the actual products. The results confirm these expectations and show that the physical and virtual products are adequate benchmarks (F( 2, 207) =.60, p =.550; MlabVR = 5.03 vs. MonlineVR = 4.90 vs. Mstudio test = 5.14). In addition, the results on task involvement (i.e., the extent to which participants were focused on the experimental task at hand) show no significant differences between the groups (F( 2, 207) = 1.09, p =.338; MlabVR = 5.84 vs. MonlineVR = 5.61 vs. Mstudio test = 5.62). Supplemental Analysis: Assessment of Virtual Reality Advantages in Sales ForecastingStudies 1a and 1b reveal beneficial consequences of using virtual reality technologies for prelaunch sales forecasting. Study 2 offers suggestive evidence why these benefits occurred. However, due to the absence of a non–virtual reality control condition in Studies 1a and 1b and the more focused design of Study 2 (i.e., virtual reality mechanisms), we still cannot conclusively disentangle how much of the observed forecasting advantage can be attributed to virtual reality. To approximate this, we jointly used the results from Studies 1b and 2: Although Study 2 has a smaller scope and does not completely replicate the simulation of Study 1b, both studies are comparable because they include the identical DIY store simulation. We conducted the approximation in two steps.First, we approximated the total effect of using (vs. not using) virtual reality for prelaunch sales forecasting. To do so, we built an artificial, non–virtual reality control condition for Study 1b in leveraging the non–virtual reality control condition of Study 2 (i.e., studio test using real products): we screened Study 2 data for differences between the virtual reality and non–virtual reality conditions in metrics that serve as inputs for forecasting. This screening revealed that while the preferences for the focal new durable product were highly similar across studies for the lab and online virtual reality participants, these preferences were 1.2 times higher for non–virtual reality participants. To account for this difference, we multiplied the preferences of Study 1b participants by 1.2 to create the artificial, non–virtual reality control condition. Using this control condition, we then calculated another sales forecast for Study 1b (Modification 1 in Table 6), which uses neither virtual reality data (i.e., no actions, time, or visits in the information search stage and no virtual purchases in the purchase decision stage) nor virtual reality for the visualization of the purchase journey (i.e., because participants' preferences were adjusted; see previous step). Comparing the Modification 1 forecast with the Study 1b forecast indicates that the usage of virtual reality accounts for an accuracy increase of roughly 50%–70% (Table 6).GraphTable 6. Approximation of Virtual Reality's Impact on Forecasting Accuracy. Extended Macro-Flow ModelVisualizationof Purchase JourneyMAPEMonthly ForecastStudy 1b(Original Prelaunch Forecast)MAPEMonthly ForecastStudy 1b (Forecast with Actual Ad Spending)AwarenessInformation SearchPurchase DecisionAdsWOMThirdActionsTimeVisitsVirtual PurchasesPreferencesData Provided byAutomated TrackingStep 1: Approximatingthe total effect of using (vs. not using) virtual reality for prelaunch sales forecastingVirtual reality prelaunch sales forecasting approach✓✓✓✓✓✓✓✓Lab and online virtual reality(Study 1b)44.4%26.5%Modification 1:No virtual reality data and visualization✓✓✓————✓Studio test (Study 2)∼100%∼95%Total Effect: Approximated Accuracy Increase Due to Using Virtual Reality:▵ ≈ 55%▵ ≈ 70%Step 2: Splitting thetotal effect betweenvirtual reality's automated trackingand visualization capabilityVirtual reality prelaunch sales forecasting approach✓✓✓✓✓✓✓✓Lab and online virtual reality(Study 1b)44.4%26.5%Modification 2:No virtual reality data✓✓✓————✓Lab and online virtual reality(Study 1b)∼70%∼50%Effect from Virtual Reality's Automated-Tracking Capability:▵ ≈ 25%▵ ≈ 25%Effect from Virtual Reality's Visualization Capability:(i.e., difference between total effect and effect from virtual reality's automated-tracking capability)▵ ≈ 30%▵ ≈ 45% Second, we split the total effect between virtual reality's automated-tracking and visualization capability. To do so, we calculated one more sales forecast (Modification 2 in Table 6) that contains virtual reality for the visualization of the purchase journey (i.e., by using original preferences of Study 1b participants) but does not include any virtual reality data (i.e., no data on actions, time, or visits during information search and no virtual purchases). Thus, comparing this modification with the original sales forecast of Study 1b provides insights on the contribution of virtual reality's automated-tracking capability, which is roughly 25%. Next, to isolate the contribution of virtual reality's visualization capability, we jointly interpreted the results of Modifications 1 and 2. The joint interpretation is necessary because it is impossible to calculate a sales forecast using data tracked in virtual reality without having a visualization in virtual reality. Findings indicate that virtual reality's visualization capability accounts for a forecasting accuracy of roughly 30%–45% in Study 1b (Table 6). Thus, the impact of virtual reality's visualization capability seems to be higher than its automated-tracking capability. However, due to the artificial nature of the non–virtual reality condition, we still cannot conclusively disentangle the exact impact of virtual reality. Discussion Summary of Key FindingsThe results of Study 1a (kitchen appliance) and Study 1b (gardening tool) show that our new virtual reality sales forecasting approach yields highly accurate predictions. Moreover, these results indicate that these benefits occurred because lab virtual reality motivated, on average, consistent and realistic consumer behavior, whereas these metrics were at a lower level for online virtual reality. To provide further insight, in Study 2 we compared the two virtual reality types with a studio test with real products. The findings confirm that lab virtual reality simulations motivated more consistent consumer behavior than online virtual reality, and that virtual reality creates superior behavioral consistency compared with the studio test. Moreover, the findings indicate that these effects are due to advantages that virtual reality creates in terms of presence and vividness, not alternative theoretical explanations, such as convenience and decision uncertainty. Using Study 2 and Study 1b findings jointly allowed us to approximate that the forecasting advantage attributed to virtual reality is likely to range between 50% and 70% (Table 6). Theoretical Contributions and Future Research DirectionsThis investigation yields results that advance prior work in several ways. First, researchers have developed forecasting models for new durables that are applied close to launch ([25]). We advance the literature by conceptualizing and testing a novel virtual reality sales forecasting approach that can also be applied in earlier NPD stages because firms do not need a fully developed product but only a virtual blueprint, which serves as input for the programming of the virtual reality simulation. This flexible application of the new approach is an important advancement, because in earlier NPD stages, investments in a new product are relatively small compared with investments in later stages ([51]). Future work could build on these insights and investigate important extensions. For example, in both field studies, our collaborating partners were able to categorize the products, anticipate the time until launch, as well as the market environment at launch. However, because for some innovations and at very early stages of NPD the categorization of a new durable may be difficult (Web Appendix W6), future investigators could develop more precise solutions that help anticipate the future market environment or simulate the exact time until launch.Second, in developing the new virtual reality forecasting approach, we extended previous macro-flow models in three ways. We added new behavioral states (e.g., aware by third-party information, extensive and flexible information search); we introduced new equations for calculating the flows (e.g., Equation 5 in Table 2) and new determinants of the flows (e.g., behavioral data from virtual reality). Through these amendments, the extended macro-flow model covers the purchase journey comprehensively and realistically. It is therefore likely to be more generally applicable, as in our two field studies with two very different durables, than previous macro-flow models that focused on one particular industry (e.g., automotive in [69]]). Future work could build on these insights and examine how macro-flow models as well as the virtual reality simulations need to be adapted to adequately predict buyer behavior for other types of products or markets and therefore extend the external validity of the new forecasting approach. For example, if the product's value is low (and buyers are not very systematic in their behavior), how should behavioral data and preference measurement data be combined for the purchase decision stage (Equation 7, Table 2)? If the innovation is a service, are there additional determinants that need to be included in the information search stage (Equation 5, Table 2) to account for the service's individuality? How should competing services be depicted in virtual reality? Finally, if the new product is sold in a business-to-business market with different members involved in the decision making, should they jointly use the virtual reality simulation? How should their data be aggregated to predict the buying center purchase decision (Equation 8, Table 2)?Third, researchers have focused on developing and testing prelaunch sales forecasting approaches ([69]; [67]). We advance the literature by providing additional insights into why forecasting advantages occur. In doing so, the findings also expand the knowledge on the mechanisms of virtual reality. However, while we show across two large-scale field studies that the two key features we add—virtual reality and macro-flow model extensions—substantially improve the forecasting accuracy, we cannot conclusively assess how much of the accuracy increase is due to virtual reality. Thus, we encourage future studies on virtual reality sales forecasting to include a non–virtual reality control condition and to more directly isolate the impact of virtual reality and the contributions by each of its mechanisms.Finally, we offer the first systematic marketing study of virtual reality in NPD. Future work should build on these insights to further examine how firms can use virtual reality to advance their capabilities. For example, as [19] has shown, firms' marketing capabilities fall into two main categories: customer linking and market sensing. While we leverage virtual reality for an improved market sensing in terms of highly accurate prognoses, future work could focus on how to use virtual reality to improve the market sensing related to diagnostics (e.g., by studying consumer heterogeneity or idiosyncratic purchase journeys, testing marketing materials, or optimizing the marketing mix) and customer-linking purposes (e.g., by integrating this knowledge into new products, offering remote services or trainings). Managerial ImplicationsOur study contains actionable implications that fall into three categories: ( 1) application fields of virtual reality for NPD, ( 2) visualization mode selection, and ( 3) guidance for implementing virtual reality for NPD. These implications are derived from our field studies (Studies 1a and 1b), the controlled laboratory experiment (Study 2), and several in-depth interviews that we conducted with senior managers of leading durable producers. Application fieldsDespite the increasing interest in virtual reality, the in-depth interviews revealed that many companies remain reluctant to use virtual reality as a matter of routine, given, for instance, the complexity and costs of programming virtual reality environments. Moreover, the interviews surfaced that companies experience challenges when implementing virtual reality in NPD due to a lack of knowledge within the company and that effective use cases for this technology remain scarce. We identified prelaunch sales forecasting as a promising application field for virtual reality. Furthermore, the interviews hinted at additional applications for virtual reality in NPD associated with its superior visualization capability, as a senior manager of a global technology company pointed out:Virtual reality is a great tool to showcase products, objects and entire worlds. We can depict those products, objects and worlds in a realistic manner, show them to other people and especially make our products more easily understandable. This is something very valuable during new product development.Drawing on this insight and blending it with the existing literature, we identified other use cases for virtual reality across all NPD stages. For example, research has shown that virtual reality can be beneficial in the early stages of NPD and improve idea generation outcomes by creating immersive collaboration platforms ([ 5]). In addition, we demonstrate how virtual reality can improve forecasting outcomes using realistic and comprehensive purchase journeys (Studies 1a and 1b). Moreover, using the interview insights, we also derive use cases for NPD's medial stages, such as concept and product development. For instance, one of our interview partners, a senior manager of a leading household appliance producer, reported very positive experiences with virtual prototype testing:To make our innovation process more agile, we recently experimented with virtual prototypes and compared the results to actual prototype testing. Stunningly, we did not find any differences in diagnostic information between the two![10] Visualization mode selectionThe results show that virtual reality can reap large benefits, such as more flexible use throughout NPD or more accurate prognoses. However, the results also reveal some limitations of virtual reality: it can be expensive and have some use constraints when testing haptics ([16]; [53]). We therefore present specific guidelines for managers on how to choose between lab virtual reality and online virtual reality, as well as when to eschew virtual reality (Figure 5).Graph: Figure 5. Guidance for managers on when to utilize virtual reality in NPD.First, in weighing the costs and benefits of virtual reality, we recommend that managers use virtual reality simulations to improve NPD decisions that require early, detailed, and precise consumer information (i.e., user experience testing, purchase journey analyses, prelaunch sales forecasting, and point-of-sale optimization). Moreover, we suggest that the business case for using virtual reality is especially strong if a new durable is expensive, is highly innovative, or requires considerable explanation. We make this recommendation because findings reveal that using virtual reality results in realistic consumer behavior even for these products, whereas prior work has shown that accurately predicting sales for such products is hardly possible with traditional methods ([37]).Second, we suggest that managers carefully choose between lab and online virtual reality as the visualization mode. Managers should choose lab virtual reality if highly accurate consumer insights are required, because our findings reveal that lab virtual reality generates significantly higher presence, which manifests in more consistent behavior. In contrast, managers should use online virtual realities when cost constraints are high, data gathering needs to be conducted promptly, and large samples are required. If companies require scalability and very detailed and realistic insights, they can combine online and lab virtual realities (Figure 5). Finally, managers should refrain from using virtual reality when they only need high-level insights. For example, if they need to make general go/no-go decisions or long-term market share predictions, virtual reality simulations may be too expensive and traditional market research methods, such as purchase intention surveys or conjoint analysis, are likely to have a superior cost–benefit ratio. In addition, when testing haptics, traditional studio tests with physical prototypes may lead to better diagnostic information as simulating haptics in virtual reality is difficult ([16]). Guidance for implementing virtual reality for NPDWe have two recommendations for managers interested in implementing virtual reality in their firms' NPD. We recommend clearly defining the aims of using virtual reality and specifying a clear and comprehensive implementation plan to ensure focus throughout the project (Web Appendix W8). Our experience from working with and interviewing durable producers on new virtual reality use cases reveals that many companies get very excited about this technology, sometimes resulting in unrealistic expectations regarding simulation scope and length. To avoid overburdening of the virtual reality simulations, we recommend refraining from ""all-in-one-solutions"" and aim for one tool for one purpose (e.g., forecasting, prototype testing). Using virtual reality several times during NPD creates synergies because programming of virtual prototypes or environments can be slightly adjusted to different tests and contexts. "
42,"When to Use Markets, Lines, and Lotteries: How Beliefs About Preferences Shape Beliefs About Allocation When allocating scarce goods and services, firms often either prioritize those willing to spend the most resources (e.g., money, in the case of markets; time, in the case of lines) or simply ignore such differences and allocate randomly (e.g., through lotteries). When do these resource-based allocation rules seem most appropriate, and why? Here, the authors propose that people are more likely to endorse markets and lines when these systems increase the likelihood that scarce goods and services go to those who have the strongest preferences—that is, when they help sort preferences. This is most feasible when preferences are dissimilar (i.e., some consumers want something much more than others). Consequently, people are naturally attuned to preference variance: when preferences for something are similar, markets and lines seem less appropriate, because it is unlikely that the highest bidders or those who have waited the longest actually have the strongest preferences. However, when preferences are dissimilar, markets and lines seem more appropriate, because they can more easily sort preferences. Consumers thus react negatively when firms use resource-based allocation rules in situations where preferences cannot be easily sorted (e.g., when preferences are similar).Keywords: allocation; customer segmentation; fairness; lines; lotteries; markets; queues; scarcityWhen allocating scarce goods and services, there are many ways to determine who gets what ([63]). Often, however, firms either prioritize those willing to spend the most resources (e.g., money, in the case of markets; time, in the case of lines) or simply ignore such differences and allocate randomly (e.g., through lotteries). For example, Live Nation, a concert promoter, auctions tickets to the highest bidders (i.e., to those willing to spend the most money; ""Ticketmaster Auctions""). In contrast, its chief rival, AEG, administers lotteries, selling face-value tickets to randomly selected fans (""Fair AXS""). Or consider the television network NBC, which allocates advance tickets to tapings of Saturday Night Live via lottery before the start of each season. After the start of the new season, however, it allocates standby tickets via lines (i.e., to those willing to spend the most time).Given the considerable differences between these systems, when do markets, lines, and lotteries seem most appropriate, and why?The economics of market design (e.g., auction theory [[55]], matching theory [[64]]) offers a rich toolkit for determining which allocation rules are optimized for different goals, but it does not provide guidance for which the public will most readily endorse or regard as most fair. Yet, this is a critical issue for marketing theory and practice: beliefs about fairness not only pose a fundamental psychological question for researchers but also place significant constraints on firms ([42]; [62]).In this research, we suggest that people more strongly endorse markets and lines when they believe these resource-based allocation rules increase the likelihood that scarce goods and services will go to the consumers who have the strongest preferences—that is, when markets and lines can help sort preferences. Critically, people believe this is most feasible when preferences are dissimilar (i.e., some consumers want something much more than others). So, for example, it might seem more appropriate for concert promoters and television networks to use a market or line when tickets are broadly available to the general public (where preferences are dissimilar) but less appropriate when tickets are available only to a fan club (where preferences are similar). Beliefs About Markets and LinesThere are many reasons why markets might seem appropriate for determining who gets what. Markets facilitate price discovery. They can help supply meet demand. They might also encourage innovation and entrepreneurship and are generally viewed as legitimate and just ([40]). In addition, the norms of exchange underlying markets in consumer contexts are a basic feature of social relationships more broadly ([26]). As a result, markets have sprung up in many unconventional settings. For example, some food banks bid on donations ([60]), some college students bid on classes ([13]), and even some prisoners of war invented currency to bid on rations ([61]).Likewise, there are many reasons why lines, queues, or first-come, first-served policies might seem appropriate. Firms benefit when lines signal positive product or firm characteristics, particularly when demand exceeds supply ([ 2]; [ 8]), and consumers benefit from their inherent egalitarianism. They require people to spend time, a resource believed to be more equally distributed than money ([67]).Yet there are also many compelling reasons why these allocation rules might seem inappropriate, particularly with respect to markets. For example, people believe it is taboo to exchange resources such as money for something sacred, such as human organs ([27]; [54]; [77]). Moreover, consumers are wary of the possibility that markets will generate unfair profits ([42]; [57]) or incentivize actions inconsistent with social good ([11]). Meanwhile, because waiting can be aversive, lines sometimes trigger negative reactions from customers, including frustration, anxiety, and boredom ([19]; [22]; [46]; [74]; [82]).Prior research has therefore identified many specific instances in which people endorse or resist markets and lines, but there is not yet a systematic framework for understanding these beliefs more broadly. Indeed, even studies that directly compare these allocation rules with each other (e.g., [28]; [41]; [65]) primarily describe consumer attitudes without explaining why they hold them. Furthermore, prior work does not predict when one approach might seem more appropriate than another. Our theory aims to address this gap. Beliefs About PreferencesWe assert that beliefs about when to use markets and lines depend on the extent to which these allocation rules can help sort preferences. This assertion is based on prior research, which shows that people care a great deal about distributive efficiency, or the allocation of goods and services to those with the strongest preferences ([47]; [48]; [49]; [81]). Moreover, recent work demonstrates that people view allocation rules as fairer when they make it possible for consumers to signal their preferences clearly ([67]). So, although there are many goals that markets and lines can potentially help achieve, people seem particularly focused on whether these allocation rules ensure that scarce goods and services go to those who want them the most. But when and how is this possible?We suggest that the answer depends on beliefs about preference variance, which, in turn, shape attitudes about whether preference sorting is feasible. In particular, we propose that when people believe everyone has dissimilar preferences for something (e.g., some consumers want it much more than others), they anticipate that it will be easier for a market or line to sort those with stronger preferences from those with weaker preferences; conversely, when consumers believe everyone has similar preferences (e.g., all consumers want something to roughly the same degree), they anticipate that sorting them will be more difficult.This reasoning suggests that consumers will view markets and lines as less appropriate (and less fair) when preferences are similar and more appropriate (and fairer) when they are not. This is because when preferences are similar, people will doubt that the highest bidders or those who wait the longest actually have the strongest preferences. In other words, preference sorting seems less feasible. So, it might seem fairer to simply ignore these trivial differences, which would be difficult to accurately detect anyway. Instead, it could seem more appropriate to allocate randomly (i.e., use a lottery). However, when preferences are dissimilar, it will seem more plausible that the highest bidders or those who wait the longest actually have the strongest preferences. Now, preference sorting seems more feasible, and ignoring those nontrivial differences in preferences (e.g., by using a lottery) would seem unfair, because someone with very weak preferences would have the same chance at acquiring something as someone with very strong preferences.It is worth noting that it is mechanically the case that a market or line can more easily sort preferences when they are dissimilar. Yet it is unclear whether consumers acknowledge or appreciate this basic economic insight, much like they fail to acknowledge or appreciate others. For example, people often do not recognize the positive gains from trade (instead assuming exchanges are zero sum; [ 3]; [36]; [39]) or the incentive value of profit (instead viewing it harmful to society; [11]).Beliefs about the appropriateness of markets and lines could be more strongly tied to any number of other factors aside from their ability to sort preferences. For example, they could depend on which allocation rule reflects the status quo ([43]), whether prices reflect quality ([16]; [51]; [75]), reference transactions ([ 1]; [30]; [42]), or religious and moral views ([27]; [54]; [77]). But if our assertion holds, then people's intuitions about preference sorting may represent a key way in which lay economic beliefs align with textbook economic principles. Hypotheses and StudiesFirst, we propose that the distribution of preferences will influence endorsement of markets, lines, and lotteries—as well as perceptions of fairness (because we assume that people endorse allocation rules they regard as fair). H1:  Consumers are more likely to endorse and regard as fair resource-based allocation rules (e.g., markets and lines) when they believe preferences are dissimilar.Second, intuitions about preference sorting will play an explanatory role. H2:  The belief that resource-based allocation rules (e.g., markets and lines) help sort preferences mediates the effect of preference variance on endorsement of resource-based allocation rules.Several theoretical and managerial implications follow from these predictions (Figure 1). First, implicit in H1 and H2 is the assumption that willingness to spend resources and preferences are correlated (if sometimes imperfectly; e.g., [69]; [72]; [80]; [82]). Therefore, factors that undermine this correlation should attenuate the effect. One such variable is inequality salience. For example, people find that it is easier to infer preferences from the amount of time someone is willing to spend to acquire something than from the amount of money they are willing to spend. This is, in part, because time is believed to be more equally distributed than money ([67]). So, if inequality in the distribution of a resource were salient, it might reduce the perceived ratio of signal (e.g., preferences) to noise (e.g., spending uncorrelated with preferences). This, in turn, would render preference sorting less feasible—even if preferences were dissimilar. As such, moderation by inequality salience would corroborate our proposed preference sorting mechanism. H3:  Inequality in the distribution of a resource, when salient, moderates the effect, attenuating endorsement of resource-based allocation rules.Graph: Figure 1. Conceptual framework.Second, there may be certain goods or services that people simply think should never be allocated on the basis of willingness to spend resources. For example, people treat wants (learned desires) differently than needs (basic requirements; [10]; [21]; [44]; [52]), which are protected by sacred values ([ 4]; [76]) and governed by moral reasoning ([ 6]; [38]; [73]). As a result, people are often uncomfortable with using markets to allocate needs ([ 5]; [53]; [66]), especially when the neediest have the fewest resources. This suggests that even if preferences for something construed as a need were dissimilar—and furthermore even if those preferences could be sorted by a market or line—people would nevertheless prefer a different basis for allocation (likely one sensitive to differences in need, rather than want). So, for needs, preference sorting should no longer matter. H4:  The type of good or service, when perceived as a need, moderates the effect, attenuating endorsement of resource-based allocation rules.Finally, when firms misapply these allocation rules (i.e., choose the option regarded as less appropriate), the resulting perceptions of unfairness will yield negative downstream consequences. This is consistent with work showing that consumers are less likely to patronize businesses believed to have engaged in unfair practices ([14]; [33]). Perceptions of unfairness reduce willingness to pay (WTP; [12]), trigger complaints ([37]), decrease satisfaction ([34]; [58]), and can even arouse a desire for vengeance ([ 7]). H5:  Misapplication of these allocation rules (e.g., use of a resource-based allocation system when preferences are similar) reduces purchase intentions.We conducted a total of 13 studies (N = 5,159; Table 1) to explore this account, and we report all variables tested. For studies that included instructional manipulation checks ([59]), we excluded failures prior to analysis. Data, stimuli, and code are publicly available.[ 6]GraphTable 1. Overview of Studies. StudyNHyp.ContributionMain FindingEndorsement of Markets and LinesSig.High Pref. VarianceLow Pref. VariancePilot A200H1Establishes correlationBeliefs about the distribution of preferences for 25 real-world items were correlated with endorsement of a market for allocating them.——***Pilot B199H1Establishes correlationBeliefs about the distribution of preferences for 25 real-world items were correlated with endorsement of a line for allocating them.——***1a525H1Establishes causal effectWhen preferences were dissimilar, participants endorsed a market.47%31%***1b602H1Establishes causal effectWhen preferences were dissimilar, participants endorsed a line.59%35%***2a405H1Reveals that consumers try to infer preference variance (without prompting)Participants more strongly endorsed a market for allocating concert tickets to the general public (dissimilar preferences) than a fan club (similar preferences).3.662.43***2b222H1Reveals that consumers try to infer preference variance (without prompting)Participants more strongly endorsed a line for allocating concert tickets to the general public (dissimilar preferences) than a fan club (similar preferences).4.803.89***3366H2Tests mediation by preference sorting; examines fairnessParticipants more strongly endorsed a market or line for allocating beer when preferences were dissimilar, due to preference sorting.3.512.07***4202H2Tests mediation by preference sorting; presents a consequential choiceParticipants cast consequential votes for allocating a prize to the highest bidder (e.g., use a market) when preferences for it were dissimilar.53%37%*5566H3Shows that inequality salience breaks the link between preference variance and preference sortingWhen preferences for an electric truck were dissimilar, participants endorsed a market for allocating it, but not when inequality was salient.3.432.49***6376H4Shows that product type breaks the link between preference sorting and endorsement of resource-based allocation rulesWhen families differed according to how much they wanted rental cabins, participants endorsed market for allocating them, but not when families differed according to how much they needed rental cabins.4.013.05**7508H5Documents implications for purchase intentionsMisapplication of these allocation rules (e.g., using a market when preferences are similar) reduced purchase intentions.3.643.05*Supp. 1a493H1Extends causal effectWhen WTP for basketball tickets varied, participants endorsed a market.37%22%***Supp. 1b495H1Extends causal effectWhen willingness to wait for basketball tickets varied, participants endorsed a line.88%78%** 1 *p < .05.2 **p < .01.3 ***p < .001.Specifically, in Pilots A and B, we examine the relationship between beliefs about preference variance and endorsement of markets, lines, and lotteries for 25 real-world products and services. We then manipulate preference variance directly (Studies 1a and 1b) and indirectly (i.e., leaving participants to infer it; Studies 2a and 2b). Next, to probe our proposed mechanism, we explore whether intuitions about preference sorting mediate the effect (Studies 3 and 4) and test two theoretically derived moderators: inequality salience (Study 5) and product type (Study 6). Finally, to highlight managerial implications, we examine whether misapplication of these allocation rules reduces purchase intentions (Study 7).[ 7] Pilots A and B: Beliefs About Real-World PoliciesWe first tested whether beliefs about preference variance predict endorsement of markets (Pilot A) and lines (Pilot B) for allocating 25 real-world goods and services. MethodFor Pilot A, we recruited 200 Amazon Mechanical Turk (MTurk) participants (Mage = 32.81 years; 84 women, 116 men); for Pilot B, we recruited 199 MTurk participants (Mage = 34.96 years; 82 women, 117 men). Both pilots employed a within-subject design, in which participants evaluated 25 items along two dimensions, in two counterbalanced blocks: preference variance and endorsement of markets (vs. lotteries; Pilot A) or endorsement of lines (vs. lotteries; Pilot B).We measured preference variance by asking participants ""whether people differ in how much they want or need 25 different products and services."" Specifically, for each item (presented in random order), we asked, ""For [item], what do you think is generally the case?"" (1 = ""Some people want/need to purchase [item], while some people do not want/need to purchase [item],"" and 7 = ""Everyone wants/needs to purchase [item]."").We measured endorsement of markets, lines, and lotteries by asking participants ""how 25 different products and services should be allocated."" Specifically, for each item (presented in random order), we asked, ""Imagine that at the current price there are not enough available [item] for everyone who wants or needs them. How should the [item] be allocated?"" One option was a lottery: ""Use a lottery (i.e., select people randomly) to decide who gets to purchase the [item]. The people selected can get [item] at the current price. The people not selected will not be able to get [item]."" In Pilot A, the alternative was a market: ""Sell the [item] to the people who will pay the most. The people willing to pay the most will get [item] at the price they offer. The people willing to pay the least will not be able to get [item]."" In Pilot B, the alternative was a line: ""Use a first-come, first-served policy to decide who gets to purchase the [item]. The people who are the first to request (or have spent the most time waiting) will be able to get [item]. The people who are the last to request (or have spent the least time waiting) will not be able to get [item]."" Results and DiscussionWe reverse-coded preference variance ratings for ease of explication (so higher numbers correspond to greater preference variance). We then calculated the correlation between preference variance and endorsement of a market (market = 1, lottery = 0) or a line (line = 1, lottery = 0) across all items (i.e., using 25 pairs of observations). We observed a positive relationship in both pretests (Pilot A: r = .86, p < .001; Pilot B: r = .77, p < .001; Figure 2).Graph: Figure 2. Pilots A and B: Perceived preference variance for a product or service correlates with endorsement of markets and lines for allocating that product or service.We further analyzed this relationship at the participant level. We fit a random-effects logistic regression (to account for repeated measurement) with preference variance as the independent variable and allocation decision as the dependent variable. We observed a positive relationship between preference variance and both endorsement of a market (z = 21.61, p < .001) and endorsement of a line (z = 11.06, p < .001).These initial findings characterize a strong, positive relationship between beliefs about preference variance and endorsement of resource-based allocation rules. However, this could simply be a feature of the particular set of products and services that we tested. And, of course, these pilots are correlational. Do beliefs about preference variance actually have a causal effect? Studies 1a and 1b: Preference Variance Increases Endorsement of Markets and LinesStudy 1 tests whether beliefs about preference variance increases endorsement of both markets (Study 1a) and lines (Study 1b) (H1). MethodFor Study 1a, we recruited 525 MTurk participants (Mage = 35.77 years; 313 women, 212 men); for Study 1b, we recruited 602 MTurk participants (Mage = 37.16 years; 286 women, 316 men). Both studies employed a single-factor (condition: variance vs. no variance—high vs. no variance—low), between-subjects design. Participants were randomly assigned to a condition and one of two scenarios (product vs. ticket). In the product scenario, participants read that ""a retailer has a limited supply of a very popular product, and there is just one item left."" In the ticket scenario, participants read that ""a venue has a limited supply of tickets for a very popular upcoming event, and there is just one ticket left.""In the variance condition, we told participants that ""three people all want the [product/ticket] to varying degrees"" and that Persons A, B, and C, respectively, were ""extremely,"" ""moderately,"" or ""only a little [interested in the product/excited about the event]."" In the no variance—high condition, we told participants ""three people all want the [product/ticket] to the same extent"" and that Persons A, B, and C were all ""extremely [interested in the product/excited about the event]."" The no variance—low condition was identical to the no variance—high condition, but Persons A, B, and C were all ""only a little [interested in the product/excited about the event]."" We included the no variance—low condition to account for the possibility that consumers are simply uncomfortable using markets for allocation when demand is uniformly high ([11]; [42]).We then asked participants to choose between a resource-based allocation rule versus random allocation (counterbalanced). In Study 1a, participants selected either ""choose someone randomly"" (lottery) or ""choose the person who is willing to pay the most money"" (market). In Study 1b, participants selected either ""choose someone randomly"" (lottery) or ""choose the person who is willing to wait the longest in line"" (line). Results and DiscussionFor these analyses, we collapsed across the product and ticket scenarios (and note that the effects did not vary by scenario). In Study 1a, participants were more likely to endorse a market, relative to a lottery, in the variance condition (47%, 95% confidence interval [CI]: [40%, 55%]; Figure 3) than in both the no variance—low condition (32%, 95% CI: [26%, 39%]; z = 2.93, p = .003, Φc = .16) and the no variance—high condition (30%, 95% CI: [23%, 37%]; z = 3.38, p = .001, Φc = .18). In Study 1b, participants were more likely to endorse a line, relative to a lottery, in the variance condition (59%, 95% CI: [52%, 65%]) than in both the no variance—low condition (36%, 95% CI: [30%, 43%]; z = 4.41, p < .001, Φc = .22) and the no variance—high condition (33%, 95% CI: [27%, 40%]; z = 5.00, p < .001, Φc = .25).Graph: Figure 3. Studies 1a and 1b: Preference variance increases endorsement of markets and lines.These results illustrate that when some consumers have much stronger preferences than others, markets and lines seem more appropriate. We also find a similar effect when participants view only proxies for preferences (e.g., WTP, wait times) that vary a lot or a little (see Supplemental Studies 1a and 1b in the Web Appendix). However, in these initial studies, we explicitly gave participants this information. Do people naturally attend to preference variance in the absence of such prompting? Studies 2a and 2b: Inferences About PreferencesBecause different consumers maintain different preferences ([32]; [70]), what ""works"" for one group might not for another. We predicted that endorsement of resource-based allocation rules would depend on whether participants thought about a group that they inferred had similar or dissimilar preferences (H1). MethodFor Study 2a, we recruited 405 MTurk participants (Mage = 37.46 years; 204 women, 201 men); for Study 2b, we recruited 222 participants from the behavioral laboratory at a West Coast business school (Mage = 22.05 years; 169 women, 53 men). Both studies employed a single-factor (condition: general public vs. fan club), between-subjects design. We described a scenario in which the band Radiohead was playing ""a one-night-only show in Los Angeles"" and made tickets available either to ""the general public"" or to ""members of its Los Angeles fan club."" We expected that participants would infer lower preference variance within the fan club than among the general public.In Study 2a, we asked whether the band should allocate tickets, at face value, via ""a lottery"" or ""sell the tickets,"" at their stated price, to those ""willing to pay the most"" (1 = ""Definitely use a lottery,"" and 7 = ""Definitely sell the tickets to the people willing to pay the most""). In Study 2b, we asked whether the band should allocate the tickets, at face value, via ""a lottery"" or ""sell the tickets,"" at face value, to those ""willing to wait the longest in line"" (1 = ""Definitely use a lottery,"" and 7 = ""Definitely sell the tickets to the people willing to wait the longest"").Finally, as a manipulation check, we measured inferences about preference variance: ""Among members of the [general public (i.e., everyone in the city of Los Angeles)/members of the Los Angeles fan club (i.e., die-hard fans)], what do you think is generally the case?"" (1 = ""Everyone is interested in the tickets to a similar degree,"" and 7 = ""Some are not interested in the tickets at all, some are moderately interested in the tickets, and some are extremely interested in the tickets""). Results and DiscussionConfirming the effect of the manipulation, in Study 2a, participants inferred greater preference variance in the general public condition (M = 5.97, 95% CI: [5.76, 6.18]) than in the fan club condition (M = 3.58, 95% CI: [3.28, 3.87]; t(403) = 13.00, p < .001, d = 1.29). In Study 2b, participants inferred greater preference variance in the general public condition (M = 6.06, 95% CI: [5.84, 6.29]) than in the fan club condition (M = 4.50, 95% CI: [4.13, 4.86]; t(220) = 7.24, p < .001, d = .97).Moreover, in Study 2a, participants were more likely to endorse a market, relative to a lottery, in the general public condition (M = 3.66, 95% CI: [3.34, 3.97]) than in the fan club condition (M = 2.43, 95% CI: [2.16, 2.70]; t(403) = 5.88, p < .001, d = .58). In Study 2b, participants were more likely to endorse a line, relative to a lottery, in the general public condition (M = 4.80, 95% CI: [4.46, 5.14]) than in the fan club condition (M = 3.89, 95% CI: [3.51, 4.27]; t(220) = 3.55, p < .001, d = .48).These additional analyses corroborate our claim that the appropriateness of markets, lines, and lotteries depends not only on what is being allocated, but to whom. What explains this pattern, though? We propose that consumers endorse markets and lines when they believe these resource-based allocation rules increase the likelihood that those who want or need something the most will get it—that is, when they help sort preferences. In addition, thus far we have asked participants what should be done, rather than what would be the fairest thing to do. While our account implies beliefs about the latter shape beliefs for the former, we have yet to test this assumption empirically. Study 3: Intuitions About Preference Sorting Mediate the EffectStudy 3 offers initial process evidence for our account by testing whether intuitions about preference sorting explain why preference variance increases both endorsement and the perceived fairness of markets and lines (H1). We predicted that these intuitions would play a mediating role (H2). MethodWe recruited 366 MTurk participants (Mage = 43.80 years; 186 women, 180 men). Study 3 employed a 2 (condition: variance vs. no variance) × 2 (resource: money vs. time), between-subjects design. All participants read, ""A local craft brewery has just released a new, limited-edition beer. This new beer is an India pale ale (IPA), and there are only 10 available cases. The brewery announced the release in a Facebook post to its 100 followers.""In the variance condition, participants read, ""All 100 followers would be willing to purchase a case, but some of these followers are more excited than others (i.e., some love IPAs, while others only somewhat like IPAs)."" In the no-variance condition, participants read, ""Because the company is known for its IPAs, all 100 followers are extremely excited and would be willing to purchase a case (i.e., they all love IPAs).""We then explained, ""One option is to enter all 100 followers into a lottery. The 10 cases would be sold to 10 randomly selected people (at the standard price)."" In the money condition, we said, ""Another option is to offer the available cases to those who are willing to pay the most. The 10 cases would be sold to the 10 people willing to pay the most (at their stated price)."" In the time condition, we said, ""Another option is to offer the available cases on a first-come, first-served basis. The 10 cases would be sold to the 10 people willing to wait in line the longest.""We asked (counterbalanced), ""What should the brewery do?"" and ""What would be the fairest thing for the brewery to do?"" (money condition: 1 = ""Definitely use a lottery,"" and 7 = ""Definitely sell the cases to those who are willing to pay the most""; time condition: 1 = ""Definitely use a lottery,"" and 7 = ""Definitely sell the cases to those who are willing to wait in line the longest"").Finally, we measured intuitions about preference sorting: ""If the brewery sold the available cases to [those who are willing to pay the most/wait in line the longest], how likely is it that the available cases would end up going to the people who want them the most?"" (1 = ""Not at all likely,"" and 7 = ""Extremely likely""). Results and DiscussionBeliefs about what the brewery ""should"" do and what would be the ""fairest thing for the brewery to do"" did not meaningfully differ (α = .89), so we formed a composite by taking the average. An analysis of variance (ANOVA) of this composite on condition, resource, and their interaction revealed only a main effect of preference variance (F( 1, 362) = 49.43, p < .001, d = .68), such that participants were more likely to endorse a market or line, relative to a lottery, in the variance condition (M = 3.51, 95% CI: [3.18, 3.84]) than in the no-variance condition (M = 2.07, 95% CI: [1.82, 2.32]). The simple effect of condition was significant for each resource (money condition: F( 1, 362) = 15.83, p < .001; time condition: F( 1, 362) = 35.70, p < .001).We next examined beliefs about preference sorting. An ANOVA of these beliefs on condition, resource, and their interaction revealed a main effect of preference variance (F( 1, 362) = 30.94, p < .001, d = .52), such that participants believed that a market or line would do a better job sorting preferences in the variance condition (M = 5.66, 95% CI: [5.45, 5.87]) than in the no-variance condition (M = 4.82, 95% CI: [4.57, 5.06]). The simple effect of condition was significant for each resource (money condition: F( 1, 362) = 16.80, p < .001; time condition: F( 1, 362) = 14.17, p < .001). We also observed a main effect of resource (F( 1, 362) = 9.65, p = .002, d = .25), such that participants believed that preference sorting was more likely in the time condition (M = 5.44, 95% CI: [5.22, 5.65]) than in the money condition (M = 5.04, 95% CI: [4.79, 5.29]).Finally, we tested for mediation. Indeed, beliefs about preference sorting mediated the effect of condition on endorsement of a market or line, relative to a lottery (based on 10,000 bootstrapped resamples: indirect effect = .32, SE = .07, 95% CI: [.193,.486]).This result supports the notion that consumers believe preference sorting is a basic function of markets and lines, and this is why they seem both more appropriate and fairer when preferences are dissimilar. All of the studies thus far have been hypothetical, however. Next, we test whether these findings hold when participants face real consequences for their allocation decisions. We note that although our framework applies to both markets and lines, in the remaining studies we focus specifically on attitudes toward markets and market pricing (predicting conceptually similar results for first-come, first-served policies). Study 4: Consequential Allocation DecisionsStudy 4 tests whether preference variance affects real decisions for how something should be allocated. We predicted that when participants believed that preferences for a prize varied, they would be more likely to cast votes for a market (vs. a lottery). We again predicted intuitions about preference sorting would play a mediating role (H2). MethodWe recruited 202 MTurk participants (Mage = 35.82 years; 72 women, 130 men). Study 4 employed a single-factor (condition: high variance vs. low variance), between-subjects design. We first told all participants that they would be participating in a trivia game and that their goal would be to identify characters from a popular television show (The Office). We also told participants they would have the chance to win a prize, depending on their performance.After reviewing these instructions and launching the trivia game, participants had 45 seconds to evaluate ten photos (Figure 4). They were asked to indicate which photo depicted each of ten characters that were listed below the table in random order.Graph: Figure 4. Study 4: Trivia Game Stimuli.After completing the trivia game participants read, ""We have one ( 1) The Office-theme card game (see below) to offer as a gift to participants in this study."" We displayed the prize and asked, ""How much of your base pay ($1.00) would you be willing to exchange for this gift?"" Participants responded on a sliding scale, ranging from 0 to 100 cents.In the high-variance condition, we told participants, ""All participants, regardless of their score, will be eligible to receive this gift. And we are asking all participants, regardless of their score, to vote on how this gift will be awarded."" In the low-variance condition, we told participants, ""Only those participants who earned a perfect score will be eligible to receive this gift. But we are asking all participants, regardless of their score, to vote on how this gift will be awarded."" We then asked, ""Should we choose [someone/one of these die-hard fans] randomly, or should we 'sell' it to the highest bidder (i.e., the participant who is willing to give up the most of his/her $1.00 base pay)? Note that we will actually tally these votes and use the outcome to decide how to award this gift"" (""Choose randomly"" or ""'Sell' it to the highest bidder"").Finally, after casting a vote, participants responded to four follow-up questions. First, to test our proposed mechanism, we captured intuitions about preference sorting: ""If we 'sold' it to the highest bidder (among [everyone who scored between 0%–100%/only those who scored 100%]), would that make it more likely or less likely that the person who wants this card game the most will be able to get it?"" (1 = ""Less likely,"" 4 = ""Neither,"" and 7 = ""More likely""). We also asked participants to guess how many characters they correctly identified (0–10) and to indicate whether they were familiar with (1 = ""Not at all familiar,"" and 7 = ""Very familiar"") and a fan of (1 = ""Definitely not,"" and 7 = ""Definitely"") the television show. Results and DiscussionParticipants were likelier to vote for a market (i.e., sell the prize to the highest bidder), relative to a lottery, in the high-variance condition (53%, 95% CI: [43%, 62%]) than in the low-variance condition (37%, 95% CI: [28%, 47%]; χ2( 1) = 5.18, p = .023, Φc = .16).[ 8] Participants also indicated that they believed a market would make it more likely that the person who wanted the card game the most would be able to get it (i.e., sort preferences) in the high-variance condition (M = 5.80, 95% CI: [5.52, 6.08]) than in the low-variance condition (M = 5.42, 95% CI: [5.12, 5.72]; t(200) = 1.86, p = .064, d = .26). Furthermore, these beliefs mediated the effect of condition on voting for a market (based on 10,000 bootstrapped resamples: indirect effect = .02, SE = .02, bias-corrected 95% CI: [.001,.066]).It is also worth pointing out that unlike in the previous studies, participants here voted for an allocation rule to which they, themselves, would be subjected. Interestingly, objective performance and endorsement of a market were weakly but negatively correlated (r = −.13, p = .06). In other words, those with low scores—those less likely to be fans of the show and consequently those with lower WTP—nevertheless tended to believe the prize should be ""sold"" to the highest bidder, possibly recognizing the potential to improve distributive efficiency (even though allocation through market pricing would mean they, themselves, were unlikely to win).Studies 3 and 4 reveal that people more strongly endorse resource-based allocation rules when preferences are dissimilar, because markets and lines are likelier to allocate scarce goods and services to those with the strongest preferences (i.e., sort preferences). Next, we turn to two theoretically derived moderators of our basic model. Study 5: Moderation by Inequality SaliencePrevious research has found that inequality in the distribution of a resource makes it difficult to clearly signal preferences ([67]). So, when inequality is salient, preference variance should no longer matter because there is no reliable way to sort those differences (H3). MethodWe recruited 566 Prolific participants (Mage = 37.71 years; 279 women, 287 men). Study 5 employed a 2 (condition: variance vs. no variance) × 2 (inequality: salient vs. baseline), between-subjects design. All participants first reviewed a vignette describing the introduction of ""a new, highly anticipated, all-electric pickup truck."" We explained that because ""the company can only produce a limited supply,"" potential buyers would need to submit a waitlist application that included contact information, a description of their interest, and a refundable deposit.In the variance condition, participants read, ""The people on the waitlist each have dramatically different levels of desire for the truck."" In the no-variance condition, participants read, ""The people on the waitlist all have exactly the same level of desire for the truck."" Participants in the inequality-salient condition were told, ""The people on the waitlist each earn dramatically different incomes."" In the baseline condition, participants read nothing else. Finally, we asked, ""How should the company allocate the available trucks?"" (1 = ""Choose people randomly [and sell at list price],"" and 7 = ""Choose the people willing to pay the most money [and sell at the offered price]""). Results and DiscussionAn ANOVA of allocation rule on condition, inequality, and their interaction revealed a main effect of condition (F( 1, 562) = 7.10, p = .008), which was qualified by an interaction (F( 1, 562) = 7.93, p = .005). Decomposition revealed a simple effect of condition at baseline (F( 2, 562) = 15.46, p < .001, d = .45; Figure 5), replicating the basic effect: participants were more likely to endorse a market, relative to a lottery, in the variance condition (M = 3.43, 95% CI: [3.10, 3.75]) than in the no-variance condition (M = 2.49, 95% CI: [2.16, 2.83]). However, there was no such simple effect of condition when inequality was salient (F( 2, 562) = .01, p = .916, d = .01; Mvariance = 2.81, 95% CI: [2.47, 3.15]; Mno variance = 2.83, 95% CI: [2.49, 3.18]).Graph: Figure 5. Study 5: Inequality salience attenuates the effect of preference variance on endorsement of markets.Study 5 confirms that when inequality is salient, preference sorting seems less feasible—even when preferences are dissimilar—so resource-based allocation rules seem less appropriate. The next study tests whether there are certain goods or services that people simply think should never be allocated on the basis of willingness to spend resources. Study 6: Moderation by Product Type (Wants vs. Needs)People often disapprove of resource-based allocation rules for allocating needs, which can impose taboo trade-offs ([ 5]; [53]). We therefore expected that for something people need (as opposed to merely want), resource-based allocation rules seem less appropriate (H4). MethodWe recruited 376 MTurk participants (Mage = 34.59 years; 167 women, 209 men). Study 6 employed a 2 (condition: variance vs. no variance) × 2 (type: want vs. need), between-subjects design. All participants first read: ""Throughout the country, the U.S. Forest Service maintains a number of restricted-use cabins on protected land. These cabins are not typically open to the public, but are rather used for operational purposes.""In the want condition, we explained that ""the agency has decided to make these cabins available for short-term rental to people who are interested in vacationing at these sites."" In the need condition, we explained that because ""forest fires near one residential neighborhood have significantly diminished air quality and now pose a serious safety hazard, ... the Forest Service is making some cabins available for short-term rental."" Participants then read, ""There is now only one cabin left and several families still [want/need] it."" In the variance condition, we told participants, ""These families, however, each have dramatically different levels of [need/desire] for the cabin."" In the no-variance condition, we told participants, ""These families, however, all have exactly the same level of [need/desire] for the cabin.""Finally, we asked (counterbalanced), ""What should the Forest Service do?"" and ""What would be the fairest thing for the Forest Service to do?"" (1 = ""Choose a family randomly,"" and 7 = ""Choose the family willing to pay the most money for it""). Results and DiscussionBeliefs about what the Forest Service ""should"" do and what would be the ""fairest thing for the Forest Service to do"" did not meaningfully differ (α = .88), so we formed a composite by taking the average. An ANOVA of this composite on condition, type, and their interaction revealed main effects of condition (F( 1, 372) = 4.09, p = .044) and type (F( 1, 372) = 4.53, p = .034), which were qualified by an interaction (F( 1, 372) = 5.40, p = .021). Decomposition revealed a simple effect of condition for wants (F( 1, 372) = 9.38, p = .002; d = .43), replicating the basic effect: participants were more likely to endorse a market, relative to a lottery, in the variance condition (M = 4.01, 95% CI: [3.56, 4.46]) than in the no-variance condition (M = 3.05, 95% CI: [2.64, 3.46]) (Figure 6). However, there was no such simple effect of condition for needs (F( 1, 372) = .05, p = .831; d = .03; Mvariance = 3.03, 95% CI: [2.61, 3.45]; Mno variance = 3.10, 95% CI: [2.62, 3.57]).Graph: Figure 6. Study 6: Preference variance increases endorsement of resource-based allocation rules for wants, but not for needs.Study 6 reveals that even when preferences for something construed as a need are dissimilar—and furthermore even when those preferences could be sorted by a market—people still resist resource-based allocation rules. This could be due to hesitance regarding taboo trade-offs, which possibly shift people from consequentialist moral reasoning (see the ""General Discussion"" section). Or it may be that in these situations people prefer a different basis for allocation (likely one sensitive to differences in need, rather than want). In our final study, we examine how consumers respond when they cannot choose the allocation rule themselves (as is typically the case), underscoring the managerial implications of our theory. Study 7: Implications for Purchase IntentionsAn expansive body of literature has documented the numerous negative consequences that result from perceptions of unfairness in the marketplace (e.g., [ 7]; [12]; [14]; [33]; [34]; [37]; [58]). This suggests that firms may be penalized for choosing allocation rules that our framework characterizes as inappropriate (H5). MethodWe recruited 508 MTurk participants (Mage = 40.17 years; 272 women, 236 men). Study 7 employed a 2 (condition: variance vs. no variance; within-subjects) × 2 (system: market vs. lottery; between-subjects) mixed design. All participants first read, ""The U.S. Centers for Disease Control and Prevention (CDC) recommends wearing face masks to help slow the spread of the coronavirus."" While ""surgical masks and cloth masks are widely available,"" N95 respirators ""are still in short supply.""[ 9] We then explained that the ""largest domestic manufacturer of N95 respirators is 3M, which also makes a wide array of other products, including sticky notes, tape, bandages, air filters, water filters, sponges, and much more.""We then described two cities, one with greater preference variance than the other: ""In the city of Springfield, each resident has dramatically different desire for an N95 respirator""; alternatively, ""In the city of Greenville, all residents have identical desire for an N95 respirator."" Preferences for N95s, therefore, varied in Springfield, but not in Greenville.Those assigned to the market system indicated how fair it would be ""if 3M used an auction to allocate its available N95s to the highest bidders (at their stated price)"" in each of Springfield and Greenville (counterbalanced). Those assigned to the lottery system indicated how fair it would be ""if 3M used a lottery to allocate its available N95s randomly (at list price)"" in each of Springfield and Greenville (counterbalanced; for both questions, 1 = ""Extremely unfair,"" and 7 = ""Extremely fair"").On the next page, we measured purchase intentions (counterbalanced): ""If 3M used [an auction/a lottery] to allocate N95s in Springfield, would that affect your willingness to purchase 3M products?"" And: ""If 3M used [an auction/a lottery] to allocate N95s in Greenville, would that affect your willingness to purchase 3M products?"" (for both questions, 1 = ""It would make me less likely to purchase other 3M products,"" and 7 = ""It would make me more likely to purchase other 3M products""). Results and DiscussionA mixed ANOVA of fairness on system (between-subjects), variance (within-subjects), and their interaction revealed a main effect of system (F( 1, 467) = 100.30, p < .001) and a main effect of variance (F( 1, 467) = 37.68, p < .001), which were qualified by an interaction (F( 1, 467) = 141.02, p < .001). Decomposition revealed that participants believed it was fairer to use a market to allocate the available N95s in the city where preferences varied (i.e., Springfield; M = 3.62, 95% CI: [3.38, 3.86]) than in the city where preferences did not vary (i.e., Greenville; M = 3.04, 95% CI: [2.81, 3.28]; F( 1, 467) = 16.21, p < .001, d = .30) (Figure 7). By contrast, participants believed it was fairer to use a lottery to allocate the available N95s in the city where preferences did not vary (i.e., Greenville; M = 5.64, 95% CI: [5.43, 5.84]) than in the city where preferences varied (M = 3.82, 95% CI: [3.57, 4.06]; F( 1, 467) = 164.70, p < .001, d = .89).Graph: Figure 7. Study 7: Misapplication of these allocation rules (e.g., use of a market when preferences are similar) reduces perceptions of fairness and purchase intentions.A mixed ANOVA of purchase intentions on system (between-subjects), variance (within-subject), and their interaction revealed a main effect of system (F( 1, 467) = 86.12, p < .001) and a main effect of variance (F( 1, 467) = 3.78, p = .052), which were qualified by an interaction (F( 1, 467) = 45.82, p < .001). Decomposition revealed that participants were less likely to purchase other 3M products if the company used a market to allocate the available N95s in the city where preferences did not vary (i.e., Greenville; M = 2.87, 95% CI: [2.68, 3.05]) than in the city where preferences varied (i.e., Springfield; M = 3.16, 95% CI: [2.99, 3.34]; F( 1, 467) = 11.46, p < .001, d = .20) (Figure 7). By contrast, participants were less likely to purchase 3M products if the company used a lottery to allocate the available N95s in the city where preferences varied (i.e., Springfield; M = 3.72, 95% CI: [3.57, 3.87]) than in the city where preferences did not vary (i.e., Greenville; M = 4.23, 95% CI: [4.08, 4.39]); F( 1, 467) = 38.54, p < .001, d = .47).This final study demonstrates that when a company fails to apply the more appropriate allocation rule (as characterized by our framework), purchase intentions suffer. However, we acknowledge the possibility that participants could have made different inferences about the two cities (given our within-subject design), though it is not clear in what direction this would have systematically affected judgments. For example, residents of a city might express uniformly high desire for N95 respirators because their public health infrastructure is poorly equipped (and thus lacks supplies) or well equipped (reflecting a citizenry that enthusiastically adopts new mitigation tactics). Nevertheless, the findings here highlight the importance of anticipating how the appropriateness of allocation rules for some products can potentially affect downstream purchase intensions for other products. General DiscussionIn this research, we offer a general account of when and why people favor the use of markets, lines, and lotteries. We believe that understanding these lay economic beliefs is of broad theoretical interest. Yet these intuitions also have practical consequences, as they shape perceptions of fairness in the marketplace. To that end, our account builds on prior work showing that consumers care deeply about distributive efficiency ([47]; [48]; [49]). Perhaps as a result of this, we find that people are naturally attuned to how preferences are distributed. And thus their views about when to use markets, lines, and lotteries depend on the extent to which they believe preferences vary. Theoretical and Practical ImplicationsOf course, preference variance is not the only factor that shapes views about how to allocate things. For example, Study 5 demonstrates that inequality reduces support for resource-based allocation rules. People are uncomfortable with inequality in general ([24]), but our results reveal that at least some of this discomfort stems from skepticism about whether resource-based allocation rules can improve distributive efficiency when spending is uncorrelated with preferences. In addition, inequality may furthermore affect perceptions of unfairness simply because people regard any form of inequality as unfair (e.g., [23]; [45]; [56]; [71]; [79]).It is further plausible that the source of inequality could matter as well. For example, inequality arising from differences in work ethic probably attenuate the effect less than inequality arising from differences in inheritance ([17]). And resources themselves can often be exchanged for each other (e.g., paying money to jump a queue and save time), suggesting another potential moderator future research might explore.More broadly, our findings enrich the literature exploring when people most readily adopt preference-based versus other allocation norms. For example, prior work has argued that people especially desire improvements in distributive efficiency when there is an insufficient supply of something and preferences vary ([20]; [68]; [81]). Our conceptual framework reveals that these are necessary, but not sufficient, conditions: people also need to believe that stated preferences (signaled by the resources consumers are willing to spend) are appropriate for determining who should get what and that resources spent reliably signal those preferences.We believe that our work yields several additional theoretical insights. We identify a novel source of market aversion. For example, previous work has found that market aversion can occur when people attach moral value to things ([77]) or react negatively to profit-taking ([11]; [42]; [57]). Here, we propose that market aversion can also be traced to views about the very purpose of markets to begin with. That is, consumers seem to believe that a primary function is to help sort preferences—identifying those who most want something and allocating accordingly. And so they will exhibit market aversion when this goal is infeasible (because preferences are too similar).This basic insight might apply to other allocation rules in nonconsumer contexts, as well. For example, a primary function of admissions committees at elite universities can be viewed as ""merit sorting""—allocating limited seats in each freshman class to the most qualified applicants. However, merit sorting should be similarly infeasible when, in this case, qualifications are too similar. This has led some experts to call for lottery admissions for applicants who meet certain academic thresholds ([ 9]; [18]; [35]). To the extent that many other potential bases for allocation exist—for example, differences in need (e.g., Study 6), future potential ([78]), and emotional resonance ([31]; [50])—our framework might similarly apply.It is also likely that there exist other moderators for the model described here. For example, one interpretation of Study 6 is that the prospect of allocating wants versus needs shifts people from consequentialist moral reasoning (wherein they think about the costs and benefits of using markets and lines) to deontological moral reasoning (wherein they adhere to simple ethical rules and heuristics; [ 6]; [38]; [73]). If true, then the numerous other factors that have been shown to shift reliance on consequentialist versus deontological moral reasoning (e.g., whether outcomes are framed as gains vs. losses or benefits vs. harms; [ 4]; [29]) might further moderate the effects we document.More practically, highlighting preference variance might soften some resistance to market pricing. For example, during emergencies, demand for certain products or services can increase dramatically. When prices follow suit, firms are often accused of price gouging ([25]), a practice that people seem to oppose uniformly ([15]; [42]). However, our findings suggest some potential nuance: consumers might actually tolerate raising prices if they appreciate that doing so can help direct scarce goods and services to those who will make the best use of them—that is, improve distributive efficiency through preference sorting. Indeed, previous work has argued that while raising the price of hotel rooms in the path of a hurricane ""does not literally increase the supply of hotel rooms, it increases the available supply"" ([83], p. 363).Implications for segmentation are worth highlighting, as well. As underscored by Studies 2a and 2b, preference variance between segments often differs, suggesting another consideration marketers should weigh with respect to their pricing tactics. To adapt a classic example: business travelers usually pay higher fares for flights than leisure travelers with the exact same itinerary. Airlines are able to price discriminate thusly because business travelers typically make purchases much later than leisure travelers (and fares tend to increase over time). However, flights are often oversold, requiring airlines to set prices for not completing a trip as planned (i.e., compensating a traveler for instead taking the next available flight). Here, preference variance among business travelers, who probably have tighter schedules, is likely lower than preference variance among leisure travelers, who are less likely to have appointments to keep. So, when deciding whom to leave behind, it could make more sense to use random allocation (e.g., a lottery) for the business segment and a resource-based allocation rule (e.g., a market based on willingness to accept) for the leisure segment.Finally, although consumers generally view lines as a fairer alternative to markets ([28]; [41]; [65]), our theory cautions against their uncritical adoption. We find that the same conditions that give rise to market aversion also dampen support for first-come, first-served policies: if consumers believe lines cannot accurately sort preferences (because they are too similar), then they will resist using them all the same. ConclusionPeople often disagree about how to allocate things fairly, and it can sometimes seem like these disagreements stem from intractable differences in moral convictions or political philosophies (e.g., socialism vs. capitalism). However, our work suggests a more flexible view. People actually seem to earnestly try to discern the nature of preferences and choose an allocation rule that fits. It thus reveals an interesting way in which people apply their lay economic beliefs. Consumers desire distributive efficiency in that they believe things should go to those who want them the most, but psychology shapes views about when this goal is possible and how best to achieve it. "
43,"Why Salespeople Avoid Big-Whale Sales Opportunities Contrary to the intuition that salespeople gravitate toward big-whale sales opportunities, in reality they often avoid them. To study this phenomenon, the authors integrate contingent decision-making and conservation-of-resources theories to develop and test a framework of salespeople's decision making when prospecting. Study 1 reveals that the performance impact of salesperson initial judgment of opportunity magnitude follows an inverted U-shape, indicating that salespeople's avoidance of large opportunities results from rational benefit–cost analyses due to their conservation of resources. Interestingly, salespeople use a calibration decision-making strategy (i.e., calculating expected benefits by accounting for conversion uncertainty) at the portfolio rather than prospect level, in solution- but not product-selling contexts. Ignoring this calibration effect can lead to under- or overestimation of conversion rates of up to 100%. Study 2 shows that salespeople's past performance success and experience bias this calibration. Simulations reveal that when high performers or inexperienced salespeople believe their portfolio magnitude is large and conversion uncertainty low, they are less concerned about resource conservation and improve their quota attainment by 50%. Study 3 confirms the theoretical mechanism. These findings shed new light on salespeople's decision making and suggest ways for sales professionals to improve effectiveness when prospecting.Keywords: salesperson judgment; uncertainty; solution selling; prospecting; conservation-of-resources theoryCentral to a firm's customer acquisition is salesperson prospecting, which involves identifying sales opportunities among potential customers. Prior research on salesperson prospecting has underscored its importance not only for firms' customer relationship management (CRM) but also for salesperson performance (e.g., [48]). However, more than 40% of salespeople report that prospecting is challenging and full of uncertainty, taking on average 25% of their time ([ 8]). Whereas some practitioners emphasize the pursuit of large prospects because these ""big whales"" help firms and salespeople achieve rapid sales growth, others warn against prioritizing such prospects because they can easily drain salesperson and company resources ([32]). Moreover, ""in the time it takes to land one major deal, [the salesperson] could have closed five smaller deals"" ([19]). Although practitioners appear to recognize salespeople's benefit–cost trade-offs when prospecting, academic research has not systematically examined this important phenomenon.Marketing research on salesperson prospecting has developed along two major streams. One stream focuses on salesperson judgment of market opportunities, such as market demand for a new brand, customer needs, and expected performance (e.g., [27]; [31]; [66]). This research stream shows a linear positive relationship between customer demand judgment and salesperson performance. The other stream emphasizes the role of salespeople as decision makers in dealing with various types of uncertainty, such as salespeople's general risk aversion, context-specific uncertainty, or salesperson idiosyncratic characteristics (e.g., [ 2]; [10]; [41]; [55]; [60]). Although these research streams provide useful insights into the information salespeople use in their decision making, three important research gaps exist.First, when prospecting, salespeople typically identify multiple potential opportunities but pursue only some of them. However, research is scant on the potentially curvilinear impact of salesperson judgment of opportunity magnitude—defined as a salesperson's judgment of the size of an opportunity—on sales performance. Although anecdotal evidence suggests that salespeople focus on large opportunities, other sources allude to major drawbacks in pursuing them. Opening this black box can be useful for improving companies' prospecting effectiveness. Second, there is a lack of understanding of how conversion uncertainty affects salespeople's decision making when prospecting. A focus on conversion uncertainty is important, because prospecting is costly for the firm and for salespeople. Third, little is known about how such decision making varies between salespeople and selling contexts. Knowledge of these contingencies help sales managers to effectively manage salesperson prospecting behavior.To address these gaps, we seek answers to three key questions. First, what is salespeople's benefit–cost trade-off after they form an initial judgment of opportunity magnitude, and how does this affect their sales performance? The focus on initial judgments is based on prior research that underscores the importance of a primacy effect in both decision making and salesperson–customer interactions ([17]; [27]). Second, how does a salesperson's initial judgments of opportunity conversion uncertainty change the sales performance outcome of the benefit–cost analysis? Given that salespeople's compensation generally depends on conversion, understanding the effect of opportunity conversion uncertainty, or a salesperson's initial judgment of the likelihood to convert an opportunity into a deal, is important. Third, what are important boundary conditions of the effects of these initial judgments? We focus on two sets of moderators: ( 1) the selling context (i.e., product vs. solution selling) and ( 2) key salesperson characteristics (i.e., past performance success and salesperson experience). In doing so, we also explore the role of information level (i.e., prospect and portfolio levels) in salesperson decision making.To answer our questions, we develop and test a contingency framework of salespeople's decision making when prospecting for market opportunities in a sequence of three studies. For theoretical foundation, we integrate research on contingency decision making ([46]) and conservation of resources (COR) ([30]). We augment these theories with field notes from in-depth interviews with sales professionals. While Studies 1 and 2 rely on multisource field data, Study 3 is a scenario-based experiment to provide evidence of the benefit–cost analysis as the underlying mechanism. Together, this multimethod approach allows us to rigorously triangulate the effects and unpack the theoretical mechanisms.This research makes several contributions. First, we contribute to the emerging literature on salesperson judgment and decision making when prospecting by unpacking the underlying decision process. We provide theoretical arguments and strong empirical evidence that explains why salespeople avoid big-whale prospects. Specifically, we show that, based on their initial judgment of opportunity magnitude, salespeople conduct a benefit–cost analysis under resource constraints to decide which opportunity to pursue. This analysis results in an inverted U-shaped relationship between magnitude and performance. Spotlight analyses show that a one-standard-deviation (SD) increase in opportunity magnitude lowers salespeople's conversion rate by 10%.Second, we provide insights into the effect of conversion uncertainty on the salesperson decision-making process when prospecting. The results show that when selling solutions, salespeople use a calibration decision-making strategy, in which the effects of opportunity magnitude are conditional on conversion uncertainty. However, this strategy occurs only at the portfolio level, underscoring the role of salesperson portfolio as a decision-making context. Ignoring the calibration effect in estimating performance outcomes may lead to under- or overestimation of conversion rates of up to 100%. When selling products, salespeople use a compensatory decision-making strategy that accounts for the effects of portfolio magnitude and conversion uncertainty in an additive manner. These findings extend prior work (e.g., [60]) on uncertainty in personal selling and salesperson decision making.Third, we provide empirical evidence for how, in a solution-selling context, the calibration effect varies depending on salesperson past performance success and experience. The results suggest that when faced with high levels of conversion uncertainty, high performers and inexperienced salespeople perform much worse because their resource-conserving tendency makes them more sensitive to the cost increases associated with uncertainty. Simulations reveal that their quota attainment can suffer by as much as 50%. These insights extend prior research focusing on the salesperson–customer dyad in business-to-business (B2B) marketing and retail encounters (e.g., [27]; [43]). Background Literature and Conceptual FrameworkSalespeople are generally assigned to a territory or a customer segment, and their sales opportunities can be self-generated or assigned ([48]). Within a given period, they move these sales opportunities through a funnel from prospects to closed sales deals. At any given time, salespeople form a judgment of the magnitude of specific sales prospects, with a certain level of conversion uncertainty. Prior research on salesperson prospecting provides useful insights into why salespeople fail to follow sales leads, how their judgment of opportunities linearly influences their performance, and how they deal with uncertainty. However, it has not examined why and when salespeople pursue or avoid big opportunities. To shed light on these issues, we view salesperson prospecting as decision making under resource constraints. In this section, we first briefly review the relevant literature and then present our conceptual framework. Prospecting as Decision Making Under Resource Constraints Decision-making frameworksTwo major decision-making frameworks are the benefit–cost framework ([ 6]) and perceptual frameworks, such as prospect theory ([59]). In their review of these two frameworks, [46] posit that the former provides insights into rational decision making under multiple alternatives while the latter is useful in explaining cognitive biases and heuristics in decision making. In their review, they also underscore task and individual characteristics, such as willingness to bear uncertainty, as important contingencies of individual decision making. COR theoryUnlike the majority of general decision-making frameworks that assume away any resource constraint, COR theory emphasizes that people ""strive to retain, protect, and build resources and that what is threatening to them is the potential or actual loss of these valued resources"" ([30], p. 513). Furthermore, people must invest resources to gain resources, and those who experience a lack of resources attempt to conserve remaining resources ([26]). We argue that COR theory is particularly relevant in the context of B2B salespeople's prospecting for three reasons. First, unlike simple, low-effort choices between two lotteries, the pursuit of a prospect is costly—salespeople need to invest their time, effort, and resources in converting prospects into sales ([48]). Second, uncertainty in prospecting brings salespeople's resource constraints to the fore. Unlike gambling, which can be replayed, a forgone sales opportunity might be gone for good, and a failure to convert opportunities represents a loss of resources. Thus, salespeople need to balance between risk seeking and COR. Third, salespeople differ in terms of resource constraints ([48]). Contingency Framework of Salesperson Decision Making When ProspectingWe integrate decision-making frameworks with COR theory to propose a contingency framework of salesperson decision making when prospecting. Our framework focuses on the initial judgments of sales opportunities in terms of magnitude and conversion uncertainty. First, although salespeople encounter multiple market opportunities, they only invest their resources into converting some of them. The benefit–cost framework suggests that this decision is based on rational benefit–cost analyses of opportunity magnitude before action ([ 6]). COR theory offers a similar explanation that the pursuit of an opportunity is a trade-off between resource acquisition (e.g., the expected benefits of a sale) and resource conservation (e.g., the costs of resources expended on pursuing the opportunity). Second, salespeople make this decision under uncertainty. In line with contingency decision-making frameworks and COR theory, we expect that salespeople's benefit–cost analysis of opportunity magnitude is contingent on conversion uncertainty. This is because uncertainty prevents action by obfuscating ""whether the potential reward of action is worth the potential costs"" ([40], p. 139; see also [26]; [46]).Third, task and personal factors represent additional contingencies that distort the rational benefit–cost analyses. We focus on two sets of contingencies. The first is the selling context (i.e., product vs. solution selling), in which a product denotes a physical object that can be sold in a transactional way (e.g., lamps) and a solution refers to a product-service system (e.g., smart lighting) that requires a relational process and tailoring. Solution selling represents a more uncertain task than product selling ([57]; [60]). Examining the selling context is important because many firms that shift from product to solution selling often struggle to cope with the inherent greater uncertainty (e.g., [18]; [60]). The second set includes two salesperson characteristics related to the propensity to conserve resources under uncertainty. Past performance success refers to salesperson quota attainment in the previous quota period. Salesperson experience refers to a salesperson's time in the sales territory, with the company, and in the sales profession ([ 2]). We focus on these two moderators because prior research suggests these factors are related to how salespeople deal with uncertainty and conserve resources ([26]; [30]; [48]). Overview of StudiesTo test our conceptual framework, we conducted three studies using multiple methods. We provide an overview of our conceptual framework, hypotheses, and the studies in Figure 1. Study 1 focuses on how salesperson initial judgments of opportunity magnitude determine the actual conversion of a prospect into a sale, which in the aggregate influences the salesperson conversion rate at the portfolio level. In doing so, we also investigate how salespeople calibrate opportunity magnitude for opportunity conversion uncertainty and whether such calibration differs between product and solution selling. In Study 2, we examine the heterogeneity of such calibration effect, with a focus on salespeople's past performance success and experience. The dependent variable in Study 2 is salesperson quota achievement, which is theoretically connected with the conversion rate examined in Study 1. In Study 3, a scenario-based experiment, we elucidate the underlying benefit–cost mechanism and the role of resource slack. Table 1 summarizes the key concepts in our framework and corresponding operational measures.Graph: Figure 1. A contingency framework of salesperson decision making when prospecting and overview of three studies.GraphTable 1. Overview of Key Concepts and Operationalizations in Studies 1 and 2. Key ConceptsDescription and Conceptual MeaningConceptual FoundationsOperationalizationStudy 1Study 2Representative StudiesPerformance OutcomeSales performanceDegree to which the salesperson obtains a desired outcomeAhearne et al. (2010)Prospect-level performance: A binary measure, where 0 reflects no deal and 1 reflects that a deal has been made (objective likelihood of conversion)✓Smith, Gopalakrishna, and Chatterjee (2006); Mayberry, Boles, and Donthu (2018)Portfolio-level performance: The ratio of prospects that are successfully turned into deals in a salesperson's portfolio (objective conversion rate)✓Own operationalizationSalesperson performance: Percentage of quota attainment✓Ahearne et al. (2010)Initial Cues and Judgment FormationOpportunity magnitudeThe size of a potential sales optionKumar, Petersen, and Leone (2013)Prospect magnitude: Log-transformed salesperson initial judgment of revenue for a prospect (i.e., deal magnitude in $)✓Mayberry, Boles, and Donthu (2018)Portfolio baseline magnitude: The mean of all prospects' magnitude in a salesperson' portfolio✓Own operationalizationPortfolio magnitude: Reflective four-item construct capturing a solution-selling salesperson's initial estimates in terms of the size of order intake, sales volume, revenue, and profits of their entire portfolio.✓Van der Borgh, De Jong, and Nijssen (2019)Opportunity conversion uncertaintyThe subjective likelihood of being able to convert an opportunity into a desired sales outcomeMcMullen and Shepherd (2006)Prospect conversion uncertainty: Categorical measure differentiating among high, medium, and low levels of likelihood to convert a prospect into a paying customer within six months (−1, 0, and 1), based on salesperson initial judgment.✓Own operationalizationPortfolio baseline conversion uncertainty: The average of prospect conversion uncertainty across all the prospects in a salesperson's portfolio✓Own operationalizationPortfolio conversion uncertainty: Reflective four-item construct capturing a salesperson's initial judgment of uncertainty for realizing anticipated outcomes for solution selling for their entire portfolio (in terms of size of order intake, sales volume, revenue, and profits)✓Own operationalizationContingenciesSalesperson characteristicsDifferences in motivation, attitude, or risk propensity that determine whether a salesperson is willing to bear uncertainty or notMcMullen and Shepherd (2006); Payne, Bettman, and Johnson (1992)Past performance success: The percentage of quota achieved in the previous quota cycle✓Mayberry, Boles, and Donthu (2018)Salesperson experience: Composite measure consisting of three measures of sales experience (i.e., time in sales territory, time with the company, and time in the sales profession). We z-scored these scores and averaged them to form an overall experience index.✓Ahearne et al. (2010)Task characteristicsVarious dimensions, descriptors, or attributes of a particular organizational position that determine task execution and/or outcomesPayne, Bettman, and Johnson (1992)Binary measure indicating whether a prospect requires product selling or solution selling✓Mayberry, Boles, and Donthu (2018)Information levelsDenotes the reference class of judgments, distinguishing between judgment about the specific case or the aggregate of multiple casesSniezek and Buckley (1995)Data are separated into information at the single case level (prospect information) and aggregate level (portfolio baseline information)✓Own operationalization  Study 1: Understanding the Interplay Between Opportunity Magnitude and Conversion UncertaintyStudy 1 examines the interaction effect between opportunity magnitude and conversion uncertainty in product- and solution-selling contexts at both the prospect and portfolio levels. We supplement our theoretical development for this study with verbatim quotes from a qualitative study of seven salespeople (for a description of respondents, see Web Appendix W1). Study 1 Hypothesis Development Benefit–cost analysis of opportunity magnitudeWe predict that the effect of opportunity magnitude on salesperson performance follows an inverted U-shaped relationship. This is due to two countervailing underlying mechanisms: a linear positive effect from potential benefits of pursuing an opportunity and a curvilinear effect from potential costs of such pursuit. We follow [24] recommendation to visually summarize this benefit–cost analysis in Figure 2, Panel A. On the one hand, the higher the magnitude of the opportunity, the greater the extrinsic and intrinsic benefits of pursuing a sizable opportunity. Extrinsic benefits take the form of potential compensation and recognition from the selling firm, whereas intrinsic benefits include the potential enjoyment in pursuing sizable opportunities and the learning associated with the pursuit ([ 7]; [15]).Graph: Figure 2. Illustration of theoretical arguments for the inverted U-shaped effect and the calibration effect.On the other hand, pursuing a sizable opportunity entails substantial explicit and implicit costs. Salespeople incur explicit costs when pursing an opportunity because they need to invest resources, such as time and effort ([48]). Implicit costs refer to the opportunity costs of such pursuit because when pursuing an opportunity, salespeople must forgo other opportunities ([54]). As the opportunity magnitude increases, both explicit and implicit costs accelerate significantly because salespeople are constrained by limited resources and information-processing capacity ([30]; [48]; [55]). One senior salesperson of a software company explained this issue succinctly:Big opportunities? Um, the pros. The prospect of earning a load of money.... Second, obviously it's also more satisfying or fulfilling.... So, it's more complex, which is also like a nice challenge. Plus, you learn the most from complex deals and bigger deals.... But it costs a lot of time and whatever time you spend on one deal you cannot reinvest anymore in smaller deals. So, there's always a trade-off.Therefore, when both potential benefits and costs are considered, the effect of opportunity magnitude on salesperson performance will incrementally increase at first, but after a threshold, the costs to act on moderate to large opportunities outweigh their benefits. Thus, H1:  All else being equal, salesperson judgment of opportunity magnitude has a curvilinear, inverted U-shaped effect on salesperson performance. Opportunity magnitude–conversion uncertainty calibrationBy itself, conversion uncertainty can be a source of benefits for salespeople because uncertainty stimulates positive feelings and excitement ([50]). However, conversion uncertainty also increases costs of pursuing—both explicit costs (i.e., salespeople need to exert greater effort to convert highly uncertain opportunities) and implicit costs (i.e., highly uncertain opportunities carry higher opportunity costs). Under the compensatory decision-making strategy, salespeople assess the benefits and costs of opportunity magnitude and conversion uncertainty in an additive manner. However, our interviews suggest that salespeople at times calibrate for conversion uncertainty in a multiplicative rather than additive manner. For example, one solution-selling salesperson was very clear on his decision-making strategy to deal with conversion uncertainty:When I take a look at a deal that has a very high certainty, so say you've a 90% closing chance, but it's very small in size, and you have a very big deal that has a small closing chance. Yeah, I can just multiply it [size by uncertainty] and see where I get the most buck for my uncertainty, so to speak. Even though it's very simple, it's pretty effective.We refer to this multiplicative strategy as the calibration hypothesis, such that the inverted U-shaped relationship between opportunity magnitude and salesperson performance in H1 is contingent on conversion uncertainty ([40]). We again follow [24] recommendation to visually summarize how conversion uncertainty influences the benefit and cost functions in our arguments in Figure 2, Panel B.In terms of the benefit function, the (previously noted) solution-selling salesperson's calculus is consistent with both expectancy theory and COR theory ([30]; [64]). These theories suggest that salespeople calculate the expected benefits of an action by multiplying its benefits by the success odds (i.e., expected benefits = magnitude-based benefits × conversion uncertainty). Thus, when conversion uncertainty is high, the expected benefits of pursuing a large opportunity are lower, making it less motivating to pursue. Importantly, this calculus is universal, without evoking any individual characteristics as contingencies. Therefore, conversion uncertainty weakens the slope of the benefit line, shifting the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left.How do salespeople calibrate for conversion uncertainty in assessing the costs of pursuing an opportunity? As mentioned previously, conversion uncertainty can be positively stimulating for risk seekers but harmfully costly for people who want to conserve resources. In this regard, prior research indicates that salespeople are heterogeneous in their risk-seeking behavior for various reasons, such as their past performance success and their capability (e.g., [42]). Given this heterogeneity, the cost function can swing in either direction, and thus we predict that, in the aggregate, opportunity conversion uncertainty may appear as not having an influence on the cost function. For the leftward shifting effect to occur, opportunity conversion uncertainty only needs to shift the benefit function downward and does not need to change the shape of the cost function ([24]). Thus, H2:  Opportunity conversion uncertainty moderates the effect of opportunity magnitude on salesperson performance, such that it shifts the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left. Solution- versus product-selling taskPrior research on decision making suggests that, under high levels of situational uncertainty, people search for a relevant reference class to calibrate their judgments ([23]; [33]). However, [60] suggest that, beyond outcome uncertainty, such as conversion uncertainty, solution selling has a higher level of need and process uncertainty than product selling. We argue that it is this difference in overall situational uncertainty that causes salespeople to calibrate for conversion uncertainty differently when selling products versus solutions. Specifically, because need and process uncertainties are higher in solution selling, salespeople do not have a reliable frame of reference to count on. By contrast, because need and process uncertainties are lower in product selling, salespeople can confidently draw from their knowledge of customer needs and requirements, the sales process, and product configurations to deal with conversion uncertainty. Therefore, compared with product-selling salespeople, solution-selling salespeople are more sensitive to conversion uncertainty and tend to calibrate for this uncertainty more intensely. Thus, we expect the weakening effect of conversion uncertainty on the benefits function predicted in H2 to be stronger in solution- than product-selling contexts. H3:  The leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance is stronger in solution- than product-selling contexts. Institutional ContextWe collected data from a Fortune Global 500 firm, a market leader in lighting products and solutions for enterprise customers; at the time of data collection, the company generated more than $25 billion annually in total revenue. The company provides a broad portfolio of lighting offerings, ranging from products (e.g., luminaires, lighting electronics, horticulture lighting) to system solutions (e.g., connected, smart luminaires; lighting management software). Customers come from various industries, such as food and fashion retail, health care, education, sports, municipalities, hospitality, infrastructure, and manufacturing. For its field-based sales approach, the company relies primarily on direct sales, and salespeople are subject to the same compensation and incentive scheme. Salespeople obtain a fixed yearly salary plus commission (maximum 30% of the fixed salary). To explore the impact of initial judgments of opportunity magnitude and conversion uncertainty, we gathered archival data from the company's sales force automation (SFA) system for all prospects within one market. For every prospect, we obtained transaction-level records from January 2016 to May 2017, including initial estimates of opportunity magnitude (i.e., prospect deal size) and conversion uncertainty, updated estimates, and the final sales outcome. The SFA data cover 12,988 B2B prospects, handled by 173 salespeople, who logged 110,278 events in total. We provide supplemental information about the research context and the SFA data in Web Appendix W2. Manifest VariablesBecause we are interested in the performance impact of a salesperson's initial judgments, we aggregated the event-level data to the prospect level. This approach allows us to estimate a two-level model in which prospect-level data (case-specific; within salesperson) are nested within portfolio-level data (baseline; between salesperson). Focal variablesA unique feature of Study 1 is that we leverage the company's SFA data to operationalize the key variables. We measure opportunity magnitude as the salesperson's initial point estimate of a prospect in terms of revenue. Following [62], we log-transform the measure to correct for right-skewness. We measure opportunity conversion uncertainty as a categorical measure that captures the probability of converting a prospect into a deal within six months. We coded these categories as −1 (low), 0 (medium), and 1 (high) to facilitate interpretation and enhance model parsimony ([16]). We measure prospect-level performance as a binary measure that indicates the actual conversion of a prospect at the end of the sales cycle (0 = no deal, 1 = a deal). Portfolio-level performance indicates the salesperson's portfolio-level conversion rate, aggregated from their prospect-level actual conversion. Control variablesTo obtain unbiased estimates, we control for the nonlinear effects of uncertainty by including a square term ([20]). To zero in on the effects of initial judgments, we control for several time-related dynamics. Specifically, we control for five (e.g., [ 1]). First, we control for duration of a sales cycle by including the sales cycle length ([39]). Second, we control for frequency of leads by including workload ([48]), measured as the total number of leads under a salesperson's wing during the assessment. Previous studies have shown that workload affects judgments (e.g., [22]). Third, we control for the frequency of uncertainty updates (i.e., process uncertainty), which reflects the total number of changes a salesperson has made after the initial uncertainty estimate. It reflects the doubt inherent in the sales process and filters out variation in the dependent variable after salespeople made their initial judgments, which is the focus of the article. Fourth, we control for accuracy and recency effects by including the difference between the initial and final estimates for opportunity magnitude and uncertainty (magnitude accuracy = [last estimate − first estimate]; conversion uncertainty accuracy = [first estimate − last estimate]). Fifth, we control for timing- and sequence-related dynamics by including time fixed effects. We use dummy variables to account for the prospect's industry (i.e., public, office, retail, and other). We also control for the potentially curvilinear effect of uncertainty because prior research suggests that people respond more rigorously to two ends of the uncertainty continuum than to moderate levels of uncertainty ([ 2]; [55]; [67]). Web Appendix W3 provides sample descriptives and the correlation matrix. Empirical Strategy Levels of analysis and centeringAs is true in many B2B selling contexts, salespeople are responsible for a portfolio of prospects. In the ""Study 1 Hypothesis Development"" subsection, we use the term ""opportunity"" without specifying whether this opportunity is at the prospect or portfolio level. However, previous studies in decision making (e.g., [33]; [58], [59]) show that people ( 1) can leverage two levels of information (case-specific and base-rate) and ( 2) use a reference point. From a multilevel perspective, portfolio baseline judgments are essentially salesperson-level constructs that capture between-salesperson variation, serving the function of the base-rate information about the sales territory. Prospect-level judgments reflect within-salesperson judgment about specific prospects relative to each salesperson's portfolio baseline judgments ([12]). Because assuming that an effect existing at a higher level will generalize to a lower level (or vice versa) can be erroneous ([13]), we employ multilevel modeling techniques to estimate the impact of a salesperson's initial judgments of opportunity magnitude and conversion uncertainty on performance and test the effect at the prospect (case-specific) and portfolio (baseline) levels. Conceptually, people tend to rely heavily on the mean value in their decision making ([28])—akin to a salesperson's baseline. Therefore, in Study 1, we examine the portfolio average magnitude and average conversion uncertainty as the reference points at the portfolio level and refer to these as portfolio baseline magnitude and conversion uncertainty.We specify a multilevel model using Mplus 8.3 ([44]). To allow for unbiased estimates at the between and within levels, we decompose the manifest variables into uncorrelated latent ""between"" and ""within"" components ([47]). Latent means of focal variables are estimated at the between level, while ""pure"" within-person effects are estimated in the within-level portion of the model. This specification is necessary for teasing apart prospect- and portfolio-level effects ([47]). EstimationThe complexity of the model and the use of a binary dependent variable did not allow use of robust maximum likelihood estimation techniques because of a lack of model convergence. As an alternative, we employed Bayesian estimation techniques and therefore specified a Bayesian multilevel probit model. We estimate two sets of models for the pooled (no separation of selling contexts), product-selling, and solution-selling data, respectively. Models 1–3 are main-effects-only models, and Models 4–6 are the interaction-effects models. All the manifest variables are standardized to aid in interpretation, with the exception of conversion uncertainty. We provide details on the model specification and estimation in Web Appendix W4. Endogeneity considerationsThe effect of opportunity magnitude and conversion uncertainty on performance may be endogenous, because common unobserved factors may influence both predictors and outcomes in our model (due to, e.g., simultaneity, measurement error, omitted variables). Following prior studies ([22]; [49]; [63]), we address endogeneity in three ways: ( 1) by adopting a rich data-modeling approach, ( 2) by controlling for endogeneity due to omitted variables, and ( 3) by checking for endogeneity due to selection bias. Because we consider a multilevel setting, we need to address endogeneity at each level ([37]). We correct for Level 1 endogeneity using a control function procedure ([49]) and check for robustness with an instrument-free Gaussian copula approach ([45]). We control for Level 2 (cross-level) endogeneity in our multilevel latent covariate model by allowing a correlation between random intercepts and slopes ([ 3]). A direct test of the random-effects assumption (Wald  χ12   = 2.226, p = .136) indicates that Level 2 endogeneity is not a concern ([ 3]). Web Appendix W4 provides further details of model-free evidence of inverted U-shaped relationship, robustness checks of adding higher-order terms and seasonal variation to the empirical model, and endogeneity corrections. ResultsWe present the results of our tests for H1 and H2 for solution selling at the portfolio level before discussing the differences between solution selling and product selling (H3). In the ""Discussion"" section, we explore differences across portfolio and prospect levels. Benefit–cost analysis of opportunity magnitudeTable 2 shows the results of the analyses. To test H1, we follow a rigorous three-step procedure ([24]). First, we find a significant, negative effect of (opportunity magnitude)2 on salesperson performance for solution selling (Model 3: γ02 = −.142, p < .001). We plot this effect in Panel A of Figure 3, which shows an inverted U-shape. Second, we formally test that the marginal effects on the left side of the turning point of the inverted U-shape are positive and significant and those on the right side of the turning point are negative and significant. Mathematically, we test whether γ01 + 2γ02XL is positive and significant and γ01 + 2γ02XH is negative and significant, where XL and XH represent low and high values of opportunity magnitude within the data range, respectively. For portfolio baseline magnitude, the results confirm this pattern (for details, see Web Appendix W5). Third, we examine whether the turning point (i.e., X) is located within the data range. Taking the first derivative of the Level 2 equation specified for Model 3 and setting it to zero yields a turning point X of −γ01/2γ02. We found that the turning point is 2.52 SD below the mean value and within the data range (Xsolution = −2.52 SD; 95% confidence interval [CI] = [−4.15, −1.38]). Overall, these results confirm an inverted U-shaped relationship between opportunity magnitude and salesperson performance for solution selling, in support of H1.Graph: Figure 3. Study 1: inverted U-shape and the moderating effect of opportunity conversion uncertainty in solution selling.GraphTable 2. Study 1—Results of Multilevel Probit Analyses: Effect of Initial Judgment on Performance Outcomes. Step 1Step 2aHyp.Model 1: PooledModel 2: ProductsModel 3: SolutionsModel 4: PooledModel 5: ProductsModel 6: SolutionsbSDbSDbSDbSDbSDbSDL2: DV = Portfolio-Level PerformanceMagnitude (γ01)−.536***.149−.301*.162−.716**.232−.562***.178−.377*.198−.240.303Magnitude2 (γ02)−.264***.056−.225**.066−.142***.045−.261***.056−.234***.069−.161***.048H1Uncertainty (γ04)−1.136***.249−1.026***.269−1.541**.481−1.248***.262−1.086***.279−1.556***.466Magnitude × Uncertainty (γ03)——————.051.158.108.175−.663**.227H2/H3L1: DV = Prospect-Level PerformanceMagnitude (β1j)−.456***.075−.410***.062−.536***.155−.393***.052−.414***.065−.484***.159Magnitude2 (β2j)−.142***.026−.158***.022−.098**.038−.123***.019−.157***.022−.129***.043Uncertainty (β4j)−.932***.112−.877***.105−.571*.258−.911***.094−.885***.104−.510***.259Magnitude × Uncertainty (β3j)——————−.017.040.011.045−.053.086ControlsUncertainty2 (L2).323.275.249.3041.100*.515.394***.273.251.309.929*.508Uncertainty2 (L1)−.093.080−.129.083−.167.228−.058.077−.126.079−.235.237Process uncertainty−.270***.016−.288***.017−.204***.040−.276***.016−.290***.017−.207***.040Sales cycle length.092***.021.088***.023.015.142.087***.020.091***.022.069.144Magnitude accuracy−.047***.012−.026*.013−.095***.035−.047***.013−.027*.013−.096**.035Uncertainty accuracy.820***.028.835***.031.675***.073.819***.028.834***.031.684***.073Workload−.004.008.002.008−.008.019−.002.006.003.007−.010.019ε^Magnitudeb.020.039−.006.038.168.119−.023.032−.008.039.114.124Magnitude × ε^Magnitude−.006.015−.011*.016.096*.058−.038**.016−.012.017.149**.066ε^Uncertainty.062*.030.015.028−.095.070.000.027.015.029−.112.071Uncertainty × ε^Uncertainty.109***.021.106*.018.191***.047.095***.016.103***.020.194***.048IMR−1.291***.084−1.416***.088−2.292***.593−1.027***.079−1.423***.087−2.112***.592Industry fixed effectsYesYesYesYesYesYesTime fixed effectsYesYesYesYesYesYesConstant (γ00)1.364***.2021.282***.1611.465.8331.199***.1561.220***.1731.101.866Pseudo-R2: L2/L1.795/.554.645/.545.805.716.801/.509.678/.543.831/.713n (prospects)/N (salespeople)12,988/17310,991/1661,997/12112,988/17310,991/1661,997/121 5 *p < .05.1 **p < .01.2 ***p < .001; unstandardized coefficients.6 a To determine the extent of bias in our estimates ([24]), we also tested an extension of Models 4–6, in which we included Magnitude2 × Uncertainty at L2 (γ05) and L1 (β5j). The results for these models showed nonsignificant effects. The results show that neither interaction is significant (  γ05   = .064, p > .05;  β5j   = .002, p > .05).7 b We also tested the robustness of our findings by controlling for endogeneity using the instrument-free Gaussian copula approach. Findings are similar when compared with the control function approach reported in this table. For details of the results of these additional analyses, see Web Appendix W4.8 Notes: L2 = portfolio level; L1 = prospect level. Magnitude = opportunity magnitude; Uncertainty = opportunity conversion uncertainty. IMR = inverse Mills ratio. Opportunity magnitude–conversion uncertainty calibration hypothesisTo test H2, we add the interaction term (magnitude × uncertainty)ij to the equation. The results of Model 6 in Table 2 confirm that the interaction is significant and negative in the solution-selling context (γ03 = −.663, p <.01). However, we cannot determine significance from the estimated interaction term alone in a nonlinear model (probit) with nonlinear interaction terms ([65]). Therefore, we formally test how the turning point changes as conversion uncertainty changes. To do so, we derive the turning point ""magnitude*"" (X*) by setting the first derivative of Model 6's equation with respect to X to zero. Then, we take the derivative of the turning point with respect to conversion uncertainty (Z) to show how the turning point changes as conversion uncertainty changes, yielding δX*/δZ = (−γ02 γ05)/[2(γ02)2]. We find that this term is significant and negative (b = −2.032, p <.01), in support of H2. Solution- versus product-selling task hypothesisTo test H3, we compare the results for the two selling tasks (see Table 2). At the portfolio level, in contrast with the significant and negative interaction in the solution-selling context (Model 6: γ03 = −.663, p <.01), we find no interaction effect of opportunity magnitude and conversion uncertainty on salesperson performance for product selling (Model 5: γ03 = .108, p >.10). Testing the difference between solution versus product selling reveals significant differences at the portfolio level (Δ[γ03_Solution; γ03_Product] = .771, p <.01), in support of H3. DiscussionStudy 1 provides evidence of an inverted U-shaped relationship between opportunity magnitude and sales performance, across levels and selling context. However, we only find evidence of the calibration hypothesis for the solution-selling context at the portfolio level. This suggests that salespeople respond differently to opportunities of different magnitude, depending on the baseline conversion uncertainty of their portfolio of solution prospects. To illustrate the impact of this effect, we ran a simple counterfactual analysis and compared the calibration model with a compensatory model (i.e., fixing the interaction coefficient γ03 to zero). The results show that under certain conditions the compensatory model over- or underestimates conversion rates by almost 100% (e.g., predict 100% conversion, 0% ""true"" value). For large opportunity magnitude (>∼$26.750), the compensatory model overestimates conversion rates by up to 30% for high conversion uncertainty but underestimates conversion rates by up to 90% for low conversion uncertainty. Overall, these results highlight the importance of the calibration model.We also found that the information level (i.e., prospect- and portfolio-level) matters. While prior research on decision making generally focuses on two simple choices, it might be cognitively impossible for salespeople to constantly make decisions at the prospect level while juggling a portfolio of prospects in their sales funnel. Consistent with this notion, our findings in Table 2 suggest that salespeople adopt a simpler compensatory decision-making strategy at the prospect level (i.e., that accounts for conversion uncertainty in an additive/subtractive manner) but rely on a more complex decision-making strategy at the portfolio level (i.e., a calibration strategy that accounts for conversion uncertainty in an interactive manner, using portfolio baseline information) for solution selling. In practice, salespeople generally have an idea about this so-called portfolio baseline in their assigned territory, such as the average magnitude and uncertainty of their portfolio. They then assess individual opportunities relative to this baseline and prioritize accordingly ([53]; [58]). Study 2: Examining the Heterogeneity of the Calibration Effect in Solution SellingAlthough Study 1 shows that salespeople calibrate for opportunity conversion uncertainty when selling solutions at the portfolio level, it does not investigate how salespeople differ in their calibration. Study 2 focuses on salespeople's past performance success and experience as two key boundary conditions that bias their rational calibration of benefit–cost analyses. Empirically, a test of these contingencies is a three-way interaction test of the two-way interaction in H2. Study 2 Hypothesis Development Past performance successWe first focus on how past performance success influences the way salespeople calibrate their benefit function for conversion uncertainty differently. Compared with salespeople with low past performance, those with high past performance have a higher sense of competence. As a result, they are more risk-seeking and view uncertain opportunities as challenging and intrinsically motivating ([42]; [56]). To these high performers, the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, salespeople with low past performance repulse opportunities that have high conversion uncertainty. This aversion arises because these highly uncertain opportunities not only threaten their potential extrinsic benefits (e.g., losing compensation and rewards) but also represent an unreliable path to achieve intrinsic benefits (e.g., bolstering their lack of competence; see [15]). Thus, the downward shift of the benefit function created by opportunity conversion uncertainty (predicted in H2) is weaker among salespeople whose past performance success is high (vs. low).In terms of the cost function, being successful in the prior period induces high performers to be more sensitive to opportunity conversion uncertainty for two reasons. First, consistent with COR theory, they are likely to slow down to conserve their resources to reduce stress ([30]). Empirical evidence shows that people tend to hold back after achieving a goal before working on the next goal (e.g., a resetting period; [11]; [34]). Second, high performers are more sensitive to high implicit costs associated with high conversion uncertainty because opportunities that can be converted with certainty allow them to maintain their status (e.g., [38]). Therefore, for high performers, opportunity conversion uncertainty is likely to shift their cost function upward more strongly. This upward shift is especially strong when opportunity magnitude is large because large opportunities require them to invest much more resources. By contrast, while salespeople whose past performance was less successful are also sensitive to the opportunity costs associated with high conversion uncertainty, their main concern is to prove themselves to the firm. Therefore, these poor performers need to exert greater efforts and cannot afford to conserve their resources. As a result, poor performers' cost function shifts upward less strongly when opportunity conversion uncertainty is high.Taken together, compared with salespeople who are low past performers, high performers view opportunities with high conversion uncertainty as more beneficial but also more costly. In prospecting, although all salespeople have limited resources ([48]), high performers are more inclined to conserve their resources than poor performers. Thus, for high past performers, we predict that the upward shifting effect of conversion uncertainty on the cost function will outweigh its downward yet weaker shifting effect on the benefit function. As a result, the expected net benefits of pursuing an opportunity will be lower when conversion uncertainty is high, causing the inverted U-shaped relationship between opportunity magnitude and salesperson performance to shift more strongly to the left ([24]). H4:  The greater salesperson past performance success, the stronger is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Salesperson experienceWe argue that because experienced salespeople differ from less experienced salespeople in terms of resources, they calibrate their benefits and costs under conversion uncertainty differently. First, they have better network-based resources in the form of relationships they have built over time. Second, they are more knowledgeable about various aspects of the sales process (e.g., customers, the market, the competition, the company), another resource critical for success in prospecting ([48]).In terms of the benefit function, the resources accumulated over time make experienced salespeople believe they are more capable, resulting in more risk-seeking behavior ([42]; [56]). For them, the challenge associated with uncertain opportunities can be intrinsically motivating. Because experienced salespeople are more strongly motivated by intrinsic than extrinsic benefits (e.g., [14]), the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, the lack of capability and resources makes inexperienced salespeople more concerned about potential losses of both intrinsic and extrinsic benefits at high levels of opportunity conversion uncertainty ([42]). Therefore, the downward shifting effect created by opportunity conversion uncertainty on the benefit function is stronger among inexperienced salespeople than experienced ones.In terms of the cost function, the abundance of aforementioned resources makes experienced salespeople less concerned about COR when pursuing opportunities with high conversion uncertainty. Conversely, given their lack of resources, inexperienced salespeople are more concerned about conserving their limited resources and are more sensitive to the costs associated with high conversion uncertainty ([26]; [30]). Thus, opportunity conversion uncertainty is likely to create a weaker upward shift of the cost function among experienced than inexperienced salespeople. Taking the benefit and cost effects together, experienced salespeople expect greater net benefits when conversion uncertainty is high. For them, the inverted U-shaped relationship between opportunity magnitude and salesperson performance shifts less strongly to the left ([24]). H5:  The greater salesperson experience, the weaker is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Institutional ContextIn Study 2, we corroborate Study 1's findings and examine the postulated boundary conditions of salespeople's calibration for conversion uncertainty in solution selling at the portfolio level. We collected data from the sales organization of a large firm ($16.4 billion in total revenue per year). The firm, which operates in the B2B market, provides information and technology solutions (e.g., workspace systems, data center solutions, managed services, security) to customers in industries such as finance, government, education, transport, service, retail, and media. Field-based salespeople are grouped according to the industries the firm serves, with each assigned a territory. All the salespeople are subject to the same compensation and incentive scheme and obtain a fixed yearly salary plus commission (with a progressive plan for all sales beyond quota). Using a survey instrument, we collected information about the salespeople's perceptions of their portfolios. Of the 248 salespeople, 211 completed the questionnaire (85% response rate). Consistent with the length of the average sales cycle, we collected objective salesperson performance from the firm's records six months after the survey. MeasuresIn Study 2, we examine the portfolio magnitude and conversion uncertainty in the aggregate at the portfolio level. Thus, portfolio magnitude corresponds to opportunity magnitude, and portfolio conversion uncertainty corresponds to opportunity conversion uncertainty. Focal variablesTo measure portfolio magnitude, we use the expected customer demand scale from [61]. The scale has four items that cover salespeople's judgment of the opportunity magnitude in terms of order intake, sales volume, revenue, and profits for the solutions in their portfolio. To measure portfolio conversion uncertainty, we developed a new scale that asks salespeople to assess their degree of (un)certainty about the portfolio magnitude. We inversely coded the scores to obtain uncertainty scores. We obtained salesperson past performance success (in the previous quota cycle) and salesperson performance from company databases. We used the percentage of quota achievement, as previous studies indicate that it accurately captures measurable task performance output while accounting for situational factors ([ 2]). Following previous studies (e.g., [ 2]), we operationalize salesperson experience as a composite measure consisting of three separate measures of experience: time in sales territory, time with the company, and time in the sales profession. Control variablesWe control for nonlinear effects of uncertainty by including a squared term ([20]). Dummy variables account for salespeople's industry. We also account for individual characteristics that may influence their judgments (i.e., age and workload). Finally, we control for salesperson trait competitiveness, measured with a scale from [ 9]. Web Appendix W6 provides measurement scales and descriptives of Study 2. Empirical Strategy Validation of measurement modelA confirmatory factor analysis of the measures indicated good model fit (  χ412   = 88.108, p < .01; comparative fit index = .950; Tucker–Lewis index = .933; root mean square error of approximation = .074; square root mean residual = .045; [ 5]). The scales achieved sufficient reliability, with composite reliabilities between.77 and.90 and average variances extracted exceeding.50 for all constructs, indicating reliability. The average variance extracted of each construct exceeds the average variance shared with any other construct, providing evidence of discriminant validity. In addition, all factor loadings are significant (p < .01) and have standardized values ranging from.65 to.91, thus demonstrating convergent validity of the constructs. To examine the effects of opportunity magnitude, conversion uncertainty, past performance success, and salesperson experience on salesperson performance, we specified a multilevel model in Mplus 8.3 ([44]) to control for the nesting of the data. We provide the model specification in Web Appendix W7. Endogeneity considerationsThe effect of salesperson opportunity magnitude and conversion uncertainty on sales performance may be spurious as a result of omitted variables (e.g., group-level factors) and correlation between independent variables and the error terms. For example, a sales manager's and coworkers' judgments may influence a salesperson's judgments and performance outcomes. To control for possible endogeneity in our analyses, we adopted [21] control function procedure. Web Appendix W7 provides further details. ResultsWe present the results of our retests of the main effects of the opportunity magnitude (H1) and calibration (H2) hypotheses for solution selling at the portfolio level. We then report the findings regarding the boundary conditions of past performance and salesperson experience (H4 and H5). Main effect of opportunity magnitudeWe report the results in Table 3. To retest H1 about the inverted U-shaped effect of opportunity magnitude on salesperson performance, we again rely on the three-step approach. First, in line with our results from Study 1, we find that opportunity magnitude2 has a significant, negative effect (Model 7: ζ2 = −.059, p < .05). Second, we formally test marginal effects. We find that the slope is positive and significant for low values of opportunity magnitude and negative and significant for high values (see Web Appendix W5). Third, we calculate the turning point. The estimated turning point is just above the average of the opportunity magnitude scale (i.e., mean + .36 = 3.55), with an estimated CI well within the data range (95% CIraw score = [3.02, 4.78]). These results confirm the inverted U-shaped relationship between opportunity magnitude and salesperson performance, corroborating H1.GraphTable 3. Study 2: Results (Solution-Selling Context). PortfolioRobust Maximum Likelihood EstimatesHyp.Model 7Model 8Model 9Model 10bSDbSDbSDbSDMagnitude (ζ1).042.043.040.043.057.041.098**.038Magnitude2 (ζ2)−.059*.026−.074**.029−.075**.028−.116***.030H1Uncertainty (ζ3)−.087**.037−.088*.038−.121*.056−.098.059Past performance success (ζ4).040.027.042.028.037.039.034.037Salesperson experience (ζ5).015.078.029.078.071.108.035.109Moderation EffectsMagnitude × Uncertainty (ζ6)——−.044*a.025−.044.027−.066*.028H2Magnitude2 × Uncertainty (ζ7)————.026.016.018.013Magnitude × Past perf. success (ζ8)————−.051.037−.026.034Uncertainty × Past perf. success (ζ9)————−.054.037−.012.043Magnitude2 × Past perf. success (ζ10)————.007.026−.023.032Magn. × Uncert. × Past perf. success (ζ11)——————−.058*.027H4Magn.2 × Uncert. × Past perf. success (ζ12)——————−.022.016Magnitude × Salesperson exp. (ζ13)————−.107*.064−.142*.059Uncertainty × Salesperson exp. (ζ14)————.041.039.150*.079Magnitude2 × Salesperson exp. (ζ15)————.024.041.075*.033Magn. × Uncert. × Salesperson exp. (ζ16)——————.016.044H5Magn.2 × Uncert. × Salesperson exp. (ζ17)——————−.070**.024ControlsAge.019.049.017.048.015.051.014.056Workload−.123***.038−.119***.037−.113**.038−.101**.038Trait competitiveness.029.041.037.040.033.040.054.040Dummy Govt. & Edu.−.018.147−.020.148−.049.123−.010.142Dummy Industry & Transport−.074.110−.075.112−.134.107−.102.130Dummy Services, Retail & Media.014.110.006.112−.031.105.044.136Uncertainty2.054**.020.041*.020.035.022.027.022Constant1.826***.0871.841***.0891.890***.0791.876***.100Pseudo-R2.161.171.208.250Log-likelihood−800.078−798.029−793.395−788.232Log-likelihood χ2 diff. test (d.f.)b—5.07(1)*11.33(7)344.20(4) ***cN (salespeople)211211211211 9 *p < .05.3 **p < .01.4 ***p < .001; unstandardized coefficients.10 a To determine the extent of bias in our estimates ([24]), we also examined an extension of Model 8 in which we added Magnitude2 × Uncertainty (ζ7) to our equation. The results show that the coefficient of this interaction is not statistically different from zero (ζ7 = .018, p > .10) and does not improve model fit (χ21 = .660, p > .10). See also Web Appendix W7.11 b When using the MLR estimator in Mplus, a log-likelihood difference test statistic is calculated using log-likelihoods and scaling correction factors for the null and alternative models.12 c Model fit of Model 10 is also significantly better than that of Model 8 (χ21 = 41.97, p < .001).13 Notes: Magnitude = opportunity magnitude; uncertainty = opportunity conversion uncertainty. Moderating role of conversion uncertaintyTo retest H2, we add ζ6(magnitude × uncertainty)jh to the equation and test its significance. Model 8 in Table 3 shows that the interaction is significant and negative (ζ6 = −.044, p < .05). We estimate the change in the turning point using the same approach we reported in Study 1. The result again confirms H2, as the change in the turning point is negative and significant (b = −.301, p < .05). Salesperson characteristics as moderatorsTo test the moderating role of past performance success and salesperson experience, we extend Model 8's equation and test ζ11 and ζ16. Model 10 in Table 3 shows that ζ11 is negative and significant (ζ11 = −.058, p < .05), in support of H4. By contrast, ζ16 is not significant (ζ16 = .016, p > .10) and thus does not support a shifting effect, as postulated in H5. Instead, we find a significant, negative curvilinear moderating effect of salesperson experience (ζ17 = −.070, p < .01), suggesting a flipping effect.We performed several additional robustness checks of our results. First, we used the Wilcoxon rank-sum test (p = .270) to compare respondents and nonrespondents. The tests showed no significant differences between respondents and nonrespondents, alleviating concerns about self-selection bias in our sample. Second, results from Ramsey's regression error specification test (RESET) (χ2 = 1.14, p = .285) alleviate concerns about omitted variables. Third, the maximum variance inflation factors is 3.62, well below the threshold value of 10 ([25]), indicating no multicollinearity issues. Fourth, to test heteroskedasticity we conducted the Cameron–Trivedi test (p = .337) and Breusch–Pagan test (p = .269), neither of which was significant, thus alleviating concerns about heteroskedasticity in our results. DiscussionThe results of Study 2 not only corroborate the key findings of Study 1 in a different context but also reveal the boundary conditions of salesperson calibration. Figure 4, Panel A, reveals that high past performance success triggers COR, whereas low past performance success provokes more risk-seeking behavior. Salespeople with high past performance success perform best under low uncertainty, whereas those with low past performance success do better under high uncertainty (see right-hand and left-hand sides, respectively). Figure 4, Panel B, shows that highly experienced salespeople perform best under the most challenging situations (low/moderate opportunity magnitude; high conversion uncertainty). However, the left-hand side shows that for inexperienced salespeople, high conversion uncertainty dampens quota achievement significantly. These findings suggest that less experienced salespeople tend to conserve resources under high conversion uncertainty, whereas highly experienced salespeople are more willing to bear uncertainty because they have more resources available.Graph: Figure 4. Study 2: three-way moderating effects (opportunity magnitude × opportunity conversion uncertainty × salesperson characteristics). Study 3: Unpacking the Benefit–Cost Mechanisms in ProspectingStudy 3, a scenario-based experiment, has three objectives. First, we replicate the inverted U-shaped effect of opportunity magnitude on sales performance in a controlled setting. Second, we unpack the underlying benefit–cost mechanism of this effect assumed in Studies 1 and 2. Third, we examine the role of resource slack to show the appropriateness of using COR theory. Study 3 Hypothesis DevelopmentAccording to COR theory, salespeople with limited resources are more likely to conserve them than those with abundant resources ([26]; [30]). For the former, small opportunities do not provide significant resources, whereas large opportunities are prohibitively resource straining. Thus, the indirect negative effect of opportunity magnitude on willingness to pursue an opportunity through costs is amplified when salesperson resource slack is limited. For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs follows an inverted U-shape. By contrast, salespeople with high resource slack are motivated to pursue larger opportunities because they have the resources and, by mobilizing them, can gain even more resources ([26]). For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs is convex. Thus, H6:  Salesperson resource slack buffers the negative effect of costs on salesperson willingness to pursue an opportunity. As a corollary, the total indirect effect of opportunity magnitude on salesperson willingness to pursue an opportunity follows an inverted U-shape only when salesperson resource slack is low. Method and Results Sample and procedureGiven the consistent findings of an inverted U-shape across levels, in Study 3 we focus on the prospect level. We partnered with a prominent market research firm to access a diverse panel of salespeople from various industries. The research firm randomly recruited 216 experienced salespeople (64% 36–45 years of age, 62% male, 53% in the information technology industry) for our between-subjects experiment.We then randomly assigned them to one of five scenarios. Each scenario informed participants that they were assigned a territory where the typical revenue of a prospect was $50,000. We included this portfolio baseline information to ensure the design matches with Study 1 and real-life selling contexts. They identified a new sales lead (Prospect A) with a specific opportunity magnitude. In line with data from Study 1, we set the opportunity magnitude at five levels: $1,000, $10,000, $50,000, $250,000, and $1,000,000. After participants read the scenario, we assessed their willingness to pursue Prospect A, anticipated costs, and anticipated benefits on a seven-point scale (1 = ""strongly disagree,"" and 7 = ""strongly agree"") and their resource slack for prospecting activities. We used the natural variation of salespeople's resource slack in their jobs, as previous research shows that resource slack affects people's framing of costs and benefits in decision making ([68]). Post hoc tests indicated that resource slack was not differently distributed between treatment groups (F = .30; p > .10), thereby providing evidence that the manipulation itself did not affect participants' perceptions of resource slack. We included the manipulation check, attention and realism checks and demographic questions. ResultsAnalysis of variance revealed significant between group differences in willingness to pursue the prospect (F( 4, 211) = 2.835, p = .025). Specifically, willingness to pursue is significantly greater (p < .05) in the $50,000 condition (5.50) than in the small ($1,000; 4.75) or large ($1,000,000; 4.96) conditions. Thus, we replicate the inverted U-shaped effect of opportunity magnitude found in Studies 1 and 2. We then specified the path model of the Study 3 panel in Figure 1. We find that the effect of costs on willingness to pursue is contingent on resource slack (b = .242, p < .01). To test the mediating benefit–cost mechanism and the COR effect, we examined the ""instantaneous conditional indirect effect"" of opportunity magnitude on willingness to pursue, with salesperson resource slack as the moderator ([29]). The results show that when resource slack is low, the total indirect effect of opportunity magnitude through the two mediators is only significant at moderate levels of opportunity magnitude (θopp.mag=2 = .111, p < .05). When resource slack is high, moderate to high levels of opportunity magnitude translate significantly into willingness to pursue (θopp.mag=5 = .202, p < .01). These results lend support to H6 and our contention that, under resource constraints, the inverted U-shaped effect of opportunity magnitude operates through the benefit–cost mechanism. For further details, see Web Appendix W8. General DiscussionIntegrating decision-making and COR theories, we develop and test a framework of salesperson decision making when prospecting in three multimethod studies. Together, the empirical evidence explains why salespeople avoid big-whale sales opportunities. Theoretical ContributionsOur research stems from the idea that salespeople differ from participants in studies that focus on low-effort, constraint-free, and repeatable choices ([36]). First, the potential benefits—extrinsic and/or intrinsic—of salespeople's decisions are consequential rather than trivial. Second, given their resource constraints and the ephemeral nature of sales opportunities, their costs—explicit and/or implicit—are not negligible. Third, their decision-making context abounds with uncertainties. Our findings confirm and provide novel insights into the theoretical importance of these differences for research on salespeople's decision making, especially when prospecting. Benefit–cost analysis in salesperson prospectingWe provide strong empirical evidence that in deciding on which opportunities to pursue, salespeople conduct a benefit–cost analysis based on their initial judgment of opportunity magnitude. We show that the relationship between initial judgment of opportunity magnitude and actual conversion follows an inverted U-shape, regardless of selling task (product vs. solution selling) and information level (prospect vs. portfolio). This finding debunks the intuition that salespeople gravitate toward big-whale opportunities, an insight that extends current understanding of salesperson prospecting behavior. Our result also confirms that salespeople's initial judgment of opportunity magnitude exerts a strong impact on their subsequent behavior and performance, even after controlling for transient phases. This finding complements prior research on salesperson intuition ([27]) and on primacy and anchoring effects ([58]). Salesperson calibration for conversion uncertaintyWe also found that solution-selling salespeople take into consideration opportunity conversion uncertainty in their benefit–cost analysis. Due to this calibration, the inverted U-shaped relationship between opportunity magnitude and performance shifts to the left. This shift implies that salespeople are generally more risk-seeking when opportunity magnitude ranges from small to moderate and risk-averse when opportunity magnitude is large. The counterfactual analyses we conducted show that the calibration effect reduces misspecification of conversion rates by up to 100%, when compared with the estimates from a compensatory decision strategy in which uncertainty is simply factored in as an extra cost. This finding provides a more nuanced understanding of the differences between salespeople's decision-making strategies (i.e., calibration vs. compensatory) when prospecting. Furthermore, it joins two separate streams of research on salesperson decision making, one focusing on salesperson judgment of demands and the other on uncertainty. Selling contexts and calibrationCompared with product selling, solution selling is full of uncertainties (e.g., need, process, outcome; [60]). Although these uncertainties are likely to influence salesperson behavior, salesperson behavior and decision making in solution selling has not received much academic research. We contribute to the literature by showing that salespeople indeed use different decision-making strategies in solution selling versus product selling. Specifically, they rely on a calibration decision-making strategy only in solution selling and only at the portfolio level. At the prospect level, regardless of the selling context, salespeople assess each individual opportunity relative to their portfolio baseline in terms of magnitude and conversion uncertainty using a compensatory strategy in which a large magnitude can make up for high uncertainty (and vice versa). Salesperson characteristics and calibration in solution sellingWe find that salesperson past performance success and salesperson experience are important contingencies of salespeople's decision-making process under uncertainty (i.e., calibration). The interaction plots (Figure 4) suggest that salespeople who have achieved past performance success and/or have low experience tend to conserve their resources and become more risk averse when selling solutions. They perform better under low or average than high conversion uncertainty conditions. Experienced salespeople are able to overcome this cautious approach.Our findings also address the contrast between the COR perspective ([30]) and the risk-seeking perspective based on research on risky choice, such as gambling ([56]). While the latter perspective is not specific to the selling context, the COR argument is uniquely relevant to the personal selling context, as it accounts for the notions that ( 1) salespeople are subject to resource constraints and their efforts are costly and ( 2) salespeople need to conserve resources to avoid stress in the long run ([48]). Therefore, researchers who apply decision-making theories to the context of personal selling will benefit from accounting for the uniqueness of salespeople as decision makers. Information level and calibrationOur findings highlight a dual information-processing framework in salesperson decision making when prospecting for solution-selling opportunities ([51]; [58]). Under these conditions, we find that salesperson performance is a function of two processes. At the prospect level, salespeople rely on a simple compensatory model in their decision making, such that a large magnitude can make up for high uncertainty (and vice versa). At the portfolio level, they integrate information about both the magnitude and uncertainty of the prospects they targeted in a more complex calibration model. In this decision-making strategy, conversion uncertainty interacts with opportunity magnitude in driving salesperson portfolio performance. This insight is a meaningful step toward a better understanding of salespeople's prioritization of resources and the importance of considering salesperson characteristics in prospecting. It also sheds first light on potential differences between findings at the prospect level and those at the portfolio level (i.e., a lack of homology) and calls for additional multilevel research of this kind. Managerial ImplicationsOur findings provide both managers and salespeople with several new insights into salesperson decision making when prospecting. We underscore key performance implications of our findings by simulating several what-if analyses using the parameters from our results. Managing salespeople's avoidance of large opportunitiesThe results from three studies consistently show that, all else being equal, salespeople are likely to gravitate toward medium-sized opportunities, leaving smaller and larger opportunities unattended. Using simulated data from Study 1, we find that a salesperson with a prospect whose magnitude equals their baseline opportunity magnitude of ∼$26,750 will have an 86.5% probability of successful conversion. Yet receiving a new prospect that is 1 SD larger in terms of magnitude (∼$143,250) will decrease the conversion odds by more than 15%. This effect is due to the propensity to conserve resources when there are constraints, as Study 3 further shows that the anticipated costs only affect salespeople's pursuit of large prospects when operating under resource constraints.Thus, to assuage salespeople's avoidance of big-whale deals, managers can leverage their firms' CRM databases. Specifically, a manager can use historical CRM data to calculate the baseline estimates of opportunity magnitude (and conversion uncertainty) for each salesperson. Then, the manager can use this information to match marketing-generated prospects with a salesperson's portfolio baseline, because a large difference in opportunity magnitude between a new opportunity and the salesperson baseline is demotivating and decreases conversion success. Furthermore, when necessary, managers should alter salespeople's benefit–cost calculus when prospecting. For example, they should provide salespeople who work on relatively large opportunities with extra benefits (both extrinsic and intrinsic) and additional resources (to relax the resource constraints), thereby increasing the likelihood of conversion. In addition, managers could pair salespeople with peers with larger portfolio baselines to create ad hoc sales teams to follow up. Such a temporary arrangement can reduce the costs for the focal salespeople. Salesperson calibration for conversion uncertainty in solution sellingOur findings show that salespeople's decision-making strategy differs between solution and product selling. For product selling, salespeople rely on a compensatory decision-making strategy at both prospect and portfolio levels. For solution selling, however, they rely on a calibration decision-making strategy, which is noncompensatory in nature, with conversion uncertainty acting as the calibrator of opportunity magnitude at the portfolio level. This calibration effect underscores the important role of portfolio-level information, in terms of both magnitude and conversion uncertainty, in salesperson decision making for solutions. Thus, sales managers should pay close attention to this important ""between-salespeople"" difference when reallocating prospects for maximum effect. Continuing with the previous example from the simulated data, a sales manager could intuitively decide to allocate the solution-selling prospect of ∼$143,250 to a salesperson with a portfolio baseline magnitude of about the same size (i.e., ∼$143,250). However, if this salesperson's portfolio baseline conversion uncertainty is 1 SD higher, the probability of closing the deal decreases by 36.2%. To reduce conversion uncertainty, managers can play an active role by, for example, using more behavior-based control to curtail salespeople's pursuit of highly uncertain opportunities and providing them with more frequent feedback. Firms can also leverage advanced sales analytics capabilities to decrease uncertainty in opportunity costs and train salespeople how to use the information in their prospecting decisions. Boundary conditions of conversion uncertainty calibration for solution sellingOur results indicate that past performance success and experience can alter the way salespeople calibrate for conversion uncertainty. Thus, these two variables are important for managers as well as salespeople. In a post hoc analysis, we used Study 2's results to predict salesperson quota achievement under various combinations of levels of salespeople's past performance success and experience (±1 SD as high and low values). Drawing on the results summarized in Table 4, Panel A, we derive the most effective managerial actions for managing salesperson prospecting in Table 4, Panel B. Three insights are worth noting. First, regardless of past performance success, salespeople's quota attainment is the worst when they gravitate toward highly certain but small opportunities. Second, salespeople who performed well in the past are most likely to ""hit"" quota again when the portfolio opportunity is large and conversion uncertainty is low (96%). Nevertheless, these high performers become average performers when conversion uncertainty and portfolio magnitude are average (ranging from 50% to 79%). Therefore, an effective way to manage high performers' prospecting is to give them a large portfolio but also help them reduce conversion uncertainty. This combination allows them to conserve resources while also maintaining high levels of performance. By contrast, salespeople who performed poorly in the past can achieve a quota attainment as high as 83% when they have a large portfolio and conversion uncertainty is high. The increase in opportunity magnitude is more motivating to these poor performers—they are willing to exert greater efforts without conserving resources to prove themselves. Thus, an effective, but perhaps counterintuitive, way to manage poor performers' prospecting is to give them a larger, more uncertain portfolio to challenge them.GraphTable 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. A: Post Hoc Analysis for Study 2Moderating Role of Salesperson Past Performance Success (H4)Low Past Performance Success (−1 SD)High Past Performance Success (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower36%40%48%39%50%64%Average68%67%71%84%81%79%1 SD higher67%76%83%96%70%50% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. Moderating Role of Salesperson Experience (H5)Low Salesperson Experience (−1 SD)High Salesperson Experience (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower29%28%27%54%76%106%Average106%75%53%53%75%106%1 SD higher118%84%60%60%62%64% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. B: Managerial TakeawaysObservation from DataSuggested Managerial ActionRegardless of past performance success, salespeople's quota attainment is worst when they gravitate toward highly certain but small opportunities.Point out the importance of ""bread-and-butter"" prospects, as salespeople may ignore them while such prospects could be of strategic importance.Salespeople with greater past performance success perform relatively well when portfolio opportunity is large and conversion uncertainty is low.Give these salespeople a large portfolio but also help them reduce conversion uncertainty (to help them conserve resources).Salespeople with lower past performance success tend to perform better for relatively larger and more uncertain portfolios.Give these salespeople a larger, more uncertain portfolio to challenge them, while also providing opportunities to recuperate from poor past performance.Inexperienced salespeople perform better for relatively larger, but certain portfolios.Reduce conversion uncertainty (e.g., via information provision, training) and provide them with larger portfolios.Experienced salespeople tend to perform especially well for relatively small to moderate portfolio magnitude with relatively high conversion uncertainty.Challenge these salespeople with opportunities that have high conversion uncertainty, while ensuring the portfolio itself is not too large. 14 Notes: 100% = on-target performance. Shaded boxes reflect higher levels of quota achievement.Third, when conversion uncertainty is reduced, inexperienced salespeople who handle a large portfolio can go from zero to hero, as their quota attainment increases from 60% to 118%. Inexperienced salespeople also achieve low quota (under 30%) when their portfolio is small, regardless of conversion uncertainty. Therefore, an effective way to manage inexperienced salespeople's prospecting is to reduce conversion uncertainty and provide them with ample opportunities. By contrast, experienced salespeople do not perform well when their portfolio opportunity is large, regardless of conversion uncertainty (range: 60%–64%). However, they thrive under high conversion uncertainty and when their portfolio is moderate in size, with a quota attainment exceeding 100%. Thus, an effective way to manage experienced salespeople is to challenge them with opportunities that have high conversion uncertainty, while ensuring the portfolio opportunity itself is not too large.What do these results mean for salespeople? Our results show that salespeople need to be cognizant of potential biases created by their past performance success and experience. This is because these biases can significantly improve or impair their sales performance, as indicated by the aforementioned potential gains and losses in quota attainment. By changing the benefit–cost analysis and reducing factors that drive conversion uncertainty (e.g., learn from peers, ask managers for support, form ad hoc teams), salespeople can become more effective in closing big-whale deals and hitting their targets despite conversion uncertainty. Limitations and Future Research DirectionsWhile our research covers three empirical contexts and our data came from multiple sources, this article has several limitations. First, although opportunity magnitude and conversion uncertainty are two of the most important factors in salesperson decision making, they are by no means the only factors. In our research, we included several contingencies and control variables to account for heterogeneity. Nevertheless, we urge further research to consider other aspects as contingencies of salesperson calibration, such as salesperson perceptual accuracy in forming judgments of opportunity magnitude and uncertainty, customer characteristics, competition, and the source of the sales leads ([46]; [48]). Future research could also explore how managers can influence salesperson calibration (e.g., through incentives, by changing baseline conversion uncertainty via altering the composition of self-generated and assigned leads). Moreover, the nature of uncertainty itself and how it affects judgment and decision making could be further explored. Second, we focus on portfolio baseline magnitude and conversion uncertainty as the frames of reference for how salespeople form relative comparisons of prospects in their portfolios. Although this focus is both theoretically and empirically justified, further research could examine other reference points as suggested in the judgment–decision making literature (e.g., sales goals, status quo, minimum requirement). Other measures of magnitude, such as customer lifetime value, could also be examined.Third, we control for several time-related effects in Study 2 but did not examine the dynamics. Although the first impression generally serves as the anchor point, people adjust their anchors as they receive new information ([58]). Further research could examine how salespeople update their judgment of uncertainty over time by exploring how this effect manifests itself in salesperson prospecting. Fourth, we focus on salesperson past performance success and salesperson experience as moderators, but other moderators may exist, such as control systems and training. Finally, although theoretical arguments exist in support of the moderating role of salespeople's past performance success and experience, future research could explicitly test how these contingencies influence the underlying benefit–costs analysis. "
