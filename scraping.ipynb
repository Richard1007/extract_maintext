{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['A New Livestream Retail Analytics Framework to Assess the Sales Impact of Emotional Displays']\n",
    "API_KEY = '<670656238a7ae259c899c553e69a1807>'\n",
    "\n",
    "def get_url(url):\n",
    "    payload = {'api_key': API_KEY, 'url': url, 'country_code': 'us'}\n",
    "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\n",
    "    return proxy_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://scholar.google.co.jp/scholar?&as_sdt=0%2C5&as_q=A%20New%20Livestream%20Retail%20Analytics%20Framework%20to%20Assess%20the%20Sales%20Impact%20of%20Emotional%20Displays\n",
      "ci_num_list []\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import csv\n",
    "# import datetime\n",
    "# import difflib\n",
    "# import os\n",
    "# import pprint\n",
    "import re\n",
    "import time\n",
    "# import timeit\n",
    "# import warnings\n",
    "from time import sleep\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def make_url(keyword, conf, author, year, paper_id=None):\n",
    "    \"\"\"make url for search papers\n",
    "    normal search (keyword, conf, author, year) or target search (paper_id)\n",
    "    :param keyword: str or None\n",
    "    :param conf: str or None, conference information\n",
    "    :param author: str or None, author information\n",
    "    :param year: int or None, published year\n",
    "    :param paper_id: None or int, paper information\n",
    "    :return: url\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        keyword is not None\n",
    "        or conf is not None\n",
    "        or author is not None\n",
    "        or year is not None\n",
    "        or paper_id is not None\n",
    "    ), \"KeywordNotFoundError\"\n",
    "    url = \"https://scholar.google.co.jp/scholar?\"\n",
    "    if paper_id is not None:\n",
    "        url += f\"&cites={paper_id}\"\n",
    "    else:\n",
    "        url += \"&as_sdt=0%2C5\"\n",
    "        if keyword is not None:\n",
    "            url += f\"&as_q={'%20'.join(keyword.split())}\"\n",
    "        else:\n",
    "            url += \"&as_q=\"\n",
    "        if conf is not None:\n",
    "            url += f\"&as_publication={'%20'.join(conf.split())}\"\n",
    "        if author is not None:\n",
    "            author = \"+\".join(author.split())\n",
    "            url += f\"&as_sauthors={'%20'.join(author.split())}\"\n",
    "        if year is not None:\n",
    "            url += f\"&as_ylo={year}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_snippet(soup):\n",
    "    \"\"\"obtain snippet from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: snippet_list\n",
    "    \"\"\"\n",
    "    tags = soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
    "    snippet_list = [tags[i].text for i in range(len(tags))]\n",
    "    return snippet_list\n",
    "\n",
    "\n",
    "def get_title_and_url(soup):\n",
    "    \"\"\"obtain title and url from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: title_list, url_list\n",
    "    \"\"\"\n",
    "    tags1 = soup.find_all(\"h3\", {\"class\": \"gs_rt\"})\n",
    "    title_list = []\n",
    "    url_list = []\n",
    "    for tag1 in tags1:\n",
    "        # タイトル取得\n",
    "        # PDF, 書籍, B, HTML, 引用, Cのタグを除去\n",
    "        title = re.sub(r\"\\[(PDF|書籍|B|HTML|引用|C)\\]\", \"\", tag1.text)\n",
    "        # 空白区切りを廃止\n",
    "        title = \"_\".join(title.split(\" \"))\n",
    "        if title[0] == \"_\":\n",
    "            title = title[1:]\n",
    "        title_list.append(title)\n",
    "\n",
    "        # url取得\n",
    "        try:\n",
    "            url = tag1.select(\"a\")[0].get(\"href\")\n",
    "            url_list.append(url)\n",
    "        except IndexError:\n",
    "            url_list.append(None)\n",
    "    return title_list, url_list\n",
    "\n",
    "\n",
    "def get_writer_and_year(soup):\n",
    "    \"\"\"obtain writer(author) and year from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: writer_list, year_list\n",
    "    \"\"\"\n",
    "    tags2 = soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "    writer_list = []\n",
    "    year_list = []\n",
    "    for tag2 in tags2:\n",
    "        # 著者取得\n",
    "        \"\"\"\n",
    "        writer = tag2.text\n",
    "        writer = re.sub(r\"\\d\", \"\", writer)\n",
    "        for char in range(0, len(writer)):\n",
    "            if writer[char] == \"-\":\n",
    "                writer = writer[2 : char - 1]\n",
    "                break\n",
    "        \"\"\"\n",
    "        writer = tag2.text.split(\"\\xa0- \")[0]\n",
    "        writer_list.append(writer)\n",
    "\n",
    "        # 論文発行年取得\n",
    "        year = tag2.text\n",
    "        year = re.sub(r\"\\D\", \"\", year)\n",
    "        # yearが5桁以上だった場合の例外処理\n",
    "        if len(year) > 4:\n",
    "            year_list.append(year[len(year) - 4 : len(year)])\n",
    "        else:\n",
    "            year_list.append(year)\n",
    "    return writer_list, year_list\n",
    "\n",
    "\n",
    "def get_citations(soup):\n",
    "    \"\"\"obtain number of citations from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: ci_num_list\n",
    "    \"\"\"\n",
    "    tags3 = soup.find_all(text=re.compile(\"被引用\"))\n",
    "    ci_num_list = []\n",
    "    for tag3 in tags3:\n",
    "        # 被引用数取得\n",
    "        citation = tag3.replace(\"被引用\", \"\")[1:3]\n",
    "        ci_num_list.append(int(citation))\n",
    "    return ci_num_list\n",
    "\n",
    "\n",
    "def get_id(soup):\n",
    "    \"\"\"obtain paper id from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: ci_num_list\n",
    "    \"\"\"\n",
    "    tags4 = soup.find_all(\"div\", {\"class\": \"gs_fl\"})\n",
    "    p_id_list = []\n",
    "    for tag4 in tags4:\n",
    "        # 論文ID取得\n",
    "        try:\n",
    "            elem = tag4.find_all(\"a\")[2][\"href\"]\n",
    "            a = 15\n",
    "            while True:\n",
    "                if elem[a] == \"&\":\n",
    "                    break\n",
    "                a += 1\n",
    "            p_id_list.append(elem[15:a])\n",
    "        except:\n",
    "            print(\"\")\n",
    "    return p_id_list\n",
    "\n",
    "def year_list_to_cite_years(year_list,p_year):\n",
    "    \"\"\"convert year_list into cite_years\n",
    "    :param year_list,p_year:\n",
    "    :return: cite_years\n",
    "    \"\"\"\n",
    "    year_list_int = []\n",
    "    for s in year_list:\n",
    "        try:\n",
    "            year_list_int.append(int(s))\n",
    "        except:\n",
    "            pass\n",
    "    y = [p_year+i for i in range(2021 - p_year + 1)]\n",
    "    cite_years = [0 for _ in range(2021 - p_year + 1)]\n",
    "    for year in year_list_int:\n",
    "        if year >= p_year and year <= 2021:\n",
    "            cite_years[year - p_year] += 1\n",
    "    list_return = [y, cite_years]\n",
    "#    cite_years = pd.DataFrame(cite_years,\n",
    "#                       index=y,\n",
    "#                       columns=['total'])\n",
    "#    cite_years  = cite_years.T\n",
    "    return list_return\n",
    "\n",
    "def grep_candidate_papers(url):\n",
    "    \"\"\"scrape first 10 papers and choose one\n",
    "    :param url:\n",
    "    :return: target paper information (title, writer, year, citations, url, paper_id, snippet)\n",
    "    \"\"\"\n",
    "    html_doc = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "\n",
    "    title_list, url_list = get_title_and_url(soup)\n",
    "    writer_list, year_list = get_writer_and_year(soup)\n",
    "    ci_num_list = get_citations(soup)\n",
    "    print('ci_num_list',ci_num_list)\n",
    "    p_id_list = get_id(soup)\n",
    "    snippet_list = get_snippet(soup)\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"paper number: {str(i)}\")\n",
    "        print(f\"paper title: {title_list[i]}\")\n",
    "        print(f\"published year: {year_list[i]}\")\n",
    "\n",
    "        # print(f\"citations: {ci_num_list}\")\n",
    "        print(f\"citations: {ci_num_list[i]}\")\n",
    "\n",
    "    target_paper_num = -1\n",
    "    while target_paper_num < 0 or target_paper_num >= len(title_list):\n",
    "        target_paper_num = int(input(\"Select paper number: \"))\n",
    "        if target_paper_num < 0 or target_paper_num >= len(title_list):\n",
    "            print(\"Index out of range! Please re-enter\")\n",
    "\n",
    "    target_paper = {\n",
    "            \"title\": title_list[target_paper_num],\n",
    "        \"writer\": writer_list[target_paper_num],\n",
    "        \"year\": year_list[target_paper_num],\n",
    "        \"citations\": ci_num_list[target_paper_num],\n",
    "        \"url\": url_list[target_paper_num],\n",
    "        \"paper_id\": p_id_list[target_paper_num],\n",
    "        \"snippet\": snippet_list[target_paper_num],\n",
    "    }\n",
    "    return target_paper\n",
    "\n",
    "\n",
    "def scraping_papers(url):\n",
    "    \"\"\"scrape 100 papers\n",
    "    :param url: target url\n",
    "    :return: title_list, url_list, writer_list, year_list, ci_num_list, p_id_list, snippet_list\n",
    "    \"\"\"\n",
    "    url_each = url.split(\"&\")\n",
    "    url_each[0] = url_each[0] + \"start={}\"\n",
    "    url_base = \"&\".join(url_each)\n",
    "\n",
    "    title_list = []\n",
    "    url_list = []\n",
    "    writer_list = []\n",
    "    year_list = []\n",
    "    ci_num_list = []\n",
    "    p_id_list = []\n",
    "    snippet_list = []\n",
    "\n",
    "    for page in range(0, 100, 10):\n",
    "        print(\"Loading next {} results\".format(page + 10))\n",
    "        url_tmp = url_base.format(page)\n",
    "        html_doc = requests.get(url_tmp).text\n",
    "        soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "\n",
    "        title_list_tmp, url_list_tmp = get_title_and_url(soup)\n",
    "        writer_list_tmp, year_list_tmp = get_writer_and_year(soup)\n",
    "        ci_num_list_tmp = get_citations(soup)\n",
    "        p_id_list_tmp = get_id(soup)\n",
    "        snippet_list_tmp = get_snippet(soup)\n",
    "\n",
    "        title_list.extend(title_list_tmp)\n",
    "        url_list.extend(url_list_tmp)\n",
    "        writer_list.extend(writer_list_tmp)\n",
    "        year_list.extend(year_list_tmp)\n",
    "        ci_num_list.extend(ci_num_list_tmp)\n",
    "        p_id_list.extend(p_id_list_tmp)\n",
    "        snippet_list.extend(snippet_list_tmp)\n",
    "\n",
    "        sleep(np.random.randint(5, 10))\n",
    "    return (\n",
    "        title_list,\n",
    "        url_list,\n",
    "        writer_list,\n",
    "        year_list,\n",
    "        ci_num_list,\n",
    "        p_id_list,\n",
    "        snippet_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def write_csv(\n",
    "    conf,\n",
    "    title_list,\n",
    "    url_list,\n",
    "    writer_list,\n",
    "    year_list,\n",
    "    ci_num_list,\n",
    "    p_id_list,\n",
    "    snippet_list,\n",
    "):\n",
    "    \"\"\"write csv\n",
    "    :param conf, title_list, url_list, writer_list, year_list, ci_num_list, snippet_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"conference\",\n",
    "        \"title\",\n",
    "        \"writer\",\n",
    "        \"year\",\n",
    "        \"citations\",\n",
    "        \"url\",\n",
    "        \"paper ID\",\n",
    "        \"snippet\",\n",
    "    ]\n",
    "    path = \"data/conf_csv/\" + conf + \".csv\"\n",
    "    with open(path, \"w\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(labels)\n",
    "        for title, url, writer, year, ci_num, p_id, snippet in zip(\n",
    "            title_list,\n",
    "            url_list,\n",
    "            writer_list,\n",
    "            year_list,\n",
    "            ci_num_list,\n",
    "            p_id_list,\n",
    "            snippet_list,\n",
    "        ):\n",
    "            csv_writer.writerow([conf, title, writer, year, ci_num, url, p_id, snippet])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #conf = \"ICASSP\"\n",
    "    conf = 'arxiv'\n",
    "    keyword = \"A New Livestream Retail Analytics Framework to Assess the Sales Impact of Emotional Displays\"\n",
    "    year = \"2018\"\n",
    "    url = make_url(keyword=keyword, conf=None, author=None, year=None)\n",
    "    print(f\"url: {url}\")\n",
    "\n",
    "    # select target paper\n",
    "    target_paper = grep_candidate_papers(url)\n",
    "    print(f\"target paper: {target_paper}\")\n",
    "\n",
    "    # create paper list about target paper's citation\n",
    "    url_cite = make_url(\n",
    "        keyword=None, conf=None, author=None, year=None, paper_id=target_paper[\"paper_id\"]\n",
    "    )\n",
    "    (\n",
    "        title_list,\n",
    "        url_list,\n",
    "        writer_list,\n",
    "        year_list,\n",
    "        ci_num_list,\n",
    "        p_id_list,\n",
    "        snippet_list,\n",
    "    ) = scraping_papers(url_cite)\n",
    "\n",
    "    cite_year = year_list_to_cite_years(year_list,int(target_paper['year']))\n",
    "    print(cite_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(keyword, conf, author, year, paper_id=None):\n",
    "    \"\"\"make url for search papers\n",
    "    normal search (keyword, conf, author, year) or target search (paper_id)\n",
    "    :param keyword: str or None\n",
    "    :param conf: str or None, conference information\n",
    "    :param author: str or None, author information\n",
    "    :param year: int or None, published year\n",
    "    :param paper_id: None or int, paper information\n",
    "    :return: url\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        keyword is not None\n",
    "        or conf is not None\n",
    "        or author is not None\n",
    "        or year is not None\n",
    "        or paper_id is not None\n",
    "    ), \"KeywordNotFoundError\"\n",
    "    url = \"https://scholar.google.co.jp/scholar?\"\n",
    "    if paper_id is not None:\n",
    "        url += f\"&cites={paper_id}\"\n",
    "    else:\n",
    "        url += \"&as_sdt=0%2C5\"\n",
    "        if keyword is not None:\n",
    "            url += f\"&as_q={'%20'.join(keyword.split())}\"\n",
    "        else:\n",
    "            url += \"&as_q=\"\n",
    "        if conf is not None:\n",
    "            url += f\"&as_publication={'%20'.join(conf.split())}\"\n",
    "        if author is not None:\n",
    "            author = \"+\".join(author.split())\n",
    "            url += f\"&as_sauthors={'%20'.join(author.split())}\"\n",
    "        if year is not None:\n",
    "            url += f\"&as_ylo={year}\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(soup):\n",
    "    \"\"\"obtain number of citations from soup\n",
    "    :param soup: parsed html by BeautifulSoup\n",
    "    :return: ci_num_list\n",
    "    \"\"\"\n",
    "    tags3 = soup.find_all(text=re.compile(\"被引用\"))\n",
    "    ci_num_list = []\n",
    "    for tag3 in tags3:\n",
    "        # 被引用数取得\n",
    "        citation = tag3.replace(\"被引用\", \"\")[1:3]\n",
    "        ci_num_list.append(int(citation))\n",
    "    return ci_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.co.jp/scholar?&as_sdt=0%2C5&as_q=Blame%20the%20Bot:%20Anthropomorphism%20and%20Anger%20in%20Customer–Chatbot%20Interactions\n"
     ]
    }
   ],
   "source": [
    "keyword = \"Blame the Bot: Anthropomorphism and Anger in Customer–Chatbot Interactions\"\n",
    "url = make_url(keyword=keyword, conf=None, author=None, year=None)\n",
    "html_doc = requests.get(url).text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "print(get_citations(soup))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
